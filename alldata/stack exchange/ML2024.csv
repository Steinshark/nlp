Post Id,Parent Id,Body,Score,PostType
"77743214","","<p>I am trying to train some machine learning models to predict the price action for 4 chosen stocks from a list of NASDAQ-100 stocks.</p>
<p>I am very new to Python, so I've run into a few issues I have not been able to fix. The first has been while trying to use the ARIMA model. I get the following error upon executing my code:</p>
<p><code>None if faux_endog else np.any(np.isnan(self.endog))) TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''</code></p>
<p>I have already tried using <code>dropna()</code>, <code>fillna()</code> and <code>isna()</code> to find/remove NaN or NULL values. Therefore there should be none left.</p>
<p>This is my code:</p>
<pre><code># Imports
import os
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import io
from PIL import Image
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf

# Chosen stocks from NASDAQ-100
chosen_stocks = ['CTSH', 'BKNG', 'REGN', 'MSFT']

def get_data():
    # Get list of tickers
    tickers = open(&quot;dataset/nasdaq_100_tickers.txt&quot;, &quot;r&quot;)
    data = tickers.read().splitlines()

    # Check if the data has already been downloaded, drop NaN values
    if os.path.exists('dataframe.csv'):
        dataframe = pd.read_csv('dataframe.csv', index_col=&quot;Date&quot;, parse_dates=True).dropna()
    else:
        # Download Close data from Yahoo Finance
        data = yf.download(tickers=data, period='1y', interval='1d')['Close']
        data.to_csv('dataframe.csv')
        # Convert array to Pandas dataframe, drop NaN values
        complete_data = data.dropna()
        dataframe = pd.DataFrame(complete_data)
    dataframe.drop(['GEHC'], axis=1, inplace=True) # Dropping GEHC because it contains NULL values

    return dataframe



def arima_prediction(stock):
    train_data, test_data = stock[3:int(len(dataframe) * 0.5)], stock[int(len(dataframe) * 0.5):]
    train_arima = train_data
    test_arima = test_data

    history = [x for x in train_arima]
    y = test_arima
    predictions = list()
    model = ARIMA(history, order=(1, 1, 0))
    model_fit = model.fit()
    forecast = model_fit.forecast()[0]
    predictions.append(forecast)
    history.append(y[0])

    for i in range(1, len(y)):
        # Predict
        model = ARIMA(history, order=(1, 1, 0))
        model_fit = model.fit()
        forecast = model_fit.forecast()[0]
        # Invert transformed prediction
        predictions.append(forecast)
        # Observation
        observation = y[i]
        history.append(observation)

    # Report performance
    mean_squared = mean_squared_error(y, predictions)
    print('Mean Squared Error: ' + str(mean_squared))
    mean_absolute = mean_absolute_error(y, predictions)
    print('Mean Absolute Error: ' + str(mean_absolute))
    root_mean_squared = math.sqrt(mean_squared_error(y, predictions))
    print('Root Mean Squared Error: ' + str(root_mean_squared))

dataframe = get_data()
for stock in chosen_stocks:
    arima_prediction(stock)
</code></pre>
<p>My dataframe looks like this:</p>
<pre><code>                  AAPL        ABNB  ...         ZM          ZS
Date                                ...                       
2022-12-15  136.500000   90.610001  ...  70.199997  117.169998
2022-12-16  134.509995   89.570000  ...  69.860001  114.209999
2022-12-19  132.369995   85.930000  ...  69.089996  112.269997
2022-12-20  132.300003   87.620003  ...  68.559998  113.540001
2022-12-21  135.449997   87.070000  ...  69.930000  112.769997
...                ...         ...  ...        ...         ...
2023-11-28  190.399994  127.559998  ...  67.529999  193.850006
2023-11-29  189.369995  126.480003  ...  67.949997  199.839996
2023-11-30  189.949997  126.339996  ...  67.830002  197.529999
2023-12-01  191.240005  135.020004  ...  70.290001  198.029999
2023-12-04  188.669998  134.539993  ...  67.720001  197.919998
</code></pre>
<p>The full traceback is:</p>
<pre><code>
Traceback (most recent call last):
  File &quot;C:/Users/xxx/source/repos/Project/main.py&quot;, line 370, in &lt;module&gt;
    arima_prediction(stock)
  File &quot;C:/Users/xxx/source/repos/Project/main.py&quot;, line 217, in arima_prediction
    model = ARIMA(history, order=(1, 1, 0))
  File &quot;C:\Users\xxx\source\repos\Project\venv\lib\site-packages\statsmodels\tsa\arima\model.py&quot;, line 158, in __init__
    self._spec_arima = SARIMAXSpecification(
  File &quot;C:\Users\xxx\source\repos\Project\venv\lib\site-packages\statsmodels\tsa\arima\specification.py&quot;, line 458, in __init__
    None if faux_endog else np.any(np.isnan(self.endog)))
TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

Process finished with exit code 1
</code></pre>
<p>Any help is appreciated.</p>
","0","Question"
"77743228","","<p>I have a subclass of torch.nn.Module, whose initialiser have the following form:
(in class A)</p>
<pre><code>def __init__(self, additional_layer=False):
    ...
    if additional_layer:
        self.additional = nn.Sequential(nn.Linear(8,3)).to(self.device)
    else:
        self.additional = None
    ...
    ...
</code></pre>
<p>I train with additional_layer=True and save the model with <code>torch.save</code>. The object I save is <code>model.state_dict()</code>. Then I load the model for inference. But then I get the following error:</p>
<pre><code>model.load_state_dict(best_model[&quot;my_model&quot;])

RuntimeError: Error(s) in loading state_dict for A:
        Unexpected key(s) in state_dict: &quot;additional.0.weight&quot;
</code></pre>
<p>Is using an optional field which can be None disallowed?? How to handle this properly? [Also posted <a href=""https://www.reddit.com/r/pytorch/comments/18w4ipz/handling_models_with_optional_members_can_be_none/"" rel=""nofollow noreferrer"">here</a>]</p>
","1","Question"
"77748547","","<p>Im trying to calculate the r squared value after the creation of a model using sklearn linear regression.</p>
<p>Im simply</p>
<ol>
<li>importing a csv dataset</li>
<li>filtering the interesting columns</li>
<li>splitting the dataset in train and test</li>
<li>creating the model</li>
<li>making a prediction on the test</li>
<li>calculating the r squared in order to see how good is the model to fit the test dataset</li>
</ol>
<p>the dataset is taken from <a href=""https://www.kaggle.com/datasets/jeremylarcher/american-house-prices-and-demographics-of-top-cities"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/jeremylarcher/american-house-prices-and-demographics-of-top-cities</a></p>
<p>the code is as following</p>
<pre><code>''' Lets verify if there s a correlation between price and beds number of bathroom'''

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df = pd.read_csv('data/American_Housing_Data_20231209.csv')

df_interesting_columns = df[['Beds', 'Baths', 'Price']]

independent_variables = df_interesting_columns[['Beds', 'Baths']]
dependent_variable = df_interesting_columns[['Price']]

X_train, X_test, y_train, y_test = train_test_split(independent_variables, dependent_variable, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

prediction = model.predict(X_test)

print(model.score(y_test, prediction))
</code></pre>
<p>but i get the error</p>
<p>ValueError: The feature names should match those that were passed during fit.
Feature names unseen at fit time:</p>
<ul>
<li>Price
Feature names seen at fit time, yet now missing:</li>
<li>Baths</li>
<li>Beds</li>
</ul>
<p>what am I doing wrong?</p>
","1","Question"
"77748737","","<p>I'm working on a program that calculates word and sentence embeddings using GPT-2, specifically the <code>GPT2Model</code> class. For word embedding, I extract the last hidden state <code>outputs[0]</code> after forwarding the <code>input_ids</code>, that has a shape of <code>batch size x seq len</code>, to the <code>GPT2Model</code> class. As for sentence embedding, I extract the hidden state of the word at the end of sequence. This is the code I have tried:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
captions = [&quot;example caption&quot;, &quot;example bird&quot;, &quot;the bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;very good&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = word_embedding[ :, -1, : ].contiguous()

</code></pre>
<p>I'm not sure if my calculation for word and sentence embedding are correct, can anyone help me confirm this?</p>
","0","Question"
"77749447","","<p>I'm trying to plot learning curves for different metrics such as training and validation loss and accuracy.</p>
<pre><code>for lr in learning_rates: 
    opt = SGD(learning_rate = lr)
    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    network = model.fit(x_train_subset, y_train_subset, epochs=iterations, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    train_loss_values = network.history['loss']
    train_accuracy_values = network.history['accuracy']
    val_loss_values = network.history['val_loss']
    val_accuracy_values = network.history['val_accuracy']

    plt.plot(epochs, train_accuracy_values, label=f'LR = {lr}')
    
plt.title('Training Accuracy over Epochs for Different Learning Rates')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<p>This is the code I have and it works exactly how I want it to for the one graph with training accuracy as the metric.</p>
<p>Output of the above code:</p>
<p><img src=""https://i.sstatic.net/72RRq.png"" alt="""" /></p>
<p>However, I want to replicate this four times for all the different metrics stored in the following code:</p>
<pre><code>train_loss_values = network.history['loss']
train_accuracy_values = network.history['accuracy']
val_loss_values = network.history['val_loss']
val_accuracy_values = network.history['val_accuracy']
</code></pre>
<p>If I simply do the following, it does not create 4 separate graphs, it just adds all data to one.</p>
<pre><code>for lr in learning_rates: 
    opt = SGD(learning_rate = lr)
    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    network = model.fit(x_train_subset, y_train_subset, epochs=iterations, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    train_loss_values = network.history['loss']
    train_accuracy_values = network.history['accuracy']
    val_loss_values = network.history['val_loss']
    val_accuracy_values = network.history['val_accuracy']

    plt.plot(epochs, train_accuracy_values, label=f'LR = {lr}')
    plt.plot(epochs, train_loss_values, label=f'LR = {lr}')
    plt.plot(epochs, val_accuracy_values, label=f'LR = {lr}')
    plt.plot(epochs, val_loss_values, label=f'LR = {lr}')
    
plt.title('Training Accuracy over Epochs for Different Learning Rates')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<p>I asked ChatGPT and it always suggested running the entire neural network again 4 times, but I have already run it once and collected all the data I need in lists. I just need to create 4 separate plots.</p>
","0","Question"
"77750389","","<ul>
<li>step1: pipeline is created</li>
<li>step2: convert pipleine to dataframe</li>
<li>step3: I am trying convert pipeline to data frame but exception is raised. how to solve this problem</li>
<li>step4: how to solve ValueError: Shape of passed values is (8631, 28), indices imply (8631, 17) on top of pipeline conversion to dataframe,</li>
</ul>
<pre><code>from sklearn.preprocessing import FunctionTransformer, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

import pandas as pd
from sklearn.model_selection import train_test_split

print(&quot;step1: import lib&quot;)
print(&quot;step2: loading raw data&quot;)
df = pd.read_csv(&quot;online_shoppers_intention.csv&quot;)

print(&quot;step3: data preparition&quot;)
X = df.drop(['Revenue'], axis = 1)
y = df['Revenue']

print(&quot;step4: data splitting&quot;)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 0)
names = X_train.columns.tolist()

numeric_transformer = SimpleImputer(strategy = 'constant')
categorical_transformer = OneHotEncoder(handle_unknown = 'ignore')

numerical_cols = X.select_dtypes(exclude = &quot;object&quot;).columns.values.tolist()
categorical_cols = X.select_dtypes(exclude = ['int', 'float64', 'bool']).columns.values.tolist()
    
preprocessor = ColumnTransformer(
    transformers=[
    ('num', numeric_transformer, numerical_cols)
    ,('cat', categorical_transformer, categorical_cols)
    ],
    remainder = 'passthrough')

pipe_preprocessor = Pipeline(steps = [(&quot;preprocessor&quot;, preprocessor), (&quot;pandarizer&quot;, FunctionTransformer(lambda x: pd.DataFrame(x, columns = names)))]).fit(X_train)
    
X_train_pipe = pipe_preprocessor.transform(X_train)
X_test_pipe = pipe_preprocessor.transform(X_test)
</code></pre>
","1","Question"
"77753838","","<p>I tried</p>
<pre class=""lang-html prettyprint-override""><code>&lt;td align=&quot;left&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;n1&quot;&gt;&lt;/td&gt;
</code></pre>
<p>to take input.</p>
<pre><code>17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189
</code></pre>
<p>I feed that into a numpy array in <code>view.py</code>.</p>
<pre><code>np.array((request.GET['n1']))
</code></pre>
<p>But I am getting the following error message.</p>
<pre><code>ValueError at /prediction/output
Expected 2D array, got 1D array instead:
array=['17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189'].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p><code>view.py</code>  (<code>v1 = np.array((request.GET['n1']))</code> is on the 8th line)</p>
<pre class=""lang-py prettyprint-override""><code>def output(request):
    dff = pd.read_csv(r'C:\Users\Downloads\data.csv')
    y = dff['diagnosis'].values
    x = dff.drop('diagnosis', axis=1).values
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.40)
    model = LogisticRegression()
    model.fit(x_train, y_train)
    v1 = np.array((request.GET['n1']))

pred = model.predict([v1])
    pred1 = &quot;&quot;
    if pred==[1]:
        pred1 = &quot;positive&quot;
    else:
        pred1 = &quot;negative&quot;
return render(request, 'prediction.html', {&quot;predictResult&quot;:**pred1**})

</code></pre>
<p><code>prediction.html</code> (the 6th line is to get the input)</p>
<pre class=""lang-html prettyprint-override""><code>&lt;div&gt;
    &lt;form action=&quot;output&quot;&gt;
        &lt;table &gt;
            &lt;tr&gt;
                &lt;td align=&quot;right&quot;&gt;Pregnancies&lt;/td&gt;
               &lt;td align=&quot;left&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;n1&quot;&gt;&lt;/td&gt;
            &lt;/tr&gt;
    &lt;/table&gt;
     &lt;input type=&quot;submit&quot;&gt;
    &lt;/form&gt;

    Result:{{ predictResult }}
&lt;/div&gt;
</code></pre>
","0","Question"
"77755646","","<p>I have this document</p>
<p>It is not normal text</p>
<p>It is a text of Scientific terminologies</p>
<p>The text of these documents are like this</p>
<pre><code>RepID,Txt

1,K9G3P9 4H477 -Q207KL41 98464 ... Q207KL41
2,D84T8X4 -D9W4S2 -D9W4S2 8E8E65 ... D9W4S2 
3,-05L8NJ38 K2DD949 0W28DZ48 207441 ... K2D28K84
</code></pre>
<p>I can build a feature set using BOW algorithm</p>
<p>Here is my code</p>
<pre><code>def BOW(df):
  CountVec = CountVectorizer() # to use only  bigrams ngram_range=(2,2)
  Count_data = CountVec.fit_transform(df)
  Count_data = Count_data.astype(np.uint8)
  cv_dataframe=pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names_out(), index=df.index)  # &lt;- HERE
  return cv_dataframe.astype(np.uint8)

df_reps = pd.read_csv(&quot;c:\\file.csv&quot;)
df = BOW(df_reps[&quot;Txt&quot;])
</code></pre>
<p>The result will be the count of words in the &quot;<strong>Txt</strong>&quot; column.</p>
<pre><code>RepID K9G3P9  4H477 -Q207KL41 98464 ... Q207KL41
1     2       8     3         2     ... 1
2     0       1     2         4     ... 2
</code></pre>
<p>The trick and here where I need the help, is that some of these terms have a <strong>-</strong> ahead of it, and that should count as negative value</p>
<p>So if the a text have these values <code>Q207KL41 -Q207KL41 -Q207KL41</code></p>
<p>in that case the terms that starts with - should be count as negative and therefore, the BOW for the <code>Q207KL41</code> is <strong>-1</strong></p>
<p>instead of having a feature for <code>Q207KL41</code> and <code>-Q207KL41</code>
they both count towards the same term <code>Q207KL41</code> but with <strong>positive and -negative</strong></p>
<p>so the dataset after BOW will look like this</p>
<pre><code>RepID K9G3P9  4H477 Q207KL41 98464 ... 
1     2       8     -2         2     ...
2     0       1     0         4     ...
</code></pre>
<p>How to do that?</p>
","0","Question"
"77760620","","<p>I am trying to train a simple binary classification with SetFit, but I have a problem with the library. I use huggingface to manage my dataset. The dataset does consist of a text and a label column. If I print my dataset it looks like this:</p>
<pre><code>dataset = load_dataset(&quot;&lt;my-dataset&gt;&quot;)
print(dataset)
</code></pre>
<p>with output:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 20
    })
    eval: Dataset({
        features: ['text', 'label'],
        num_rows: 10
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 135
    })
})
</code></pre>
<p>Here is my code for the training:</p>
<pre><code># Initialize SetFit model with a pre-trained model and define label name
model = SetFitModel.from_pretrained(
    &quot;paraphrase-multilingual-mpnet-base-v2&quot;,
    labels=[&quot;negative&quot;, &quot;positive&quot;],
)

# Define the training arguments
args = TrainingArguments(
    batch_size=32,
    num_epochs=8,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True
)

# Initialize the trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;eval&quot;],
    metric=&quot;accuracy&quot;,
    column_mapping={&quot;text&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;}  # Map dataset columns to text/label expected by trainer
)

# Train the model
trainer.train()
</code></pre>
<p>But the problem I now have is that the training behaves very weird. I do not get any Training or Validation losses, nor do the evaluation steps ever finish. I don't know what the problem is.</p>
<p><a href=""https://i.sstatic.net/GCvmK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GCvmK.png"" alt=""Overview of training"" /></a></p>
<p>Also please note, that I slightly changed the parameters to increase the training speed. It normally has more steps and so on. It still behaves weirdly with the normal parameters. I also use version 1.0.1 of SetFit. I haven't found any issues regarding this in the GitHub repository.</p>
","1","Question"
"77762264","","<p>This training curve is for a Transformer model that processes 2D (excluding batch) sequential signal and uses Adam optimizer, 32 batch size and for the learning rate: a custom LR Scheduler that replicates the warmup scheduler that is used at 'Attention is All You Need' paper. Training curve as below plateaus with eventual Training loss slightly lower than Validation loss, but training loss never starts back to climb, which I interpreted as the model never starts overfitting and just stops re-adjusting weights after around epoch 90.</p>
<p>Better interpretation and solutions to improve this model?</p>
<p><a href=""https://i.sstatic.net/Fi5OY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fi5OY.png"" alt=""enter image description here"" /></a></p>
<p>Below is my brief reproducible code:</p>
<pre><code>x_train = np.random.normal(size=(32, 512, 512))
batch_size = 32
H, W = x_train.shape
rows, cols = np.indices((H, W), sparse=True)
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[rows, 1:, cols] = 1
padding_mask = padding_mask_init[:batch_size]
embed_dim = 512
dense_dim = 2048
num_heads = 2
shape = (batch_size, embed_dim, 512) #(32, 512, 512)
decoder_inputs = layers.Input(batch_input_shape=shape, dtype=tensorflow.float16)
mha_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
layernorm_1 = layers.LayerNormalization()

Z = decoder_inputs
Z = mha_1(query=Z, value=Z, key=Z, use_causal_mask=True, attention_mask=padding_mask)
Z = layernorm_1(Z + decoder_inputs)
Z = mha_2(query=Z, value=decoder_inputs, key=decoder_inputs, attention_mask=padding_mask)
outputs = layers.TimeDistributed(keras.layers.Dense(embed_dim, activation=&quot;softmax&quot;))(Z)

model = keras.Model(decoder_inputs, outputs)
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule(embed_dim, 3000),beta_1=0.9,beta_2=0.98,epsilon=1.0e-9), metrics=[&quot;accuracy&quot;])

history = model.fit(dataset, epochs=200, validation_data=val_dataset)
</code></pre>
","-1","Question"
"77766048","","<p>I tried to  model the simplest coin flipping game where you have to predict if it is going to be a head.  Sadly it won't run, given me:</p>
<pre><code>Using cpu device
Traceback (most recent call last):
  File &quot;/home/user/python/simplegame.py&quot;, line 40, in &lt;module&gt;
    model.learn(total_timesteps=10000)
  File &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py&quot;, line 315, in learn
    return super().learn(
  File &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py&quot;, line 264, in learn
    total_timesteps, callback = self._setup_learn(
  File &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/base_class.py&quot;, line 423, in _setup_learn
    self._last_obs = self.env.reset()  # type: ignore[assignment]
  File &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py&quot;, line 77, in reset
    obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
TypeError: CoinFlipEnv.reset() got an unexpected keyword argument 'seed'
</code></pre>
<p>Here is the code:</p>
<pre><code>import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

class CoinFlipEnv(gym.Env):
    def __init__(self, heads_probability=0.8):
        super(CoinFlipEnv, self).__init__()
        self.action_space = gym.spaces.Discrete(2)  # 0 for heads, 1 for tails
        self.observation_space = gym.spaces.Discrete(2)  # 0 for heads, 1 for tails
        self.heads_probability = heads_probability
        self.flip_result = None

    def reset(self):
        # Reset the environment
        self.flip_result = None
        return self._get_observation()

    def step(self, action):
        # Perform the action (0 for heads, 1 for tails)
        self.flip_result = int(np.random.rand() &lt; self.heads_probability)

        # Compute the reward (1 for correct prediction, -1 for incorrect)
        reward = 1 if self.flip_result == action else -1

        # Return the observation, reward, done, and info
        return self._get_observation(), reward, True, {}

    def _get_observation(self):
        # Return the current coin flip result
        return self.flip_result

# Create the environment with heads probability of 0.8
env = DummyVecEnv([lambda: CoinFlipEnv(heads_probability=0.8)])

# Create the PPO model
model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)

# Train the model
model.learn(total_timesteps=10000)

# Save the model
model.save(&quot;coin_flip_model&quot;)

# Evaluate the model
obs = env.reset()
for _ in range(10):
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    print(f&quot;Action: {action}, Observation: {obs}, Reward: {rewards}&quot;)
</code></pre>
<p>What am I doing wrong?</p>
<p>This is in version 2.2.1.</p>
","2","Question"
"77769033","","<p>Suppose I construct an ensemble of two estimators, where each estimator runs its own parameter search:</p>
<p>Imports and regression dataset:</p>
<pre><code>from sklearn.ensemble import VotingRegressor, StackingRegressor, RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression

from sklearn.model_selection import RandomizedSearchCV

X, y = make_regression()
</code></pre>
<p>Define two self-tuning estimators, and ensemble them:</p>
<pre><code>rf_param_dist = dict(n_estimators=[1, 2, 3, 4, 5])
rf_searcher = RandomizedSearchCV(RandomForestRegressor(), rf_param_dist, n_iter=5, cv=3)

dt_param_dist = dict(max_depth=[4, 5, 6, 7, 8])
dt_searcher = RandomizedSearchCV(DecisionTreeRegressor(), dt_param_dist, n_iter=5, cv=3)

ensemble = StackingRegressor(
    [ ('rf', rf_searcher), ('dt', dt_searcher) ]
).fit(X, y)
</code></pre>
<p>My questions are about how <code>sklearn</code> handles the fitting of <code>ensemble</code>.</p>
<p>Q1) We have two unfitted estimators in parallel, and both need to be fitted before <code>ensemble.predict(...)</code> would work. But we can't fit any of the estimators without first getting a prediction from the ensemble. How does <code>sklearn</code> handle this circular dependency?</p>
<p>Q2) Since we have two estimators running independent tuning, does each estimator make the false assumption that the parameters of the other estimator are fixed? So we end up with a poorly-defined optimisation problem.</p>
<hr />
<p>For reference, I think the correct way to jointly optimise the models of an ensemble would be to define a single CV that searches over all parameters jointly, shown below. But my questions are about how <code>sklearn</code> handles the special case described earlier.</p>
<pre><code>#Joint optimisation
ensemble = VotingRegressor(
    [ ('rf', RandomForestRegressor()), ('dt', DecisionTreeRegressor()) ]
)

jointsearch_param_dist = dict(
    rf__n_estimators=[1, 2, 3, 4, 5],
    dt__max_depth=[4, 5, 6, 7, 8]
)

ensemble_jointsearch = RandomizedSearchCV(ensemble, jointsearch_param_dist)
</code></pre>
","1","Question"
"77775519","","<p>How do I get the class names of segmented instances when detecting multiple classes in YOLOv8? The detections do not have a <code>.cls</code> attribute like here <a href=""https://stackoverflow.com/questions/75277492/yolov8-get-predicted-class-name"">YOLOv8 get predicted class name</a>. Also the docs do not seem to mention anything e.g. <a href=""https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results"" rel=""nofollow noreferrer"">here</a></p>
<p>When I use the <code>show=true</code> argument in the prediction function, the classes are distinguished in the resulting image, but I cannot get them programmatically. My code that gets me all detections I wanjt but does not let me know which one is which:</p>
<pre><code>from ultralytics import YOLO
model = YOLO(&quot;path/to/best.pt&quot;)
result = model.predict(os.path.join(cut_dir, im_name), save_conf=True, show=True)
if result[0].masks is not None:
    for counter, detection in enumerate(result[0].masks.data):
         detected = np.asarray(detection.cpu())                
</code></pre>
","0","Question"
"77776124","","<p>I got this part of the code from my teammate for our project, I got this error in local I have trained dataset using XGB Classifier.</p>
<p>my code is:</p>
<pre><code>#  XGBoost Classifier Model
from xgboost import XGBClassifier

# instantiate the model
xgb = XGBClassifier()

# fit the model 
xgb.fit(X_train,y_train)
</code></pre>
<p>then I got this error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[70], line 8
      5 xgb = XGBClassifier()
      7 # fit the model 
----&gt; 8 xgb.fit(X_train,y_train)

File ~/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/core.py:730, in require_keyword_args.&lt;locals&gt;.throw_if.&lt;locals&gt;.inner_f(*args, **kwargs)
    728 for k, arg in zip(sig.parameters, args):
    729     kwargs[k] = arg
--&gt; 730 return func(**kwargs)

File ~/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/sklearn.py:1471, in XGBClassifier.fit(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)
   1466     expected_classes = self.classes_
   1467 if (
   1468     classes.shape != expected_classes.shape
   1469     or not (classes == expected_classes).all()
   1470 ):
-&gt; 1471     raise ValueError(
   1472         f&quot;Invalid classes inferred from unique values of `y`.  &quot;
   1473         f&quot;Expected: {expected_classes}, got {classes}&quot;
   1474     )
   1476 params = self.get_xgb_params()
   1478 if callable(self.objective):

ValueError: Invalid classes inferred from unique values of `y`.
</code></pre>
<p>Expected: [0 1], got [-1 1] ,, I heard the y_train must be encoded in a newer update but I'm kinda new to these things I have no clue how to do that either.</p>
","-1","Question"
"77777706","","<p><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn </code></p>
<p>i'm getting this error with the following training loop, the grads must have been set by the sequential itself, but it's saying no grads.</p>
<pre><code>&quot;&quot;&quot;Training&quot;&quot;&quot;
Epochs = 100


for epoch in range(Epochs):
    model.train()

    train_logits = model(X_train)
    train_preds_probs = torch.softmax(train_logits,dim=1).argmax(dim=1).type(torch.float32)
    loss = loss_fn(train_preds_probs,y_train)
    train_accu = accuracy(y_train,train_preds_probs)
    print(train_preds_probs)
    optimiser.zero_grad()

    loss.backward()

    optimiser.step()

    #training
    model.eval()
    with torch.inference_mode():
        test_logits = model(X_test)
        test_preds = torch.softmax(test_logits.type(torch.float32),dim=1).argmax(dim=1)
        test_loss = loss_fn(test_preds,y_train)
        test_acc = accuracy(y_test,test_preds)

    
    if epoch%10 == 0:
        print(f'Epoch:{epoch} | Train loss: {loss} |Taining acc:{train_accu} | Test Loss: {test_loss} | Test accu: {test_acc}')





</code></pre>
<p>I tried surfing the internet for this, but didn't get a solution.</p>
<p>Any help is appreciated!</p>
","0","Question"
"77781970","","<p>I am utilizing Microsoft’s lightgbm (lgbm) library. Whilst my lgbm script is VERY similar to my scripts for xgboost and random forests (which both work fine), I appear to consistently get the following error on both the Mac Book Pro and MacStudios (with M1 chips) when using lgbm:</p>
<p><code>joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.</code></p>
<p><code>The exit codes of the workers are {SIGSEGV(-11)}</code></p>
<p>Relevant Code:</p>
<pre><code>_train_x, _val_x, _train_y, _val_y = train_test_split(_train_x, _train_y, test_size = 0.2)
    
lgbm_model = LGBMClassifier(bagging_fraction = 0.75, bagging_freq = 5, random_state=42, verbose=-1, force_col_wise=True)
    
kfoldcv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)
    
lgbm_random_search = RandomizedSearchCV(estimator = lgbm_model, param_distributions = self._param_dict, n_iter = self.num_searches, cv = kfoldcv, verbose=2, random_state=42, n_jobs=-1)
    
lgbm_random_search.fit(_train_x, _train_y)

self._CrossVal_largest_accscore = lgbm_random_search.best_score_
    
lgbm_model = LGBMClassifier(n_jobs=-1, verbose=-1, force_col_wise=True, bagging_fraction = 0.75, bagging_freq = 5, **lgbm_random_search.best_params_)

lgbm_model.fit(_train_x, _train_y, callbacks=[early_stopping(50), log_evaluation(50)], eval_set=[(_val_x,_val_y)])
</code></pre>
<p>NB when I simply remove the clause njobs=-1 my program just terminates when running the line:</p>
<p><code>lgbm_random_search.fit(_train_x, _train_y)</code></p>
<p>Environment:</p>
<p>System Software Overview:</p>
<pre><code>System Version: macOS 14.0 (23A344)
Kernel Version: Darwin 23.0.0
Boot Volume: Macintosh 
HDBoot Mode: Normal
Secure Virtual Memory: Enabled
System Integrity Protection: Enabled
</code></pre>
<p>Hardware Overview:</p>
<pre><code>  Model Name: MacBook Pro
  Model Number: MK1F3B/A
  Chip: Apple M1 Pro
  Total Number of Cores: 10 (8 performance and 2 efficiency)
  Memory: 16 GB
  System Firmware Version: 10151.1.1
  OS Loader Version: 10151.1.1
  Activation Lock Status: Enabled
</code></pre>
<p>Application Software</p>
<pre><code>Visual Studio Code==1.72.2
python==3.10.121
</code></pre>
<p>Python Packages</p>
<hr />
<pre><code>anaconda-client==1.12.0
anaconda-navigator==2.4.2 
conda==23.7.2
conda-build==3.26.0
joblib==1.3.0
lightgbm==4.0.0
matplotlib==3.7.1
matplotlib-inline==0.1.6
numpy==1.23.5
pandas==2.0.3
scikit-image==0.20.0
scikit-learn==1.3.0
scipy==1.11.1
statsmodels==0.14.0
sympy==1.12
xgboost==2.0.0
</code></pre>
<p>I have reviewed and attempted some of the suggested solutions in the following websites to no avail (eg removing njobs argument; adding ‘pre_dispatch=2’ argument; reinstalling anaconda, lightgbm, joblib; using a lower number of estimators 30 to 300 etc):</p>
<ul>
<li><p><a href=""https://stackoverflow.com/questions/60729502/gridsearchcv-with-n-jobs-1-is-not-working-for-decision-tree-random-forest-class"">GridSearchCV with n_jobs=-1 is not working for Decision Tree/Random Forest classification</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/54139403/how-do-i-fix-debug-this-multi-process-terminated-worker-error-thrown-in-scikit-l"">How do I fix/debug this Multi-Process terminated worker error thrown in scikit learn</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/76929127/terminatedworkererror-in-gridsearch"">TerminatedWorkerError in GridSearch</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/62575859/a-worker-process-managed-by-the-executor-was-unexpectedly-terminated"">A worker process managed by the executor was unexpectedly terminated</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/54139403/how-do-i-fix-debug-this-multi-process-terminated-worker-error-thrown-in-scikit-l/54141605#54141605"">How do I fix/debug this Multi-Process terminated worker error thrown in scikit learn</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/58027364/terminatedworkererror-a-worker-process-managed-by-the-executor-was-unexpectedly"">TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/60782660/issues-with-multiple-jobs-when-using-randomizedsearchcv"">Issues with multiple jobs when using RandomizedSearchCV</a></p>
</li>
</ul>
","0","Question"
"77785423","","<p>I used pre-train bert model for binary classification. After training my model with my small data, I wanted to extract summary graph like this <a href=""https://i.sstatic.net/xspuL.png"" rel=""nofollow noreferrer"">the graph I want</a>. However, I want to replace these important features  with words.</p>
<p>However, I am not sure everything is okay because the shape of shap_value is only two dimensional. Actually, this is sensible. Nevertheless, I did not get the graph because I encountered two problems if I use this code:</p>
<pre><code>shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=comments_text)`
</code></pre>
<p>Problem is too unsensible: If I change <code>shap_values[:,:10]</code> with <code>shap_values</code>  or <code>shap_values[0]</code> or <code>shap_values.values</code> vb. I always come across</p>
<pre><code>516: assert len(shap_values.shape) != 1, &quot;Summary plots need a matrix of 
shap_values, not a vector.&quot; ==&gt; AssertionError: Summary plots need a matrix of 
shap_values, not a vector.
</code></pre>
<p>(fist problem)</p>
<p>By the way, my shap_value consist of 10 input(shape_value.shape). If I choose for max value a range from 1 to 147 everything fine for drawing the graph. However,in this time, the graph is not suitable: My graph consist of only blue dot(-second problem-). Like this <a href=""https://i.sstatic.net/rjAXU.png"" rel=""nofollow noreferrer"">only blue not</a>.</p>
<p>Note: <code>shap_values[:,:10]</code> if the number (10) change different number, the graph show diffent word however the total number of the graph same (max 20). Only some words order can be changing.</p>
<p>Minimal reproducible example:</p>
<pre><code>import nlp
import numpy as np
import pandas as pd
import scipy as sp
import torch
import transformers
import torch
import shap

# load a BERT sentiment analysis model
tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(
    &quot;distilbert-base-uncased&quot;
)
model = transformers.DistilBertForSequenceClassification.from_pretrained(
    &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
).cuda()


if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device(&quot;cpu&quot;)

def f(x):
    # Encode the batch of sentenc
    inputs = tokenizer.batch_encode_plus(x.tolist(), max_length=450,add_special_tokens=True, return_attention_mask=True,padding='max_length',truncation=True,return_tensors='pt')

    # Send the tensors to the same device as the model
    input_ids = inputs['input_ids'].to(device)
    attention_masks = inputs['attention_mask'].to(device)
    # Predict
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units
    return val
# Build an explainer using a token masker
explainer = shap.Explainer(f, tokenizer )

imdb_train = nlp.load_dataset(&quot;imdb&quot;)[&quot;train&quot;]
shap_values = explainer(imdb_train[:10], fixed_context=1, batch_size=16)
cohorts = {&quot;&quot;: shap_values}
cohort_labels = list(cohorts.keys())
cohort_exps = list(cohorts.values())
for i in range(len(cohort_exps)):
    if len(cohort_exps[i].shape) == 2:
        cohort_exps[i] = cohort_exps[i].abs.mean(0)
features = cohort_exps[0].data
feature_names = cohort_exps[0].feature_names
#values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))], dtype=object)
values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])
feature_importance = pd.DataFrame(list(zip(feature_names, sum(values))), columns=['features', 'importance'])
feature_importance.sort_values(by=['importance'], ascending=False, inplace=True)
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=imdb_train['text'][10:20],show=False)

</code></pre>
<p>The above code produce the same result. I spent approximately 200 computer units and I did not succeed it :(. How can I do?</p>
","0","Question"
"77785503","","<p>I have trained a simple neural network model to make a binary classification and be able o separate real from fake news</p>
<pre><code>#Create the class of the model
class FakeNewsDetectionModelV0(nn.Module):
     def __init__(self, input_size):
        super().__init__()
        
        self.layer_1=nn.Linear(in_features=input_size, out_features=8)
        self.layer_2=nn.Linear(in_features=8, out_features=1) #takes the 5 features from the previous layer and outputs a single feature

     #define a forward() for the forward pass
     def forward(self, x, mask):
        
        # Apply the mask to ignore certain values
        if mask is not None:
            x = x * mask

        x = self.layer_1(x)
        x = self.layer_2(x)
        return x



</code></pre>
<p>I use CountVectorizer to turn text to list and subsequrently to tensor</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(min_df=0, lowercase=False)
vectorizer.fit(df['text'])

X=vectorizer.fit_transform(df['text']).toarray()
</code></pre>
<p>The problem is that because the dataset has more than 9000 entries the input size the model is trained on is really large (around 120000). So when i try to make predictions on single sentences, because the size is significally smaller i need to excessively pad the sentence to make it fit the model's input which greatly affect my model's accuracy.</p>
<pre><code>from io import StringIO
from torch.nn.functional import pad
import string
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences


try:
    #nltk.download('stopwords')
    nltk.download('punkt')
except:
    print(&quot;error in downloading stopwords&quot;)

def normalise_text (text):

  text = text.lower() # lowercase
  text = text.replace(r&quot;\#&quot;,&quot;&quot;) # replaces hashtags
  text = text.replace(r&quot;http\S+&quot;,&quot;URL&quot;)  # remove URL addresses
  text = text.replace(r&quot;@&quot;,&quot;&quot;)
  text = text.replace(r&quot;[^A-Za-z0-9()!?\'\`\&quot;]&quot;, &quot; &quot;)
  text = text.replace(&quot;\s{2,}&quot;, &quot; &quot;)
  text = re.sub(r'[^\w\s]', '', text)
  return text

def fake_news_detection(df, model, model_input_size):
    predictions = []
    max_words = 10000
    max_length = model_input_size

    model.eval()

    for prediction_data in df['text'][:4000]:
        prediction_data=normalise_text(prediction_data)

        #print([prediction_data])



        # Use CountVectorizer to transform text data to array
        vectorizer = CountVectorizer(min_df=0, lowercase=False)
        prediction_data_array = vectorizer.fit_transform([prediction_data]).toarray()

        #tokenizer = Tokenizer(num_words=max_words)
        #tokenizer.fit_on_texts([prediction_data])
        #sequences = tokenizer.texts_to_sequences([prediction_data])


        #prediction_data_array = pad_sequences(sequences, maxlen=max_length,value=-1.0)

        #print(prediction_data_array.shape)

        # Check the shape of the transformed data
        current_input_size = prediction_data_array.shape[1]


        prediction_data_tensor = torch.tensor(prediction_data_array, dtype=torch.float32)


        # If the shape doesn't match, resize it
        if current_input_size != model_input_size:

            print(current_input_size)
            padding = model_input_size - current_input_size
            prediction_data_tensor = pad(prediction_data_tensor, (0, padding), 'constant', value = 0)
            mask_tensor = torch.ones_like(prediction_data_tensor)
            mask_tensor[:, -padding:] = 0  # Set values in the padded region to 0
            #print(torch.unique(mask_tensor, return_counts=True))

            # Apply the mask to ignore certain values
            #prediction_data_tensor = prediction_data_tensor * mask_tensor



        # Assuming your model takes input_data as input
        with torch.inference_mode():
            prediction = torch.round(torch.sigmoid(model(prediction_data_tensor, mask_tensor))).squeeze()

        predictions.append(round(prediction.item()))

    print(f&quot;our data tensor shape is  {prediction_data_tensor.shape}&quot;)

    predictions_tensor = torch.FloatTensor(predictions)

    return predictions_tensor
</code></pre>
<p>Does anyone know any workaround that allows me to fit the data to my model withou dropping its accuracy score ?</p>
<p>Tried : padding the vectors when making predictions on data that is of small size</p>
<p>Expected : Accurate predictions similar to the results i got in training/evaluation process</p>
<p>Got : Inaccurate predictions of really low accuracy (around 43%)</p>
","0","Question"
"77788410","","<p><strong>My understanding of Recursive Feature Elimination Cross Validation:</strong> (<code>sklearn.feature_selection.RFECV</code>) You provide an algorithm which is trained on the entire dataset and creates a feature importance ranking using attributes <code>coef_</code> or <code>feature_importances_</code>. Now with all features included, this algorithm is evaluated by cross validation. Then the feature ranked at the bottom is removed and the model is retrained on the dataset and creates a new ranking, once again assessed by cross validation. This continues until all but one feature remain (or as specified by <code>min_features_to_select</code>), and the final number of features chosen depends on what yielded the highest CV score. (<a href=""https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"" rel=""nofollow noreferrer"">Source</a>)</p>
<p><strong>Question:</strong> The CV score for each number of features is stored in <code>rfecv.cv_results_[&quot;mean_test_score&quot;]</code>, and I've been facing trouble trying to replicate these scores without using scikit's built in method.</p>
<p>This is what I have tried to obtain the score for n-1 features, where n is the total number of features.</p>
<pre><code>from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate
from sklearn.feature_selection import RFECV

alg = DecisionTreeClassifier(random_state = 0)
cv_split = StratifiedKFold(5)
# train is a pandas dataframe, x_var and y_var are both lists containing variable strings
X = train[x_var]
y = np.ravel(train[y_var])

alg.fit(X, y)
lowest_ranked_feature = np.argmin(alg.feature_importances_)
x_var.pop(lowest_ranked_feature)

one_removed_feature = train[x_var]
alg.fit(one_removed_feature, y)
cv_score = cross_validate(alg, one_removed_feature, y, cv=cv_split, scoring=&quot;accuracy&quot;)
np.mean(cv_score[&quot;test_score&quot;])
</code></pre>
<p>And this is the inbuilt method that provides a different score:</p>
<pre><code>rfecv = RFECV(
    estimator=alg,
    step=1,
    cv=cv_split,
    scoring=&quot;accuracy&quot;,
)

rfecv.fit(X, y)
rfecv.cv_results_[&quot;mean_test_score&quot;][-2]
</code></pre>
<p><strong>How do I get the exact scores as calculated in the inbuilt method?</strong></p>
<p>I would also like to mention that I did try this first with all n features, and my method matched with
<code>rfecv.cv_results_[&quot;mean_test_score&quot;][-1]</code>.</p>
","1","Question"
"77790906","","<p>I am writing a handwritten digit recognition neural network algorithm in python, without using prewritten ML libraries. I am currently attempting to implement a DenseLayer class and within it a forward propogation function. My current function can be seen below.</p>
<pre><code>class DenseLayer:
  ...
  
  ...
  def for_prop(self, input_data):
    self.input = input_data

    transpose_weights = self.weights.T
    # matMulComponent = np.matmul(input_data, transpose_weights)
    print(f&quot;transpose shape: {transpose_weights.shape} and input shape {input_data.shape}&quot;)
    matMulComponent = input_data.T @ transpose_weights
    print(len(matMulComponent))

    z = matMulComponent + self.biases.T
    f_wb = self.act_fun(z)
    
    

    self.output = f_wb.reshape(-1, 1)
    print(f&quot;result of shape: {self.output.shape}&quot;)
    return self.output
</code></pre>
<p>The issue is I am doing a lot of reshaping and transposing to get my result. This doesnt seem efficient.</p>
<p>So my question is:</p>
<ul>
<li>is this fine to implement (ergo will it cause innefficiencies)</li>
<li>Is there a better way to do this forward propogation function</li>
</ul>
<p>This is what my input data array looks like (i just printed it and took a ss). My input data for reference is a flattened 28*28 array with each cell representing a colour. I first normalised the data (z-score noramlisation)</p>
<p><a href=""https://i.sstatic.net/tUZNs.png"" rel=""nofollow noreferrer"">Input data image</a></p>
<p>I also have taken a screenshot of my weight format for the first layer if that helps. (keep in mind it is transposed before use in the for_prop function).</p>
<p><a href=""https://i.sstatic.net/02WBS.png"" rel=""nofollow noreferrer"">picture of weights matrix for first hidden layer</a></p>
<p>Forward propogation does seem to be working though which is good:<a href=""https://i.sstatic.net/OO1p4.png"" rel=""nofollow noreferrer"">Forward prop progress</a></p>
","0","Question"
"77791682","","<p>I tried setting up multiple sub-processes, and using <code>PyTorch</code> to train a <em>separate model</em> on a <em>separate dataset</em> <strong>within each sub-process</strong>. Here is my code: (no <code>cuda</code>/GPU involved yet)</p>
<pre><code>##################################################################################
# this part of code has nothing to do with the error, we include it for the completeness
import torch
from torch import nn
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_california_housing

class CADataset(torch.utils.data.Dataset):
  '''
  Prepare the Boston dataset for regression
  '''
  def __init__(self, X, y, scale_data=True):
    if not torch.is_tensor(X) and not torch.is_tensor(y):
      # Apply scaling if necessary
      if scale_data:
          X = StandardScaler().fit_transform(X)
      self.X = torch.from_numpy(X)
      self.y = torch.from_numpy(y)

  def __len__(self):
      return len(self.X)

  def __getitem__(self, i):
      return self.X[i], self.y[i]

class MLP(nn.Module):
  '''
    Multilayer Perceptron for regression.
  '''
  def __init__(self):
    super().__init__()
    self.layers = nn.Sequential(
      nn.Linear(8, 32),
      nn.ReLU(),
      nn.Linear(32, 16),
      nn.ReLU(),
      nn.Linear(16, 1)
    )

  def forward(self, x):
    '''
      Forward pass
    '''
    return self.layers(x)

def mlp_demo(branchID: int):
  housing = fetch_california_housing() # in this toy example data from all branches are the same, while in my real application they are not.
  print('in branch {}'.format(branchID))
  print(housing.data.shape)
  print(housing.target.shape)

  # Prepare CA dataset
  dataset = CADataset(housing.data, housing.target)
  trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)

  # Initialize the MLP
  mlp = MLP()

  # Define the loss function and optimizer
  loss_function = nn.L1Loss()
  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)

  # Run the training loop
  for epoch in range(0, 5): # 5 epochs at maximum

    # Print epoch
    print(f'Starting epoch {epoch+1}')

    # Set current loss value
    current_loss = 0.0

    # Iterate over the DataLoader for training data
    for i, data in enumerate(trainloader, 0):
      # Get and prepare inputs
      inputs, targets = data
      inputs, targets = inputs.float(), targets.float()
      targets = targets.reshape((targets.shape[0], 1))
      # Zero the gradients
      optimizer.zero_grad()
      # Perform forward pass
      outputs = mlp(inputs)
      # Compute loss
      loss = loss_function(outputs, targets)
      # Perform backward pass
      loss.backward()
      # Perform optimization
      optimizer.step()
      # Print statistics
      current_loss += loss.item()
      if i % 20 == 0:
          print('Loss after mini-batch %5d: %.3f' %
                (i + 1, current_loss / 500))
          current_loss = 0.0
  # Process is complete.
  print('Training process has finished.')

##################################################################################
# above code has nothing to do with the error, we include it for the completeness

from torch.multiprocessing import Pool, set_start_method

if __name__ == '__main__':
  # Set fixed random number seed
  torch.manual_seed(42)
  try:
    set_start_method('spawn')
  except RuntimeError:
    pass

  with Pool() as pool:
      pool.map(mlp_demo, range(3))
</code></pre>
<p>I learnt to import the <code>set_start_method</code> function from <a href=""https://stackoverflow.com/questions/48822463/how-to-use-pytorch-multiprocessing"">here</a>, but I still got the following error:</p>
<pre><code>multiprocessing.pool.RemoteTraceback:
&quot;&quot;&quot;
Traceback (most recent call last):
  File &quot;/usr/lib64/python3.9/multiprocessing/pool.py&quot;, line 125, in worker
    result = (True, func(*args, **kwds))
  File &quot;/usr/lib64/python3.9/multiprocessing/pool.py&quot;, line 48, in mapstar
    return list(map(*args))
  File &quot;/home/wangyu/code/test_cuda/demo.py&quot;, line 72, in mlp_demo
    for i, data in enumerate(trainloader, 0):
  File &quot;/usr/local/lib64/python3.9/site-packages/torch/utils/data/dataloader.py&quot;, line 441, in __iter__
    return self._get_iterator()
  File &quot;/usr/local/lib64/python3.9/site-packages/torch/utils/data/dataloader.py&quot;, line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File &quot;/usr/local/lib64/python3.9/site-packages/torch/utils/data/dataloader.py&quot;, line 1042, in __init__
    w.start()
  File &quot;/usr/lib64/python3.9/multiprocessing/process.py&quot;, line 118, in start
    assert not _current_process._config.get('daemon'), \
AssertionError: daemonic processes are not allowed to have children
&quot;&quot;&quot;

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/wangyu/code/test_cuda/demo.py&quot;, line 110, in &lt;module&gt;
    pool.map(mlp_demo, range(3))
  File &quot;/usr/lib64/python3.9/multiprocessing/pool.py&quot;, line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File &quot;/usr/lib64/python3.9/multiprocessing/pool.py&quot;, line 771, in get
    raise self._value
AssertionError: daemonic processes are not allowed to have children
</code></pre>
<p>In my real application, I have several datasets, and their trainings are independent. I know I could run multiple instances of <code>python3 my_script.py --dataset=&lt;my_ds&gt;</code>, but since their pre-processings are correlated and the results of the trainings are to be aggregated, I really want it could be done within one python script (and one python instance).</p>
<p>Is there any way taht I can fix the <code>demon</code> error?</p>
","1","Question"
"77792137","","<p>I'm training model with the following parameters:</p>
<pre><code>Seq2SeqTrainingArguments(
    output_dir                   = &quot;./out&quot;, 
    overwrite_output_dir         = True,
    do_train                     = True,
    do_eval                      = True,
    
    per_device_train_batch_size  = 2, 
    gradient_accumulation_steps  = 4,
    per_device_eval_batch_size   = 8, 
    
    learning_rate                = 1.25e-5,
    warmup_steps                 = 1,
    
    save_total_limit             = 1,
       
    evaluation_strategy          = &quot;epoch&quot;,
    save_strategy                = &quot;epoch&quot;,
    logging_strategy             = &quot;epoch&quot;,  
    num_train_epochs             = 5,   
    
    gradient_checkpointing       = True,
    fp16                         = True,    
        
    predict_with_generate        = True,
    generation_max_length        = 225,
          
    report_to                    = [&quot;tensorboard&quot;],
    load_best_model_at_end       = True,
    metric_for_best_model        = &quot;wer&quot;,
    greater_is_better            = False,
    push_to_hub                  = False,
)
</code></pre>
<p>I assume that <code>warmup_steps=1</code> fixes the learning rate.
However, after finished training I'm looking on the file <code>trainer_state.json</code>, and it seems that the learning rate is not fixed.</p>
<p>Here are the values of <strong>learning_rate</strong> and <strong>step</strong>:</p>
<p>learning_rate,     steps</p>
<pre><code>1.0006 e-05       1033
7.5062 e-06       2066
5.0058 e-06       3099
2.5053 e-06       4132
7.2618 e-09       5165
</code></pre>
<p>It seems that the learning rate is not fixed on <strong>1.25e-5</strong> (after step 1). What am I missing? How to I fix the learning rate.</p>
","1","Question"
"77792551","","<p>I am installing the <a href=""https://github.com/unit8co/darts/blob/master/INSTALL.md#enabling-optional-dependencies"" rel=""nofollow noreferrer"">DARTS TimeSeries library</a> but I run into an issue of dependencies installation. In the DARTS installation guide it said if we run into that issue we have to refer to the official installation guide for PyTorch, then try installing Darts again. Then, when I tried to install torch on python 3.12.1 I run into this error :</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)</p>
<p>ERROR: No matching distribution found for torch.</p>
</blockquote>
<p>How to solve that?</p>
<p>I am using PyCharm as Python code editor.</p>
<p>I tried a <code>pip install darts</code> and it did not install all the packages and encounter this error  error: subprocess-exited-with-error</p>
<pre><code>  pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─&gt; [136 lines of output]
      Collecting setuptools&gt;=64.0
        Obtaining dependency information for setuptools&gt;=64.0 from https://files.pythonhosted.org/packages
</code></pre>
<p>Then, I tried to install torch with <code>pip install torch</code> and encountered this error:</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)</p>
<p>ERROR: No matching distribution found for torch</p>
</blockquote>
","1","Question"
"77794688","","<p>I am trying to make plots of a SHAP analysis of a XGBoost model I trained. Something similar to <a href=""https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20XGBoost.html"" rel=""nofollow noreferrer"">this</a>.</p>
<p>However, I used Dart booster, so <code>shap.TreeExplainer</code> does not work. Then, I am trying to use the <code>shap.KernelExplainer</code> which should work for me. However, it is not accepting any common type of input.</p>
<p>My code is like this:</p>
<p><strong>First Attempt</strong></p>
<pre><code># Data to predict
full_data = xgb.DMatrix(full_X, label=full_y, feature_names=feature_names)

# Pre-trained XGB model using DART booster
loaded_model.set_param({&quot;device&quot;: &quot;cuda&quot;})


xgb_predict = lambda x: loaded_model.predict(x)
explainer = shap.KernelExplainer(xgb_predict, full_data)
</code></pre>
<p>And I get :</p>
<pre><code>TypeError: Unknown type passed as data object: &lt;class 'xgboost.core.DMatrix'&gt;
</code></pre>
<p><strong>Second Attempt</strong></p>
<p>I have also tried to provide a numpy array:</p>
<pre><code>X_np = np.array(full_X)

explainer = shap.KernelExplainer(xgb_predict, X_np)
</code></pre>
<p>But it also returns an error:</p>
<pre><code>TypeError: ('Expecting data to be a DMatrix object, got: ', &lt;class 'numpy.ndarray'&gt;)
</code></pre>
<p>I am using shap 0.44.0 and xgboost 2.0.2</p>
<p>How can I resolve the problem?</p>
","-1","Question"
"77797473","","<p>I am currently working on a house price prediction task where I have logarithmically transformed the target variable (price) due to its non-normal distribution. I am using metrics such as RMSE, MAE, and MAPE, and for model training, I utilized cross_val_score.</p>
<p>After obtaining predictions, I took the exponential of MAE and MAPE metrics to revert them to the original scale. However, I encountered unexpectedly small values; both metrics were equal to 1. I suspect that these values are incorrect.</p>
<pre><code>kf = KFold(n_splits=5, random_state=42, shuffle=True)

def rmse_cv(model):
    mse_scorer = make_scorer(mean_squared_error)
    rmse = np.sqrt(cross_val_score(model, train, y_train, scoring=mse_scorer, cv=kf))
    return rmse

def mae_cv(model):
    mae_scorer = make_scorer(mean_absolute_error)
    mae = cross_val_score(model, train, y_train, scoring=mae_scorer, cv=kf)
    return mae

def mape_cv(model):
    mape_scorer = make_scorer(mean_absolute_percentage_error)
    mape = cross_val_score(model, train, y_train, scoring=mape_scorer, cv=kf)
    return mape

lightgbm = LGBMRegressor(num_leaves=6, max_depth=7, random_state=42, n_estimators=500, objective='regression')

rmse = rmse_cv(lightgbm)
mae = mae_cv(lightgbm)
mape = mape_cv(lightgbm)
print('Lightgbm rmse %.4f' % (rmse.mean()))
print('Lightgbm mae %.4f' % (mae.mean()))
print('Lightgbm mape %.4f' % (mape.mean()))

Lightgbm rmse 0.1331
Lightgbm mae 0.0874
Lightgbm mape 0.0073
</code></pre>
<p>I expected to obtain reasonable and interpretable values that reflect the model's performance on the original scale. However, both metrics yielded unexpectedly small values of 1, which seems inaccurate. I anticipated a more meaningful representation of model error on the original price scale.</p>
","0","Question"
"77797592","","<pre><code>dataAgeNull = data[data[&quot;Age&quot;].isnull()]
dataAgeNotNull = data[data[&quot;Age&quot;].notnull()]
remove_outlier = dataAgeNotNull[(np.abs(dataAgeNotNull[&quot;Fare&quot;]-dataAgeNotNull[&quot;Fare&quot;].mean())&gt;(4*dataAgeNotNull[&quot;Fare&quot;].std()))|
                      (np.abs(dataAgeNotNull[&quot;Family_Size&quot;]-dataAgeNotNull[&quot;Family_Size&quot;].mean())&gt;(4*dataAgeNotNull[&quot;Family_Size&quot;].std()))                     
                     ]
rfModel_age = RandomForestRegressor(n_estimators=2000,random_state=42)
ageColumns = ['Embarked', 'Fare', 'Pclass', 'Sex', 'Family_Size', 'Title1', 'Title2','Cabin','Ticket_info']
rfModel_age.fit(remove_outlier[ageColumns], remove_outlier[&quot;Age&quot;])

ageNullValues = rfModel_age.predict(X= dataAgeNull[ageColumns])
dataAgeNull.loc[:,&quot;Age&quot;] = ageNullValues
data = dataAgeNull.append(dataAgeNotNull)
data.reset_index(inplace=True, drop=True)
</code></pre>
<p>Why do we use outlier data of &quot;Fare&quot; and &quot;family_size&quot; to train RandomForestRegressor to fill missing data of &quot;Age&quot;?</p>
<p>I tried to understand this code but still cannot figure it out
4</p>
","-1","Question"
"77800583","","<p>I am trying to convert XGBoost shapely values into an SHAP explainer object. Using the example [here][1] with the built in SHAP library takes days to run (even on a subsampled dataset) while the XGBoost library takes a few minutes. However. I would like to output a beeswarm graph that's similar to what's displayed in the example [here][2].</p>
<p>My thought was that I could use the XGBoost library to recover the shapely values and then plot them using the SHAP library, but the beeswarm plot requires an explainer object. How can I convert my XGBoost booster object into an explainer object?</p>
<p>Here's what I tried:</p>
<pre><code>import shap
booster = model.get_booster()
d_test = xgboost.DMatrix(X_test[0:100], y_test[0:100])
shap_values = booster.predict(d_test, pred_contribs=True)
shap.plots.beeswarm(shap_values)
</code></pre>
<p>Which returns:</p>
<pre><code>TypeError: The beeswarm plot requires an `Explanation` object as the `shap_values` argument.
</code></pre>
<p>To clarify, I would like to create the explainer object out of values generated by the xgboost built-in library, if possible. Avoiding the shap.explainer or shap.TreeExplainer function calls is a priority because they take much much longer (days) to return rather than minutes.
[1]: <a href=""https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Python%20Version%20of%20Tree%20SHAP.html"" rel=""nofollow noreferrer"">https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Python%20Version%20of%20Tree%20SHAP.html</a>
[2]: <a href=""https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html#A-simple-beeswarm-summary-plot"" rel=""nofollow noreferrer"">https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html#A-simple-beeswarm-summary-plot</a></p>
","2","Question"
"77804296","","<p>I have an example table that I would like to conduct KKNN to classify on. The variable, <code>V4</code> is the response and I want the classifier to see if a new data point will classify as <code>0</code> or <code>1</code> (the actual data has 12 columns and the 12th column is the response but I will simplify the example nonetheless</p>
<pre><code>library(kknn)

data &lt;- data.frame(
  V1=c(1.2, 2.5, 3.1, 4.8, 5.2), 
  V2=c(0.7, 1.8, 2.3, 3.9, 4.1), 
  V3=c(2.3, 3.7, 1.8, 4.2, 5.5), 
  V4= c(0, 1, 0, 1, 0)
)
</code></pre>
<p>Now, I want to build a  <code>kknn</code>classification via LOOCV using a <code>for</code> loop. Lets assume <code>kknn=3</code></p>
<pre><code>for (i in 1:nrow(data)) {
  train_data &lt;- data[-i, 1:3]
  train_data_response &lt;- data.frame(data[-i, 4])
  colnames(train_data_response) &lt;- &quot;Response&quot;
  test_set &lt;- data[i, 3]
  model &lt;- kknn(formula=train_data_response ~ ., data.frame(train_data), 
                data.frame(test_set), k=3, scale=TRUE) 
}
</code></pre>
<p>Now I get this error that says:</p>
<pre><code>Error in model.frame.default(formula, data = train) : 
  invalid type (list) for variable 'train_data_response'
</code></pre>
<p>Is there any way on how I can solve this error? I thought <code>kknn</code> accepts matrix or dataframes. My training and testing data are indeed dataframes so what gives?</p>
<p>Also, am I doing the LOOCV correctly?</p>
","1","Question"
"77804804","","<pre><code> ID Ever_Married    Graduated   Gender  Profession  Spending_Score  Segmentation    Family_Size Age Work_Experience
0   462809  0   0   1   5   2   3   3   4   1
1   462643  1   1   0   2   0   0   2   18  15
2   466315  1   1   0   2   2   1   0   44  1
3   461735  1   1   1   7   1   1   1   44  0
4   462669  1   1   0   3   1   0   5   20  15
... ... ... ... ... ... ... ... ... ... ...
8063    464018  0   0   1   9   2   3   6   4   0
8064    464685  0   0   1   4   2   3   3   15  3
8065    465406  0   1   0   5   2   3   0   14  1
8066    467299  0   1   0   5   2   1   3   8   1
8067    461879  1   1   1   4   0   1   2   17  0
8068 rows × 10 columns
</code></pre>
<pre><code>data1=data.drop([&quot;ID&quot;,&quot;Segmentation&quot;],axis=1)

from sklearn.model_selection import train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 from sklearn.neighbors import KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])
</code></pre>
<pre><code> UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  #warnings.warn(
array([1])
</code></pre>
<p>When I make a prediction, I was not expecting this warning.</p>
","-1","Question"
"77805776","","<p>I'm trying to calculate word and sentence embeddings using Roberta, for word embeddings, I extract the last hidden state <code>outputs[0]</code> from the <code>RobertaModel</code> class, but I'm not sure if this is the correct way to calculate.</p>
<p>As for sentence embeddings, I don't know how to calculate them, this is the code I have tried:</p>
<pre><code>from transformers import RobertaModel, RobertaTokenizer
import torch

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
captions = [&quot;example caption&quot;, &quot;lorem ipsum&quot;, &quot;this bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;example&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = ?????
</code></pre>
<p>How to calculate word and sentence embeddings using Roberta?</p>
","1","Question"
"77812921","","<p>In the decoder part of <a href=""https://arxiv.org/abs/1505.04597"" rel=""nofollow noreferrer"">UNet</a> architecture, Upsampling layer is often followed by a Conv2d.
Here is an example:</p>
<pre><code>class UpConv(nn.Module):
    def __init__(self, in_chans, out_chans):
        super().__init__()
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2, mode = &quot;bilinear&quot;, align_corners=True),
            nn.Conv2d(in_chans, out_chans, kernel_size = 1),
        )
        self.conv = DoubleConv(out_chans*2, out_chans)
...
</code></pre>
<p>Could someone please explain why we need the Conv2d?</p>
<p>I read that Conv2d layer allows to learn spatial hierarchies in upsampled feature map. But I don't understand why it is relevant as 2D convolution has been applied during encoding, meaning that spatial information has been learnt already.</p>
<p>My guess is that Conv2d is used to adjust the number of output channels. Is this right?</p>
","0","Question"
"77813309","","<p>I have a data set with 90 variables and 200000 obs. It is unbalanced as it has only 4% cases where target variable is 1, in all other cases it is 0.</p>
<p>I split it to 2 sets: fitting(185000) and holdout sample &quot;df_holdout&quot; (15000 obs.)
So, I decided to take from the fitting sample for model fitting all cases where target variable = 1 and the same amount of cases where target variable = 0. (in total the set &quot;df&quot; included 25000 obs.)</p>
<p>Variables have names var_01, var_02, var_03, ... var_90 , where var_90 was renamed into &quot;target&quot;.</p>
<p>I have a stack of workflows.</p>
<p>This is the code that I use for model fitting:</p>
<pre><code>rf_tune    &lt;- parsnip::rand_forest(mode=&quot;classification&quot;,
                                                   mtry = tune(), 
                                                   trees = 1000,
                                                   min_n = tune()) %&gt;%
                                                   set_engine(&quot;ranger&quot;,
                                                              importance = &quot;impurity&quot;)
svm_tune              &lt;-  parsnip::svm_poly(mode = &quot;classification&quot;,
                                                   engine = &quot;kernlab&quot;,
                                                   cost = tune(),
                                                   degree = tune(),
                                                   scale_factor = tune(),
                                                   margin = tune())

  

  # Create data split object
  df_split &lt;- initial_split(df, prop = 0.75,
                            strata = target)
  
  # Create the training data
  df_train &lt;- df_split %&gt;% 
    training()
 
  df_test &lt;- df_split %&gt;% 
    testing()
   
  # create a recipe
  df_recipe &lt;- recipe(target ~., data = df_train) %&gt;% 
    step_zv(all_predictors()) %&gt;%
    step_normalize(all_numeric()) %&gt;% 
    step_corr(threshold = 0.7) %&gt;% 
    step_dummy(all_nominal_predictors(), -all_outcomes())
  
  df_recipe %&gt;% 
    prep(df_train) %&gt;% 
    bake(df_train)

all_models_set &lt;- 
    workflow_set(preproc = list(df_recipe = df_recipe),
                 models =  list(rf_tune,
                                svm_tune),
                 cross = TRUE)

set.seed(123)

  cv &lt;-  vfold_cv(df_training, v=5, repeats=1, strata=target) 
  
  df_metr &lt;- metric_set(accuracy, roc_auc,sens,spec)
  
  
  all_models &lt;-
    all_models_set %&gt;%
    workflow_map(&quot;tune_grid&quot;,
                 resamples = cv,
                 grid = 10,
                 control =  control_resamples( save_pred = T, save_workflow = T, verbose = T), 
                 metrics = df_metr
    )
  
 
 # Get the workflow ID for the top model from our workflow set
  best_workflow &lt;-
    rank_results(all_models, rank_metric = &quot;roc_auc&quot;, select_best = TRUE) %&gt;% 
    filter(.metric==&quot;roc_auc&quot; &amp; rank==1)
  
  
  final_model &lt;-
    extract_workflow_set_result(all_models, pull(best_tuned_workflow, wflow_id)) %&gt;% 
    select_best(metric = &quot;roc_auc&quot;) 
  
  
  # Fit final model on Train and predict on Test set
  final_model_pred &lt;- 
    extract_workflow(all_models, pull(best_tuned_workflow, wflow_id)) %&gt;% # extract the workflow
    finalize_workflow(final_model) %&gt;% 
    last_fit(df_split) # fit the model on Train and score on Test
  
  # final workflow extraction
  wf_final_model &lt;- extract_workflow(final_model_pred)
</code></pre>
<p>After I created a model and trained the workflow (wf_final_model), I saved it and wanted to use for prediction on a holdout sample. However, when I tried to do it I got an error message:</p>
<pre><code>predict(wf_final_model, df_holdout)

Error: Missing data in columns: var_02_X4, var_02_X7, var_02_X9, var_02_X10, var_02_X11, var_02_X12, var_02_X13, var_02_X15, var_02_X17, var_02_X18, var_02_X20, var_02_X21, var_02_X22, var_02_X23, var_02_X24, var_02_X25, var_02_X26, var_02_X27, var_02_X28, var_02_X29, var_02_X30, var_02_X31, var_02_X33, var_02_X34, var_30_X2, var_30_X3, var_30_X6, var_30_X7, var_30_X9, var_30_X11, var_30_X13, var_30_X14, var_30_X15, var_30_X16, var_30_X17, var_30_X18, var_30_X19, var_30_X20, var_30_X22, var_30_X23, var_30_X24, var_30_X25, var_30_X26, var_30_X27, var_30_X33, var_30_X43, var_30_X46, var_30_X48, var_30_X49, var_30_X51, var_30_X56, var_30_X57, var_30_X60, var_36_X14, var_36_X18, var_36_X21, var_36_X24, var_36_X28, var_36_X29, var_36_X32, var_36_X44, var_36_X57, var_36_X61, var_36_X63, var_36_X85, var_36_X125, var_36_X130, var_36_X136, var_36_X144, var_36_X147, var_36_X148, var_36_X166, var_36_X169, var_36_X171, var_89_X3, var_89_X4, var_89_X5, var_89_X6, var_89_X7, var_89_X8, var_89_X9, va
In addition: Warning messages:
1: Novel levels found in column 'var_02': '2', '5'. The levels have been removed, and values have been coerced to 'NA'. 
2: Novel levels found in column 'var_30': '39', '41', '42', '47', '54'. The levels have been removed, and values have been coerced to 'NA'. 
3: Novel levels found in column 'var_36': '118'. The levels have been removed, and values have been coerced to 'NA'. 
4: Novel levels found in column 'var_89': '2'. The levels have been removed, and values have been coerced to 'NA'. 
5: There are new levels in a factor: NA 
6: There are new levels in a factor: NA 
7: There are new levels in a factor: NA 
8: There are new levels in a factor: NA 
</code></pre>
<p>I don't have any variables with such names neither in training set, nor in test or holdout set.
As I understand, such variables depict interactions, but I am not sure how to handle it.
Can you help me please to fix the error in order to get the predictions?</p>
","0","Question"
"77820555","","<p>I'm experiencing an issue with SHAP's partial dependence plot when using a train-test split for a linear regression model in Python. When I calculate SHAP values and plot the partial dependence for the first observation in my test set, the alignment of the data point and the baseline seems off.</p>
<p>Here's a simplified version of my code:</p>
<pre><code>import shap
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.pyplot as plt
import requests

def load_data() -&gt; pd.DataFrame:
    &quot;&quot;&quot;
    Loads and returns the dataset from the given URL as a Pandas DataFrame.

    Returns:
        pd.DataFrame: The loaded dataset.
    &quot;&quot;&quot;
    url = &quot;https://archive.ics.uci.edu/static/public/165/concrete+compressive+strength.zip&quot;

    r = requests.get(url)

    if r.ok:
        with zipfile.ZipFile(BytesIO(r.content)) as thezip:
            with thezip.open(&quot;Concrete_Data.xls&quot;) as thefile:
                return pd.read_excel(thefile, header=0)
    else:
        raise Exception(&quot;Something went wrong.&quot;)

df = load_data()

df = df.rename(
    columns={
        'Cement (component 1)(kg in a m^3 mixture)':'cement',
        'Blast Furnace Slag (component 2)(kg in a m^3 mixture)':'blast',
        'Fly Ash (component 3)(kg in a m^3 mixture)':'ash',
        'Water  (component 4)(kg in a m^3 mixture)':'water',
        'Superplasticizer (component 5)(kg in a m^3 mixture)':'superplasticizer',
        'Coarse Aggregate  (component 6)(kg in a m^3 mixture)':'coarse',
        'Fine Aggregate (component 7)(kg in a m^3 mixture)':'fine',
        'Age (day)':'age',
        'Concrete compressive strength(MPa, megapascals) ': 'strength'
    }
)
df = df.drop_duplicates()
X = df.drop(['strength'], axis=1) 
y = df['strength']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Initialize SHAP explainer and calculate values for the test set
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_test)

# Plot partial dependence for the first test observation
idx = 0
shap.partial_dependence_plot(
    &quot;cement&quot;, model.predict, X_test,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)

# Save the plot
plt.tight_layout()
plt.savefig('shap_dependence_plot.png', dpi=300)
</code></pre>
<p>However, when I generate the plot, the data point (black dot) does not align with the expected value line (blue line) for the feature of interest. It seems to be shifted along the y-axis. Here's the output plot for reference:</p>
<p><img src=""https://i.sstatic.net/wWwXr.png"" alt=""enter image description here"" /></p>
<p>The plot seems correct when I initialize the SHAP explainer with the entire dataset X instead of just X_train:</p>
<pre><code>explainer = shap.Explainer(linreg, X)
shap_values = explainer(X_test)

idx = 0
shap.partial_dependence_plot(
    &quot;cement&quot;, model.predict, X_test,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)
</code></pre>
<p>Result:</p>
<p><a href=""https://i.sstatic.net/oTE6b.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTE6b.png"" alt=""enter image description here"" /></a></p>
<p>Can someone explain why this misalignment occurs and how to correct the partial dependence plot when using a train-test split?</p>
<p>Any insights or suggestions would be greatly appreciated!</p>
","1","Question"
"77822962","","<p>Consider the following code snippet taken from MLflow <a href=""https://mlflow.org/docs/latest/python_api/mlflow.data.html#mlflow-data"" rel=""nofollow noreferrer"">documentation page</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow.data
import pandas as pd
from mlflow.data.pandas_dataset import PandasDataset

# Construct a Pandas DataFrame using iris flower data from a web URL
dataset_source_url = &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot;
df = pd.read_csv(dataset_source_url)
# Construct an MLflow PandasDataset from the Pandas DataFrame, and specify the web URL
# as the source
dataset: PandasDataset = mlflow.data.from_pandas(df, source=dataset_source_url)

with mlflow.start_run():
    # Log the dataset to the MLflow Run. Specify the &quot;training&quot; context to indicate that the
    # dataset is used for model training
    mlflow.log_input(dataset, context=&quot;training&quot;)

# Retrieve the run, including dataset information
run = mlflow.get_run(mlflow.last_active_run().info.run_id)
dataset_info = run.inputs.dataset_inputs[0].dataset
print(f&quot;Dataset name: {dataset_info.name}&quot;)
print(f&quot;Dataset digest: {dataset_info.digest}&quot;)
print(f&quot;Dataset profile: {dataset_info.profile}&quot;)
print(f&quot;Dataset schema: {dataset_info.schema}&quot;)

# Load the dataset's source, which downloads the content from the source URL to the local
# filesystem
dataset_source = mlflow.data.get_source(dataset_info)
dataset_source.load()
</code></pre>
<p>This code is starting a new run and logging an input which is a dataset. Does this mean that in MLflow we are saving datasets as separate runs? If that's the case, how are we associating the training of a model that has its own run to a dataset? I'm confused how MLflow handles/tracks datasets! TBH, I was expecting datasets to be a different entity type (than runs) and we could link them to each run which is for a model training.</p>
","3","Question"
"77825016","","<p>I have an array of input data, these are 5 arrays of different lengths. How to compose the correct tensor and form for training?</p>
<pre><code>[
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] ],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] ],
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] ],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] ]
],
...
],
]
</code></pre>
","0","Question"
"77827691","","<p>i created a training pipeline in azure ml designer. Now, i need to deploy this model by adding components for registering and deployment. I guess i can use 'execute python script' component to do this. However i can not figure out how can I connect the 'trained best model', which is the output of the 'tune model hyperparameters' component, with 'execute python script' component. So, any idea of how to achieve this task ? I would appreciate your help.</p>
<p>Here is my pipeline :</p>
<p><a href=""https://i.sstatic.net/4uvH9.png"" rel=""nofollow noreferrer"">training pipeline</a></p>
","0","Question"
"77829488","","<pre><code>import torch
import torch.optim as optim
import torch.nn as nn

input = torch.tensor([1.,2.], requires_grad=True)
sigmoid = nn.Sigmoid()

interm = sigmoid(input)

optimizer = optim.SGD([input], lr=1, momentum=0.9)

for epoch in range(5):
    optimizer.zero_grad()
    loss = torch.linalg.vector_norm(interm - torch.tensor([2.,2.]))
    print(epoch, loss, input, interm)

    loss.backward(retain_graph=True)
    optimizer.step()
    print(interm.grad)
</code></pre>
<p>So I created this simplified example with an <strong>input</strong> going into a sigmoid as an intermediate activation function.</p>
<p>I am trying to find the <strong>input</strong> that results in <strong>interm</strong> =  [2.,2.]</p>
<p>But the gradients are not passing through. Anyone know why?</p>
","0","Question"
"77829624","","<p>I'm creating a CNN for skin lesion classification.
I recently added a weighter loss function to my model to try and improve its accuracy, but even with the new weighted losses, my model still only achieved around 65% accuracy. There aren't any errors.
How can I improve on my model?</p>
<p>(hopefully) all the relevant code is below:</p>
<p>My model code:</p>
<pre><code>#classification

def classi(input_shape):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(64, 3, padding=&quot;same&quot;)(inputs)
    x = layers.Activation(&quot;relu&quot;)(x)
    x = layers.BatchNormalization()(x)
    #classi layers
    for filters in [96, 128, 256, 320, 512]:#, 1024, 2048]: #change # of filters??
        x = layers.Conv2D(filters, 3, padding=&quot;same&quot;)(x)
        x = layers.Activation(&quot;relu&quot;)(x)
        x = layers.BatchNormalization()(x)

        x = layers.Conv2D(filters, 3, padding=&quot;same&quot;)(x)
        x = layers.Activation(&quot;relu&quot;)(x)
        x = layers.BatchNormalization()(x)

        x = layers.MaxPool2D(3, strides=2, padding=&quot;same&quot;)(x)

    #output
    x = layers.Dropout(rate=0.1)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation=&quot;relu&quot;)(x)
    #x = layers.Dense(64, activation=&quot;relu&quot;)(x)
    #x = layers.Dense(16, activation=&quot;sigmoid&quot;)(x)

    output = layers.Dense(7, activation=None)(x)

    model = k.Model(inputs=inputs, outputs=output, name=&quot;classification&quot;)
    return model

classification = classi((256,256,3))
classification.summary()

classification.save_weights(&quot;classification.h5&quot;)
</code></pre>
<p>My weighted loss function (cross entropy loss but with weights)</p>
<pre><code>#weighted binary loss
def get_weights(labels):
    cols = len(labels.columns)-2 #assumes 1 column for image ids
    pos_freqs = []
    neg_freqs = []
    pos_weights = []
    neg_weights = []
    for i in range(cols):
        pos_freqs.append(np.mean(labels[labels.columns[i+1]].tolist())) #get column values and sum
        neg_freqs.append(1-pos_freqs[i]) 
        pos_weights.append(neg_freqs[i]) 
        neg_weights.append(pos_freqs[i])
    
    return pos_weights, neg_weights


def weighted_cross_entropy_loss(y_true, y_pred):
    pos_weights, neg_weights = get_weights(pd.read_csv(cls_train_gt))
    #get frequencies to calculate weights
    loss = 0.0
    #print(k.backend.cast(-(neg_weights[0]*(1-y_true[:, 0])), 'float16'))
    for i in range(len(pos_weights)):
        loss += k.backend.mean(k.backend.cast(-(neg_weights[i]*(1-y_true[:, i])), 'float16')
                                 * k.backend.cast(k.backend.log((1-y_pred[:, i])), 'float16')
                                 + (k.backend.cast(pos_weights[i]*y_true[:, i], 'float16')  
                                    * k.backend.cast(k.backend.log((y_pred[:, i])), 'float16')))
    return loss
</code></pre>
<p>This is my code for loading my dataset:</p>
<pre><code>#For loading classification labels and images.

def load_images_and_labels(images_path, labels_path, batch_size, image_shape, verbose=False):
    ds_images = []
    ds_labels = []
    data_indexes = []
    labels = pd.read_csv(labels_path)
    images = os.listdir(images_path)
    if verbose:
        print(f&quot;loading images from {images_path} and labels from {labels_path}&quot;)
    for i in range(batch_size):
        random_index = np.random.randint(0, len(images)-2)
        if random_index &gt;= len(images):
            random_index -=1
        img = cv2.imread(os.path.join(images_path, images[random_index]))
        #print(random_index)
        #print(len(labels.columns))
        row = labels.iloc[random_index, 1:]

        if img is not None and row is not None:
            if random_index not in data_indexes:
                data_indexes.append(random_index)
                ds_images.append(np.array(cv2.resize(img, dsize=image_shape)))
                ds_labels.append(row.values)
    return np.array(ds_images).astype(np.int16), np.array(ds_labels).astype(np.int16)
</code></pre>
<p>And here is my code for training the model:</p>
<pre><code>datagen = ImageDataGenerator(rescale=1./255,
                             rotation_range=0.1,
                             horizontal_flip=True,
                             vertical_flip=True,
                             )

classification.load_weights('classification.h5') #reset weights
optimizer = tf.keras.optimizers.SGD(learning_rate=0.2)
classification.compile(optimizer=optimizer, loss=weighted_cross_entropy_loss, metrics=[&quot;binary_accuracy&quot;, 'MeanSquaredError', 'AUC']) 

callback_list = [tf.keras.callbacks.EarlyStopping(patience=1.5)] #can adjust to improve accuracy
batch_size=16
spe = 4 #steps per epoch
epochs = 80 
seed = 123

cls_val = r'validation/ISIC2018_Task3_Validation_Input/'
cls_val_gt = &quot;validation_ground_truth/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv&quot;

cls_train = r'train/ISIC2018_Task3_Training_Input/'#r&quot;classi/ISIC2018_Task3_Training_Input/ISIC2018_Task3_Training_Input/&quot; 
cls_train_gt = 'train_ground_truth/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv'#(&quot;classi/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv&quot;)

#organize_images_to_classes(class_train_gt, class_train)

class_val = r&quot;classi/ISIC2018_Task3_Validation_Input/ISIC2018_Task3_Validation_Input/&quot; 
class_val_gt = pd.read_csv(&quot;classi/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv&quot;)
organize_images_to_classes(class_val_gt, class_val)
&quot;&quot;&quot;


for i in range(epochs):
    train_ds, train_gt = load_images_and_labels(cls_train, cls_train_gt, batch_size, (256,256), True)
    val_ds, val_gt = load_images_and_labels(cls_val, cls_val_gt, batch_size, (256,256), True)
    
    #print(train_ds)
    #print(train_gt)

    print(f&quot;train_ds len: {len(train_ds)}, train labels len: {len(train_gt)}&quot;)
    cls_train_gen = datagen.flow(x=train_ds, y=train_gt, seed=seed, batch_size=batch_size)
    val_train_gen = datagen.flow(x=val_ds, y=val_gt, seed=seed, batch_size=batch_size)

    history = classification.fit(x=cls_train_gen.x, y=cls_train_gen.y, steps_per_epoch=spe, callbacks=callback_list, verbose=1)#, validation_data=val_dataset, validation_batch_size=16)
    print(f&quot;--------------- Done epoch {i+1} -----------------&quot;)

classification.save_weights(&quot;final_class.h5&quot;)
</code></pre>
","-1","Question"
"77834092","","<p>I have a data set in which i have diameters of lunar craters. I need to group them into different categories.(using python)</p>
<p>Column names in my data are ID, latitude, londitude, diameter and depth. Values seperated by space</p>
<p>For eg: there is a crater of 980m as diameter then it does into the class of craters whose diameter is less than 1 km (let us name that category as SET1) similarly there is a crater of diameter 40 km then it goes into the category of craters whose diameter is less than 50km but greater than 30km (let us name it as SETX).
I need to create such categories and classify all these craters into them.</p>
<p>I also need to count the number of craters in each such categories.
Also note there are almost 0.8 million craters in my data.</p>
<p>I need ideas or solution to how can I can solve the above problem.</p>
","-3","Question"
"77834628","","<p>I have encountered an issue where the recall score obtained using <code>recall_score(y, y_pred)</code> does not match the value calculated manually using <code>confusion_matrix</code>.</p>
<p>Not only that, but recall is exactly the same value as specificity which I've also calculated manually below.</p>
<p>Here is the relevant code I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>recall = recall_score(y, y_pred) # &lt;-- different score

conf_matrix = confusion_matrix(y, y_pred)
tn, fp, fn, tp = conf_matrix.ravel()
manual_recall = tp / (tp + fn) # &lt;-- to this score
specificity = tn / (tn + fp) # &lt;-- and is the same as the score above
</code></pre>
<p>Here's an example of a confusion matrix as printed in the terminal where this happens:</p>
<pre><code>[[34  6]
 [20 20]]
</code></pre>
<p>Sci Kit Recall: 0.85
Manual Recall: 0.5</p>
<p>or</p>
<pre><code>[[29 11]
 [ 9 31]]
</code></pre>
<p>Sci Kit Recall: 0.725
Manual Recall: 0.775</p>
<p><strong>Problem:</strong></p>
<p>Recall as returned by scikit-learn and manual recall do not produce the same value.</p>
<p><strong>Question:</strong></p>
<p>Why might the <code>recall_score</code> and manual calculation using <code>confusion_matrix</code> yield different results for the recall score?</p>
<p><strong>More information...</strong></p>
<ul>
<li><p>It is a binary classification problem.</p>
</li>
<li><p>I'm using the default threshold for <code>recall_score</code>.</p>
</li>
<li><p>I've tried to determine whether the confusion table is accurate (it is).</p>
</li>
</ul>
","1","Question"
"77836071","","<p>I am working on a perceptron problem and I have made some fake data and the perceptron algorithm does not converge when the data is linearly separable.</p>
<p>Here is the fake data that is linearly separable.</p>
<pre><code>np.random.seed(42)
linear_df = pd.DataFrame({
    'X1': np.round(np.concatenate([np.random.uniform(low = 0, high= 5, size=4), np.random.uniform(low=8, high=12, size=4)]), 1),
    'X2': np.round(np.concatenate([np.random.uniform(low = 0, high= 5, size=4), np.random.uniform(low=8, high=12, size=4)]),1),
    'Y': [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]
})
</code></pre>
<p>then I run perceptron on it</p>
<pre><code>clf = Perceptron(verbose=1, max_iter=1000)
X = linear_df[['X1', 'X2']]
y = linear_df['Y']
clf.fit(X, y)
linear_coef = clf.coef_
linear_bias = clf.intercept_[0]
print(clf.coef_)
print(clf.intercept_)
print(clf.score(X, y))
</code></pre>
<p>Convergence after 8 epochs took 0.00 seconds
[[ 2.3 -2.6]]
[17.]
0.5</p>
<p>But it says it converges after 8 Epochs and it does not produce the correct output.]</p>
<p>Here is the plot:</p>
<p><img src=""https://i.sstatic.net/8JDdU.png"" alt=""perceptron plot"" /></p>
<p>Any ideas?</p>
","2","Question"
"77836878","","<p>I wanted to convert my trained model for better inference performance, by using TF-TRT.
I used the nvidia tensorflow docker image, and had no problem with running test code.</p>
<p>Test code is from here: <a href=""https://github.com/jhson989/tf-to-trt"" rel=""nofollow noreferrer"">https://github.com/jhson989/tf-to-trt</a></p>
<p>and Detail Docker Image tag: nvcr.io/nvidia/tensorflow:23.12-tf2-py3</p>
<p>But when I tried to convert my trained model, it didn't work.</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.python.compiler.tensorrt import trt_convert as trt

# The trained model is .h5 format
h5_model_path = 'model/path/h5/model_name'
h5_model = keras.models.load_model(model_path, compile=False)

# Need to convert .h5 to saved_model format for using TF-TRT
saved_model_path = 'model/path/saved_model/model_name'
tf.saved_model.save(h5_model, saved_model_path)

# Make a Converter
conversion_param = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP16)
converter = trt.TrtGraphConverterV2(input_saved_model_dir=saved_model_path, conversion_params=conversion_param)

# Error occurs from here
converter.convert()
</code></pre>
<p>And this error occurred.</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py&quot;, line 92, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /model/path/saved_model/model_name/variables/variables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py&quot;, line 1031, in load_partial
    loader = Loader(object_graph_proto, saved_model_proto, export_dir,
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py&quot;, line 226, in __init__
    self._restore_checkpoint()
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py&quot;, line 561, in _restore_checkpoint
    load_status = saver.restore(variables_path, self._checkpoint_options)
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py&quot;, line 1415, in restore
    reader = py_checkpoint_reader.NewCheckpointReader(save_path)
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py&quot;, line 96, in NewCheckpointReader
    error_translator(e)
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/py_checkpoint_reader.py&quot;, line 31, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /model/path/saved_model/model_name/variables/variables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/model/code/convert_model.py&quot;, line 106, in eval
    converter.convert()
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py&quot;, line 1453, in convert
    self._saved_model = load.load(self._input_saved_model_dir,
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py&quot;, line 900, in load
    result = load_partial(export_dir, None, tags, options)[&quot;root&quot;]
  File &quot;/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py&quot;, line 1034, in load_partial
    raise FileNotFoundError(
FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /model/path/saved_model/model_name/variables/variables
 You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.
</code></pre>
<p>I already confirmed the saved_model version of my model has same directory inside with test code.
Specifically '/model/path/saved_model/model_name/variables' directory, with variables.data-00000-of-00001 and variablevariables.index.</p>
","0","Question"
"77837026","","<p>I have a GAMLSS trained model which I have saved using <code>saveRDS()</code> in .rda format.
For eg, I trained model as:</p>
<pre><code>gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test) 
</code></pre>
<p>When I load the above model after clearing all env variables, and use predict function for new data:</p>
<pre><code>predict(model_old, newdata = new_data) 
</code></pre>
<p>I get below error:</p>
<pre><code>Error in eval(Call$data) : object 'test' not found 
</code></pre>
<p>But this test is old dataset which shouldn't have any significance here. I am not able to understand what's wrong with this. Because of this, I cannot run REST API.</p>
<p>When all my env variables are there after GAMLSS model is trained, then when I use predict immediately, it works! But I want to use predictions for later.</p>
","0","Question"
"77840804","","<p>I´m trying to install the <code>neurolab</code> package to my <code>Python 3.11</code> environment.
I use <code>Anaconda</code> with <code>Python version 3.11</code>. The available versions of <code>Neurolab</code> found on <code>Anaconda</code> page are those:</p>
<p><a href=""https://i.sstatic.net/VxWrl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VxWrl.png"" alt=""enter image description here"" /></a></p>
<p>But none of this versions work on <code>Python 3.11</code> . I tried to instal via <code>Anaconda Navigator</code> and <code>CMD (I use Windows 11)</code>.</p>
<p>How to install this package (or a similar version that works) with my version of Python?</p>
","-1","Question"
"77840815","","<p>Here is my CNN model, this is for a grayscale image that is 400x400:</p>
<pre><code>import torch
import torch.nn as nn

class MModel(nn.Module):
    def __init__(self):
        super(MModel, self).__init__()
        
        # Define convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        
        # Calculate the size of the flattened feature map before the fully connected layers
        self.fc_input_size = 64 * 200 * 200
        
        # Define fully connected layers
        self.fc1 = nn.Linear(self.fc_input_size, 128)
        self.fc2 = nn.Linear(128, 18)  # Adjust the output size based on your requirements
        
    def forward(self, x):
        # Apply convolutional and pooling layers
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        
        # Flatten the feature map
        x = x.view(-1, self.fc_input_size)
        
        # Apply fully connected layers
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

# Create an instance of the CNN model
model = MModel()
</code></pre>
<p>and here is my training loop:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10

for epoch in range(num_epochs):
    running_loss = 0.0

    for i, data in enumerate(train_DL, 0):
        inputs, labels = data

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate the loss
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()

        if i % 10 == 9:  # Print every 10 mini-batches
            print(f&quot;[{epoch + 1}, {i + 1}] Loss: {running_loss / 10:.3f}&quot;)
            running_loss = 0.0

print(&quot;Training finished&quot;)
</code></pre>
<p>When running this code, i get this error, my batch size is 32 for my DataLoader:</p>
<pre><code>ValueError: Expected input batch_size (8) to match target batch_size (32).
</code></pre>
<p>I tried changing the batch size to 8, which still gave me a value error. Also other solutions found on StackOverflow don't seem to be working.</p>
","0","Question"
"77841705","","<h3>Problem</h3>
<p>I have a <code>*.tfrecords</code> file that I want to feed in a <code>ConvLSTM2D</code> model, created using Tensorflow. Here is the model structure.</p>
<pre class=""lang-py prettyprint-override""><code>model = Sequential([
    ConvLSTM2D(64, (3, 3), activation='relu', input_shape=(20, 224, 224, 3), return_sequences=True),
    BatchNormalization(),
    ConvLSTM2D(64, (3, 3), activation='relu', return_sequences=True),
    BatchNormalization(),
    Flatten(),
    Dense(1, activation='sigmoid')
])
</code></pre>
<p>When i try to fit my data into the model, <strong>it takes up all of the system ram</strong>.</p>
<h6>Tested on M1 MacBook 2020 (Jupyter Notebook, Pycharm), Google Colab.</h6>
<pre class=""lang-py prettyprint-override""><code>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(train_input_fn(), steps_per_epoch=5,validation_data=val_input_fn(),epochs=10)
</code></pre>
<h2>What we are doing</h2>
<p>We have shanghai dataset which have fight and non fight dataset. so we are trying to predict and classify fight and non fight video using Convolutional Long Short term Memory.</p>
<p>We have 800 train videos. We capture frames with 250 ms interval and convert all into numpy array. Then we store all the arrays in a TFRecord file.</p>
<p>When we pass the dataset into our model, we do so using this function <code>train_input_fn()</code> which reads the tfrecord file and pass data into our model.</p>
<hr />
<p>You can see the colab notebook from <a href=""https://colab.research.google.com/drive/1xUNIu6VBCJGioPPQEBh93RJRNsrjDhly?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
<p>Dataset Structure is given below:</p>
<pre><code>Dataset
    - train
        - Fight      # has 800 *.avi files
        - NonFight   # has 800 *.avi files
    - val
        - Fight      # has 200 *.avi files
        - NonFight   # has 200 *.avi files

</code></pre>
<h2>What we have tried?</h2>
<ul>
<li>We have tried minimizing <code>batch_size</code> from 64 down to 16.</li>
<li>Reduced the whole dataset from 800 videos down to 200 videos in train set</li>
<li>Tried to reduce the filter size of <code>ConvLSTM2D</code></li>
<li>Did all the same things with *.mp4</li>
<li>Reduced one layer of <code>BatchNormalization()</code> and <code>ConvLSTM2D</code></li>
</ul>
","0","Question"
"77842203","","<p>This is my complete code:</p>
<pre><code>!pip install -q transformers einops accelerate langchain bitsandbytes sentence_transformers faiss-cpu        pypdf sentencepiece 
from langchain import HuggingFacePipeline 
from transformers import AutoTokenizer 
from langchain.embeddings import HuggingFaceEmbeddings 
from langchain.document_loaders.csv_loader import CSVLoader 
from langchain.vectorstores import FAISS, Chroma
from langchain.chains import RetrievalQA 
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
import accelerate
import transformers 
import torch 
import textwrap 
loader = CSVLoader('/kaggle/input/csvdata/chatdata.csv', encoding=&quot;utf-8&quot;, csv_args={'delimiter': ','}) 
data = loader.load() 

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cpu'}) 

db = FAISS.from_documents(data, embeddings)


#Mistral 7B model llm

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig,
    TextStreamer,
    pipeline,
)

MODEL_NAME = &quot;mistralai/Mistral-7B-Instruct-v0.1&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, device_map=&quot;auto&quot;, torch_dtype=torch.float16, load_in_8bit=True
)

generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
generation_config.max_new_tokens = 1024
generation_config.temperature = 0.0001
generation_config.do_sample = True
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)


llm = pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=tokenizer,
    return_full_text=True,
    generation_config=generation_config,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    streamer=streamer,
)


def format_prompt(prompt, system_prompt=&quot;&quot;):
    if system_prompt.strip():
        return f&quot;[INST] {system_prompt} {prompt} [/INST]&quot;
    return f&quot;[INST] {prompt} [/INST]&quot;


SYSTEM_PROMPT = &quot;&quot;&quot;
You are a Clinical Data Scientist and Data Analyst specializing in statistical data analysis and report generation. Your mission is to provide accurate and insightful data-driven solutions for healthcare and clinical research. As you respond, channel the expertise and precision typical of a seasoned data professional in the field of clinical data science.
If you encounter a question for which you don't have the necessary information, it's important to refrain from providing speculative or inaccurate answers.
&quot;&quot;&quot;.strip()

chain = ConversationalRetrievalChain.from_llm(
    llm,
    chain_type=&quot;stuff&quot;,
    retriever=db.as_retriever(),
    return_source_documents=True,
    verbose=True,
)
</code></pre>
<p>Here I'm facing the error:</p>
<pre><code>ValidationError: 2 validation errors for LLMChain
llm
  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)
llm
  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)


from textwrap import fill

result = chain(input(&quot;ClinicalTrial Planimeter ChatBot ---&quot;)
)
print(fill(result[&quot;result&quot;].strip(), width=80))
</code></pre>
<p>This llm chain is programmed to chat with csv using llm, vector database and prompt, I'm facing the above error on running ConversationalRetrievalChain</p>
","-1","Question"
"77843151","","<p>I am trying to create aws glue spark job to train one of the data set . I am using xgboost algorithm in 1.3-1 version . When i try to run the estimator , i am having issue</p>
<p>infrastructure : aws glue 4.00 spark shell</p>
<p>all file folders are s3 path</p>
<p>code snippet.</p>
<pre><code>xgb_script_mode_estimator = XGBoost(
    entry_point=&quot;training.py&quot;,
    hyperparameters=hyperparameters,
    role=role,
    instance_count=1,
    instance_type=instance_type,
    framework_version=&quot;1.3-1&quot;,
    output_path=&quot;s3://{}/{}/{}/output&quot;.format(hyperparameters['bucket_nm'], '/output/', job_name),
   
</code></pre>
<p>error :</p>
<p>FileNotFoundError: [Errno 2] No such file or directory: 'training.py'</p>
<p>I placed the &quot;glue script&quot; and training.py in the same job bucket in same folder with <strong>init</strong>.py file .</p>
<p>The XGBoost function is not recognizing the training.py in the same folder (no name mismatch for the training file including case)</p>
","0","Question"
"77843515","","<p>I trying to train a dataset using the Xgboost algorithm for 1.7-1 version . While calling Xgboost function it is throwing out error as follows .</p>
<pre><code>2024-01-19:02:57:27:INFO] Imported framework sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] No GPUs detected (normal if no gpus installed)
[2024-01-19:02:57:27:INFO] Invoking user training script.
[2024-01-19:02:57:27:ERROR] Reporting training FAILURE
[2024-01-19:02:57:27:ERROR] framework error: 
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 2318, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 1105, in fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 1041, in frombuf
    raise EmptyHeaderError(&quot;empty header&quot;)
tarfile.EmptyHeaderError: empty header
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py&quot;, line 84, in train
    entrypoint()
  File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py&quot;, line 102, in main
    train(framework.training_env())
  File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py&quot;, line 87, in train
    framework.modules.run_module(
  File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py&quot;, line 290, in run_module
    _files.download_and_extract(uri, _env.code_dir)
  File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py&quot;, line 131, in download_and_extract
    with tarfile.open(name=dst, mode=&quot;r:gz&quot;) as t:
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 1621, in open
    return func(name, filemode, fileobj, **kwargs)
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 1674, in gzopen
    t = cls.taropen(name, mode, fileobj, **kwargs)
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 1651, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 1514, in __init__
    self.firstmember = self.next()
  File &quot;/miniconda3/lib/python3.8/tarfile.py&quot;, line 2333, in next
    raise ReadError(&quot;empty file&quot;)
tarfile.ReadError: empty file
empty file
</code></pre>
<p>I have two source files having same structure with .csv extension .
I am not sure why it is complaining of tar file empty</p>
","-1","Question"
"77843560","","<p>I am trying to build a datapipeline for inference on some video files. The videos are alot in number hence, I am using the the tf.data.dataset pipeline with a from_generator method to create manageable batches. But the dataset keeps yielding same output over again.</p>
<p>Here is my code:</p>
<pre><code>enteclass FrameGenerator:
def __init__(self, video_paths, n_frames, training=False):
    &quot;&quot;&quot;
        Returns a set of frames from a video in the video_paths list

        Args:
        video_paths: a list of path to videos
        n_frames: Number of frames
        training: A boolean to determine if a training dataset should be created
    &quot;&quot;&quot;

    self.video_paths = video_paths
    self.n_frames = n_frames
    self.training = training

def __call__(self):
    &quot;&quot;&quot;
        gets called and yields a set of frames
        each time the instance of the class is called
    &quot;&quot;&quot;
    video_paths = self.video_paths
    # print(type(video_paths))
    if self.training:
        random.shuffle(video_paths)

    for path in video_paths:
        video_frames = frames_from_video_file(path, self.n_frames)
        file_name = path.split('/')[-1]
        # print(file_name, &quot;filfile&quot;)
        yield video_frames, file_namer code here
</code></pre>
<p>and the code is instantiated here:</p>
<pre><code>output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype=tf.float32), tf.TensorSpec(shape = None, dtype=tf.string))

dataset = tf.data.Dataset.from_generator(FrameGenerator(video_paths, 20, training=False), output_signature=output_signature)
</code></pre>
<p>I have the batch size as 10
and an instance of the dataset thus:</p>
<pre><code>inference_datasets = dataset.batch(BATCH_SIZE)
</code></pre>
<p>But when I call this:</p>
<pre><code>sample_inference_dataset = next(iter(inference_datasets))
</code></pre>
<p>and this:</p>
<pre><code>enter codesample_inference_dataset[1]
</code></pre>
<p>It yields the same sets of files.</p>
","0","Question"
"77843589","","<p>I want the output becomes non NaN value.
the problem in this line</p>
<pre><code>f_bp_max.loc[l, 'max'] = df_frcst[df_frcst['Datetime'].dt.year == k]['Forecast'].max()
</code></pre>
<p>When I print this:</p>
<pre><code>df_frcst[df_frcst['Datetime'].dt.year == k]['Forecast']
</code></pre>
<p>Output is:</p>
<pre><code>Series([], Name: Forecast, dtype: float64)
</code></pre>
<pre><code>import pandas as pd 
import time
import datetime
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize_scalar

start_time = time.time()

pct_select = 0.98

#* data forecast
df_frcst = pd.read_csv('lstm_forecast_results.csv')
df_frcst['Datetime'] = pd.to_datetime(df_frcst['Datetime'])
df_frcst_simple = df_frcst[['Datetime', 'Forecast']]
df_frcst_simple = df_frcst_simple.rename({'Datetime':'waktu', 'Forecast':'bp' }, axis = 1)



df_bp_max = pd.DataFrame()
for k in range (2024, 2034, 1):
    l = k - 2024
    df_bp_max.loc[l, 'tahun'] = str(k)
    df_bp_max.loc[l, 'max'] = df_frcst[df_frcst['Datetime'].dt.year == k]['Forecast'].max()
    df_bp_max.loc[l, 'min'] = df_frcst[df_frcst['Datetime'].dt.year == k]['Forecast'].min()
#df_bp_max['LF'] = df_bp_max['min']/df_bp_max['max']
df_bp_max['tahun'] = df_bp_max['tahun'].apply(str)
#df_bp_max = df_bp_max.set_index('tahun')

print(&quot;==============================================================&quot;)
print(df_bp_max)
</code></pre>
<p>input files</p>
<pre><code>Datetime,Actual,Forecast
2022-01-01 12:30:00,17809.0,17343.484
2022-01-01 13:00:00,17772.61,17382.861
2022-01-01 13:30:00,17867.8,17414.637
2022-01-01 14:00:00,17773.68,17504.357
2022-01-01 14:30:00,17869.88,17530.559
2022-01-01 15:00:00,17786.7,17592.822
2022-01-01 15:30:00,17943.11,17626.775
2022-01-01 16:00:00,18125.29,17686.678
2022-01-01 16:30:00,18463.05,17760.666
2022-01-01 17:00:00,18786.99,17892.475
2022-01-01 17:30:00,19238.97,18048.4
2022-01-01 18
......
</code></pre>
<p>Output:</p>
<pre><code>==============================================================
  tahun  max  min
0  2024  NaN  NaN
1  2025  NaN  NaN
2  2026  NaN  NaN
3  2027  NaN  NaN
4  2028  NaN  NaN
5  2029  NaN  NaN
6  2030  NaN  NaN
7  2031  NaN  NaN
8  2032  NaN  NaN
9  2033  NaN  NaN
</code></pre>
<p>none of this row have a NaN value because my input file is not empty</p>
","0","Question"
"77848436","","<p>I got sequential time series data. At each time stamp, there is only variable to observe (if my understanding is correct this means number of features = 1). I want to train a simple RNN with more than one layer to predict the next observation.</p>
<p>I created training data using sliding window, with window size set to 8. To give a concrete idea, below is my original data, training data and target .</p>
<p><strong>Sample Data</strong></p>
<p><code>0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37 0.49</code></p>
<p><strong>Training Data</strong></p>
<pre><code>X = 
    0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 
    0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53
    0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37

</code></pre>
<p>corresponding targets are</p>
<pre><code>Y = 
     0.53 
     3.37
     0.49
</code></pre>
<p>I set the batch size to 3. But it gives me an error saying</p>
<p><code>RuntimeError: input.size(-1) must be equal to input_size. Expected 8, got 1</code></p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import numpy as np

X = np.array( [ [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] ], dtype=np.float32)

Y = np.array([[0.53], [3.37], [0.49]], dtype=np.float32)

class RNNModel(nn.Module):
    def __init__(self, input_sz, n_layers):
        super(RNNModel, self).__init__()
        self.hidden_dim = 3*input_sz
        self.n_layers = n_layers
        output_sz = 1
        self.rnn = nn.RNN(input_sz, self.hidden_dim, num_layers=n_layers, batch_first=True)
        self.linear = nn.Linear(self.hidden_dim, output_sz)

    def forward(self,x):
        batch_sz = x.size(0)
        hidden = torch.zeros(self.n_layers, batch_sz, self.hidden_dim) #initialize n_layer*batch_sz number of hidden states of dimension hidden_dim)
        out, hidden = self.rnn(x, hidden)
        out = out.contiguous().view(-1, self.hidden_dim)
        return out,hidden

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = RNNModel(8,2)
X = torch.tensor(X[:,:,np.newaxis])
Y = torch.tensor(Y[:,:,np.newaxis])
X = X.to(device)
Y = Y.to(device)
model = model.to(device)
optimizer = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

loader = data.DataLoader(data.TensorDataset(X, Y), shuffle=False, batch_size=3)

n_epoch = 10
for epoch in range(n_epoch):
    model.train()
    for X_batch, Y_batch in loader:
        Y_pred = model(X_batch)
        loss = loss_fn(Y_pred,Y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    if epoch % 10 != 0:
        continue
        model.eval()
        with torch.no_grad():
            Y_pred = model(X)
            train_rmse = np.sqrt(loss_fn(Y_pred,Y))
        print(&quot;Epoch %d: train RMSE %.4f&quot; % (epoch, train_rmse))

</code></pre>
<p>What am I doing wrong? Can anyone help me?</p>
","0","Question"
"77851097","","<p>Using <code>TreeExplainer</code> in SHAP, I could not plot the Waterfall Plot.</p>
<p>Error Message:</p>
<pre><code>---&gt; 17 shap.plots.waterfall(shap_values[0], max_display=14) 
TypeError: The waterfall plot requires an `Explanation` object as the
`shap_values` argument.
</code></pre>
<p>Since my model is tree based, I use TreeExplainer (because of using xgb.XGBClassifier).</p>
<p>If I use the <code>Explainer</code> instead <code>TreeExplainer</code>, I can plot Waterfall Plot.</p>
<p>My code is given below:</p>
<pre><code>import pandas as pd

data = {
    'a': [1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8, 1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8],
    'b': [2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10, 2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10],
    'c': [1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1, 1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1],
    'd': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],
    'e': [1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1, 1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1],
    'f': [1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1],
    'g': [3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2],
    'h': [1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5],
    'i': [1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6],
    'j': [5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6],
    'k': [3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1, 3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1],
    'r': [1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]
}

df = pd.DataFrame(data)

X = df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
y = df.iloc[:,11]

from sklearn.model_selection import train_test_split, GridSearchCV
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)

import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth'     :   [6],
    'n_estimators'  :   [500],
    'learning_rate' :   [0.3]
}


grid_search_xgboost =   GridSearchCV(
    estimator       =   xgb.XGBClassifier(),
    param_grid      =   param_grid,
    cv              =   3,  
    verbose         =   2,  
    n_jobs          =   -1  
)

grid_search_xgboost.fit(X_train, y_train)

print(&quot;Best Parameters:&quot;, grid_search_xgboost.best_params_)
best_model_xgboost = grid_search_xgboost.best_estimator_

import shap

explainer = shap.TreeExplainer(best_model_xgboost)
shap_values = explainer.shap_values(X_train)

shap.summary_plot(shap_values, X_train, plot_type=&quot;bar&quot;)

shap.summary_plot(shap_values, X_train)

for name in X_train.columns:
    shap.dependence_plot(name, shap_values, X_train)

shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0], matplotlib=True)

shap.decision_plot(explainer.expected_value, shap_values[:10], X_train.iloc[:10])

shap.plots.waterfall(shap_values[0], max_display=14)
</code></pre>
<p>Where is the problem?</p>
","2","Question"
"77858297","","<p>I have spacy model which I am using for inference in .pkl format. The datatype of .pkl file is &lt;class 'spacy.lang.en.English'&gt;. I want to make inference script run on GPU. I tried using different methods using spacy gpu, numba etc.</p>
<pre><code>import spacy  
spacy.prefer_gpu() # or spacy.require_gpu()
nlp = spacy.load(&quot;content/path&quot;)
</code></pre>
<p>It didn't work, I think by converting the .pkl into .pt will work by loading the pt file to 'cuda' device. Please suggest approach to handle this scenario.</p>
","0","Question"
"77859735","","<p>I've successfully trained some promising models using Azure AutoML and now I want to deploy them locally.</p>
<p>I used simple CSV files as datasets (using Azure ML v1 APIs) to train the model. Afterward, I downloaded the model and inserted it using the scoring script provided by Microsoft in Conda:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------
import json
import logging
import os
import pickle
import numpy as np
import pandas as pd
import joblib

import azureml.automl.core
from azureml.automl.core.shared import logging_utilities, log_server
from azureml.telemetry import INSTRUMENTATION_KEY

from inference_schema.schema_decorators import input_schema, output_schema
from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType
from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType
from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType

data_sample = PandasParameterType(pd.DataFrame({""SIO2"": pd.Series([0.0], dtype=""float32""), ""AL2O3"": pd.Series([0.0], dtype=""float32""), ""B2O3"": pd.Series([0.0], dtype=""float32""), ""CAO"": pd.Series([0.0], dtype=""float32""), ""K2O"": pd.Series([0.0], dtype=""float32""), ""NA2O"": pd.Series([0.0], dtype=""float32""), ""PBO"": pd.Series([0.0], dtype=""float32""), ""Li2O"": pd.Series([0.0], dtype=""float32""), ""MgO"": pd.Series([0.0], dtype=""float32""), ""SRO"": pd.Series([0.0], dtype=""float32""), ""BAO"": pd.Series([0.0], dtype=""float32""), ""ZNO"": pd.Series([0.0], dtype=""float32""), ""P2O5"": pd.Series([0.0], dtype=""float32""), ""ZRO2"": pd.Series([0.0], dtype=""float32""), ""TIO2"": pd.Series([0.0], dtype=""float32""), ""Bi2O3"": pd.Series([0.0], dtype=""float32"")}))
input_sample = StandardPythonParameterType({'data': data_sample})

result_sample = NumpyParameterType(np.array([0.0]))
output_sample = StandardPythonParameterType({'Results':result_sample})
sample_global_parameters = StandardPythonParameterType(1.0)

try:
    log_server.enable_telemetry(INSTRUMENTATION_KEY)
    log_server.set_verbosity('INFO')
    logger = logging.getLogger('azureml.automl.core.scoring_script_v2')
except:
    pass


def init():
    global model
    # This name is model.id of model that we want to deploy deserialize the model file back
    # into a sklearn model
    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')
    path = os.path.normpath(model_path)
    path_split = path.split(os.sep)
    log_server.update_custom_dimensions({'model_name': path_split[-3], 'model_version': path_split[-2]})
    try:
        logger.info(""Loading model from path."")
        model = joblib.load(model_path)
        logger.info(""Loading successful."")
    except Exception as e:
        logging_utilities.log_traceback(e, logger)
        raise

@input_schema('Inputs', input_sample)
@input_schema('GlobalParameters', sample_global_parameters, convert_to_provided_type=False)
@output_schema(output_sample)
def run(Inputs, GlobalParameters=1.0):
    data = Inputs['data']
    result = model.predict(data)
    return {'Results':result.tolist()}</code></pre>
</div>
</div>
</p>
<p>The environment is set up using the .yml file, but I always get the error message:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Cell In[1], line 12
      9 import pandas as pd
     10 import joblib
---&gt; 12 import azureml.automl.core
     13 from azureml.automl.core.shared import logging_utilities, log_server
     14 from azureml.telemetry import INSTRUMENTATION_KEY

ModuleNotFoundError: No module named 'azureml'
</code></pre>
<p>However, I don't believe it's related to packages. When I attempt to deploy the models via a real-time endpoint in Azure and test them using the &quot;Test&quot; tab after successful implementation, I receive the following error message:</p>
<p>Interestingly, I don't get this error message in VS. Instead, when executing the run function there, I only receive the following error message:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Invalid input data type to parse. Expected: &lt;class 'dict'&gt; but got &lt;class 'inference_schema.parameter_types.pandas_parameter_type.PandasParameterType'&gt;
Stapelüberwachung:
 &gt;  File ""C:\Users\weightedfinalmodel\scoring_file_v_2_0_0.py"", line 59, in &lt;module&gt; (Current frame)
 &gt;    run(data_sample)
""inference_schema.schema_decorators"" geladen
""__main__"" geladen
""runpy"" geladen
Das Programm ""python.exe"" wurde mit Code 4294967295 (0xffffffff) beendet.</code></pre>
</div>
</div>
</p>
<p>That looks better already, but I seem to be overlooking something.</p>
","0","Question"
"77859877","","<p>I have two models, <code>model_A</code> and <code>model_B</code>. I want to do element wise addition with those two models and use the result as input to <code>model_C</code>. So, I have this code:</p>
<pre><code>from tensorflow.keras.layers import Conv2D, BatchNormalization, \
    Activation, Input, Add
from tensorflow.keras.models import Model
import numpy as np
import tensorflow as tf

def model_A(inputs):
    x1 = Conv2D(32, 3, padding='same')(inputs)
    x1 = BatchNormalization()(x1)
    x1 = Activation('relu')(x1)
    
    x2 = Conv2D(32, 3, padding='same')(x1)
    model = Model(inputs=inputs, outputs=x2, name='model_A')
    return model
    

def model_B(inputs):
    f1 = Conv2D(32, 3, padding='same')(inputs)
    f1 = BatchNormalization()(f1)
    f1 = Activation('relu')(f1)
    
    f2 = Conv2D(32, 3, padding='same')(f1)

    model = Model(inputs=inputs, outputs=f2, name='model_B')
    return model

def model_C(inputs):
    f1 = Conv2D(32, 3, padding='same')(inputs)
    f1 = BatchNormalization()(f1)
    f1 = Activation('relu')(f1)
    
    f2 = Conv2D(16, 3, padding='same')(f1)
    f2 = BatchNormalization()(f2)
    f2 = Activation('relu')(f2)

    f3 = Conv2D(1, 3,  padding='same')(f2)

    model = Model(inputs=inputs, outputs=f3, name='model_C')
    return model
    
def model_final(height, width, channels):
    inputs = Input((height, width, channels))
    
    modelA = model_A(inputs)
    modelB = model_B(inputs)
    
    addition = Add()([modelA.output, modelB.output])
    
    modelC = model_C(addition)
    
    return Model(inputs, modelC.output)
    
a = np.random.uniform(0, 1, (100, 32, 32, 3))
b = np.random.uniform(0, 1, (100, 32, 32, 3))
c = np.random.uniform(0, 1, (100, 32, 32, 3))
    
model = model_final(32, 32, 3)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer,
              loss='mae',
              metrics=['mae'])
    
</code></pre>
<p>If I run the code, I receive <code>Graph disconnected</code> at <code>Model(inputs=inputs, outputs=f3, name='model_C')</code>. So, in order to solve this problem, I am doing:</p>
<pre><code>def model_final(height, width, channels):
    inputs = Input((height, width, channels))
    
    modelA = model_A(inputs)
    modelB = model_B(inputs)
    
    addition = Add()([modelA.output, modelB.output])
    
    inputs_C = Input((height, width, 32))
    modelC = model_C(inputs_C)
    modelC = modelC(addition)
    
    model = Model(inputs, modelC)
    return model
</code></pre>
<p>which compiles fine. But, I am not sure if this is right. If the logic of doing this is correct!</p>
","0","Question"
"77862602","","<p>I load model from Hugging Face using pipeline:</p>
<pre><code>device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = &quot;openai/whisper-large-v3&quot;

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    &quot;automatic-speech-recognition&quot;,
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=20,
    batch_size=16,
    return_timestamps=True,
    torch_dtype=torch_dtype,
    device=device,
)
</code></pre>
<p>And i want to process big audio file</p>
<pre><code>result = pipe(&quot;filename3.mp3&quot;)
</code></pre>
<p>But google collab says i ran out of RAM. Is it possible to pipe output to file rather than variable?</p>
","-1","Question"
"77864227","","<p>Here is my training code.</p>
<pre><code>from accelerate.utils import write_basic_config
write_basic_config()

import os

os.environ[&quot;MODEL_NAME&quot;] = &quot;runwayml/stable-diffusion-v1-5&quot;
os.environ[&quot;INSTANCE_DIR&quot;] = &quot;/notebooks/me_photos&quot;
os.environ[&quot;OUTPUT_DIR&quot;] = &quot;/notebooks/me_model_1_22&quot;
script_path = &quot;/notebooks/diffusers/examples/dreambooth/train_dreambooth_lora.py&quot;

!accelerate launch {script_path} \
  --pretrained_model_name_or_path={os.environ[&quot;MODEL_NAME&quot;]} \
  --instance_data_dir={os.environ[&quot;INSTANCE_DIR&quot;]} \
  --output_dir={os.environ[&quot;OUTPUT_DIR&quot;]} \
  --instance_prompt=&quot;a photo of Ryan&quot; \
  --resolution=512 \
  --train_batch_size=1 \
  --learning_rate=2e-6 \
  --max_train_steps=2400 \
  --gradient_checkpointing \
  --use_8bit_adam \
  --with_prior_preservation \
  --prior_loss_weight=1.0 \
  --class_data_dir=&quot;/notebooks/faces_prior_preservation&quot; \
  --class_prompt=&quot;a photo of person face&quot;
</code></pre>
<p>Here is the code to produce image.</p>
<pre><code>from diffusers import DiffusionPipeline
import torch

# Initialize logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()


pipe = DiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, 
                                         torch_dtype=torch.float16, 
                                         use_safetensors=True, 
                                         variant=&quot;fp16&quot;)
pipe.to(&quot;cuda&quot;)


pipe.load_lora_weights(&quot;/notebooks/me_model_1_22&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot; , adapter_name=&quot;me&quot;)

active_adapters = pipe.get_active_adapters()
active_adapters

logger.info(f&quot;LoRA {active_adapters} loaded successfully.&quot;)


# Generate an image
prompt = &quot;a photo of Ryan&quot;
lora_scale= 1
image = pipe(
    prompt, num_inference_steps=30, cross_attention_kwargs={&quot;scale&quot;: lora_scale}
).images[0]

# Save the image
output_path = &quot;/notebooks/image_of_me2.png&quot;
image.save(output_path)
logger.info(f&quot;Image saved at {output_path}&quot;)

# Clean up
del image
torch.cuda.empty_cache()


</code></pre>
","-2","Question"
"77864368","","<p>Llama-cpp-python gives me Assertion Error even though im using the GGUF Format.</p>
<p>I am trying to run an AI model in python 3.7.2 with llama-cpp-python 0.1.85, everytime I run this my code I get this error:</p>
<pre><code>error loading model: MapViewOfFile failed: Not enough memory resources are available to process this command.

llama_load_model_from_file: failed to load model
Traceback (most recent call last):
  File &quot;server.py&quot;, line 26, in &lt;module&gt;
    n_ctx=N_CTX,
  File &quot;D:\AI 2\Venv\lib\site-packages\llama_cpp\llama.py&quot;, line 323, in __init__
    assert self.model is not None
AssertionError
</code></pre>
<p>I am using the GGUF format, so I don't know what the problem is, it works fine on a second computer but not on my main machine, any help?</p>
","0","Question"
"77869443","","<p>I have an example class as follows:</p>
<pre><code>class MLP(nn.Module):
    # Declare a layer with model parameters. Here, we declare two fully
    # connected layers
    def __init__(self):
        # Call the constructor of the `MLP` parent class `Module` to perform
        # the necessary initialization. In this way, other function arguments
        # can also be specified during class instantiation, such as the model
        # parameters, `params` (to be described later)
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # Hidden layer
        self.out = nn.Linear(256, 10)  # Output layer

    # Define the forward propagation of the model, that is, how to return the
    # required model output based on the input `X`
    def forward(self, X):
        # Note here we use the funtional version of ReLU defined in the
        # nn.functional module.
        return self.out(torch.relu(self.hidden(X)))
</code></pre>
<p>Calling the class is like this:
<code>net = MLP() net(X)</code></p>
<p>Now, I need to create a similar class and function for a model with 4 layers:</p>
<p>Layers  Configuration   Activation Function
fully connected input size 128, output size 64  ReLU
fully connected input size 64, output size 32   ReLU
dropout probability 0.5 -
fully connected input size 32, output size 1    Sigmoid</p>
<p>I need to pass the following assertion:</p>
<pre><code>model = Net()

assert model.fc1.in_features == 128
assert model.fc1.out_features == 64
assert model.fc2.in_features == 64
assert model.fc2.out_features == 32
assert model.fc3.in_features == 32
assert model.fc3.out_features == 1

x = torch.rand(2, 128)
output = model.forward(x)
assert output.shape == (2, 1), &quot;Net() is wrong!&quot;
</code></pre>
<p>Here is what I have so far:</p>
<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
                
        
        self.fc1 = nn.Linear(128, 64)
        self.fc2 = nn.Linear(64, 32)
        self.dropout = nn.Dropout(p=0.5)
        self.fc3 = nn.Linear(32, 1)
        

    def forward(self, x):
        return self.fc3(torch.sigmoid(self.dropout(self.fc2(torch.relu(self.fc1(torch.relu(X)))))))
       
</code></pre>
<p>But I'm getting an error:</p>
<pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x20 and 128x64)
</code></pre>
<p>How to resolve it?</p>
","2","Question"
"77870847","","<p>I have a labeled dataset with X shape being 7000 x 2400 and y shape being 7000. The data is heavily imbalanced, so I am trying to generate synthetic samples using SMOTE. However I want to identify the synthetic samples that SMOTE actually generated.
As an example , here's a code snippet:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from imblearn.over_sampling import SMOTE

iris = load_iris()

X = iris['data']
y = iris['target']

#The data is balanced , so I intentionally remove some samples
X = X[:125,::]
y = y[:125]

oversample = SMOTE()
X_smt, y_smt = oversample.fit_resample(X, y)
</code></pre>
<p>The arrays X_smt and y_smt have both the original samples and the synthetic samples. Is there a simple way to identify the synthetic samples by index or some other mechanism ?</p>
","0","Question"
"77872605","","<p>I am using PyTorch for image classification.</p>
<p>I coded the following train function that worked with a simple linear model:</p>
<pre><code>criterion = nn.CrossEntropyLoss()
def train(model, dataloader, epoch):
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
running_loss, running_acc = 0., 0.
loss_history = []
accuracy_history = [](data_train):.2f}%&quot;)

for i in range(1, epoch + 1):
  model.train()
  for inputs, targets in dataloader:
      inputs, targets = inputs.to(device), targets.to(device)
      outputs = model(inputs)
      loss = criterion(outputs, targets)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      preds = torch.argmax(outputs, 1)
      running_loss += loss.item()
      running_acc += torch.sum(preds == targets).item()

  print(f&quot;[TRAIN epoch {i}] Loss: {running_loss/len(data_train):.2f} Acc: {100 * running_acc/len
 
</code></pre>
<p>I have the pre-trained VGG16 model and I want to change the weights of its last layer:</p>
<pre><code>model_vgg = models.vgg16(weights='DEFAULT')
model_vgg.classifier[6] = nn.Linear(4096, 2)

for param in model_vgg.parameters():
    param.requires_grad = False
model_vgg.classifier[-1].requires_grad = True

train(model_vgg, train_loader, 2)
</code></pre>
<p>However, when training it I get the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)

&lt;timed eval&gt; in &lt;module&gt;

&lt;ipython-input-27-1f64686a5cfd&gt; in train(model, dataloader, epoch)
     39           loss = criterion(outputs, targets)
     40           optimizer.zero_grad()
---&gt; 41           loss.backward()
     42           optimizer.step()
     43           preds = torch.argmax(outputs, 1)

/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
--&gt; 251     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    252         tensors,
    253         grad_tensors_,

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>How do I fix this?</p>
","0","Question"
"77876955","","<p>I'm trying to train a simple CNN with Flux and running into a weird issue...during training the loss appears to go down (indicating that it's working) but despite what the loss curve suggested the &quot;trained&quot; model output was very bad, and when I calculated the loss by hand I noticed that it differed from what the training indicated it should be (it was acting like it hadn't been trained at all).</p>
<p>I then started calculating the loss returned inside the gradient vs. outside, and after a lot of digging I think the problem is related to the <code>BatchNorm</code> layer. Consider the following minimum example:</p>
<pre><code>using Flux
x = rand(100,100,1,1) #say a greyscale image 100x100 with 1 channel (greyscale) and 1 batch
y = @. 5*x + 3 #output image, some relationship to the input values (doesn't matter for this)
m = Chain(BatchNorm(1),Conv((1,1),1=&gt;1)) #very simple model (doesn't really do anything but illustrates the problem)
l_init = Flux.mse(m(x),y) #initial loss after model creation
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m) #loss calculated by gradient
l_final = Flux.mse(m(x),y) #loss calculated again using the model (no parameters have been updated)
println(&quot;initial loss: $l_init&quot;)
println(&quot;loss calculated in withgradient: $l_grad&quot;)
println(&quot;final loss: $l_final&quot;)
</code></pre>
<p>All of the losses above will be different, sometimes pretty drastically (when running just now I got 22.6, 30.7, and 23.0), when I think they should all be the same?</p>
<p>Interestingly if I remove the <code>BatchNorm</code> layer, the outputs are all the same, i.e. running:</p>
<pre><code>using Flux
x = rand(100,100,1,1) #say a greyscale image 100x100 with 1 channel (greyscale) and 1 batch
y = @. 5*x + 3 #output image
m = Chain(Conv((1,1),1=&gt;1))
l_init = Flux.mse(m(x),y) #initial loss after model creation
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m)
l_final = Flux.mse(m(x),y)
println(&quot;initial loss: $l_init&quot;)
println(&quot;loss calculated in withgradient: $l_grad&quot;)
println(&quot;final loss: $l_final&quot;)
</code></pre>
<p>Produces the same number for each loss calculation.</p>
<p>Why does including the <code>BatchNorm</code> layer change the value of the loss like this?</p>
<p>My (limited) understanding was that this was just supposed to normalize the input values, which I understand could affect the loss between the unormalized and normalized case, but I don't understand why it would produce different values of the loss for the same input values on the same model without any of the parameters of said model being updated?</p>
","2","Question"
"77877253","","<p>I am trying to predict the winner of an NBA game using a random forest classifier. I have sought to remove and modify my list of features so that I can increase accuracy and decrease noise.</p>
<p>I implemented the solution found here: <a href=""https://datascience.stackexchange.com/questions/57697/decision-trees-should-we-discard-low-importance-features"">https://datascience.stackexchange.com/questions/57697/decision-trees-should-we-discard-low-importance-features</a>, where I would loop through the top N most important features and plot out the resulting accuracy. After all my features have gone through that loop, I'm left with a plot that looks like this:
<a href=""https://i.sstatic.net/ItOsV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ItOsV.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, the resulting graph is kind of all over the place. Do I remove the features that have a negative slope? Or what's the threshold to removing features? Is there a better way to calculate noise? How would I get the most accurate model given that I have so many features with such a variable impact on my model accuracy on training data?</p>
","0","Question"
"77879635","","<p>Currently to reinitialize a model for <code>AutoModelForSequenceClassification</code>, we can do this:</p>
<pre><code>from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification

m = &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;
config = AutoConfig.from_pretrained(m)
model_from_scratch = AutoModel(config)

model_from_scratch.save_pretrained(&quot;frugalscore_tiny_bert-from_scratch&quot;)

model = AutoModelForSequenceClassification(
  &quot;frugalscore_tiny_bert-from_scratch&quot;, local_files_only=True
)
</code></pre>
<h3>Is there some way to reinitialize the model weights without saving a new pretrained model initialized with <code>AutoConfig</code>?</h3>
<pre><code>model = AutoModelForSequenceClassification(
  &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;, 
  local_files_only=True
  reinitialize_weights=True
)
</code></pre>
<p>or something like:</p>
<pre><code>model = AutoModelForSequenceClassification(
  &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;, 
  local_files_only=True
)

model.reinitialize_parameters()
</code></pre>
","1","Question"
"77881238","","<p>I have a pandas dataset of various features, including datetime feature.
It looks like this:</p>
<pre><code>           DD SSCL1 SEG_CLASS_CODE  FCLCLD  PASS_BK  SA  AU  DTD  DAY_OF_YEAR
0  2018-01-01     C              C       1        0   0  18   -1            1
1  2018-01-01     C              C       0        0   7  26   -1            1
2  2018-01-01     C              C       0        0   9  18   -1            1
3  2018-01-01     C              C       1       10   0  18   -1            1
4  2018-01-01     C              C       0        9   1  18   -1            1
</code></pre>
<p>I need to use <code>DD</code> column to train the model. The problem is how to encode this column?</p>
<p>I can`t use Cyclic Feature Encoding, described here:
<a href=""https://stackoverflow.com/questions/46428870/how-to-handle-date-variable-in-machine-learning-data-pre-processing"">How to handle date variable in machine learning data pre-processing</a>
because in the field for which I am teaching the model, 2020 is not the same as 2018, and February 2022 is not February 2023. So, years, months and days sometimes differ from each other.</p>
<p>My idea is to somehow transform datetime to int. For example, to get total days or hours or minutes or seconds, but i do not know the starting point (Maybe January 1st, 1970 as usual).
The easiest way to use: <code>dataset['DD']).apply(lambda x: x.value)</code>, so I`ll get something like this:</p>
<pre><code>0          1514764800000000000
1          1514764800000000000
2          1514764800000000000
3          1514764800000000000
4          1514764800000000000
                  ...         
1450583    1577577600000000000
1450584    1577664000000000000
1450585    1577664000000000000
1450586    1577145600000000000
1450587    1577232000000000000
Name: DD, Length: 1450588, dtype: int64
</code></pre>
<p>After that I would like to use MinMaxScaler or Standardscaler.</p>
<p>Are there any ways to encode datetime according to my requirements?</p>
","0","Question"
"77881247","","<p>I am encountering an issue while working with a PyTorch convolutional neural network. The error message I'm receiving is:</p>
<p>Given groups=1, weight of size [8, 3, 5, 5], expected input[1, 128, 128, 3] to have 3 channels, but got 128 channels instead</p>
<p>Context:</p>
<p>Model Architecture:</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.cnn_model = nn.Sequential(
        nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 3, stride = 5),
        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 2, stride = 5))
        
        self.fc_model = nn.Sequential(
        nn.Linear(in_features = 256, out_features = 120),
        nn.Tanh(),
        nn.Linear(in_features = 120, out_features = 84),
        nn.Tanh(),
        nn.Linear(in_features = 84, out_features = 1))
        
    def forward(self, x):
        x = self.cnn_model(x)
        x = x.view(x.size(0), -1)
        x = self.fc_model(x)
        x = F.sigmoid(x)
        
        return x
</code></pre>
<p>Data Loading:</p>
<pre><code>class MRI(Dataset):
    def __init__(self):
        # Load images and labels
        tumor = []
        path_tumor = '/kaggle/input/brain-tumor/Dataset/Yes_Data/*.jpg'
        for f in glob.iglob(path_tumor):
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            tumor.append(img)

        healthy = []
        path_healthy = '/kaggle/input/brain-tumor/Dataset/No_data/*.jpg'
        for f in glob.iglob(path_healthy):
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            healthy.append(img)

        # Convert lists to numpy arrays
        healthy = np.array(healthy)
        tumor = np.array(tumor)
        
        # Create labels
        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)
        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)

        # Concatenate images and labels
        images = np.concatenate((tumor, healthy), axis=0)
        labels = np.concatenate((tumor_label, healthy_label), axis=0)
        self.images = images
        self.labels = labels

    def __getitem__(self, index):
        sample = {'image': self.images[index], 'label': self.labels[index]}
        return sample

    def __len__(self):
        return self.images.shape[0]

    def normalize(self):
        self.images = (self.images / 255.0).astype(np.float32)
</code></pre>
<p>Error Context:</p>
<pre><code>model.eval()
outputs = []
y_true = []

with torch.no_grad():
    for sample in dataloader:
        for i in range(sample['image'].size(0)):
            image = sample['image'][i].squeeze().to(device).float()
            label = sample['label'][i].to(device)

            y_hat = model(image)
            outputs.append(y_hat.cpu().detach().numpy())
            y_true.append(label.cpu().detach().numpy())

</code></pre>
<p>I have tried every debugging method out there, printing and checking the shape<a href=""https://i.sstatic.net/LDvT3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LDvT3.png"" alt=""Debugging"" /></a></p>
","0","Question"
"77883294","","<p>I've been trying for days to do an LSTM Multi Target without success, for a dataset with the first 8 columns being targets and the other columns features, generating dimension errors.   The challenge consists of predicting 8 targets with integer values ​​that can be 0 or 1 or 2, based on the feature values. Previously I successfully created an LSTM that predicted a single target column, which was the sum of all 8 columns. But this sum generated undesirable results in the confidence score. What´s mistake?</p>
<pre><code>import pandas as pd
import numpy as np

# Set seed for reproducibility
np.random.seed(42)

# Create DataFrame
data = {'col_' + str(i+1): np.random.choice([0, 1, 2], 100) if i &lt; 8 else np.random.uniform(-0.99, 0.99, 100) for i in range(100)}
df = pd.DataFrame(data)
df.head()

# Display the DataFrame
print(df)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Extracting targets and features
targets = df.iloc[:, :8].values
features = df.iloc[:, 8:].values

# Scaling features
scaler = StandardScaler()
features = scaler.fit_transform(features)

# Convert targets to LongTensor
targets = torch.tensor(targets, dtype=torch.long)

# Convert features to FloatTensor
features = torch.tensor(features, dtype=torch.float32)

# Split the data into train and test sets
features_train, features_test, targets_train, targets_test = train_test_split(
    features, targets, test_size=0.2, random_state=None
)

# Create DataLoader
train_dataset = TensorDataset(features_train, targets_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Define the LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

# Set hyperparameters
input_size = features.shape[1]
hidden_size = 64
num_layers = 2
output_size = 8  # Number of target classes
num_epochs = 10
learning_rate = 0.001

# Instantiate the model, loss function, and optimizer
model = LSTMModel(input_size, hidden_size, num_layers, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_features, batch_targets in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_features)
        loss = criterion(outputs, batch_targets)
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    test_outputs = model(features_test)
    _, predicted = torch.max(test_outputs, 1)

# Calculate accuracy
correct = (predicted == targets_test).sum().item()
total = targets_test.size(0)
accuracy = correct / total
print(f'Test Accuracy: {accuracy:.4f}')
</code></pre>
<p>Runtime Error:</p>
<pre><code>--------------------------------------------------------------------------- IndexError                                Traceback (most recent call last) Cell In[4], line 60 58 for batch_features, batch_targets in train_loader: 59     optimizer.zero_grad() ---&gt; 60     outputs = model(batch_features) 61     loss = criterion(outputs, batch_targets) 62     loss.backward()

File c:\Users\Admin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py:1501, in Module._call_impl(self, *args, **kwargs) 1496 # If we don't have any hooks, we want to skip the rest of the logic in 1497 # this function, and just call forward. 1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1499         or _global_backward_pre_hooks or _global_backward_hooks 1500         or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1501     return forward_call(*args, **kwargs) 1502 # Do not call functions when jit is used 1503 full_backward_hooks, non_full_backward_hooks = [], []

Cell In[4], line 40 38 def forward(self, x): 39     out, _ = self.lstm(x) ---&gt; 40     out = self.fc(out[:, -1, :]) 41     return out

IndexError: too many indices for tensor of dimension 2 
</code></pre>
<p>I try modify many times the Class LSTModel without a secure result</p>
","0","Question"
"77883357","","<p>This example doesn’t show how to save the k-means cluster model into the database. Is there a way to do that and is there example code for it?</p>
<p><a href=""https://docs.oracle.com/en/database/oracle/machine-learning/oml4py/1/mlpug/k-means.html#GUID-7909D96B-D3B9-411B-BAD5-96DAFAF06B42"" rel=""nofollow noreferrer"">Oracle Machine Learning for Python</a></p>
<p>Here's how I create the model,</p>
<pre><code># Create a KM model object and fit it.
km_mod = oml.km(n_clusters = 3, **setting).fit(data)
</code></pre>
","-1","Question"
"77884077","","<p>I need to compute the covariance matrix of the parameter gradients taken on the points of a subset of the CIFAR10 dataset. For that I have this code:</p>
<pre><code>from torch.func import functional_call, vmap, grad

model1 = LogisticModel().to(device)

def loss_fn(predictions, targets):
  loss = nn.CrossEntropyLoss()
  return loss(predictions, targets)

def compute_loss(params, buffers, sample, target):
  batch = sample.unsqueeze(0)
  targets = target.unsqueeze(0)

  predictions = functional_call(model1, (params, buffers), (batch,))
  loss = loss_fn(predictions, targets)
  return loss

ft_compute_grad = grad(compute_loss)
ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))

def sherman_morrison_update(A, u, v):
  vT = v.T
  Au = A @ u

  alpha = 1/(1 + vT@Au)
  A = A - alpha*torch.outer(Au, vT@A)
  return A

testloader1 = DataLoader(test_dataset, batch_size = 512)
params = {k: v.detach() for k, v in model1.named_parameters()}
buffers = {k: v.detach() for k, v in model1.named_buffers()}
w = 0

p_covs = {p:torch.eye(q.flatten().shape[0]).to(device) for p,q in param_grads.items()}
param_grad_mean = {p:torch.zeros(q.flatten().shape[0]).to(device) for p,q in param_grads.items()}

for x,y in tqdm(testloader1):
  param_grads = ft_compute_sample_grad(params, buffers, x.to(device), y.to(device))
  for p,q, mean in zip(param_grads.values(), p_covs, param_grad_mean):
    for p_grad in p:
      w += 1
      diff = p_grad.flatten() - param_grad_mean[mean]
      param_grad_mean[mean] += diff / w
      p_covs[q] = sherman_morrison_update(A=p_covs[q], u=diff, v= diff)
</code></pre>
<p>Now, this is very inefficient and doing this in every iteration is quite time taking.
Also, we can't really take the gradients of the parameters at all points at once because that causes memory issues (thus I shifted to Sherman-Morrison).</p>
<p>Is there a way to make it more efficient? A better implementation of Sherman-Morrison? Anything else?</p>
","0","Question"
"77885918","","<p>I have designed a simple MLP model trained on 6k data samples.</p>
<pre><code>class MLP(nn.Module):
    def __init__(self,input_dim=92, hidden_dim = 150, num_classes=2):
        super().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        #self.softmax = nn.Softmax(dim=1)

        self.layers = nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.num_classes),

        )

    def forward(self, x):
        x = self.layers(x)
        return x
</code></pre>
<p>and the model has been instantiated</p>
<pre><code>model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)

optimizer = Optimizer.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
criterion = nn.CrossEntropyLoss()
</code></pre>
<p>and the hyperparameters:</p>
<pre><code>num_epoch = 300   # 200e3//len(train_loader)
learning_rate = 1e-3
batch_size = 64
device = torch.device(&quot;cuda&quot;)
SEED = 42
torch.manual_seed(42)
</code></pre>
<p>My implementation mostly follows <a href=""https://stackoverflow.com/questions/71199036/pytorch-nn-not-as-good-as-sklearn-mlp"">this question</a>. I save the model as pre-trained weights <code>model_weights.pth</code>.</p>
<p>The accuracy of <code>model</code> on the test dataset is <code>96.80%</code>.</p>
<p>Then, I have another 50 samples (in <code>finetune_loader</code>) that I am trying to fine-tune the model on these 50 samples:</p>
<pre><code>model_finetune = MLP()
model_finetune.load_state_dict(torch.load('model_weights.pth'))
model_finetune.to(device)
model_finetune.train()
# train the network
for t in tqdm(range(num_epoch)):
  for i, data in enumerate(finetune_loader, 0):
    #def closure():
      # Get and prepare inputs
      inputs, targets = data
      inputs, targets = inputs.float(), targets.long()
      inputs, targets = inputs.to(device), targets.to(device)
      
      # Zero the gradients
      optimizer.zero_grad()
      # Perform forward pass
      outputs = model_finetune(inputs)
      # Compute loss
      loss = criterion(outputs, targets)
      # Perform backward pass
      loss.backward()
      #return loss
      optimizer.step()     # a

model_finetune.eval()
with torch.no_grad():
    outputs2 = model_finetune(test_data)
    #predicted_labels = outputs.squeeze().tolist()

    _, preds = torch.max(outputs2, 1)
    prediction_test = np.array(preds.cpu())
    accuracy_test_finetune = accuracy_score(y_test, prediction_test)
    accuracy_test_finetune
    
    Output: 0.9680851063829787
</code></pre>
<p>The accuracy remains the same as before fine-tuning the model to 50 samples, I checked, and the output probabilities are also the same.</p>
<p>What could be the reason? Am I making some mistakes in the code for fine-tuning?</p>
","-1","Question"
"77888418","","<p>I'm trying to install and use <a href=""https://github.com/IDEA-Research/GroundingDINO"" rel=""nofollow noreferrer"">grounding dino</a> in a Sagemaker instance (with GPU) but i got the error:</p>
<pre><code>NameError: name '_C' is not defined
</code></pre>
<p>I found out the reason is because the variable CUDA_HOME is not configured so to solve it I need to set the variable, but after searching for answers(I already checked the common path /usr/local/cuda) I cannot find the path where cuda is installed in sagemaker instances.</p>
<p>Where is cuda installed in sagemaker instances so I can set CUDA_HOME?</p>
","-2","Question"
"77890079","","<pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras import layers

# set the embedding dimension
embedding_dim = 100

# create the model
model = Sequential([
    layers.Embedding(max_words, embedding_dim, input_length=max_length),
    layers.LSTM(64),
    layers.Dense(1, activation='sigmoid')
])

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# print the model summary
print(model.summary())
</code></pre>
<p>I tried to compile the model above in VSCode with Jupyter Notebook (.ipynb) but ran into the following error:</p>
<p>AttributeError: module 'keras.src.backend' has no attribute 'floatx'</p>
<p>Initially, I managed to compile the model with no problems, but caused VScode to crash when fitting the model. After reloading VSCode, I'm getting this error.</p>
<p>To explain the context, I'm trying to build a really basic NLP model to classify Amazon reviews based on sentiment. I'm also on Python 3.11 and Tensorflow version 2.15</p>
<p>At first I tried the following:</p>
<pre><code>import keras.backend as K
K.set_floatx('float32')
</code></pre>
<p>But I got the same error. Then I tried reseting VSCode and running the notebook again but still I'm getting the same error?</p>
","2","Question"
"77890171","","<p>Here is what I tried:</p>
<pre><code>pip install pickle5
</code></pre>
<p>Here is a <a href=""https://i.sstatic.net/PQ52a.png"" rel=""nofollow noreferrer"">snapshot</a> with the error messages.</p>
<p>Here is also the error I get:</p>
<p><code>ERROR: Could not build wheels for pickle5, which is required to install pyproject.toml-based projects</code></p>
<p>I tried installing and reinstalling Microsoft Visual C++ as suggested in some other posts but it didn't work.</p>
","1","Question"
"77891268","","<p>I'm trying to do an object detection model on tensorflow with keras but I've been having so difficulty. I automated the task of finding the bounding boxes of my training images(training dataset is of a game) and dumped then all on a .csv file containing all the data pertaining to it: frame in which object appeared, coordinates of the bounding box, and class of the object. Each bounding box has a different row on the dataset even if multiple boxes appear on the same frame.</p>
<p>I'm trying to import my dataset into Tensorflow using this function:</p>
<pre><code>def Data_Loader(annotation_file):
    data=pd.read_csv(annotation_file)
    data_groups=data.groupby('filename')

    Dataset={'images':[], 'bounding_boxes':[]}
    ngroups=data_groups.ngroups

    for image_name, group in data_groups:

        BBoxes={'classes': [], 'boxes':[]}
        for _, row in group.iterrows():
            BBoxes['boxes'].append(Get_BBOX(row))
            BBoxes['classes'].append(class_ids.index(row['class']))

       
        Dataset['bounding_boxes'].append(tf.data.Dataset.from_tensor_slices(BBoxes))
        image=load_img(image_name,(224, 224))
        Dataset['images'].append(tf.constant(image))
 

    Dataset=tf.data.Dataset.from_tensor_slices(Dataset)

    return(Dataset)

</code></pre>
<p>Where the following is how the bounding boxes are loaded:</p>
<pre><code>def Get_BBOX(row):
    xmin=int(row['xmin'])
    ymin=int(row['ymin'])
    xmax=int(row['xmax'])
    ymax=int(row['ymax'])

    bbox=np.array([xmin, ymin, xmax, ymax])

    return bbox

</code></pre>
<p>And the image is loaded like this:</p>
<pre><code>def load_img(filename, target_size):
    img = tf.keras.utils.load_img(filename, target_size=target_size) 
    img = tf.keras.utils.img_to_array(img) 

    return (img)

</code></pre>
<p><a href=""https://keras.io/guides/keras_cv/object_detection_keras_cv/"" rel=""nofollow noreferrer"">I'm using this tutorial from Keras to guide myself</a></p>
<p>But whenever I reach the part of the tutorial for applying the map on the data, I get the following error message:</p>
<pre><code>'_VariantDataset' object is not subscriptable
</code></pre>
<p>Anyone knows what I could be doing wrong and how to fix it?</p>
<p>I tried multiple times to make several conversions of types, changing from dictionaries containing list to lists of dictionaries and all other kinds of things. None of which yielded any results.</p>
","0","Question"
"77891386","","<p>When I try to use sympy to find the derivative of the loss function, it raises a conflict with format string.</p>
<pre><code>import numpy as np
import sympy as sp

def predict(X, w, b):
    return np.dot(X, w) + b

def loss(X, w, b, Y):
    return np.mean((predict(X, w, b) - Y) ** 2)

X, Y = np.loadtxt(&quot;code/02_first/pizza.txt&quot;, unpack=True, skiprows=1)

# Convert X and Y to sympy symbols
X, w, b, Y = sp.symbols(&quot;X w b Y&quot;)

def gradient(X, w, b, Y):
    loss_expr = loss(X, w, b, Y)
    dw_dX = sp.diff(loss_expr, w)
    db_dX = sp.diff(loss_expr, b)
    return dw_dX, db_dX

def train(X, Y, iterations, lr):
    w = sp.symbols('w')
    b = sp.symbols('b')
    
    for i in range(iterations):
        loss_value = loss(X, w, b, Y)
        print(f&quot;Iteration: {i:4d}, Loss: {loss_value:.10f}&quot;)
        dw_dX, db_dX = gradient(X, w, b, Y)
        w -= dw_dX * lr
        b -= db_dX * lr
    return w, b

w, b = train(X, Y, iterations=20000, lr=0.001)

print(f&quot;\nw = {w:.10f}, b = {b:.10f}&quot;)
print(f&quot;Prediction: x = 20 =&gt; y = {predict(20, w, b):.2f}&quot;)
</code></pre>
<pre><code>TypeError: unsupported format string passed to Pow.__format__
</code></pre>
<p>The data is here in txt (or via the <a href=""https://media.pragprog.com/titles/pplearn/code/02_first/pizza.txt"" rel=""nofollow noreferrer"">link here</a>):</p>
<pre><code>Reservations  Pizzas
13            33
2             16
14            32
23            51
13            27
1             16
18            34
10            17
26            29
3             15
3             15
21            32
7             22
22            37
2             13
27            44
6             16
10            21
18            37
15            30
9             26
26            34
8             23
15            39
10            27
21            37
5             17
6             18
13            25
13            23
</code></pre>
<p>I can just use numpy but in doing so I need to calculate the loss function myself, which is not effective (and easy to raise error with brackets).</p>
<p>Why the error and why sympy is not compatible with format string? Also, how to generate a correct script with sympy?</p>
","0","Question"
"77891961","","<p>can you please help me.........
I have my custom trained model (best.pt), it detects two things person and headlight. Now I want the output according to these conditions: 1. If model detect only headlight return 0, 2. If model detect only person return 1, 3. If model detect headlight and person both return 0.</p>
<pre><code>import cv2
from ultralytics import YOLO

video_path = 'data/video1.mp4'
video_out_path = 'out.mp4'

cap = cv2.VideoCapture(video_path)

# Check if the video file is opened successfully
if not cap.isOpened():
    print(&quot;Error: Could not open the video file.&quot;)
    exit()

ret, frame = cap.read()

# Check if the first frame is read successfully
if not ret:
    print(&quot;Error: Could not read the first frame from the video.&quot;)
    exit()

cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),
                          (int(cap.get(3)), int(cap.get(4))))  # Use cap.get(3) and cap.get(4) for width and height

model = YOLO(&quot;bestall5.pt&quot;)

detection_threshold = 0.5
while ret:
    results_list = model(frame)

    headlight_detected = False
    person_detected = False

    # Iterate through the list of results
    for results in results_list:
        # Check if the current result has the necessary attributes
        if hasattr(results, 'xyxy'):
            for result in results.xyxy:
                x1, y1, x2, y2, score, class_id = result.tolist()
                x1, x2, y1, y2 = int(x1), int(x2), int(y1), int(y2)

                # Assuming class_id is the index of the class in the model's class list
                class_name = model.names[class_id]

                if class_name == &quot;headlight&quot; and score &gt; detection_threshold:
                    headlight_detected = True
                elif class_name == &quot;person&quot; and score &gt; detection_threshold:
                    person_detected = True

    # Output based on the specified conditions
    if headlight_detected and person_detected:
        output = 0
    elif headlight_detected:
        output = 0
    elif person_detected:
        output = 1
    else:
        output = -1  # No person or headlight detected

    print(&quot;Output:&quot;, output)

    cap_out.write(frame)

    cv2.imshow('Object Detection', frame)
    
    # Break the loop if 'q' key is pressed
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

    ret, frame = cap.read()

cap.release()
cap_out.release()
cv2.destroyAllWindows()
</code></pre>
<p>I tried this but getting only -1 as output but my video has both headlight and person</p>
","-1","Question"
"77892019","","<p>I want to apply the exact same transformation to two images (image and segmentation data) using torchio. Both these images are stored in numpy arrays called <code>image_data</code> and <code>segmentation_data</code>.</p>
<p>So far, I added some augmentations:</p>
<pre><code>self.augmentations = tio.Compose([
            affine_transform,
            elastic_transform,
            flip_transform,
            swap_transform
        ])
</code></pre>
<p>where e.g. elastic_transform = tio.RandomElasticDeformation and tried to apply these to the images in the following way:</p>
<pre><code>        subject_image = tio.Subject(image=tio.ScalarImage(tensor=image_data))
        subject_segmentation = tio.Subject(
            image=tio.ScalarImage(tensor=segmentation_data))
        dataset = tio.SubjectsDataset([subject_image, subject_segmentation])
        dataset = self.augmentations(dataset)
        image_data = dataset[0]['image'].data
        segmentation_data = dataset[1]['image'].data
</code></pre>
<p>Unfortunately, that's incorrect (beacuse Compose won't work with a SubjectsDataset). How to do it correctly?</p>
","1","Question"
"77892097","","<p>I am running the NVIDIA-Optical-Character-Detection-and-Recognition-Solution locally on my Jetson. I want to block the pipeline of OCDNet and only infer with OCRNet. I commented out all the code related to OCDNet. By the result is coming out as empty inference.</p>
","0","Question"
"77892140","","<p>I am developing a machine learning model using CNN for brain tumor detection. I have trained a model and saved it and now want to integrate with a simple website. But I can't seem to solve this error and can't even understand why the error is occurring in the first place.</p>
<pre><code>from flask import Flask, render_template, request, redirect, url_for
import os
from werkzeug.utils import secure_filename
import torch
from torchvision import transforms
from PIL import Image
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
import glob
from torch.utils.data import Dataset, DataLoader, ConcatDataset

class Dataset(object):
    &quot;&quot;&quot;An abstract class representing a Dataset.

    All other datasets should subclass it. All subclasses should override
    ``__len__``, that provides the size of the dataset, and ``__getitem__``,
    supporting integer indexing in range from 0 to len(self) exclusive.
    &quot;&quot;&quot;

    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def __add__(self, other):
        return ConcatDataset([self, other])

class MRI(Dataset):
    def __init__(self):
        
        tumor = []
        healthy = []
        # cv2 - It reads in BGR format by default
        for f in glob.iglob(r&quot;C:\Users\ishan\Desktop\Ishan\Coding\Hackathon\Dataset\Yes_Data/*.jpg&quot;):
            img = cv2.imread(f)
            img = cv2.resize(img,(128,128)) # I can add this later in the boot-camp for more adventure
            b, g, r = cv2.split(img)
            img = cv2.merge([r,g,b])
            img = img.reshape((img.shape[2],img.shape[0],img.shape[1])) # otherwise the shape will be (h,w,#channels)
            tumor.append(img)

        for f in glob.iglob(r&quot;C:\Users\ishan\Desktop\Ishan\Coding\Hackathon\Dataset\No_data/*.jpg&quot;):
            img = cv2.imread(f)
            img = cv2.resize(img,(128,128)) 
            b, g, r = cv2.split(img)
            img = cv2.merge([r,g,b])
            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))
            healthy.append(img)

        # our images
        tumor = np.array(tumor,dtype=np.float32)
        healthy = np.array(healthy,dtype=np.float32)
        
        # our labels
        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)
        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)
        
        # Concatenates
        self.images = np.concatenate((tumor, healthy), axis=0)
        self.labels = np.concatenate((tumor_label, healthy_label))
        
    def __len__(self):
        return self.images.shape[0]
    
    def __getitem__(self, index):
        
        sample = {'image': self.images[index], 'label':self.labels[index]}
        
        return sample
    
    def normalize(self):
        self.images = self.images/255.0


class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        self.cnn_model = nn.Sequential(
        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size=2, stride=5),
        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size=2, stride=5))
        
        self.fc_model = nn.Sequential(
        nn.Linear(in_features=256, out_features=120),
        nn.Tanh(),
        nn.Linear(in_features=120, out_features=84),
        nn.Tanh(),
        nn.Linear(in_features=84, out_features=1))
        
    def forward(self, x):
        x = self.cnn_model(x)
        x = x.view(x.size(0), -1)
        x = self.fc_model(x)
        x = torch.sigmoid(x)
        
        return x

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'static/uploads'
app.config['ALLOWED_EXTENSIONS'] = {'jpg'}
path = r'C:\Users\ishan\Desktop\Ishan\Coding\Project\FinalModel.pth'

# Create an instance of the model
model_instance = CNN()

# Load your trained model
model_instance.load_state_dict(torch.load('FinalModel.pth', map_location=torch.device('cpu')))

# Set the model in evaluation mode
model_instance.eval()

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
    ])
    image = Image.open(image_path).convert('RGB')
    return transform(image).unsqueeze(0)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return redirect(request.url)

    file = request.files['file']

    if file.filename == '':
        return redirect(request.url)

    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)

        # Make predictions
        input_tensor = preprocess_image(file_path)
        with torch.no_grad():
            output = model(input_tensor)  # Output is a tensor
            prediction = &quot;Tumor Detected&quot; if output.item() &gt; 0.5 else &quot;No Tumor&quot;
        return render_template('index.html', prediction=prediction, image_path=file_path)

    return redirect(request.url)

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p>This is the error:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;C:\Users\ishan\Desktop\Ishan\Coding\Project\app.py&quot;, line 113, in &lt;module&gt;
    model_instance.load_state_dict(torch.load('FinalModel.pth', map_location=torch.device('cpu')))
  File &quot;C:\Users\ishan\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;, line 2103, in load_state_dict
    raise TypeError(f&quot;Expected state_dict to be dict-like, got {type(state_dict)}.&quot;)
TypeError: Expected state_dict to be dict-like, got &lt;class 'type'&gt;
</code></pre>
<p>How can I get rid of this error?</p>
","0","Question"
"77893385","","<p>I am trying to get <code>yolo</code> to use my <code>gpu</code>, and I have gotten it to start, but then it reaches the stage of scanning the train and afterwards val images, but just freezes after doing the train ones.</p>
<p>This is the output and where it stops:</p>
<pre><code>Ultralytics YOLOv8.1.6 🚀 Python-3.11.7 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 4070 SUPER, 12281MiB)
    engine\trainer: task=detect, mode=train, model=yolov8s.yaml, data=path_n_class.yaml, epochs=300, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\detect\train
    Overriding model.yaml nc=80 with nc=1

                   from  n    params  module                                       arguments                     
  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 
  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                
  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             
  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               
  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           
  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              
  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           
  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              
  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           
  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          
 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          
 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 
 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              
 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 
 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              
 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 
 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          
YOLOv8s summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs

Freezing layer 'model.22.dfl.conv.weight'
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...
100%|██████████| 6.23M/6.23M [00:00&lt;00:00, 7.51MB/s]
AMP: checks passed ✅
train: Scanning C:\Users\lichs\Desktop\pj\8nano100\training\datasets\data\labels\train.cache... 762 images, 204 backgrounds, 0 corrupt: 100%|██████████| 762/762 [00:00&lt;?, ?it/s]
</code></pre>
<p>I have tried to run the same code just removing the device=0 argument so it goes back to using the cpu, but it still does the same.</p>
","0","Question"
"77893929","","<p>Im working on this deep learning project in pytorch where I have 2 fully connected neural networks and I need to train then test them. But when I run the code in google colab it is not much faster than running it on my CPU on my PC. I have colab pro btw. It is also using 0.6 out of the 40GB GPU RAM of the A100 GPU.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim


device = torch.device(&quot;cuda:0&quot;)
# Define transform
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load FashionMNIST dataset
trainset = torchvision.datasets.FashionMNIST('./data', download=True, train=True, transform=transform)
testset = torchvision.datasets.FashionMNIST('./data', download=True, train=False, transform=transform)

# Create data loaders
trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=1  , shuffle=False, num_workers=2)

# Define constant for classes
classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')




# Define the fully connected neural network
class FCNN(nn.Module):
    def __init__(self, num_layers=1):
        super(FCNN, self).__init__()
        self.num_layers = num_layers
        self.fc_layers = nn.ModuleList()
        if self.num_layers == 1:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
        elif self.num_layers == 2:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
            self.fc_layers.append(nn.Linear(1024, 1024))
        self.output_layer = nn.Linear(1024, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        for layer in self.fc_layers:
            x = nn.functional.relu(layer(x))
        x = self.output_layer(x)
        return x

# Modify the train function to move inputs and labels to the GPU
def train(net, criterion, optimizer, epochs=15):
    for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()

            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 2000 == 1999:
                print('[%d, %5d] loss: %.2f' %
                      (epoch + 1, i + 1, running_loss / 2000))
                running_loss = 0.0

# Define function to test accuracy
def test(net):
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy: %d %%' % (
            100 * correct / total))

# Main function
if __name__ == &quot;__main__&quot;:
    # Define the network
    net1 = FCNN(num_layers=1)
    net2 = FCNN(num_layers=2)
    net2.to(device)

    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.0)
    optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.0)

    # Train and test network with 1 FC layer
    #print(&quot;Training network with 1 layer...&quot;)
    #train(net1, criterion, optimizer1)
    #test(net1)

    # Train and test network with 2 FC layers
    print(&quot;Training network with 2 layers...&quot;)
    train(net2, criterion, optimizer2)
    test(net2)
</code></pre>
<p>tried using different GPUS in google colab</p>
<p>tried adding this line to always use CUDA cores:</p>
<pre><code>device = torch.device(&quot;cuda:0&quot;)
</code></pre>
<p>and had the network use the device:</p>
<pre><code>device = torch.device(&quot;cuda:0&quot;)
</code></pre>
","0","Question"
"77897573","","<p>I need help with adding <a href=""https://docs.ultralytics.com/models/yolov8/"" rel=""nofollow noreferrer"">YOLOv8 model</a> to this below code instead of using an InceptionV3 to extract the image features for my project. I need to pass the detected objects and extracted features from the YOLOv8 model to generate the caption using the transformer.</p>
<pre><code>def CNN_Encoder_Incep():
    inception_v3 = tf.keras.applications.InceptionV3(
        include_top=False,
        weights='imagenet'
    )
    inception_v3.trainable = False

    output = inception_v3.output
    output = tf.keras.layers.Reshape(
        (-1, output.shape[-1]))(output)

    cnn_model = tf.keras.models.Model(inception_v3.input, output)
    return cnn_model

class ImageCaptioningModel(tf.keras.Model):

    def __init__(self, cnn_model, encoder, decoder, image_aug=None):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.image_aug = image_aug
        self.loss_tracker = tf.keras.metrics.Mean(name=&quot;loss&quot;)
        self.acc_tracker = tf.keras.metrics.Mean(name=&quot;accuracy&quot;)


    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)


    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)


    def compute_loss_and_acc(self, img_embed, captions, training=True):
        encoder_output = self.encoder(img_embed, training=True)
        y_input = captions[:, :-1]
        y_true = captions[:, 1:]
        mask = (y_true != 0)
        y_pred = self.decoder(
            y_input, encoder_output, training=True, mask=mask
        )
        loss = self.calculate_loss(y_true, y_pred, mask)
        acc = self.calculate_accuracy(y_true, y_pred, mask)
        return loss, acc


    def train_step(self, batch):
        imgs, captions = batch

        if self.image_aug:
            imgs = self.image_aug(imgs)

        img_embed = self.cnn_model(imgs)

        with tf.GradientTape() as tape:
            loss, acc = self.compute_loss_and_acc(
                img_embed, captions
            )

        train_vars = (
            self.encoder.trainable_variables + self.decoder.trainable_variables
        )
        grads = tape.gradient(loss, train_vars)
        self.optimizer.apply_gradients(zip(grads, train_vars))
        self.loss_tracker.update_state(loss)
        self.acc_tracker.update_state(acc)

        return {&quot;loss&quot;: self.loss_tracker.result(), &quot;acc&quot;: self.acc_tracker.result()}


    def test_step(self, batch):
        imgs, captions = batch

        img_embed = self.cnn_model(imgs)

        loss, acc = self.compute_loss_and_acc(
            img_embed, captions, training=False
        )

        self.loss_tracker.update_state(loss)
        self.acc_tracker.update_state(acc)

        return {&quot;loss&quot;: self.loss_tracker.result(), &quot;acc&quot;: self.acc_tracker.result()}

    @property
    def metrics(self):
        return [self.loss_tracker, self.acc_tracker]

cnn_model = CNN_Encoder_Incep()
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)
</code></pre>
<p>I tried doing this but i keep getting multiple errors when i try to pass this to the cnn_model variable.</p>
<pre><code>def CNN_Encoder():
    yolov8_model = tf.keras.models.load_model('./content/yolov8n_objdet_oidv7_640x640.pt')
    yolov8_model.trainable = False
    output = yolov8_model.output
    output = tf.keras.layers.Reshape((-1, output.shape[-1]))(output)
    cnn_model = tf.keras.models.Model(yolov8_model.input, output)
    cnn_model_onnx = cnn_model.export(format='onnx')
    return cnn_model
</code></pre>
","-1","Question"
"77901612","","<p>I am trying to fine tune wav2vec2 model with my dataset. For this reason I loaded audios. Now want to downsample them to 16kHz. But <code>librosa.reshape</code> function is giving an error which I couldn't resolve. The error message is:</p>
<blockquote>
<p>resample() takes 1 positional argument but 3 were given</p>
</blockquote>
<p>Firstly, I tried to load it with <code>librosa</code> with sampling rate 16kHz. But as I have less experience in this field, and I'm facing problem in the later part of my project because of this. I found a code which supposed to resample the audio signal. I tried to use it, but faced the above mentioned problem.</p>
<p>This part works fine:</p>
<pre class=""lang-py prettyprint-override""><code>database={}
audios = []
psr = []
for path in df['audio']:
  speech_array,sr = torchaudio.load(path)
  audios.append(speech_array[0].numpy())
  psr.append(sr)
database['audio'] = audios
database['psr'] = psr
</code></pre>
<p>And I get an error for every index:</p>
<pre class=""lang-py prettyprint-override""><code>import librosa
import numpy as np

# Assuming 'database' is your DataFrame containing 'audio' and 'psr' columns

# List to store new sampling rates
new_sr = []

# Resample each audio signal and store the new sampling rate
for i in range(len(database['psr'])):
    try:
        audio_signal = np.asarray(database['audio'][i])  # Convert audio to numpy array
        original_sr = database['psr'][i]  # Original sampling rate

        # Check if the audio signal is mono (single-channel)
        if audio_signal.ndim == 1:
            # Resample mono audio signal
            resampled_audio = librosa.resample(audio_signal, original_sr, 16000)
        else:
            # Resample each channel separately for multi-channel audio
            resampled_channels = []
            for channel in audio_signal:
                resampled_channel = librosa.resample(channel, original_sr, 16000)
                resampled_channels.append(resampled_channel)
            resampled_audio = np.array(resampled_channels)

        # Store resampled audio back in DataFrame
        database['audio'][i] = resampled_audio

        # Store new sampling rate (16000 Hz)
        new_sr.append(16000)
    except Exception as e:
        print(f&quot;Error processing audio at index {i}: {e}&quot;)

# Add new sampling rates to the DataFrame
database['newsr'] = new_sr
</code></pre>
","1","Question"
"77902803","","<p>let's say I have an input dataframe, rows are number of data, columns are 5 features: ft1, ft2,.....,ft5.</p>
<p>Then I have a simple binary classification neural network which the hidden layer has 2 neurons. something like this:
<a href=""https://i.sstatic.net/YomX7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YomX7.png"" alt=""simple representation"" /></a></p>
<p>Can I control the input that being fed into each neuron? like only ft1, 3, 5 to neuron 1 and only ft2, 4 to neuron 2?</p>
<p>The first thing I can think of is masking, but it seems not work like that, maybe I can tweak the weight? Any suggestions?</p>
","0","Question"
"77904323","","<p>I am having some trouble in reshaping my 3D data for training in the model.fit function for my 3DConv Neural Network using Tensorflow and Keras. I have added both code and console output for your information. I would really appreciate if anyone would highlight any mistakes and help me solve the issue.</p>
<p><strong>Code:</strong></p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import keras
from keras import layers
import tensorflow as tf

**# Used np.load to load the four .npy files.
# Two training data files and two validation data files. First is Original data and second is labeling data
# Normalization has already been applied.**

print(&quot;x_train Shape Before Expanding: &quot;, x_train.shape)
print(&quot;y_train Shape Before Expanding: &quot;, y_train.shape)  # Label Data
print(&quot;x_val Shape Before Expanding: &quot;, x_val.shape)
print(&quot;y_val Shape Before Expanding: &quot;, y_val.shape)      # Label Data

x_train = np.expand_dims(x_train, axis=-1)
x_val = np.expand_dims(x_val, axis=-1)
y_train = np.expand_dims(y_train, axis=-1)
y_val = np.expand_dims(y_val, axis=-1)

print(&quot;x_train Shape After Expanding: &quot;, x_train.shape)
print(&quot;y_train Shape After Expanding: &quot;, y_train.shape)
print(&quot;x_val Shape After Expanding: &quot;, x_val.shape)
print(&quot;y_val Shape After Expanding: &quot;, y_val.shape)

batch_size = 1

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))

train_dataset = train_dataset.batch(batch_size)
validation_dataset = validation_dataset.batch(batch_size)

# Print shapes after batching
print(&quot;\nShapes of batches in train_dataset:&quot;)
for batch in train_dataset.take(1):
    x_batch, y_batch = batch
    print(&quot;Input Batch Shape:&quot;, x_batch.shape)
    print(&quot;Output Batch Shape:&quot;, y_batch.shape)

print(&quot;\nShapes of batches in validation_dataset:&quot;)
for batch in validation_dataset.take(1):
    x_batch, y_batch = batch
    print(&quot;Input Batch Shape:&quot;, x_batch.shape)
    print(&quot;Output Batch Shape:&quot;, y_batch.shape)


# Model
inputs = keras.Input((401, 701, 255, 1))
x = layers.Conv3D(filters=64, kernel_size=3, activation=&quot;relu&quot;)(inputs)
x = layers.MaxPooling3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv3D(filters=64, kernel_size=3, activation=&quot;relu&quot;)(x)
x = layers.MaxPooling3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv3D(filters=128, kernel_size=3, activation=&quot;relu&quot;)(x)
x = layers.MaxPooling3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv3D(filters=256, kernel_size=3, activation=&quot;relu&quot;)(x)
x = layers.MaxPooling3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.GlobalAveragePooling3D()(x)
x = layers.Dense(units=512, activation=&quot;relu&quot;)(x)
x = layers.Dropout(0.3)(x)

outputs = layers.Dense(units=1, activation=&quot;sigmoid&quot;)(x)

model = keras.Model(inputs, outputs, name=&quot;MyModel&quot;)

print(model.summary())

initial_learning_rate = 0.0001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)

model.compile(
    loss=&quot;binary_crossentropy&quot;,
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
    metrics=[&quot;acc&quot;]
)

epochs = 3
model.fit(train_dataset,
          validation_data=validation_dataset,
          epochs=epochs,
          shuffle=True,
          verbose=2)
</code></pre>
<p><strong>Console Output:</strong></p>
<pre><code>x_train Shape Before Expanding: (401, 701, 255) 
y_train Shape Before Expanding: (401, 701, 255) 
x_val Shape Before Expanding: (200, 701, 255) 
y_val Shape Before Expanding: (200, 701, 255)

x_train Shape After Expanding: (401, 701, 255, 1) 
y_train Shape After Expanding: (401, 701, 255, 1) 
x_val Shape After Expanding: (200, 701, 255, 1) 
y_val Shape After Expanding: (200, 701, 255, 1)

Shapes of batches in train_dataset: 
Input Batch Shape: (1, 701, 255, 1) 
Output Batch Shape: (1, 701, 255, 1) 

Shapes of batches in validation_dataset: 
Input Batch Shape: (1, 701, 255, 1) 
Output Batch Shape: (1, 701, 255, 1)

Model: &quot;MyModel&quot;

input_1 (InputLayer) [(None, 401, 701, 255, 0
1)]

conv3d (Conv3D) (None, 399, 699, 253, 6 1792
4)

max_pooling3d (MaxPooling3 (None, 199, 349, 126, 6 0
D) 4)

batch_normalization (Batch (None, 199, 349, 126, 6 256
Normalization) 4)

conv3d_1 (Conv3D) (None, 197, 347, 124, 6 110656
4)

max_pooling3d_1 (MaxPoolin (None, 98, 173, 62, 64) 0
g3D)

batch_normalization_1 (Bat (None, 98, 173, 62, 64) 256
chNormalization)

conv3d_2 (Conv3D) (None, 96, 171, 60, 128 221312
)

max_pooling3d_2 (MaxPoolin (None, 48, 85, 30, 128) 0
g3D)

batch_normalization_2 (Bat (None, 48, 85, 30, 128) 512
chNormalization)

conv3d_3 (Conv3D) (None, 46, 83, 28, 256) 884992

max_pooling3d_3 (MaxPoolin (None, 23, 41, 14, 256) 0
g3D)

batch_normalization_3 (Bat (None, 23, 41, 14, 256) 1024
chNormalization)

global_average_pooling3d ( (None, 256) 0
GlobalAveragePooling3D)

dense (Dense) (None, 512) 131584

dropout (Dropout) (None, 512) 0

dense_1 (Dense) (None, 1) 513

================================================================= Total params: 1352897 (5.16 MB) Trainable params: 1351873 (5.16 MB) Non-trainable params: 1024 (4.00 KB)

None Epoch 1/3

**ValueError: Input 0 of layer &quot;MyModel&quot; is incompatible with the layer: expected shape=(None, 401, 701, 255, 1), found shape=(None, 701, 255, 1)**

</code></pre>
","0","Question"
"77906395","","<p>I am trying to create an ARIMA model to predict stock market values (experimenting with this, not going to step on it to use it real life) and export it in PNG format for all the 512 stocks i have in my dataset. In the PNG it will be shown the Actual and the predicted value.</p>
<p>Error:</p>
<pre><code>forecast, stderr, conf_int = model_fit.forecast(steps=len(X_test))   
ValueError: too many values to unpack (expected 3)
</code></pre>
<pre><code>import os
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Load Training and Test Data.
train_df = pd.read_csv(r'mypath\train.csv', parse_dates=['datetime'])
test_df = pd.read_csv(r'mypath\test.csv', parse_dates=['datetime'])

# Getting unique stock names.
stocks = train_df['Short Company Name'].unique()

# Create a directory to save pngs.
output_directory = r'ARIMAprediction'
os.makedirs(output_directory, exist_ok=True)

for stock in stocks:
    print(f&quot;Processing Stock: {stock}&quot;)

    # Filter data for the current stock.
    train_stock = train_df[train_df['Short Company Name'] == stock]
    test_stock = test_df[test_df['Short Company Name'] == stock]

    # Extracting features (for this, all the dataset is used).
    features = ['open', 'high', 'low', 'close', 'volume']
    X_train, y_train = train_stock[features], train_stock['close']
    X_test, y_test = test_stock[features], test_stock['close']

    # Fit ARIMA model
    order = (5, 1, 2)  
    model = ARIMA(y_train, order=order)
    model_fit = model.fit()

    # Prediction.
    forecast, stderr, conf_int = model_fit.forecast(steps=len(X_test))
</code></pre>
","-3","Question"
"77906649","","<p>Why both the embeddings are different even when i generate them using same BartForConditionalGenration model?</p>
<p>First embedding is generated by combining token embedding and positional embedding from</p>
<pre><code>embed_pos = modelBART.model.encoder.embed_positions(input_ids.input_ids)
inputs_embeds = modelBART.model.encoder.embed_tokens(input_ids.input_ids)
</code></pre>
<p>The Second embedding by the model via</p>
<pre><code>output = modelBART(input_ids.input_ids)
print(&quot;\n\n output: \n\n&quot;,output.encoder_last_hidden_state)
</code></pre>
<p>Shouldn't the embedding by first and second be same? What to do so that difference of the embedding from first and second be zero?</p>
","0","Question"
"77906886","","<p>I'm new to machine learning and scikit-learn. but tomorrow i have to submit a task to teacher, pls help. i need to extract features from all images in specified directory, then apply PCA method to this images and create scatter plot, which shows images distribution.</p>
<p>should i use libraries like scikit-learn to extract those features? how can i plot 2d scatter?</p>
<p>i already take a look at this article <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a> but still have not clear understanding.</p>
","-1","Question"
"77907033","","<p>I'm working on modifying a working Flask application I've been using to serve an ML model to now serve predictions from a new, updated model.</p>
<p>The model is an sklearn pipeline object I've trained and serialized using the Pickle library.
It includes several Column Transformer steps, Imputation, Encoding, and finally a Prediction step. With the way some of the custom Column Transformers are written, it's important that the output of each intermediate step is a Pandas Dataframe rather than an array, which is how the model was trained and serialized.</p>
<p>Here's where it starts to get weird:</p>
<ul>
<li>When I load and make a prediction with the Model in what I understand to be the &quot;application context,&quot; the Pipeline works as expected and returns a prediction.</li>
<li>When I use the <em>same</em> loaded Model with the same data in a request context, the intermediate Pipeline steps <em>don't</em> return Pandas outputs, causing the Pipeline to fail once the second step is reached and looks for column names that don't exist in the array output.</li>
</ul>
<p>My questions are:</p>
<ol>
<li><em>Why</em> is this behavior different in each of these two contexts?</li>
<li>How can I solve this problem without going back to reconfigure the pipeline to use arrays instead and retraining--which I don't want to do for several reasons.</li>
<li>Is there a way I can configure the setting &quot;sklearn.set_config(transform_output=&quot;pandas&quot;)&quot; just in the request context?</li>
</ol>
<p>Here's my basic code:</p>
<pre><code>    import os
    import pickle
    import pandas as pd
    from flask import (Flask, redirect, make_response)
    import logging
    
    #define app
    app = Flask(__name__)
    
    # load the trained Model
    model = pickle.load(open(&quot;model.pkl&quot;), &quot;rb&quot;)
    
    # load test record 
    test_record = pd.read_csv(&quot;test_record.csv&quot;)
    
    # Make a prediction using the model and test record. This step works.
    try:
        model.predict(test_record)
    except Exception as e: 
        log.debug(str(e), stack_info=True)
    
    
    @app.route('/predict')
    def predict():
        # Make a prediction using the model and test record. This step *doesn't* work.
        try:
            prediction = model.predict(test_record)
        except Exception as e: 
            log.debug(str(e), stack_info=True)
     
        return make_response('Test Record Prediction: ' + str(prediction),200)
    
    # Start the Flask app
    if __name__ == '__main__':
        if os.environ['ENV'] in {'local','local_w_db','DEV'}:
            app.run(debug=True)
        else:
            app.run()
</code></pre>
<p><strong>Environment Specs:</strong></p>
<pre><code>python==3.11.7
Flask==3.0.1
scikit-learn==1.3.2
scikit-learn-intelex==2023.2.1
scipy==1.11.4
pandas==2.1.4
category_encoders==2.6.3
werkzeug==3.0.1
joblib==1.2.0
</code></pre>
<p><strong>I've tried:</strong></p>
<ol>
<li>Running the model both within and outside of the request context.</li>
</ol>
<ul>
<li>Inside the request context, I've traced the execution of the predict step far enough to see that the output of the first ColumnTransformer is an array, where it needs to be a Pandas df.</li>
<li>Outside of the request context, the outputs of all ColumnTransformers and intermediate Pipeline steps is &quot;pandas,&quot; how the model was configured.</li>
</ul>
<ol start=""2"">
<li><p>Setting the sklearn transform output to always be pandas by running these lines at the beginning of my flask app code, which didn't make a difference:</p>
<pre><code>import sklearn
sklearn.set_config(transform_output=&quot;pandas&quot;)
</code></pre>
</li>
</ol>
<p><strong>Here is the stack trace</strong> that is returned with the Exception on the prediction step, which points to an issue in the threading configurations rather than the sklearn settings:</p>
<pre><code>[2024-01-30 08:58:44,666] DEBUG [app.predict:109] - Specifying the columns using strings is only supported for pandas DataFrames
    Stack (most recent call last):
    File &quot;c:\Users\user\.vscode\extensions\ms-python.python-2023.22.1\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydev_bundle\pydev_monkey.py&quot;, line 1118, in __call__
        ret = self.original_func(*self.args, **self.kwargs)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\threading.py&quot;, line 1002, in _bootstrap
        self._bootstrap_inner()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\threading.py&quot;, line 1045, in _bootstrap_inner
        self.run()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\threading.py&quot;, line 982, in run
        self._target(*self._args, **self._kwargs)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\socketserver.py&quot;, line 691, in process_request_thread
        self.finish_request(request, client_address)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\socketserver.py&quot;, line 361, in finish_request
        self.RequestHandlerClass(request, client_address, self)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\socketserver.py&quot;, line 755, in __init__
        self.handle()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\werkzeug\serving.py&quot;, line 390, in handle
        super().handle()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\http\server.py&quot;, line 436, in handle
        self.handle_one_request()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\http\server.py&quot;, line 424, in handle_one_request
        method()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\werkzeug\serving.py&quot;, line 362, in run_wsgi
        execute(self.server.app)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\werkzeug\serving.py&quot;, line 323, in execute
        application_iter = app(environ, start_response)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\flask\app.py&quot;, line 1488, in __call__
        return self.wsgi_app(environ, start_response)
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\flask\app.py&quot;, line 1463, in wsgi_app
        response = self.full_dispatch_request()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\flask\app.py&quot;, line 870, in full_dispatch_request
        rv = self.dispatch_request()
    File &quot;..\miniforge3\envs\APP_ENV\Lib\site-packages\flask\app.py&quot;, line 855, in dispatch_request
        return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)    # type: ignore[no-any-return]
    File &quot;C:\Users\user\app_directory\app.py&quot;, line 109, in predict
        log.debug(str(e), stack_info=True)
</code></pre>
","1","Question"
"77907561","","<p>I am working on an OCR-task and for evaluation purposes want to calculate a confusion matrix for my model. I want it to basically show how often a character is predicted correctly and how often it is predicted as other characters (and which ones!).</p>
<p>My problem currently is, that a simple pair-wise comparison is difficult due to string-size mismatches and/or additional/missing characters (mainly whitespaces). I was thinking about adding the information about how often a character would need to be inserted/deleted using the Levenshtein distance calculation algorithm, but I'm still unsure on how to handle that.</p>
<p>Are there any state-of-the-art approaches that are commonly used for this? I did some research, but couldn't find anything significant.</p>
","0","Question"
"77909132","","<p>I try to make CNN-LSTM model with Kvasir dataset. I split my dataset with <code>image_dataset_from_directory</code> as foloowing:</p>
<pre><code>dataset_path = &quot;/kaggle/working/dataset&quot;
image_size = 224, 224
batch_size = 64

train_ds = image_dataset_from_directory(
  dataset_path,
  validation_split=0.2,
  subset=&quot;training&quot;,
  label_mode=&quot;categorical&quot;,
  seed=23,
  image_size=image_size,
  color_mode =&quot;rgb&quot;,
  batch_size=batch_size)


val_ds = image_dataset_from_directory(
  dataset_path,
  validation_split=0.2,
  subset=&quot;validation&quot;,
  label_mode=&quot;categorical&quot;,
  seed=23,
  image_size=image_size,
  color_mode=&quot;rgb&quot;,
  batch_size=batch_size)
</code></pre>
<p>And this function gave me a BatchDataset. Then I make Cardinality as following:</p>
<pre><code>val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)
</code></pre>
<p>And then</p>
<pre><code>AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)
</code></pre>
<p>And also this code gave me a Prefetch Dataset. It give when I run <code>print(train_ds)</code> :</p>
<pre><code>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf.float32, name=None))&gt; 
</code></pre>
<p>Then I added my model,</p>
<pre><code>    model = tf.keras.models.Sequential([
    # Convolutional layers with batch normalization and max pooling
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), activation=None,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), activation=None)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # Flatten the output and add Dense layers
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,activation='tanh'),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # Output layer with 8 nodes for classification
    tf.keras.layers.Dense(8, activation='softmax')
])

# Compile the model

model.compile(optimizer=AdamW(learning_rate=0.001, weight_decay=0.004, beta_1=0.9, beta_2=0.999, epsilon=1e-8),
          loss=CategoricalCrossentropy(),
          metrics=['accuracy'])
</code></pre>
<p>It didn't run when I fit this model and I got an error:</p>
<pre><code>ValueError: Exception encountered when calling layer 'time_distributed_4' (type TimeDistributed).
    
    Input 0 of layer &quot;conv2d_2&quot; is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, 224, 3)
    
    Call arguments received by layer 'time_distributed_4' (type TimeDistributed):
      • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)
      • training=True
      • mask=None
</code></pre>
<p>I don't know how to overcome this problem, can you help me?</p>
","0","Question"
"77912045","","<p>I am trying to do time series forecasting on a bunch of classes and date time but my graph looks like this for some reason my full code is below:</p>
<pre><code>from google.colab import drive
drive.mount('/content/gdrive', force_remount = True)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

data = pd.read_csv('gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv', parse_dates=['time_date'], index_col='time_date')
class_id = data['class_id']
time_date = data.index.date
data['date'] = data.index.date

class_id = data['class_id']
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data['count'] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform('size').gt(1)]

!pip install pandas-datareader

import pandas_datareader.data as web
import datetime

import pandas as pd 
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

plt.ylabel('Amount of classes')
plt.xlabel('Date')
plt.xticks(rotation=45)

out.index = pd.to_datetime(out['date'], format='%Y-%m-%d')
plt.plot(out.index, out['count'], )

</code></pre>
<p><a href=""https://i.sstatic.net/RfqCm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RfqCm.png"" alt=""results of plt"" /></a></p>
<p>while the <a href=""https://builtin.com/data-science/time-series-forecasting-python"" rel=""nofollow noreferrer"">blog</a> where I got this time series code from has this kind of result</p>
<p><a href=""https://i.sstatic.net/G1mdF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G1mdF.png"" alt=""enter image description here"" /></a></p>
<p>So I'm not sure if I should proceed or not XD</p>
<p>my input data is this:<br/>
timestamp / class_id<br/>
2021-09-27 06:00:00 / A<br/>
2021-09-27 03:00:00 / A<br/>
2021-09-27 01:00:00 / A<br/>
2021-09-27 08:29:00 / C<br/>
2021-05-23 08:08:49 / B<br/>
2021-05-23 03:21:49 / B<br/>
2021-05-23 01:22:11 / C<br/></p>
<p>after processing it and adding count and date columns:<br/>
count / timestamp / class_id / date<br/>
1 / 2021-09-27 06:00:00 / A / 2021-09-27<br/>
2 / 2021-09-27 03:00:00 / A / 2021-09-27<br/>
3 / 2021-09-27 01:00:00 / A / 2021-09-27<br/>
1 / 2021-09-27 08:29:00 / C / 2021-09-27<br/>
1 / 2021-05-23 08:08:49 / B / 2021-05-23<br/>
2 / 2021-05-23 03:21:49 / B / 2021-05-23<br/>
1 / 2021-05-23 01:22:11 / C / 2021-05-23<br/></p>
<p>I tried a code below but for some reason the first graph is empty</p>
<pre><code>plt.ylabel('Amount of classes')
plt.xlabel('date')
plt.xticks(rotation=45)

out.index = pd.to_datetime(out['date'], format='%Y-%m-%d')
out.groupby('class_id').plot()
plt.plot(out.index, out['count'], )
</code></pre>
<p><a href=""https://i.sstatic.net/veJz0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/veJz0.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"77918581","","<p>I have tried to predicted the household consumption power task with the LSTM model in Pytorch, I have tried to add the normalization through the MinMaxScaler(), and tries to to reshape it to the desire shape for the input as [batch_size, sequence_length, num_features], I have 3 features in this case I and have set the sequence length equals to 1 as well, the problem is that when I train the model, the train loss and test loss is relatively low, although the model is poorly plotting the straight line doesn't fit anything. Here is the code and 1 test for the training data</p>
<pre><code>training_year = [2006, 2007, 2008]
testing_year = [2009, 2010]
training_df = copy.deepcopy(machine_learning_df.loc[machine_learning_df['year'].isin(training_year), :])
testing_df = copy.deepcopy(machine_learning_df.loc[machine_learning_df['year'].isin(testing_year), :])
features_list = ['date_ordinal', 'global_intensity', 'sub_metering3']
targeted_variable = 'global_active_power'
x_train = training_df[features_list].astype(int).values
y_train = training_df[targeted_variable].astype(int).values
x_test = testing_df[features_list].astype(int).values
y_test = testing_df[targeted_variable].astype(int).values

scaler = MinMaxScaler()

BATCH_SIZE = 32

x_train = scaler.fit_transform(x_train.reshape(-1, 3))
x_test = scaler.fit_transform(x_test.reshape(-1, 3))

x_train = torch.tensor(x_train).view(-1, 1, 3).type(torch.float32)
y_train = torch.tensor(y_train).type(torch.float32)
x_test = torch.tensor(x_test).view(-1, 1, 3).type(torch.float32)
y_test = torch.tensor(y_test).type(torch.float32)

train_dataset = TensorDataset(x_train, y_train)
train_dataloader = DataLoader(dataset=train_dataset, 
                              shuffle=True,
                              batch_size=BATCH_SIZE)
test_dataset = TensorDataset(x_test, y_test)
test_dataloader = DataLoader(dataset=test_dataset,
                             shuffle=False,
                             batch_size=BATCH_SIZE)

#Create the neural network 
class LSTMmodel(nn.Module): 
    def __init__(self,
                 output_size, 
                 input_size,
                 hidden_size,
                 num_layers): 
        super(LSTMmodel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.stacked_layer = nn.Sequential(
            nn.Linear(in_features=hidden_size, out_features=512), 
            nn.ReLU(), 
            nn.Dropout(p=0.2), 
            nn.Linear(in_features=512, out_features=256), 
            nn.ReLU(),
            nn.Dropout(p=0.2), 
            nn.Linear(in_features=256, out_features=125), 
            nn.ReLU(), 
            nn.Dropout(p=0.2),
            nn.Linear(in_features=125, out_features=64), 
            nn.ReLU(),
            nn.Dropout(p=0.2), 
            nn.Linear(in_features=64, out_features=output_size), 
            nn.ReLU()
        )
    
    def forward(self, x): 
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        out = self.stacked_layer(out)
        return out 

#Initialize the model 
model = LSTMmodel(input_size=3, output_size=1, hidden_size=16, num_layers=3)
loss_fn = nn.MSELoss() 
learning_rate = 0.1
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#Create the traning and testing model 
clip_value = 1
epoches = 1
for epoch in range(epoches):
    print(f'Epoch {epoch + 1}')
    model.train() 
    for batch, (train_features, train_label) in enumerate(train_dataloader): 
        train_pred = model(train_features)
        train_loss = loss_fn(train_pred, train_label.unsqueeze(dim=1))
        train_loss.backward() 
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)
        optimizer.step() 
        optimizer.zero_grad() 
        if batch % 4000 == 0: 
            print(f'Looked at {batch * len(train_features)}/{len(train_dataloader.dataset)} samples')
        
    model.eval() 
    total_test_loss = 0
    test_feature_list = []
    test_pred_list = []
    with torch.inference_mode(): 
        for test_feature, test_labels in test_dataloader: 
            test_pred = model(test_feature)
            test_loss = loss_fn(test_pred, test_labels.unsqueeze(dim=1))
            test_pred_list.extend(test_pred.detach().numpy())
            test_feature_list.extend(test_labels.detach().numpy())
            total_test_loss += test_loss 
        print(f'Train loss {train_loss.item():.4f} | Test loss {test_loss.item():.4f} | Total test loss {total_test_loss}')

test_feature_array = np.array(test_feature_list)
test_pred_array = np.array(test_pred_list)
predicted_df = pd.DataFrame({
    'date_ordinal': testing_df['date_ordinal'],
    'global_active_power': test_pred_array.flatten()
})

def plotting_predicted(training_data, testing_data, predicted_data=None): 
    plt.figure(figsize=(12, 6))
    #Plot the training data 
    sns.lineplot(data=training_data, x='date_ordinal', y='global_active_power', label='Training data')

    #Plot the testing data
    sns.lineplot(data=testing_data, x='date_ordinal', y='global_active_power', label='Testing data')

    if predicted_data is not None: 
        sns.lineplot(data=predicted_data, x='date_ordinal', y='global_active_power', label='Predicted data')

    plt.xlabel('Date')
    plt.xticks(rotation='vertical')
    plt.ylabel('Total consumptions')
    plt.legend()
    plt.show()
plotting_predicted(training_df, testing_df, predicted_df)
</code></pre>
<pre><code>Epoch 1
Looked at 0/1070566 samples
Looked at 128000/1070566 samples
Looked at 256000/1070566 samples
Looked at 384000/1070566 samples
Looked at 512000/1070566 samples
Looked at 640000/1070566 samples
Looked at 768000/1070566 samples
Looked at 896000/1070566 samples
Looked at 1024000/1070566 samples
Train loss 1.0000 | Test loss 0.0769 | Total test loss 39069.015625
</code></pre>
<p><a href=""https://i.sstatic.net/vZqrh.png"" rel=""nofollow noreferrer"">Image of the predicted data</a></p>
<p>I have tried to reshape the size to either (-1, 1, 3) or (-1, 3, 1), I'm not pretty sure which part in my understanding has caused the problem for the prediction</p>
","0","Question"
"77919632","","<p>I know that <code>torch.argmax(x, dim = 0)</code> returns the index of the first maximum value in <code>x</code> along dimension <code>0</code>. But is there an efficient way to return the indexes of the first <code>n</code> maximum values? If there are duplicate values I also want the index of those among the <code>n</code> indexes.</p>
<p>As a concrete example, say <code>x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])</code>. I would like a function</p>
<pre><code>generalized_argmax(xI torch.tensor, n: int)
</code></pre>
<p>such that
<code>generalized_argmax(x, 4)</code>
returns <code>[0, 2, 4, 5]</code> in this example.</p>
","2","Question"
"77921360","","<p>I'm trying to use the image_classifier from mediapipe_model_maker to train a custom tflite image classification model. All I see for data augmentation is a boolean that can be passed in the options called &quot;do_data_augmentation&quot; that does random augmentation (cropping, flipping, etc.).</p>
<pre><code>spec = image_classifier.SupportedModels.MOBILENET_V2
hparams=image_classifier.HParams(epochs=100, export_dir=&quot;exported_model_2&quot;,do_data_augmentation=False)
options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)

model = image_classifier.ImageClassifier.create(
    train_data = train_data,
    validation_data = validation_data,
    options=options,
)
</code></pre>
<p>I want to be specific about the data augmentation and keep only flipping, exposure and blur. Is that possible? Thanks.</p>
","1","Question"
"77922080","","<p>I'm new to all this. Can someone please tell me what's going on? So how can I fix it?</p>
<p><a href=""https://i.sstatic.net/wXJsL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Basically, I want to use tf, but I'm stuck. Because I've never used it before.
I would like to use it with my detection model.</p>
<p>Thank you everyone for all your help.
With respect, Good luck to all of you.</p>
","0","Question"
"77923033","","<p>I have followed this example for setting up an Elasticsearch cluster for image similarity search: <a href=""https://github.com/radoondas/flask-elastic-image-search"" rel=""nofollow noreferrer"">https://github.com/radoondas/flask-elastic-image-search</a></p>
<p>Not being familiar with elasticsearch, I blindly followed this example which includes have an ML node with a pretrained model. It works well. However, I suspect that we are not actually using the ML node.</p>
<p>I extract the dense vectors in my application and then index them, and I also extract the vectors in my application when I query. I do not use elasticsearch to extract the dense vectors.</p>
<p>Is there any &quot;magic&quot; that makes elasticsearch use the pretrained model behind the scenes when I index a dense vector or perform a KNN query? Or is the pretrained model and ML node all extra stuff we don't need in our implementation?</p>
","0","Question"
"77929619","","<p>Here is the code</p>
<pre><code>import os 

import streamlit as st 
from dotenv import load_dotenv 
import google.generativeai as gen_ai  

load_dotenv() 

st.set_page_config(
    page_title=&quot;Chat with Gemini Pro&quot;,
    page_icon=&quot;:brain:&quot;,
    layout=&quot;centered&quot; 
)

GOOGLE_API_KEY = os.getenv(&quot;GOOGLE_API_KEY&quot;) 
gen_ai.configure(api_key=GOOGLE_API_KEY) 
model = gen_ai.GenerativeModel(&quot;gemini-pro&quot;) 

def translate_role_for_streamlit(user_role): 
  if user_role == &quot;model&quot;: 
    return &quot;assistant&quot; 
  else: 
    return user_role 

if &quot;chat_session&quot; not in st.session_state:
  st.session_state.chat_session = model.start_chat(history=[])
st.title(&quot;CAIE Bot&quot;) 

for message in st.session_state.chat_session.history:
  with st.chat_message(translate_role_for_streamlit(message.role)): 
    st.markdown(message.parts[0].text) 

user_prompt = st.chat_input(&quot;Ask CAIE bot...&quot;) 
if user_prompt: 
  st.chat_message(&quot;user&quot;).markdown(user_prompt) 
  gemini_response = st.session_state.chat_session.send_message(user_prompt) 
  with st.chat_message(&quot;assistant&quot;): 
    st.markdown(gemini_response.text)  

!streamlit run main.py
</code></pre>
<p>I'm getting error in line</p>
<pre><code>if &quot;chat_session&quot; not in st.session_state:
</code></pre>
<p>I wanted to check if user had an active chat with the bot and if it did, then the bot would save it previous conversation history for the next conversation. And after a session is over, history would then reset</p>
","0","Question"
"77930819","","<p>I am calling Google Gemini-Pro API in a consecutive manner (like about 50 queries per minute). I believe I have properly set my VertexAI project and credentials. When the number of consecutive queries I used was below a constant bar, the queries would run through and the responses would be received just fine. However, once the number of queries increased over the aforementioned bar, the following error would show up:</p>
<blockquote>
<p>IndexError - list index out of range</p>
</blockquote>
<p>Note that the number of queries &quot;bar&quot; over which this error will occur depend on the length of each query and is consistent if the length of the queries stays the same across program executions. For example, after trying to increase my query length by rougly 20%, the bar dropped from roughly 330 queries to roughly 60 queries.</p>
<blockquote>
<p>File
&quot;/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py&quot;,
line 1315, in text
return self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError: list index out of range</p>
</blockquote>
<p>What is causing this? I have set the VertexAI server location to be: &quot;us-central1&quot;, which should only have a quota of 300 queries/minute as far as I know. Since I am doing the API call consecutively but below the rate of 60 queries/minute, I think I am in the ok zone for usage. I am currently using the free VertexAI trial account (with 300 USD free credit).</p>
<p>The Gemini Pro API Call function that I have written is:</p>
<pre><code>def gemini_response(message: str) -&gt; str:
    # Initialize Vertex AI
    vertexai.init(project=&quot;project-id-0123&quot;, location=&quot;us-central1&quot;)

    # Load the model
    model = GenerativeModel(&quot;gemini-pro&quot;)

    # Query the model
    response = model.generate_content(message)
    return response.text
</code></pre>
<p>When debugging what's wrong with the <code>candidates</code> variable, the variable inspection results look like:</p>
<pre><code>&gt; self 
&gt; prompt_feedback {block_reason: OTHER} 
&gt; usage_metadata {prompt_token_count: 505   total_token_count: 505 } 

&gt; self.candidates 
&gt; []

&gt; self._raw_response 
&gt; prompt_feedback {block_reason: OTHER}
&gt; usage_metadata {prompt_token_count: 505   total_token_count: 505 }
</code></pre>
","3","Question"
"77930973","","<pre><code># Import the required modules

from anomalib.data import MVTec
from anomalib.models import Patchcore
from anomalib.engine import Engine
</code></pre>
<p>error:</p>
<pre><code>ModuleNotFoundError: No module named 'anomalib.engine'
</code></pre>
<p>I'm trying to run this.... have followed the library installation and seen the
<a href=""https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html"" rel=""nofollow noreferrer"">https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html</a>
I think its either becuase the engine has been modified or it has been dropped by the library...</p>
<p>How to resolve this?</p>
","0","Question"
"77931021","","<p>When i run the last part to train model, i am not able to it is getting Intrupted Check the image for the error. As you see i want to detect whether i am able to train my model to detect the animal e.g. between Cat and Dog. <a href=""https://drive.google.com/drive/folders/11T76B8UkTg9lU-sPhPlWqn6MOVhQ-FjS"" rel=""nofollow noreferrer"">Dataset.</a></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

# Initiallising the CNN
classifier = Sequential()

#Convolution
classifier.add(Conv2D(32,(3,3), input_shape = (64, 64, 3), activation = 'relu'))

# Pooling
classifier.add(MaxPooling2D(pool_size = (2,2)))

# Adding a second convolutional layer
classifier.add(Conv2D(32, (3,3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2,2)))

# Flattening
classifier.add(Flatten())

# Full connection 
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 1, activation = 'sigmoid'))

# Compiling the CNN
classifier.compile(optimizer = 'adam', loss= 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the CNN to the images

from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255,
                                  shear_range = 0.2,
                                  zoom_range = 0.2,
                                  horizontal_flip = True)

training_set = train_datagen.flow_from_directory(r&quot;D:\Neural Network\Neural Network Full Course-20240203T042209Z-001\Neural Network Full Course - Copy\Neural Network\training_set&quot;,
                                                target_size= (64,64),
                                                batch_size = 32,
                                                class_mode = 'categorical')

test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory(r&quot;D:\Neural Network\Neural Network Full Course-20240203T042209Z-001\Neural Network Full Course - Copy\Neural Network\test_set&quot;,
                                                target_size= (64,64),
                                                batch_size = 32,
                                                class_mode = 'categorical')

classifier.fit(training_set,
               steps_per_epoch=700,
               epochs=10,
               validation_data=test_set,
               validation_steps=10)
train_datagen = ImageDataGenerator(rescale = 1./255,
                                  shear_range = 0.2,
                                  zoom_range = 0.2,
                                  horizontal_flip = True)

training_set = train_datagen.flow_from_directory(r&quot;D:\Neural Network\Neural Network Full Course-20240203T042209Z-001\Neural Network Full Course - Copy\Neural Network\training_set&quot;,
                                                target_size= (64,64),
                                                batch_size = 32,
                                                class_mode = 'categorical')

test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory(r&quot;D:\Neural Network\Neural Network Full Course-20240203T042209Z-001\Neural Network Full Course - Copy\Neural Network\test_set&quot;,
                                                target_size= (64,64),
                                                batch_size = 32,
                                                class_mode = 'categorical')

##### Here i am getting error
classifier.fit(training_set,
               steps_per_epoch=700,
               epochs=10,
               validation_data=test_set,
               validation_steps=10)
</code></pre>
<p>Error:</p>
<p><img src=""https://i.sstatic.net/I0QK0.jpg"" alt=""Image showing error"" /></p>
","-1","Question"
"77932307","","<p>I have been looking into linfa for Machine Learning in Rust, specifically the Linear Regression model. I want to be able to save and load my trained Linear Regressiom model however I'm unable to find a way to do this.</p>
<h2><strong>Approach 1:</strong></h2>
<p>So far my approach has been to obtain the main parameters that are involved in training that I can obtain from linfa's implementation of Linear Regression, and store them in a struct that I can store as a JSON file (done through serde_json). After this however I can't figure out how to load it back up for training.</p>
<p>Details of the above are given as follows:</p>
<p>Struct to store trained parameters:</p>
<pre><code>struct ModelJson {
    coefficients: Vec&lt;f64&gt;,
    intercept: f64,
}
</code></pre>
<p>The storing procedure:</p>
<pre><code>let model = lin_reg.fit(&amp;dataset)?;
let model_json = ModelJson {
    coefficients: model.params().to_vec(),
    intercept: model.intercept(),
};
</code></pre>
<p>How the stored data looks:</p>
<pre><code>{&quot;coefficients&quot;:[-0.00017907873576254802,-0.00100659702068151,-0.0008275037845519519,0.0004613216043979551,0.0010300634934599436],&quot;intercept&quot;:50.525680622870084}
</code></pre>
<h2><strong>Approach 2:</strong></h2>
<p>With respect to serializing and deserializing the whole model I found the following information stating that there is support for the same in linfa.
<a href=""https://github.com/rust-ml/linfa/issues/67"" rel=""nofollow noreferrer"">Loading and Saving Models</a></p>
<p>This leads us to my second approach wherein I used the serde feature of linfa-linear (containing the LinearRegression model), by including the following in my Cargo.toml first:</p>
<p><code>linfa-linear = {version=&quot;0.7.0&quot;, features=[&quot;serde&quot;]}</code></p>
<p>This feature implements the following for LinearRegression as per my understanding of the implementation:
<a href=""https://serde.rs/derive.html"" rel=""nofollow noreferrer"">Serde Serialize and Deserialize Implementations - derive</a></p>
<p>Aforementioned Implementation:</p>
<pre><code>#[cfg_attr(
    feature = &quot;serde&quot;,
    derive(Serialize, Deserialize),
    serde(crate = &quot;serde_crate&quot;)
)]
/// A fitted linear regression model which can be used for making predictions.
pub struct FittedLinearRegression&lt;F&gt; {
    intercept: F,
    params: Array1&lt;F&gt;,
}
</code></pre>
<p>Found in: <a href=""https://github.com/rust-ml/linfa/blob/4e40ce6375249bb823bc71de067477b472ae4c01/algorithms/linfa-linear/src/ols.rs#L63"" rel=""nofollow noreferrer"">linfa-linear derive implementation</a></p>
<p>Which I implemented as follows:</p>
<pre><code>let model = lin_reg.fit(&amp;dataset)?;
let serialized = serde_json::to_string(&amp;model).unwrap();
</code></pre>
<p>This method however gave the following error:</p>
<pre><code>the trait bound `FittedLinearRegression&lt;f64&gt;: serde::ser::Serialize` is not satisfied
the following other types implement trait `serde::ser::Serialize`:
  bool
  char
  isize
  i8
  i16
  i32
  i64
  i128
and 133 othersrustcClick for full compiler diagnostic
main.rs(82, 22): required by a bound introduced by this call
</code></pre>
<p>Is there another way to do this or is there some way I could make one of these approaches work?</p>
","1","Question"
"77933401","","<p>Here my code:</p>
<pre><code># AnyNan values in the target column or in my dataset
training_data.dropna(inplace=True, axis=0)
testing_data.dropna(inplace=True, axis=0)

# Perform one hot encoding on HomePlanet, 

features = ['HomePlanet', 'Destination', 'CryoSleep', 'VIP' ]
X= pd.get_dummies(training_data[features]).astype(int)
y = pd.get_dummies(training_data.Transported).astype(int)
x_test = testing_data[features]

# Creating my model

X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.6, test_size=0.4, random_state=42)
rt_model = RandomForestRegressor()
rt_model.fit(X_train,y_train)
predictions = rt_model.predict(X_test)

#save the csv

output = pd.DataFrame({'PassengerId': testing_data.PassengerId, 'Transported': predictions})
output.to_csv('submission.csv', index=False)
print(&quot;Your submission was successfully saved!&quot;)
</code></pre>
<p>When I print the length of <code>X</code> , <code>y</code>  and <code>X_train</code>, <code>y_train</code> after train-test split I get:</p>
<pre><code>6606 6606
3963 3963
2643 2643
</code></pre>
<p>I tried reshaping X and y.</p>
<p>I tried performing one hot Encoding on my <code>x_test dataframe</code>.</p>
<p>I did the <code>iloc</code> method on my array.</p>
<p>The problem only comes from the last part trying to save it as a <code>csv</code>.</p>
","-1","Question"
"77933640","","<p>The table from this <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">paper</a> that explains various approaches to obtain the embedding, I think these approaches are also applicable to Roberta too:</p>
<p><a href=""https://i.sstatic.net/C7rQY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C7rQY.png"" alt=""enter image description here"" /></a></p>
<p>I'm trying to calculate the weighted sum of last 4 hidden layers using Roberta to obtain token embedding, but I don't know if this is the correct way to do, this is the code I have tried:</p>
<pre><code>from transformers import RobertaTokenizer, RobertaModel
import torch

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=True)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=True)

states = output.hidden_states
token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()
</code></pre>
","2","Question"
"77934889","","<p>Cannot find any WEKA package for CAIM discretization on web search. I need the package for WEKA v3.</p>
<p>Searched google for WEKA package but didnot find any, though <a href=""https://waikato.github.io/weka-wiki/packages/unofficial/"" rel=""nofollow noreferrer"">some docs</a> say it exists.</p>
<p>Can anyone provide a working link of CAIM package for WEKA?</p>
","0","Question"
"77936741","","<p>I am trying to plot arrows from each data point towards the line in the graph using matplotlib.</p>
<p><a href=""https://i.sstatic.net/sCkRn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sCkRn.png"" alt=""Graph"" /></a></p>
<p>I want the arrow to represent the distance between each point and the line. How can I do this?</p>
<p>Here's my code:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np

# Create a straight line (45-degree angle)
x_line = np.linspace(0, 10, 100)
y_line = x_line

# Add some random points around the line
num_points = 20
x_points = np.linspace(2, 8, num_points)  # Adjust the range as needed
y_points = x_points + np.random.normal(0, 0.5, num_points)  # Add some randomness

# Plot the line
plt.plot(x_line, y_line, label='Line', color='blue')

# Plot the points
plt.scatter(x_points, y_points, label='Points', color='red')

# Set labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot Around a Line')

# Show legend
plt.legend()

# Display the plot
plt.show()
</code></pre>
<p>I tried doing this myself but failed:
<a href=""https://i.sstatic.net/bTxIP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bTxIP.png"" alt=""enter image description here"" /></a></p>
<p>Code:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np

# Create a straight line (45-degree angle)
x_line = np.linspace(0, 10, 100)
y_line = x_line

# Add some random points around the line
num_points = 20
x_points = np.linspace(2, 8, num_points)  # Adjust the range as needed
y_points = x_points + np.random.normal(0, 0.5, num_points)  # Add some randomness

# Plot the line
plt.plot(x_line, y_line, label='Line', color='blue')

# Plot the points
plt.scatter(x_points, y_points, label='Points', color='red')

# Add arrows from each point to the line
for x, y in zip(x_points, y_points):
    plt.arrow(x, y, 0, y - x, color='black', linestyle='dashed', linewidth=0.5, head_width=0.2)

# Set labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot Around a Line')

# Show legend
plt.legend()

# Display the plot
plt.show()
</code></pre>
<p>As you can see the data points shifted and the arrows point outwards rather than inwards or towards the line.</p>
","0","Question"
"77937339","","<p>I have a headache and following code, that doesn't let me sleep at night:</p>
<pre><code>1 import torch
  2 import torch.nn as nn
  3 import torch.optim as optim
  4 import numpy as np
  6 samples = torch.linspace(0, 100,100) # GENERATE THE SET
  7 train_split = int(len(samples)*0.8)
  8 x_train, x_test = samples[:train_split], samples[train_split:]
  9 y_labels = 2*samples-4 # define the function
 10 y_labels += torch.tensor(np.random.normal(0, 5, len(samples))) # ADD NOISE
 13 class NeuralNetwork(nn.Module):
 14     def __init__(self):
 15         super().__init__()
 16         self.fc1 = nn.Linear(1, 1)
 17     def forward(self, x):
 18         return self.fc1(x)
 19 model = NeuralNetwork()
 20 loss_func = nn.MSELoss()
 21 optimizer = optim.Adam(model.parameters(), lr=0.001)
 22 num_epochs = 50
 23 for epoch in range(num_epochs):
 24     for inputs, labels in zip(x_train, y_labels[:train_split]):                                                                                                                                         
 25      
 26         y_pred = model(inputs) # File &quot;.../torch/nn/modules/linear.py&quot;, line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: ArrayRef: invalid index Index = 18446744073709551615; Length = 0
 27         loss = loss_func(y_pred, labels)
 28         optimizer.zero_grad()
 29         loss.backward()
 30         optimizer.step()
 31         print(&quot;Epoch %d  - loss: %.4f%&quot; % (epoch, loss))


</code></pre>
<p>It is a simple one layer thing, and I'm doing it intiuitevly so no need to call me a goof.
I run it on couple machines, no difference...</p>
<p>P.S. ANY SUGGESTIONS ON FURTHER IMPROVEMENTS IN THE PROCESS OF WRITING THOSE BEASTS WOULD BE APPRECIATED!!!</p>
","1","Question"
"77938129","","<p>I am trying to build a RNN from scratch, with the help of this repository (<a href=""https://github.com/nicklashansen/rnn_lstm_from_scratch/tree/master"" rel=""nofollow noreferrer"">https://github.com/nicklashansen/rnn_lstm_from_scratch/tree/master</a>), but the training loss after each epoch stays the same. The code for the training loop is as follows:</p>
<pre><code># Hyper-parameters
num_epochs = 1000

# Initialize a new network
params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)

# Initialize hidden state as zeros
hidden_state = np.zeros((hidden_size, 1))

# Track loss
training_loss, validation_loss = [], []

def check_if_params_updated(old_params, new_params):
    # This function checks if two sets of parameters are different
    for old_param, new_param in zip(old_params, new_params):
        if not np.array_equal(old_param, new_param):
            return True  # Parameters have been updated
    return False  # Parameters have not been updated


# For each epoch
for i in range(num_epochs):
    
    # Track loss
    epoch_training_loss = 0
    epoch_validation_loss = 0
    
     # For each sentence in validation set
    for inputs, targets in val_loader:
        
        # One-hot encode input and target sequence
        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)
        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)
        
        # Re-initialize hidden state
        hidden_state = np.zeros_like(hidden_state)

        # Forward pass
        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)

        # Backward pass
        loss, _ = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)
        
        # Update loss
        epoch_validation_loss += loss
    
    # For each sentence in training set
    for inputs, targets in train_loader:
        
        # One-hot encode input and target sequence
        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)
        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)
        
        # Re-initialize hidden state
        hidden_state = np.zeros_like(hidden_state)

        # Forward pass
        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)

        # Backward pass
        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)
        print(inputs_one_hot.shape)
        
        if np.isnan(loss):
            raise ValueError('Gradients have vanished/exploded!')
        
        # Update parameters
        params = update_parameters(params, grads, lr=1e-3)
        
        # Update loss
        epoch_training_loss += loss
        
    # Save loss for plot
    training_loss.append(epoch_training_loss/len(training_set))
    validation_loss.append(epoch_validation_loss/len(validation_set))

    # Print loss every 100 epochs
    if i % 100 == 0:
        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')


# Get first sentence in test set
inputs, targets = test_set[1]

# One-hot encode input and target sequence
inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)
targets_one_hot = one_hot_encode_sequence(targets, vocab_size)

# Initialize hidden state as zeros
hidden_state = np.zeros((hidden_size, 1))

# Forward pass
outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)
output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]
print('Input sentence:')
print(inputs)

print('\nTarget sequence:')
print(targets)

print('\nPredicted sequence:')
print([idx_to_word[np.argmax(output)] for output in outputs])

# Plot training and validation loss
epoch = np.arange(len(training_loss))
plt.figure()
plt.plot(epoch, training_loss, 'r', label='Training loss',)
plt.plot(epoch, validation_loss, 'b', label='Validation loss')
plt.legend()
plt.xlabel('Epoch'), plt.ylabel('NLL')
plt.show()
</code></pre>
<p>I have tried checking if my parameters are updating and they are, also tried checking the gradient and they are not exponentially small. The loss decreases after each iteration, but the loss of the total epoch increases. You can find the full code, which includes the forward and backward pass in the repository(<a href=""https://github.com/dangerdude237/RNN_From_Scratch"" rel=""nofollow noreferrer"">https://github.com/dangerdude237/RNN_From_Scratch</a>).</p>
<p>Edit: So got the problem of the loss staying the same after each epoch fixed, I needed to change the loss function. But now the loss after each epoch increases.</p>
","0","Question"
"77941625","","<p>I need help on a project I have where I have a list of names referring to multiple products but some names refer to the same product just written in a different way (ex below) what's the best way to classify the most similar names hat are most likely referring to the same product in groups</p>
<p>Example
just one product could be named like this:</p>
<p>-EXFORGE 5 MG/160 B/28 COMP</p>
<p>-EXFORGE 5MG /160 Bte 28</p>
<p>-EXFORGE COMP 5 MG _160 B / 28</p>
<p>-Exforge 5 MG 160 B/28 COMP.</p>
<p>Tried FuzzyWuzzy library but it didn't give me a good result
this was what I did</p>
<pre><code>from fuzzywuzzy import process

# Assuming 'data' is your column of product names in tabular data
data = [
    'EXFORGE 5 MG/160 B/28 COMP',
    'EXFORGE 5MG /160 Bte 28',
    'doliprane 500MG',
    'dolipran 5.0 MG',
    'EXFORGE COMP 5 MG _160 B / 28',
    'Exforge 5 MG 160 B/28 COMP.',
    'Doliprane 500MG Comp',
    'EXFORGE CP.PEL 5MG/ 160 MG 28',
    'Ciprofloxacine 500 MG/5 ML IV 100 ML Solution for Infusion',
    'CIPROFLOXACINE 500MG/5ML IV 100 ML Sol for Inf',
    'Ciprofloxacine Sol for Infusion 500 MG/5 ML 100 ML',
    'CIPROFLOXACINE 500 MG/5 ML IV 100 ML Sol for Infusion',
    'Ciprofloxacine 500 MG IV 100 ML Sol for Inf',
    'Paracetamol 500 MG Tab 30s',
    'PARACETAMOL 500MG Tab 30s',
    'Paracetamol Tab 500 MG 30s',
    'PARACETAMOL Tab 500 MG 30s',
    'Paracetamol 500 MG 30s Tab',
    'Lisinopril 10 MG Tab 28s',
    'LISINOPRIL 10MG Tab 28s',
    'Lisinopril Tab 10 MG 28s',
    'LISINOPRIL Tab 10 MG 28s',
    'Lisinopril 10 MG 28s Tab',
    'Simvastatin 20 MG Tab 100s',
    'SIMVASTATIN 20MG Tab 100s',
    'Simvastatin Tab 20 MG 100s',
    'SIMVASTATIN Tab 20 MG 100s',
    'Simvastatin 20 MG 100s Tab',
    'Omeprazole 20 MG Caps 28s',
    'OMEPRAZOLE 20MG Caps 28s',
    'Omeprazole Caps 20 MG 28s',
    'OMEPRAZOLE Caps 20 MG 28s',
    'Omeprazole 20 MG 28s Caps'
]

# Define a threshold for similarity
threshold = 87

# Initialize groups list
groups = []

# Function to add similar names to a group
def add_to_group(group, name):
    for existing_name in group:
        if process.extractOne(existing_name, [name])[1] &gt;= threshold:
            return True
    return False

# Iterate through data to form groups
for name in data:
    added = False
    for group in groups:
        if add_to_group(group, name):
            group.add(name)
            added = True
            break
    if not added:
        groups.append({name})

# Print the groups of most similar names
for idx, group in enumerate(groups, 1):
    print(f&quot;Group {idx}: {group}&quot;)
</code></pre>
<p>and got just 3 groups instead of 7 and when I adjusted the threshold to just 88 instead of 87 it went 10 groups</p>
","2","Question"
"77942574","","<p>I am working on a multi-output classification task using Keras and ResNet50. The dataset consists of facial images, and each image has four associated labels representing the intensity of boredom, engagement, confusion, and frustration, each ranging from 1 to 4.</p>
<ol>
<li>Frame: Absolute paths to the images.</li>
<li>Boredom: Intensity of boredom (1 to 4).</li>
<li>Engagement: Intensity of engagement (1 to 4).</li>
<li>Confusion: Intensity of confusion (1 to 4).</li>
<li>Frustration: Intensity of frustration (1 to 4).</li>
</ol>
<p>I want to create a ResNet-based model with multiple outputs to predict these four labels simultaneously.</p>
<pre><code>from tensorflow import keras
from tensorflow.keras import Input
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models

base_model = ResNet50(input_shape=(128, 128, 3),
                      include_top=False,
                      weights='imagenet')

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(64, activation='relu'),
    layers.Dense(4, activation='softmax', name='Boredom'),
    layers.Dense(4, activation='softmax', name='Engagement'),
    layers.Dense(4, activation='softmax', name='Confusion'),
    layers.Dense(4, activation='softmax', name='Frustration')
])


model.compile(optimizer='adam',
              loss={'Boredom': 'categorical_crossentropy',
                    'Engagement': 'categorical_crossentropy',
                    'Confusion': 'categorical_crossentropy',
                    'Frustration': 'categorical_crossentropy'},
              metrics=['accuracy'])
model.summary()
</code></pre>
<pre><code>Error:
ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['Boredom', 'Engagement', 'Confusion']). Valid mode output names: ['Frustration']. Received struct is: {'Boredom': 'categorical_crossentropy', 'Engagement': 'categorical_crossentropy', 'Confusion': 'categorical_crossentropy'}
</code></pre>
<p>I don't really know what is wrong, the names are matching</p>
","0","Question"
"77942581","","<p>When I try to fit the training dataset to the GridsearchCV, it states that the scoring must be a str among:</p>
<pre><code>{'jaccard_samples', 'precision_macro', 'balanced_accuracy', ...&lt;long list of allowed classes, see edit history&gt;...}, 
a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. 
Got {'precision', 'recall', 'accuracy', 'f1'} instead.&quot;
</code></pre>
<p>Even though my scoring is a list of: <code>&quot;{'precision','f1','recall','accuracy'}&quot;</code>.</p>
<p>I run the code with the Jupyter kernel extension in VS code. I tried changing it to only go for the scoring of 'Recall' and it worked, however that I would like to have multiple scoring variables.</p>
","1","Question"
"77946209","","<p>From keras's LSTM documentation the input should be A 3D tensor with shape (batch, timesteps, feature)</p>
<p>The output will be (batch, units) where units is number features we want from LSTM unit.</p>
<p>As my knowledge single cell of lstm takes hidden state, cell state and single number as input for timestamp t and passes it's output to next cell in form of c(t+1) and h(t+1). But from the documentation code it is generating output in 2D form?</p>
<pre><code>inputs = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
output = lstm(inputs)
output.shape
(32, 4)
</code></pre>
<p>Question 1: How vector representation is passed to LSTM? (At each timestamp it is passing 8 features. If there are 8 lstm units running in parallel then output should also be of size 8)</p>
<p>Question 2: How the final output is of size 4. (If we ignore batch size)</p>
","0","Question"
"77947679","","<p>I have a training set of shape (1280, 100, 20, 4096) which I feed to a transformer-based model for binary classification (labels are either 0 or 1). This results in an big amount of features that I'm struggling to handle (I've tried to feed it to the model in batches, but I'm not sure about the best approach. Right now I just reduced it to (450, 100, 20, 4096), but any suggestion is appreciated), but my problem at the moment is that no matter how many epochs I train my model on, the accuracy will always be of 67,5% (which is the percentage of 0-labeled features in the test set), precision and recall on the test set will always be 0%. I've tried to normalize my data before feeding it to the model:</p>
<pre><code>    scaler = StandardScaler()
    train_data = scaler.fit_transform(train_data.reshape(-1, train_data.shape[-1])).reshape(train_data.shape)
    test_data = scaler.transform(test_data.reshape(-1, test_data.shape[-1])).reshape(test_data.shape)
</code></pre>
<p>but this didn't result in any improvement. The model I'm using is based on an encoder-only transformer:</p>
<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 20, 4096)]   0         
_________________________________________________________________
frame_position_embedding (Po (None, 100, 20, 4096)     8192000   
_________________________________________________________________
transformer_layer (Encoder)  (None, 100, 20, 4096)     134299652 
_________________________________________________________________
global_max_pooling (GlobalMa (None, 4096)              0         
_________________________________________________________________
dropout (Dropout)            (None, 4096)              0         
_________________________________________________________________
output (Dense)               (None, 1)                 4097      
=================================================================
Total params: 142,495,749
Trainable params: 142,495,749
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>During training, I can see that loss, accuracy, precision and recall reach decent levels, but when I evaluate the model on the test set all these values are as I previously described:</p>
<pre><code>Epoch 100/100
29/29 [==============================] - 90s 3s/step - loss: 0.0839 - accuracy: 0.9610 - recall: 0.9316 - precision: 0.9589
2024-02-06 12:38:38.815759: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 9175040000 exceeds 10% of free system memory.
9/9 [==============================] - 21s 2s/step - loss: 9.4117 - accuracy: 0.6750 - recall: 0.0000e+00 - precision: 0.0000e+00
Test accuracy: 67.5%
Test recall: 0.0%
Test precision: 0.0%
</code></pre>
<p>The model optimizer is adam, the loss is binary crossentropy. Activation is sigmoid.
I'm struggling in finding an adeguate tuning for the model, and even to understand its current behavior. In addition, it's not clear to me if feeding the sets in batches both to the scaler and the fit functions would change the actual training of the model.</p>
","0","Question"
"77951236","","<p>I want to measure the r2_score in the kc_house_data data available in kaggle but I connot understand why it gives me a negative result.Can you explain me please?</p>
<pre><code>df[&quot;combined&quot;] = bedrooms_norm + bathrooms_norm + sqftliving_norm + sqftabove_norm + long_norm + lat_norm + sqftliving15_norm +yrbuilt_norm + grade_norm + floors_norm + sqftbasement_norm+condition_norm
cdf = df[[&quot;combined&quot;, &quot;price&quot;]]

def log(a, Beta_1, Beta_2, Beta_3):
    y = (Beta_1 * (np.power(Beta_2, a)) + Beta_3)
    return y
beta_3 = 0.10
beta_2 = 1.5
beta_1 = 1
x_data, y_data = (df[&quot;combined&quot;].values, df[&quot;price&quot;].values)
x_data_norm = x_data / max(x_data)
y_data_norm = y_data / max(y_data)
y_pred = log(x_data_norm, beta_1, beta_2, beta_3)
from scipy.optimize import curve_fit
popt, pcov = curve_fit(log, x_data_norm, y_data_norm)
x = np.linspace(4, 12, 21613)
x = x / max(x)
plt.figure(figsize=(8,5))
y = log(x, *popt)
plt.plot(x_data_norm, y_data_norm, 'ro', label='data')
plt.plot(x, y, linewidth=3.0, label='fit')
plt.legend(loc='best')
plt.ylabel('price')
plt.xlabel('combined')
plt.show()

from sklearn.metrics import r2_score
print(&quot;R2-score: %.2f&quot; % r2_score(y_data_norm , y))

</code></pre>
<p>I tried to use r2_score in sklearn.metrics and I expected a number between 0 to 1 but I don't understand why it calculates a negative number</p>
<pre><code>from sklearn.metrics import r2_score
print(&quot;R2-score: %.2f&quot; % r2_score(y_data_norm , y))
R2-score: -59.51
</code></pre>
","-2","Question"
"77954817","","<p>I am wondering, what are the side effects of these two scenarios.</p>
<ol>
<li><code>netA</code> and <code>netB</code> share weights but have one optimizer each</li>
<li><code>netA</code> and <code>netB</code> share weights but use a single optimizer</li>
</ol>
<p>The results are very similar, but not equal.</p>
<p>Consider this example</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

torch.manual_seed(0)

share_fc2 = True
batch_size = 4
channels = 2

training_data_A = torch.rand((batch_size, channels))
training_data_B = torch.rand((batch_size, channels))


class Net(torch.nn.Module):
    &quot;&quot;&quot;Minimal network&quot;&quot;&quot;

    def __init__(self) -&gt; None:
        super().__init__()
        self.fc1 = nn.Linear(channels, channels, bias=False)
        self.fc2 = nn.Linear(channels, channels, bias=False)

    def forward(self, x):
        return self.fc2(self.fc1(x))


netA = Net().requires_grad_()
netB = Net().requires_grad_()

if share_fc2:
    # replace fc2 by netA.fc2
    netB.fc2 = netA.fc2

lossA = netA(training_data_A).mean()
lossB = netA(training_data_A).mean()

lossA.backward()
lossB.backward()
</code></pre>
<h3>Option 1</h3>
<pre class=""lang-py prettyprint-override""><code># OPTION 1 (independent run, seed used)
optA = torch.optim.Adam(params=netA.parameters())  # contains shared fc2
optB = torch.optim.Adam(params=netB.parameters())  # conteins shared fc2
optA.step()
optB.step()

print(list(netA.parameters()))
print(list(netB.parameters()))
</code></pre>
<h4>output</h4>
<p><a href=""https://i.sstatic.net/t7YR1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t7YR1.png"" alt=""enter image description here"" /></a></p>
<h3>Option 2</h3>
<pre class=""lang-py prettyprint-override""><code>opt = torch.optim.Adam(
    params=(
        list(netA.parameters())  # contains netA.fc1, netA/B.fc2
        + list(netB.parameters())[:1]  # contains netA.fc1
    )
)
opt.step()
print(list(netA.parameters()))
print(list(netB.parameters()))

</code></pre>
<h4>output</h4>
<p><a href=""https://i.sstatic.net/JoQKR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JoQKR.png"" alt=""enter image description here"" /></a></p>
<h3>Questions</h3>
<ul>
<li>are the differences due to Adam's internal parameter adjustments and thus can be ignored</li>
<li>Are the differences due to mathematics in gradient calculation and application?</li>
<li>Is one option better than the other?</li>
</ul>
","0","Question"
"77957522","","<p>I'm trying to implement the Info NCE loss of this <a href=""https://arxiv.org/pdf/1807.03748.pdf"" rel=""nofollow noreferrer"">paper</a> with my own image dataset. I'm following the implementation from this <a href=""https://github.com/sthalles/SimCLR/tree/master"" rel=""nofollow noreferrer"">repo</a> and using the following code:</p>
<pre><code>def info_nce_loss(self, features):

        labels = torch.cat([torch.arange(self.args.batch_size) for i in range(self.args.n_views)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(self.args.device)

        features = F.normalize(features, dim=1)

        similarity_matrix = torch.matmul(features, features.T)
        # assert similarity_matrix.shape == (
        #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)
        # assert similarity_matrix.shape == labels.shape

        # discard the main diagonal from both: labels and similarities matrix
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.args.device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        # assert similarity_matrix.shape == labels.shape

        # select and combine multiple positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)

        # select only the negatives the negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)

        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.args.device)

        logits = logits / self.args.temperature
        return logits, labels
</code></pre>
<p>to train my model in a self-supervised manner. I was using <code>batch_size</code> of 32 with the above loss function in my code and everything was working fine. But when I change the <code>batch_size</code> to any other number for instance, 256, I get the following error:</p>
<pre><code>The shape of the mask [512, 512] at index 0 does not match the shape of the indexed tensor [2, 2] at index 0. 
</code></pre>
<p>The error originates at this line:</p>
<pre><code>labels = labels[~mask].view(labels.shape[0], -1)
</code></pre>
<p>I tried resizing my images but that didn't help either. Any idea on what could be the issue here?</p>
","1","Question"
"77958199","","<p>i am using the example 'CameraWebServer' on my esp32-cam board. The upload setting are the listed above:</p>
<pre><code>Board:  AI Thinker ESP32-CAM;
CPU Frequency: 240 MHZ ;
Flash Frequency: 80 Mhz;
Flash Mode: QIO.
Arduino IDE 2.0.0
esp32 by Espressif version 2.0.14
</code></pre>
<p>With those setting i'm able to upload my code , but the fece recognition fuction is not working. When i click in 'enroll face' nothing happens and my Serial Monitor display the message EV-VSYNC-OVF. How to resolve this?</p>
<p>Additionally i already tried unsuccessfully modify the upload seetings and change the parameters config.frame_size and config.xclk_freq_hz in the file 'CameraWebServer.ino'.</p>
","-1","Question"
"77959410","","<h2>This fails</h2>
<pre class=""lang-py prettyprint-override""><code>import torch

def test1():  
  layer = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test1()
</code></pre>
<p>with error</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-3-bb36a010bd86&gt; in &lt;cell line: 10&gt;()
      8     x = 5 - torch.sum(layer(torch.ones(90)))
      9     x.backward()
---&gt; 10 test1()
     11 # and this works as well
     12 

2 frames
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    249     # some Python versions print out the first line of a multi-line function
    250     # calls in the traceback and some print out the last line
--&gt; 251     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    252         tensors,
    253         grad_tensors_,

RuntimeError: Function TBackward0 returned an invalid gradient at index 0 - got [10, 90] but expected shape compatible with [10, 100]
</code></pre>
<h2>This works</h2>
<pre class=""lang-py prettyprint-override""><code>import torch

def test2():  
  layer = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x    #main change
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test2()
</code></pre>
<h2>and this works as well</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
def test3():  
  layer = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight)   #main change
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test3()
</code></pre>
<p>I encountered this when trying to implement a paper on model pruning (Temporal Neuron Variance Pruning). I believe this has something to do with the autograd graph, but I have am not sure what exactly is going on. I've already seen the link on pruning and got my code working using the 3rd snippet. I am now trying to figure out why 1 and 2 did not work. Is there some explanation for why these almost identical code snippets work or fail?</p>
<h2>Major points I'd like to figure out -</h2>
<ol>
<li>what is <code>TBackward0</code></li>
<li>where is it defined</li>
<li>where is the runtime error raised</li>
<li>why is the compatibility with the old shape expected - especially when the grad has been modified correctly (I am assuming I have edited the tensors correctly because cases 2, 3 work)</li>
<li>can I change something else (other than the 2 working cases) to make this work ?</li>
</ol>
","4","Question"
"77963476","","<p>I want to implement masked_fill in mlx but it doesn't play nice with float('-inf')</p>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html</a></p>
<p>I'm trying to use mlx.core.where for this</p>
<pre><code>masked_tensor = mlx.core.where(mask, mlx.core.array(float('-inf')), mlx.core.array(0))
</code></pre>
<p>but for the mask</p>
<pre><code>array([[False, False, True, True],                                                                                                                                                      
       [False, False, True, True],                                                                                                                                                     
       [False, False, True, True],                                                                                                                                                    
       [False, False, True, True]], dtype=bool) 
</code></pre>
<p>this returns</p>
<pre><code>array([[nan, nan, -inf, -inf],                                                                                                                                              
       [nan, nan, -inf, -inf],                                                                                                                                                
       [nan, nan, -inf, -inf],                                                                                            
       [nan, nan, -inf, -inf]], dtype=float32)   
</code></pre>
<p>which is not what I want. Ideally it would return</p>
<pre><code>array([[0, 0, -inf, -inf],                                                                                                                                              
       [0, 0, -inf, -inf],                                                                                                                                                
       [0, 0, -inf, -inf],                                                                                            
       [0, 0, -inf, -inf]], dtype=float32)  
</code></pre>
<p>help</p>
","0","Question"
"77963903","","<p>I am trying to time each round of an adaboost algorithm (how long each additional tree takes to build). I conda installed scikit-learn 1.4.0 (because on their website it says this version) and all the other reqs to run code.</p>
<p>Here is my code:</p>
<pre><code>Y, z = parse.getHARData() #returns my features Y and labels z

Z_train, Z_test, j_train, j_test = train_test_split(Y, z, test_size=0.30, shuffle=True)

b_estimator = DecisionTreeClassifier(max_depth=DEPTH)

ada = AdaBoostClassifier(estimator=b_estimator, n_estimators=NUMTREES)

elapsed_times = []

for stage in range(NUMTREES): start_time = time.time()

    # Access and fit the current base estimator
    base_estimator = ada._make_estimator(append=True, random_state=42)
    base_estimator.fit(Z_train, j_train)
    
    elapsed_time = time.time() - start_time
    elapsed_times.append(elapsed_time)
</code></pre>
<p>I was expecting this to start the timer, grow a tree using the forest's previous information, add that tree to the ensemble,  stop the time, and append the elapsed time to elapsed_times.</p>
<p>instead, its returning this error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
Cell In[9], line 7
      4 start_time = time.time()
      6 # Access and fit the current base estimator
----&gt; 7 base_estimatr = ada._make_estimator(append=True, random_state=42)
      8 base_estimatr.fit(Z_train, j_train)
     10 elapsed_time = time.time() - start_time

File ~/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/ensemble/_base.py:141, in BaseEnsemble._make_estimator(self, append, random_state)
    135 def _make_estimator(self, append=True, random_state=None):
    136     &quot;&quot;&quot;Make and configure a copy of the `estimator_` attribute.
    137 
    138     Warning: This method should be used to properly instantiate new
    139     sub-estimators.
    140     &quot;&quot;&quot;
--&gt; 141     estimator = clone(self.estimator_)
    142     estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})
    144     if random_state is not None:

AttributeError: 'AdaBoostClassifier' object has no attribute 'estimator_'
</code></pre>
","0","Question"
"77964228","","<p>I'm having some issues trying to understand how to use the &quot;|&quot; pipe symbol in langchain when declaring a chain.</p>
<pre><code>prompt_template = &quot;&quot;&quot;
  Respond based only on the following context:
  {context}

As a seasoned expert tasked with optimizing a given project, your expertise is crucial 
in addressing its challenges and seizing opportunities.

Issues and Opportunities:
{issues_and_opportunities}

Business Goals:
{business_goals}

Project Description:
{description}

Please furnish a comprehensive response in JSON format, covering the following 
components:

1. Proposed Solution:
 - Articulate a detailed plan to overcome identified challenges and capitalize on 
opportunities.

2. Technological Details:
 - Conduct an in-depth analysis of the technologies earmarked for this project.
 - Specify programming languages, frameworks, and platforms to be employed.
 - Example: Python, Azure, Pytorch, Tensorflow, AWS, Openai, LLM...

 Example Output (in JSON format):
{{

 &quot;solution&quot;: &quot;Your detailed solution goes here&quot;,
 &quot;technologies&quot;: [&quot;Technology 1&quot;, &quot;Technology 2&quot;, ...],
 }}
&quot;&quot;&quot;

prompt = ChatPromptTemplate.from_template(prompt_template)
</code></pre>
<p>Then I build my retrieval</p>
<pre><code>retriever = vectordb.as_retriever()
</code></pre>
<p>and my llm</p>
<pre><code>llm = AzureChatOpenAI(
    api_key=openai_api_key,
    api_version=openai_api_version,
    azure_endpoint=openai_api_base,
    model=llm_model)
</code></pre>
<p>Then I add my outputparser</p>
<pre><code>from langchain.output_parsers import ResponseSchema, StructuredOutputParser
from langchain.callbacks import get_openai_callback

solution_schema = ResponseSchema(name=&quot;solution&quot;, description=&quot;as given&quot;)
technologies_schema = ResponseSchema(name=&quot;technologies&quot;, description=&quot;as given&quot;)

response_schemas = [solution_schema,
                    technologies_schema]

output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
</code></pre>
<p>and finally when I tried to put all this together using a chain I fail miserably</p>
<pre><code>rag_chain = (
    {&quot;context&quot;: retriever, &quot;issues_and_opportunities&quot;: RunnablePassthrough(), &quot;business_goals&quot;: RunnablePassthrough(), &quot;description&quot;: RunnablePassthrough()}
    | prompt
    | llm
    | output_parser
)

rag_chain.invoke(issues_and_opportunities, business_goals, description)
</code></pre>
<p>Getting this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-30-3a4e499badd2&gt; in &lt;cell line: 1&gt;()
----&gt; 1 rag_chain.invoke(issues_and_opportunities, business_goals, description)

TypeError: RunnableSequence.invoke() takes from 2 to 3 positional arguments but 4 were given
</code></pre>
","4","Question"
"77967230","","<p>I've been attempting to &quot;transform&quot; a multiclass classification system using ViTForImageClassification into a multilabel. However, I've been running into some problems. <br>
(The original multiclass system which I'm attempting to convert can be found <a href=""https://www.kaggle.com/code/dima806/full-flat-tyre-image-detection-vit"" rel=""nofollow noreferrer"">here</a>.) <br><br>
The folder structure for the dataset is as follow:</p>
<pre><code>/dataset
./class1
./class2
./class3
./class1-class2
./class2-class3
</code></pre>
<p>The code I have so far to prepare the dataset is as follow:</p>
<pre class=""lang-py prettyprint-override""><code>file_names = []
labels = []
all_labels = []

for file in sorted((Path('/content/dataset').glob('*/*.*'))):
    folder = str(file).split('/')[-2].split('.')[0]
    label = folder.split('-')
    for l in label:
      if not set([l + '.class']).issubset(all_labels):
        all_labels.append(str(label[0]) + '.class')
    labels.append([x + '.class' for x in label])
    file_names.append(str(file))

print(len(file_names), len(labels))

df = pd.DataFrame.from_dict({&quot;image&quot;: file_names, &quot;label&quot;: labels})

mlb = MultiLabelBinarizer()
mlb_result = mlb.fit_transform([df.loc[i,'label'] for i in range(len(df))])
df_final = pd.concat([df['image'],pd.DataFrame(mlb_result,columns=list(mlb.classes_))],axis=1)
dataset = Dataset.from_pandas(df_final).cast_column(&quot;image&quot;, Image())

labels_list = list(set(all_labels))

label2id, id2label = dict(), dict()
for i, label in enumerate(labels_list):
    label2id[label] = i
    id2label[i] = label

ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)
print(ClassLabels)

dataset = dataset.train_test_split(test_size=0.8, shuffle=True)

train_data = dataset['train']

test_data = dataset['test']
model_str = 'google/vit-base-patch16-224-in21k'
processor = ViTImageProcessor.from_pretrained(model_str)

image_mean, image_std = processor.image_mean, processor.image_std
size = processor.size[&quot;height&quot;]

normalize = Normalize(mean=image_mean, std=image_std)

_train_transforms = Compose(
    [
        Resize((size, size)),
        RandomRotation(10),
        RandomAdjustSharpness(2),
        ToTensor(),
        normalize
    ]
)

_val_transforms = Compose(
    [
        Resize((size, size)),
        ToTensor(),
        normalize
    ]
)

def train_transforms(examples):
    examples['pixel_values'] = [_train_transforms(image.convert(&quot;RGB&quot;)) for image in examples['image']]
    return examples

def val_transforms(examples):
    examples['pixel_values'] = [_val_transforms(image.convert(&quot;RGB&quot;)) for image in examples['image']]
    return examples

train_data.set_transform(train_transforms)

test_data.set_transform(val_transforms)
</code></pre>
<p>The code that I have to prepare the model is:</p>
<pre class=""lang-py prettyprint-override""><code>model = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list), problem_type=&quot;multi_label_classification&quot;)

model.config.id2label = id2label
model.config.label2id = label2id

accuracy = evaluate.load(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    predictions = eval_pred.predictions

    label_ids = eval_pred.label_ids

    predicted_labels = predictions.argmax(axis=1)

    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']

    return {
        &quot;accuracy&quot;: acc_score
    }

metric_name = &quot;accuracy&quot;

model_name = &quot;multilabel-classifier&quot;
num_train_epochs = 30

args = TrainingArguments(
    output_dir=model_name,
    logging_dir='./logs',
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=1e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=8,
    num_train_epochs=num_train_epochs,
    weight_decay=0.02,
    warmup_steps=50,
    remove_unused_columns=False,.
    save_strategy='epoch',
    load_best_model_at_end=True,
    save_total_limit=1,
    report_to=&quot;mlflow&quot;
)

# Attempting to shape it: pixel_values of shape (batch_size, num_channels, height, width) and labels of shape (batch_size, num_labels)
def collate_fn(examples):
    pixel_values = torch.stack([example[&quot;pixel_values&quot;] for example in examples])

    temp = []
    for example in examples:
      temp2 = []
      for label in example:
        if label != 'image' and label != 'pixel_values':
          temp2.append(example[label])
      temp.append(temp2)

    print(temp)

    labels = torch.tensor(temp)
    print(labels)

    return {&quot;pixel_values&quot;: pixel_values, &quot;labels&quot;: labels}

trainer = Trainer(
    model,
    args,
    train_dataset=train_data,
    eval_dataset=test_data,
    data_collator=collate_fn,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)
</code></pre>
<p>The code I have to train the model is:</p>
<pre class=""lang-py prettyprint-override""><code>trainer.evaluate()
trainer.train()
</code></pre>
<p>The current error I'm running into is:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-13-bceb9305a605&gt; in &lt;cell line: 5&gt;()
      3 # to assess how well the model is performing on unseen data.
      4 
----&gt; 5 trainer.evaluate()

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)
   3193         raise ValueError(f&quot;Target size ({target.size()}) must be the same as input size ({input.size()})&quot;)
   3194 
-&gt; 3195     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)
   3196 
   3197 

RuntimeError: result type Float can't be cast to the desired output type Long
</code></pre>
<p>I believe I'm preparing the dataset wrong as I can infer from <a href=""https://github.com/huggingface/transformers/issues/16003"" rel=""nofollow noreferrer"">here</a>, however I'm not sure how to continue / fix what I currently have.</p>
","0","Question"
"77969343","","<p>Imagine you have trained a model containing an Embedding layer.</p>
<p>Your model performs well and you're happy with your embedding.</p>
<p>Then, suddenly, you want to add a new item in your vocabulary.
In other words you want to compute the embedding of this new item.</p>
<p>Basically an Embedding layer is a lookup table, used to turn positive integers into dense vectors of fixed size, and now you want to consider a new integer that was not there during the training.</p>
<p>How can you do this <strong>without</strong> retraining the model from scratch?</p>
<p>Does it make sense (after some matrix shape adjustement) to relaunch the training freezing all the parameters except the ones used to embed the new item?</p>
","0","Question"
"77969990","","<p>let us suppose we have the following simplified code:</p>
<pre><code>import pandas as pd
import shap
from sklearn.ensemble import  RandomForestRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
mylabel =LabelEncoder()
data =pd.read_csv(&quot;https://raw.githubusercontent.com/krishnaik06/Multiple-Linear-Regression/master/50_Startups.csv&quot;)
data['State'] =mylabel.fit_transform(data['State'])
print(data.head())
model =RandomForestRegressor()
y =data['Profit']
X =data.drop('Profit',axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=1)
model.fit(X_train,y_train)
explainer =shap.TreeExplainer(model)
shap_values =explainer.shap_values(X_train)
plt.figure(figsize=(30,30))
plt.subplot(2,1,1)
shap.summary_plot(shap_values, X_train, feature_names=X.columns, plot_type=&quot;bar&quot;)
plt.subplot(2,1,2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns)
plt.show()
</code></pre>
<p>when i  run this code, i am getting  two image on different figure :
one image :
<a href=""https://i.sstatic.net/zcV7w.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zcV7w.png"" alt=""enter image description here"" /></a></p>
<p>and another image :
<a href=""https://i.sstatic.net/5AVK9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5AVK9.png"" alt=""enter image description here"" /></a></p>
<p>i want to  plot  they next to each other, as you  see  i have used subplot :</p>
<pre><code>plt.subplot(2,1,1)
shap.summary_plot(shap_values, X_train, feature_names=X.columns, plot_type=&quot;bar&quot;)
plt.subplot(2,1,2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns)
</code></pre>
<p>but it does not work, i  was trying to use  this code :</p>
<pre><code>fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))
shap.dependence_plot('age', shap_values[1], X_train, ax=axes[0, 0], show=False)
shap.dependence_plot('income', shap_values[1], X_train, ax=axes[0, 1], show=False)
shap.dependence_plot('score', shap_values[1], X_train, ax=axes[1, 0], show=False)
plt.show()
</code></pre>
<p>but summary_plot does not have argument  ax, so how can i use  it?</p>
","-1","Question"
"77973831","","<p>I am getting an error when passing in multiple images to train. But it is fine when passing just one image. The images are of the same size.</p>
<p>Here is the code:</p>
<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers, datasets, models

# Load the template image
template_image = tf.keras.preprocessing.image.load_img('Template.jpg')
template_array = tf.keras.preprocessing.image.img_to_array(template_image)

# Load the actual image
actual_image = tf.keras.preprocessing.image.load_img('Actual.jpg')
actual_array = tf.keras.preprocessing.image.img_to_array(actual_image)

# Create a model
model = tf.keras.Sequential([
  layers.InputLayer(input_shape=(template_array.shape)),
  layers.Conv2D(16, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dense(2, activation='softmax'),
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
for layer in model.layers:
    print(layer.output_shape)

template_array = template_array.reshape((1, 549, 549, 3))
actual_array = actual_array.reshape((1, 549, 549, 3))
train_x = [template_array, actual_array]
y_train = np.array([1,0])
y_train = y_train.reshape(1,2)
train_y = [y_train, y_train]

print(&quot;X shape is: &quot;)
print(template_array.shape)
print(&quot;Y shape is: &quot;)
print(y_train)

# Train the model
model.fit(x=train_x, y=train_y, epochs=10)

# Make predictions
predictions = model.predict([actual_array])

# Check for incorrect or missing parts
for i in range(len(predictions)):
  if predictions[i][0] &gt; predictions[i][1]:
    print('Part {} is missing or incorrect'.format(i))

</code></pre>
<p>Error received:</p>
<pre><code>ValueError: Layer &quot;sequential_28&quot; expects 1 input(s), but it received 2 input tensors. Inputs received: 
[&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 549, 549, 3) dtype=float32&gt;, 
&lt;tf.Tensor 'IteratorGetNext:1' shape=(None, 549, 549, 3) dtype=float32&gt;]
</code></pre>
<p>If I were to pass x=template_array and y = y_train it runs fine. But that means I trained off of only one image.</p>
<p>Am I not able to make an array of my images and a corresponding array of classification to pass all at once? How do I go about passing in all of my train data at once?</p>
","0","Question"
"77974163","","<p>I've been stuck with an issue of a model not being able to be trained for some time now, and I was hoping to get some help on it. Here is part of the code</p>
<pre><code>link = &quot;https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2&quot;
input_layer = tf.keras.layers.Input(shape=(224, 224, 3), dtype=tf.float32, name=&quot;input&quot;, trainable=False)
feature_extractor = hub.KerasLayer(link, trainable=False)

model = tf.keras.Sequential([
    input_layer,
    feature_extractor,
    tf.keras.layers.Dense(data_info.features['label'].num_classes, activation=&quot;softmax&quot;)
])
</code></pre>
<p>And here is the error I'm getting:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In[64], line 7
      4 input_layer = tf.keras.layers.Input(shape=(224, 224, 3), dtype=tf.float32, name=&quot;input&quot;)
      5 feature_extractor = hub.KerasLayer(link, trainable=False)
----&gt; 7 model = tf.keras.Sequential([
      8     input_layer,
      9     feature_extractor,
     10     tf.keras.layers.Dense(data_info.features['label'].num_classes, activation=&quot;softmax&quot;)
     11 ])
     13 model.compile(optimizer=&quot;adam&quot;, loss=&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
     14 model.summary()

File c:\Users\steel\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\trackable\base.py:204, in no_automatic_dependency_tracking.&lt;locals&gt;._method_wrapper(self, *args, **kwargs)
    202 self._self_setattr_tracking = False  # pylint: disable=protected-access
    203 try:
--&gt; 204   result = method(self, *args, **kwargs)
    205 finally:
    206   self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

File c:\Users\steel\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\src\utils\traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
...


Call arguments received by layer &quot;keras_layer_28&quot; (type KerasLayer):
  • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)
  • training=None
</code></pre>
<p>I've tried a number of different things like adding or removing the input layer and changing some of the parameters in that layer, but I'm not having luck. I wonder if it's because of the network I've extracted, but wanted to get some confirmation on that first because I'm not 100% sure how to determine if that's the issue.</p>
","2","Question"
"77975756","","<p>I tried using GPU on XGBoost training on windows with device = cuda it worked and training time reduced drastically now i want to do this experiment on my Mac M1 Pro.</p>
<p>How to enable GPU for XGBoost on m1 pro chip set.</p>
<p>I tried looking into documentation where i was unable to find information.</p>
","2","Question"
"77976770","","<p>I would like to create my own scikit-learn transformer for encoding numerical features that contain a taxonomy, like zipcodes or industry codes (NAICS, MCC etc). In these kind of codes there is a structure: e.g. MCC 3000-3999 is 'travel and entertainment' which is broken down further into more granular categories like 'airlines', 'car rental' etc. We cannot use them as ordinal features but if we treat them as pure categorical features (e.g. by One-Hot-Encoding) we need to choose at which level of the code structure to apply the feature encoding.</p>
<p>To solve this I created my own scikit-learn transformer which is a variation of the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html"" rel=""nofollow noreferrer"">TargetEncoder</a> using a decision tree. The code is shown below. It's important to realize that during model training, out of sample decision tree regressor scores should be used to avoid overfitting. For that reason I implemented my own <code>fit_transform</code> function that produces these out of sample scores:</p>
<pre><code>from sklearn.tree import DecisionTreeRegressor

from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.model_selection import cross_val_predict

class TaxonomyEncoder(TransformerMixin, BaseEstimator):

def __init__(self, n_leafs=10, cv=3):
    self.n_leafs = n_leafs
    self.cv = cv

def fit(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs).fit(X,y)
    return self

def transform(self, X):
    return self.tree_.predict(X).reshape(-1,1)

def fit_transform(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs)
    return cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)
</code></pre>
<p>The transformer works fine, except when using within a <code>ColumnTransformer</code>:</p>
<pre><code>from sklearn.compose import ColumnTransformer

transformer = ColumnTransformer([('taxonomy', TaxonomyEncoder(), ['mcc'])])
transformer.fit(df[['mcc']], df['y'])
transformer.transform(df[['mcc']])
</code></pre>
<p>Then I get the error that the decision tree is not fitted yet:</p>
<pre><code>NotFittedError: This DecisionTreeRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
</code></pre>
<p>Apparently, scikit-learn does some checks below the surface that causes this error. Note that there is actually no reason to have a fitted decision tree as the tree is refitted within the <code>cross_val_predict</code> function. How can I solve this problem?</p>
<p>A fully working example to reproduce the error is shown below:</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'mcc':[3000,3500,7339], 'y':[0,0,1]})

te = TaxonomyEncoder().fit(df[['mcc']], df['y'])
te.transform(df[['mcc']])
</code></pre>
<p>gives:</p>
<pre><code>array([[0.],
       [0.],
       [1.]])
</code></pre>
<p>and also fit_transform gives results as expected:</p>
<pre><code>te.fit_transform(df[['mcc']], df['y'])

array([[0.],
       [0.],
       [0.]])
</code></pre>
<p>But when wrapped in a ColumnTransformer, things go wrong:</p>
<pre><code>transformer = ColumnTransformer([('taxonomy', TaxonomyEncoder(), ['mcc'])])
transformer.fit(df[['mcc']], df['y'])
transformer.transform(df[['mcc']])
</code></pre>
","1","Question"
"77977110","","<p>I'm following a tutorial, but I'm getting the error:</p>
<p><code>ValueError: Could not interpret metric identifier: loss</code></p>
<p>at:
<code>\keras\src\metrics\__init__.py:205, in get(identifier)</code></p>
<p>It seems like in the wrapper.py line 532 <code>metric_name(key)</code> should receive the name of a loss function, but it's actually receiving the string &quot;loss&quot;</p>
<p>Here is the relevant code for the error:</p>
<pre><code>def buildNetwork():
    classificator = Sequential()
    classificator.add(Dense(units = 20, activation='relu', kernel_initializer='random_uniform', input_shape=(30,)))
    classificator.add(Dense(units = 20, activation='relu', kernel_initializer='random_uniform'))
    classificator.add(Dense(units = 1, activation='sigmoid'))
    optimizer = keras.optimizers.Adam(learning_rate=0.001, weight_decay=0.000001)
    classificator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])
    return classificator


classifier = KerasClassifier(model=buildNetwork,batch_size=10,epochs=100, loss='binary_crossentropy')

score = cross_val_score(estimator=classifier, X=data, y=truth, cv=10, error_score='raise')
</code></pre>
<p>If inside <code>keras_metric_get</code> I manually set <code>identifier = &quot;binary_crossentropy&quot;</code> it works fine</p>
<p>I don't know if it's a compatibility problem, but I have Keras 3.0.8, TF 2.15.0 and SciKeras 0.12.0</p>
","1","Question"
"77978874","","<p>Is there any way to get the intermediate steps (similar to the callback functions in Tensorflow or Pytorch Adams optimizer) when using Qiskit's internal ADAM optimizer?</p>
<p>I am implementing a Variational Quantum Circuit(VQC) to train the quantum machine learning model using Qiskit's Adam optimizer.</p>
<p>I looked through <a href=""https://docs.quantum.ibm.com/api/qiskit/qiskit.algorithms.optimizers.ADAM"" rel=""nofollow noreferrer"">the document</a> of ADAM optimizer in Qiskit, but I could not find the appropriate methods to get them.</p>
<p>I could get the intermediate training information with the Pytorch Adam optimizer, but I wanted to know whether I could do it with Qiskit.</p>
<p>I am using qiskit v0.45.2.</p>
","0","Question"
"77982056","","<p>I have a list of keras models already trained. I want to use them with StackingClassifier in scikit learn. Due to the fact that keras doesn't have predict_proba method I created a wrapper.</p>
<p>If use my models wrapped for VotingClassifier with soft and hard methods, it works.</p>
<p>But when I use stacking models, after first run, it shows me this error. I didn't found anything about it.</p>
<pre><code>class KerasWrapperWithEncoder(BaseEstimator, ClassifierMixin):
    def __init__(self, keras_model, classes_):
        self.keras_model = keras_model
        self.encoder = OneHotEncoder(sparse_output=False)
        # L'encoder OneHotEncoder est passé ici
        self.classes_ = classes_  # Définit les classes disponibles

    def fit(self, X, y):
        # Models are fitted dont fit again
        y_reshaped = y.reshape(-1, 1)
        self.encoder.fit(y_reshaped)
        return self

    def predict(self, X):
        # Utilise les modèles entraînés pour faire des prédictions
        predictions = self.keras_model.predict(X)
        np_argmax = np.argmax(predictions, axis=1)
        print(predictions)
        print(np_argmax)
        return np_argmax

    def predict_proba(self, X):
        # Retourne les probabilités des classes pour les modèles de classification
        probabilities = self.keras_model.predict(X)
        print(&quot;Shape of probabilities:&quot;, probabilities.shape)  # Debug
        return probabilities


keras_wrapped_models_with_encoder = [
    (name.replace(' ', '_').replace('__', '_'), KerasWrapperWithEncoder(model, _target_classes_))
    for name, model in keras_models.items()
]

voting_clf = VotingClassifier(
         estimators=all_estimators,
         voting='soft', 
         n_jobs=3,  
         verbose=True)
voting_clf .fit(X_train, y_train) # works perfectly 

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
keras_stacking_models_current_year = StackingClassifier(
    estimators=all_estimators,
    final_estimator=LogisticRegression(),
    cv=cv,
    verbose=3,
    # n_jobs=2
)


keras_stacking_models_current_year.fit(X_train, y_train) # throws error

</code></pre>
<pre><code>======================================stacking_models_all_models=============================
12105/12105 [==============================] - 25s 2ms/step
Shape of probabilities: (387348, 3)
Number of classes in training fold (1) does not match total number of classes (3). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds
_enforce_prediction_order(classes, predictions, n_classes, method)
   1457             dtype=predictions.dtype,
   1458         )
-&gt; 1459         predictions_for_all_classes[:, classes] = predictions
   1460         predictions = predictions_for_all_classes
   1461     return predictions

ValueError: shape mismatch: value array of shape (387348,3) could not be broadcast to indexing result of shape (387348,1,3)
</code></pre>
","1","Question"
"77983736","","<p>I want to track an entire notebook and log the parameters of cleaning steps that occur before training a model. I'd like to use <a href=""https://mlflow.org/docs/latest/index.html"" rel=""nofollow noreferrer""><code>mlflow</code></a> to do it, but on all the docs it looks like you have to track models using this format:</p>
<pre><code>with mlflow.start_run():
    ...
</code></pre>
<p>Is there a way to track an entire notebook using mlflow without the <code>with</code> block?</p>
","0","Question"
"77984286","","<p>I'm trying to use CUML's ensemble RandomForestClassifier to fit my data. When I try and fit, I get a type-error saying an integer is required.</p>
<p>My X_train is a dataframe that I converted to a numpy array and ensured the type is a float32. The original dataframe has only int64 and float32 types, as I used a labelencoder to encode string columns and then removed them.</p>
<p>My Y_train is a series that I converted to a numpy array and tried to make Int32 and Int64 (as I believed that's where the error could be).</p>
<pre><code># Convert X_train DataFrame to numpy array with float32 dtype
X_train_np = X_train.values.astype(np.float32)

# Convert y_train Series to numpy array with float32 dtype
y_train_np = y_train.values


import cuml
from cuml.ensemble import RandomForestClassifier
from cuml.model_selection import GridSearchCV

# Create the cuML Random Forest classifier
cuml_classifier = RandomForestClassifier(random_state=42,n_streams=1)

# Define the parameter grid for grid search
param_grid = {
    'max_depth': [5],
    'n_estimators': [5],
    'split_criterion': ['entropy']
}

# Create the GridSearchCV object
grid_search = GridSearchCV(
    cuml_classifier,
    param_grid=param_grid,
    scoring='roc_auc',
    cv=3,
    return_train_score=True,
    refit=&quot;AUC&quot;,
    verbose=1
)

# Fit the grid search to the training data
grid_search.fit(X_train_np, y_train_np)
</code></pre>
<pre><code>ValueError: 
All the 3 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
3 fits failed with the following error:
Traceback (most recent call last):
  File &quot;/opt/conda/envs/rapids-23.12/lib/python3.10/site-packages/sklearn/model_selection/_validation.py&quot;, line 890, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &quot;/opt/conda/envs/rapids-23.12/lib/python3.10/site-packages/nvtx/nvtx.py&quot;, line 115, in inner
    result = func(*args, **kwargs)
  File &quot;/opt/conda/envs/rapids-23.12/lib/python3.10/site-packages/cuml/internals/api_decorators.py&quot;, line 188, in wrapper
    ret = func(*args, **kwargs)
  File &quot;randomforestclassifier.pyx&quot;, line 463, in cuml.ensemble.randomforestclassifier.RandomForestClassifier.fit
TypeError: an integer is required
</code></pre>
<p>How to resolve this?</p>
<p>I have checked y_train and all values have a binary 1 or 0 as expected, there are no NaN, None or null values.</p>
<p>Both y_train and X_train are the same length.</p>
<p>I also tried passing X_train as a dataframe and y_train as a series instead of converting to numpy arrays. The same error occurred.</p>
<p>I tried the above and kept the astype(&quot;float32&quot;) and astype(&quot;int32&quot;) but that gave the same error.</p>
<pre><code># Create the cuML Random Forest classifier
cuml_classifier = RandomForestClassifier(max_depth=5, n_estimators=5,random_state=42)

trained_RF = cuml_classifier.fit(X_train_np, y_train_np)

predictions = cuml_classifier.predict(X_train)

cu_score = cuml.metrics.accuracy_score(y_train,predictions)
sk_score = accuracy_score(y_train.values,predictions)

print( &quot; cuml accuracy: &quot;, cu_score )
print( &quot; sklearn accuracy : &quot;, sk_score )
</code></pre>
<p>I tried without gridsearch and just directly and it worked without the error. I still need the gridsearch though.</p>
","0","Question"
"77986399","","<p>I am creating a machine learning model to classify images, and I am creating my datasets. I have a folder that contains my train, test, and validation datasets (train_ds, test_ds, val_ds respectively). I then define data augmentation as follows:</p>
<pre><code>tf.random.set_seed(42)

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip(&quot;horizontal_and_vertical&quot;),
    layers.experimental.preprocessing.RandomRotation(0.2), 
    layers.experimental.preprocessing.RandomZoom(
        height_factor=(-0.3, -0.03),
        width_factor=None), 
])
</code></pre>
<p>and I change my train_ds to account for data augmentation as follows. This code also shuffles the train_ds and applies autotuning for all ds:</p>
<pre><code>AUTOTUNE = tf.data.AUTOTUNE

resize_and_rescale = keras.Sequential([
  layers.Resizing(224, 224),
  layers.Rescaling(1./255)
])

def prepare(ds, shuffle=False, augment=False):
  # Resize and rescale all datasets.
  ds = ds.map(lambda x, y: (resize_and_rescale(x), y),
              num_parallel_calls=AUTOTUNE)

  if shuffle:
    ds = ds.shuffle(1000)

  # Use data augmentation only on the training set.
  if augment:
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),
                num_parallel_calls=AUTOTUNE)

  # Use buffered prefetching on all datasets.
  return ds.prefetch(buffer_size=AUTOTUNE)


train_ds = prepare(train_ds, shuffle=True, augment=True)
val_ds = prepare(val_ds)
test_ds = prepare(test_ds)
</code></pre>
<p>When I check to see my train_ds size after augmentation, it remains the original size it was before augmentation. Shouldn't it be at <strong>least 4 times</strong> the original size, as <strong>3</strong> data augmentation methods were applied?</p>
<p><strong>Note 1</strong>: I define my datasets as follows:</p>
<pre><code>import tensorflow as tf

# Define data directories
train_dir = 'MyDatabase/train/' 
val_dir = 'MyDatabase/val/'
test_dir = 'MyDatabase/test/'

# Generate datasets
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    )

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    val_dir,
    batch_size=batch_size,
    )

test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    batch_size=batch_size,
    )

</code></pre>
<p><strong>Note 2</strong>: I do not want to add my data_augmentation inside my model's layers, as I want to save the datasets in folders separately rather than apply augmentation every time I run my model.</p>
<p>To see my dataset size, I first use a back-of-the envelope calculation, where I multiply cardinality by batch size:</p>
<pre><code>train_size = tf.data.experimental.cardinality(train_ds).numpy()
print(train_size*batch_size)

val_size = tf.data.experimental.cardinality(val_ds).numpy()
print(val_size*batch_size)

test_size = tf.data.experimental.cardinality(test_ds).numpy()
print(test_size*batch_size)
</code></pre>
<p>The number of images in train_ds is same before and after augmentation.</p>
<p>When I merge the train_ds with an augmneted_ds as follows, I only get <strong>double</strong> the size rather than <strong>4 times</strong> the original dataset size.</p>
<pre><code>augmented_ds  = prepare(train_ds, shuffle=True, augment=True) # augmnet the train_ds
train_ds = prepare(train_ds, shuffle=True, augment=False) #keep the original train_ds
val_ds = prepare(val_ds)
test_ds = prepare(test_ds)

combined_ds = tf.data.Dataset.concatenate(augmented_ds, train_ds)
</code></pre>
<p>And I feel I should not be combining the datasets.</p>
","0","Question"
"77986616","","<pre class=""lang-py prettyprint-override""><code>from llama_index.core.embeddings import resolve_embed_model
</code></pre>
<p>error:</p>
<pre><code>ImportError: cannot import name 'resolve_embed_model' from
 'llama_index.core.embeddings' (/usr/local/lib/python3.10/dist-packages/llama_index/core/embeddings/__init__.py)
</code></pre>
<p>After installing the Llama index , I am getting the error above.</p>
","1","Question"
"77987953","","<p>I need to Crop a square shape around a Centre point (x,y) and I try to create my custom function below.</p>
<pre><code>from PIL import Image

def Crop_Image(image):
    cropsize = 224
    croplength = cropsize/2
    x = Xcentre
    y = Ycentre
    image_crop = image.crop((x-croplength, y-croplength, x+croplength, y+croplength))
    return image_crop
</code></pre>
<p>Then passing Crop_Image function in to the Imagedatagenerator.</p>
<pre><code>datagen = ImageDataGenerator(rescale=1./255,
                             validation_split=0.2,
                             rotation_range=0.2,
                             width_shift_range=0.2,
                             height_shift_range=0.2,
                             shear_range=0.2,
                             zoom_range=0.2,
                             horizontal_flip=True,
                             fill_mode='nearest',
                             preprocessing_function=Crop_Image)
</code></pre>
<p>but in the training process, I have got some error.</p>
<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'crop'
</code></pre>
<p>How can I solve this error?</p>
","0","Question"
"77991202","","<pre><code>#training random forest model
set.seed(8431)
cvforest.train &lt;- train(x = train[, c(3:122, 124:138)], y = train[, 123],  
                        method = &quot;cforest&quot;, metric = &quot;RMSE&quot;,
                        trControl = trainControl(method = &quot;cv&quot;, number = 5),
                        controls = cforest_unbiased(ntree = 1000, minsplit = 5, minbucket = 5)
) 

cvforest.model.train&lt;-cvforest.train$finalModel #final model object

#calculating shapley values
X &lt;- train[, c(3:122, 124:138)]
y &lt;- train[, 123]

predictor &lt;- iml::Predictor$new(cvforest.model.train, data = X, y = y)
shapley_vals &lt;- iml::Shapley$new(predictor)
</code></pre>
<p>Using the code above, I fit a random forest model using the caret and party packages in R. Now I want to calculate Shapley values for this model. However, when I create the 'shapley_vals' object using the code above, I find the 'results' and 'fit' options to be NULL. I'm not sure what steps I'm missing. Any advice on what to do next?</p>
","0","Question"
"77994342","","<p>I have used five machine learning algorithms, including the artificial neural network (ANN), random forest, CatBoost, SVM, and K-nearest neighbors (KNN) for a multiclass classification. How can I compute the computational complexity of these models?</p>
<p>I have also utilized the Power-Law Committee Machine (PLCM) to combine the outputs the five machine learning algorithms as the sixth method. I also need to calculate the computational complexity of this method.</p>
","-1","Question"
"77995524","","<p>I am trying to build my project with cmake, I am using different libraries for this one of which contains the following function:</p>
<pre class=""lang-cpp prettyprint-override""><code>  void forward(const MatrixType&amp; input, MatrixType&amp; output) {
    // Copy the input to the internal state buffer
    states_[0] = input;
    // Compute forward pass through hidden layers
    for (size_t i=0; i &lt; dimensions_.size()-2; i++) {
      states_[i+1] = weights_[i]*states_[i] + biases_[i];
      activation_.evaluate(states_[i+1]);
    }
    // Compute forward pass of the linear output layer
    states_.back() = weights_.back()*states_.end()[-2] + biases_.back();
    // Copy the output state to the provided buffer
    output = states_.back();
  }
</code></pre>
<p>In my own file, <code>RLController.cpp</code>, I am trying to call this function at line 260 as <code>ctrl_-&gt;forward(obScaled_, actScaled_);</code>.
Both <code>obScaled_</code> and <code>actScaled_</code> are defined in my header file as <code>Eigen::Matrix&lt;float, -1, 1&gt;  obScaled_, actScaled_</code> and are set to zero with <code>ctrl.actScaled_.setZero(ctrl.mlp_output_size);</code>.
When I do so I get this error:</p>
<blockquote>
<p>error: cannot bind non-const lvalue reference of type ‘eignets::MultiLayerPerceptron&lt;float, eignets::ActivationType::HyperbolicTangent&gt;::MatrixType&amp;’ {aka ‘Eigen::Matrix&lt;float, -1, -1&gt;&amp;’} to an rvalue of type ‘eignets::MultiLayerPerceptron&lt;float, eignets::ActivationType::HyperbolicTangent&gt;::MatrixType’ {aka ‘Eigen::Matrix&lt;float, -1, -1&gt;’}
257 |     ctrl_-&gt;forward(obScaled_, actScaled_);</p>
</blockquote>
<p>Isn't that exactly how you are supposed to call by reference?</p>
<p>What I find weird is that it used to work but after migration to another folder it stopped working. But still during build there don't seem to be any errors related to dependencies.</p>
<p>For completeness this is the function defined in RLController.cpp where I call this forward function:</p>
<pre class=""lang-cpp prettyprint-override""><code>void RLController::ComputeAction(float obs[], int obs_size){
    // obs is the observation vector from the environment
    // compute the action vector with the observation vector and the ctrl_ model

    // scale the observation vector
    getObMean(obs);
    getObStd(obs);
    for (int i = 0; i &lt; obs_size; ++i) {  
        obScaled_[i] = ((obs[i] - mean)) / std_dev;  
    }

    ctrl_-&gt;forward(obScaled_, actScaled_);

    // convert the action vector to double
    actDouble_ = actScaled_.cast&lt;double&gt;();
    exact_ -&gt; setDTarget(actDouble_); 

}
</code></pre>
<p>forward should just pass <code>obScaled_</code> through a multi layer perceptron that I have already trained and give me 3 action values in <code>actScaled_</code>, that I passed by reference. I am getting stuck at building with cmake. if I cancel out line 260 from RLController.cpp it builds successfully.</p>
","-1","Question"
"77996526","","<p>I have created a <a href=""https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines"" rel=""nofollow noreferrer"">custom prediction routine</a> on Vertex AI, uploaded the model, and am able to generate predictions with it through the UI. Now, I would like to incorporate this into a Vertex AI Pipeline, to run batch predictions after a data generation step. I am using the Kubeflow Pipelines SDK.</p>
<p>To do so, I am thinking of using the <a href=""https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.8.0/api/v1/batch_predict_job.html#v1.batch_predict_job.ModelBatchPredictOp"" rel=""nofollow noreferrer""><code>ModelBatchPredictOp</code></a> prebuilt component. For this to work, I need to import the model into the pipeline, for example with an <a href=""https://www.kubeflow.org/docs/components/pipelines/v2/components/importer-component/"" rel=""nofollow noreferrer"">importer component</a>. HOWEVER, the importer component requires an artifact URI, which my model does not have because it uses a custom container. It is baked into the container image; it is not sitting in GCS.</p>
<p>So I tried, though I did not really expect it to work, writing a quick custom importer component, which returns the model object, but I get a type mismatch. See sample code below:</p>
<pre><code>from helper import data_component
from kfp.dsl import component, pipeline, Output, Model
from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp


@component(packages_to_install=[&quot;google-cloud-aiplatform&quot;])
def custom_importer(model: Output[Model]):
    from google.cloud import aiplatform
    return aiplatform.Model(model_name=&quot;model-id&quot;)


@pipeline(name=&quot;prediction-pipeline&quot;)
def pipeline():
    data_task = data_component()
    
    importer_task = custom_importer()

    batch_predict_op = ModelBatchPredictOp(
        job_display_name=&quot;batch_predict_job&quot;,
        model=importer_task.output,
        gcs_source_uris=data_task.outputs[&quot;dataset&quot;],
        gcs_destination_output_uri_prefix=&quot;bucket&quot;,
        instances_format=&quot;csv&quot;,
        predictions_format=&quot;jsonl&quot;,
        starting_replica_count=1,
        max_replica_count=1,
    )

</code></pre>
<p>The <code>ModelBatchPredictOp</code> doesn't like the input type for arg <code>model</code>: <code>InconsistentTypeException: Incompatible argument passed to the input 'model' of component 'model-batch-predict': Argument type 'system.Model@0.0.1' is incompatible with the input type 'google.VertexModel@0.0.1'</code></p>
<p><strong>How can I incorporate batch prediction from a custom prediction routine into a Vertex AI Pipeline?</strong></p>
","1","Question"
"78004822","","<p>I wanted to create a neural network to predict the hypotenuse of a triangle given the other two sides. For this, I use the pythagorean theorem to create 10,000 values which are used to train the model. The problem is that even though my average loss is 0.18, the accuracy is 0%. What am I doing wrong?</p>
<pre><code>class SimpleMLP(nn.Module):
    def __init__(self, num_of_classes=10):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(2, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            # Output matches input and number of classes
            nn.Linear(64, num_of_classes),
        )

    def forward(self, x):
        return self.layers(x)


class PythagoreanDataset(Dataset):
    def __init__(self, transform=None):
        self.values = self._get_pythagorean_values()

    def __getitem__(self, index):
        a, b, c = self.values[index]
        label = torch.as_tensor([c], dtype=torch.float)
        data = torch.as_tensor([a, b], dtype=torch.float)
        return data, label

    def __len__(self):
        return len(self.values)

    def _get_pythagorean_values(self, array_size: int = 10000) -&gt; list:
        values = []
        for i in range(array_size):
            a = float(randint(1, 500))
            b = float(randint(1, 500))
            c = math.sqrt(pow(a, 2) + pow(b, 2))
            values.append((a, b, c))
        return values


def _correct(output, target):
    predicted_digits = output.argmax(1)  # pick digit with largest network output
    correct_ones = (predicted_digits == target).type(
        torch.float
    )  # 1.0 for correct, 0.0 for incorrect
    return correct_ones.sum().item()


def train(
    data_loader: DataLoader,
    model: torch.nn.Module,
    criterion: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
):
    model.train()

    num_batches = len(data_loader)
    num_items = len(data_loader.dataset)
    train_loss = 0
    total_loss = 0
    total_correct = 0
    for data, target in data_loader:
        # Copy data and targets to device
        data = data.to(device)
        target = target.to(device)

        # Do a forward pass
        output = model(data)

        # Calculate the loss
        loss = criterion(output, target)
        total_loss += loss

        # Count number of correct digits
        total_correct += _correct(output, target)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    train_loss = float(total_loss / num_batches)
    accuracy = total_correct / num_items
    print(f&quot;Train accuracy: {accuracy:.2%}, Average loss: {train_loss:7f}&quot;)
    return train_loss


def test(
    test_loader: DataLoader,
    model: torch.nn.Module,
    criterion: torch.nn.Module,
    device: torch.device,
):
    model.eval()

    num_batches = len(test_loader)
    num_items = len(test_loader.dataset)

    test_loss = 0
    total_correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            # Copy data and targets to GPU
            data = data.to(device)
            target = target.to(device)

            # Do a forward pass
            output = model(data)

            # Calculate the loss
            loss = criterion(output, target)
            test_loss += loss.item()

            # Count number of correct digits
            total_correct += _correct(output, target)

    test_loss = test_loss / num_batches
    accuracy = total_correct / num_items
    print(f&quot;Test accuracy: {100*accuracy:&gt;0.1f}%, average loss: {test_loss:&gt;7f}&quot;)
    return test_loss



def main():
    device = &quot;cpu&quot;

    dataset = PythagoreanDataset()

    # Creating data indices for training and validation splits:
    validation_split = 0.2
    dataset_size = len(dataset)
    indices = list(range(dataset_size))
    split = int(np.floor(validation_split * dataset_size))
    train_indices, val_indices = indices[split:], indices[:split]

    # Creating PT data samplers and loaders:

    train_sampler = SubsetRandomSampler(train_indices)
    valid_sampler = SubsetRandomSampler(val_indices)

    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)
    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=valid_sampler)

    model = SimpleMLP(num_of_classes=1).to(device)
    print(model)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters())

    epochs = 500

    losses = []
    for epoch in tqdm(range(epochs)):
        print(f&quot;Training epoch: {epoch+1}&quot;)
        train_loss = train(train_loader, model, criterion, optimizer, device=device)
        test_loss = test(test_loader, model, criterion, device=device)
        losses.append((train_loss, test_loss))

    plot_loss_curves(losses=losses)

    # Example prediction
    test_input = torch.tensor([[3, 4]], dtype=torch.float32)
    predicted_output = model(test_input)
    print(&quot;Predicted hypotenuse:&quot;, predicted_output.item())
---
</code></pre>
<p><a href=""https://i.sstatic.net/Bb7sI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Bb7sI.png"" alt=""enter image description here"" /></a></p>
","-1","Question"
"78008233","","<p>I'm trying to get the KL divergence between 2 distributions using Pytorch, but the output is often negative which <a href=""https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative"">shouldn't be the case</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch 
import torch.nn.functional as F

x_axis_kl_div_values = []
for epoch in range(200):
    # each epoch generates 2 different distributions
    input_1 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0)
    input_2 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0)

    kl_divergence = F.kl_div(input_1.log(), input_2, reduction='batchmean')
    x_axis_kl_div_values.append(kl_divergence.item())

x_axis_kl_div_values 
&gt;&gt;&gt; 
[324.4713134765625,
 -69.10758972167969,
 -92.42606353759766,
</code></pre>
<p>From the Pytorch forum I found <a href=""https://discuss.pytorch.org/t/kl-divergence-produces-negative-values/16791/1"" rel=""nofollow noreferrer"">this</a> that mentions that their issue was that the inputs were not proper distributions, which is not the case in my code as I'm creating a normal distribution. From <a href=""https://stackoverflow.com/questions/62806681/pytorch-kldivloss-loss-is-negative"">this SO thread</a> it seems like their issue was that the <code>nn.KLDivLoss</code> expects the input to be log-probabiltie, but again, I did that in my code. So I'm not sure what I'm missing</p>
","1","Question"
"78012530","","<p>I am trying to understand the following code:</p>
<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset
dataset = pd.read_csv('winequality-red.csv')

# Extracting features and target variable
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Creating polynomial features
poly_reg = PolynomialFeatures(degree=5)  
X_poly_train = poly_reg.fit_transform(X_train)
X_poly_test = poly_reg.transform(X_test) 

# Fitting Linear Regression to the dataset
regressor = LinearRegression()
regressor.fit(X_poly_train, y_train)

# Predicting the Test set results
y_pred = regressor.predict(X_poly_test)

# Printing the predicted values and actual values
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
</code></pre>
<p>I want to print the predicted values of the dependent variable vs the actual values of y.
I know there is something like inverse transform that needs to be applied but I am unable to find any such method.
This is the dataset: <a href=""https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009/data"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009/data</a></p>
","-1","Question"
"78016312","","<p>Im working with a Keras model in a binary-classification problem, the problem that im facing is that the training dataset has grown so much i can no longer fit it into memory, so i re-write my dataset generation unit to produce many numpy arrays and save them on the disk, (basically splitting the dataset on processable chunks) but what im seeing now  is that the model seems to forget previous training data (since its tweaking its weights to adjust to the new data) so from 26k samples (each file is 500 samples) the model performs just based on the last 500 samples. This is a bit of the code that im using to train:</p>
<pre><code>for fname in input_file_names:
    np_file = np.load(f&quot;{TRAINING_FOLDER}/{fname}&quot;, mmap_mode='r')
    X = np_file['array1']
    y = np_file['array2']

    length_to_use = X.shape[0]
    reached_training_targets += X.shape[0]
    if reached_training_targets &gt; NUM_SAMPLES:
        length_to_use -= (reached_training_targets - NUM_SAMPLES)

    if length_to_use &lt;= 0:
        break

    print(f&quot;Training batch on {length_to_use} samples from file {fname}...&quot;)
    X = X[:length_to_use]
    y = y[:length_to_use]

    rand_idx = np.random.permutation(X.shape[0])
    X = X[rand_idx]
    y = y[rand_idx]

    model.fit(X, y, epochs=EPOCHS, batch_size=32, verbose=2, callbacks=[early_stopping, lr_schedule])
    np_file.close()
</code></pre>
<p>I already tried reducing starting learning rate and different optimizer. This is a new problem for me, since as a few days ago the whole dataset was able to fit in memory, but now, i need to deal with this and i dont know what is the best technique.</p>
","-1","Question"
"78016395","","<p>I have a validation dataset of images to be classified by my CNN model. I want to load these images using <code>pytorch</code>. <code>torchvision.datasets.ImageFolder()</code> function doesn't work, since there are no targets, because the dataset is unclassified. I'm assuming that I need to write a custom dataset class, that I would later put in <code>torch.utils.data.DataLoader()</code>. I've searched online, but I'm still not really understanding how the class should look like.</p>
<p>I've tried this</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset
from torchvision.io import read_image
import os


class Dset(Dataset):
    def __init__(self, dir: str, transform=None) -&gt; None:
        self.transform = transform
        self.images = os.listdir(dir)
        self.dir = dir
    
    def __getitem__(self, index: int) -&gt; torch.Tensor:
        image = read_image(f'{self.dir}/{self.images[index]}')
        if self.transform is not None:
            image = self.transform(image)
        return image

    def __len__(self) -&gt; int:
        return len(self.images)
</code></pre>
<p>But after this cell (all images are in <code>.data/</code>)</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import transforms

batch_size = 64
transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor()])
data = Dset('data', transform=transform)
trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)

images, labels = iter(trainloader)
</code></pre>
<p>I am encountering this error: <code>TypeError: Input image tensor permitted channel values are [1, 3], but found 4</code></p>
<p><strong>Update</strong></p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset
from torchvision.io import read_image, ImageReadMode
import os


class Dset(Dataset):
    def __init__(self, dir: str, transform=None) -&gt; None:
        self.transform = transform
        self.images = os.listdir(dir)
        self.dir = dir
    
    def __getitem__(self, index: int) -&gt; torch.Tensor:
        image = read_image(f'{self.dir}/{self.images[index]}', mode=ImageReadMode.RGB)
        if self.transform is not None:
            image = self.transform(image)
        return image

    def __len__(self) -&gt; int:
        return len(self.images)
</code></pre>
<p>The error was caused by the alpha channel in the images.</p>
<p>After fixing that, I'm encountering this: <code>TypeError: pic should be PIL Image or ndarray. Got &lt;class 'torch.Tensor'&gt;</code></p>
<p><strong>Update 2</strong></p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import transforms

batch_size = 64
transform = transforms.Compose(
[transforms.ToPILImage(), transforms.Resize((512, 512)),
transforms.Grayscale(), transforms.ToTensor()]
)
data = Dset('data', transform=transform)
trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)

images = iter(trainloader)[0]
</code></pre>
<p><code>torchvision.transforms.ToTensor</code> only converts PIL Image or <code>numpy.ndarray</code> to tensor.</p>
<p>Last line results in: <code>TypeError: '_SingleProcessDataLoaderIter' object is not subscriptable</code></p>
","1","Question"
"78019286","","<pre><code>from borb.pdf import Document
from borb.pdf import Page
from borb.pdf import Paragraph
from borb.pdf import Run

pdf_text = &quot;&quot;&quot;
Long text here
divided in various sections
&quot;&quot;&quot;

# Create a PDF document
document = Document()

# Split resume text into sections
sections = pdf_text.split(&quot;\n\n&quot;)

# Iterate over sections and add them to the PDF document
for section in sections:
    paragraph = Paragraph()
    runs = [Run(text=section)]
    paragraph.add(*runs)
    page = Page()
    page.add(paragraph)
    document.add(page)

# Save the PDF document to a file
document.save(&quot;updated.pdf&quot;)



</code></pre>
<p>In the above code I'm trying to insert text into pdf using borb</p>
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-19-d90b0d6013d2&gt; in &lt;cell line: 8&gt;()
      6 from borb.pdf import Paragraph
      7 #from borb.pdf.text.run import Run
----&gt; 8 from borb.pdf import Run
      9 #from borb.pdf.text.run import run
     10 

ImportError: cannot import name 'Run' from 'borb.pdf' (/usr/local/lib/python3.10/dist-packages/borb/pdf/__init__.py)
</code></pre>
<p>But I'm getting the above error, is 'Run' function deprecated ?  and How to solve above error ?
Thanks.</p>
","2","Question"
"78019397","","<p>Is there any way to find the feature importances from a histgradientboosting classifier model in python?</p>
<p>I tried using model.feature_importances_ but the error message was</p>
<pre><code>AttributeError: 'HistGradientBoostingClassifier' object has no attribute 'feature_importances_'
</code></pre>
<p>I wanted to get all the features in a list with percentage of importance of each feature.</p>
","1","Question"
"78021659","","<p>getting the following error when running</p>
<pre><code>depth_estimator = pipeline(task=&quot;depth-estimation&quot;, model=&quot;LiheYoung/depth-anything-base-hf&quot;)
</code></pre>
<blockquote>
<p>ValueError: The checkpoint you are trying to load has model type
<code>depth_anything</code> but Transformers does not recognize this
architecture. This could be because of an issue with the checkpoint,
or because your version of Transformers is out of date.</p>
</blockquote>
","0","Question"
"78024641","","<p><a href=""https://i.sstatic.net/bhuFN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bhuFN.png"" alt=""results"" /></a></p>
<p>for some reason my test accuracy is 0.0</p>
<p>below is my full code</p>
<pre><code>#seperating the date and count
date = data3['visit_date']
count = data3['visit_count']

labels = pd.get_dummies(date)

X_train, X_test, y_train, y_test = train_test_split(count, labels, test_size=0.2, random_state=42)
# Split the data into training and testing sets

# Define the LSTM model architecture
embedding_dim = 100
model = Sequential()
model.add(Embedding(100, embedding_dim, input_length=100))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(len(labels.columns), activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the LSTM model
epochs = 100
batch_size = 640
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(&quot;LSTM Test Accuracy:&quot;, accuracy)
</code></pre>
<p>my input data is like this:<br/>
index / visit_date / visit_count<br/>
0 / 2020-07-09 / 12<br/>
1 / 2020-07-10 / 8<br/>
2 / 2020-07-11 / 14<br/>
3 / 2020-07-12 / 12<br/>
4 / 2020-07-13 / 6<br/>
visit_date is a datetime column while visit_count is int64</p>
<p>I am trying to predict the visit_count based on the visit_date</p>
","0","Question"
"78025319","","<p>I'm using sklearn in python, and cross_val_score() returns an accuracy with maximum 3 decimal places, how can i get the full value?
I get: 0.XXX, but i need: 0.XXXXXXXXX, this only happens with accuracy. Thank you.</p>
<pre><code>cv = RepeatedKFold(n_splits=10, n_repeats=50, random_state=42)

forest = RandomForestClassifier(criterion='log_loss', max_depth=7, max_features='log2', min_samples_leaf=1, min_samples_split=2, n_estimators=10, n_jobs=-1, random_state=42)

scores = cross_val_score(forest, X_phishing, y_phishing, cv=cv, n_jobs=-1)

print(scores)
</code></pre>
<p>Probably a dataset issue, but can't figure out what's wrong.</p>
","0","Question"
"78025623","","<p>I've got the following code. I'll include only the problematic bits:</p>
<pre><code>class C:
    def __init__(self,**kwargs):
        self.kwargs = kwargs

    def fit(...):
        ...
        c = self.learner(**self.kwargs)
</code></pre>
<p>I'm attempting to use this with the following class:</p>
<pre><code>class learner:
    def __init__(self, X):
        self.X = X
</code></pre>
<pre><code>kwargs={'X': X}
inst = C(kwargs=kwargs)
inst.fit(...)
</code></pre>
<p>Which is yielding the error, at the line <code>inst.fit(...)</code>. The error is: <code> learner.__init__() got an unexpected keyword argument 'kwargs'</code>. How do I fix my instantiation? It seems as though the dictionary is not being unpacked, despite employing <code>**</code>. Help would be very appreciated.</p>
","1","Question"
"78026404","","<p>I'm running the following example:</p>
<pre><code>from transformers import TimmBackboneConfig, TimmBackbone

# Initializing a timm backbone
configuration = TimmBackboneConfig(&quot;resnet50&quot;)

# Initializing a model from the configuration
model = TimmBackbone(configuration)

# Accessing the model configuration
configuration = model.config
</code></pre>
<p>from <a href=""https://huggingface.co/docs/transformers/main/en/main_classes/backbones"" rel=""nofollow noreferrer"">here</a></p>
<p>Now I want to use other backbone model from the supported list in the same page,</p>
<ul>
<li>BEiT</li>
<li>BiT</li>
<li>ConvNet</li>
<li>ConvNextV2</li>
<li>DiNAT</li>
<li>DINOV2</li>
<li>FocalNet</li>
<li>MaskFormer</li>
<li>NAT</li>
<li>ResNet</li>
<li>Swin Transformer</li>
<li>Swin Transformer v2</li>
<li>ViTDet</li>
</ul>
<p>where can I find the dictionary from this list, I mean what is the string that I should use for each of the model?</p>
<p>For example:</p>
<ul>
<li>&quot;ResNet&quot; -&gt; 'resnet50'</li>
<li>&quot;BEiT&quot; -&gt; ???</li>
</ul>
<p>I have tried to search for it in the some of the links in the page and in google but didn't find this information anywhere.</p>
","0","Question"
"78027259","","<p>In Python, when you train LSTM model you can train the model on part of the data. Then at inference you can give it whatever input you like for example 10 recent timesteps that is not part of the training set. It will produce the output. Now can ARIMA operate in the same way? Can we give it input sequence ? Or does it use the training data to predict next steps ?</p>
<p>Below is my code:</p>
<pre><code>import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import torch
import sys
import math

# Read the dataset from CSV file
df = pd.read_csv('sm_data.csv', header=None)

# Take the first 102 rows as training data and the rest as test data
train_data = df.iloc[:102, :]
test_data = df.iloc[102:, :]


# Iterate over each trend
forecast_results = {}
for column in df.columns:
    # Fit ARIMA model on training data
    model = ARIMA(train_data[column], order=(10,1,0))
    model_fit = model.fit()

    # Forecast 36 months ahead
    forecast = model_fit.forecast(steps=36)

    # Store forecast results
    forecast_results[column] = forecast

# Convert forecast results to DataFrame
forecast_df = pd.DataFrame(forecast_results)

# Save forecast results to CSV
forecast_df.to_csv('forecast_results.csv', index=False)
</code></pre>
","1","Question"
"78029057","","<p>I am trying to train a machine learning image classification model however i keep recieving a value error.</p>
<p>This is the code</p>
<pre><code>model.compile(
  optimizer=&quot;adam&quot;,
  loss='sparse_categorical_crossentropy',
  metrics=['acc'])
history = model.fit(np.array(X_train), np.array(y_train), epochs=5, validation_data=(X_val, y_val))
</code></pre>
<p>The error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
/var/folders/hc/jd3r3_g12355379n5gx9cy7m0000gn/T/ipykernel_18704/2140074756.py in &lt;module&gt;
      3   loss='sparse_categorical_crossentropy',
      4   metrics=['acc'])
----&gt; 5 history = model.fit(np.array(X_train), np.array(y_train), epochs=5, validation_data=(X_val, y_val))

/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/data_adapter.py in _check_data_cardinality(data)
   1958             )
   1959         msg += &quot;Make sure all arrays contain the same number of samples.&quot;
-&gt; 1960         raise ValueError(msg)
   1961 
   1962 

ValueError: Data cardinality is ambiguous:
  x sizes: 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96
  y sizes: 308
Make sure all arrays contain the same number of samples.
</code></pre>
<p>However when i print the length of each array i am using they are all the same size?</p>
<pre><code>print(len(y_train))
3693
</code></pre>
<pre><code>print(len(X_train))
3693
</code></pre>
<p>The shape of both produce:</p>
<pre><code>np.array(X_train).shape
(3693, 96, 96, 3)

np.array(y_train).shape
(3693,)
</code></pre>
<p>How can I fix this?</p>
","1","Question"
"78031144","","<p>I am learning ML NEAT with Neataptic within NodeJS.</p>
<p>I am making a basic weather prediction model. Essentially predict the next temperature.</p>
<p>I have this type of data, where I want to give samples of the last 3 days with 2 features date and avgTemp.</p>
<pre><code>      {
        &quot;inputs&quot;: [
          {
            &quot;AvgTemperature&quot;: &quot;47.9&quot;,
            &quot;Date&quot;: &quot;1/1/2020&quot;
          },
          {
            &quot;AvgTemperature&quot;: &quot;47.8&quot;,
            &quot;Date&quot;: &quot;1/2/2020&quot;
          },
          {
            &quot;AvgTemperature&quot;: &quot;48.8&quot;,
            &quot;Date&quot;: &quot;1/3/2020&quot;
          },
        ],
        &quot;expectedOutput&quot;: {
          &quot;AvgTemperature&quot;: &quot;46.9&quot;
        }
      },
    ]

</code></pre>
<p>I am unsure on what format the network is expecting.</p>
<p>these are a few of the ideas I had:</p>
<pre><code>genom.activate([
[47.9, 1/1/2020],
[50, 1/2/2020],
[55, 1/3/2020]
])

with a network setup of maybe 
/*
const inputLen = 3 //To represent 3 samples
//or
const inputLen = 2 //To represent 2 features
const inputLen = [3,2] //To represent 3 samples and 2 features
*/
            const outputLen = 1

            this.neat = new Neat(
                inputLen,
                outputLen,
                null,
                {
                    elitism: 5,
                    popsize: 50,
                    mutationRate: 0.3,
                    network: new architect.Random(inputLen, 3, outputLen),
                }
            );


</code></pre>
<p>or which I think is most likely, I need to flatten the inputs. And if that's the case, how should I flatten it, like either of the below. and if I do need to flatten it I assume the would be the features*samples=inputLen
And how will it know which are different features and which are like via the date, or does that not matter it will just find its own way of related the temps and dates.</p>
<pre><code>genom.activate([date, temp, date, temp ...])
//or 
genom.activate([date, date, temp, temp ...])  
</code></pre>
<p>---- SIDE NOTES ----</p>
<ul>
<li>Yes I will be normalizing my input values</li>
<li>I have about 600 blocks of samples I will be feeding it.</li>
<li>I did google this and looked on this site. Though I couldn't find much on neataptic spesfically and on wether its required to flatten the data, more so I found just how to .flatten() an array.</li>
</ul>
","0","Question"
"78037608","","<p>In the code provided below I am to visualize the results of SHAP values of a random forest model.</p>
<p>The code is in R and it is shown below:</p>
<pre><code># Load necessary libraries
library(randomForest)
library(DALEX)
library(beeswarm)

data &lt;- my_database

# Splitting the data into features and target
features &lt;- data[, -which(names(data) %in% &quot;Clus.1&quot;)]
target &lt;- data$Clus.1

# Train a random forest model
rf_model &lt;- randomForest(features, target)

# Create an explainer object
explainer &lt;- DALEX::explain(rf_model, data = features, y = target)

# Compute SHAP values
shapley_values &lt;- DALEX::predict_parts(explainer, new_observation = features)

# Plot bee swarm
beeswarm(shapley_values$shap_1)
</code></pre>
<p>I have tried to use <code>beeswarm package</code></p>
<p>and I ended up with this error:</p>
<pre><code>beeswarm(shapley_values$shap_1)
Error in rep(nms, sapply(x, length)) : invalid 'times' argument
</code></pre>
<p>Can you please suggest me what is wrong about the <code>beeswarm</code>, or other  similiar packages?</p>
<p><a href=""https://imgur.com/pFGolIs"" rel=""nofollow noreferrer"">Output of what I am trying to do</a></p>
<p><a href=""https://imgur.com/oAgi3GV"" rel=""nofollow noreferrer"">And this is the output I am getting</a> if I use <code>plot(shapley_values)</code></p>
","1","Question"
"78039476","","<p>I am running a Machine Learning model on an ESP32 which operates with Micropython. To infer the model, I trained a Random Forest Classification Model within Python using SciKitLearn library. To export the model, I used the <a href=""https://github.com/BayesWitnesses/m2cgen"" rel=""nofollow noreferrer"">m2cgen</a> library which converts the model to plain python code.</p>
<p>The resulting python file is approximately 873 KB in size and looks like the following:</p>
<pre><code>def add_vectors(v1, v2):
    return [sum(i) for i in zip(v1, v2)]
def mul_vector_number(v1, num):
    return [i * num for i in v1]
def score(input):
    if input[31] &lt;= 14182.0:
        if input[12] &lt;= -7552.0:
            if input[43] &lt;= -2098.0:
                if input[49] &lt;= -2522.0:
                    if input[58] &lt;= -4395.5:
                        if input[3] &lt;= 4716.5:
                            if input[45] &lt;= 4954.0:
                                if input[34] &lt;= 1353.5:
                                    var0 = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]
                                else:
                                    var0 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
                            else:
                                var0 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
                        else:
                            var0 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]

[...]

return mul_vector_number(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(add_vectors(var46, var47), var48), var49), var50), var51), var52), var53), var54), var55), var56), var57), var58), var59), var60), var61), var62), var63), var64), var65), var66), var67), var68), var69), var70), var71), var72), var73), var74), var75), var76), var77), var78), var79), var80), var81), var82), var83), var84), var85), var86), var87), var88), var89), var90), var91), var92), var93), var94), var95), var96), var97), var98), var99), var100), 0.01)

</code></pre>
<p>I then want to store this file on an ESP32 microcontroller, flashed with MicroPython, in order to infer the model.</p>
<p>When uploading the file into the Flash memory of the ESP32, everything works fine. (Flash size is around 4 MB, of which 2 MB is available for the user). Then I tried to import the file in the main.py file on the ESP32 to utilize the score function.</p>
<pre><code>from rf_model import score
</code></pre>
<p>However, when running the script, the program throws the following error:</p>
<pre><code>MemoryError: Memory allocation failed, allocating 136 bytes
</code></pre>
<p>How to fix this issue?</p>
","0","Question"
"78040679","","<p>I have 2 datasets which make up the (x,y) coordinates of 2 sine curves and their respective outputs.
The sine curves are concentric.</p>
<p>The bigger sine curve has an output label of 1 and the smaller sine curve an output label of -1.</p>
<p>I have to train a model which will take in a new (x,y) coordinate and output either -1 or +1, depending on which curve it might be a part of.</p>
<p>Now I have prepared the data like so-</p>
<pre><code>#generate data_set - sin wave 1 and sin wave 2

def wave_Lower_training_data(n = 300):
  #generate random input points for wave 1
  X1 = random.uniform(0,2*pi,n) #generates 'n' radian values b/w 0 to 2pi
  #wave output
  X2 = sin(X1) #lower curve
  X1 = X1.reshape(n, 1)
  X2 = X2.reshape(n,1)
  X = hstack((X1,X2))
  # y = -ones((n,1))
  y = -ones((n,1))

  return X,y

def wave_Higher_training_data(n = 300):
  #generate random input points for wave 1
  X1 = random.uniform(0,2*pi,n) #generates 'n' radian values b/w 0 to 2pi
  #wave output
  X2 = 2*sin(X1) #higher curve
  X1 = X1.reshape(n, 1)
  X2 = X2.reshape(n,1)
  X = hstack((X1,X2))
  y = ones((n,1))

  return X,y

X1, y1 = wave_Lower_training_data()
X2, y2 = wave_Higher_training_data()

# print(X1)

# Combine the training data
X_combined = vstack((X1, X2))
y_combined = vstack((y1, y2))




# print(&quot;Combined X shape:&quot;, X_combined)
# print(&quot;Combined y shape:&quot;, y_combined)
</code></pre>
<p>And this is how I've attempted to model-</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        # Define model
        self.fc1 = nn.Linear(2, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 16)
        self.fc4 = nn.Linear(16,1)
        self.tanh = nn.Tanh()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.tanh(self.fc4(x))
        return x

# Convert training data into tensors usable by PyTorch
X_tensor = torch.tensor(X_combined, dtype=torch.float32)
y_tensor = torch.tensor(y_combined, dtype=torch.int64)
print(y_tensor)

# Splitting of training data into 80-20
X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=786)

model = Model()
criterion = nn.HingeEmbeddingLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Training loop
n_epochs = 10000
for epoch in range(0, n_epochs):
    optimizer.zero_grad()
    output = model(X_train)
    loss = criterion(output.squeeze(), y_train.long())
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f&quot;Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}&quot;)

with torch.no_grad():
    y_pred = model(X_val)
    y_pred_class = torch.sign(y_pred)
    accuracy = (y_pred_class.squeeze().long() == y_val.long()).sum().item() / len(y_val)
    print(f&quot;Validation Accuracy: {accuracy}&quot;)

</code></pre>
<p>As i inspect the the loss percentage every 100 epochs, it is clear that after a point, It is stagnant?</p>
<p>Then when i get to the prediction part-</p>
<pre><code>#predictions
def get_user_input():
    them to a list:
    feature1 = float(input(&quot;Enter the first feature value: &quot;))
    feature2 = float(input(&quot;Enter the second feature value: &quot;))
    return [feature1, feature2]

# Get user input
X_new = get_user_input()
X_ip = torch.tensor(X_new, dtype=torch.float32)
y_pred = model(X_ip)

print(y_pred)


pyplot.figure(figsize=(10, 6))
pyplot.scatter(X1[:, 0], X1[:, 1], label='Wave 1', color='blue', s=10)
pyplot.scatter(X2[:, 0], X2[:, 1], label='Wave 2', color='red', s=10)
user_x = X_new[0]  # Example x-coordinate for user input
user_y = X_new[1]  # Example y-coordinate for user input
pyplot.scatter(user_x, user_y, color='green', label='User Input Point')

pyplot.xlabel('Angle (radians)')
pyplot.ylabel('Amplitude')
pyplot.title('Sine Waves with Amplitudes 1 and 2')
pyplot.legend()
pyplot.show()
</code></pre>
<p><a href=""https://i.sstatic.net/9MDnU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9MDnU.png"" alt=""enter image description here"" /></a></p>
<p>This is the output I get-</p>
<pre><code>Enter the first feature value: 1.9
Enter the second feature value: 2
tensor([-1.], grad_fn=&lt;TanhBackward0&gt;)
</code></pre>
<p>From the graph it is very clear that the output should be [+1], as it resembled the upper curve. But i still get [-1]?</p>
<p>I always get [-1] no matter what input I give.</p>
<p>Any help?</p>
<p><strong>EDIT</strong> : <strong>I've tried the same using BCELoss() and it seems to work. But I need to map it to {-1,1} and not {0,1}.</strong></p>
","1","Question"
"78040978","","<p>I'm trying to fine-tune llama2-13b-chat-hf with an open source datasets.</p>
<p>I always used this template but now I'm getting this error:</p>
<p>ImportError: Using <code>bitsandbytes</code> 8-bit quantization requires Accelerate: <code>pip install accelerate</code> and the latest version of bitsandbytes: <code>pip install -i https://pypi.org/simple/ bitsandbytes</code></p>
<p>I installed all the packages required and these are the versions:</p>
<pre><code>    accelerate @ git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344
    bitsandbytes==0.42.0
    datasets==2.17.1
    huggingface-hub==0.20.3
    peft==0.8.2
    tokenizers==0.13.3
    torch==2.1.0+cu118
    torchaudio==2.1.0+cu118
    torchvision==0.16.0+cu118
    transformers==4.30.0
    trl==0.7.11
</code></pre>
<p>Anyone know if that is a version issues?
How did you fix that?</p>
<p>I tried to install other version but nothing.</p>
","5","Question"
"78041279","","<p>I'm encountering an issue with excessive GPU RAM consumption while training a large language model on a relatively small dataset. Despite using only 200 rows of data, the training process consumes around 40 GB of RAM on an Nvidia A100 GPU, which seems disproportionately high for the dataset size.</p>
<p>Environment:</p>
<ol>
<li>Model: vilsonrodrigues/falcon-7b-instruct-sharded (a variant of a
large language model with 7 billion parameters)</li>
<li>Dataset Size: 200 rows</li>
<li>GPU: Nvidia A100</li>
<li>Framework: PyTorch &amp; Google Colab,</li>
<li>Transformers library by Hugging Face (specify version)</li>
<li>Training Configuration:</li>
<li>Batch size: 1
<ul>
<li>Gradient accumulation steps: 16</li>
<li>Mixed precision (FP16) enabled</li>
</ul>
</li>
</ol>
<p><strong>Code: <a href=""https://colab.research.google.com/drive/1TNra_fwJbQ9M3FsFB1z8sniOxLzkDJfo?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1TNra_fwJbQ9M3FsFB1z8sniOxLzkDJfo?usp=sharing</a></strong></p>
<p>Issue:
The training consumes around 40 GB of GPU RAM, which seems excessive for the small size of the dataset and the training configuration used. I've already employed strategies such as reducing the batch size, enabling mixed precision training, and using gradient accumulation to manage memory usage, but the issue persists.</p>
<pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 240.38 MiB is free. Process 36185 has 15.54 GiB memory in use. Of the allocated memory 15.19 GiB is allocated by PyTorch, and 43.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>Questions:</p>
<ul>
<li>Are there any recommended strategies to further reduce GPU RAM usage for training large language models on small datasets?</li>
<li>Could there be potential misconfigurations or inefficiencies in my training setup that I might be overlooking?</li>
<li>Is there a way to optimize the utilization of the A100 GPU for such training tasks to prevent excessive memory usage?</li>
</ul>
","-2","Question"
"78043661","","<p>At the start of my application, I initialize a connection to WeightsAndBiases via</p>
<pre class=""lang-py prettyprint-override""><code>run = wandb.init(project=&quot;...&quot;)
</code></pre>
<p>Later on, I would like to check if a <code>wandb</code> run has been initialized.
Is there a way to do this without passing the <code>run</code> object around?</p>
","0","Question"
"78044014","","<p>I am trying to train a model that has two outputs with gradient descent. My cost function therefore returns two errors. What is the typical way to deal with this problem?</p>
<p>I've seen mentions here and there of this problem, but I haven't come up with a satisfactory solution.</p>
<p>This is a toy example that reproduces my problem:</p>
<pre><code>from jax import jit, random, grad
import optax


@jit
def my_model(forz, params):
    a, b = params

    a_vect = a + forz**b
    b_vect = b + forz**a

    return a_vect, b_vect*50.


@jit
def rmse(predictions, targets):

    rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))
    return rmse


@jit
def my_loss(forz, params, true_a, true_b):

    sim_a, sim_b = my_model(forz, params)

    loss_a = rmse(sim_a, true_a)
    loss_b = rmse(sim_b, true_b)

    return loss_a, loss_b


grad_myloss = jit(grad(my_loss, argnums=1))

# synthetic true data
key = random.PRNGKey(758493)
forz = random.uniform(key, shape=(1000,))

true_params = [8.9, 6.6]
true_a, true_b = my_model(forz, true_params)

# Train
model_params = random.uniform(key, shape=(2,))
optimizer = optax.adabelief(1e-1)
opt_state = optimizer.init(model_params)

for i in range(1000):

    grads = grad_myloss(forz, model_params, true_a, true_b)  # this fails
    updates, opt_state = optimizer.update(grads, opt_state)
    model_params = optax.apply_updates(model_params, updates)
</code></pre>
<p>I understand that either the two errors has to be somehow aggregated to a single one implementing some kind of normalization to the losses (my output vectors have non-comparable units),</p>
<pre><code>@jit
def normalized_rmse(predictions, targets):
   std_dev_targets = jnp.std(targets)
   rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))
   return rmse/std_dev_targets


@jit
def my_loss_single(forz, params, true_a, true_b):

   sim_a, sim_b = my_model(forz, params)

   loss_a = normalized_rmse(sim_a, true_a)
   loss_b = normalized_rmse(sim_b, true_b)

   return jnp.sqrt((loss_a ** 2) + (loss_b * 2)) 
</code></pre>
<p>or I should use the Jacobian matrix (<code>jacrev</code>) somehow?</p>
","1","Question"
"78046996","","<p>I make a dataframe in my code and fill it in with data. Then in another function I call on that data and send it to a file. Only, when I call on it later and try to send its header to a file I get this error:</p>
<pre><code>    column_names = clsResults.columns.tolist()
                   ^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'columns'
</code></pre>
<p>How can this be possible? I assigned freakin headers to it when I made the dataframe!</p>
<p>Here is me making the frame and filling it with content:</p>
<pre><code>
def growClassifier(NUMTREES: int, DEPTH: int, X: pd.DataFrame , y: np.ndarray):
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True)

    print(f'\nBuilding classification forest with {NUMTREES} trees each {DEPTH} deep\n')

    # Initialize the week classifier 
    base_estimator = DecisionTreeClassifier(max_depth=DEPTH)

    start_time = time.time()

    ada = AdaBoostClassifier(estimator=base_estimator, n_estimators=NUMTREES)
    ada.fit(X_train, y_train)

    elapsed_time = time.time() - start_time

    # Use staged_predict to get staged predictions
    staged_test_predictions = ada.staged_predict(X_test)
    

    f1_test, accuracy_test, precision_test, recall_test, buildtime_test = [], [], [], [], []
    # Iterate over staged predictions and evaluate performance at each stage
    for i, y_pred in enumerate(staged_test_predictions, start=1):
        # print(f'i:{i}\ny_pred:{y_pred}')
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_test.append(accuracy)
        
        precision = precision_score(y_test, y_pred, average=&quot;weighted&quot;, zero_division=0)
        precision_test.append(precision)

        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        recall_test.append(recall)

        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        f1_test.append(f1)

        # print(f'accuracy:{accuracy}')

    adaClsResults = pd.DataFrame()

    numTrees, treeDepth = [], [] 
    for x in range(1, NUMTREES+1, 1):
        # print(i, x)
        numTrees.append(x) 
        treeDepth.append(DEPTH)
        buildtime_test.append(elapsed_time)

    if (i&gt;40): 
        while (i &lt; NUMTREES):
            accuracy_test.append(0)
            precision_test.append(0)
            recall_test.append(0)
            f1_test.append(0)
            buildtime_test[i] = 0
            i += 1

        adaClsResults['numTrees'] = numTrees
        adaClsResults['treeDepth'] = treeDepth
        adaClsResults['f1'] = f1_test
        adaClsResults['accuracy'] = accuracy_test
        adaClsResults['precision'] = precision_test
        adaClsResults['recall'] = recall_test
        adaClsResults['buildTime'] = buildtime_test
        return adaClsResults

    else: 
        print(f'\n\nfailed. Only boosted {i} times. Did not have  {NUMTREES} stages. running again \n\n')
        growClassifier(NUMTREES, DEPTH, X, y)
</code></pre>
<p>Ideas about how to improve the code would help also but my main concern is how come when I try to access my object later in the below function. I get the error I mentioned above.</p>
<pre><code>
def classificationRuns(model: str, task: str, allDatasets: list, clsDatasets: list, ESTNUM: int, startDEPTH: int, endDEPTH: int, MAX_RUNS: int, rawDataPath: str, aggDataPath: str ):

    for dataset in allDatasets:
        if dataset in clsDatasets: 

            # Get the data 
            X,y = parse.getClsData(dataset)
            while startDEPTH &lt; endDEPTH: 

                runNumber = 1
                while (runNumber &lt; MAX_RUNS + 1):
                    print(f'\nRun number:\t{runNumber}')

                    # run forest building
                    clsResults = growClassifier(ESTNUM, startDEPTH, X, y)

                    column_names = clsResults.columns.tolist()

                    # Join column names with tab separators
                    header = '\t'.join(column_names)

                    # Set file name system for raw data
                    saveRawDataHere = os.path.join(rawDataPath, dataset, f'_{ESTNUM}_{startDEPTH}_{dataset}_{model}_{task}_')

                    # add header to raw and agg file
                    with open(saveRawDataHere, 'a') as raw_file:
                        if isEmpty(saveRawDataHere):
                            raw_file.write(f&quot;{header}\n&quot;) 
                    
                    # Set file name system for agg data
                    saveAggDataHere = os.path.join(aggDataPath, f'_{dataset}_{model}_{task}_')
                    
                    # add header to agg data file 
                    with open(saveAggDataHere, 'a') as agg_file:
                        if isEmpty(saveAggDataHere):
                            agg_file.write(f&quot;{header}\n&quot;)

                    
                    # write data to file
                    print(f'saving data in {saveRawDataHere}')
                    clsResults.to_csv(saveRawDataHere, mode='a', index=False, header=False, sep='\t')
                    # increment counter    
                    runNumber += 1
                startDEPTH +=1
</code></pre>
<p>The main concern is</p>
<pre><code>                    clsResults = growClassifier(ESTNUM, startDEPTH, X, y)

                    column_names = clsResults.columns.tolist()
</code></pre>
<p>This is where the error occurs.</p>
<p>I've tried giving it a header when I first create it by</p>
<pre><code>    adaClsResults = pd.DataFrame(columns=X_train.colums)
</code></pre>
<p>but still end up getting the same error.</p>
","-2","Question"
"78048772","","<p>I'm working on an optimization problem for a refrigeration system using the Gekko Python package, and I'm facing issues with integrating Gekko Linear Regression models (Gekko_LinearRegression_Modified) into my optimization model. Specifically, I encounter an error related to the definition of intermediate variables without equality expressions when I try to update costs based on predictions and set constraints based on the model's predictions. Below is a simplified version of my code:</p>
<pre><code>from gekko import GEKKO
from gekko import ML
from gekko.ML import Gekko_GPR, Gekko_SVR, Gekko_LinearRegression
# Initialize Gekko model

m = GEKKO(remote=False)  # 'remote=False' for local execution

price_vec_list = [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 1, 1, 1] # Price vector

# Create a Gekko array of parameters
price_vec_gekko = m.Array(m.Param, len(price_vec_list))
for i, val in enumerate(price_vec_list):
    price_vec_gekko[i].value = val

coolLoad = m.Const(3) # Cooling load
Q_Spc = m.Const(5) # Storage size
n = len(price_vec_gekko)  # Directly use the length of the price vector

T_brine_in = m.Array(m.Param, n) # Brine inlet temperature
for i in range(n):
    T_brine_in[i].VALUE = 25

ST_V = m.Array(m.MV, n) # Compressor control variable
for i in range(n):
    ST_V[i].VALUE = 0.5
    ST_V[i].LOWER = 0.43  # Correct property for lower limit
    ST_V[i].UPPER = 1  # Correct property for upper limit
    ST_V[i].status = 1  # Allow the solver to optimize this variable

coolLoad_sum_min = np.cumsum(np.full(n, coolLoad)) 
coolLoad_sum_max = coolLoad_sum_min + Q_Spc # Cold water storage cap

costs = m.Var(value=0)  # Initialize cost variable as a Gekko variable`

# Modify the Gekko_LinearRegression class initialization to handle scalar intercepts and coefficients
class Gekko_LinearRegression_Modified(Gekko_LinearRegression):
    def __init__(self, model, m):
        self.m = m
        # Check if coef_ is an array and handle accordingly
        if hasattr(model.coef_, &quot;__len__&quot;):
            self.coef = model.coef_[0] if len(model.coef_) &gt; 1 else model.coef_
        else:
            self.coef = model.coef_
        # Check if intercept_ is a scalar and handle accordingly
        self.intercept = model.intercept_ if np.isscalar(model.intercept_) else model.intercept_[0]


    
# Using the modified class for predictions directly in equations
for i in range(n):
    # Directly use the prediction in the equation, avoid intermediate if not necessary
    costs_increment = Gekko_LinearRegression_Modified(MLA_LR_P, m).predict([ST_V[i], T_brine_in[i]]) / 1000 * price_vec_gekko[i]
    Qo_constraint = Gekko_LinearRegression_Modified(MLA_LR_Q, m).predict([ST_V[i], T_brine_in[i]]) / 1000

    # Update costs based on predictions and prices
    m.Equation(costs == costs + costs_increment)

    # Constraints based on Qo_Gekko_Model, ensure these are properly defined as equations
    m.Equation(Qo_constraint &lt; coolLoad_sum_max[i])
    m.Equation(Qo_constraint &gt; coolLoad_sum_min[i])        

   
m.Minimize(costs)  # Minimize
m.options.SOLVER = 1  # APOPT Solver
m.options.IMODE = 1
# Solve model
m.solve(disp=True)
</code></pre>
<p>The error message is:</p>
<pre><code>APMonitor, Version 1.0.0
APMonitor Optimization Suite


 @error: Intermediate Definition
 Error: Intermediate variable with no equality (=) expression
   (((p25)*(2487.122166666668))-766.6071363636368)]
 STOPPING...

Exception                                 Traceback (most recent call last)
Cell In[14], line 81
     79 m.options.IMODE = 1
     80 # Solve model
---&gt; 81 m.solve(disp=True)
     83 # Output results
     84 print(f'Optimized ST_V: {ST_V.VALUE[0]}')

File ~\anaconda3\lib\site-packages\gekko\gekko.py:2140, in GEKKO.solve(self, disp, debug, GUI, **kwargs)
   2138         print(&quot;Error:&quot;, errs)
   2139     if (debug &gt;= 1) and record_error:
-&gt; 2140         raise Exception(apm_error)
   2142 else: #solve on APM server
   2143     def send_if_exists(extension):

Exception: @error: Intermediate Definition
 Error: Intermediate variable with no equality (=) expression
   (((p25)*(2487.122166666668))-766.6071363636368)]
 STOPPING...
</code></pre>
<p>If I don't use the class Gekko_LinearRegression_Modified, then I get this error</p>
<pre><code>IndexError                                Traceback (most recent call last)
Cell In[15], line 64
     35 # Assuming MLA_LR_P and MLA_LR_Q are already defined and trained linear regression models
     36 #for i in range(n):
     37  #   Pel_Gekko_Model = Gekko_LinearRegression(MLA_LR_P, m).predict([ST_V[i], T_brine_in[i]])
   (...)
     60         
     61 # Using the modified class for predictions directly in equations
     62 for i in range(n):
     63     # Directly use the prediction in the equation, avoid intermediate if not necessary
---&gt; 64     costs_increment = Gekko_LinearRegression(MLA_LR_P, m).predict([ST_V[i], T_brine_in[i]]) /     1000 * price_vec_gekko[i]
     65     Qo_constraint = Gekko_LinearRegression(MLA_LR_Q, m).predict([ST_V[i], T_brine_in[i]]) / 1000
     67     # Update costs based on predictions and prices

File ~\anaconda3\lib\site-packages\gekko\ML.py:838, in Gekko_LinearRegression.__init__(self, model, m)
    836 self.m = m
    837 self.coef = model.coef_[0]
--&gt; 838 self.intercept = model.intercept_[0]

IndexError: invalid index to scalar variable.
</code></pre>
","2","Question"
"78049931","","<p>I following this tutorial.
<a href=""https://colab.research.google.com/github/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/Tutorial3.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/Tutorial3.ipynb</a>
I want to try it on custom dataset. How do I use my own dataset instead of Cora ? I would love any idea.</p>
","0","Question"
"78051902","","<p>I am trying to import <code>SFTTrainer</code> from <code>trl</code> package. I am getting the error message - <code>ImportError: cannot import name 'override' from 'typing_extensions' (D:\...\..\)</code>. <strong>I am able to do it perfectly well in Google Colab, no issues</strong>. Why is this? How can I import on my local PC/ <code>Jupyter</code>?</p>
<p>I installed the <code>trl</code> package and tried importing <code>SFFTrainer</code> using:</p>
<pre><code>!pip install trl
from trl import SFTTrainer
</code></pre>
<p>I tried uninstalling and reinstalling <code>typing-extensions</code>, that did not work. I uninstalled <code>trl</code> and tried installing it from <code>Anaconda powershell prompt</code> and I tried a few other installations, updates and uninstallations. I even tried importing <code>trl</code> from <code>github</code> and then load. None of them is working.</p>
<p>How to resolve this?</p>
","0","Question"
"78053061","","<p>I have imported a Kaggle dataset containing a large collection of X-ray images. I aim to detect the lung regions and the outer part of the rib cage, which I want to turn black. Initially, I attempted an approach where it would outline the rib cage, as it is the brightest connected part covering the lungs. However, the dataset images present an issue with labeling L and R text on the side. To mitigate this, I cropped a 10% margin to avoid the texts and then applied my algorithm. Unfortunately, I did not achieve the desired output. Can you help me improve my algorithm or suggest another approach to separate both lungs?  I prefer not to use any pre-trained models.</p>
<p>Here is the code:</p>
<pre><code>import cv2
import numpy as np
import matplotlib.pyplot as plt

def crop_xray(image_path, margin_percentage=10):

    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    height, width = image.shape
    margin_x = int(width * margin_percentage / 100)
    margin_y = int(height * margin_percentage / 100)

    cropped_image = image[margin_y:height-margin_y, margin_x:width-margin_x]

    return cropped_image, image

def add_outline(image):
    
    blurred = cv2.GaussianBlur(image, (5, 5), 0)
    
    edges = cv2.Canny(blurred, 30, 150)
    
    kernel = np.ones((5, 5), np.uint8)
    closed_edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
    
    dilated = cv2.dilate(closed_edges, kernel, iterations=3)
    
    dilated = cv2.bitwise_not(dilated)
    
    _, mask = cv2.threshold(dilated, 0, 255, cv2.THRESH_BINARY)
    
    outlined_image = cv2.bitwise_and(image, image, mask=mask)
    
    return outlined_image

xray_image_path = &quot;/kaggle/input/chest-xray-pneumonia/chest_xray/test/NORMAL/IM-0075-0001.jpeg&quot;

cropped_image, original_image = crop_xray(xray_image_path)

outlined_image = add_outline(cropped_image)

plt.figure(figsize=(15, 5))

# Original image
plt.subplot(1, 3, 1)
plt.imshow(original_image, cmap='gray')
plt.title('Original Image')
plt.axis('on')

# Cropped image
plt.subplot(1, 3, 2)
plt.imshow(cropped_image, cmap='gray')
plt.title('Cropped Image with 10% Margin')
plt.axis('on')

# Outlined image
plt.subplot(1, 3, 3)
plt.imshow(outlined_image, cmap='gray')
plt.title('Outlined Image')
plt.axis('on')

plt.show()
</code></pre>
<p>output i got
<a href=""https://i.sstatic.net/RcErf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RcErf.png"" alt=""enter image description here"" /></a></p>
<p>desired output [somewhat this, can be different but should detect the lungs region]
<a href=""https://i.sstatic.net/K5n0k.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/K5n0k.jpg"" alt=""enter image description here"" /></a></p>
","0","Question"
"78056806","","<p>I'm making a regression model with Tensorflow, but when I use <code>tf.keras.metrics.R2Score()</code> as a metric, it fails with <code>ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;</code> after the first epoch. (But works fine up until then) However, if I use a different metric (<code>tf.keras.metrics.RootMeanSquaredError()</code>, it works fine.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

weather_states = pd.read_sql(&quot;SELECT stations.id, stations.capacity_kw, start, wind_speed_10m, wind_direction_10m, wind_speed_80m, wind_direction_80m, wind_speed_180m, wind_direction_180m FROM stations INNER JOIN weather_states ON stations.id = weather_states.station WHERE weather_states.source = 'openmeteo_forecast/history/best' AND stations.source = 'wind'&quot;, db_client)
 
grid_states = pd.read_sql(&quot;SELECT start, wind FROM grid_states&quot;, db_client)
 
def create_x_y(df: tuple[Any, pd.DataFrame]):
    start = df[1][&quot;start&quot;].iloc[0]
    res = df[1].sort_values(&quot;id&quot;).drop([&quot;id&quot;, &quot;start&quot;], axis=1)
    temp_wind = grid_states.loc[grid_states[&quot;start&quot;] == start][&quot;wind&quot;].to_list()
    wind_kw = temp_wind if len(temp_wind) &gt;= 1 else None
    res_flat_df = pd.DataFrame(res.to_numpy().reshape((1, -1)))
    res_flat_df[&quot;wind_kw&quot;] = wind_kw
    return res_flat_df
 
data = pd.concat(map(create_x_y, weather_states.groupby(&quot;start&quot;))).dropna()
from sklearn.model_selection import train_test_split
 
 
data = data.astype(&quot;float32&quot;)
train, test = train, test = train_test_split(data.dropna(), test_size=0.2)
 
train_y = train.pop(&quot;wind_kw&quot;)
train_x = train
 
 
test_y = test.pop(&quot;wind_kw&quot;)
test_x = test
 
norm = tf.keras.layers.Normalization()
norm.adapt(train_x)
 
model = tf.keras.Sequential([
    norm,
    tf.keras.layers.Dense(16, activation=&quot;linear&quot;),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation=&quot;linear&quot;),
])
 
 
model.compile(
    optimizer=tf.keras.optimizers.legacy.Adam(0.001),
    metrics=[tf.keras.metrics.R2Score(dtype=tf.float32)],
    loss=tf.keras.losses.MeanSquaredError(),
)
 
model.fit(train_x, train_y, epochs=7, batch_size=2)
 
tf.keras.models.save_model(model, 'wind.keras')
</code></pre>
<p><code>print(data.describe())</code></p>
<pre><code>              0            1            2            3            4  ...          241          242          243          244      wind_kw
count    1896.0  1896.000000  1896.000000  1896.000000  1896.000000  ...  1896.000000  1896.000000  1896.000000  1896.000000  1896.000000
mean   144000.0     4.315717   189.610759     5.791377   193.830169  ...     3.881292   145.420359     4.572205   143.642405  1292.576958
std         0.0     2.482439   113.178764     2.926497   113.685887  ...     2.612259    93.293471     2.775681    94.721086   611.333721
min    144000.0     0.100000     1.000000     0.100000     1.000000  ...     0.100000     2.000000     0.000000     1.000000    34.263000
25%    144000.0     2.110000    88.000000     3.487500    90.000000  ...     1.900000    67.000000     2.500000    63.000000   793.109500
50%    144000.0     4.110000   199.000000     5.500000   231.000000  ...     3.075000   137.000000     3.940000   135.000000  1251.590000
75%    144000.0     6.220000   291.000000     7.882500   294.000000  ...     5.502500   205.000000     6.082500   205.000000  1761.926750
max    144000.0    11.670000   360.000000    15.210000   360.000000  ...    14.460000   360.000000    16.980000   360.000000  3008.125000
</code></pre>
<pre class=""lang-py prettyprint-override""><code>print(type(data))
#&lt;class 'pandas.core.frame.DataFrame'&gt;
print(data.dtypes)
#0          float32
#1          float32
#2          float32
#3          float32
#4          float32
#            ...   
#241        float32
#242        float32
#243        float32
#244        float32
#wind_kw    float32
#Length: 246, dtype: object
print(data.shape)
#(1896, 246)
</code></pre>
<p>I can't seem to find any information online about this error when using R2Score- any ideas as to what could be the issue?</p>
","2","Question"
"78057268","","<p>I have a model defined like so:</p>
<pre class=""lang-py prettyprint-override""><code>rng = 42
model = Pipeline([
    ('scaler', RobustScaler()),
    ('feature', SelectKBest(k=42)),
    ('model', SGDClassifier(loss='modified_huber', shuffle=True, random_state=rng))
])
</code></pre>
<p>That when I train+predict in two separate program executions (one ad-hoc, another with a cron job) with the exact same inputs, I get different model weights, and thus, prediction results.</p>
<p>I noticed that 'hinge' loss is the only reproducible model with the exact same weights. What is it about the other loss functions that prevent them from being reproduced?</p>
<p>I've checked and double-checked that the inputs are the same, and verified with other loss functions.</p>
","1","Question"
"78058612","","<p>I am deploying ml model using Gradio, after deploying on Gradio I get error after I input and output shows error</p>
<p>code for this output is</p>
<pre><code>import gradio as gr
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Sample data (replace this with your actual data)
X_train = np.array([[230.1, 37.8, 69.2],
                    [44.5, 39.3, 45.1],
                    [17.2, 45.9, 69.3],
                    [151.5, 41.3, 58.5],
                    [180.8, 10.8, 58.4]])
y_train = np.array([22.1, 10.4, 9.3, 18.5, 12.9])

# Initialize and train your Linear Regression model
scaler = MinMaxScaler()
scaler.fit(X_train) 
X_train_scale = scaler.transform(X_train)  

lm = LinearRegression()
lm.fit(X_train_scale, y_train)

# Define prediction function
def predict_sales(tv, radio, newspaper):
    # Scale the input features
    input_features = scaler.transform([[tv, radio, newspaper]])
    # Predict sales
    prediction = lm.predict(input_features)
    return prediction[0]

# Create Gradio Interface
tv_input = gr.Number(label=&quot;TV&quot;)
radio_input = gr.Number(label=&quot;Radio&quot;)
newspaper_input = gr.Number(label=&quot;Newspaper&quot;)
output_text = gr.Textbox(label=&quot;Predicted Sales&quot;)

gr.Interface(fn=predict_sales, 
             inputs=[tv_input, radio_input, newspaper_input], 
             outputs=output_text,
             title=&quot;Sales Prediction&quot;,
             description=&quot;Enter advertising expenses to predict sales&quot;,
            debug=True,enable_queue=True).launch()
</code></pre>
<p>]</p>
<p><a href=""https://i.sstatic.net/2ZQBj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2ZQBj.png"" alt=""enter image description here"" /></a>
How do I solve this error?</p>
","0","Question"
"78060462","","<p>I am trying to use the random forest model to predict social media ads effects based on age and estimated salary, this is my code but i keep getting Attribute error prompting up.</p>
<pre><code>from sklearn.tree import export_graphviz
from IPython import display
from sklearn.ensemble import RandomForestRegressor

m = RandomForestRegressor(n_estimators=5, max_depth=3, bootstrap=False, n_jobs=-1)
m.fit(x_train, y_train)

# data = data.drop('Purchased', axis=1)

str_tree = export_graphviz(m, 
                          out_file=None,
                          feature_names=data.columns[2:4], 
                          filled=True,
                          special_characters=True,
                          rotate=True,
                          precision=1)

display.display(str_tree)
</code></pre>
<p>This is the error i keep getting. Despite the fact that i didn't call the <code>tree_ parameter</code>.</p>
<pre><code>File ~\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\tree\_export.py:465, in _DOTTreeExporter.export(self, decision_tree)
    463     self.recurse(decision_tree, 0, criterion=&quot;impurity&quot;)
    464 else:
--&gt; 465     self.recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)
    467 self.tail()

AttributeError: 'RandomForestRegressor' object has no attribute 'tree_'
</code></pre>
","0","Question"
"78060804","","<p>I want to get shap value per class. I have checked tutorial and I found below example how to do this. However, the code do not work because of <code>shap_value.shape</code> is (10,None,6). 10 is your the number of samples, 4 is class.</p>
<pre class=""lang-py prettyprint-override""><code>import datasets
import pandas as pd
import transformers
import shap

dataset = datasets.load_dataset(&quot;emotion&quot;, split=&quot;train&quot;)
data = pd.DataFrame({&quot;text&quot;: dataset[&quot;text&quot;], &quot;emotion&quot;: dataset[&quot;label&quot;]})

# load the model and tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(
    &quot;nateraw/bert-base-uncased-emotion&quot;, use_fast=True
)
model = transformers.AutoModelForSequenceClassification.from_pretrained(
    &quot;nateraw/bert-base-uncased-emotion&quot;
).cuda()

# build a pipeline object to do predictions
pred = transformers.pipeline(
    &quot;text-classification&quot;,
    model=model,
    tokenizer=tokenizer,
    device=0,
    return_all_scores=True,
)
explainer = shap.Explainer(pred)
shap_values = explainer(data[&quot;text&quot;][:3])
shap.plots.bar(shap_values[:, :, &quot;joy&quot;].mean(0))
</code></pre>
<p>Are there any way to get bar plot for per class?</p>
","0","Question"
"78064402","","<p>I am running this relatively straightforward algorithm.</p>
<p>if I understand the algorithm correctly if you cluster to, say, 8 clusters, you should had the results for all clusters above 8, right?</p>
<p>Would you actually have to run the code multiple times, or how would you retrieve the intermediate clustering?</p>
<pre><code>%%time
for k in K:
    start_time = time.time()  # Start timing
    
    s[k] = []
    db[k] = []
    
    np.random.seed(123456)  # for reproducibility
    model = AgglomerativeClustering(linkage='ward', connectivity=w.sparse, n_clusters=k)
    y = model.fit(cont_std)
    cont_std_['AHC_k'+ str(k)] = y.labels_
    
    silhouette_score = metrics.silhouette_score(cont_std, y.labels_, metric='euclidean')
    print('silhouette at k=' + str(k) + ': ' + str(silhouette_score))
    s[k].append(silhouette_score)
    
    davies_bouldin_score = metrics.davies_bouldin_score(cont_std, y.labels_)
    print(f'davies bouldin at k={k}: {davies_bouldin_score}')
    db[k].append(davies_bouldin_score)
    
    end_time = time.time()  # End timing
    print(f&quot;Time for k={k}: {end_time - start_time} seconds&quot;)  # Print the duration for the cycle
</code></pre>
","1","Question"
"78066771","","<p>Below is my Model</p>
<pre><code># Since we are predicting a value for every timestep, we set return_sequences=True
input = Input(batch_shape=ip_shape)
mLSTM = LSTM(units=32, return_sequences=True, stateful=True)(input)
mDense = Dense(units=32, activation='linear')(input)
mSkip = Add()([mLSTM, mDense])

mSkip = Dense(units=1, activation='linear')(mSkip)
model = Model(input, mSkip)

adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss=total_loss)
model.summary()
</code></pre>
<p>Model Summary</p>
<pre><code>Model: &quot;model_3&quot;
_______________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
===============================================================================================
 input_5 (InputLayer)        [(104, 22050, 1)]            0         []                            
                                                                                                  
 lstm_4 (LSTM)               (104, 22050, 32)             4352      ['input_5[0][0]']             
                                                                                                  
 dense_5 (Dense)             (104, 22050, 32)             64        ['input_5[0][0]']             
                                                                                                  
 add_3 (Add)                 (104, 22050, 32)             0         ['lstm_4[0][0]',              
                                                                     'dense_5[0][0]']             
                                                                                                  
 dense_6 (Dense)             (104, 22050, 1)              33        ['add_3[0][0]']               
                                                                                                  
===============================================================================================
Total params: 4449 (17.38 KB)
Trainable params: 4449 (17.38 KB)
Non-trainable params: 0 (0.00 Byte)
_______________________________________________________________________________________________
</code></pre>
<p>I am using a custom loss function. I am not sure if it could be messing with the shapes while backproping</p>
<pre><code>def total_loss(y_true, y_pred):
    ratio = 0.5
    dc_loss = math_ops.pow(math_ops.subtract(math_ops.mean(y_true, 0), math_ops.mean(y_pred, 0)), 2)
    dc_loss = math_ops.mean(dc_loss, axis=-1)
    dc_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    dc_loss = math_ops.div(dc_loss, dc_energy)

    esr_loss = math_ops.squared_difference(y_pred, y_true) 
    esr_loss = math_ops.mean(esr_loss, axis=-1)
    esr_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    esr_loss = math_ops.div(esr_loss, esr_energy)

    return (ratio)*dc_loss + (1-ratio)*esr_loss
</code></pre>
<p>Finally the error: Let me know if the whole traceback is needed</p>
<pre><code>InvalidArgumentError: Graph execution error:

...

Incompatible shapes: [104,22050,32] vs. [32,22050,1]
     [[{{node gradient_tape/total_loss/BroadcastGradientArgs}}]] [Op:__inference_train_function_9604]
</code></pre>
<p>Setting stateful=False seems to work but I don't get why</p>
","0","Question"
"78067888","","<p>I want to deploy a flask app on a python webhost. The app is built using machine learning models link KNN, SVM, etc. I have developed the models using the scikit-learn library.
The app works fine on the my local machine, but encounters Internal Server Error when I run it on the host.
I checked the error log and found that when running the ML models, the server runs out of resources. How can I fix this?</p>
<p>I have tried limiting the n_jobs in scikit-learn models to 1, but it didn't work.</p>
","0","Question"
"78067906","","<p>i am using latest tensorflow package and isic skin cancer image dataset. am completely new to this machine learning and neural nets and was trying to classify images using tensorflow neural networks and keras.</p>
<p>i used the code off from kaggle and changed it a bit up to suit my need for the model however the code is not running and giving me an error that i am not understanding and able to do anything as the vanialla code ran just fine.</p>
<p>How to resolve this error?</p>
<p>this is the function that i have written to make the nerural network. The code is based of from a kaggle project and i have tried to make functions which can be used to easily supply desired parameters and hyper parameters to easily train the models.</p>
<pre><code>#function to select the optimizers
def select_optimizer(optimizer , lr):
    optimizers = {
      &quot;adam&quot;: tf.keras.optimizers.Adam(learning_rate=lr),
      &quot;sgd&quot;: tf.keras.optimizers.SGD(learning_rate=lr),
      &quot;lion&quot;: tf.keras.optimizers.Lion(learning_rate = lr),
      &quot;adafactor&quot;: tf.keras.optimizers.Adafactor(learning_rate = lr)}
    return optimizers[optimizer]
   
# final function that compiles the prototype model and gets the model and history    
def proto_model(n , neuron_density , num_classes , acti_functions , final_acti , lr , loss_func , optimize , epochs):
    
    model = Sequential([layers.experimental.preprocessing.Rescaling(1.0 / 255 , input_shape = (img_height , img_width , 3))])
    #initial properties of the layer of the neurons in the first element of the arrays 
    model.add(Conv2D(neuron_density[0] , 3 , padding = &quot;same&quot; , activation = acti_functions[0]))
    model.add(MaxPool2D())
    
    #for loop for the internal desne layer creations
    for i in range(1 , n):
        model.add(Conv2D(neuron_density[i] , 3 , padding = &quot;same&quot; , activation = acti_functions[i]))
        model.add(MaxPool2D())
    
    
    #final output dense layer of the neurons taken from the final layer of the arrays 
    model.add(Flatten())
    model.add(Dense(neuron_density[n-1] , activation = acti_functions[n-1]))
    model.add(Dense(units = num_classes , activation = final_acti))
    
    optimizer_algo = select_optimizer(optimize , lr)
    model.compile(optimizer = optimizer_algo, loss = loss_func,
    metrics = ['accuracy'])
     
    history = model.fit(train_ds , validation_data = val_ds , epochs = epochs)
    
    return model , history
</code></pre>
<p>The parameters that i have supplied :-</p>
<pre><code>densities = [32 , 64 , 128 , 256 , 512 , 1024] 
acties = [&quot;relu&quot; , &quot;relu&quot; , &quot;relu&quot; , &quot;relu&quot; , &quot;relu&quot; , &quot;relu&quot;] 
fin_acti = &quot;softmax&quot;
n = 6
classes = 9
learning_rate = 0.001
loss_func = &quot;BinaryCrossentropy&quot;
optimizer = &quot;adam&quot;
epochs = 10
model , history = proto_model(n , densities , classes , acties , fin_acti , learning_rate , loss_func , optimizer , epochs)
</code></pre>
<p>The above were the exact parameters that were even in vanilla original code however this time i have made it into arrays and supplied to the function that i have created from the vanilla code.</p>
<p>Error that i have encountered :
ValueError: <code>logits</code> and <code>labels</code> must have the same shape, received ((None, 9) vs (None, 1)).</p>
<p>also shows :-</p>
<pre><code>Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[25], line 10
      8 optimizer = &quot;adam&quot;
      9 epochs = 10
---&gt; 10 model , history = proto_model(n , densities , classes , acties , fin_acti , learning_rate , loss_func , optimizer , epochs)

Cell In[24], line 33, in proto_model(n, neuron_density, num_classes, acti_functions, final_acti, lr, loss_func, optimize, epochs)
     29 optimizer_algo = select_optimizer(optimize , lr)
     30 model.compile(optimizer = optimizer_algo, loss = loss_func,
     31 metrics = ['accuracy'])
---&gt; 33 history = model.fit(train_ds , validation_data = val_ds , epochs = epochs)
     35 return model , history

File ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File C:\Users\SUBHOJ~1\AppData\Local\Temp\__autograph_generated_fileoc8gwbam.py:15, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__train_function(iterator)
     13 try:
     14     do_return = True
---&gt; 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:
</code></pre>
","0","Question"
"78069410","","<p>I am trying to create data assest with ADLS gen 2, and read a delta table on adls gen folder something like this:</p>
<pre><code>/
└── my-data
    ├── _delta_log
    ├── part-0000-xxx.parquet
    └── part-0001-xxx.parquet
</code></pre>
<p>Currently, when creating the data asset I used file dataset type ML v1 APIs, but when reading the table, it shows all the rows(even the deleted ones), and not the most recent version.</p>
<p>I have attempted to create it all the other data asset types for azure Ml v1/v2.
I ideally want to read the most recent version of the delta table and also have the option to change version.</p>
<p>No sucess. How to resolve this?</p>
","0","Question"
"78070068","","<p>My neural network only outputs one value across the entire batch even as the inputs change.
It is defined with two inputs - distance, x and time, t. The outputs are pressure, h and flowrate, q.</p>
<p>The neural network is physics informed so it solves the PDEs that govern fluid flow in a pipe for each iteration and includes this as part of the loss function. This is given as loss_pde and is defined in the training loop where F1 and F2 are the respective equations that include the PDEs.</p>
<p>As general testing and bug-fixing I have changed the activation function, optimiser and learning rate to try and fix the problem but it has no effect on the results</p>
<p>The neural network architecture is defined as:</p>
<pre><code>class FCN(nn.Module):
def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):
    super().__init__()
    activation = nn.Tanh
    self.fcs = nn.Sequential(
        nn.Linear(N_INPUT, N_HIDDEN),
        activation()
    )
    self.fch = nn.Sequential(*[
        nn.Sequential(
            nn.Linear(N_HIDDEN, N_HIDDEN),  # Adjust input size to N_HIDDEN
            activation()
        ) for _ in range(N_LAYERS - 1)
    ])
    self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)  # Output layer

def forward(self, x, t):
    inputs = torch.cat([x, t], axis = 1)
    inputs = self.fcs(inputs)
    inputs = self.fch(inputs)
    outputs = self.fce(inputs)
    return outputs

pinn = FCN(2, 2, 9, 12)
</code></pre>
<p>The training loop is defined as:</p>
<pre><code>for i in range (200001):

  w_f = 1e-3

  optimiser.zero_grad()

  colloc_output = pinn(x_colloc, t_colloc)

  h_hat, q_hat = colloc_output[:,0], colloc_output[:,1]

  dq_dt = torch.autograd.grad(q_hat, t_colloc, torch.ones_like(q_hat), create_graph=True)[0]
  dq_dx = torch.autograd.grad(q_hat, x_colloc, torch.ones_like(q_hat), create_graph=True)[0]
  dh_dt = torch.autograd.grad(h_hat, t_colloc, torch.ones_like(h_hat), create_graph=True)[0]
  dh_dx = torch.autograd.grad(h_hat, x_colloc, torch.ones_like(h_hat), create_graph=True)[0]

  F1 = Cs_A * dq_dt + q_hat * dq_dx + g * Cs_A**2 * dh_dx + f * (torch.abs(q_hat) * q_hat) / (2 * diam)
  F2 = Cs_A * dh_dt + q_hat * dh_dx + a**2/g * dq_dx

  loss_pde = torch.mean(F1**2 + F2**2)

  train_output = pinn(x_train_rand, t_train_rand)

  loss_data = torch.mean((train_output - hq_tr)**2)

  loss = w_f * loss_pde + loss_data 

  loss.backward()
  optimiser.step()
</code></pre>
<p>The results from this are shown as:
<a href=""https://i.sstatic.net/jfJVT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jfJVT.png"" alt=""enter image description here"" /></a></p>
<p>The inputs are a tensor of size (380,2) with each column representing x and t respectively. The x column is filled with one value = 155 m and the time column is filled with 380 values from 0, 10s. The grey line shows the expected behaviour of the system and the blue line shows the current output from the NN.
The data loss only reduces to a value of ~163 as this is the closest it can get to with only one value for the whole batch, the first 5 values are shown in the results above to demonstrate that all values are the same.</p>
<p><strong>UPDATE</strong>:</p>
<p>I have now defined the inputs of the neural network as part of a dataset and then using a dataloader I have inputted the values into the neural network and the same problem occurs. The dataset and dataloader has been defined as follows</p>
<pre><code>class TrainData(Dataset):
def __init__(self, x_traindata, t_traindata, h_train, q_train):
    train_input = torch.cat([x_traindata, t_traindata], axis = 1)
    train_aim = torch.cat([h_train, q_train], axis = 1)
    self.train_input = train_input
    self.train_aim = train_aim
    
def __len__(self):
    return self.train_input.size(0)

def __getitem__(self, idx):
    input_value_x = self.train_input[idx, 0]
    input_value_t = self.train_input[idx, 1] 
    output_value_h = self.train_aim[idx, 0]
    output_value_q = self.train_aim[idx, 1]
    return input_value_x, input_value_t, output_value_h, output_value_q

class CollocData(Dataset):
    def __init__(self, x_colloc, t_colloc):
        colloc_input = torch.cat([x_colloc, t_colloc], axis = 1)
        self.colloc_input = colloc_input
        
    def __len__(self):
        return self.colloc_input.size(0)
    
    def __getitem__(self, idx):
        input_x = self.colloc_input[idx, 0]
        input_t = self.colloc_input[idx, 1]
        return input_x, input_t
</code></pre>
<p>Where my inputs into the dataset are defined as:</p>
<pre><code>x_train = torch.tensor(x_inp.values).to(dtype=torch.float32)
t_train = torch.tensor(t_inp.values).to(dtype=torch.float32)
h_train = torch.tensor(h_inp.values).to(dtype=torch.float32)
q_train = torch.tensor(q_inp.values).to(dtype=torch.float32)

x_colloc = torch.linspace(0, 300, 500).view(-1,1).requires_grad_(True)
t_colloc = torch.linspace(0, 10, 500).view(-1,1).requires_grad_(True)

test_dataset = TrainData(x_train, t_train, h_train, q_train)
colloc_dataset = CollocData(x_colloc, t_colloc)

train_dataloader = DataLoader(test_dataset, batch_size = x_train.size(0), shuffle=True)
colloc_dataloader = DataLoader(colloc_dataset, batch_size = x_colloc.size(0), shuffle = True)
</code></pre>
<p>The start of the training loop has been redefined as:</p>
<pre><code>for i in range (200001):
for train_batch, colloc_batch in zip(train_dataloader, colloc_dataloader):

    optimiser.zero_grad()
    
    train_x, train_t, target_h, target_q = train_batch
    colloc_x, colloc_t = colloc_batch
        
    colloc_output = pinn(colloc_x.unsqueeze(1), colloc_t.unsqueeze(1))
        
    h_hat, q_hat = colloc_output[:,0], colloc_output[:,1]
</code></pre>
<p>These corrections should mean the data is inputted correctly thus the error must be from another section of the code.</p>
","0","Question"
"78070239","","<p>I am follwing this tutorial for <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-azure-ml-in-a-day?view=azureml-api-2"" rel=""nofollow noreferrer"">Azure ML</a>
via Azure ML studio web and &quot;notebooks&quot; inside it</p>
<p>while invoking MLClient object instance, I am getting below error</p>
<pre><code>ws = ml_client.workspaces.get(WS_NAME)
print(ws.location,&quot;:&quot;, ws.resource_group)
</code></pre>
<p>Things I have tried</p>
<ol>
<li><p>Creating terminal inside Azure ML Studio web, but it needs separate compute</p>
</li>
<li><p>I can only use serverless spark compute which shows available while running notebooks</p>
</li>
<li><p>Local powershell with Az login which authenticated successfully ( not sure if AZ CLI path can be set here and if that will help at studio web notebook executions)</p>
</li>
</ol>
<p>How to resolve this?</p>
<pre><code>ClientAuthenticationError: DefaultAzureCredential failed to retrieve a token from the included credentials. Attempted credentials:  EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured. Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.  ManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.    SharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.     AzureCliCredential: Azure CLI not found on path     AzurePowerShellCredential: PowerShell is not installed  AzureDeveloperCliCredential: Azure Developer CLI could not be found.
</code></pre>
","1","Question"
"78071238","","<pre class=""lang-py prettyprint-override""><code>resnet_50 = &quot;https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/classification/versions/1&quot;
feature_extractor_model = resnet_50
import tensorflow_hub as hub
feature_extractor_layer = hub.KerasLayer(
    feature_extractor_model,
    input_shape=(224, 224, 3),
    trainable=False)
num_classes = len(class_names)
model = tf.keras.Sequential()
model.add(feature_extractor_layer)
model.summary()
</code></pre>
<p>I am trying to add the <code>feature_extractor_layer</code> to the Sequential, but getting the following error:</p>
<blockquote>
<p>ValueError: Only instances of <code>keras.Layer</code> can be added to a Sequential model. Received: &lt;tensorflow_hub.keras_layer.KerasLayer object at 0x7b6c00794850&gt; (of type &lt;class 'tensorflow_hub.keras_layer.KerasLayer'&gt;)</p>
</blockquote>
<p>How to resolve this?</p>
","1","Question"
"78072453","","<p>I need to use swin transfomer as a backbone for my SVM model. I need to extract the features first, therefore I am trying to remove the last layer and train the model. This is a sample executable code.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np

HUB_URL = &quot;SharanSMenon/swin-transformer-hub:main&quot;
MODEL_NAME = &quot;swin_tiny_patch4_window7_224&quot;
model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True)
model = nn.Sequential(*list(model.children())[:-1])          #--&gt; If you comment this line, the code is executing
for param in model.parameters():
    param.requires_grad = False


dummy_tensor = torch.randn(32, 3, 224, width)

# Print the shape of the dummy tensor
model(dummy_tensor)
</code></pre>
<p><a href=""https://i.sstatic.net/j7MhL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j7MhL.png"" alt=""enter image description here"" /></a></p>
<p>What am I missing, I tried the same process with resnet50 and it worked fine. Why am I not able to remove the last layer and get only the output features? what should I do? any suggestions?</p>
","0","Question"
"78072628","","<p>I have a variable, say, <code>a</code> which has some gradient associated with it from some operations before. Then I have integers <code>b</code> and <code>c</code> which have no gradients. I want to compute the minimum of <code>a</code>, <code>b</code>, and <code>c</code>. MWE is as given below.</p>
<pre><code>import torch

a = torch.tensor([4.], requires_grad=True)  # As an example I have defined a leaf node here, in my program I have an actual variable with gradient
b = 5
c = 6
d = torch.min(torch.tensor([a, b, c]))  # d does not have gradient associated
</code></pre>
<p>How can I write this differently so that the gradient from <code>a</code> flows through to <code>d</code>? Thanks.</p>
","2","Question"
"78073319","","<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report

# Step 1: Load the dataset
df = pd.read_csv('articles1.csv')

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['content'], df['author'], test_size=0.2, random_state=42)

# Step 3: Create a Pipeline
# This pipeline will first convert the text data into TF-IDF vectorized form,
# then apply a Naive Bayes classifier to perform the author identification.
model = make_pipeline(TfidfVectorizer(), MultinomialNB())

# Step 4: Train the model
model=model.fit(X_train, y_train)

The error is 

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-21-00ad871f55dc&gt; in &lt;module&gt;
     18 
     19 # Step 4: Train the model
---&gt; 20 model=model.fit(X_train, y_train)


~\anaconda3\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    344             if self._final_estimator != 'passthrough':
    345                 fit_params_last_step = fit_params_steps[self.steps[-1][0]]
--&gt; 346                 self._final_estimator.fit(Xt, y, **fit_params_last_step)
    347 
    348         return self

~\anaconda3\lib\site-packages\sklearn\naive_bayes.py in fit(self, X, y, sample_weight)
    610         self : object
    611         &quot;&quot;&quot;
--&gt; 612         X, y = self._check_X_y(X, y)
    613         _, n_features = X.shape
    614         self.n_features_ = n_features



ValueError: Input contains NaN
</code></pre>
<p>Please check for me and help me crosscheck what might be wrong with the code. The work is to predict authors. Kindly help me go through it and analyse it better. Thank you all. It is  the use of binomial naive bayes algorithm for machine analysis. The last part is also complaining of the input containing Not A Number. The source of dat without number can not be ascertained as yet.</p>
","0","Question"
"78073911","","<p>Let's say I have a folder called 'test' with folders inside, 'images' and 'labels'. I also have a YOLOv8 model which I've trained called 'best.pt. My labels are polygons (yolo-obb .txt files).</p>
<p>I want to find the mean average precision (MAP) of my YOLOv8 model on this test set.</p>
<p>I've read both the documentation for predicting and benchmarking, however, I'm struggling to find an example of calculating map from some test images.</p>
<p><a href=""https://docs.ultralytics.com/modes/predict/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/predict/</a></p>
<p><a href=""https://docs.ultralytics.com/modes/benchmark/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/benchmark/</a></p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO

# Load a pretrained YOLOv8n model
model = YOLO('best.pt')

# Run inference on an image
results = model(['test/images/bus.jpg', 'test/images/zidane.jpg'])  # list of 2 Results objects
</code></pre>
<p>I imagine I have to put the list of images in the above, then write code to calculate map for everything in the test folder and average it. Are there packages that have already done this?</p>
<p>What's the code to achieve this task?</p>
","1","Question"
"78074180","","<p>I am trying yo run SimpleNet network and when I enter the command <code>bash run.sh</code> on the terminal I see the following output:</p>
<pre><code>
Matplotlib created a temporary cache directory at /tmp/matplotlib-ovjj_rt5 because the default path (/home/username/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Traceback (most recent call last):
  File &quot;main.py&quot;, line 43, in &lt;module&gt;
    @main.result_callback()
TypeError: 'NoneType' object is not callable
</code></pre>
<p><strong>Steps to Produce</strong></p>
<ol>
<li><p>Install WSL 20.04 on a Windows 11 computer.</p>
</li>
<li><p>Install python 2.7 and 3.8.10 in WSL</p>
</li>
<li><p>Build OpenCV 4.2.0 from sources with python support.</p>
</li>
<li><p>Clone this repository <a href=""https://github.com/DonaldRR/SimpleNet"" rel=""nofollow noreferrer"">here</a> into home directory.</p>
</li>
<li><p>Install required packages listed <a href=""https://github.com/DonaldRR/SimpleNet?tab=readme-ov-file#environment"" rel=""nofollow noreferrer"">here</a> with pip3.</p>
</li>
<li><p>Download the <a href=""https://www.mvtec.com/company/research/datasets/mvtec-ad/"" rel=""nofollow noreferrer"">MVTech dataset</a> into a newly created directory under your home directory.</p>
</li>
<li><p>Extract the tar archive with the command <code>tar -xf mvtec_anomaly_detection.tar.xz</code> inside the directory into which you have downloaded the dataset.</p>
</li>
<li><p>Edit first and tenth lines of run.sh to make it point to the dataset I have just downloaded.</p>
</li>
<li><p>Enter the command <code>bash run.sh</code></p>
</li>
<li><p>See the error on your terminal</p>
</li>
</ol>
<p><strong>Envirovment:</strong></p>
<ul>
<li><p>WSL 20.04</p>
</li>
<li><p>Python 3.8.10</p>
</li>
<li><p>OpenCV 4.2.0 (Built from source)</p>
</li>
<li><p>Torch 1.12.1</p>
</li>
<li><p>Torchvision 0.13.1</p>
</li>
<li><p>numpy 1.22.4</p>
</li>
</ul>
<p><strong>What Did I Try?</strong></p>
<ul>
<li>I have tried to debug main.py code.</li>
</ul>
<p>I have seen that up until <code>@main.result_callback()</code>, it was working as expected</p>
<ul>
<li>Searched If it have happened to someone else</li>
</ul>
<p>I have seen that someone have solved the issue <a href=""https://github.com/DonaldRR/SimpleNet/issues/40"" rel=""nofollow noreferrer"">here</a>. But, I couldn't understand what exactly they did to solve the issue.</p>
<p><strong>What I Have Expected</strong></p>
<p>I have expected that as I run the command <code>bash run.sh</code> I would see that the network begin it's training.</p>
","1","Question"
"78074844","","<p>I want to train the model, but when I enter the x_test and y_test values for the score after the fit model, it gives me a mismatch error.</p>
<pre><code>## This proje is my first personal project trying 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import sklearn

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score

train = pd.read_csv(&quot;/Users/ahmeteminguney/Desktop/house/train.csv&quot;)
home_test = pd.read_csv(&quot;/Users/ahmeteminguney/Desktop/house/test.csv&quot;)
home_result = pd.read_csv(&quot;/Users/ahmeteminguney/Desktop/house/sample_submission.csv&quot;)
x = train.drop(&quot;SalePrice&quot;, axis=1)
y = train[&quot;SalePrice&quot;]
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,
                                                    y, 
                                                    test_size=0.2, 
                                                    )


all_data = pd.concat([x_train, home_test])
missing_columns = all_data.columns[all_data.isnull().any()].tolist()
def fill_missing_values(data):
    for column in missing_columns:
        if data[column].dtype == 'float64' or data[column].dtype == 'int64':
            data[column].fillna(data[column].mean(), inplace=True)
        else:
            data[column].fillna(data[column].mode()[0], inplace=True)
    return data


train_filled = fill_missing_values(train)
home_test_filled = fill_missing_values(home_test)

x_train = pd.get_dummies(x_train)
x_test = pd.get_dummies(x_test)
y_train = pd.get_dummies(y_train)
y_test = pd.get_dummies(y_test)

from sklearn.neighbors import KNeighborsClassifier
model3 = KNeighborsClassifier()
model3.fit(x_train,y_train)
model3.score(x_test,y_test)`


</code></pre>
<p>ValueError: The feature names should match those that were passed during fit.
Feature names seen at fit time, yet now missing:</p>
<p>ı try a lot of way but ı didnt fix</p>
","-5","Question"
"78076083","","<p>I am trying to follow tutorial video and run <a href=""https://colab.research.google.com/github/camenduru/PanoHead-colab/blob/main/PanoHead_custom_colab.ipynb#scrollTo=v9wpwlGfiX2e"" rel=""nofollow noreferrer"">this notebook</a> on google colab.</p>
<p>After running second block of code, the following error is produced:</p>
<pre><code>/content/3DDFA_V2/bfm/bfm.py:34: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.
  self.keypoints = bfm.get('keypoints').astype(np.long)  # fix bug
Traceback (most recent call last):
  File &quot;/content/3DDFA_V2/recrop_images.py&quot;, line 322, in &lt;module&gt;
    main(args)
  File &quot;/content/3DDFA_V2/recrop_images.py&quot;, line 183, in main
    tddfa = TDDFA(gpu_mode=gpu_mode, **cfg)
  File &quot;/content/3DDFA_V2/TDDFA.py&quot;, line 34, in __init__
    self.bfm = BFMModel(
  File &quot;/content/3DDFA_V2/bfm/bfm.py&quot;, line 34, in __init__
    self.keypoints = bfm.get('keypoints').astype(np.long)  # fix bug
  File &quot;/usr/local/lib/python3.10/dist-packages/numpy/__init__.py&quot;, line 328, in __getattr__
    raise AttributeError(&quot;module {!r} has no attribute &quot;
AttributeError: module 'numpy' has no attribute 'long'. Did you mean: 'log'?
cp: cannot stat '/content/3DDFA_V2/crop_samples/img/*': No such file or directory
</code></pre>
<p>update: have found more info about the particular error here:</p>
<p><a href=""https://stackoverflow.com/questions/76389395/attributeerror-module-numpy-has-no-attribute-long"">AttributeError: module &#39;numpy&#39; has no attribute &#39;long&#39;</a></p>
<p>Would like to try to roll back <code>numpy</code> to previous version but not sure how to go about doing that in a colab like this.</p>
","-2","Question"
"78076239","","<p>I have a relatively simple requirement but surprisingly this does not seem to be straightforward to implement in pytorch. Given a neural network with $P$ parameters that outputs a vector of length $Y$ and a batch of $B$ data inputs, I would like to calculate the gradients of the outputs with respect to the model's parameters.</p>
<p>In other words, I would like the following function:</p>
<pre><code>def calculate_gradients(model, X):
    &quot;&quot;&quot;
    Args:
        nn module with P parameters in total that outputs a tensor of size (B, Y).
        torch tensor of shape (B, .).

    Returns:
        torch tensor of shape (B, Y, P)
    &quot;&quot;&quot;
    # function logic here
</code></pre>
<p>Unfortunately, I don't currently see an obvious way of calculating this efficiently, especially without aggregating over the data or target dimensions. A minimal working example below involves looping over input and target dimensions, but surely there is a more efficient way?</p>
<pre><code>import torch
from torchvision import datasets, transforms
import torch.nn as nn

###### SETUP ######

class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h = self.fc1(x)
        pred = self.fc2(self.relu(h))
        return pred
    
train_dataset = datasets.MNIST(root='./data', train=True, download=True, 
                            transform=transforms.Compose(
                                [transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))
        ]))

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)

X, y = next(iter(train_dataloader))  # take a random batch of data

net = MLP(28*28, 20, 10)  # define a network


###### CALCULATE GRADIENTS ######
def calculate_gradients(model, X):
    # Create a tensor to hold the gradients
    gradients = torch.zeros(X.shape[0], 10, sum(p.numel() for p in model.parameters()))

    # Calculate the gradients for each input and target dimension
    for i in range(X.shape[0]):
        for j in range(10):
            model.zero_grad()
            output = model(X[i])
            # Calculate the gradients
            grads = torch.autograd.grad(output[j], model.parameters())
            # Flatten the gradients and store them
            gradients[i, j, :] = torch.cat([g.view(-1) for g in grads])
            
    return gradients

grads = calculate_gradients(net, X.view(X.shape[0], -1))
</code></pre>
<p><strong>Edit:</strong>
I ran some quick benchmarks of Felix Zimmermann's solution which does indeed provide some nice speedups for this toy problem on my machine.</p>
<pre><code>import time

start = time.time()
for _ in range(1000):
    grads = calculate_gradients(net, X.view(X.shape[0], -1))
end = time.time()
print('Loop solution', end - start)

start = time.time()
for _ in range(1000):
    params = {k: v.detach() for k, v in net.named_parameters()}
    buffers = {k: v.detach() for k, v in net.named_buffers()}
    grads2 = torch.vmap(one_sample)(X.flatten(1))
end = time.time()
print('Vmap solution', end - start)
</code></pre>
<p>Which outputs</p>
<pre><code>Loop solution 8.408899307250977
Vmap solution 2.355229139328003
</code></pre>
<p>Note that the performance gains are likely to be much greater in more realistic settings with larger batches on GPUs.</p>
","2","Question"
"78077221","","<h4>I'm attempting to build a Quantum Neural Network (QNN) using PyTorch and PennyLane. However, I'm encountering a dimension error specifically when defining the quantum layer.</h4>
<p>I have successfully set up my PyTorch and PennyLane environment, but when I try to define the quantum layer using PennyLane, I receive a dimension error. I suspect this might be due to a mismatch in the dimensions of my input data and the expected input shape of the quantum layer.</p>
<h2>My Code:</h2>
<pre><code># Get data 
train = datasets.MNIST(root=&quot;data&quot;, download=True, train=True, transform=ToTensor())
dataset = DataLoader(train, 32)
n_qubits = 2
dev = qml.device(&quot;default.qubit&quot;, wires=n_qubits)

@qml.qnode(dev)
def qnode(inputs, weights_0, weight_1):
    print(inputs)
    qml.RX(inputs[0], wires=0)
    qml.RX(inputs[1], wires=1)
    qml.Rot(*weights_0, wires=0)
    qml.RY(weight_1, wires=1)
    qml.CNOT(wires=[0, 1])
    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))
weight_shapes = {&quot;weights_0&quot;: 3, &quot;weight_1&quot;: 1}
qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)
print(qlayer)
class ImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(qlayer,
            nn.Conv2d(1, 32, (3, 3)),
            nn.ReLU(),
            nn.Conv2d(32, 64, (3, 3)),
            nn.ReLU(),
            nn.Conv2d(64, 64, (3, 3)),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * (28 - 6) * (28 - 6), 10)
        )

    def forward(self, x):
        result = self.model(x)
        return result
# Instance of the neural network, loss, optimizer
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
# Instance of the neural network, loss, optimizer
clf = ImageClassifier().to('cpu')
opt = Adam(clf.parameters(), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()

# Training flow 
if __name__ == &quot;__main__&quot;:
    for epoch in range(1):  # train for 10 epochs
        for batch in dataset:
            X, y = batch
            X, y = X.to('cpu'), y.to(device)
            yhat = clf(X)
            loss = loss_fn(yhat, y)

            # Apply backprop 
            opt.zero_grad()
            loss.backward()
            opt.step()

        print(f&quot;Epoch:{epoch} loss is {loss.item()}&quot;)
</code></pre>
<h2>The Error I found:</h2>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-84-a98a57a9f607&gt; in &lt;cell line: 9&gt;()
     12             X, y = batch
     13             X, y = X.to('cpu'), y.to(device)
---&gt; 14             yhat = clf(X)
     15             loss = loss_fn(yhat, y)
     16 

10 frames
/usr/local/lib/python3.10/dist-packages/pennylane/qnn/torch.py in &lt;listcomp&gt;(.0)
    427 
    428         if len(x.shape) &gt; 1:
--&gt; 429             res = [torch.reshape(r, (x.shape[0], -1)) for r in res]
    430 
    431         return torch.hstack(res).type(x.dtype)

RuntimeError: shape '[896, -1]' is invalid for input of size 28
</code></pre>
","0","Question"
"78077447","","<p>I am trying to find the equivalent keras representation of the following PyTorch line:</p>
<pre><code>conv1 = nn.Conv2d(int(filters*s), filters, kernel_size=(3, 1), stride=int(1/s), padding=(1, 0))
</code></pre>
<p>Here is the full code</p>
<pre><code>import torch
from torch import nn

import torch
import tensorflow as tf
import numpy as np

from keras import initializers
from tensorflow.keras import layers

# constants between both libraries
filters = 2
s = 1

# Set the seed for reproducible set of numbers between keras and pytorch
torch.manual_seed(0)
np.random.seed(0)
tf.random.set_seed(0)

# Create a tensor in PyTorch of shape (128, 2, 1024, 1)
pytorch_tensor = torch.rand((128, 2, 1024, 1))

# Convert the PyTorch tensor to a NumPy array
numpy_array = pytorch_tensor.numpy()

# Reshape the NumPy array to the desired shape for Keras (128, 1024, 1, 2)
numpy_array = np.transpose(numpy_array, (0, 2, 3, 1))

# Create a tensor in Keras (TensorFlow) from the NumPy array
keras_tensor = tf.constant(numpy_array)

# PyTorch zeropad and convolution 
pytorch_conv1 = nn.Conv2d(int(filters*s), filters, kernel_size=(3, 1), stride=int(1/s), padding=(1, 0))(pytorch_tensor)

print(f&quot;Pytorch values: {pytorch_conv1}&quot;)

# Keras zeropad and convolution 
keras_zeropad = layers.ZeroPadding2D(padding=((0,0),(1,0)))(keras_tensor)
keras_conv1 = layers.Conv2D(filters, kernel_size=(3, 1), strides=int(1/s), padding='valid', data_format='channels_last')(keras_zeropad)

print(f&quot;Keras tensor values: {keras_conv1}&quot;)
</code></pre>
<p>I thought the issue was padding since PyTorch and Keras handle it differently. Namely, inside Keras’ <code>Conv2D</code> layer the padding can only be set to <code>valid</code> or <code>same</code>. Instead I am using the <code>ZeroPadding2D</code> layer before the <code>Conv2D</code> so asymmetric padding can be done in Keras. However, despite this I still cannot get the output data to match.</p>
<p>I wrote the following code which generates the same input tensor for PyTorch and Keras so I know the data is the same. I’ve also tried initializing the weights and bias's the same way but the data still doesn’t match.</p>
<p>I’ve seen posts about using the PyTorch weights in Keras to achieve the same results but I would like to avoid that for my application.</p>
<p><strong>Update:</strong>
Changing the Keras Conv2D layer to the following still produces vastly different results compared to PyTorch. The padding and shape are equal, however Keras produces new data every time. I’ve looked into kernel initializers but nothing has produced data that matches PyTorch.</p>
<pre><code>keras_zeropad = layers.ZeroPadding2D(padding=(1,0))(keras_tensor)
keras_conv1 = layers.Conv2D(filters, kernel_size=(3, 1), strides=int(1/s), padding='valid', data_format='channels_last')(keras_zeropad)

print(f&quot;Keras tensor values: {keras_conv1}&quot;)
print(f&quot;Keras tensor shape: {keras_conv1.shape}&quot;)
</code></pre>
<p>For readability the input tensor shape was changed to (1,2,4,1). Here is the PyTorch and Keras data that should match.</p>
<pre><code>Pytorch values: tensor([[[[-0.1842],
          [-0.2021],
          [-0.3707],
          [-0.2296]],

         [[ 0.2727],
          [ 0.0941],
          [ 0.0927],
          [ 0.1981]]]], grad_fn=&lt;ConvolutionBackward0&gt;)
PyTorch tensor shape: torch.Size([1, 2, 4, 1])
Keras tensor values: [[[[-0.57757664  0.40883008]]

  [[ 0.21837917  0.6590299 ]]

  [[ 0.28047863  0.3622663 ]]

  [[ 0.2800742   0.2882987 ]]]]
Keras tensor shape: (1, 4, 1, 2)
</code></pre>
","0","Question"
"78078811","","<p>This is a followup question from <a href=""https://stackoverflow.com/q/78072628/6997665"">here</a>. I have obtained a tensor, say, <code>d</code> with gradients. Now I have another tensor array, say <code>e</code> from which I need to pick the first <code>d</code> elements. MWE below.</p>
<pre><code>import torch

a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
c = torch.tensor([6.])
d = a.min(b).min(c)

e = torch.arange(10)
f = e[:d]  # Throws error &quot;TypeError: only integer tensors of a single element can be converted to an index&quot;
</code></pre>
<p>Based on the answer <a href=""https://discuss.pytorch.org/t/typeerror-only-integer-tensors-of-a-single-element-can-be-converted-to-an-index/45641/2"" rel=""nofollow noreferrer"">here</a>, the following line works.</p>
<pre><code>f = e[:d.to(dtype=torch.long)]
</code></pre>
<p>However, the gradients are lost. Is there someway I can pass the gradients or this operation is not differentiable at all? Many thanks.</p>
","1","Question"
"78080687","","<p>I am trying to implement a keras data generator class. One of the attributes is the <code>shuffle</code> which if set true when the class is initialised and the <code>on_epoch_end()</code> is called, it will shuffle the indices of the samples in the design matrix, which in our case is a np array where each sample contains the file path of the png image. Problem is I used the scikit-learn <code>X_train, X_val, y_train, y_val = train_test_split(file_paths, labels, test_size=0.33, random_state=42)</code> so if I only shuffle the indices of the <code>X_train</code> without appropriately shuffling the <code>y_train</code> in the same way, the X data will not match their correct labels if shuffle in the <code>on_epoch_end()</code>. Does anyone know how this can be approached? I will insert the code of my class for reference.</p>
<pre><code>class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, file_paths, labels, batch_size=32, dim=(240,320), n_channels=3, shuffle=True):
        self.dim = dim
        self.batch_size = batch_size
        self.labels = labels
        self.file_paths = file_paths
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()

    def on_epoch_end(self):
      'Updates indexes after each epoch'
      self.indexes = np.arange(len(self.file_paths))
      if self.shuffle == True:
          np.random.shuffle(self.indexes)
</code></pre>
","0","Question"
"78089332","","<p>I was looking into KerasClassifier, as I would like to plug it in a scikit-learn pipeline, but I'm getting the aforementioned ValueError.</p>
<p>The following code should be able to reproduce the error I'm getting:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from scikeras.wrappers import KerasClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import load_iris
import numpy as np

data = load_iris()
X = data.data
y = data.target

def create_model():
    model = Sequential()
    model.add(Dense(8, input_dim=4, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model

clf = KerasClassifier(build_fn=create_model,
                      epochs=100,
                      batch_size=10,
                      verbose=1)

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', clf)
])

kf = KFold(n_splits=5, shuffle=True, random_state=42)
results = cross_val_score(pipeline, X, y, cv=kf)
print(&quot;Cross-Validation Accuracy:&quot;, np.mean(results))
</code></pre>
<p>It seems that my model is being compiled as the epochs are run. However, afterwards, I get the error:</p>
<pre><code>ValueError: Could not interpret metric identifier: loss
</code></pre>
<p>The versions for the tensorflow and scikeras libraries are:</p>
<pre><code>scikeras==0.12.0
tensorflow==2.15.0
</code></pre>
<p><strong>EDIT</strong>:
Eventually I experimented with different library versions and the following allowed me to run the code successfully, it seems the issue was caused by scikit-learn's version:</p>
<pre><code>scikeras==0.12.0
tensorflow==2.15.0
scikit-learn==1.4.1
</code></pre>
","4","Question"
"78092718","","<p>When training a Yolo model like below:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO

# Load a model
model = YOLO('yolov8s.pt')  

results = model.train(data='coco128.yaml', 
epochs=100, imgsz=640, save_period=1)
</code></pre>
<p>The save_period option will save every epoch. When the best epoch is found the file is saved as best.pt.</p>
<p>The file size of best.pt is ~27MB and each epoch is ~120MB. Is it possible to use the compression applied to the best epoch at the end of training to every epoch (even if this is done after training).</p>
","0","Question"
"78093891","","<p>I am new to neural networks and currently working with TensorFlow. For an experiment, I would like to build a model that consistently produces the same output for identical inputs. However, my initial attempt using a trivial test and setting the <code>batch_size</code> equal to the size of the training data did not achieve this goal:</p>
<pre><code>model = keras.Sequential([keras.layers.Dense(1)])
model.compile( loss=&quot;MSE&quot;, metrics=[keras.metrics.BinaryAccuracy()])
model.fit(
  training_inputs,
  training_targets,
  epochs=5,
  batch_size=1000,
  validation_data=(val_inputs, val_targets)
)
</code></pre>
<p>I suspect that the default optimizer, <code>SGD</code> (Stochastic Gradient Descent), might be causing random outputs.</p>
<p>My questions are:</p>
<ul>
<li>Are there any other factors in above code, besides the default optimizer (<code>SGD</code>), that can introduce randomness in the output of above neural network model?</li>
<li>How can I modify the provided code to ensure that the model produces the same output for the same input?</li>
</ul>
<p>Thank you for your assistance.</p>
","1","Question"
"78094285","","<p>I'm trying to work on feature selection to identify the features that are related to the response. So far I've used RFE, RFECV, SelectFromModel and SequentialFeatureSelection from sklearn. After model fit, the selector's estimator attributes allow me to obtain resulting coefficients for all models except SequentialFeatureSelection. Is there a solution for this?</p>
<p>I've tried comparing it to other models, but seems like the selector's estimator attributes are not applicable for SequentialFeatureSelection. The score attribute doesn't help either. Is there maybe another way?</p>
<p>RFE:</p>
<pre><code>estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select=3, step=1)
selector = selector.fit(df[features], df.Y)
coef = selector.estimator_.coef_
</code></pre>
<p>SFS:</p>
<pre><code>sfs = SequentialFeatureSelector(estimator, n_features_to_select=3, scoring=(make_scorer(R_squared)))
selector = sfs.fit(df[features], df.Y)
coef = ?
</code></pre>
","0","Question"
"78098823","","<p>I have just started doing Vision Transformer from scratch using pytorch. And the I got error like this when I run the training helper code. I know it is about the shape is not match, but I don't know which one I should do. The code is like this :</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torchvision
from torchvision import transforms

from google.colab import drive
drive.mount('/content/gdrive')

import zipfile

zip_ref = zipfile.ZipFile('/content/gdrive/MyDrive/dataset/data9k.zip', 'r')
zip_ref.extractall(&quot;/content/dataset&quot;)
zip_ref.close()

import os
from torchvision import datasets

data_dir = 'dataset/datasets'

train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'),
                                      transform=transforms.Compose([
                                          transforms.Resize((224, 224)),
                                          transforms.ToTensor(),
                                          transforms.Normalize(mean=[0.5, 0.5, 0.5],
                                 std=[0.5, 0.5, 0.5])
                                      ]))

patch_size = 16
in_channels = 3
embed_dim = 768
num_heads = 8
num_layers = 12
num_classes = 4
cls_token = 0
epochs = 5

class PatchEmbedding(torch.nn.Module):
    def __init__(self, patch_size, in_channels, embed_dim):
        super().__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.projection = torch.nn.Linear(patch_size**2 * in_channels, embed_dim)

    def forward(self, x):
        B, C, H, W = x.shape
        x = x.reshape(B, C, H // self.patch_size, W // self.patch_size, self.patch_size, self.patch_size)
        x = x.permute(0, 1, 4, 2, 5, 3)
        x = x.flatten(2)
        x = self.projection(x)
        return x

class MultiHeadAttention(torch.nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.q_proj = torch.nn.Linear(embed_dim, embed_dim)
        self.k_proj = torch.nn.Linear(embed_dim, embed_dim)
        self.v_proj = torch.nn.Linear(embed_dim, embed_dim)
        self.out_proj = torch.nn.Linear(embed_dim, embed_dim)

    def forward(self, q, k, v):
        B, T, E = q.shape
        q = self.q_proj(q).reshape(B, T, self.num_heads, self.head_dim)
        k = self.k_proj(k).reshape(B, T, self.num_heads, self.head_dim)
        v = self.v_proj(v).reshape(B, T, self.num_heads, self.head_dim)
        attn = torch.einsum('bhnd,bhnd-&gt;bhn', q, k) / self.head_dim**0.5
        attn = attn.softmax(dim=2)
        out = torch.einsum('bhn,bhnd-&gt;bhnd', attn, v)
        out = out.reshape(B, T, E)
        out = self.out_proj(out)
        return out

class TransformerEncoder(torch.nn.Module):
    def __init__(self, embed_dim, num_heads, num_layers):
        super().__init__()
        self.layers = torch.nn.ModuleList([
            torch.nn.Sequential(
                MultiHeadAttention(embed_dim, num_heads),
                torch.nn.LayerNorm(embed_dim),
                torch.nn.Linear(embed_dim, embed_dim),
                torch.nn.GELU(),
                torch.nn.Dropout(0.1),
            )
            for _ in range(num_layers)
        ])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

class VisionTransformer(torch.nn.Module):
    def __init__(self, patch_size, in_channels, embed_dim, num_heads, num_layers, num_classes):
        super().__init__()
        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)
        self.encoder = TransformerEncoder(embed_dim, num_heads, num_layers)
        self.classifier = torch.nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        x = self.patch_embed(x)
        x = self.encoder(x)
        x = x[:, cls_token]
        x = self.classifier(x)
        return x

model = VisionTransformer(patch_size, in_channels, embed_dim, num_heads, num_layers, num_classes)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()

from torch.utils.data import DataLoader

dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)

for epoch in range(epochs):
    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss = criterion(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print('Epoch: {}, Batch: {}, Loss: {:.4f}'.format(
                epoch, batch_idx, loss.item()))
</code></pre>
<p>I got this following error</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-49-fcd52f81880e&gt; in &lt;cell line: 1&gt;()
      2     for batch_idx, (data, target) in enumerate(dataloader):
      3         data, target = data.to(device), target.to(device)
----&gt; 4         output = model(data)
      5         loss = criterion(output, target)
      6         optimizer.zero_grad()

8 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)
    112 
    113     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 114         return F.linear(input, self.weight, self.bias)
    115 
    116     def extra_repr(self) -&gt; str:

RuntimeError: mat1 and mat2 shapes cannot be multiplied (30x50176 and 768x768)
</code></pre>
<p>How to fix it please?
I want to know how to solve the problem</p>
","0","Question"
"78099801","","<p>I am training a model to differentiate between real and fake data.
I have a real dataset and a fake dataset, and since I didn't know how to train a single model on both, I am now training a model for real and a model for fake.</p>
<p>The problem is that training any of the models seems impossible, as I get this error:</p>
<pre><code>ValueError: Input X contains infinity or a value too large for dtype('float16')
</code></pre>
<p>For this code</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

scaler = RobustScaler()
X = np.load(&quot;fake_x.npy&quot;)
x_data_flat = X.reshape(X.shape[0], -1)
x_data_scaled = scaler.fit_transform(x_data_flat)
y = np.load(&quot;fake_y.npy&quot;)
x_data_flat, y = shuffle(x_data_flat, y)
X_train,X_test,y_train,y_test = train_test_split(x_data_scaled,y,test_size=0.1, train_size=0.1)

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(&quot;Accuracy:&quot;, accuracy)

print(&quot;Classification Report:&quot;)
print(classification_report(y_test, y_pred))


conf_matrix = confusion_matrix(y_test, y_pred)
print(&quot;Confusion Matrix:&quot;)
print(conf_matrix)
</code></pre>
<p>The data is certainly too large, x.shape = (750,1998,101) and y.shape = (750,496).
Data Link: <a href=""https://drive.google.com/drive/folders/1EYnOIOWP17ALs-903ESFM-02_6VszJEx"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1EYnOIOWP17ALs-903ESFM-02_6VszJEx</a>
For the classification itself, I wouldl use a similarity score on the input and whoever gets a higher similarity score the input will belong to its class.</p>
<p>Using a 2D spectrogram that was originally real.npz but extracted to real_x.npy and real_y.npy to make it easier, the model should recognize whether the input belongs to real or to the fake.npz which I extracted to fake_x.npy and fake_y.npy</p>
","0","Question"
"78100096","","<p>I wanted to predict the time series count of Dengue cases for given covariates using a 1D CNN model. The loss function and metrics such as MSE and MAE seem satisfactory. However, the predicted plots for both the training and testing sets do not match the actual data. I'm unsure what went wrong. Here are the complete codes:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Load the data
file_path = 'https://gist.githubusercontent.com/JishanAhmed2019/a7666a3651d27bf03dc93e63aac896b0/raw/f93f28aeaa41418689744a4fbb5bde29114f9872/Dengue.csv'
data = pd.read_csv(file_path, index_col='Date', sep='\t', parse_dates=True)

# Split the data into training and testing sets before scaling
split_fraction = 0.85
split_point = int(len(data) * split_fraction)

train_data = data.iloc[:split_point]
test_data = data.iloc[split_point:]

# Extract features and targets from both sets
X_train, y_train = train_data.drop('Dhaka_Dengue', axis=1), train_data['Dhaka_Dengue']
X_test, y_test = test_data.drop('Dhaka_Dengue', axis=1), test_data['Dhaka_Dengue']

# Apply scaling separately to avoid data leakage
scaler_features = MinMaxScaler()
scaler_target = MinMaxScaler()

X_train_scaled = scaler_features.fit_transform(X_train)
X_test_scaled = scaler_features.transform(X_test)

y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1)).flatten()

# Manually split the training data to create a validation set
val_fraction = 0.15
val_split_point = int(len(X_train_scaled) * (1 - val_fraction))

X_train_final = X_train_scaled[:val_split_point]
y_train_final = y_train_scaled[:val_split_point]
X_val = X_train_scaled[val_split_point:]
y_val = y_train_scaled[val_split_point:]

# Reshape for 1D CNN input
X_train_final_reshaped = X_train_final.reshape((X_train_final.shape[0], X_train_final.shape[1], 1))
X_val_reshaped = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the 1D CNN model
model = Sequential([
    Conv1D(32, 5, padding='same', activation=LeakyReLU(alpha=0.1), input_shape=(X_train_final_reshaped.shape[1], 1)),
    MaxPooling1D(2, padding=&quot;same&quot;),
   # Conv1D(16, 5, padding='same', activation=LeakyReLU(alpha=0.1)),
   # MaxPooling1D(2, padding=&quot;same&quot;),
    Flatten(),
    #Dense(32, activation='relu'),
    Dropout(0.20),
    Dense(1)
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')

# Train the model
history = model.fit(
    X_train_final_reshaped, y_train_final,
    epochs=500,
    batch_size=32,
    verbose=1,
    validation_data=(X_val_reshaped, y_val),
    callbacks=[early_stopping]
)

# Make predictions
train_predictions = model.predict(X_train_final_reshaped)
val_predictions = model.predict(X_val_reshaped)
test_predictions = model.predict(X_test_reshaped)

# Inverse transform predictions and actual values to original scale
train_predictions_inverse = scaler_target.inverse_transform(train_predictions).flatten()
val_predictions_inverse = scaler_target.inverse_transform(val_predictions).flatten()
test_predictions_inverse = scaler_target.inverse_transform(test_predictions).flatten()

y_train_inverse = scaler_target.inverse_transform(y_train_final.reshape(-1, 1)).flatten()
y_val_inverse = scaler_target.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_test_inverse = scaler_target.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()

# Calculate and print RMSE and MAE for training, validation, and test sets
train_rmse = np.sqrt(mean_squared_error(y_train_inverse, train_predictions_inverse))
val_rmse = np.sqrt(mean_squared_error(y_val_inverse, val_predictions_inverse))
test_rmse = np.sqrt(mean_squared_error(y_test_inverse, test_predictions_inverse))

train_mae = mean_absolute_error(y_train_inverse, train_predictions_inverse)
val_mae = mean_absolute_error(y_val_inverse, val_predictions_inverse)
test_mae = mean_absolute_error(y_test_inverse, test_predictions_inverse)

print(&quot;Training RMSE:&quot;, train_rmse, &quot;MAE:&quot;, train_mae)
print(&quot;Validation RMSE:&quot;, val_rmse, &quot;MAE:&quot;, val_mae)
print(&quot;Testing RMSE:&quot;, test_rmse, &quot;MAE:&quot;, test_mae)

# Plot training and validation loss
plt.figure(figsize=(10, 4))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Plot actual vs. predicted for the test set
plt.figure(figsize=(10, 4))
plt.plot(test_data.index, y_test_inverse, label='Actual')
plt.plot(test_data.index, test_predictions_inverse, label='Predicted')
plt.title('Test Set Actual vs. Predicted')
plt.legend()
plt.show()


# Plot the actual vs. predicted values for the training set
plt.figure(figsize=(14, 5))
plt.plot(train_dates, y_train_inverse, label='Train Actual')
plt.plot(train_dates, train_predictions_inverse, label='Train Predictions')
plt.title('Training Predictions vs Actual')
plt.ylabel('Dengue Incidents')
plt.xlabel('Date')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# Plot the actual vs. predicted values for the testing set
plt.figure(figsize=(14, 5))
plt.plot(test_dates, y_test_inverse, label='Test Actual')
plt.plot(test_dates, test_predictions_inverse, label='Test Predictions')
plt.title('Testing Predictions vs Actual')
plt.ylabel('Dengue Incidents')
plt.xlabel('Date')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 7))  # Set the figure size for better readability
plt.plot(data.index, data['Dhaka_Dengue'], label='Actual Dengue Count', color='blue')
plt.title('Actual Dengue Count Time Series')
plt.xlabel('Date')
plt.ylabel('Dengue Count')
plt.legend()
plt.xticks(rotation=45)  # Rotate date labels for better readability
plt.tight_layout()
plt.show()

</code></pre>
","-1","Question"
"78102637","","<p>I'm working on a machine learning project in PyTorch where I need to optimize a model using the full batch gradient descent method. The key requirement is that the optimizer should use all the data points in the dataset for each update. My challenge with the existing torch.optim.SGD optimizer is that it doesn't inherently support using the entire dataset in a single update. This is crucial for my project as I need the optimization process to consider all data points to ensure the most accurate updates to the model parameters.</p>
<p>Additionally, I would like to retain the use of Nesterov momentum in the optimization process. I understand that one could potentially modify the batch size to equal the entire dataset, simulating a full batch update with the SGD optimizer. However, I'm interested in whether there's a more elegant or direct way to implement a true Gradient Descent optimizer in PyTorch that also supports Nesterov momentum.</p>
<p>Ideally, I'm looking for a solution or guidance on how to implement or configure an optimizer in PyTorch that meets the following criteria:</p>
<ul>
<li>Utilizes the entire dataset for each parameter update (true Gradient Descent behavior).</li>
<li>Incorporates Nesterov momentum for more efficient convergence.</li>
<li>Is compatible with the rest of the PyTorch ecosystem, by subclassing torch.optim.Optimizer</li>
</ul>
","0","Question"
"78102877","","<p>I made a neural network model with PyTorch, it kind of resembles the vgg19 model. Every time i enter a value for batch size i got the error:</p>
<pre><code>ValueError: Expected input batch_size (1) to match target batch_size (16).
</code></pre>
<p>This is not happening with the batch size value of 1, it starts training without any complication.</p>
<p>I simply want to alter the batch size without getting any error.</p>
<p>I could not find (or understand) a solution for this issue by myself. I provide the model script below.</p>
<p>Note : In the train() method of the model, a defult value of batch size is given as 16. With this configuration it throws error as i showed above, i set the batch size value to 1 when i call this method and it works fine.</p>
<pre><code>import os
import glob

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

from torchvision import datasets, transforms

class ICModel(nn.Module):

    def __init__(self):
        super().__init__()
        # CNN
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2)

        # FULLY CONNECTED LAYERS
        self.fc1 = nn.Linear(61952, 256)
        self.fc2 = nn.Linear(256, 64)
        self.out = nn.Linear(64, 2)
    def forward(self, x):
        # CONV - 1
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, kernel_size=3, stride=1)
        # CONV - 2
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, kernel_size=3, stride=1)
        # CONV - 3
        x = F.relu(self.conv3(x))
        x = F.max_pool2d(x, kernel_size=3, stride=1)
        # CONV - 4
        x = F.relu(self.conv4(x))

        flattened_size = x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3]

        x = x.view(-1, flattened_size)
        # FULLY CONNECTED LAYERS
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)

        return F.log_softmax(x, dim=1)

    def train(self, dataset_dir='', epochs=5, batch_size=16, seed=35, learning_rate=0.001, model_weights_path=''):
        if dataset_dir == '':
            raise Exception(&quot;Please enter a valid dataset directory path!&quot;)

        train_correct = []
        train_losses = []

        torch.manual_seed(seed)

        # CRITERION AND OPTIMIZER SETUP
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)

        optim_width, optim_height = 224, 224

        data_transforms = transforms.Compose([
            transforms.Resize((optim_width, optim_height)),  # Resize images to average dimensions
            transforms.ToTensor(),  # Convert images to PyTorch tensors
            transforms.Normalize(mean=[0.456, 0.456, 0.456], std=[0.456, 0.456, 0.456])  # Normalize images
        ])

        dataset = datasets.ImageFolder(root=dataset_dir, transform=data_transforms)
        train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            trn_corr = 0

            for b, (X_train, y_train) in enumerate(train_loader):
                b += 1
                y_pred = self(X_train)
                loss = criterion(y_pred, y_train)

                predicted = torch.max(y_pred, dim=1)[1]
                batch_corr = (predicted == y_train).sum()

                trn_corr += batch_corr.item()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                if b % 4 == 0:
                    print(f'Epoch: {epoch}  Batch: {b}  Loss: {loss.item()}')

        train_losses.append(loss)
        train_correct.append(trn_corr)

        if (model_weights_path != '') &amp; os.path.exists(model_weights_path) &amp; os.path.isdir(model_weights_path):
            torch.save({
                'model_state_dict': self.state_dict(),
                'optimizer_state_dict': optimizer.state_dict()
            }, model_weights_path)

    def test(self, dataset_dir='', batch_size=16):
        if dataset_dir == '':
            raise Exception(&quot;Please enter a valid dataset directory path!&quot;)

        optim_width, optim_height = 224, 224
        test_losses = []
        tst_crr = 0

        criterion = nn.CrossEntropyLoss()

        data_transforms = transforms.Compose([
            transforms.Resize((optim_width, optim_height)),  # Resize images to average dimensions
            transforms.Grayscale(),
            transforms.ToTensor(),  # Convert images to PyTorch tensors
            transforms.Normalize(mean=[0.456], std=[0.456])  # Normalize images
        ])

        dataset = datasets.ImageFolder(root=dataset_dir, transform=data_transforms)
        test_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)

        with torch.no_grad():
            for b, (X_test, y_test) in enumerate(test_loader):
                y_val = self(X_test)
                predicted = torch.max(y_val.data, dim=1)[1]
                tst_crr += (predicted == y_test).sum()

            loss = criterion(y_val, y_test)
            test_losses.append(loss.item())

            test_results = {
                'true_positive': tst_crr,
                'false_positive': len(dataset.imgs) - tst_crr
            }

        return test_results, test_losses


</code></pre>
<p>And this is the full traceback of the error :</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/eaidy/Repos/ML/inclination-classification-pytorch/src/main.py&quot;, line 12, in &lt;module&gt;
    ic_model.train(dataset_dir=train_dataset_absolute_path, epochs=1, batch_size=16, learning_rate=1e-5)
  File &quot;/Users/eaidy/Repos/ML/inclination-classification-pytorch/src/models/cnn_model.py&quot;, line 94, in train
    loss = criterion(y_pred, y_train)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/eaidy/Repos/ML/inclination-classification-pytorch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/eaidy/Repos/ML/inclination-classification-pytorch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/eaidy/Repos/ML/inclination-classification-pytorch/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py&quot;, line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/eaidy/Repos/ML/inclination-classification-pytorch/.venv/lib/python3.11/site-packages/torch/nn/functional.py&quot;, line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Expected input batch_size (1) to match target batch_size (16).
</code></pre>
","0","Question"
"78111173","","<p>I think my torchserve loaded config.properties correctly because the number of worker is 2 as I set. But the batch_size is 1 instead of 20.</p>
<p>Anyone has an idea what might go wrong ? Thanks !</p>
<p>I have checked and torchserve load config.properties correctly, alas it ignored the batch_size and max_batch_delay specified in config.properties.</p>
<p>Here is my config.properties for the reference</p>
<pre><code>inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
log_file=/ml_server/logs/torchserve.log
default_workers_per_model=2
number_of_netty_threads=32
job_queue_size=1000
batch_size=20
max_batch_delay=10
</code></pre>
<p>Below is the log, worker with batchSize: 1</p>
<pre><code>ml-server  | 2024-03-06T00:11:11,091 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: _model, batchSize: 1
ml-server  | 2024-03-06T00:11:11,091 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: _model, batchSize: 1
</code></pre>
","0","Question"
"78112023","","<p>I am working on a classification problem related to heart disease using RandomForestClassifier. While performing hyperparameter tuning on RandomForestClassifier, I am facing the following issue. I am using <code>sklearn</code> <code>Pipeline</code> and <code>ColumnTransformer for preprocessing</code>.</p>
<pre><code>Error: 720 fits failed out of a total of 2160.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.
UserWarning: One or more of the test scores are non-finite
</code></pre>
<pre><code>numerical_pipeline = Pipeline(
steps=[('scaler',StandardScaler())]
)

categorical_pipeline = Pipeline(
steps=[('encoder',OneHotEncoder(handle_unknown='ignore'))]  
)

preprocessor = ColumnTransformer(
[('numerical_pipeline',numerical_pipeline,numerical_features),
 ('categorical_pipeline',categorical_pipeline,categorical_features)]`

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)`

scaled_X_train = preprocessor.fit_transform(X_train)
scaled_X_test = preprocessor.transform(X_test)`

param_grid={'max_depth':[3,5,10,None],
          'n_estimators':[10,100,200],
          'max_features':[1,3,5,7],
          'min_samples_leaf':[1,2,3],
          'min_samples_split':[1,2,3]
       }

grid = GridSearchCV(RandomForestClassifier(),param_grid=param_grid,cv=5,scoring='accuracy',verbose=True,n_jobs=-1)
grid.fit(scaled_X_train,y_train)
</code></pre>
","-3","Question"
"78112607","","<p>I'm trying to use gradient descent on a data set. What I have written is</p>
<pre><code>import numpy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('C:/Users/Teacher/Downloads/data.csv')
X = data.iloc[:, 0]  # selects all data from first column in data
Y = data.iloc[:, 1]
plt.scatter(X,Y)
plt.show()
n = len(X)

a = 0
b = 0
L = .001

for i in range(1000):
    y_predicted = a * X + b
    pd_a = (1 / n) * sum((y_predicted - Y) * X)
    pd_b = (1 / n) * sum(y_predicted - Y)
    a = a - L * pd_a
    b = b - L * pd_b
print(a, b)
plt.scatter(X, Y)
c, d = numpy.polyfit(X, Y, 1)
print(c, d)
plt.plot([min(X), max(X)], [a * x + b for x in [min(X), max(X)]], [c * x + d for x in [min(X), max(X)]])
plt.show()
</code></pre>
<p>If I instead define X and <code>Y = np.random.rand(20)</code>, then everything seems to work fine, so I the issue appears to be with the iput from csv.
However, the scatterplot for X and Y is still fine, even when I define them as the first and second column of my data set, so I'm not sure what's going on.</p>
<p>Edit: Here is an image of the scatterplot after defining X = data.iloc[:, 0]
Y = data.iloc[:, 1]</p>
<p><a href=""https://i.sstatic.net/rWV6n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rWV6n.png"" alt=""enter image description here"" /></a></p>
<p>Here is an image of the plot and line at the end of the code.</p>
<p><a href=""https://i.sstatic.net/RVNyl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RVNyl.png"" alt=""enter image description here"" /></a></p>
<p>The result of print(data.head()):</p>
<p><a href=""https://i.sstatic.net/J0KdG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/J0KdG.png"" alt=""enter image description here"" /></a></p>
<p>Edit: reading just one line of the csv:</p>
<p><a href=""https://i.sstatic.net/NidsE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NidsE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/mDPyq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mDPyq.png"" alt=""enter image description here"" /></a></p>
","-1","Question"
"78112634","","<p>I'm implementing a perceptron model for dummy 2D data. Below is how I generate my data</p>
<pre><code>#numpoint
n = 15
#f(x) = w0 + ax1 + bx2
#then if f(x) = 0
#x2 = (-w0 - ax1)/b 
intercept = 30
a = 4
b = 2
#generate random points from 0 - 20
x1 = np.random.uniform(-20, 20, n) #return a np array
x2 = np.random.uniform(-20, 20, n)
y = []
#plot f(x)
plt.plot(x1, (-intercept - a*x1)/b, 'k-') 
plt.ylabel(&quot;x2&quot;)
plt.xlabel(&quot;x1&quot;)

#plot colored points
for i in range(0, len(x1)):
    f = intercept + a * x1[i] + b * x2[i]
    if (f &lt;= 0):
        plt.plot(x1[i], x2[i], 'ro')
        y.append(-1)
    if (f &gt; 0):
        plt.plot(x1[i], x2[i], 'bo')
        y.append(1)
y = np.array(y)
# Add x0 for threshold
x0 = np.ones(n)
stacked_x = np.stack((x0,x1,x2))
stacked_x
</code></pre>
<p>This is the visualization of the data</p>
<p><a href=""https://i.sstatic.net/4STIs.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>This is my Perceptron Model</p>
<pre><code>class PLA():
    def __init__(self, numPredictors):
        self.w = np.random.rand(1,numPredictors+1) #(1, numPredictors+1)
        self.iter = 0
    def fitModel(self, xData, yData):
        while(True): 
            yhat = np.matmul(self.w, xData).squeeze() #from(1,n) to (,n)
            compare = np.sign(yhat) == yData          
            ind = [i for i in range(0,len(compare)) if compare[i] == False] #misclassified index
            print(len(ind))
            if len(ind) == 0:    
                break
            for i in ind:
                update = yData[i]* xData[:, i] #1d array
                self.w = self.w + np.transpose(update[:,np.newaxis]) #tranpose to match weight's shape
            self.iter += 1
</code></pre>
<p>When I visualize the model</p>
<pre><code>pla1 = PLA(2)
pla1.fitModel(stacked_x, y)
#plot colored points
for i in range(0, len(x1)):
    if (y[i] == -1):
        plt.plot(x1[i], x2[i], 'ro')
    if (y[i] == 1):
        plt.plot(x1[i], x2[i], 'bo')
plt.plot(x1, (-pla1.w[0][0] - pla1.w[0][1]*x1)/(pla1.w[0][1]), 'g-', label = &quot;PLA&quot;)
plt.plot(x1, (-intercept - a*x1)/b, 'k-', label = &quot;f(x)&quot;)
plt.xlabel(&quot;x1&quot;)
plt.ylabel(&quot;x2&quot;)
plt.legend()
</code></pre>
<p>The line I got from the perceptron algorithm is incorrect</p>
<p><a href=""https://i.sstatic.net/zIkKz.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>This is another run with different data parameters and sample size (n = 30)</p>
<p><a href=""https://i.sstatic.net/UsCld.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried printing out the update at each iteration and it was working as I intended. I'm not sure what is causing my algorithm to stop even though there're still misclassified points. I've been stucked on this for a few day.  I really appreciate any input.</p>
","0","Question"
"78112934","","<p>I am trying to build a pdf chat bot where you upload a pdf and ask questions related to you pdf. For this, I was thinking of a RAG based application . So i wanted to create vector embeddings of my input pdf but when i do this,</p>
<pre><code>from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-en-v1.5&quot;)
index_creator = VectorstoreIndexCreator(
    vectorstore_cls = Cassandra,
    embedding = embed_model,
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 400,
        chunk_overlap = 30
    ),

    vectorstore_kwargs={
        'session': session,
        'keyspace': keyspace,
        'table_name': table_name
    }
)
</code></pre>
<p>I am getting validation error.</p>
<pre><code>---------------------------------------------------------------------------
ValidationError                           Traceback (most recent call last)
&lt;ipython-input-17-b83dc7fd1587&gt; in &lt;cell line: 4&gt;()
      2 keyspace = &quot;pdf_qa_name&quot;
      3 
----&gt; 4 index_creator = VectorstoreIndexCreator(
      5     vectorstore_cls = Cassandra,
      6     embedding = embed_model,

/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py in __init__(__pydantic_self__, **data)
    339         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)
    340         if validation_error:
--&gt; 341             raise validation_error
    342         try:
    343             object_setattr(__pydantic_self__, '__dict__', values)

ValidationError: 1 validation error for VectorstoreIndexCreator
embedding
  instance of Embeddings expected (type=type_error.arbitrary_type; expected_arbitrary_type=Embeddings)
</code></pre>
<p>Any idea?</p>
<p>Tried 2 different models(Jina and BAAI/bge). The error is not going. I am using open ai gpt 3.5 api.</p>
","1","Question"
"78114397","","<p>I want to use the <code>StackingClassifier</code> class from <code>sklearn</code> to create an ensamble learner.</p>
<p>Specifically, I have two different types of data: 2D images and classical tabular data. I would like to use a MLP classifier for the 2D images and a RandomForest classifier for the tabular data. Then I would like to use a RandomForest classifier to combine the predictions from the two models above into my final predicitions.</p>
<p>Since there is no way to provide separate training data to an instance of <code>StackingClassifier</code> I have resorted to the following trick: I have created two <code>Pipeline</code>s with a custom selector that return a value from a dictionary. While I can fit and predict the pipelines separately, I have a <code>ValueError: Found input variables with inconsistent numbers of samples</code> when I try to fit and predict the stacked model.</p>
<p>Below the code that I have used:</p>
<pre><code>import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

#Customized Selector 
class DictionarySelector(BaseEstimator, TransformerMixin):
    def __init__(self, in_dict, key):

        self.in_dict = in_dict
        self.key = key
        
    def fit(self, x, y = None):
        return(self)
    
    def transform(self, key):
        return(self.in_dict[self.key])
    
    
        

# Generate some example data 
# For demonstration purposes, I'm using the Iris dataset
data = load_iris()
X_images = data.data[:, :2]  # Let's say that these are my images
X_vector = data.data[:, 2:]  # Unidimensional vector 
y = data.target  # Target classes (3 classes)

# Split the data into train and test sets
X_images_train, X_images_test, X_vector_train, X_vector_test, y_train, y_test = train_test_split(
    X_images, X_vector, y, test_size=0.2, random_state=42
)

# Define the MLP for images
image_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation=&quot;relu&quot;, max_iter=1000)

# Define the Random Forest for vectors
vector_model = RandomForestClassifier(n_estimators=100, criterion=&quot;gini&quot;, max_depth=None)
#Create dict with the two data types
in_dict = {'images' : X_images_train, 'vector' : X_vector_train}
#MLP pipeline for images
pipe_images = Pipeline([
    ('select', DictionarySelector(in_dict, 'images')),
    ('clf', image_model)
])

#check: it works
print(pipe_images.fit(X_images_train, y_train).predict(X_images_test))

#features pipeline
pipe_vector = Pipeline([
    ('select', DictionarySelector(in_dict, 'vector')),
    ('clf', vector_model)
])
#check: it works
print(pipe_images.fit(X_vector_train, y_train).predict(X_vector_test))

# Create a stacked model
stacked_model = StackingClassifier(
    estimators=[
        (&quot;image_mlp&quot;, pipe_images),
        (&quot;vector_rf&quot;, pipe_vector),
    ],
    final_estimator=RandomForestClassifier(n_estimators=100),
)

#The stacked model throw the ValueError
stacked_model.fit(in_dict, y_train)
</code></pre>
","1","Question"
"78115138","","<p>To get familiar with Gradient Descent algorithm, I tried to create my own Linear Regression model. It works fine for few data points. But when try to fit it using more data, w0 and w1 are always increasing in size. Can some explain this phenomena?</p>
<pre><code>class LinearRegression:
    def __init__(self, x_vector, y_vector):

        self.x_vector = np.array(x_vector, dtype=np.float64)
        self.y_vector = np.array(y_vector, dtype=np.float64)
        self.w0 = 0
        self.w1 = 0

    def _get_predicted_values(self, x):
        formula = lambda x: self.w0 + self.w1 * x
        return formula(x)

    def _get_gradient_matrix(self):
        predictions = self._get_predicted_values(self.x_vector)
        w0_hat = sum((self.y_vector - predictions))
        w1_hat = sum((self.y_vector - predictions) * self.x_vector)

        gradient_matrix = np.array([w0_hat, w1_hat])
        gradient_matrix = -2 * gradient_matrix

        return gradient_matrix

    def fit(self, step_size=0.001, num_iterations=500):
        for _ in range(1, num_iterations):
            gradient_matrix = self._get_gradient_matrix()
            self.w0 -= step_size * (gradient_matrix[0])
            self.w1 -= step_size * (gradient_matrix[1])

    def _show_coeffiecients(self):
        print(f&quot;w0: {self.w0}\tw1: {self.w1}\t&quot;)

    def predict(self, x):
        y = self.w0 + self.w1 * x
        return y
</code></pre>
<pre><code># This works fine
x = [x for x in range(-3, 3)]
f = lambda x: 5 * x - 7
y = [f(x_val) for x_val in x]

model = LinearRegression(x, y)
model.fit(num_iterations=3000)

model.show_coeffiecients() #output : w0: -6.99999999999994   w1: 5.00000000000002

#While this doesn't
x = [x for x in range(-50, 50)] # Increased the number of x values
f = lambda x: 5 * x - 7
y = [f(x_val) for x_val in x]

model = LinearRegression(x, y)
model.fit(num_iterations=3000)

model.show_coeffiecients()
</code></pre>
<p>The last line produces a warning:</p>
<pre><code>RuntimeWarning: overflow encountered in multiply
w1_hat = sum((self.y_vector - predictions) * self.x_vector)
formula = lambda x: self.w0 + self.w1 * x
</code></pre>
","-1","Question"
"78118283","","<p>I am trying to create a PyTorch dataloader for my dataset. Each image has a certain number of cars and a bounding box for each of them, not all images have the same amount of bounding boxes.</p>
<p>You probably wont be able to run it, but here is some info.
This is my data loader</p>
<pre><code>class AGR_Dataset(Dataset):
    def __init__(self, annotations_root, img_root, transform=None):
        &quot;&quot;&quot;
        Arguments:
            annotations_root (string): Path to the csv file with annotations.
            img_root (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        &quot;&quot;&quot;
        self.annotations_root = annotations_root
        self.img_root = img_root
        self.transform = transform

    def __len__(self):
        return len(self.annotations_root)
    
    def __getitem__(self, idx):
        # idx may be the index or image name, I think image naem
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        idx_name = os.listdir(self.img_root)[idx]
        # print(idx_name)
        
        img_name = os.path.join(self.img_root, idx_name)
        annotation_data = os.path.join(self.annotations_root, f&quot;{idx_name.removesuffix('.jpg')}.txt&quot;)
        # print(img_name, annotation_data)

        image = io.imread(img_name)

        with open(annotation_data, 'r') as file:
            lines = file.readlines()
            img_data = []
            img_labels = []
            for line in lines:
                line = line.split(',')
                line = [i.strip() for i in line]
                line = [float(num) for num in line[0].split()]
                img_labels.append(int(line[0]))
                img_data.append(line[1:])

        boxes = tv_tensors.BoundingBoxes(img_data, format='CXCYWH', canvas_size=(image.shape[0], image.shape[1]))

        # sample = {'image': image, 'bbox': boxes, 'labels': img_labels}
        sample = {'image': image, 'bbox': boxes}

        if self.transform:
            sample = self.transform(sample)

        print(sample['image'].shape)
        print(sample['bbox'].shape)
        # print(sample['labels'].shape)
        return sample
</code></pre>
<p>I run my transforms and create the dataloader</p>
<pre><code>data_transform = v2.Compose([
    v2.ToImage(),
    # v2.Resize(680),
    v2.RandomResizedCrop(size=(680, 680), antialias=True),
    # v2.ToDtype(torch.float32, scale=True),
    v2.ToTensor()
])

transformed_dataset = AGR_Dataset(f'{annotations_path}/test/', 
                        f'{img_path}/test/',
                        transform=data_transform)

dataloader = DataLoader(transformed_dataset, batch_size=2,
                        shuffle=False, num_workers=0)
</code></pre>
<p>Then I am trying to iterate through it with this, and eventually view and image with the bounding boxes.</p>
<pre><code>for i, sample in enumerate(dataloader):
    print(i, sample)
    print(i, sample['image'].size(), sample['bbox'].size())

    if i == 4:
        break
</code></pre>
<p>With a batch size of 1, it runs properly, with a batch size of 2, I get this error</p>
<pre><code>torch.Size([3, 680, 680])
torch.Size([12, 4])

torch.Size([3, 680, 680])
torch.Size([259, 4])

RuntimeError: stack expects each tensor to be equal size, but got [12, 4] at entry 0 and [259, 4] at entry 1
</code></pre>
<ol>
<li>I believe it is due to the number of bounding boxes not being equal, but how do I overcome this?</li>
<li>Do I need the ToTensor in my transforms? I am starting to think I don't as v2 uses ToImage(), and ToTensor is becoming depreciated.</li>
</ol>
<p>Any other comments or help would be appreciated.
I am not sure how to create a working example, I will continue to try.</p>
<p><strong>What I have tried</strong>
I have tried not loading the bounding boxes in as tensors, by commenting the <code>tv_tensors.BoundingBoxes</code> line in the dataloader, but then for some reason my resize doesnt work properly.</p>
<p>I just tried splitting bboxes and images like this in the dataloader</p>
<pre><code>sample = image
    target = {'bbox': boxes, 'labels': img_labels}
</code></pre>
<p>No luck with that</p>
","0","Question"
"78119974","","<p>I am a newbie in the NN field and I am doing some training with pytorch.<br />
I decided to make a simple vanilla NN.<br />
I used a personal dataset i had with 2377 numerical features and 6277 examples.</p>
<p>My first try was to make the NN predict each single example, so the pseudocode would look like</p>
<pre><code>for i in range(...):
    X = ... # features
    y = ... # outcome
    y_pred = model(X[i])
    loss = criterion(y_pred, y)

    y_pred.size # [1,1]
    y.size # [1,1]
</code></pre>
<p>This took about 10 seconds per epoch and i decided to improve it using mini batches.</p>
<p>So i define the batch size at the beginning and the NN in Pytorch is defined like this</p>
<pre><code>batch_size = 30
n_inputs = X.size[1] #2377

## 2 hidden layers
model = nn.Sequential(
    nn.Linear(n_inputs, 1024),
    nn.ReLU(),
    nn.Linear(1024, 512),
    nn.ReLU(),
    nn.Linear(512, 356),
    nn.ReLU(),
    nn.Linear(356, batch_size),
    nn.ReLU(),
)
</code></pre>
<p>And then I do the training in batches</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(5):
    totalloss = 0  
    permutation = torch.randperm(X.size()[0])
    for i in range(0, X.size()[0], batch_size):
        optimizer.zero_grad()
        indices = permutation[i:i+batch_size]
        batch_x, batch_y = x[indices], y[indices]

        ypred = model(batch_x)
        loss = criterion(ypred, batch_y) 
        totalloss += loss.item()
        
        ## update the weights
        loss.backward()
        optimizer.step()
</code></pre>
<p>Now the problem is that my NN always outputs 100 values <strong>but</strong> the last batch size can vary.<br />
In fact, if i choose 100 as batch size the last batch will be made of 77 examples (6277%100).</p>
<p>I am sure there is a way around this problem, and that there is a mistake in my structure, but i cannot see it.</p>
<p>Can you help me generalize the training in batch to work with any number of examples and batch size?</p>
","1","Question"
"78119978","","<p>I want use callbacks and eval_set etc.
but i have a problem:</p>
<pre><code>from sklearn.multiclass import OneVsRestClassifier
import lightgbm
</code></pre>
<pre><code>verbose = 100
params = {
    &quot;objective&quot;: &quot;binary&quot;,
    &quot;n_estimators&quot;: 500,
    &quot;verbose&quot;: 0
}
fit_params = {
    &quot;eval_set&quot;: eval_dataset,
    &quot;callbacks&quot;: [CustomCallback(verbose)]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train,  **fit_params)
</code></pre>
<p>how i can hand over fit_params to my estimator? I get</p>
<pre><code>----------------------------------------------------------------------
---&gt; 13 clf.fit(X_train, y_train,  **fit_params)

TypeError: OneVsRestClassifier.fit() got an unexpected keyword argument 'eval_set'
</code></pre>
","1","Question"
"78120856","","<pre><code>import pandas as pd 

file = pd.read_excel('slr06.xlsx')
# Data 
x = pd.DataFrame(file, columns=['X'])
y = pd.DataFrame(file, columns=['Y'])
# Parameters 
w = 0.0
b = 0.0
# HyperParameters 
learning_rate = 0.01

# Creating Gradient Descent 

def descend(x, y, w, b, learning_rate):
    dl_dw = 0.0
    dl_db = 0.0
    n = x.shape[0]
    # loss = (y-yhat)**2
    for xi, yi in zip(x, y):
        dl_dw = -2*xi*(yi-(w*xi+b))
        dl_db = -2*(yi-(w*xi+b))
    # Make an Update
    w = w - learning_rate*(1/n)*dl_dw
    b = b - learning_rate*(1/n)*dl_db
    
    return w, b
# Iteratively make updates 
for epoch in range(500):
    w, b = descend(x, y, w, b, learning_rate)
    yhat = w * x + b
    loss = np.divide(np.sum((y - yhat)**2, axis=0), x.shape[0])
    print(f&quot;{epoch} loss is {loss} parameters w: {w} | b:{b}&quot;)

</code></pre>
<p>I am new to ML and I was practicing with a dataset and I got error</p>
<p><a href=""https://i.sstatic.net/RoVwl.png"" rel=""nofollow noreferrer"">This is the picture of a dataset - </a></p>
<p><a href=""https://i.sstatic.net/HGDdt.png"" rel=""nofollow noreferrer"">This is the picture of error - </a></p>
<p>I was creating a gradient descent model that I have practiced before with a different dataset. This I tried with random data to practice and better understand Gradient Descent</p>
","-1","Question"
"78126160","","<p>I checked all the tensors and input parameters, they were all leaf, according to the code below,</p>
<pre><code>def train_step(w1,b1):
    print(&quot;w=&quot;,w1)
    trainable_variables = [w1,b1]
    optimizer = torch.optim.SGD(trainable_variables, lr=learning_rate)
    loss = Variable(loss2_function(), requires_grad = True)
    print(loss.backward())
    with torch.no_grad():
        w1 -=(learning_rate * w1.grad)
        b1 -= (learning_rate * b1.grad)
        w1.grad.zero_()
        b1.grad.zero_()
    optimizer.step()
    optimizer.zero_grad()
</code></pre>
<p>I still get none, and even with the change in learning rate, weight and bias, the network still does not work, please guide me.</p>
","0","Question"
"78127612","","<p>I am working on a basic text classification problem, I want to use a stacking classifier along with some fine-tuning of the parameters of my base classifiers to get high-accuracy results.</p>
<p>My dataset has 8000 rows and 2 cols (text and class). The below piece of code seems to be stuck and I am not well versed in the field (beginner) to spot the problem.</p>
<pre><code>import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import NuSVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score, log_loss, classification_report, confusion_matrix

# Define parameter grids for classifiers
param_grid_nusvc = {
    'nu': [0.1, 0.3, 0.5, 0.7, 0.9],
    'kernel': ['linear', 'rbf'],
}

param_grid_logreg = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2'],
}

# Perform grid search for classifiers with improved clarity
nusvc_grid_search = GridSearchCV(NuSVC(probability=True), param_grid_nusvc, cv=2, scoring='accuracy')  # Use accuracy scoring
logreg_grid_search = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=2, scoring='accuracy')

nusvc_grid_search.fit(X_train, y_train)
logreg_grid_search.fit(X_train, y_train)

# Get best parameters
best_params_nusvc = nusvc_grid_search.best_params_
best_params_logreg = logreg_grid_search.best_params_

# Set up base classifiers with best parameters
best_nusvc = NuSVC(probability=True, **best_params_nusvc)
best_logreg = LogisticRegression(**best_params_logreg)

# Setting up stacking classifier
sc = StackingClassifier(
    estimators=[
        ('NuSVC', best_nusvc),
        ('LDA', LinearDiscriminantAnalysis())
    ],
    final_estimator=best_logreg
)

sc.fit(X_train, y_train)

# Evaluate the combined classifiers
print('****Results****')
train_predictions = sc.predict(X_test)
acc = accuracy_score(y_test, train_predictions)
print(&quot;Accuracy: {:.4%}&quot;.format(acc))

train_predictions_proba = sc.predict_proba(X_test)
ll = log_loss(y_test, train_predictions_proba)
print(&quot;Log Loss: {}&quot;.format(ll))

# Print classification report (optional)
print('\nClassification Report:')
print(classification_report(y_test, train_predictions))

# Print confusion matrix (optional)
print('\nConfusion Matrix:')
print(confusion_matrix(y_test, train_predictions))
</code></pre>
<p>some changes in the above have been made from the advice from chatGPT to guide me on how to fine tune using grid search. The code seems to be stuck (about 20 mins). Without the grid search it seemed to run in around 2-3 mins easily.</p>
","0","Question"
"78127879","","<pre><code>import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

whitewine_data = pd.read_csv('winequality-white.csv', 
delimiter=';')

variables = ['alcohol_cat', 'alcohol', 'sulphates', 'density', 
'total sulfur dioxide', 'citric acid', 'volatile acidity', 
'chlorides']

X = whitewine_data[variables]
y = whitewine_data['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.2)

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

predictions = model.predict([[0.27, 0.36, 0.045, 170, 1.001, 
0.45, 8.9, 0]])
print(f'Predicted Output: {predictions}')
print(f'Accuracy: {accuracy * 100}%')
print(f'F1 Score: {f1 * 100}% ')
</code></pre>
<p>This initial model resulted in a accuracy score of 57%</p>
<p>==============================================================</p>
<pre><code>whitewine_data = pd.read_csv('winequality-white.csv', 
delimiter=';')

# Variables to be dropped from the data set - NOT THE INPUT 
VARIABLES
variables = ['fixed acidity', 'residual sugar', 'free sulfur 
dioxide', 'pH', 'quality', 'isSweet']

X = whitewine_data.drop(variables, axis=1)
y = whitewine_data['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.2)

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

joblib.dump(model, 'WhiteWine_Quality_Predictor.joblib')
</code></pre>
<p>Creating the Saved model</p>
<p>==============================================================</p>
<pre><code>whitewine_data = pd.read_csv('winequality-white.csv', 
delimiter=';') 

variables = ['volatile acidity', 'citric acid', 'chlorides', 
'total sulfur dioxide', 'density', 'sulphates', 'alcohol', 
'alcohol_cat']

X_test = whitewine_data[variables]
y_test = whitewine_data['quality']  

model = joblib.load('WhiteWine_Quality_Predictor.joblib')

y_pred = model.predict(X_test)

f1 = f1_score(y_test, y_pred, average='weighted')
accuracy = accuracy_score(y_test, y_pred)
predictions = model.predict([[0.27, 0.36, 0.045, 170, 1.001, 
0.45, 10.9, 3]])

print(f'F1 Score: {f1 * 100}%')
print(f'Model Accuracy: {accuracy * 100}%')
print(f'Predicted Output: {predictions}')
</code></pre>
<p>Calling saved model now resulted in 92% accuracy</p>
<p>Question: How does calling a saved model result in the increase
of accuracy I saw</p>
","-4","Question"
"78131238","","<p>I'm in the process of Data splitting and Cross Validation.
For the data splitting, I need to extract ONLY the test dataset and leave the rest of the data as is for cross validation. And I'm geeting an error of <em>ValueError: could not convert string to float: 'Curtis RIngraham Directge'</em> at the end of Cross Validation. How should I fix it?</p>
<p><strong>Data Splitting</strong></p>
<pre><code>from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

# First extract our test data and store it in x_test, y_test
features = features_df.to_numpy()
labels = labels_df.to_numpy()
_x, x_test, _y, y_test = train_test_split(features, labels, test_size=0.10, random_state=42)

# set k = 5
k = 5

kfold_spliiter = KFold(n_splits=k)

folds_data = [] # this is an inefficient way but still do it

fold = 1
for train_index, validation_index in kfold_spliiter.split(_x):
    x_train , x_valid = _x[train_index,:],_x[validation_index,:]
    y_train , y_valid = _y[train_index,:] , _y[validation_index,:]
    print (f&quot;Fold {fold} training data shape = {(x_train.shape,y_train.shape)}&quot;)
    print (f&quot;Fold {fold} validation data shape = {(x_valid.shape,y_valid.shape)}&quot;)
    fold+=1
    folds_data.append((x_train,y_train,x_valid,y_valid))
</code></pre>
<p><strong>Cross Validation</strong></p>
<pre><code>best_validation_accuracy = 0
best_model_name = &quot;&quot;
best_model = None

# Iterate over all models
for model_name in all_models.keys():

    print (f&quot;Evaluating {model_name} ...&quot;)
    model = all_models[model_name]

    # Let's store training and validation accuracies for all folds
    train_acc_for_all_folds = []
    valid_acc_for_all_folds = []

    #Iterate over all folds
    for i, fold in enumerate(folds_data):
        x_train, y_train, x_valid, y_valid = fold

        # Train the model
        _ = model.fit(x_train,y_train.flatten())

        # Evluate model on training data
        y_pred_train = model.predict(x_train)

        # Evaluate the model on validation data
        y_pred_valid = model.predict(x_valid)

        # Compute training accuracy
        train_acc = accuracy_score(y_pred_train , y_train)

        # Store training accuracy for each folds
        train_acc_for_all_folds.append(train_acc)

        # Compute validation accuracy
        valid_acc = accuracy_score(y_pred_valid , y_valid.flatten())

        # Store validation accuracy for each folds
        valid_acc_for_all_folds.append(valid_acc)

    #average training accuracy across k folds
    avg_training_acc = sum(train_acc_for_all_folds)/k

    print (f&quot;Average training accuracy for model {model_name} = {avg_training_acc}&quot;)

    #average validation accuracy across k folds
    avg_validation_acc = sum(valid_acc_for_all_folds)/k

    print (f&quot;Average validation accuracy for model {model_name} = {avg_validation_acc}&quot;)

    # Select best model based on average validation accuracy
    if avg_validation_acc &gt; best_validation_accuracy:
        best_validation_accuracy = avg_validation_acc
        best_model_name = model_name
        best_model = model
    print (f&quot;-----------------------------------&quot;)

print (f&quot;Best model for the task is {best_model_name} which offers the validation accuracy of {best_validation_accuracy}&quot;)
</code></pre>
<p>Tried to find any remaining x_train, y_train, x_valid, and y_valid string values, but could not find any.</p>
","-2","Question"
"78136820","","<p>I would Like to know that is there a way to install Nvidia Rapids directly in windows 11 and use rather than using it with wsl2 or docker? Or is there a way to connect jupyter lab host to dataspell?</p>
<p>I tried installing cuMl directly to windows via github but am failing everytime. I want to use cuml in jetbrains dataspell thats why seeking help.</p>
","1","Question"
"78137739","","<p>I have implemented the following code in Keras that uses the California housing dataset in an attempt to plot the values of theta 1 and theta 2, and how the chose of stochastic gradient descent, batch gradient or mini batch influences in the results:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler

# Load California housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target

# Normalize features
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)

def build_model():
    model = Sequential([
    Dense(64, activation=&quot;relu&quot;,input_shape=(8,)),
    Dense(64, activation=&quot;relu&quot;),
    Dense(1)
    ])
    #model.compile(optimizer=&quot;rmsprop&quot;, loss=&quot;mse&quot;, metrics=[&quot;mae&quot;])
    return model


sgd_optimizer = SGD(lr=0.001)  # Stochastic Gradient Descent
minibatch_sgd_optimizer = SGD(lr=0.001)  # Mini-batch Gradient Descent
batch_sgd_optimizer = SGD(lr=0.001)  # Batch Gradient Descent


# Compile the model
model=build_model()
model.compile(loss='mse', optimizer=sgd_optimizer)

theta1_sgd, theta2_sgd = [], []
theta1_minibatch_sgd, theta2_minibatch_sgd = [], []
theta1_batch_sgd, theta2_batch_sgd = [], []


# Function to perform gradient descent and store theta values
def perform_gradient_descent(optimizer, batch_size=None):
    theta1_list, theta2_list = [], []
    loss_history = []
    for _ in range(5):  # Number of epochs
        history=model.fit(X_normalized, y, epochs=1, batch_size=batch_size, verbose=0)
        loss_history.append(history.history['loss'][0])
        weights = model.layers[0].get_weights()[0].flatten()  # Get current theta values
        theta1_list.append(weights[0])
        theta2_list.append(weights[1])
    print(theta1_list,&quot; &quot;,theta2_list)
    return loss_history,theta1_list, theta2_list

# Perform gradient descent with different optimizers
loss_sgd, theta1_sgd, theta2_sgd = perform_gradient_descent(sgd_optimizer, batch_size=1)  # Stochastic Gradient Descent
loss_minibatch_sgd, theta1_minibatch_sgd, theta2_minibatch_sgd = perform_gradient_descent(minibatch_sgd_optimizer, batch_size=32)  # Mini-batch Gradient Descent
loss_batch_sgd, theta1_batch_sgd, theta2_batch_sgd = perform_gradient_descent(batch_sgd_optimizer, batch_size=len(X_normalized))  # Batch Gradient Descent

# Plotting the loss versus number of epochs
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(loss_sgd) + 1), loss_sgd, label='Stochastic Gradient Descent')
plt.plot(range(1, len(loss_minibatch_sgd) + 1), loss_minibatch_sgd, label='Mini-batch Gradient Descent')
plt.plot(range(1, len(loss_batch_sgd) + 1), loss_batch_sgd, label='Batch Gradient Descent')
plt.xlabel('Number of Epochs')
plt.ylabel('Loss')
plt.title('Loss vs. Number of Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the gradient descent trajectories
plt.figure(figsize=(10, 6))
plt.plot(theta1_sgd, theta2_sgd, label='Stochastic Gradient Descent', marker='o')
plt.plot(theta1_minibatch_sgd, theta2_minibatch_sgd, label='Mini-batch Gradient Descent', marker='s')
plt.plot(theta1_batch_sgd, theta2_batch_sgd, label='Batch Gradient Descent', marker='x')
plt.xlabel('Theta 1')
plt.ylabel('Theta 2')
plt.title('Gradient Descent Trajectories')
#plt.xlim(-0.08, -0.05)  # Set limit for Theta 1
#plt.ylim(0.02, 0.03)  # Set limit for Theta 2
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<p>However, the problem I found is that sometimes the values of the list that hold the theta values are Nan and in other cases are normal values. I have noticed this when the number of epochs increase above 10, why is that?</p>
","0","Question"
"78139097","","<p>I am trying to create a PyCaret Model</p>
<pre><code># load dataset
from pycaret.datasets import get_data
insurance = get_data('insurance')

# init environment
from pycaret.regression import *

r1 = setup(insurance, target = 'charges', session_id = 123,
           normalize = True,
           polynomial_features = True,
           bin_numeric_features= ['age', 'bmi'])
</code></pre>
<p>I see 55 columns are now created. How do I get column names of PyCaret Model</p>
","0","Question"
"78139165","","<p>I am new to Julia and trying to fit a simple classification tree</p>
<p>Packages import and env activation:</p>
<pre><code>using Pkg 
Pkg.activate(&quot;.&quot;)

using CSV
using DataFrames
using Random
using Downloads
using ARFFFiles
using ScientificTypes
using DataFramesMeta
using DynamicPipe
using MLJ
using MLJDecisionTreeInterface
</code></pre>
<p>Data:</p>
<pre><code>titanic_reader  = CSV.File(&quot;/home/andrea/dev/julia/titanic.csv&quot;; header = 1);
titanic = DataFrame(titanic_reader);

# remove missing values
titanic =  dropmissing(titanic);


titanic = @transform(titanic, 
    :class=categorical(:class), 
    :sex=categorical(:sex),  
    :survived=categorical(:survived)
    );
</code></pre>
<p>Check the data</p>
<pre><code>first (titanic , 3)

3×4 DataFrame
 Row │ class  sex     age      survived 
     │ Cat…   Cat…    Float64  Cat…     
─────┼──────────────────────────────────
   1 │ 3      male       22.0  N
   2 │ 1      female     38.0  Y
   3 │ 3      female     26.0  Y
</code></pre>
<p>Check the data schema</p>
<pre><code>schema(titanic);


┌──────────┬───────────────┬───────────────────────────────────┐
│ names    │ scitypes      │ types                             │
├──────────┼───────────────┼───────────────────────────────────┤
│ class    │ Multiclass{3} │ CategoricalValue{Int64, UInt32}   │
│ sex      │ Multiclass{2} │ CategoricalValue{String7, UInt32} │
│ age      │ Continuous    │ Float64                           │
│ survived │ Multiclass{2} │ CategoricalValue{String1, UInt32} │
└──────────┴───────────────┴───────────────────────────────────┘
</code></pre>
<p>Schema seems ok to me</p>
<p>Prepare data for modelling:</p>
<pre><code># target and features
y, X = unpack(titanic, ==(:survived), rng = 123);

# partitiont training &amp; test 
(X_trn, X_tst), (y_trn, y_tst)  = partition((X, y), 0.75, multi=true,  rng=123);
</code></pre>
<p>Fit the model:</p>
<pre><code># model
mod = @load DecisionTreeClassifier pkg = &quot;DecisionTree&quot; ;
fm = mod() ;
fm_mach = machine(fm, X_trn, y_trn);
</code></pre>
<p>and here is the problem:</p>
<pre><code>Warning: The number and/or types of data arguments do not match what the specified model
│ supports. Suppress this type check by specifying `scitype_check_level=0`.
│ 
│ Run `@doc DecisionTree.DecisionTreeClassifier` to learn more about your model's requirements.
│ 
│ Commonly, but non exclusively, supervised models are constructed using the syntax
│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are
│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`
│ sample or class weights.
│ 
│ In general, data in `machine(model, data...)` is expected to satisfy
│ 
│     scitype(data) &lt;: MLJ.fit_data_scitype(model)
│ 
│ In the present case:
│ 
│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{3}}, AbstractVector{Multiclass{2}}}}, AbstractVector{Multiclass{2}}}
│ 
│ fit_data_scitype(model) = Tuple{Table{&lt;:Union{AbstractVector{&lt;:Continuous}, AbstractVector{&lt;:Count}, AbstractVector{&lt;:OrderedFactor}}}, AbstractVector{&lt;:Finite}}
└ @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:231
</code></pre>
<p>Clearly when fitting the model:</p>
<pre><code>fit!(fm_mach)
</code></pre>
<p>I get an error</p>
<pre><code>[ Info: It seems an upstream node in a learning network is providing data of incompatible scitype. See above. 
ERROR: ArgumentError: Unordered CategoricalValue objects cannot be tested for order using &lt;. Use isless instead, or call the ordered! function on the parent array to change this
Stacktrace:
</code></pre>
<p>I am almost sure the error depends on the data type specification but, I cannot work the solution out.</p>
","0","Question"
"78139855","","<p>I am trying to encode some text using HuggingFace's mt5-base model. I am using the model as shown below</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

def get_t5_embeddings(texts):
    last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state
    pooled_sentence = torch.max(last_hidden_state, dim=1)
    return pooled_sentence[0].detach().numpy()
</code></pre>
<p>I was doing some experiments when I noticed that the same text had a low cosine similarity score with itself. I did some digging and realized that the model was returning very different embeddings if I did the encoding in batches. To validate this, I ran a small experiment that generated embeddings for <code>Hello</code> and a list of 10 <code>Hello</code>s incrementally. and checking the embeddings of the <code>Hello</code> and the first <code>Hello</code> in the list (both of which should be same).</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(1, 10):
    print(i, (get_t5_embeddings([&quot;Hello&quot;])[0] == get_t5_embeddings([&quot;Hello&quot;]*i)[0]).sum())
</code></pre>
<p>This will return the number of values in the embeddings that match each other.
This was the result:</p>
<pre><code>1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27
</code></pre>
<p>Every time I run it, I get mismatches if the batch size is more than 768.</p>
<p>Why am I getting different embeddings and how do I fix this?</p>
","1","Question"
"78140043","","<p>How do i change the input shape of the pytorch resnet50 model before training on my dataset</p>
<p>I faced an error when i converted the trained model to .tflite format to use in a flutter app, which basically wanted me to change the input tensor of my model to 1, 224, 224, 3 from 1, 3, 224, 224.</p>
","0","Question"
"78143186","","<p>I want to run fine-tuned stable diffusion models in my local pc using python. For example juggernaut: <a href=""https://huggingface.co/RunDiffusion/Juggernaut-XL-v9"" rel=""nofollow noreferrer"">https://huggingface.co/RunDiffusion/Juggernaut-XL-v9</a></p>
<p>This is my code (it works with stable-diffusion-xl-base-1.0):</p>
<pre><code>import random
from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline
import torch
import gc
import time

# for cleaning memory
gc.collect()
torch.cuda.empty_cache()

start_time = time.time()

model = &quot;RunDiffusion/Juggernaut-XL-v9&quot;
pipe = DiffusionPipeline.from_pretrained(
    model,
    torch_dtype=torch.float16,
)

pipe.to(&quot;cuda&quot;)

prompt = (&quot;a portrait of male as a knight in middle ages, masculine looking, battle in the background, sharp focus, highly detailed, movie-style lighting, shadows&quot;)
seed = random.randint(0, 2**32 - 1)

generator = torch.Generator(&quot;cuda&quot;).manual_seed(seed)
image = pipe(prompt=prompt, generator=generator, num_inference_steps=1)
image = image.images[0]
image.save(f&quot;output_images/{seed}.png&quot;)

end_time = time.time()

total_time = end_time - start_time
minutes = int(total_time // 60) 
seconds = int(total_time % 60) 

print(f&quot;Took: {minutes} min {seconds} sec&quot;)
print(f&quot;Saved to output_images/{seed}.png&quot;)

</code></pre>
<p>But I am getting:</p>
<blockquote>
<p>OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory</p>
</blockquote>
<p>Maybe because of python, cuda versions. I'm dropping down my libraries versions:</p>
<p>Python 3.9.0</p>
<p>PyTorch: 2.2.0+cu118</p>
<p>CUDA : 11.8</p>
<p>Diffusers: 0.26.3</p>
<p>Transformers: 4.38.1</p>
","1","Question"
"78144820","","<p>I'm currently training a RNNModel using python darts. For comparing different trained models I want to extract the train_loss and val_loss from the fit method. How would I do this? I've read something about a metric collection but can't figure out how to use it.</p>
<p>This is my current code</p>
<pre><code>from darts.models import RNNModel
from darts import TimeSeries

train = # training data as TimeSeries
model = RNNModel(model=&quot;LSTM&quot;, input_chunk_length=self.past_samples)
model.fit(train)
</code></pre>
<p>The loss is shown in the console during training but I don't know how to access it.</p>
<p>So far I've tried to find any docs online and asked bing chat and ChatGPT for help. However they told me to use <code>model.history.history[&quot;loss&quot;]</code> which doesn't exist</p>
","0","Question"
"78145229","","<p>Each point has an x, y, and size.</p>
<p>For example these should result in similar:</p>
<p>Plot 1-A:</p>
<p><img src=""https://i.sstatic.net/WIc1d.png"" alt=""Plot 1-A"" /></p>
<p>Plot 1-B:</p>
<p><img src=""https://i.sstatic.net/MOy1W.png"" alt=""Plot 1-B"" /></p>
<p>And these should not result in similar:</p>
<p>Plot 2-A:</p>
<p><img src=""https://i.sstatic.net/CmqMq.png"" alt=""Plot 2-A"" /></p>
<p>Plot 3-A:</p>
<p><img src=""https://i.sstatic.net/XVyNw.png"" alt=""Plot 3-A"" /></p>
<p>Are there any algorithms or ways to determine similarity of the plots.</p>
<p>I tried creating a feature vector for each graph like number of points in each quadrant, the largest point size, distance between the two largest points and doing cosine similarity. But I keep getting high similarities for non matching graphs. I was looking into creating a ML model for this and was looking into siamese model, but cannot get it to train correctly.</p>
","-2","Question"
"78145824","","<p>I am working on a fraud detection project using GNN. My graph has banking codes (SWIFT BIC codes) as nodes and the edges represent transactions.
Below are the shapes of my tensors:</p>
<ul>
<li>Node Features Tensor Shape: <code>torch.Size([210, 6])</code></li>
<li>Edge Features Tensor Shape: <code>torch.Size([200, 4])</code></li>
<li>Adjacency Matrix Tensor Shape: <code>torch.Size([210, 210])</code></li>
<li>Labels Tensor Shape: <code>torch.Size([200, 1])</code></li>
</ul>
<p>I tried numerous attempts but currently am following this tutorial: <a href=""https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html"" rel=""nofollow noreferrer"">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html</a></p>
<p>Below is my GNN code:</p>
<pre class=""lang-py prettyprint-override""><code>class GCNLayer(nn.Module):

    def __init__(self, c_in, c_out):
        super().__init__()
        self.projection = nn.Linear(c_in, c_out)

    def forward(self, node_feats, adj_matrix):
        # Num neighbours = number of incoming edges
        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
        node_feats = self.projection(node_feats)
        print(&quot;node_feats &quot;,node_feats)
        node_feats = torch.bmm(adj_matrix, node_feats)
        node_feats = node_feats / num_neighbours
        return node_feats

layer = GCNLayer(c_in=6, c_out=210)
layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])
layer.projection.bias.data = torch.Tensor([0., 0.])

with torch.no_grad():
    out_feats = layer(node_features_tensor, adjacency_matrix_tensor)

print(&quot;Adjacency matrix&quot;, adjacency_matrix_tensor)
print(&quot;Input features&quot;, node_features_tensor)
print(&quot;Output features&quot;, out_feats)
</code></pre>
<p>But whatever I try I keep on getting dimension errors during multiplication: &quot;</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (210x6 and 2x2).</p>
</blockquote>
<p>I know that we are trying to multiply <code>node_Features_tensor (210,6)</code> by <code>adjacency_matrix_tensor (210,210)</code> but I have been stuck on this for days!</p>
<p>I tried multiple implementations of GNN/GCN. I am expecting to be able to train my model.</p>
","0","Question"
"78146852","","<p>I am making a simple program that given a prompt, will predict the emotions associated. I have a dataset in the form of a CSV file with 416809 items containing examples like so:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>text</th>
<th>label</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>i just feel really helpless and heavy hearted</td>
<td>4</td>
</tr>
<tr>
<td>1</td>
<td>ive enjoyed being able to slouch about relax and unwind and frankly needed it after those last few weeks around the end of uni and the expo i have lately started to find myself feeling a bit listless which is never really a good thing</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>(Label is a number from 0-5 which corresponds with one of six emotions)</p>
<p>My code is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;Loading modules...&quot;)
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder

print(&quot;Loading data...&quot;)
data = pd.read_csv('data/emotions.csv') #load dataset to data variable
X = data[&quot;text&quot;] #set X value to a table of all specified columns
y = data[&quot;label&quot;] #set y value to the column of answers

le = LabelEncoder() #encode labels
le.fit(y)
encoded_y = le.transform(y)
le.fit(X)
encoded_X = le.transform(X)

print(&quot;Training model...&quot;)
model = DecisionTreeClassifier()
model.fit([encoded_X],[encoded_y]) #train the model
prompt = input(&quot;Enter a prompt: &quot;)

encoded_prompt = le.transform([prompt])

predictions = model.predict([encoded_prompt]) #predict
</code></pre>
<p>I get an error in the last line:</p>
<pre><code>ValueError: X has 1 features, but DecisionTreeClassifier is expecting 416809 features as input. 
</code></pre>
<p>What I'm expecting is  for the <code>predictions</code> variable to be a number or a list containing a single number from 0-5 and for it to print the last line in the code.</p>
<p>How to resolve this?</p>
","0","Question"
"78149372","","<p>I am running code on an Amazon Sagemkaer notebook instance (in a plain jupyter notebook, nnot jupyterLab).</p>
<ol>
<li>How can I run the code in the background and close the browser tab? When I close the jupyter notebook tab the program stops, I want to avoid this. I read that I should not be doing the processing in the notebook itslef, rather I should use a Sagemaker processing job. How can I run a simple cell of code like the one below on a higher i</li>
</ol>
<pre><code>df_new['predicted_values'] = df_original.progress_apply(lambda x: LLM_pretrained_model.predict( x['comment_body'] )
</code></pre>
<ol start=""2"">
<li><p>After 12 hours the kernal crashes saying that I need to login again. How can I avoid this? due to the size of my data the program will take at least 28 hours to run</p>
</li>
<li><p>Is it possible to push code to GitHub from a sagemaker jupyter notebook (not a jupyterLab notebook)?</p>
</li>
</ol>
","0","Question"
"78150382","","<p>I'm performing data analysis on a dataset with categorical labels are interrelated.
My labels track experimental conditions.
In my case, labels track concentrations of combinations of two chemicals that produce an output measured by n features.</p>
<p>Is it best practice to use the categorical labels in place of the concentrations of the combinations of chemicals, or is there a better method?</p>
<p>Here's a sample of the translation between categorical label and real life condition it represents.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Condition</th>
<th>Chemical1</th>
<th>Chemical2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table></div>
","1","Question"
"78151170","","<p><a href=""https://i.sstatic.net/vpglG.png"" rel=""nofollow noreferrer"">DataFrame structure</a></p>
<p>The above image shows the structure of my data.</p>
<pre><code>from sklearn.model_selection import train_test_split
from datasets import Features, ClassLabel, Value, Dataset, DatasetDict

df_train, df_tmp = train_test_split(
        movie_df,stratify=movie_df[&quot;label&quot;], test_size=0.2)

df_val, df_test = train_test_split(
        df_tmp,stratify=df_tmp[&quot;label&quot;], test_size=0.5)

ds_features = Features({&quot;text&quot;: Value(&quot;string&quot;), &quot;label&quot;: ClassLabel(names=labels)})

dataset = DatasetDict({
    &quot;train&quot;: Dataset.from_pandas(df_train.reset_index(drop=True),features=ds_features),
    &quot;valid&quot;: Dataset.from_pandas(df_val.reset_index(drop=True),features=ds_features),
    &quot;test&quot;: Dataset.from_pandas(df_test.reset_index(drop=True),features=ds_features)})

dataset
</code></pre>
<p>this code gave me a value error as shown:</p>
<p><a href=""https://i.sstatic.net/CEXcB.png"" rel=""nofollow noreferrer"">error</a></p>
<p><a href=""https://i.sstatic.net/kb5it.png"" rel=""nofollow noreferrer"">error</a></p>
<p>I was expecting something similar to this but not with the same values:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 13267
    })
    valid: Dataset({
        features: ['text', 'label'],
        num_rows: 1658
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 1659
    })
})
</code></pre>
<p>Can anyone tell me what I am doing wrong?</p>
","0","Question"
"78152237","","<p>I dont understand why we use the 'e' so often in NN may it be sigmoid function or softmax function.</p>
<p>In sigmoid function we are essentially compressing the values y=mx+b to be in the range 0-1 so why is it that we specifically use 'e'. If we go by intuition it makes sense to use '2' instead of 'e' i mean we are going for binary classification so that makes sense right ?</p>
<p>Also in softmax function we take the e^x / sum(e^x) why do we need to do that, i mean we are trying to get the probability of which class the x belongs to right  so why can't we just you know do it like this x/sum(abs(x)) ?</p>
","3","Question"
"78152636","","<p>I am encountering an issue while trying to create an instance of GPT4AllEmbeddings. However i keep receiving the following error</p>
<pre><code>Cell In[15], line 1
----&gt; 1 vectorstore = Chroma.from_documents(documents = splits, embeddings = GPT4AllEmbeddings())
      2 retriever = vectorstore.as_retriever(search_type = 'similarity', search_kwargs = {'k':6})
      3 retrieved_docs = retriever.get_relevant_documents(&quot;What are you ?&quot;)

File ~\anaconda3\Lib\site-packages\pydantic\main.py:341, in pydantic.main.BaseModel.__init__()

ValidationError: 1 validation error for GPT4AllEmbeddings
__root__
  Unable to instantiate model (type=value_error)
</code></pre>
<p>here is the relevant code snippet</p>
<pre><code>vectorstore = Chroma.from_documents(documents = splits, embeddings = GPT4AllEmbeddings())
retriever = vectorstore.as_retriever(search_type = 'similarity', search_kwargs = {'k':6})
retrieved_docs = retriever.get_relevant_documents(&quot;What is Young Decade ?&quot;)
print(len(retrieved_docs))
print(retrieved_docs[0].page_content)
</code></pre>
<p>how to solve this error?</p>
","1","Question"
"78155034","","<p>I am running a regression task on a dataset which is composed of both authentic and augmented samples. The augmented samples are generated by jittering the authentic ones. I would like to select the best performing model with cross-validation with <code>sklearn</code>.</p>
<p>For this I would like to:</p>
<ul>
<li>Train the model on a set that is comprised of both the authentic and augmented samples. I do not want the fitting procedure to take the origin of the sample into account (i.e. it should be equivalent to run <code>estimator.fit(..., sample_weights = [1,1,..., 1]</code>).</li>
<li>Score the models based on their performance <strong>on the authentic samples only</strong>. For this, I thought of setting the weight of augmented (resp. authentic) samples to 0 (resp. 1).</li>
</ul>
<p>How to achieve this with sklearn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"" rel=""nofollow noreferrer""><code>cross_validate</code></a>?</p>
<p>I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn import model_selection
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, make_scorer
import numpy as np

n_smpl, n_feats = 100, 5
arr_source = np.random.random((n_smpl, n_feats))
arr_target = np.random.random((n_smpl, n_feats))
arr_weight = np.random.randint(0, 2, n_smpl)  # 0 for augmented, 1 for authentic

model = RandomForestRegressor()
kfold_splitter = model_selection.KFold(n_splits=5, random_state=7, shuffle=True)
my_scorers = {
    &quot;r2_weighted&quot;: make_scorer(r2_score, sample_weight=arr_weight),
    &quot;mse_weighted&quot;: make_scorer(mean_squared_error, greater_is_better=False, sample_weight=arr_weight)
}

cv_results = model_selection.cross_validate(model, arr_source, arr_target, scoring = my_scorers, cv=kfold_splitter)
</code></pre>
<p>But this returns <code>ValueError: Found input variables with inconsistent numbers of samples: [20, 20, 100]</code>. I understand that this happens because cross_validate is not able to split the sample weights according to the folds.</p>
<p>Is there any way to get this to run through cross-validate? Or any other method?</p>
","1","Question"
"78155402","","<p>I work with data and I have decent skills in python, I know how to work with different models but never before I tried to use Neural Networks.<br />
So I am new to pytorch and I decided to train using online tutorials and videos.</p>
<p>Unfortunately I discovered I really can't make these models work and I get extremely wrong results. This happens regardless of the guide I follow so it's definitely something I am doing wrong.</p>
<p>For example I followed <a href=""https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md"" rel=""nofollow noreferrer"">this step-by-step guide</a> on how to make a NN for regression using the Boston Housing Dataset.</p>
<p>Here is my code that I basically copied from the guide, so there <em>should</em> not be any difference.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd

### importing the dataset
boston = pd.read_csv('./housing.csv', header=None, sep='\s+')
boston.columns = [
    'CRIM',
    'ZN',
    'INDUS',
    'CHAS',
    'NOX',
    'RM',
    'AGE',
    'DIS',
    'RAD',
    'TAX',
    'PTRATIO',
    'B',
    'LSTAT',
    'MEDV'
]
xcol = boston.drop(columns=['MEDV']).columns
ycol = ['MEDV']

X = boston[xcol].values
y = boston[ycol].values

### Creating the Torch Dataset
class TorchDataset(torch.utils.data.Dataset):
    def __init__(self, X, y, scale_data=True):
        if not torch.is_tensor(X) and not torch.is_tensor(y):
            if scale_data:
                X = StandardScaler().fit_transform(X)
            self.X = torch.from_numpy(X)
            self.y = torch.from_numpy(y)

    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, i):
        return self.X[i], self.y[i]

### building the MLP
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(13, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
            )
        
    def forward(self, x):
        return self.layers(x)

torch.manual_seed(42)

dataset = TorchDataset(X, y)
trainloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)

mlp = MLP()

loss_function = nn.L1Loss()
optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)

### training loop
loss_vec = []

for epoch in range(1000):
    epoch_loss = 0

    for i, data in enumerate(trainloader, 0):
        inputs, targets = data
        inputs, targets = inputs.float(), targets.float()
        targets = targets.reshape((targets.shape[0], 1))

        ## Zero the gradient
        optimizer.zero_grad()
        
        ## Forward Pass
        outputs = mlp(inputs)

        ## compute loss
        loss = loss_function(outputs, targets)

        ## backward pass
        loss.backward()
        
        ## Optimization
        optimizer.step()
        
        ## Statisitcs
        epoch_loss += loss.item()
    
    loss_vec.append(epoch_loss) 

## Visualizing the Loss curve
import plotly.express as px
px.scatter(loss_vec)

## Checking the R2 score between observed and predicted values    
from sklearn.metrics import r2_score

y_pred = mlp(torch.tensor(X, dtype=torch.float)).detach().numpy()


r2_score(y.flatten(), y_pred.flatten())   ##always a big negative number
</code></pre>
<p>Here is the loss plot<br />
<a href=""https://i.sstatic.net/0yKfj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0yKfj.png"" alt=""enter image description here"" /></a></p>
<p>But the weirdest part is the predicted values</p>
<pre><code>pd.DataFrame({
    'Obs':y.flatten(),
    'Pred':y_pred.flatten()
})
</code></pre>
<p><a href=""https://i.sstatic.net/yb62U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yb62U.png"" alt=""predicted values"" /></a></p>
<p>As you can see my NN is predicting values completely out of the range.</p>
<p>Can you tell me what I am doing wrong here?</p>
","0","Question"
"78158096","","<p>I am trying to understand whether it makes sense to apply Low-rank approximations to learnable parameters in a class. The goal is to reduce parameter counts.</p>
<p>I have the following custom module :</p>
<pre class=""lang-py prettyprint-override""><code>class CustomPara(nn.Module):
    
    def __init__(self, num_blocks, in_planes, out_planes, kernel_size):
        super(CustomPara, self).__init__()
        self.coefficient_shape = (num_blocks,1,1,1,1)
        blocks = [torch.Tensor(out_planes, in_planes, kernel_size, kernel_size) for _ in range(num_blocks)]
        for i in range(num_blocks): init.kaiming_normal_(blocks[i])
        self.blocks = nn.Parameter(torch.stack(blocks)) # this is what we will freeze later

    def forward(self, coefficients):
        final_blocks =  (self.blocks*coefficients).sum(0)
        return final_blocks
</code></pre>
<p>Is it possible to reduce the number of learnable parameters here using Low-rank adaptation on the <code>blocks</code> parameter?</p>
","-1","Question"
"78161067","","<p>I'm encountering an issue with accessing .bin files in my React Native app. I followed the process of saving my TensorFlow model in .h5 format, converting it to JSON and binary files, and then moving these files to the application directory. However, while I can access the .json file, I'm unable to access the .bin file.</p>
<pre><code>const { getDefaultConfig } = require(&quot;metro-config&quot;);

module.exports = async () =&gt; {
  const defaultConfig = await getDefaultConfig();
  const { assetExts } = defaultConfig.resolver;

  return {
    resolver: {
      assetExts: [
        ...assetExts,
        &quot;json&quot;, // JSON files
        &quot;bin&quot;, // Binary files
      ],
    },
  };
};

</code></pre>
<p>Additional Notes:</p>
<p>I've ensured that the .bin files are correctly placed in the assets directory within my project.
I've also verified that there are no syntax errors or typos in the metro.config.js file.
Despite these efforts, I'm still unable to access the .bin files in my app.</p>
<p>How to resolve this issue?</p>
","2","Question"
"78163348","","<p>I don't understand what my problem is. It should work, if only because its the standard autoenoder from the tensorflow documentation.
this is the error</p>
<pre class=""lang-none prettyprint-override""><code>line 64, in call
    decoded = self.decoder(encoded)
ValueError: Exception encountered when calling Autoencoder.call().

Invalid dtype: &lt;property object at 0x7fb471cc1c60&gt;

Arguments received by Autoencoder.call():
  • x=tf.Tensor(shape=(32, 28, 28), dtype=float32)
</code></pre>
<p>and this is my code</p>
<pre><code>(x_train, _), (x_test, _) = fashion_mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

print (x_train.shape)
print (x_test.shape)

class Autoencoder(Model):
  def __init__(self, latent_dim, shape):
    super(Autoencoder, self).__init__()
    self.latent_dim = latent_dim
    self.shape = shape
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(latent_dim, activation='relu'),
    ])
    self.decoder = tf.keras.Sequential([
      layers.Dense(tf.math.reduce_prod(shape), activation='sigmoid'),
      layers.Reshape(shape)
    ])

  def call(self, x):
    encoded = self.encoder(x)
    print(encoded)
    decoded = self.decoder(encoded)
    print(decoded)
    return decoded


shape = x_test.shape[1:]
latent_dim = 64
autoencoder = Autoencoder(latent_dim, shape)

autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())

autoencoder.fit(x_train, x_train,
                epochs=10,
                shuffle=True,
                validation_data=(x_test, x_test))
</code></pre>
<p>I tried to change the database and also tried different shapes</p>
","1","Question"
"78165544","","<p>I'm trying to understand the internal working of the Linear-regression model in Scikit-learn.</p>
<p>This is my dataset</p>
<p><a href=""https://i.sstatic.net/ngx1v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ngx1v.png"" alt=""before 1 hot encoding"" /></a></p>
<p>And this is my dataset after performing one-hot-encoding.</p>
<p><a href=""https://i.sstatic.net/JKnEw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JKnEw.png"" alt=""after 1 hot encoding"" /></a></p>
<p>And this are values of the coefficients and intercept after performing linear-regression.
<a href=""https://i.sstatic.net/ELLBU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ELLBU.png"" alt=""coefficients"" /></a></p>
<p><strong>Sell Price</strong> is the dependent column and rest of the columns are features.<br/>
And these are the predicted values which works fine in this case.<br/>
<a href=""https://i.sstatic.net/cMKjL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cMKjL.png"" alt=""predicted values"" /></a></p>
<p>I noticed that the number of coefficients is 1 greater than the number of features. So this is how I generated the feature matrix:</p>
<pre><code>feature_matrix = dataFrame.drop(['Sell Price($)'], axis = 'columns').to_numpy()

# Array to be added as column
bias_column = np.array([[1] for i in range(len(feature_matrix))])

# Adding column to array using append() method
feature_matrix = np.concatenate([bias_column, feature_matrix], axis = 1)  # axis = 1 means column, 0 means row
</code></pre>
<p><strong>Result</strong><br/>
<a href=""https://i.sstatic.net/86n9N.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/86n9N.png"" alt=""enter image description here"" /></a></p>
<p>What I want to know is how does Scikit-learn use these coefficients and intercept to predict the values.<br/>
This is what I tried.<br/>
<a href=""https://i.sstatic.net/OhU4x.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OhU4x.png"" alt=""enter image description here"" /></a><br/>I also noticed that the value I get by doing this calculation is actually equal to the mileage in every case. But that's not the dependent feature here. So what's going on?</p>
","0","Question"
"78165698","","<p>I am having an issue with my autoencoder as I am shaping the ouput incorrectly. Currently the autoencoder is coded lke this.</p>
<p>I Got This Error :</p>
<blockquote>
<p>ValueError: Dimensions must be equal, but are 2000 and 3750 for
'{{node mean_absolute_error/sub}} =
Sub[T=DT_FLOAT](sequential_8/sequential_7/conv1d_transpose_14/BiasAdd,
IteratorGetNext:1)' with input shapes: [?,2000,3], [?,3750,3].</p>
</blockquote>
<p>Can someone help with adjusting the architecture if this is possible. I seem to be forgetting the original modifications I made to adjust for this initially.</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, concatenate
from tensorflow.keras.callbacks import EarlyStopping

# Provided encoder
encoder = tf.keras.models.Sequential([
    tf.keras.layers.Reshape([3750, 3], input_shape=[3750, 3]),
    tf.keras.layers.Conv1D(32, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(64, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(128, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(256, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(512, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512)
])

#latent space

decoder = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512 * 125, input_shape=[512]),
    tf.keras.layers.Reshape([125, 512]),
    tf.keras.layers.Conv1DTranspose(512, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.Conv1DTranspose(256, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.Conv1DTranspose(128, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.Conv1DTranspose(64, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    # Adjust the kernel size and padding to match the input shape
    tf.keras.layers.Conv1DTranspose(3, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;linear&quot;)
])

# Add more layers with larger kernel sizes to both encoder and decoder.
ae = tf.keras.models.Sequential([encoder, decoder])

ae.compile(
    loss=&quot;mean_squared_error&quot;, 
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001)
)
# Define the early stopping criteria
early_stopping = EarlyStopping(monitor='val_loss', patience=30, mode='min')

history = ae.fit(X_train, X_train, batch_size=8, epochs=150, validation_data=(X_val, X_val), callbacks=[early_stopping])    ```
</code></pre>
","0","Question"
"78167178","","<p>I have a custom function that splits my data into training and testing sets based on various criteria and rules. I'd like to use this function in a tidymodels workflow together with <code>fit_resamples</code>. However, when I can make my list look like a list made with <code>vfold_cv</code>, it does not seem to work. The example code I am using:</p>
<pre><code>data(ames, package = &quot;modeldata&quot;)

split_data &lt;- function(df, n) {
  set.seed(123) # for reproducibility
  df$id &lt;- seq.int(nrow(df))
  list_of_splits &lt;- list()
  
  for(i in 1:n) {
    train_index &lt;- sample(df$id, size=ceiling(nrow(df)*.8))
    train_set &lt;- df[train_index,]
    test_set &lt;- df[-train_index,]
    list_of_splits[[i]] &lt;- list(train_set = train_set, test_set = test_set)
  }
  
  return(list_of_splits)
}

splits &lt;- split_data(ames, 5)

resamples &lt;- map(splits, ~rsample::make_splits(
  x = .$train_set |&gt; select(colnames(.$test_set)),
  assessment = .$test_set
))

names(resamples) &lt;- paste0(&quot;Fold&quot;, seq_along(resamples))

resamples &lt;- tibble::tibble(splits = resamples,
                            id = names(resamples))

lm_model &lt;- 
  linear_reg() %&gt;% 
  set_engine(&quot;lm&quot;)

lm_wflow &lt;- 
  workflow() %&gt;% 
  add_model(lm_model) %&gt;%
  add_formula(Sale_Price ~ Longitude + Latitude)

res &lt;- lm_wflow %&gt;%
  fit_resamples(resamples = resamples)
</code></pre>
<p>The error returned after running that last line is:</p>
<pre><code>Error in `check_rset()`:
! The `resamples` argument should be an 'rset' object, such as the type produced by `vfold_cv()` or other 'rsample' functions.
</code></pre>
<p>If I try to force the class to be &quot;rset&quot; <code>class(resamples) &lt;- &quot;rset&quot;</code>, the list no longer looks correct and I get the same error.</p>
<p>What is the correct method of using a custom crossfold data set?</p>
<p>Note - additional question: In the example code above, the test and training set size is consistent across folds. In my actual data, this will vary slightly - does this matter at all?</p>
","0","Question"
"78167199","","<p>I have made a Random Forest Classifier model in Python, and now want to make partial dependence plot (PDP). I used scaled data for training and testing the model, and make the PDP like this:
<code>PartialDependenceDisplay.from_estimator(best_clf, X_test_final, best_features)</code>. However, the x-axis values are scaled which limits interpretability.</p>
<p>Unscaling the data <code>X_test_final</code> before calling the <code>PartialDependenceDisplay</code> does not work, any suggestions on how I can change the x-axis values from scaled to unscaled? I have scaled my data using <code>StandardScaler()</code>.</p>
","0","Question"
"78169479","","<p>I'm conducting an experiment using 'dvc repro -f', where multiple stages are executed according to the dvc.yaml configuration. For instance:</p>
<pre><code>Stages:
 Training:
   foreach:
    -cycle: 0
    -cycle: 1
    -cycle: 2
   do:
    cmd:
     python train.py
 Selection:
   foreach:
    -cycle: 0
    -cycle: 1
    -cycle: 2
   do:
    cmd:
     python train.py
</code></pre>
<p>During the execution of each stage, such as the training stage at cycle 0, I aim to extract its stage name, like 'Training_0', within a Python program and during selection stage it should be Selection_0. I'm seeking a method to extract this information either while the stage is being executed or just before its execution begins. i tried using the dvc.api but the api does not return the current stage that is running. How can I achieve this?</p>
","1","Question"
"78169647","","<p>During evaluation of the model using compare_model(). All AUCs are zero.</p>
<p>This output of Pycaret 3.3.0 is weird. what's the reason for that?
[1]: <a href=""https://i.sstatic.net/qm2ZT.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/qm2ZT.png</a></p>
","0","Question"
"78169666","","<p>It was my understanding that the final prediction of an XGBoost model (in this particular case an XGBRegressor) was obtained by summing the values of the predicted leaves <a href=""https://discuss.xgboost.ai/t/xgboost-trees-understanding/1822"" rel=""nofollow noreferrer"">[1]</a> [<a href=""https://stats.stackexchange.com/questions/502000/how-prediction-of-xgboost-correspond-to-leaves-values"">2</a>]. Yet I'm failing to match the prediction summing the values. Here is a MRE:</p>
<pre><code>import json
from collections import deque

import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
import xgboost as xgb


def leafs_vector(tree):
    &quot;&quot;&quot;Returns a vector of nodes for each tree, only leafs are different of 0&quot;&quot;&quot;

    stack = deque([tree])

    while stack:
        node = stack.popleft()
        if &quot;leaf&quot; in node:
            yield node[&quot;leaf&quot;]
        else:
            yield 0
            for child in node[&quot;children&quot;]:
                stack.append(child)


# Load the diabetes dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the XGBoost regressor model
xg_reg = xgb.XGBRegressor(objective='reg:squarederror',
                          max_depth=5,
                          n_estimators=10)

# Train the model
xg_reg.fit(X_train, y_train)

# Compute the original predictions
y_pred = xg_reg.predict(X_test)

# get the index of each predicted leaf
predicted_leafs_indices = xg_reg.get_booster().predict(xgb.DMatrix(X_test), pred_leaf=True).astype(np.int32)

# get the trees
trees = xg_reg.get_booster().get_dump(dump_format=&quot;json&quot;)
trees = [json.loads(tree) for tree in trees]

# get a vector of nodes (ordered by node id)
leafs = [list(leafs_vector(tree)) for tree in trees]

l_pred = []
for pli in predicted_leafs_indices:
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)))

assert np.allclose(np.array(l_pred), y_pred, atol=0.5) # fails
</code></pre>
<p>I also tried adding the default value (<code>0.5</code>) of the <code>base_score</code> (as written <a href=""https://stackoverflow.com/questions/68000028/how-are-the-leaf-values-of-xgboost-regression-trees-relate-to-the-prediction"">here</a>) to the total sum but it also didn't work.</p>
<pre><code>l_pred = []
for pli in predicted_leafs_indices:
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)) + 0.5) 
</code></pre>
","0","Question"
"78170066","","<h2>Problem</h2>
<p>I am calling the <code>fit_transform()</code> and <code>transform()</code> methods on a <code>Pipeline</code> object, but Python is raising an AttributeError whenever I try to do so. Here is what I'm trying to run, with imports. (Note: train/test splitting has been done already)</p>
<pre><code>from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

pipe = Pipeline([('mean_impute', SimpleImputer()), 
                 ('norm',        StandardScaler()), 
                 ('sklearn_lm',  LinearRegression())])

pipe.fit_transform(x_train, y_train)  #&lt;-- error here

x_transform = pipe.transform(x_test)  #&lt;-- and here if previous line is absent
</code></pre>
<p>The text of the error is as follows:</p>
<p><code>AttributeError: This 'Pipeline' has no attribute 'fit_transform'</code></p>
<p>What went wrong? I'm sure it's something simple.</p>
<h2>Things I have tried:</h2>
<ul>
<li>Looked over the documentation for sci-kit learn to confirm that <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow noreferrer"">these methods exist for the Pipeline object in sklearn</a></li>
<li>Checked the sizes of <code>x_train</code> and <code>y_train</code> to make sure they were the same, and that they both had headers</li>
<li>Reinstalled <code>sci-kit learn</code></li>
</ul>
","3","Question"
"78170750","","<p>I've built and trained my model weeks ago and saved it on model.json dan model.h5</p>
<p>Today, when I tried to load it using model_from_json, it gives me an error</p>
<pre class=""lang-py prettyprint-override""><code>TypeError: Could not locate class 'Sequential'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'class_name': 'Sequential', 'config': {'name': 'sequential_7', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 244, 244, 3], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_15_input'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_15', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': [None, 244, 244, 3], 'filters': 32, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 244, 244, 3]}}, {'module': 'keras.layers', 'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_14', 'trainable': True, 'dtype': 'float32', 'pool_size': [2, 2], 'padding': 'valid', 'strides': [2, 2], 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 242, 242, 32]}}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_16', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 121, 121, 32]}}, {'module': 'keras.layers', 'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_15', 'trainable': True, 'dtype': 'float32', 'pool_size': [2, 2], 'padding': 'valid', 'strides': [2, 2], 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 119, 119, 64]}}, {'module': 'keras.layers', 'class_name': 'Flatten', 'config': {'name': 'flatten_7', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 59, 59, 64]}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_14', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 222784]}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}}]}, 'keras_version': '2.13.1', 'backend': 'tensorflow'}
</code></pre>
<p>I've imports all the requirements:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from keras.preprocessing import image
from keras.models import model_from_json

from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, GlobalAveragePooling2D, Dropout, Flatten
from tensorflow.keras.applications import VGG16
</code></pre>
<p>And this is my code used to load the saves json model:</p>
<pre class=""lang-py prettyprint-override""><code>json_file = open('./model/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights(&quot;model.h5&quot;)
</code></pre>
<p>Am I missing something?</p>
","3","Question"
"78170802","","<p>I am working in YOLOv8 project to detect drowsiness and play a alarm audio file if drowsiness is detected. The problem i am facing is i can't able to play the audio in real-time because my detections are stored in results at first. Once I close the detection window then it access the data stored in the results and continuously playing the audio. How can i solve this??</p>
<pre><code>import os
from ultralytics import YOLO

import torch
import matplotlib
import numpy as np
import cv2
import pygame  

pygame.init()
sound_to_play = pygame.mixer.Sound(r'D:\ML\Syncronised vigilance for driver\alarm.wav')  
sound_to_play.play()

model = YOLO(r'C:\Users\HP\Downloads\last.pt')

cap = cv2.VideoCapture(0) 
while True:
    ret, frame = cap.read()

    results = model.predict(source=&quot;0&quot;,show=True)  
    for r in results:
        if len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            print(dclass)
            if dclass==2.0:
              sound_to_play.play()
    if cv2.waitKey(1) == ord('q'):
        break

pygame.quit()
cap.release()
cv2.destroyAllWindows()
</code></pre>
<p>Problem is my code first do detection and store it in result then go into the for loop. Expected output is it has detect as well as check the class value at the same time.</p>
","0","Question"
"78171227","","<p>I feel a bit confused about how to use together hyperparameter tuning and model evaluation correctly.</p>
<p>Should hyperparameter tuning be done on the whole dataset or only on the training set? What is the correct sequence of actions?</p>
<p>Could you please review my code and advise me the best practice considering the issue?</p>
<p>Here I am using hyperparameter tuning on the whole dataset first and then evaluate the model performance only on the train set. Is it correct? Doesn't it lead to data leakage?</p>
<p>Hyperparameter Tuning</p>
<pre class=""lang-py prettyprint-override""><code>numeric_features = X.select_dtypes(include=['int', 'float']).columns
categorical_features = X.select_dtypes(include=['object', 'category']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ]
)

en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1),
                     alphas = np.arange(0, 1.1, 0.1),
                     random_state=818,
                     n_jobs = -1)

model = make_pipeline(preprocessor, en_cv)
model.fit(X, y)

best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_
</code></pre>
<p>Model evaluation:</p>
<pre class=""lang-py prettyprint-override""><code>ElasticNet = make_pipeline(preprocessor, ElasticNet(alpha=best_alpha, l1_ratio=l1_ratio))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

ElasticNet.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(r2, mse)
</code></pre>
<p>This code, actually, took about 18 minutes to run on the dataset with about 80000 observations and about 150 columns. Is this considered adequate?</p>
","-1","Question"
"78171263","","<p><a href=""https://i.sstatic.net/iTBz6.png"" rel=""nofollow noreferrer"">part of Gradient Descent algorithm</a></p>
<pre><code>this.updateWeights = function() {
 
  let wx;
  let w_deriv = 0;
  let b_deriv = 0;

  for (let i = 0; i &lt; this.points; i++) {
    wx = this.yArr[i] - (this.weight * this.xArr[i] + this.bias);
    w_deriv += -2 * wx * this.xArr[i];
    b_deriv += -2 * wx;
  }
  
  this.weight -= (w_deriv / this.points) * this.learnc;
  this.bias -= (b_deriv / this.points) * this.learnc;
}
            
</code></pre>
<p>explain this part please!!</p>
<pre><code>-2 * wx * this.xArr[i]
</code></pre>
<p>this part is induced....?</p>
<p>how to induce by math formula....</p>
","0","Question"
"78175088","","<p>I am trying to implement a Logistic regression model, binary classifier.
I have to use stochastic gradient descent and a closed form of the gradient of binary cross entropy.
After trying to train the model on the data, it seems like the model is not working correctly:
The loss as the number of iterations is not decreasing and not converging, only for som eta=0.002 and immediately after we set a much larger eta.(we set eta&gt;&gt;0.002 then we reset eta=0.002 then it might converge).
After that, evaluating the model preformance giving very poor results, similar to a naive model that would predict all the test examples as the same:</p>
<pre><code>Confusion Matrix:
[[344. 240.]
 [294. 322.]]
True Negatives (TN): 322.0
False Positives (FP): 294.0
False Negatives (FN): 240.0
True Positives (TP): 344.0
Sensitivity (Se): 0.589041095890411
Specificity (Sp): 0.5227272727272727
Positive Predictive Value (PPV): 0.5391849529780565
Negative Predictive Value (NPV): 0.5729537366548043
Accuracy (Acc): 0.555
F1 Score: 0.563011456628478
Area Under the ROC Curve (AUC): 0.555.
</code></pre>
<p>What is wrong with this implementation?</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt


def sigmoid(z):
    sig = 1 / (1+np.exp(-z)) 
    return sig

class ManualLogisticRegression:
    def __init__(self, random_state=1):
        np.random.seed(random_state)
        self.w = np.random.randn(5)

    def fit(self, X, Y, eta=0.005, plot=False):

        if plot:
            loss_vec = np.zeros(len(X))
        for idx, (x, y) in enumerate(zip(X, Y)):

            z = np.dot(x, self.w)
            a = sigmoid(z)
            grad = np.dot(x.T, (a - y))
            self.w -= eta * grad
            if plot:
                loss_vec[idx] = self.log_loss(X, Y)
        if plot:
            plt.plot(loss_vec)
            plt.xlabel('# of iterations')
            plt.ylabel('Loss')

    def log_loss(self, x, y):

        z = np.dot(x, self.w)
        p = sigmoid(z)
        epsilon = 1e-5
        p = np.clip(p, epsilon, 1 - epsilon)
        log_loss = (-1 / len(x)) * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))
        
        return log_loss

    def predict_proba(self, x):
        &quot;&quot;&quot;
        This function computes the probability of every example in x to belong to the class &quot;1&quot; using the trained model.
        :param x: Feature matrix (could be also a single vector).
        :return: vector at the length of examples in x where every element is the probability to belong to class &quot;1&quot; per example.
        &quot;&quot;&quot;

        z = np.dot(x, self.w)
        y_pred_proba = sigmoid(z)

        return y_pred_proba

    def predict(self, x, thresh=0.5):
        &quot;&quot;&quot;
        This function labels every example according to the calculated probability with the use of a threshold.
        :param x: Feature matrix (could be also a single vector).
        :param thresh: decision threshold.
        :return: vector at the length of examples in x where every element is the estimated label (0 or 1) per example.
        &quot;&quot;&quot;

        z = np.dot(x, self.w)
        probabilities = sigmoid(z)
        y_pred = np.where(probabilities &gt;= thresh, 1, 0)

        return y_pred

    def score(self, x, y):
        &quot;&quot;&quot;
        This function computes the accuracy of the trained model's estimations.
        :param x: Feature matrix (could be also a single vector).
        :param y: Adequate true labels (either 1 or 0).
        :return: Estimator's accuracy.
        &quot;&quot;&quot;
        return np.sum(self.predict(x) == y)/len(y)

    def conf_matrix(self, x, y):
        &quot;&quot;&quot;
        This function computes the confusion matrix for the prediction of the trained model. First value of the matrix
        was given as a hint.
        :param x: Feature matrix (could be also a single vector).
        :param y: Adequate true labels (either 1 or 0).
        :return: Confusion matrix.
        &quot;&quot;&quot;
        conf_mat = np.zeros((2, 2))
        y_pred = self.predict(x)
        
        conf = (y_pred == y)
        conf_mat[0, 0] += np.sum(1 * (conf[y_pred == 0] == 1))

           #the code provided is checking if the prediction is matching a true positive case, so it is
           #calculating the number of TN,if y==0 we get True, if y==1 we get False.
        conf_mat[1, 0] += np.sum(1 * (conf[y_pred == 0] == 0)) # FN
        conf_mat[0, 1] += np.sum(1 * (conf[y_pred == 1] == 0)) # FP
        conf_mat[1, 1] += np.sum(1 * (conf[y_pred == 1] == 1)) # TP


        # --------------------------------------------------------------------------------------
        return conf_mat
</code></pre>
<p>and this is the notebook itself:</p>
<pre><code>%load_ext autoreload
%autoreload 2
from manual_log_reg import ManualLogisticRegression
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
</code></pre>
<pre><code>X = pd.read_csv('X_data.csv')
X.drop(columns=X.columns[0], axis=1, inplace=True)
X.head()
</code></pre>
<pre><code>y = pd.read_csv('y_data.csv')  # read and convert to numpy
y.drop(columns=y.columns[0], axis=1, inplace=True)
y.head()
</code></pre>
<pre><code>X = X.values  # convert to numpy
y = y.values.astype(int).flatten()  # convert to numpy integers and flatten
X = np.concatenate((np.ones((len(y), 1)), X), axis=1) # add bias term
</code></pre>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
</code></pre>
<pre><code>log_reg = ManualLogisticRegression()
</code></pre>
<pre><code>log_reg.fit(X_train, y_train, eta=0.003, plot=True)
</code></pre>
<pre><code>
sorted_weights = np.sort(np.abs(log_reg.w[:-1]))  # Exclude bias term and sort by absolute value
most_important_feature_index = np.argmax(np.abs(log_reg.w[:-1]))  # Find the index of the most important feature
most_important_feature_weight = log_reg.w[most_important_feature_index]  # Get the weight of the most important feature
print(f&quot;The most important feature is feature {most_important_feature_index + 1} with weight {most_important_feature_weight}.&quot;)
</code></pre>
<pre><code>conf_matrix = log_reg.conf_matrix(X_test, y_test)
print(&quot;Confusion Matrix:&quot;)
print(conf_matrix)

# Calculate additional performance metrics
TN = conf_matrix[1, 1]
FP = conf_matrix[1, 0]
FN = conf_matrix[0, 1]
TP = conf_matrix[0, 0]
Se = TP / (TP + FN)
Sp = TN / (TN + FP)
PPV = TP / (TP + FP)
NPV = TN / (TN + FN)
Acc = (TP + TN) / (TP + TN + FP + FN)
F1 = 2 * (PPV * Se) / (PPV + Se)

# Calculate AUC using the score method of ManualLogisticRegression
AUC = log_reg.score(X_test, y_test)

# Report the performance metrics
print(f&quot;True Negatives (TN): {TN}&quot;)
print(f&quot;False Positives (FP): {FP}&quot;)
print(f&quot;False Negatives (FN): {FN}&quot;)
print(f&quot;True Positives (TP): {TP}&quot;)
print(f&quot;Sensitivity (Se): {Se}&quot;)
print(f&quot;Specificity (Sp): {Sp}&quot;)
print(f&quot;Positive Predictive Value (PPV): {PPV}&quot;)
print(f&quot;Negative Predictive Value (NPV): {NPV}&quot;)

print(f&quot;Accuracy (Acc): {Acc}&quot;)
print(f&quot;F1 Score: {F1}&quot;)
print(f&quot;Area Under the ROC Curve (AUC): {AUC}&quot;)
conf_mat = log_reg.conf_matrix(X_test, y_test)
import seaborn as sns
import matplotlib.pyplot as plt
# Plot confusion matrix
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()
</code></pre>
","0","Question"
"78176160","","<p>I'm trying to construct a Bert Model using Tensorflow on Colab. This code was perfectly working weeks ago. Now if I try to instantiate the model I obtain the following error:</p>
<pre><code>Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFBertModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-14-b0e769ef7890&gt; in &lt;cell line: 7&gt;()
      5 SC_mask_layer = Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;attention_mask&quot;)
      6 SC_bert_model = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, attention_mask=SC_mask_layer)[1]  # Estrai il secondo output, che è il pooler_output
      8 
      9 # Aggiungi un layer di Dropout

36 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py in type_spec_from_value(value)
   1002         3, &quot;Failed to convert %r to tensor: %s&quot; % (type(value).__name__, e))
   1003 
-&gt; 1004   raise TypeError(f&quot;Could not build a TypeSpec for {value} of &quot;
   1005                   f&quot;unsupported type {type(value)}.&quot;)
   1006 

TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).

Could not build a TypeSpec for name: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert&quot;
op: &quot;Assert&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/All&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2&quot;
input: &quot;Placeholder&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/y&quot;
attr {
  key: &quot;summarize&quot;
  value {
    i: 3
  }
}
attr {
  key: &quot;T&quot;
  value {
    list {
      type: DT_STRING
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
      type: DT_STRING
      type: DT_INT32
    }
  }
}
 of unsupported type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;.

Call arguments received by layer 'embeddings' (type TFBertEmbeddings):
  • input_ids=&lt;KerasTensor: shape=(None, 128) dtype=int32 (created by layer 'input_ids')&gt;
  • position_ids=None
  • token_type_ids=&lt;KerasTensor: shape=(None, 128) dtype=int32 (created by layer 'tf.fill_5')&gt;
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>The code for the model is:</p>
<pre><code>SC_input_layer = Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_ids&quot;)
SC_mask_layer = Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;attention_mask&quot;)
SC_bert_model = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)
SC_pooler_output = SC_bert_model(SC_input_layer, attention_mask=SC_mask_layer)[1]  

# Aggiungi un layer di Dropout
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = Dense(6, activation='sigmoid')(SC_dropout_layer)
SC_model = Model(inputs=[SC_input_layer, SC_mask_layer], outputs=SC_output_layer)
</code></pre>
<p>I found that installing tensorflow 2.10.0 it works but, using Google Colab i have problems with the CUDA version and using tensorflow 2.10 it doesn't recognize the GPU.
This code was working weeks ago, someone has a solution?</p>
<p>EDIT: the same error appears on Kaggle.</p>
","0","Question"
"78176290","","<p>I use a custom trained Yolov8 model to predict whether a physical door is closed or open. I have trained Yolov8 on a custom dataset but it does not make any detections even when passing the same data that was used for training.</p>
<p>I used a dataset of approximately 300 images.
This is my code:</p>
<pre><code>import os

from ultralytics import YOLO
import cv2


VIDEOS_DIR = os.path.join('.', 'videos')

video_path = os.path.join(VIDEOS_DIR, 'sample door.mp4')
video_path_out = '{}_out.mp4'.format(video_path)

cap = cv2.VideoCapture(video_path)
ret, frame = cap.read()
H, W, _ = frame.shape
out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*'MP4V'),        int(cap.get(cv2.CAP_PROP_FPS)), (W, H))

model_path = os.path.join('.', 'runs', 'detect', 'train', 'weights', 'last.pt')


model = YOLO(model_path)  # load a custom model


while ret:

    results = model(frame)[0]
    for result in results.boxes.data.tolist():
        x1, y1, x2, y2, score, class_id = result
        print(x1,y1,x2,y2)

        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)
        cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)

    out.write(frame)
    ret, frame = cap.read()

cap.release()
out.release()
cv2.destroyAllWindows()
</code></pre>
<p>The following are the results of the training: <a href=""https://i.sstatic.net/huyZR.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/huyZR.png</a></p>
","0","Question"
"78176532","","<p>I am trying to send a image to the Gradio Client API following this example:</p>
<pre><code>import { client } from &quot;@gradio/client&quot;;

const response_0 = await fetch(&quot;https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png&quot;);
const exampleImage = await response_0.blob();
                        
const app = await client(&quot;airvit2/pet_classifier&quot;);
const result = await app.predict(&quot;/predict&quot;, [
                exampleImage,   // blob in 'img' Image component
    ]);

console.log(result.data);
</code></pre>
<p>But it returns this error:</p>
<pre><code>{
    &quot;type&quot;: &quot;status&quot;,
    &quot;endpoint&quot;: &quot;/predict&quot;,
    &quot;fn_index&quot;: 0,
    &quot;time&quot;: &quot;2024-03-17T18:36:53.270Z&quot;,
    &quot;queue&quot;: true,
    &quot;message&quot;: null,
    &quot;stage&quot;: &quot;error&quot;,
    &quot;success&quot;: false
}
</code></pre>
<p>This is my Gradio code:</p>
<pre><code>from fastai.vision.all import *
import gradio as gr

learn = load_learner('model.pkl')

def predict(img):
    print(&quot;Imagem: &quot;, img)
    img = load_image(img)
    # img = PILImage.create(img)
    pred, pred_idx, probs = learn.predict(img)
    return pred

gr.Interface(fn = predict, inputs = gr.Image(type=&quot;pil&quot;, height = 224, width = 224), outputs = gr.Label(num_top_classes = 3)).launch(share = True)

</code></pre>
<p>I tried to change the Image Format to Blob, but it didn't work.</p>
","1","Question"
"78178902","","<p>in <a href=""https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.callback.EarlyStopping"" rel=""nofollow noreferrer"">docs</a>
there is a EarlyStopping for XGBClassifier:</p>
<pre><code>```
es = xgboost.callback.EarlyStopping(
    rounds=2,
    min_delta=1e-3,
    save_best=True,
    maximize=False,
    data_name=&quot;validation_0&quot;,
    metric_name=&quot;mlogloss&quot;,
    )
clf = xgboost.XGBClassifier(tree_method=&quot;hist&quot;, device=&quot;cuda&quot;, callbacks=[es])

X, y = load_digits(return_X_y=True)
clf.fit(X, y, eval_set=[(X, y)])```
</code></pre>
<p>but how &quot;validation_0&quot; refers to <code>eval_set</code> in clf.fit to let the EarlyStopping metric evaluate?</p>
<p>and I tried to apply it to XGBRegressor:</p>
<pre><code>`import xgboost as xgb
from sklearn.model_selection import cross_val_predict, KFold
import pandas as pd
import numpy as np

class CustomEarlyStopping(xgb.callback.EarlyStopping):
    def __init__(self, rounds=2, min_delta=1e-3, save_best=True, maximize=False, data_name=&quot;validation_0&quot;, metric_name=&quot;rmse&quot;):
        super().__init__(rounds=rounds, min_delta=min_delta, save_best=save_best, maximize=maximize, data_name=data_name, metric_name=metric_name)
    
# TRAIN MODEL (10x10-fold CV)
cvx = KFold(n_splits=10, shuffle=True, random_state=239)
es = CustomEarlyStopping()

model = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 10, alpha = 10, n_estimators = 500, n_jobs=-1, 
                     random_state=239,callbacks=[es])
model.set_params(tree_method='approx', device=&quot;cpu&quot;)

cv_preds = []
for i in range(0,10):
    cv_preds.append(cross_val_predict(model, np.asarray(X_train), np.asarray(y_train), cv=cvx, method='predict', n_jobs=1, verbose=2))`
</code></pre>
<p>I put data_name=&quot;validation_0&quot; in <code>EarlyStopping</code> <code>__init__</code> without naming test set in each cv fold.
what is wrong with behavior of this code? thanks.</p>
<p>code of XGBRegressor returned this error:</p>
<pre><code>ValueError: Must have at least 1 validation dataset for early stopping.
</code></pre>
<p>what should happen is cv_preds get filled with 10 ndarray of predicted y.</p>
","1","Question"
"78180809","","<p>I'm running PCA in h2o (R version) and was wondering whether it's possible to specify/apply a rotation (like oblimin or promax).  I'm looking for the rotated loadings, and the reason I'm using h2o instead of other common packages for that (like &quot;psych&quot;) is that my data set is huge (100000 columns) so I need to take advantage of h2o's nice parallel computing in Windows. The code I'm using currently is:</p>
<pre><code>library(h2o)

h2o.init(nthreads=64)

x &lt;- read.csv(&quot;file_with_100000_columns.csv&quot;)

for (i in 1:ncol(x)) {x[,i] &lt;- as.factor(x[,i])}

x &lt;- as.h2o(x)

mod &lt;- h2o.prcomp(training_frame=x,k=5,use_all_factor_levels=TRUE)
</code></pre>
<p>Thanks!</p>
","0","Question"
"78183834","","<p>I am trying to use BERT to do a text classification project. However I keep running into this error
`</p>
<pre><code>ValueError                                Traceback (most recent call last)
Cell In[37], line 4
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name='text')
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name='preprocessing')
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url, 
      6                               trainable=True, 
      7                               name='BERT_encoder')
      8 outputs = bert_encoder(preprocessed_text)
ValueError: Exception encountered when calling layer 'preprocessing' (type KerasLayer).
A KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.

Call arguments received by layer 'preprocessing' (type KerasLayer):
  • inputs=&lt;KerasTensor shape=(None,), dtype=string, sparse=None, name=text&gt;
  • training=None

A KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.


</code></pre>
<p>when building this model:</p>
<pre><code>
preprocess_url = 'https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3'
encoder_url = 'https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2'

# Bert Layers
text_input = tf.keras.Input(shape=(), dtype=tf.string, name='text')
bert_preprocess = hub.KerasLayer(preprocess_url, name='preprocessing')
preprocessed_text = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url, 
                              trainable=True, 
                              name='BERT_encoder')
outputs = bert_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1)(outputs['pooled_output'])
l = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(l)

# Construct final model
model = tf.keras.Model(inputs=[text_input], outputs=[l])
</code></pre>
<p>I've watched countless tutorials and even used the ones on the tensorflow docs put even when I copy and paste, they still don't work. I've tried different versions of tf, tf-text, and tf-hub. I am using the tensorflow-gpu-jupyter docker container for this project.</p>
<p>Here is how i'm installing the libraries:</p>
<pre><code>!pip install &quot;tensorflow-text&quot;
!pip install &quot;tf-models-official&quot;
!pip install &quot;tensorflow-hub&quot;
</code></pre>
<p>The versions are:
Tensorflow: 2.16.1
tensorflow-text: 2.16.1
tensorflow-hub: 0.16.1</p>
<p>All the other forums I saw with this issue said to do <code>tf.config.run_functions_eagerly(True)</code> but this didn't work.</p>
<p>Anything will help. Please answer if you know how to solve.</p>
","3","Question"
"78184358","","<p>Here is my conda environment's package list:</p>
<p><img src=""https://i.sstatic.net/uaSzY.png"" alt=""MY PACKAGE LIST"" /></p>
<p>As you can see, I have not installed cudatoolkit and the nvcc comand is not usable. But I do have installed pytorch in CUDA version.</p>
<p>However, when I import torch in python and check <code>torch.cuda.is_available()</code>, it returns Ture.</p>
<p>I even run this test script:</p>
<pre><code>import torch
from torch import nn
from torch.nn import Module
from torch.optim.lr_scheduler import LambdaLR


class TestNet(Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.linear = nn.Linear(10,10)

    def forward(self, x):
        return self.linear(x)
    

if __name__==&quot;__main__&quot;:
    if torch.cuda.is_available():
        device = &quot;cuda&quot;
    else:
        device = &quot;cpu&quot;
    print(f&quot;Using device {device}&quot;)
    test_samples = torch.rand([32,10]).to(device)
    gt_matrix = torch.eye(10).to(device)
    target = torch.matmul(test_samples, gt_matrix)

    model = TestNet().to(device)

    optimizer = torch.optim.SGD(model.parameters(), lr=1)
    criterion = nn.MSELoss()
    scheduler = LambdaLR(optimizer, lr_lambda=lambda x: min(x, 24)/24)

    for epoch in range(128):
        logits = model(test_samples)
        loss = criterion(logits, target)
        learning_rate = optimizer.param_groups[0][&quot;lr&quot;]

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        print(f&quot;Epoch {epoch+1}/{24}, loss {loss.item()}, lr {learning_rate}&quot;)
    
    print(&quot;Learned matrix:&quot;)
    print(model.state_dict()[&quot;linear.weight&quot;])
</code></pre>
<p>And it runs successfully.</p>
<p>So I am curious about how pytorch CUDA version actually works? Does it need a pre-installed CUDA toolkit or not? Besides, what is the difference between installing CUDA by <code>conda install cudatoolkit</code> , <code>conda install cuda</code> and even installing by graphical installer?</p>
","0","Question"
"78184444","","<p>I have 31 features to be input into an ML algorithm. Of these 22 feature values are in the range of 0 to 1 already. The remaining 9 features vary between 0 to 750. My doubt is if I choose to apply MinMaxScaler() with range set to (0,1), should all the features be scaled or only the 9 features outside the desired range be subjected to scaling? What is more appropriate?</p>
","0","Question"
"78185218","","<p>I have a Problem with importing TensorFlow in Virtual Studio Code. I tried to execute my code which starts with importing different modules. One of this lines is importing Tensorflow:</p>
<p><code>import tensorflow as tf</code></p>
<p>Which gives me the Error:</p>
<p>unhashable type: 'list'
File &quot;Link&quot;, line 10, in 
import tensorflow as tf TypeError: unhashable type: 'list'</p>
<p>First I coded this on another computer with Jupyter Notebook, where I can execute it without problems. After I got this error on the Computer on which I want to execute it, I tried to reproduce the error. I installed Virtual Studio Code on the first Computer, installed all modules and executed it successfully. It seems to be a problem in the setting or something.</p>
<p>To test I executed only this which gives me the error:</p>
<p><code>import tensorflow as tf</code></p>
<p>On both computers I had Python 3.9.0 and Tensorflow 2.16.1. After many tries like uninstall and install tensorflow, or reset Virtual Studio Code I decided to ask here. Maybe someone here knows more about this problem :)</p>
","1","Question"
"78185614","","<p>I'm trying to train a CNN model on an image dataset that has an extension <code>.npy</code>, but I get a TypeError in the train loop.</p>
<h3>Imports</h3>
<pre class=""lang-py prettyprint-override""><code>import torch
from torchvision import datasets, transforms
import torch.nn as nn
import torch.optim as optim
import tqdm
</code></pre>
<h3>CNN model</h3>
<pre class=""lang-py prettyprint-override""><code>class CNN(nn.Module):
    def __init__(self) -&gt; None:
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        self.fc1 = nn.Linear(64*64*64, 128)
        self.fc2 = nn.Linear(128, 3)

        self.relu = nn.ReLU()

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(p=0.2)
    
    def forward(self, image: torch.Tensor) -&gt; torch.Tensor:
        image = self.pool(self.relu(self.conv1(image)))
        image = self.pool(self.relu(self.conv2(image)))
        image = self.pool(self.relu(self.conv3(image)))
        image = image.view(-1, 64*64*64)
        image = self.dropout(self.relu(self.fc1(image)))
        image = self.fc2(image)
        return image
</code></pre>
<h3>Hyperparameters</h3>
<pre class=""lang-py prettyprint-override""><code>train_path = 'dataset/train'
val_path = 'dataset/val'
num_epochs = 11
num_classes = 3
batch_size = 64
learning_rate = 0.002
train_test_ratio = 0.9

device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
</code></pre>
<h3>Loading train data</h3>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

def npy_loader(path):
    sample = torch.from_numpy(np.load(path))
    return sample

transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])
dataset = datasets.DatasetFolder(
    root=train_path,
    loader=npy_loader,
    extensions='.npy',
    transform=transform
)

trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
</code></pre>
<h3>Training</h3>
<pre class=""lang-py prettyprint-override""><code>model = CNN()
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()
model.train()
for epoch in range(num_epochs):
    train_loss = 0.0
    for data, target in tqdm(trainloader):
        data = data.to(device)
        target = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * data.size(0)
    train_loss = train_loss/len(trainloader.dataset)
    print(f'Epoch: {epoch+1} \tTraining Loss: {train_loss:.6f}')
</code></pre>
<h3>Error</h3>
<p><code>TypeError: 'module' object is not callable</code> in line <code>for data, target in tqdm(trainloader):</code></p>
<p>I tried coding <code>MyDataset</code> a custom Dataset class, but that did not solve my error.</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, np_file_paths, transform=None):
        self.files = np_file_paths
        self.transform = transform
    
    def __getitem__(self, index):
        x = np.load(self.files[index])
        x = torch.from_numpy(x).float()
        if self.transform is not None:
            image = self.transform(image)
        return x
    
    def __len__(self):
        return len(self.files)
</code></pre>
","0","Question"
"78187211","","<p>I am using PyTorch's LSTM api, but have a bit of an issue. I'm using an LSTM for a dummy AI model. The task of the model is to return 1 if the previous number is less than the current one.</p>
<p>So for an array like <code>[0.7, 0.3, 0.9, 0.99]</code>, the expected outputs are <code>[1.0, 0.0, 1.0, 1.0]</code>. The first output should be <code>1.0</code> no matter what.</p>
<p>I designed the following network to try this problem:</p>
<pre class=""lang-py prettyprint-override""><code># network.py

import torch

N_INPUT = 1
N_STACKS = 1
N_HIDDEN = 3

LR = 0.001


class Network(torch.nn.Module):

    # params: self
    def __init__(self):
        super(Network, self).__init__()

        self.lstm = torch.nn.LSTM(
            input_size=N_INPUT,
            hidden_size=N_HIDDEN,
            num_layers=N_STACKS,
        )

        self.linear = torch.nn.Linear(N_HIDDEN, 1)
        self.relu = torch.nn.ReLU()

        self.optim = torch.optim.Adam(self.parameters(), lr=LR)
        self.loss = torch.nn.MSELoss()

    # params: self, predicted, expecteds
    def backprop(self, xs, es):

        # perform backprop
        self.optim.zero_grad()
        l = self.loss(xs, torch.tensor(es))
        l.backward()
        self.optim.step()

        return l

    # params: self, data (as a python array)
    def forward(self, dat):

        out, _ = self.lstm(torch.tensor(dat))

        out = self.relu(out)
        out = self.linear(out)

        return out
</code></pre>
<p>And I am calling this from this file:</p>
<pre class=""lang-py prettyprint-override""><code># main.py

import network

import numpy as np

# create a new network
n: network.Network = network.Network()


# create some data
def rand_array():

    # a bunch of random numbers
    a = [[np.random.uniform(0, 1)] for i in range(1000)]

    # now, our expected value is 0 if the previous number is greater, and 1 else
    expected = [0.0 if a[i - 1][0] &gt; a[i][0] else 1.0 for i in range(len(a))]
    expected[0] = 1.0  # make the first element always just 1.0

    return [a, expected]


# a bunch of random arrays
data = [rand_array() for i in range(1000)]

# 100 epochs
for i in range(100):
    for i in data:

        pred = n(i[0])
        loss = n.backprop(pred, i[1])
        print(&quot;Loss: {:.5f}&quot;.format(loss))
</code></pre>
<p>Now, when I run this program, I'm just getting a loss around <code>0.25</code>, and it isn't really changing once it gets there. I think the model is just picking the average value of <code>0</code> and <code>1</code> (<code>0.5</code>) for each input.</p>
<p>This leads me to the belief that the model can't see the previous data; the data is just random numbers (the expected output is based on these random numbers, though), and the model can't remember what happened before.</p>
<p>What is my issue?</p>
","0","Question"
"78189166","","<p>I'm having an issue with the preProc argument for the train function using the R caret package. I want to center and scale my predictors but ignore the factor columns. When I preProcess outside of train, it works fine but I'm hoping to pre-process within the train function. Am I missing something?</p>
<p>Below is an example where the factor predictor is ignored when using preProcess outside of train.</p>
<pre><code>df &lt;- data.frame(
    score = runif(1000, 80, 110),
    var1 = as.factor(sample(0:1, 1000, replace = TRUE)),
    var2 = runif(1000, 5, 25)
)
preProcess(df[-1], method=c('center','scale'))

Created from 1000 samples and 2 variables

Pre-processing:
  - centered (1)
  - ignored (1)
  - scaled (1)
</code></pre>
<p>Here is what happens when I use preProc inside of train</p>
<pre><code>df &lt;- data.frame(
    score = runif(1000, 80, 110),
    var1 = as.factor(sample(0:1, 1000, replace = TRUE)),
    var2 = runif(1000, 5, 25)
)
mod &lt;- train(score ~., data = df,
             method = &quot;lm&quot;,
             preProc = c(&quot;center&quot;, &quot;scale&quot;))
mod$preProcess

Created from 1000 samples and 2 variables

Pre-processing:
  - centered (2)
  - ignored (0)
  - scaled (2)
</code></pre>
","2","Question"
"78192634","","<p>I’m experiencing an issue with the inference API for my Vision Transformer (ViT) model, rshrott/vit-base-renovation2.</p>
<p><a href=""https://huggingface.co/rshrott/vit-base-renovation2"" rel=""nofollow noreferrer"">https://huggingface.co/rshrott/vit-base-renovation2</a></p>
<p>When I attempt to use the API, I receive the following error:</p>
<pre><code>{
“error”: &quot;HfApiJson(Deserialize(Error(“unknown variant image-feature-extraction, expected one of audio-classification, audio-to-audio, audio-source-separation, automatic-speech-recognition, feature-extraction, text-classification, token-classification, question-answering, translation, summarization, text-generation, text2text-generation, fill-mask, zero-shot-classification, zero-shot-image-classification, conversational, table-question-answering, image-classification, image-segmentation, image-to-text, text-to-speech, … visual-question-answering, video-classification, document-question-answering, image-to-image, depth-estimation, line: 1, column: 318)))”
}
</code></pre>
<p>Interestingly, when I use the transformers pipeline directly in Python, the model works as expected:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
from PIL import Image
import requests

pipe = pipeline(model=“rshrott/vit-base-renovation2”)
url = 'https://example.com/image.jpeg'
image = Image.open(requests.get(url, stream=True).raw)
preds = pipe(image)
</code></pre>
<p>This code runs without any issues and returns the expected predictions. However, the same model encounters an error when used through the inference API. I suspect there might be a configuration issue related to the expected task type, but I’m not sure how to resolve it.</p>
<p>Why is this error occurring and how can I fix it? I’ve checked the model card and configuration, but I can’t seem to find where “image-feature-extraction” is coming from or why it’s expected.</p>
","0","Question"
"78196301","","<p>I have a retrieval tensorflow training model and i use a <code>tfrs.layers.factorized_top_k.BruteForce</code> to get a prediction of firsts k's nearby candidates as implemented below:</p>
<pre><code>index = tfrs.layers.factorized_top_k.BruteForce(final_model.query_model)

index.index_from_dataset(
    tf.data.Dataset.zip((parsed_topK.batch(128).map(lambda x: x['id']), parsed_topK.batch(128).map(final_model.candidate_model)))
)
</code></pre>
<p>and get the top 5 results:</p>
<pre><code>results = index(input_query, k=5)
</code></pre>
<p>I want to know if is possible to turn the search database, represented in this code by <code>parsed_topK</code>, in to an input to the model, something like:</p>
<pre><code>index(input_query, input_candidates, k=5)
</code></pre>
<p>where <code>input_candidates = parsed_topK</code> in this example</p>
<p>I have tryed to call <code>final_model.predict(input_query, input_candidates)</code>, but i need to implement a call() method and i dont know what this method need to do.</p>
","0","Question"
"78196623","","<p>I am running a simple script within my Anaconda virtual env</p>
<pre><code>from deepface import DeepFace

face_analysis = DeepFace.analyze(img_path = &quot;face3.jpeg&quot;)
print(face_analysis)
</code></pre>
<p>But I keep getting this error.</p>
<pre><code>Action: age:  25%|██████████████████████████▊                                                                                | 1/4 [00:02&lt;00:06,  2.08s/it]
Traceback (most recent call last):
  File &quot;C:\Users\Ctrend.pk\Cheer-Check\test2.py&quot;, line 9, in &lt;module&gt;
    analysis = DeepFace.analyze(img_path)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\DeepFace.py&quot;, line 222, in analyze
    return demography.analyze(
           ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\modules\demography.py&quot;, line 157, in analyze
    apparent_age = modeling.build_model(&quot;Age&quot;).predict(img_content)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\modules\modeling.py&quot;, line 57, in build_model
    model_obj[model_name] = model()
                            ^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\extendedmodels\Age.py&quot;, line 32, in __init__
    self.model = load_model()
                 ^^^^^^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\extendedmodels\Age.py&quot;, line 61, in load_model
    age_model = Model(inputs=model.input, outputs=base_model_output)
                             ^^^^^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\ops\operation.py&quot;, line 228, in input
    return self._get_node_attribute_at_index(0, &quot;input_tensors&quot;, &quot;input&quot;)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\ops\operation.py&quot;, line 259, in _get_node_attribute_at_index
    raise ValueError(
ValueError: The layer sequential_1 has never been called and thus has no defined input.
</code></pre>
<p>Deepface version: 0.0.87</p>
<p>tensorflow
Version: 2.16.1</p>
<p>I think it fetches the age but then doesn't proceed. What am I missing out on?</p>
","-2","Question"
"78196675","","<p>I am trying to deploy a basic python app which uses the Deepface.analyze function. When trying to deploy the app on Vercel, I get this error:</p>
<pre><code>Size of uploaded file exceeds 300MB
</code></pre>
<p>Is it because of large libraries like deepFace and tensorflow? Or could it because of my code structure?</p>
<p>I have a basic python flask app with this structure:</p>
<pre><code>static
--styles.css
templates
--index.html
app.py
requirements.txt
</code></pre>
","0","Question"
"78196998","","<p>I'm new to PyTorch and creating a multi-output linear regression model to color words based on their letters. (This will help people with grapheme-color synesthesia have an easier time reading.) It takes in words and outputs RGB values. Each word is represented as a vector of 45 floats [0,1], where (0, 1] represents letters and 0 represents that no letter exists in that place. The output for each sample should be a vector [r-value, g-value, b-value].</p>
<p>I'm getting</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (90x1 and 45x3)</p>
</blockquote>
<p>when I try to run my model in the training loop.</p>
<p>Looking at extant Stack Overflow posts, I think this means that I need to reshape my data, but I don't know how/where to do so in a way that would solve this problem. Especially considering that I don't know where that 90x1 matrix came from.</p>
<p><strong>My Model</strong></p>
<p>I started simple; multiple layers can come after I can get a single layer to function.</p>
<pre><code>class ColorPredictor(torch.nn.Module):
    #Constructor
    def __init__(self):
        super(ColorPredictor, self).__init__()
        self.linear = torch.nn.Linear(45, 3, device= device) #length of encoded word vectors &amp; size of r,g,b vectors
        
    # Prediction
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        y_pred = self.linear(x)
        return y_pred
</code></pre>
<p><strong>How I'm loading my data</strong></p>
<pre><code># Dataset Class
class Data(Dataset):
    # Constructor
    def __init__(self, inputs, outputs):
        self.x = inputs # a list of encoded word vectors
        self.y = outputs # a Pandas dataframe of r,g,b values converted to a torch tensor
        self.len = len(inputs)
    
    # Getter
    def __getitem__(self, index):
        return self.x[index], self.y[index]
    
    # Get number of samples
    def __len__(self):
        return self.len
</code></pre>
<pre><code># create train/test split
train_size = int(0.8 * len(data))
train_data = Data(inputs[:train_size], outputs[:train_size])
test_data = Data(inputs[train_size:], outputs[train_size:])
</code></pre>
<pre><code># create DataLoaders for training and testing sets
train_loader = DataLoader(dataset = train_data, batch_size=2)
test_loader = DataLoader(dataset = test_data, batch_size=2)
</code></pre>
<p><strong>The testing loop, where the error occurs</strong></p>
<pre><code>for epoch in range(epochs):
    # Train
    model.train() #training mode
    for x,y in train_loader:
        y_pred = model(x) #ERROR HERE
        loss = criterion(y_pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
      
</code></pre>
<p><strong>Error Traceback</strong>
<a href=""https://i.sstatic.net/2Ojco.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2Ojco.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/X45n4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X45n4.png"" alt=""enter image description here"" /></a></p>
<h1>New Attempt:</h1>
<p>Changed the 45x1 input tensor to a 2x45 input tensor, with the second column being all zeros. This works for the first run through the train_loader loop, but during the second run through the train_loader loop I get another matrix multiplication error, this time for matrices of sizes 90x2 and 45x3.</p>
","0","Question"
"78197199","","<p>Suppose for a given data set: T’s and Y’s are arrays with T = [0 1 2 3 5 6 7] Y= [4 7 9 3 6 1] So at T=0, Y=4 and so on Z = [Red] with just one element in Z. T and Y are continuous inputs. Z discrete (can be either red or yellow).</p>
<p>Another given set: T = [1 3 4 9 3] Y= [4 9 2 1 6] Z=[Yellow]</p>
<p>Suppose I have many similar sets, what classification techniques can I use to explore relations which take T and Y as continuous array inputs and outputs categorical Z of single element?</p>
<p>I am a bit confused as the inputs themselves are arrays whereas the output is just a single element</p>
","-2","Question"
"78197496","","<p>Value error problem using tensorflow
I have an issue from part of my tensor flow code when building a neural network: The error is a</p>
<pre><code>ValueError: Argument(s) not recognized : {'lr' : 0.001}
</code></pre>
<p>The section of my code referred to is:</p>
<pre><code>model.compile( loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizer.Adam(lr=0.001), metrics=[&quot;accuracy&quot;],)
</code></pre>
<p>Any ideas?</p>
<p>After importing the required models (following tensorflow tutorial)
such as keras; likewise layers and models from tensorflow.keras, and mnist from tensorflow.keras.darasets</p>
<p>I set model equals</p>
<pre><code>keras.sequential....
model.compile...
model.fit...
model.evaluate.
</code></pre>
<p>After all this, instead of getting the accuracy of my training and testing datasets I defined in my code, I get the value error</p>
","1","Question"
"78199806","","<p>I have to implement an equivalent function to compute the hessian of the logistic loss, written as a sum of logarithm of exponential terms. I have implemented the following function in python:</p>
<pre><code>def hessian(self,w,hess_trick=0):
        hess = 0
        for x_i,y_i in zip(self.data, self.labels):
            hess += np.exp(y_i * np.dot(w.T, x_i))/((1 + np.exp(y_i * np.dot(w.T,x_i)))**2) *       np.outer(x_i,x_i.T)
        return hess + lambda_reg * np.identity(w.shape[0]) + hess_trick * 10**(-12) *    np.identity(w.shape[0])
</code></pre>
<p>My question is how i can code an equivalent, but faster function, without using the slow python for?</p>
<p>Since i am not so confident with numpy, i tried to code the following function:</p>
<pre><code>    def new_hessian(self, w, hess_trick=0):
        exp_term = np.exp(self.labels * np.dot(self.data, w))
        sigmoid_term = 1 + exp_term
        inv_sigmoid_sq = 1 / sigmoid_term ** 2

        diag_elements = np.sum((exp_term * inv_sigmoid_sq)[:, np.newaxis] * self.data ** 2, axis=0)
        off_diag_elements = np.dot((exp_term * inv_sigmoid_sq) * self.data.T, self.data)
        hess = np.diag(diag_elements) + off_diag_elements
        regularization = lambda_reg * np.identity(w.shape[0])

        hess += hess_trick * 1e-12 * np.identity(w.shape[0])

        return hess + regularization
</code></pre>
<p>By debugging this function, i saw that there is a fundamental problem. For small values of the number of features(say less than 200), the two implementations of the hessian are not equal. When i increase the number of features, the two functions seems to be equal. The problem is that when testing those implementations using Newton's method to optimize the log loss, the faster implementations converges in more iterations than the first(but slower in terms of runtime) implementation.</p>
","-1","Question"
"78201576","","<p>I am trying to solve a seq-to-seq problem with a LSTM in Pytorch. Concretely, I am taking sequences of 5 elements, to predict the next 5 ones. My concern has to do with the data transformations. I have tensors of size [bs, seq_length, features], where <code>seq_length = 10</code>, and <code>features = 1</code>. Each feature is an int number between 0 and 3 (both included).</p>
<p>I believed input data had to be transformed to float range [0, 1] with a MinMaxScaler, in order to make the LSTM learning process easier. After that, I apply a Linear layer, which transforms the hidden states into the corresponding output, with size <code>features</code>. My definition of the LSTM network in Pytorch:</p>
<pre class=""lang-py prettyprint-override""><code>class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_prob):
        super(LSTM, self).__init__()
        self.lstm_layer = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout_prob)
        self.output_layer = nn.Linear(hidden_dim, output_dim)

    ...

    def forward(self, X):
        out, (hidden, cell) = self.lstm_layer(X)
        out = self.output_layer(out)
        return out
</code></pre>
<p>The code I use to do the training loop is the following:</p>
<pre class=""lang-py prettyprint-override""><code>def train_loop(t, checkpoint_epoch, dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, X in enumerate(dataloader):
        X = X[0].type(torch.float).to(device)

        # X = torch.Size([batch_size, 10, input_dim])
        # Split sequences into input and target
        inputs = transform(X[:, :5, :]) # inputs = [batch_size, 5, input_dim]
        targets = transform(X[:, 5:, :]) # targets = [batch_size, 5, input_dim]

        # predictions (forward pass)
        with autocast():
            pred = model(inputs)  # pred = [batch_size, 5, input_dim]
            loss = loss_fn(pred, targets)

        # backprop
        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            #print(f&quot;Current loss: {loss:&gt;7f}, [{current:&gt;5d}/{size:&gt;5d}]&quot;)

        # Delete variables and empty cache
        del X, inputs, targets, pred
        torch.cuda.empty_cache()

    return loss
</code></pre>
<p>The code I used for preprocessing the data:</p>
<pre class=""lang-py prettyprint-override""><code>def main():
    num_agents = 2
    # Open the HDF5 file
    with h5py.File('dataset_' + str(num_agents) + 'UAV.hdf5', 'r') as f:
        # Access the dataset
        data = f['data'][:]
        # Convert to PyTorch tensor
        data_tensor = torch.tensor(data)

        size = data_tensor.size()
        seq_length = 10
        reshaped = data_tensor.view(-1, size[2], size[3])

        r_size = reshaped.size()
        reshaped = reshaped[:, :, 1:]
        reshaped_v2 = reshaped.view(r_size[0], -1)

        dataset = create_dataset(reshaped_v2.numpy(), seq_length)

        f.close()

    dataset = TensorDataset(dataset)

    # Split the dataset into training and validation sets
    train_size = int(0.8 * len(dataset))  # 80% for training
    val_size = len(dataset) - train_size  # 20% for validation
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, pin_memory=True)
    val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, pin_memory=True)
</code></pre>
<p>Trying this, the model was not learning properly, so I was thinking that maybe directly calculating the loss between <code>targets</code> (float values in range [0, 1]) and <code>pred</code> (what I believe are float values in range [-1, 1] because of tanh activation functions from LSTM layer), with different scales might be wrong. Then, I tried aplying a sigmoid activation function right after the Linear layer in the forward pass, but wasn't learning properly neither. I tried executions with many hyperparameter combinations, but none resulted in a &quot;normal&quot; training curve. I also attach a screenshot for 5000 epochs to illustrate the training process:</p>
<p><a href=""https://i.sstatic.net/veOhS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/veOhS.jpg"" alt=""training process"" /></a></p>
<p>My questions are:</p>
<ul>
<li>What seems to be wrong in my training process?</li>
<li>Is there anything I said that is thought in a wrong way?</li>
</ul>
","0","Question"
"78203794","","<p>My goal is to deploy a model I trained with Fastai into Torchserve. I was following <a href=""https://aws.amazon.com/blogs/opensource/deploy-fast-ai-trained-pytorch-model-in-torchserve-and-host-in-amazon-sagemaker-inference-endpoint/"" rel=""nofollow noreferrer"">this tutorial</a> but got stuck on the part where he created the model class for pytorch.</p>
<p>He mentions that to run our model in Torchserve, and we need the following:</p>
<ol>
<li>A model class</li>
<li>The weights exported from pytorch (a pth file)</li>
<li>A handler</li>
</ol>
<p>Out of these, I get two: the weights and the handler. However, where I'm stuck is in the model class. He created one class file, but I have no idea where he got the <strong>DynamicUnet</strong> to use as a base for the class or how he mixed that class with <strong>unet_learner</strong> to create a custom pytorch model class. Can you help me build a model class for a model trained under the learner <strong>vision_learner</strong> and the pre-trained model of <strong>resnet50</strong>?</p>
","-1","Question"
"78204216","","<p>If I'm creating a Rolling Mean Feature based on my Sales (target) column, is it necessary to shift it?</p>
<p>Let me give an example:</p>
<p>Lets suppose I have days 01~10 in my dataset. If I create a Mean Rolling Window column of 7 Days, in my day 10th day row, It will consider the 7th day as the value of this row to calculate the Rolling Mean, for example. Now If I'm going to predict day 11, which is tomorrow, I would need the Sales value of this day in order to have the Rolling Mean, which makes no Sense.</p>
<p>So, It makes more Sense in my opinion to always get the 7 last Days, not considering the current.</p>
<p>Can anyone help?</p>
","-1","Question"
"78206448","","<p>I've been using a library in R called 'aweSOM.' It's a library based upon providing HTML-interactive visuals for self-organising maps (SOMs).</p>
<p>aweSOM produces some nicer visuals than similar SOM packages, so I'd to use it. However, the problem is two-fold:</p>
<ol>
<li>HTML-interactive visuals are not suitable for publication.</li>
<li>When I save as a PNG, there is (interactive) text remaining on the
PNG 'Hover Over The Plot For Information.'</li>
</ol>
<p>Therefore, I wondered if it is possible to write a function such that the 'plot' is saved as a PNG but without the interactive text above?</p>
<p>Therefore, effectively write a function that saves only a square of a particular size that would omit the text?</p>
<p>Would appreciate your feedback and assistance.</p>
<pre><code>install.packages(&quot;aweSOM&quot;)
library(aweSOM)

full.data &lt;- iris
train.data &lt;- full.data[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]
train.data &lt;- scale(train.data)

set.seed(1465)
init &lt;- somInit(train.data, 4, 4)
iris.som &lt;- kohonen::som(train.data, grid = kohonen::somgrid(4, 4, &quot;hexagonal&quot;), 
                         rlen = 100, alpha = c(0.05, 0.01), radius = c(2.65,-2.65), 
                         dist.fcts = &quot;sumofsquares&quot;, init = init)

superclust_pam &lt;- cluster::pam(iris.som$codes[[1]], 3)
superclasses_pam &lt;- superclust_pam$clustering

###########
#PROBLEMATIC PLOT WITH INTERACTIVE TEXT
##########

plot &lt;- aweSOMplot(som = iris.som, type = &quot;Cloud&quot;, data = full.data, 
           variables = c(&quot;Species&quot;, &quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;,  
                         &quot;Petal.Length&quot;, &quot;Petal.Width&quot;), 
           superclass = superclasses_pam)

</code></pre>
<p>All help would be appreciated with fixing the visualisation. A vignette is here:</p>
<p><a href=""https://cran.r-project.org/web/packages/aweSOM/vignettes/aweSOM.html#the-awesom-package"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/aweSOM/vignettes/aweSOM.html#the-awesom-package</a></p>
<p><a href=""https://i.sstatic.net/Kcofm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kcofm.png"" alt=""enter image description here"" /></a></p>
","2","Question"
"78207131","","<p>I'm currently working on a miniproject for my course, and I've run into a roadblock that I'm hoping to get some help with. After training an SVM model in Azure Machine Learning Studio and deploying it as a web service, I've encountered a peculiar issue—the service returns the same prediction regardless of the input data.</p>
<p>What might be going wrong?</p>
<p><a href=""https://gallery.cortanaintelligence.com/Experiment/Binary-Classifier-SVM-Web"" rel=""nofollow noreferrer"">https://gallery.cortanaintelligence.com/Experiment/Binary-Classifier-SVM-Web</a>
<a href=""https://gallery.cortanaintelligence.com/Experiment/Binary-Classifiers-SVM"" rel=""nofollow noreferrer"">https://gallery.cortanaintelligence.com/Experiment/Binary-Classifiers-SVM</a></p>
<p>Here's what I've done so far:</p>
<ul>
<li>Ensured all preprocessing steps are identical to those used during the model's training phase (including data normalization and handling of missing values).</li>
<li>Double-checked that the model is being scored with the input data.</li>
<li>Verified the web service's configuration, particularly in constructing responses to ensure no static value is being returned.</li>
<li>Made sure the input data's format matches the expected schema. The model performs well within the ML Studio environment and predicts accurately with the test data. However, the deployed web service doesn't seem to reflect this behavior and outputs a constant value.</li>
</ul>
","0","Question"
"78207935","","<p>I'm currently trying to work on a small image machine learning project. I found this person's <a href=""https://www.kaggle.com/code/faresabbasai2022/skin-cancer-classification"" rel=""nofollow noreferrer"">Kaggle code</a> and I tried replicating it from scratch. However, not even in the main part, I already faced an error.</p>
<p>I'm sure there must be a localization issue on my end on how this ended up but I can't figure what.</p>
<p>My code:</p>
<pre><code>#Import Libraries

#Data processing modules
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import cv2
#File directory modules
import glob as gb
import os
#Training and testing (machine learning) modules
import tensorflow as tf 
import keras

#Importing the images into the code

trainDataset = 'melanoma_cancer_dataset/train'
testDataset = 'melanoma_cancer_dataset/test'
predictionDataset = 'melanoma_cancer_dataset/skinTest'

#creating empty lists for the images to fall into for processing
training_List = []
testing_list = []
#making a classification dictionary for the two keys, benign and malignant
#used for inserting into the images
diction = {'benign' : 0, 'malignant' : 1}

#Read through the folder's length contents
for folder in os.listdir(trainDataset):
    data = gb.glob(pathname=str(trainDataset + folder + '/*.jpg'))
    print(f'{len(data)} in folder {folder}')
    #read the images, resize them in a uniform order, and store them in the empty lists
    for data in data:
        image = cv2.imread(data)
        imageList = cv2.resize(image(120,120))
        training_List.append(list(imageList))
</code></pre>
<p>The output of the notebook showed that it had <strong>0 images/contents</strong> stored in the folder. Now I'm kinda doubting what's happening here and would love some answers. Thanks in advance. I'm using my own VScode too.</p>
<p>This is a screenshot of my files:</p>
<p><a href=""https://i.sstatic.net/4dne3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4dne3.png"" alt=""file tree and notebooks"" /></a></p>
","2","Question"
"78208603","","<p>In the documentation of torch.nn.functional.linear (<a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html</a>), the dimensions of the weight input are (out_features, in_features) then the wight matrix is transposed when computing the output: y=xA^T+b. Why are they doing this instead of taking a matrix W of dimensions (in_features, out_features) and doing y=xW+b?</p>
<p>By doing y=xW+b the dimensions will match and so I cannot find a clear reason for the above.</p>
","0","Question"
"78208624","","<p>Getting the following error as I'm training some torch models:</p>
<pre><code>ValueError('Expected parameter scale (Tensor of shape (1, 4)) of distribution Normal(loc: torch.Size([1, 4]), scale: torch.Size([1, 4])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[inf, inf, 0., 0.]])').
</code></pre>
<p>My actions are of shape (4,) and observations (3,).</p>
<p>Does it think that infinity isn't &gt;0, or that 0 is not greater than 0? And why is this even appearing in the first place, I don't know. It is from simply training a model using model.learn in stable baselines 3. However, it learns for a while but fails at this step:</p>
<pre><code>~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py in learn(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)
    257 
    258         while self.num_timesteps &lt; total_timesteps:
--&gt; 259             continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
    260 
    261             if continue_training is False:

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py in collect_rollouts(self, env, callback, rollout_buffer, n_rollout_steps)
    167                 # Convert to pytorch tensor or to TensorDict
    168                 obs_tensor = obs_as_tensor(self._last_obs, self.device)
--&gt; 169                 actions, values, log_probs = self.policy(obs_tensor)
    170             actions = actions.cpu().numpy()
    171 

~\anaconda3\envs\\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
   1192         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1194             return forward_call(*input, **kwargs)
   1195         # Do not call functions when jit is used
   1196         full_backward_hooks, non_full_backward_hooks = [], []

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py in forward(self, obs, deterministic)
    624         # Evaluate the values for the given observations
    625         values = self.value_net(latent_vf)
--&gt; 626         distribution = self._get_action_dist_from_latent(latent_pi)
    627         actions = distribution.get_actions(deterministic=deterministic)
    628         log_prob = distribution.log_prob(actions)

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py in _get_action_dist_from_latent(self, latent_pi)
    654 
    655         if isinstance(self.action_dist, DiagGaussianDistribution):
--&gt; 656             return self.action_dist.proba_distribution(mean_actions, self.log_std)
    657         elif isinstance(self.action_dist, CategoricalDistribution):
    658             # Here mean_actions are the logits before the softmax

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\distributions.py in proba_distribution(self, mean_actions, log_std)
    162         &quot;&quot;&quot;
    163         action_std = th.ones_like(mean_actions) * log_std.exp()
--&gt; 164         self.distribution = Normal(mean_actions, action_std)
    165         return self
    166 

~\anaconda3\envs\\lib\site-packages\torch\distributions\normal.py in __init__(self, loc, scale, validate_args)
     54         else:
     55             batch_shape = self.loc.size()
---&gt; 56         super(Normal, self).__init__(batch_shape, validate_args=validate_args)
     57 
     58     def expand(self, batch_shape, _instance=None):

~\anaconda3\envs\\lib\site-packages\torch\distributions\distribution.py in __init__(self, batch_shape, event_shape, validate_args)
     55                 if not valid.all():
     56                     raise ValueError(
---&gt; 57                         f&quot;Expected parameter {param} &quot;
     58                         f&quot;({type(value).__name__} of shape {tuple(value.shape)}) &quot;
     59                         f&quot;of distribution {repr(self)} &quot;
</code></pre>
<p>Keep in mind my actions are 0&lt;=a&lt;=1. Do I need to make it 0&lt;a&lt;=1 for it to work? It doesn't seem so, because it will train train train and be fine, but then once the trial is up and it's updating weights, it dies. What could be the fix and/or explanation for this? Much appreciated.</p>
<p>It's hard for me to know what the hell it's even complaining about, because this code is deep within stable baselines 3. Could it possibly be a glitch in their packages? I expect it to update the weights and continue to run, but instead it complains that 0 isn't greater than 0.. I don't know why I care about that though, it should just keep going, no?</p>
<p>Thanks for taking a look.</p>
","0","Question"
"78209231","","<p>I am training a model using Decision Tree and parameter otimization.</p>
<p>I read that the objective of the validation set is to assess model performance during training and help tune parameters.</p>
<p>With this in my mind shouldn't I be using the validation set <code>on grid_search.fit</code> instead of using my training set?</p>
<pre><code>param_grid = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

clf = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(&quot;Best Parameters:&quot;, best_params)
print(&quot;\n&quot;)

#Validation
best_clf = grid_search.best_estimator_
val_accuracy = best_clf.score(X_val, y_val)
print(&quot;Validation Accuracy with Best Model:&quot;, val_accuracy)
print(&quot;\n&quot;)

#Test
y_test_pred = best_clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_recall = recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
print(&quot;Decision Tree Measurements on Test Set with Best Model:&quot;)
print(&quot;Accuracy:&quot;, test_accuracy)
print(&quot;Precision:&quot;, test_precision)
print(&quot;Recall:&quot;, test_recall)
print(&quot;F1 Score:&quot;, test_f1)
print(&quot;-------------------------------------------------------&quot;)
</code></pre>
","0","Question"
"78210297","","<p>I'm attempting to convert this <a href=""https://huggingface.co/UrukHan/wav2vec2-russian"" rel=""nofollow noreferrer"">model</a> in .pt format. It's working fine for me so i dont want to fine-tune it. How can i export it to .pt and run interface?</p>
<p>I tried using this to convert to .pt:</p>
<pre><code>from transformers import AutoConfig, AutoProcessor, AutoModelForCTC, AutoTokenizer, Wav2Vec2Processor
import librosa
import torch



# Define the model name
model_name = &quot;UrukHan/wav2vec2-russian&quot;

# Load the model and tokenizer
config = AutoConfig.from_pretrained(model_name)
model = AutoModelForCTC.from_pretrained(model_name, config=config)
processor = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Save the model as a .pt file
torch.save(model.state_dict(), &quot;model.pt&quot;)

# Save the tokenizer as well if needed
tokenizer.save_pretrained(&quot;model-tokenizer&quot;)
</code></pre>
<p>but unfortunately its not running the interface :</p>
<pre><code>model = AutoModelForCTC.from_pretrained(&quot;model.pt&quot;)
processor = AutoProcessor.from_pretrained(&quot;model.pt&quot;)


# Perform inference with the model
FILE = 'here is wav.wav'
audio, _ = librosa.load(FILE, sr = 16000)
audio = list(audio)
def map_to_result(batch):
  with torch.no_grad():
    input_values = torch.tensor(batch, device=&quot;cpu&quot;).unsqueeze(0) #, device=&quot;cuda&quot;
    logits = model(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  batch = processor.batch_decode(pred_ids)[0]
  return batch
map_to_result(audio)
print(map_to_result(audio))


model.eval()
</code></pre>
<p>And encountered an error:
`model.pt is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'</p>
<p>`</p>
","-1","Question"
"78210393","","<p>I'm trying to reproduce the experiments of the S5 model, <a href=""https://github.com/lindermanlab/S5"" rel=""nofollow noreferrer"">https://github.com/lindermanlab/S5</a>, but I encountered some issues when solving the environment. When I'm running the shell script<code>./run_lra_cifar.sh</code>, I get the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Path/S5/run_train.py&quot;, line 3, in &lt;module&gt;
    from s5.train import train
  File &quot;/Path/S5/s5/train.py&quot;, line 7, in &lt;module&gt;
    from .train_helpers import create_train_state, reduce_lr_on_plateau,\
  File &quot;/Path/train_helpers.py&quot;, line 6, in &lt;module&gt;
    from flax.training import train_state
  File &quot;/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py&quot;, line 19, in &lt;module&gt;
    from . import core
  File &quot;/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py&quot;, line 15, in &lt;module&gt;
    from .axes_scan import broadcast
  File &quot;/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py&quot;, line 22, in &lt;module&gt;
    from jax import linear_util as lu
ImportError: cannot import name 'linear_util' from 'jax' (/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py)
</code></pre>
<p>I'm running this on an RTX4090 and my CUDA version is 11.8. My jax version is 0.4.25 and jaxlib version is 0.4.25+cuda11.cudnn86</p>
<p>I first tried to install the dependencies using the author's</p>
<pre><code>pip install -r requirements_gpu.txt
</code></pre>
<p>However, this doesn't seem to work in my case since I can't even<code>import jax</code>. So I installed jax according to the instructions on <a href=""https://jax.readthedocs.io/en/latest/installation.html"" rel=""nofollow noreferrer"">https://jax.readthedocs.io/en/latest/installation.html</a>
by typing</p>
<pre><code>pip install --upgrade pip
pip install --upgrade &quot;jax[cuda11_pip]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
</code></pre>
<p>So far I've tried:</p>
<ol>
<li>Using a older GPU(3060 and 2070)</li>
<li>Downgrading python to 3.9</li>
</ol>
<p>Does anyone know what could be wrong? Any help is appreciated</p>
","3","Question"
"78212101","","<p>I've been encountering a perplexing issue while working on a binary text classification task. Despite experimenting with numerous deep learning models, including various architectures and hyperparameters, I consistently observe high training accuracy, typically ranging from 97% to 99%. However, when I evaluate these models on unseen testing data, their performance significantly deteriorates.</p>
<p>In an attempt to address this issue, I decided to explore machine learning models as an alternative approach. Surprisingly, models such as Random Forest yielded comparable or even superior performance to deep learning models, achieving around 97% accuracy on both training and testing data. Subsequently, I experimented with several other machine learning algorithms, and Logistic Regression emerged as the most suitable option for my specific use case.</p>
<p>Despite these findings, I remain puzzled as to why the deep learning models, despite exhibiting impressive training accuracy, fail to generalize well to unseen data. Could someone shed light on potential reasons behind this discrepancy? Are there common pitfalls or considerations specific to deep learning that I might be overlooking? Any insights or suggestions would be greatly appreciated.</p>
","-2","Question"
"78214495","","<p>This is my first attempt in RAG application. I am trying to do Q&amp;A using LLM. I will paste code below which is working fine. My problem is that the code to generate embedding run every time I run python code. Is there any way to run it only once or to check if the embeddings folder is empty, than run that code.</p>
<pre><code>from langchain_community.document_loaders import WebBaseLoader
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community import embeddings
from langchain_community.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings

model_local = ChatOllama(model=&quot;codellama:7b&quot;)

loader = TextLoader(&quot;remedy.txt&quot;)
raw_doc = loader.load()

# Split the text file content into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
splitted_docs = text_splitter.split_documents(raw_doc)

# Use embedding function to store them in vector db
ollamaEmbeddings = embeddings.ollama.OllamaEmbeddings(model=&quot;nomic-embed-text&quot;)


# used chroma vector db to store the data
vectorstore = Chroma.from_documents(
    documents=splitted_docs,
    embedding=ollamaEmbeddings,
    persist_directory=&quot;./vector/my_data&quot;,
)

# This will write the data to local
retriever = vectorstore.as_retriever()

# 4. After RAG
print(&quot;After RAG\n&quot;)
after_rag_template = &quot;&quot;&quot;
    Answer the question based only on the following context:
    {context}
    Question {question}?
&quot;&quot;&quot;
after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)
after_rag_chain = (
    {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()}
    | after_rag_prompt
    | model_local
    | StrOutputParser()
)
print(after_rag_chain.invoke(&quot;What are Home Remedy for Common Cold?&quot;))
</code></pre>
","0","Question"
"78215347","","<p>I'm working on <strong>EMNIST</strong> dataset and want to load it from PyTorch, but it returns a strange error as:</p>
<blockquote>
<p>RuntimeError: File not found or corrupted.</p>
</blockquote>
<p>Here's how i have tried to load the dataset:</p>
<pre><code>trainset = torchvision.datasets.EMNIST(root=&quot;emnist&quot;,
                                   split=&quot;letters&quot;,
                                   train=True,
                                   download=True,
                                   transform=transforms.ToTensor())
</code></pre>
<p>What might be wrong?</p>
","1","Question"
"78215783","","<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import category_encoders as ce

# Read the data
transactions_master_df = pd.read_csv('my_data.csv')

# Calculate the average house price for each district
avg_price_per_district = transactions_master_df.groupby('District')['Price'].mean().reset_index()
avg_price_per_district.rename(columns={'Price': 'AvgPrice'}, inplace=True)

#print the average price for each district with the district column next to it
print(avg_price_per_district)

# Merge the average price information with the original DataFrame
transactions_master_df = pd.merge(transactions_master_df, avg_price_per_district, on='District', how='left')

# Binary encode the 'District' feature
encoder = ce.BinaryEncoder(cols=['District'], base=6)
transactions_encoded = encoder.fit_transform(transactions_master_df)

# Concatenate additional features to the encoded DataFrame
additional_features = ['Building Age', 'Floor', 'Number of Floors', 'Elevator', 
                      'number of bathrooms', 'Otopark', 'steeped alley', 
                      'material used and luxuriness', 'view', 
                      'prestige of that district and its vicinity']

# Check if additional features are present in the transactions_encoded DataFrame
for feature in additional_features:
    if feature not in transactions_encoded.columns:
        print(f&quot;Warning: {feature} column not found in transactions_encoded DataFrame.&quot;)

# Concatenate additional features to the encoded DataFrame
final_features = pd.concat([transactions_encoded[['District_0', 'District_1', 'District_2', 'SquareMeter']], 
                            transactions_encoded[additional_features]], axis=1)

# Ensure 'final_features' contains the necessary columns for training
print(final_features.head())


</code></pre>
<p>hello,
In this code, I'm building a model for my housing Prices dataset. First I'm encoding some of my non-numeric features and then when I'm concatenating the rest of my features to receive the final_features variable, I get the following error:</p>
<pre><code>final_features = pd.concat([transactions_encoded[['District_0', 'District_1', 'District_2', 'SquareMeter']], 
---&gt; 38                             transactions_encoded[additional_features]], axis=1)
KeyError: &quot;['Building Age', 'Floor', 'Number of Floors'] not in index&quot;

</code></pre>
<p>weirdly, these features exist in my dataset but I don't know why it gives this error to me.</p>
","1","Question"
"78216115","","<p>training the sklearn sgd classifier. Based on name and age which are arrays, a color is obtained. b
Error with .fit() for sgdclassifier. Error: 'setting an array element with a sequence.' mean? Does this mean sgd classifier in sklearn cannot have arrays of arrays as inputs?</p>
<p>However no error if name and age are not arrays (and just single elements).</p>
<pre><code>from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np`

a=np.array([0, 2, 5, 2])
b=np.array( [0, 5, 0, 2])
c=np.array([2,2,0,0])
d=np.array([5,2,5,0])


age_a=np.array([5, 10, 7, 6])
age_b=np.array([3, 7, 11,8])
age_c=np.array([15, 10, 17, 2])
age_d=np.array([2, 8, 12,7])

color_a=np.array([0,2,1,1])
color_b=np.array([1,12,0,1])
color_c=np.array([0,1,1,0])
color_d=np.array([1,0,0,1])

#data2={'name':[a,b,c,d],'age':[age_a, age_b, age_c, age_d],'color':   [color_a,color_b,color_c,color_d]}

data2={'name':[a,b,c,d],'age':[age_a, age_b, age_c, age_d],'color':[0,1,0,1]}
new2 = pd.DataFrame.from_dict(data2)

print(new2)

x = new2.loc[:, new2.columns != 'color']
y = new2.loc[:, 'color']

x=np.array(x,dtype=object)
y=np.array(y)



x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)



from sklearn.linear_model import SGDClassifier

sgd_clf=SGDClassifier(random_state=42)
sgd_clf.fit(x_train, y_train)
sgd_clf.predict(x_test)`
</code></pre>
<p>`
TypeError                                 Traceback (most recent call last)
TypeError: float() argument must be a string or a real number, not 'list'</p>
<pre><code>The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Cell In[117], line 25
     21 print(y_test)
     24 clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;, max_iter=5)
     25 clf.fit(x_train, y_train)
     26 #SGDClassifier(max_iter=100)
     28 clf.predict([[2., 2.]])


ValueError: setting an array element with a sequence.
</code></pre>
","1","Question"
"78218276","","<p>I am trying to follow a tutorial whereby an ARIMA time series analysis using <em>differenced data</em> is being done:</p>
<p>The following is the python code:</p>
<pre><code>def difference(dataset):
    diff = list()
    for i in range(1, len(dataset)):
        value = dataset[i] - dataset[i - 1]
        diff.append(value)
    return Series(diff)

series = pd.read_csv('dataset.csv')
X = series.values  # The error in building the list can be seen here
X = X.astype('float32')
stationary = difference(X)
stationary.index = series.index[1:]
...
stationary.plot()
pyplot.show()
</code></pre>
<p>When the process reaches the plotting stage I get the error:</p>
<blockquote>
<p>TypeError: no numeric data to plot</p>
</blockquote>
<p>Tracing back, I find that the data that is being parsed is resulting in a collection of array. Saving the collection <strong>stationary</strong> as <code>*.csv</code> file gives me a list like:</p>
<pre><code>[11.]
[0.]
[16.]
[45.]
[27.]
[-141.]
[46.]
</code></pre>
<p>Can somebody tell me what is going wrong here?</p>
<p>PS. I have exluded the parts of import of libraries</p>
<p><strong>Edit 1</strong></p>
<p>A section of the dataset is reproduced below:</p>
<pre><code>Year,Obs
1994,21
1995,62
1996,56
1997,29
1998,38
1999,201
</code></pre>
","0","Question"
"78218890","","<p>I am trying to convert my ONNX model to Tensorflow Lite format. Simple code but getting this error. I updated my onnx version with no luck</p>
<pre><code>import onnx
import tensorflow as tf
import onnx_tf
#
#
# README: This file converts an onnx model to tflite
#
#
#

onnx_model_path = '/home/sfrye/segmentation/segmentation_checkpoints/efficientnet/modified-new.onnx'

onnx_model = onnx.load(onnx_model_path)

tf_model = onnx_tf.backend.prepare(onnx_model)
tf_model.export_graph(&quot;tflite_model.tf&quot;)
</code></pre>
<p>Here is the error</p>
<pre><code>
RuntimeError: in user code:

    File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/backend_tf_module.py&quot;, line 99, in __call__  *
        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,
    File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/backend.py&quot;, line 347, in _onnx_node_to_tensorflow_op  *
        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)
    File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/handlers/handler.py&quot;, line 58, in handle  *
        cls.args_check(node, **kwargs)
    File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/handlers/backend/resize.py&quot;, line 125, in args_check  *
        exception.OP_UNSUPPORTED_EXCEPT(
    File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/common/exception.py&quot;, line 50, in __call__  *
        raise self._func(self.get_message(op, framework))

    RuntimeError: Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow.

</code></pre>
<p>I tried updating my onnx as this fixed someones problem with this error code</p>
","0","Question"
"78219076","","<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Config, GPT2Model
from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)
print(decoder)
</code></pre>
<p>Here is the output of the console, listing the model architecture:</p>
<pre><code>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
</code></pre>
<p>I want to remove the first layer:</p>
<pre><code>(wte): Embedding(50257, 768)
</code></pre>
<p>I've tried the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model
    oldModuleList = model.bert.encoder.layer
    newModuleList = nn.ModuleList()

    # Now iterate over all layers, only keepign only the relevant layers.
    for i in range(0, len(num_layers_to_keep)):
        newModuleList.append(oldModuleList[i])

    # create a copy of the model, modify it with the new list, and return
    copyOfModel = copy.deepcopy(model)
    copyOfModel.bert.encoder.layer = newModuleList

    return copyOfModel
</code></pre>
<p>But it didn't work. Who knows how to fix it?</p>
","0","Question"
"78219696","","<p>I recently read about the “computer” built out of matchboxes designed by Donald Michie that could teach itself how to play tic tac toe. Here is the Wikipedia article about it:</p>
<p><a href=""https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine"" rel=""nofollow noreferrer"">https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine</a></p>
<p>I thought it looked interesting, so I decided to make a digital version in Python for fun and practice. It works well against random moves (I just ran it again for 5353 games on the data generated by ≈10,000 games and it won 4757 out of 5353) but it still looses often against me.</p>
<p>Here are some points that a perfect answer would address:</p>
<ul>
<li><p>How many games will it take for a “matchbox computer” that works exactly like the one Michie designed to start playing perfectly?</p>
</li>
<li><p>Did the original computer with actual matchboxes ever reach perfect
play?</p>
</li>
<li><p>Will the computer ever reach perfect play if trained against    only
random moves?</p>
</li>
</ul>
<p>EDIT:
This question is not asking for help with the code, but a comment below suggested that including the code could be a help. Here is a link to a GitHub repository I created so I could share it here:</p>
<p><a href=""https://github.com/ACertainArchangel/Recreation-Of-MENACE-Tic-Tac-Toe..git"" rel=""nofollow noreferrer"">https://github.com/ACertainArchangel/Recreation-Of-MENACE-Tic-Tac-Toe..git</a></p>
<p>Sorry, I know it's not very good and doesn't obey conventions; I've only been coding a couple of months :)</p>
","1","Question"
"78223936","","<pre><code>checkpoint = ModelCheckpoint(
    './base.model',
    monitor='val_accuracy',
    verbose=1,
    save_best_only=True,
    mode='max',
    save_weights_only=False,
    save_frequency=1
)
earlystop = EarlyStopping(
    monitor='val_loss',
    min_delta=0.001,
    patience=30,
    verbose=1,
    mode='auto'
)

opt1 = tf.keras.optimizers.Adam()

callbacks = [checkpoint,earlystop]
</code></pre>
<p>this isn't working on tensorflow 2.16.1
but, working on 2.15.0 on google colab</p>
<p>how can i fix my code or how can i install tensorflow 2.15.0 ?</p>
<p>i tried <code>pip install tensorflow=2.15.0</code>
but, it's showing error</p>
","0","Question"
"78224205","","<p>I have a tflite model that takes image as a input and predict its class. I want it to use in my unity Project. When I used the code given by chatgpt, following error occurs.can anyone help, i dont know much about unity and c#</p>
<p>Assets\Samples\Detection\Scripts\PythonBridge.cs(72,9): error CS0246: The type or namespace name ‘Tensor’ could not be found (are you missing a using directive or an assembly reference?)</p>
<p>Assets\Samples\Detection\Scripts\PythonBridge.cs(72,43): error CS0122: ‘Interpreter.GetOutputTensor(int)’ is inaccessible due to its protection level</p>
<pre><code>using UnityEngine;
using TensorFlowLite;
using System.IO;
using System.Collections.Generic;

public class ObjectDetection : MonoBehaviour
{
    [SerializeField]
    [FilePopup(&quot;*.tflite&quot;)]
    public string modelPath = &quot;model.tflite&quot;;


    [SerializeField]
    private TextAsset labelFile;

    [SerializeField]
    private Texture2D inputImage;

    private Interpreter interpreter;
    private List&lt;string&gt; labels;

    private const int IMAGE_SIZE = 224;
    private const int CHANNELS = 3;

    private void Start()
    {
        LoadModel();
        LoadLabels();
        PreprocessImage();
        RunInference();
    }

    private void LoadModel()
    {
        interpreter = new Interpreter(File.ReadAllBytes(modelPath));
    }

    private void LoadLabels()
    {
        labels = new List&lt;string&gt;();
        using (StringReader reader = new StringReader(labelFile.text))
        {
            string line;
            while ((line = reader.ReadLine()) != null)
            {
                labels.Add(line.Trim());
            }
        }
    }

    private void PreprocessImage()
    {
        Texture2D resizedImage = ResizeImage(inputImage, IMAGE_SIZE, IMAGE_SIZE);
        Color32[] pixels = resizedImage.GetPixels32();

        float[] imgArray = new float[IMAGE_SIZE * IMAGE_SIZE * CHANNELS];
        for (int i = 0; i &lt; pixels.Length; i++)
        {
            imgArray[i * 3] = pixels[i].r / 255.0f;
            imgArray[i * 3 + 1] = pixels[i].g / 255.0f;
            imgArray[i * 3 + 2] = pixels[i].b / 255.0f;
        }

        interpreter.SetInputTensorData(0, imgArray);
    }

    private void RunInference()
    {
        interpreter.Invoke();

        // Retrieve output and process predictions
        Tensor outputTensor = interpreter.GetOutputTensor(0);
        float[] results = outputTensor.Data&lt;float&gt;();

        // Find class with highest probability
        int maxIndex = 0;
        float maxProbability = 0f;
        for (int i = 0; i &lt; results.Length; i++)
        {
            if (results[i] &gt; maxProbability)
            {
                maxProbability = results[i];
                maxIndex = i;
            }
        }

        string predictedLabel = labels[maxIndex];
        Debug.Log(&quot;Predicted object: &quot; + predictedLabel);
    }

    private Texture2D ResizeImage(Texture2D source, int width, int height)
    {
        RenderTexture rt = RenderTexture.GetTemporary(width, height, 24);
        RenderTexture.active = rt;
        Graphics.Blit(source, rt);
        Texture2D result = new Texture2D(width, height);
        result.ReadPixels(new Rect(0, 0, width, height), 0, 0);
        result.Apply();
        RenderTexture.active = null;
        RenderTexture.ReleaseTemporary(rt);
        return result;
    }
}

</code></pre>
<p>I have tried to solve in chatgpt but it is not updated. I have use .h5 and python in c# and i got the output. but it is not working when exported as apk. So when searched, I saw that tensorflowlite will solve the issue</p>
","1","Question"
"78226139","","<p>Let's say I let a classification model classify a single object multiple times but under varying circumstances. Ideally it should predict the same class again and again. But in reality its class predictions may vary.</p>
<p>So given a sequence of class predictions for the single object, I'd like to measure how consistent the sequence is. To be clear, this is not about comparing predictions against some ground truth. This is about consistency within the prediction sequence itself.</p>
<ul>
<li>For instance, a perfectly consistent prediction sequence like <code>class_a, class_a, class_a, class_a</code> should get a perfect score.</li>
<li>A less consistent sequence like <code>class_a, class_b, class_a, class_c</code>
should get a lower score.</li>
<li>And a completely inconsistent sequence like
<code>class_a, class_b, class_c, class_d</code> should get the lowest score
possible.</li>
</ul>
<p>The goal is to find out on what objects we may need to keep training the classification model. If the classification model is not very consistent in its predictions for a certain object, then we might need to add that object to a dataset for further training.</p>
<p>Preferably it works for any number of possible classes and also takes into account prediction confidences. The sequence <code>class_a (0.9), class_b (0.9), class_a (0.9), class_c (0.9)</code> should give a lower score then <code>class_a (0.9), class_b (0.2), class_a (0.8), class_c (0.3)</code>, as it's no good when the predictions are inconsistent with high confidences.</p>
<p>I could build something myself, but I'd like to know if there's a standard sklearn or scipy (or similar) function for this? Thanks in advance!</p>
<p>The comment to <a href=""https://stackoverflow.com/questions/26406708/how-to-measure-the-consistence-of-a-sequence"">this question</a> suggests <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Spearman's correlation coefficient</a> or the <a href=""https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Kandell correlation coefficient</a>. I'll look into that as well.</p>
","0","Question"
"78228806","","<p>I successfully installed:</p>
<pre><code>python3 -m pip install gluoncv
</code></pre>
<p>Then I successfully installed:</p>
<pre><code>pip3 install virtualenv
</code></pre>
<p>But I encountered an issue <code>zsh: command not found: virtualenv</code> when I tried to install the following:</p>
<pre><code>virtualenv --python=/usr/bin/python2.7 python27
</code></pre>
<p>What remedy would you suggest? My goal is to instal PIP and use MLModels with Xcode. I'm a beginner programmer, so hand holding &amp; talking to me like I'm 5 year old is appreciated! Thanks!</p>
","0","Question"
"78229141","","<p>I'm having a difficult time phrasing my question, so if there's anything unclear that I can improve upon, please let me know. My goal is ultimately to determine the location of an RF transmitter using a machine learning model. There are many other techniques that could be used to identify the source of an RF signal (including triangulation and time offsets between multiple receivers), but that's not the point. I want to see if I can make this work with a ML model.</p>
<p>I'm attempting to use a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow noreferrer"">RandomForestClassifier</a> from <a href=""https://scikit-learn.org/stable/getting_started.html"" rel=""nofollow noreferrer"">scikit-learn</a> to build out a model for identifying the source of an RF signal, given the signal strength on several receivers scattered across a known area. These receivers are all linked (via network) to a central database. The receivers are in fixed locations, but the transmitter could be anywhere, so the signal strength into a receiver primarily depends on whether the transmitter has direct line of sight to the receiver. Receivers measure signal strength from 1 to 255. If it's a 0, it means the receiver didn't hear anything, so it's not recorded in the database. The fact that it won't be recorded will be important in a moment. An <code>rssi</code> of 255 is an indication of full scale into a particular receiver.</p>
<p>The database logs the receiver data every second (please see table below). Each group of <code>time</code> is a representation of what the signal looked like at each receiver at that time. As stated, if the signal wasn't heard on a receiver, it won't be logged into the database, so each group of <code>time</code> could have as few as 1 row, or as many as <code>X</code> rows, where <code>X</code> represents the total number of receivers in the system (e.g., if there are ten receivers listening on the same frequency and each receiver hears the signal, a row for all ten receivers will show up in the database, but if only three of those ten hear the signal, only three rows will be recorded in the database). Essentially, I'm trying to correlate what signal strengths look like in a database with known locations. For example, strong into <code>Red</code> and <code>Green</code> means the signal is likely coming from <code>Foo</code>, whereas strong signals into <code>Red</code> and <code>Yellow</code>, with a weak signal into <code>Blue</code> means the signal probably came from <code>Bar</code>. The known <code>location</code> data is built out manually by observing what a signal looks like when a transmitter is in a known location. It's a very tedious process.</p>
<p>The way the receiver data is logged (across multiple rows and never knowing how many rows will show up in the dataset) is causing an obvious challenge for me when I'm trying to model the data because the <code>RandomForestClassifier</code> looks at each row individually. I need the data to be grouped by date/time, but not knowing how many receivers are going to hear the signal at any given time makes it difficult for me to model the data in a more logical way. At least I haven't come up with any good ideas.</p>
<p>The table below contains a few seconds of signal data from a known location (<code>Region A</code>). Does anybody have any suggestions for how I could restructure this data to make it useful with the <code>RandomForestClassifier</code> from scikit-learn?</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Receiver Name</th>
<th>Time</th>
<th>RSSI</th>
<th>Location</th>
</tr>
</thead>
<tbody>
<tr>
<td>Red</td>
<td>2024-03-21 20:37:58</td>
<td>182</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:37:58</td>
<td>254</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:37:58</td>
<td>208</td>
<td>Region A</td>
</tr>
<tr>
<td>Red</td>
<td>2024-03-21 20:37:59</td>
<td>192</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:37:59</td>
<td>254</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:37:59</td>
<td>215</td>
<td>Region A</td>
</tr>
<tr>
<td>Red</td>
<td>2024-03-21 20:38:00</td>
<td>202</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:38:00</td>
<td>254</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:38:00</td>
<td>207</td>
<td>Region A</td>
</tr>
<tr>
<td>Yellow</td>
<td>2024-03-21 20:38:00</td>
<td>17</td>
<td>Region A</td>
</tr>
<tr>
<td>Red</td>
<td>2024-03-21 20:38:01</td>
<td>189</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:38:01</td>
<td>254</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:38:01</td>
<td>225</td>
<td>Region A</td>
</tr>
<tr>
<td>Yellow</td>
<td>2024-03-21 20:38:01</td>
<td>16</td>
<td>Region A</td>
</tr>
<tr>
<td>Red</td>
<td>2024-03-21 20:38:02</td>
<td>204</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:38:02</td>
<td>255</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:38:02</td>
<td>213</td>
<td>Region A</td>
</tr>
<tr>
<td>Yellow</td>
<td>2024-03-21 20:38:02</td>
<td>18</td>
<td>Region A</td>
</tr>
<tr>
<td>Red</td>
<td>2024-03-21 20:38:03</td>
<td>180</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:38:03</td>
<td>254</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:38:03</td>
<td>214</td>
<td>Region A</td>
</tr>
<tr>
<td>Yellow</td>
<td>2024-03-21 20:38:03</td>
<td>13</td>
<td>Region A</td>
</tr>
<tr>
<td>Red</td>
<td>2024-03-21 20:38:04</td>
<td>182</td>
<td>Region A</td>
</tr>
<tr>
<td>Blue</td>
<td>2024-03-21 20:38:04</td>
<td>254</td>
<td>Region A</td>
</tr>
<tr>
<td>Green</td>
<td>2024-03-21 20:38:04</td>
<td>213</td>
<td>Region A</td>
</tr>
<tr>
<td>Yellow</td>
<td>2024-03-21 20:38:04</td>
<td>12</td>
<td>Region A</td>
</tr>
</tbody>
</table></div>
<p>Below is the Python code I started with. It still looks at each row individually. I've also never worked with scikit-learn or Python, so I'm not confident anything below is correct:</p>
<pre><code>import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

data = pd.read_csv(&quot;data/combined.csv&quot;, header=0)

label_encoder = LabelEncoder()
print(data.columns)
data[&quot;name_encoded&quot;] = label_encoder.fit_transform(data[&quot;name&quot;])
data[&quot;location_encoded&quot;] = label_encoder.fit_transform(data[&quot;location&quot;])

x = data[[&quot;rssi&quot;, &quot;name_encoded&quot;]]  # Features (rssi and encoded name)
y = data[&quot;location_encoded&quot;]  # Target (encoded location)

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Create the Random Forest model
model = RandomForestClassifier(n_estimators=100)

# Train the model
model.fit(x_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(x_test)

# Decode predictions
location_decoder = LabelEncoder()

location_decoder.fit(data[&quot;location&quot;])  # Fit the decoder with original locations
predicted_locations = location_decoder.inverse_transform(y_pred)
print(&quot;Predicted locations:&quot;, predicted_locations)
</code></pre>
<p>Thank you in advance for any help.</p>
","0","Question"
"78230023","","<p>I am running notebook on sagemaker instance with ml.g4dn.xlarge instance.</p>
<p>I am trying to create tflite model as given below</p>
<h1>Save the model</h1>
<p>model.save(os.path.join(model_dir, 'model.h5'))</p>
<pre><code># Convert and save the model as TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True  # Enable MLIR-based conversion flow
converter.debug_info = True  # Enable the generation of a debug .mlir file
tflite_model = converter.convert()

with open(os.path.join(model_dir, 'model.tflite'), 'wb') as f:
    f.write(tflite_model)
</code></pre>
<p>I am getting an error</p>
<pre><code>Kernel Restarting
The kernel for Untitled2.ipynb appears to have died. It will restart automatically.
</code></pre>
<p>I check few so link where it is asking to upgrade the instance type. I upgraded it to ml.g4dn.xlarge but still its giving same error. Any suggestion what can be the cause?</p>
","2","Question"
"78234279","","<p>I'm encountering an issue while trying to evaluate various regression models in Python using scikit-learn. I have implemented a code to train and evaluate different algorithms, including LinearRegression, DecisionTreeRegressor, RandomForestRegressor, SVR, and MLPRegressor. However, when attempting to calculate classification metrics such as accuracy, precision, recall, and F1-score for these regression models, I'm encountering the following error:</p>
<blockquote>
<p>ValueError: Classification metrics can't handle a mix of binary and continuous targets</p>
</blockquote>
<p>From what I understand, this error occurs because I am attempting to use classification metrics on a regression problem. However, my dataset contains only binary outcome variables, so this shouldn't be an issue.</p>
<p>Here's a version of my code:</p>
<pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import sklearn.feature_selection
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor


# Find performance of model using preprocessed data
algorithms = {
    'LogisticRegression': {
        'constructor': LogisticRegression(max_iter=4),
        'predict': lambda m, x_test: m.predict(x_test),
        'predict_prob': lambda m, x_test: m.predict_proba(x_test)[::, 1],
    },
    'LinearRegression': {
        'constructor': LinearRegression(),
        'predict': lambda m, x_test: m.predict(x_test),
        'predict_prob': None,
    },
    'DecisionTreeRegressor': {
        'constructor': DecisionTreeRegressor(),
        'predict': lambda m, x_test: m.predict(x_test),
        'predict_prob': None,
    },
    'RandomForestRegressor': {
        'constructor': RandomForestRegressor(),
        'predict': lambda m, x_test: m.predict(x_test),
        'predict_prob': None,
    },
    'SVR': {
        'constructor': SVR(),
        'predict': lambda m, x_test: m.predict(x_test),
        'predict_prob': None,
    },
    'MLPRegressor': {
        'constructor': MLPRegressor(),
        'predict': lambda m, x_test: m.predict(x_test),
        'predict_prob': None,
    },
}

def ds_split(dataset, dependent_var, split):
    # Split into features and outcomes
    x = dataset.drop(dependent_var, axis=1)
    y = dataset.get(dependent_var)

    # Split data into train and test sets
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=split, random_state=1)

    # Select the best features
    select = sklearn.feature_selection.SelectKBest(k=&quot;all&quot;)
    selected_features = select.fit(x_train, y_train)
    indices_selected = selected_features.get_support(indices=True)
    colnames_selected = [x.columns[i] for i in indices_selected]

    x_train_selected = x_train[colnames_selected]
    x_test_selected = x_test[colnames_selected]

    # Split data into train and test sets
    return x_train_selected, x_test_selected, y_train, y_test


datasets = {
    'processed': ds_split(undersampled_df, 'hospital_expire_flag', 0.7)}

models = [
    {
        'algo': 'LogisticRegression',
        'ds': 'processed',
    },
    {
        'algo': 'LinearRegression',
        'ds': 'processed',
    },
    {
        'algo': 'DecisionTreeRegressor',
        'ds': 'processed',
    },
    {
        'algo': 'RandomForestRegressor',
        'ds': 'processed',
    },
    {
        'algo': 'SVR',
        'ds': 'processed',
    },
    {
        'algo': 'MLPRegressor',
        'ds': 'processed',
    },
]

</code></pre>
<pre><code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle # For saving the model to a file
from sklearn import metrics


def evaluate_model(algo, x_train, y_train, x_test, y_test, model_filename):
    # Fit the model
    model = algo['constructor']
    model.fit(x_train, y_train)

    #Save model
    with open(model_filename, 'wb') as file:
        pickle.dump(model, file)


    # Predict Test Data
    y_pred = algo['predict'](model, x_test)

    # Calculate accuracy, precision, recall, f1-score, and kappa score
    acc = metrics.accuracy_score(y_test, y_pred)
    prec = metrics.precision_score(y_test, y_pred)
    rec = metrics.recall_score(y_test, y_pred)
    f1 = metrics.f1_score(y_test, y_pred)
    kappa = metrics.cohen_kappa_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)


    # Calculate area under curve (AUC)
    y_pred_proba = algo['predict_prob'](model, x_test)
    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
    auc = metrics.roc_auc_score(y_test, y_pred_proba)

    # Display confussion matrix
    cm = metrics.confusion_matrix(y_test, y_pred)

    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa,
            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm, 'mse': mse, 'rmse': rmse, 'mae': mae}


for model in models:
    x_train, x_test, y_train, y_test = datasets[model['ds']]
    
    # Generate model filename based on algorithm
    model_name = model['algo'] 
    model_filename = f&quot;{model_name}_model.pickle&quot;
    model['metrics'] = evaluate_model(algorithms[model['algo']], x_train, y_train, x_test, y_test, model_filename)

    model_acc = model['metrics']['acc']
    model_prec = model['metrics']['prec']
    model_rec = model['metrics']['rec']
    model_f1 = model['metrics']['f1']
    model_kappa = model['metrics']['kappa']
    model_auc = model['metrics']['auc']
    
    print(f'Algorithm: {model_name} | accuracy: {model_acc} | precision: {model_prec} | rec: {model_rec} | f1: {model_f1} | kappa: {model_kappa} | auc: {model_auc})

</code></pre>
<p>I've checked and all features are binary:</p>
<pre><code>binary_columns = []
solo = []
non_binary_columns = []

for col in undersampled_df.columns:
    if undersampled_df[col].nunique() == 2:
        binary_columns.append(col)
    if undersampled_df[col].nunique() == 1:
        solo.append(col)
    if undersampled_df[col].nunique() &gt; 2:
        non_binary_columns.append(col)

print(&quot;Binary Columns:&quot;)
print(binary_columns)
print(&quot;\n Non Binary Columns:&quot;)
print(non_binary_columns)
print(&quot;\n Solo Columns:&quot;)
print(solo)
</code></pre>
<pre><code>Binary Columns:
['hospital_expire_flag', 'gender_F', 'gender_M', 'age_-65', 'age_+65', 'PAPs', 'PAPd', 'PAPm', 'SvO2 SQI', 'HR_65-89', 'HR_90-99', 'HR_+100', 'ABPs_0-119', 'ABPs_+120', 'ABPd_0-79', 'PO2 (Arterial)_0-74', 'PO2 (Arterial)_75-99', 'PO2 (Arterial)_+100', 'SaO2_0-94', 'SaO2_95-100', 'PCO2 (Arterial)_0-34', 'PCO2 (Arterial)_35-45', 'PCO2 (Arterial)_+45', 'PH (Venous)_0-7.30', 'PH (Venous)_7.31-7.41', 'PH (Venous)_7.41+', 'HCO3 (serum)_0-22', 'HCO3 (serum)_23-29', 'Temperature F_-97', 'Temperature F_97-99', 'Temperature F_+99', 'Creatinine (serum)_-0.74', 'Creatinine (serum)_0.74-1.35', 'Creatinine (serum)_+1.35', 'Total Bilirubin_0.1-1.2', 'Total Bilirubin_+1.2', 'Heart Rhythm_AF (Atrial Fibrillation)', 'Heart Rhythm_SR (Sinus Rhythm)', 'Heart Rhythm_ST (Sinus Tachycardia) ', 'Skin Temp_Cool', 'Skin Temp_Warm']

CNon Binary Columns:
[]

Solo Columns:
[]
</code></pre>
<p>Could someone please advise me on how to correct this issue?</p>
<p>I don't what I can do in order to solve this problem.</p>
","0","Question"
"78234289","","<p>I'm encountering an error while running my Python code and need assistance in resolving it. Below are the details of the :</p>
<pre><code>import pandas as pd

df_list = []
file_path = 'houses.txt'

for chunk in pd.read_csv(file_path, chunksize=1000000, names=['Size()sqft', 'No of bedrooms', 'No of floors', 'Age of home', 'Price(1000s dollar)']):
    df_list.append(chunk)

df = pd.concat(df_list)

print(df_list)

</code></pre>
<p>Output :</p>
<pre><code>0        952.0             2.0           1.0         65.0                271.5
1       1244.0             3.0           1.0         64.0                300.0
2       1947.0             3.0           2.0         17.0                509.8
3       1725.0             3.0           2.0         42.0                394.0
4       1959.0             3.0           2.0         15.0                540.0
..         ...             ...           ...          ...                  ...
95      1224.0             2.0           2.0         12.0                329.0
96      1432.0             2.0           1.0         43.0                388.0
97      1660.0             3.0           2.0         19.0                390.0
98      1212.0             3.0           1.0         20.0                356.0
99      1050.0             2.0           1.0         65.0                257.8

[100 rows x 5 columns]]
</code></pre>
<p>After removing 'chunksize'. I get this error:</p>
<p><code>TypeError: cannot concatenate object of type '&lt;class 'str'&gt;'; only Series and DataFrame objs are valid</code></p>
<p>Kindly explain what's the issue</p>
","0","Question"
"78234777","","<p>I am just getting into machine learning and I am working on using classification models. Currently I am using a mushroom classification dataset (class is poisonous or edible). The issue is that, while I am following literally the most basic possible procedure I see everyone else do, my model only returns a perfect classification. This is the code I am using to create my model.</p>
<pre class=""lang-py prettyprint-override""><code>model = KNeighborsClassifier(n_neighbors = 5)
    model.fit(X_train, y_train)
    y_preds = model.predict(X_test)
    score = accuracy_score(y_test, y_preds)
</code></pre>
<p>This returns an accuracy score of 1.0, a confusion matrix showing no confusion at all (100% correctly predicted values), and this doesn't change if I change the k number or the test size. Even setting it to 50% came back the same.</p>
<p>I have cleaned the data to the best of my abilities and the data is entirely one-hot encoded. I think maybe this is playing into affect but I am not sure. Below is the code I used to prep the data. First I filled the missing values, then encoded ordinal data. Any input is appreciated!</p>
<pre class=""lang-py prettyprint-override""><code>
    qmarks = df.loc[df['Stalk Root'].str.contains('\?')] # nan values are ? here
mode = df['Stalk Root'].mode() #most common answer is b
df_enc = df.replace('?', 'b') #replace all question marks with most common value

df_enc['Ring Number'] = df_enc['Ring Number'].replace({'n': 0, 'o': 1, 't': 2}).astype(int)
df_enc['Gill Spacing'] = df_enc['Gill Spacing'].replace({'c': 0, 'w': 1, 'd': 2}).astype(int)
df['Poisonous'] = (df['Poisonous'] == 'p').astype(int)
df_enc = pd.get_dummies(df)

</code></pre>
<p>And then I split the data as shown:</p>
<pre class=""lang-py prettyprint-override""><code>
    y= df.iloc[:, 0:1]
    X= df.iloc[:, 2:-1]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle= True)


</code></pre>
<p>I have tried changing a lot of the variables but this isn't the first dataset that has done this to me. It happened with a linear regression model on a different dataset a while back and I couldn't figure out what was wrong there either. I can imagine that the data encoding, the train test split, or user error is at fault but I don't know how to go about fixing it. I am sure however that the data is split accurately, is not the same in both the train and test splits, and the dataset has a relatively even distribution of each class. Please help!</p>
<p>Edit: Added data prep code</p>
","1","Question"
"78234946","","<p>I did sort the values. But the problem is 'до 25' (up to 25). How can i change it into '0-25' and calculate correlation coefficient of age group and overall rating.</p>
<p>Some of my data is below</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Age group</th>
<th>Overall rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>65 and older</td>
<td>38.45</td>
</tr>
<tr>
<td>55-64</td>
<td>17.66</td>
</tr>
<tr>
<td>up to 25</td>
<td>46.56</td>
</tr>
<tr>
<td>45-54</td>
<td>24.95</td>
</tr>
<tr>
<td>35-44</td>
<td>33.54</td>
</tr>
<tr>
<td>25-34</td>
<td>37.21</td>
</tr>
</tbody>
</table></div>
","0","Question"
"78239906","","<p>I am working on school project that requires me to perform manual quantization of each layer of a model. Specifically, I want to implement manually:</p>
<blockquote>
<p>Quantized activation, combined with quantized weight A - layer A -
quantized output - dequantized output - requantized output, combined
with quantized weight B - layer B - ...</p>
</blockquote>
<p>I know Pytorch already have a quantization function, but that function is limited to int8. I would like to perform quantization from bit = 16 to bit = 2, and then compare their accuracy.</p>
<p>The issue I encountered is that after quantization, the output of a layer is multi-magnitude larger (with bit = 16), and I don't know how to dequantize it back. I am performing the quantization with the same min and max of both activation and weight. So here is an example:</p>
<pre><code>Activation = [1,2,3,4]
Weight = [5,6,7,8]
Min and max across activation and weight = 1, 8
Expected, non-quantized output = 70

Quantize with bit = 16
Quantized activation = [-32768, -23406, -14044, -4681]
Quantized weight = [4681, 14043, 23405, 32767]
Quantized output = -964159613
Dequantize output with min = 1, max = 8 = -102980
</code></pre>
<p>The calculation makes sense to me, because the output involves multiplying activations and weigths, their magnitude increase is also multiplied together. If I perform dequantization once with the original min and max, it is reasonable to have a much larger output.</p>
<p>How does Pytorch handle dequantization? I attempted to locate the quantization of Pytorch, but I could not locate it. How to dequantize the output?</p>
","1","Question"
"78240714","","<p>I'm using the <a href=""https://pypi.org/project/MAPIE/"" rel=""nofollow noreferrer"">MAPIE</a> Python library for conformal predictions. In the <code>MapieClassifier</code> I choose <code>method='score'</code>. When alpha is small (between 1% and 5%) I get non empty prediction sets. However when I look at a full range of alphas from 1% to 99% (e.g. <code>alphas = np.arange(0.01,1.00,0.01)</code>) the average number of classes in the prediction sets gradually go down to zero, despite the fact that in C. Molnar's book <em>Introduction To Conformal Prediction With Python (2023)</em> p.27 it is stated that « <em>A prediction set – for multi-class tasks – is a set of one or more classes.</em> », i.e. it's a non empty subset of the set of target modalities. I used a default XGBoost model on two classification datasets and got this phenomenon of empty prediction sets with big alphas, one of which was the <a href=""https://www.kaggle.com/datasets/sansuthi/dry-bean-dataset"" rel=""nofollow noreferrer"">Dry Bean</a> dataset mentioned in Molnar's book.</p>
<p>Here's a graph of the average cardinality of the prediction sets as a function of alpha.</p>
<p><a href=""https://i.sstatic.net/ZlyJu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZlyJu.png"" alt=""MAPIE 'score' method"" /></a></p>
<p>As we observe the average eventually go below one.</p>
<p>I also tried the code in Molnar's book at p.25-28 which doesn't explicitly use MAPIE but does a similar computation and got the same result that the prediction sets are eventually empty when alpha go bigger. I expected non empty prediction sets as mentioned at p.27 of Molnar's book. I observe that when alpha go bigger, q_level goes down which in return makes q_hat smaller. It seems like q_hat as a function of q_level goes down very quickly when q_level is between 0.7 and 0.9 (see the graph below).</p>
<p><a href=""https://i.sstatic.net/tRIV0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tRIV0.png"" alt=""q_hat as a function of q_level"" /></a></p>
<p><strong>Questions :</strong></p>
<ol>
<li>Is it something to be expected that the prediction sets get empty as alpha go bigger?</li>
<li>Should I change <code>method='score'</code> to another method to get non empty prediction sets?</li>
<li>Is there a <em>good practice</em> about MAPIE that I'm missing? Maybe one should avoid using big alphas, or using the method <code>score</code>, or something else. Intuitively, it seems to me that I should always get a non empty prediction set.</li>
</ol>
<hr />
<p><strong>2024-04-05 Update</strong> :</p>
<p>So I dug a bit deeper in Molnar's book :</p>
<ul>
<li>p.50 : « To get the coverage down to 95%, MAPIE would have to start artificially generating empty prediction sets. It was a design decision of the MAPIE developers not to produce empty sets. Empty sets are difficult to interpret. »</li>
<li>p.96 : « First of all, the prediction sets will get smaller. But what if the prediction sets had to get smaller than size 1 to achieve 1−𝛼 coverage? Theoretically, the algorithm might have to produce empty sets to ensure exactly 1−𝛼 coverage. In MAPIE, this doesn't happen, so it could be that the coverage is larger than 1−𝛼 and doesn't change as you increase 𝛼. A design choice. »</li>
<li>p.96 : « But as 𝛼 gets larger, more and more of the uncertain predictions switch to empty sets. »</li>
</ul>
<p>So there is here a mention that predictions can indeed be empty but that MAPIE by design doesn't produce that, which is wrong because the default MAPIE method is 'lac' (i.e. 'score') which does produce empty sets when 𝛼 gets bigger.</p>
<p>These empty sets were mentioned in the <a href=""https://arxiv.org/abs/1609.00451"" rel=""nofollow noreferrer"">original paper</a> that defined the 'score' method :</p>
<ul>
<li>p.1 : « The optimal classifier can sometimes output the empty set, but we provide two solutions to fix this issue that are suitable for various practical needs. »</li>
<li>p.3 : « A potentially undesirable property of the optimal classifiers is that they may lead to empty predictions, that is, H(x)=∅ for some points x∈X, especially when the required coverage is low. »</li>
</ul>
<p>In the documentation of MAPIE too it is said that the method 'score' (i.e. 'lac') can produce empty sets. It seems like the best solution right now is simply to change to another method, which I tried and it does solve the issue of empty sets, as shown in these figures :</p>
<p><a href=""https://i.sstatic.net/rhTEC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rhTEC.png"" alt=""MAPIE 'aps' method"" /></a></p>
<p><a href=""https://i.sstatic.net/kTvH0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kTvH0.png"" alt=""MAPIE 'raps' method"" /></a></p>
<p><a href=""https://i.sstatic.net/0SIP9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0SIP9.png"" alt=""MAPIE 'top_k' method"" /></a></p>
<p><strong>Conclusion :</strong> The 'score' method does produce empty prediction sets when 𝛼 gets bigger and I believe that the statements in Molnar's book about the fact that MAPIE by design does not generate empty sets is wrong.</p>
","1","Question"
"78241816","","<p>I have successfully installed the following:</p>
<pre><code>tensorflow (latest version 2.16.1)
keras (latest version 3.1.1
</code></pre>
<p>I am using pycharm 2023.3.5 (community Edition). I have some lines of codes with  imports including tensorflow:</p>
<pre><code>...
from tensorflow.keras import backend as K
...
</code></pre>
<p>Whenever, I debug my code, I get an error as seen below:</p>
<pre><code>Traceback (most recent call last):
File &quot;C:\Program Files\JetBrains\PyCharm Community Edition 2023.3.5\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_xml.py&quot;, line 177, in _get_type
if isinstance(o, t[0]):
   ^^^^^^^^^^^^^^^^^^^
File &quot;C:\Program Files\Python312\Lib\site-packages\tensorflow\python\platform\flags.py&quot;, line 73, in __getattribute__
return self.__dict__['__wrapped'].__getattribute__(name)
       ~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: '__wrapped'
</code></pre>
<p>I want to believe the problem is not caused by tensorflow but I can't seem to figure out the exact problem. I have surfed the internet but to no avail. The closest solution I got was this <a href=""https://stackoverflow.com/questions/45486046/pycharm-run-debug-not-working-with-tensorflow"">question</a>, however, it appeared to me as a different problem. Please, learned people of this noble platform, come my aid.</p>
","1","Question"
"78242480","","<p>I am trying to solve a ML problem if a person will deliver an order or not. Highly Imbalance dataset. Here is the glimpse of my dataset</p>
<pre><code>[{'order_id': '1bjhtj', 'Delivery Guy': 'John', 'Target': 0},
 {'order_id': '1aec', 'Delivery Guy': 'John', 'Target': 0},
 {'order_id': '1cgfd', 'Delivery Guy': 'John', 'Target': 0},
 {'order_id': '1bceg', 'Delivery Guy': 'Tom', 'Target': 0},
 {'order_id': '1a2fg', 'Delivery Guy': 'Tom', 'Target': 0},
 {'order_id': '1cbsf', 'Delivery Guy': 'Tom', 'Target': 1},
 {'order_id': '1bc5', 'Delivery Guy': 'Jay', 'Target': 0},
 {'order_id': '1a22', 'Delivery Guy': 'Jay', 'Target': 0},
 {'order_id': '1bzc5', 'Delivery Guy': 'Jay', 'Target': 0},
 {'order_id': '1av22', 'Delivery Guy': 'Jay', 'Target': 0},
 {'order_id': '1bsc5', 'Delivery Guy': 'Jay', 'Target': 1},
 {'order_id': '1a2t2', 'Delivery Guy': 'Jay', 'Target': 0},
 {'order_id': '1bc5b', 'Delivery Guy': 'Jay', 'Target': 0},
 {'order_id': '1a22a', 'Delivery Guy': 'Mary', 'Target': 0},
 {'order_id': '1c5bv', 'Delivery Guy': 'Mary', 'Target': 0},
 {'order_id': 'vb2er', 'Delivery Guy': 'Mary', 'Target': 0},
 {'order_id': '1bs5s', 'Delivery Guy': 'Mary', 'Target': 0},
 {'order_id': '1a22n', 'Delivery Guy': 'Mary', 'Target': 0},
 {'order_id': '122a', 'Delivery Guy': 'James', 'Target': 1},
 {'order_id': '1cw5bv', 'Delivery Guy': 'James', 'Target': 0},
 {'order_id': 'vb=er', 'Delivery Guy': 'James', 'Target': 0},
 {'order_id': '1b5s', 'Delivery Guy': 'James', 'Target': 0},
 {'order_id': '1a2n', 'Delivery Guy': 'James', 'Target': 1}]


</code></pre>
<p>This is my table :</p>
<pre><code>| order_id | Delivery Guy | Target |
|----------|--------------|--------|
| 1bjhtj   | John         | 0      |
| 1aec     | John         | 0      |
| 1cgfd    | John         | 0      |
| 1bceg    | Tom          | 0      |
| 1a2fg    | Tom          | 0      |
| 1cbsf    | Tom          | 1      |
| 1bc5     | Jay          | 0      |
| 1a22     | Jay          | 0      |
| 1bzc5    | Jay          | 0      |
| 1av22    | Jay          | 0      |
| 1bsc5    | Jay          | 1      |
| 1a2t2    | Jay          | 0      |
| 1bc5b    | Jay          | 0      |
| 1a22a    | Mary         | 0      |
| 1c5bv    | Mary         | 0      |
| vb2er    | Mary         | 0      |
| 1bs5s    | Mary         | 0      |
| 1a22n    | Mary         | 0      |
| 122a     | James        | 1      |
| 1cw5bv   | James        | 0      |
| vb=er    | James        | 0      |
| 1b5s     | James        | 0      |
| 1a2n     | James        | 1      |

</code></pre>
<p>I want my machine learning model to understand each person attributes and predict these two</p>
<p>cases:
will deliver &quot;0&quot; and
will not deliver &quot;1&quot;</p>
<p>I want to split my train and test in such a way that it should preserver few rows of name and few rows of Target class so that it learns all the patterns.</p>
<p>I have used this so far</p>
<pre><code>X = df.drop(columns = &quot;Target&quot;)
y = df.Target
X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,stratify=y)
</code></pre>
<p>It does give me output of each Delivery Guy but it misses the part where we can split 'James' in such way that &quot;1&quot; will be there in train another &quot;1&quot; will be in test.
Could anyone help me approach this problem in different way.</p>
","0","Question"
"78243492","","<p>I have followed a tutorial on youtube and the tutorial shows me how to classify 2 datasets (cough, not cough), but now I need to add an extra class which is sneeze, so there are 3 classes that need to be trained on (cough, sneeze, other), and I have no idea how to do this. PLEASE HELP!!!</p>
<p>In the code, the model is training on 2 classes (cough, not_cough) and performs quite good, but I can't get it work on multiple classes like (cough,sneeze, other).</p>
<pre><code>import os
from matplotlib import pyplot as plt
import tensorflow as tf 
import tensorflow_io as tfio
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout,TimeDistributed, Reshape
from tensorflow.keras.optimizers.legacy import Adam
from keras import layers
from keras.utils import to_categorical

def load_wav_16k_mono(filename):
    # Load encoded wav file
    file_contents = tf.io.read_file(filename)
    # Decode wav (tensors by channels) 
    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)
    # Removes trailing axis
    wav = tf.squeeze(wav, axis=-1)
    sample_rate = tf.cast(sample_rate, dtype=tf.int64)
    # Goes from 44100Hz to 16000hz - amplitude of the audio signal
    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)
    return wav

def preprocess(file_path, label): 
    wav = load_wav_16k_mono(file_path)
    wav = wav[:8000]
    zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    spectrogram = tf.signal.stft(wav, frame_length=100, frame_step=20)
    spectrogram = tf.abs(spectrogram)
    spectrogram = tf.expand_dims(spectrogram, axis=2)
    return spectrogram, label


def get_CNN(input_shape):
    model = Sequential()
    model.add(Conv2D(16, (3,3), activation='relu', input_shape=input_shape))
    model.add(Conv2D(16, (3,3), activation='relu'))
    model.add(MaxPool2D((2,2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(1, activation='softmax'))
    
    model.compile('Adam', loss='BinaryCrossentropy', metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),'accuracy'])
    model.summary() # drop in some max pool layers to reduce params
    return model
    

def main():
    POS_COUGH = &quot;./data/cough&quot;
    NEG_COUGH = &quot;./data/not_cough&quot;
  
    #POS_SPEECH = &quot;./data/speech&quot;

    pos_cough = tf.data.Dataset.list_files(POS_COUGH+'\*.wav')
    neg_cough = tf.data.Dataset.list_files(NEG_COUGH+'\*.wav')
    
    #pos_speech = tf.data.Dataset.list_files(POS_SPEECH +'\*.wav')

    cough_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(pos_cough)))  
    
    not_cough_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(neg_cough))) 
    
    # Add labels and Combine Positive and Negative Samples
    cough = tf.data.Dataset.zip((pos_cough, cough_labels))
    
    not_cough = tf.data.Dataset.zip((neg_cough, not_cough_labels))
   
    negatives = not_cough
    positives = cough
    # join both sameples 
    data = positives.concatenate(negatives)

    ### 2. Create a Tensorflow Data Pipeline
    data = data.map(preprocess)
    data = data.cache()
    data = data.shuffle(buffer_size=1000)
    data = data.batch(16)
    data = data.prefetch(8)
    
    ## 3. Split data into train and test data
    train = data.take(int(len(data) * 0.7))
    test = data.skip(int(len(data) * 0.7)).take(int(len(data) - len(data) * 0.7))   #test.as_numpy_iterator().next()

    input_shape_spectrogram = (396, 65,1)
    model = get_CNN(input_shape_spectrogram)
    hist = model.fit(train, epochs=2, validation_data=test)
</code></pre>
","1","Question"
"78244042","","<p>I developed seven different hybrid ML models using metaheuristic algorithms and ANN. Interestingly, the coefficient of determination values for most of these models are higher in the testing phase compared to the training phase. This discrepancy raises the question: what could be the reason behind this phenomenon?
please put reference for your words, if possible.</p>
","0","Question"
"78245568","","<p>I have following model which forms one of the step in my overall model pipeline:</p>
<pre><code>import torch
import torch.nn as nn

class NPB(nn.Module):
    def __init__(self, d, nhead, num_layers, dropout=0.1):
        super(NPB, self).__init__()
            
        self.te = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            num_layers=num_layers,
        ) 

        self.t_emb = nn.Parameter(torch.randn(1, d))
        
        self.L = nn.Parameter(torch.randn(1, d)) 

        self.td = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            num_layers=num_layers,
        ) 

        self.ffn = nn.Linear(d, 6)
    
    def forward(self, t_v, t_i):
        print(&quot;--------------- t_v, t_i -----------------&quot;)
        print('t_v: ', tuple(t_v.shape))
        print('t_i: ', tuple(t_i.shape))

        print(&quot;--------------- t_v + t_i + t_emb -----------------&quot;)
        _x = t_v + t_i + self.t_emb
        print(tuple(_x.shape))

        print(&quot;--------------- te ---------------&quot;)
        _x = self.te(_x)
        print(tuple(_x.shape))
        
        print(&quot;--------------- td ---------------&quot;)
        _x = self.td(self.L, _x)
        print(tuple(_x.shape))

        print(&quot;--------------- ffn ---------------&quot;)
        _x = self.ffn(_x)
        print(tuple(_x.shape))

        return _x
</code></pre>
<p>Here <code>t_v</code> and <code>t_i</code> are inputs from earlier encoder blocks. I pass them as shape of <code>(4,256)</code>, where <code>256</code> is number of features and <code>4</code> is batch size. <code>t_emb</code> is temporal embedding.  <code>L</code> represents learned matrix representing the embedding of the query. I tested this module block with following code:</p>
<pre><code>t_v = torch.randn((4,256))
t_i = torch.randn((4,256))
npb = NPB(d=256, nhead=8, num_layers=2)
npb(t_v, t_i)
</code></pre>
<p>It outputted:</p>
<pre><code>=============== NPB ===============
--------------- t_v, t_i -----------------
t_v:  (4, 256)
t_i:  (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- te ---------------
(4, 256)
--------------- td ---------------
(1, 256)
--------------- ffn ---------------
(1, 6)
</code></pre>
<p>I was expecting the output should be of shape <code>(4,6)</code>, 6 values for each sample in the batch of size <code>6</code>. But the output was of size <code>(1,6)</code>. After a lot of tweaking, I tried changing <code>t_emb</code> and <code>L</code> shape from <code>(1,d)</code> to <code>(4,d)</code>, since I did not wanted all sampled to share these variables (through broadcasting:</p>
<pre><code>self.t_emb = nn.Parameter(torch.randn(4, d)) # [n, d] = [4, 256]     
self.L = nn.Parameter(torch.randn(4, d)) 
</code></pre>
<p>This gives desired output of shape (4,6:</p>
<pre><code>--------------- t_v, t_i -----------------
t_v:  (4, 256)
t_i:  (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- te ---------------
(4, 256)
--------------- td ---------------
(4, 256)
--------------- ffn ---------------
(4, 6)
</code></pre>
<p>I have following doubts:</p>
<p><strong>Q1.</strong> Exactly why changing <code>L</code> and <code>t_emb</code> shape from <code>(1,d)</code> to <code>(4,d)</code> worked? Why it did not work with <code>(1,d)</code> through broadcasting?<br />
<strong>Q2.</strong> Am I doing batching right way or the output is artificially correct while under the hood its doing something different than what I am expecting (predicting 6 values for each sample in the batch of size 4)?</p>
","0","Question"
"78246736","","<p>I am trying to re-write the source code of <a href=""https://github.com/scikit-learn/scikit-learn/blob/f07e0138b/sklearn/inspection/_permutation_importance.py#L77"" rel=""nofollow noreferrer"">scikit-learn permutation importance</a> to achieve:</p>
<ol>
<li>Compatibility with Polars</li>
<li>Compatibility with clusters of features</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
import polars.selectors as cs
import numpy as np

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=3,
    n_redundant=0,
    n_repeated=0,
    n_classes=2,
    random_state=42,
    shuffle=False,
)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
feature_names = [f&quot;feature_{i}&quot; for i in range(X.shape[1])]

X_train_polars = pl.DataFrame(X_train, schema=feature_names)
X_test_polars = pl.DataFrame(X_test, schema=feature_names)
y_train_polars = pl.Series(y_train, schema=[&quot;target&quot;])
y_test_polars = pl.Series(y_test, schema=[&quot;target&quot;])
</code></pre>
<p>To get future importances for a cluster of feature, we need to permutate a cluster of features simutiousnly then pass into the scorer to compare with the baseline score.</p>
<p>However, I am struglling to <strong>replace multiple polars dataframe columns</strong> in case of examining clusters of features:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.utils import check_random_state
random_state = check_random_state(42)
random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

X_train_permuted = X_train_polars.clone()
shuffle_arr = np.array(X_train_permuted[:, [&quot;feature_0&quot;, &quot;feature_1&quot;]])
</code></pre>
<pre class=""lang-py prettyprint-override""><code>random_state.shuffle(shuffle_arr)
X_train_permuted.replace_column( # This operation is in place
                0, 
                pl.Series(name=&quot;feature_0&quot;, values=shuffle_arr))
</code></pre>
<p>Normally the <code>shuffle_arr</code> would have a shape of (n_samples,) which can easily replace assosicated column in polars dataframe using <code>polars.DataFrame.replace_column()</code>. In this case, <code>shuffle_arr</code> has multi-dimensional shape of (n_samples, n_features in a cluster). What would be an efficient way to replace the assosicated columns?</p>
","0","Question"
"78249695","","<p>I would like to convert a <code>flax.linen.Module</code>, taken from <a href=""https://colab.research.google.com/drive/1SeXMpILhkJPjXUaesvzEhc3Ke6Zl_zxJ?usp=sharing"" rel=""nofollow noreferrer"">here</a> and replicated below this post, to a <code>torch.nn.Module</code>.</p>
<p>However, I find it extremely hard to figure out how I need to replace</p>
<ol>
<li>The <code>flax.linen.Dense</code> calls;</li>
<li>The <code>flax.linen.Conv</code> calls;</li>
<li>The custom class <code>Dense</code>.</li>
</ol>
<p>For (1.), I guess I need to use <code>torch.nn.Linear</code>. But what do I need to specify as <code>in_features</code> and <code>out_features</code>?</p>
<p>For (2.), I guess I need to use <code>torch.nn.Conv2d</code>. But, again, what do I need to specify as <code>in_channels</code> and <code>out_channels</code>.</p>
<p>I guess I know how I can port the <code>GaussianFourierProjection</code> class and how I can mimic the &quot;swish activation function&quot;. Obviously, it would be extremely helpful if someone is that familiar with both modules so that he/she can provide the corresponding <code>torch.nn.Module</code> as an answer. But it would also already be helpful, if someone could at least answer how (1.) - (3.) need to be replaced. Any help is highly appreciated!</p>
<hr />
<pre><code>#@title Defining a time-dependent score-based model (double click to expand or collapse)

import jax.numpy as jnp
import numpy as np
import flax
import flax.linen as nn
from typing import Any, Tuple
import functools
import jax

class GaussianFourierProjection(nn.Module):
  &quot;&quot;&quot;Gaussian random features for encoding time steps.&quot;&quot;&quot;  
  embed_dim: int
  scale: float = 30.
  @nn.compact
  def __call__(self, x):    
    # Randomly sample weights during initialization. These weights are fixed 
    # during optimization and are not trainable.
    W = self.param('W', jax.nn.initializers.normal(stddev=self.scale), 
                 (self.embed_dim // 2, ))
    W = jax.lax.stop_gradient(W)
    x_proj = x[:, None] * W[None, :] * 2 * jnp.pi
    return jnp.concatenate([jnp.sin(x_proj), jnp.cos(x_proj)], axis=-1)


class Dense(nn.Module):
  &quot;&quot;&quot;A fully connected layer that reshapes outputs to feature maps.&quot;&quot;&quot;  
  output_dim: int  
  
  @nn.compact
  def __call__(self, x):
    return nn.Dense(self.output_dim)(x)[:, None, None, :]    


class ScoreNet(nn.Module):
  &quot;&quot;&quot;A time-dependent score-based model built upon U-Net architecture.
  
  Args:
      marginal_prob_std: A function that takes time t and gives the standard
        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).
      channels: The number of channels for feature maps of each resolution.
      embed_dim: The dimensionality of Gaussian random feature embeddings.
  &quot;&quot;&quot;
  marginal_prob_std: Any
  channels: Tuple[int] = (32, 64, 128, 256)
  embed_dim: int = 256
  
  @nn.compact
  def __call__(self, x, t): 
    # The swish activation function
    act = nn.swish
    # Obtain the Gaussian random feature embedding for t   
    embed = act(nn.Dense(self.embed_dim)(
        GaussianFourierProjection(embed_dim=self.embed_dim)(t)))
        
    # Encoding path
    h1 = nn.Conv(self.channels[0], (3, 3), (1, 1), padding='VALID',
                   use_bias=False)(x)    
    ## Incorporate information from t
    h1 += Dense(self.channels[0])(embed)
    ## Group normalization
    h1 = nn.GroupNorm(4)(h1)    
    h1 = act(h1)
    h2 = nn.Conv(self.channels[1], (3, 3), (2, 2), padding='VALID',
                   use_bias=False)(h1)
    h2 += Dense(self.channels[1])(embed)
    h2 = nn.GroupNorm()(h2)        
    h2 = act(h2)
    h3 = nn.Conv(self.channels[2], (3, 3), (2, 2), padding='VALID',
                   use_bias=False)(h2)
    h3 += Dense(self.channels[2])(embed)
    h3 = nn.GroupNorm()(h3)
    h3 = act(h3)
    h4 = nn.Conv(self.channels[3], (3, 3), (2, 2), padding='VALID',
                   use_bias=False)(h3)
    h4 += Dense(self.channels[3])(embed)
    h4 = nn.GroupNorm()(h4)    
    h4 = act(h4)

    # Decoding path
    h = nn.Conv(self.channels[2], (3, 3), (1, 1), padding=((2, 2), (2, 2)),
                  input_dilation=(2, 2), use_bias=False)(h4)    
    ## Skip connection from the encoding path
    h += Dense(self.channels[2])(embed)
    h = nn.GroupNorm()(h)
    h = act(h)
    h = nn.Conv(self.channels[1], (3, 3), (1, 1), padding=((2, 3), (2, 3)),
                  input_dilation=(2, 2), use_bias=False)(
                      jnp.concatenate([h, h3], axis=-1)
                  )
    h += Dense(self.channels[1])(embed)
    h = nn.GroupNorm()(h)
    h = act(h)
    h = nn.Conv(self.channels[0], (3, 3), (1, 1), padding=((2, 3), (2, 3)),
                  input_dilation=(2, 2), use_bias=False)(
                      jnp.concatenate([h, h2], axis=-1)
                  )    
    h += Dense(self.channels[0])(embed)    
    h = nn.GroupNorm()(h)  
    h = act(h)
    h = nn.Conv(1, (3, 3), (1, 1), padding=((2, 2), (2, 2)))(
        jnp.concatenate([h, h1], axis=-1)
    )

    # Normalize output
    h = h / self.marginal_prob_std(t)[:, None, None, None]
    return h
</code></pre>
","0","Question"
"78250613","","<p>I want to import SimpleNodeParser from llama_index.node parser.</p>
<pre><code>from llama_index.node_parser import SimpleNodeParser
</code></pre>
<p>But when I run this I'm getting an error:</p>
<pre><code>ModuleNotFoundError: No module named 'llama_index.node_parser'
</code></pre>
<p>Help me to solve this.</p>
<p>I want to import SimpleNodeParser from llama_index.node parser.</p>
","1","Question"
"78251318","","<p>I have observed an issue while using the <strong>Hyperband</strong> algorithm in <strong>Optuna</strong>. According to the Hyperband algorithm, when <strong>min_resources</strong> = 5, <strong>max_resources</strong> = 20, and <strong>reduction_factor</strong> = 2, the search should start with an <strong>initial space of 4</strong> models for bracket <strong>1</strong>, with each model receiving <strong>5</strong> epochs in the first round. Subsequently, the number of models is reduced by a factor of <strong>2</strong> in each round and search space should also reduced by factor of <strong>2</strong> for next brackets i.e bracket 2 will have  initial search space of <strong>2</strong> models, and the number of epochs for the remaining models is doubled in each subsequent round. so total models should be <strong>11</strong> is expected but it is training lot's of models.</p>
<p>link of the article:- <a href=""https://arxiv.org/pdf/1603.06560.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.06560.pdf</a></p>
<pre><code>import optuna
import numpy as np
import pandas as pd 
from tensorflow.keras.layers import Dense,Flatten,Dropout
import tensorflow as tf
from tensorflow.keras.models import Sequential


# Toy dataset generation
def generate_toy_dataset():
    np.random.seed(0)
    X_train = np.random.rand(100, 10)
    y_train = np.random.randint(0, 2, size=(100,))
    X_val = np.random.rand(20, 10)
    y_val = np.random.randint(0, 2, size=(20,))
    return X_train, y_train, X_val, y_val

X_train, y_train, X_val, y_val = generate_toy_dataset()

# Model building function
def build_model(trial):
    model = Sequential()
    model.add(Dense(units=trial.suggest_int('unit_input', 20, 30),
                    activation='selu',
                    input_shape=(X_train.shape[1],)))

    num_layers = trial.suggest_int('num_layers', 2, 3)
    for i in range(num_layers):
        units = trial.suggest_int(f'num_layer_{i}', 20, 30)
        activation = trial.suggest_categorical(f'activation_layer_{i}', ['relu', 'selu', 'tanh'])
        model.add(Dense(units=units, activation=activation))
        if trial.suggest_categorical(f'dropout_layer_{i}', [True, False]):
            model.add(Dropout(rate=0.5))

    model.add(Dense(1, activation='sigmoid'))

    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])
    if optimizer_name == 'adam':
        optimizer = tf.keras.optimizers.Adam()
    else:
        optimizer = tf.keras.optimizers.RMSprop()

    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='val_auc')])

    return model

def objective(trial):
    model = build_model(trial)
    # Assuming you have your data prepared
    # Modify the fit method to include AUC metric
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=1)
    
    # Check if 'val_auc' is recorded
    auc_key = None
    for key in history.history.keys():
        if key.startswith('val_auc'):
            auc_key = key
            print(f&quot;auc_key is {auc_key}&quot;)
            break
    
    if auc_key is None:
        raise ValueError(&quot;AUC metric not found in history. Make sure it's being recorded during training.&quot;)
    
    # Report validation AUC for each model
    
    if auc_key ==&quot;val_auc&quot;:
        step=0
    else:
        step = int(auc_key.split('_')[-1])
    
    auc_value=history.history[auc_key][0]
    trial.report(auc_value, step=step)
    print(f&quot;prune or not:-{trial.should_prune()}&quot;)
    if trial.should_prune():
        raise optuna.TrialPruned()

    return history.history[auc_key]

# Optuna study creation
study = optuna.create_study(
    direction='maximize',
    pruner=optuna.pruners.HyperbandPruner(
        min_resource=5,
        max_resource=20,
        reduction_factor=2
    )
)

# Start optimization
study.optimize(objective)

</code></pre>
","2","Question"
"78251629","","<p>want to import HuggingFaceInferenceAPI.</p>
<pre><code>from llama_index.llms import HugggingFaceInferenceAPI
</code></pre>
<p>llama_index.llms documentation doesn't have HugggingFaceInferenceAPI module. Anyone has update on this?</p>
","0","Question"
"78252584","","<p>I am trying to scrape multi page pdf using textract.
Need to scrape pdf and format to json based on its sections, sub sections, tables.</p>
<p>while trying to UI demo with LAYOUT and Table it is exactly able to show layout title, layout section, layout text, layout footer, page number</p>
<p>same info can be observed in csv downloaded file from UI Demo: layout.csv file.
same in json file: analyzeDocResponse.json too but it has all (LINES, WORDS, LAYOUT_TITLE, and all layout related data), i think textract does all kind of block types in sequence.</p>
<p>for debugging purpose, i am using below code to print entire dictionary of block.
and also block type followed by its corresponding text.</p>
<p>if interested in pdf file: its SmPC of Drugs: <a href=""https://www.nafdac.gov.ng/wp-content/uploads/Files/SMPC/Covid19/Pfizer-BioNTech-SmPC-For-Covid-19-Vaccine.pdf"" rel=""nofollow noreferrer"">SmPC file</a></p>
<p>code 1: printing each block in json format.</p>
<pre><code>
def start_textract_job(bucket, document):
    response = textract.start_document_analysis(
        DocumentLocation={
            'S3Object': {
                'Bucket': bucket,
                'Name': document
            }
        },
        FeatureTypes=[&quot;LAYOUT&quot;]  # You can adjust the FeatureTypes based on your needs
    )
    return response['JobId']


def print_blocks(job_id):
    next_token = None
    while True:
        if next_token:
            response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
        else:
            response = textract.get_document_analysis(JobId=job_id)

        for block in response.get('Blocks', []):
            print(json.dumps(block, indent=4))

        next_token = response.get('NextToken', None)
        if not next_token:
            break
</code></pre>
<p>it is printing similiar info as per UI Demo, block type LINES, WORDS, LAYOUT_</p>
<p>but if i try to print text for each block type using below code, it fails to print for LAYOUT_ related , not sure why, am i missing anything?</p>
<p>code 2: to print block type followed by its content.</p>
<pre><code>
def start_textract_job is same as above, LAYOUT.

def print_blocks(job_id):
    next_token = None
    while True:
        if next_token:
            response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
        else:
            response = textract.get_document_analysis(JobId=job_id)

        for block in response.get('Blocks', []):
            print(f&quot;{block['BlockType']}: {block.get('Text', '')}&quot;)

        next_token = response.get('NextToken', None)
        if not next_token:
            break
</code></pre>
<p>I can see values for block type LINES, WORDS
but coming empty for LAYOUT as below, i think, it is identifying in block types but not its values.</p>
<p>LAYOUT_TITLE:
LAYOUT_FIGURE:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_PAGE_NUMBER:
LAYOUT_FOOTER:</p>
<p>any help is highly appreicated, went thru doc and few other StackOverflow questions but couldnt find any help.
New to Tetract, sorry for noob Q?, if it is :)</p>
","1","Question"
"78253278","","<p>I am working on to create a machine learning project with PyTorch, and I realize my model does not seem to learn - it always outputs a flat line with little changes, and the loss hardly reduces. In order to spot the issue, I reduced my program to another minimum program (the program in question), yet the problem remains.</p>
<p>In this reduced program, I aim to make a model to approach to the shape of y=x^2. I copied and pasted my model class <code>ANN</code> from my original bigger project, which has 6 features. For simplicity, I set first 5 features to 0 and the last one to be the actual <code>x</code> (evenly separated numbers on interval from -2 to 2), and based on <code>x</code> generate corresponding <code>y</code> values by my <code>generate_targets()</code>.</p>
<pre><code>from torch import tensor, float32
from torch import nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt


class ANN(nn.Module):
    def __init__(self, feature_num: int):
        super(ANN,self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(feature_num, 300),
            nn.Tanh(),
            nn.Linear(300, 200),
            nn.Tanh(),
            nn.Linear(200, 150),
            nn.Tanh(),
            nn.Linear(150, 50),
            nn.Tanh(),
            nn.Linear(50, 1)
        )

    def forward(self, x):
        predictions = self.layers(x)
        return predictions


class TestDataset(Dataset):
    def __init__(self, sample_num):
        self.sample_num = sample_num
        self.func_max = 2
        self.func_min = -2
        self.unit = (self.func_max - self.func_min) / self.sample_num
        self.targets = generate_targets(self.sample_num)

    def __getitem__(self, index):
        x = self.func_min + index * self.unit
        return tensor([0, 0, 0, 0, 0, x], dtype=float32), self.targets[index]

    def __len__(self):
        return self.sample_num


# Generate the list of y
def generate_targets(count):
    func_max = 2
    func_min = -2
    unit = (func_max - func_min)/count
    target_list = []
    for i in range(count):
        x = func_min + unit*i
        y = x ** 2
        target_list.append(y)
    return target_list


# The main program
def start_train():
    sample_num = 500
    train_data = TestDataset(sample_num)
    train_dataloader = DataLoader(train_data, batch_size=10, shuffle=True)

    model = ANN(6)
    model.train()
    mae_loss = nn.L1Loss()
    optimizer = optim.Adam(model.parameters())
    loss_list = []

    for i in range(sample_num):
        train_feature, train_target = next(iter(train_dataloader))
        prediction = model(train_feature)
        loss = mae_loss(prediction, train_target.float().unsqueeze(1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        loss_list += [loss.item()]
        print(f&quot;iteration [{i + 1}/{sample_num}] Loss = {loss.item():.3f}&quot;)

    plt.plot(range(sample_num), loss_list, marker='o', label='Validation')
    plt.xlabel('iterations')
    plt.ylabel('MAE loss')
    plt.title('Loss vs Iteration')
    plt.legend(loc='upper right')
    plt.savefig('Debugger_Loss_Iter.png')
    plt.close()

    comparison(model, sample_num)


# Evaluate the trained model by plugging in each x coord and see the generated comparative graph
def comparison(model: ANN, sample_num: int) -&gt; None:
    model.eval()
    prediction_list = []
    for i in range(sample_num):
        train_feature = tensor([0, 0, 0, 0, 0, i / sample_num], dtype=float32)
        prediction = model(train_feature)
        prediction_list += [prediction.item()]

    target_list = generate_targets(sample_num)
    x_list = [(i - sample_num / 2) / sample_num for i in list(range(sample_num))]

    plt.plot(x_list, target_list, marker='o', label='Target')
    plt.plot(x_list, prediction_list, marker='o', label='Prediction')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Prediction-target Comparison')
    plt.legend(loc='upper right')
    plt.savefig('Debugger_Comparison.png')
    plt.close()


if __name__ == '__main__':
    start_train()
</code></pre>
<p>After executing, the output is something like the attached image. (On the image, the scale of the x label is wrong, but it should not be part of the learning issue)</p>
<p><a href=""https://i.sstatic.net/YniVm.png"" rel=""nofollow noreferrer"">The current problematic output image of my program. The exact shape of the line may vary, but nothing close to the target.</a></p>
<p><a href=""https://i.sstatic.net/sgSB1.png"" rel=""nofollow noreferrer"">Image of loss over iterations</a></p>
<p>The correct output that I am expecting is a shape close to the target, similar to the attached image.</p>
<p><a href=""https://i.sstatic.net/XsT3D.png"" rel=""nofollow noreferrer"">A correct output image, from a single-time success for unknown reason which I failed to recreate</a></p>
<p>I notice that usually tensor mismatch is a common reason for not learning, and I have deliberately checked the shapes and dtypes of the incoming and outputting tensors. Unfortunately, I found those tensors align with my expectations.</p>
<pre><code>train:
train_feature: (10,6)     float32
train_target.shape([10,1])     float32
prediction: ([10,1])      float32

comparison:
train_feature:([10,6])       float32
prediction([10,1])      float32
</code></pre>
","0","Question"
"78253997","","<p>I am trying to do Regression on the vision transformers model and I cannot replace the last layer of classification with the regression layer</p>
<pre><code>class RegressionViT(nn.Module):
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(pretrained=pretrained)
        # Accessing the actual output feature size from vit_b_16
        self.regressor = nn.Linear(self.vit_b_16.heads[0].in_features, num_classes * batch_size)

    def forward(self, x):
        x = self.vit_b_16(x)
        x = self.regressor(x)
        return x


# Model
model = RegressionViT(num_classes=1)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

criterion = nn.MSELoss()  # Use appropriate loss function for regression
optimizer = optim.Adam(model.parameters(), lr=0.0001)

</code></pre>
<p>I get this error when I try to initialize and run the model</p>
<pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1000 and 768x32)
</code></pre>
<p>The problem is that there is a mismatch between the regression layer and the <strong>vit_b_16</strong> model layer, what would be the correct way to solve this issue</p>
","-2","Question"
"78254344","","<p>This is my code.</p>
<pre><code>import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={&quot;temperature&quot;: 0, &quot;do_sample&quot;: False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
    model_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
    device_map=&quot;auto&quot;,
    tokenizer_kwargs={&quot;max_length&quot;: 4096},
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={
        &quot;torch_dtype&quot;: torch.float16, 
        &quot;llm_int8_enable_fp32_cpu_offload&quot;: True,
        &quot;bnb_4bit_quant_type&quot;: 'nf4',
        &quot;bnb_4bit_use_double_quant&quot;:True,
        &quot;bnb_4bit_compute_dtype&quot;:torch.bfloat16,
        &quot;load_in_4bit&quot;: True}
)
</code></pre>
<h2>I got this error.
The <code>load_in_4bit</code> and <code>load_in_8bit</code> arguments are deprecated and will be removed in the future versions. Please, pass a <code>BitsAndBytesConfig</code> object in <code>quantization_config</code> argument instead.</h2>
<pre><code>ImportError                               Traceback (most recent call last)
Cell In[46], line 3
      1 import torch
----&gt; 3 llm = HuggingFaceLLM(
      4     context_window=4096,
      5     max_new_tokens=256,
      6     generate_kwargs={&quot;temperature&quot;: 0, &quot;do_sample&quot;: False},
      7     system_prompt=system_prompt,
      8     query_wrapper_prompt=query_wrapper_prompt,
      9     tokenizer_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
     10     model_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
     11     device_map=&quot;auto&quot;,
     12     tokenizer_kwargs={&quot;max_length&quot;: 4096},
     13     # uncomment this if using CUDA to reduce memory usage
     14     model_kwargs={
     15         &quot;torch_dtype&quot;: torch.float16, 
     16         &quot;llm_int8_enable_fp32_cpu_offload&quot;: True,
     17         &quot;bnb_4bit_quant_type&quot;: 'nf4',
     18         &quot;bnb_4bit_use_double_quant&quot;:True,
     19         &quot;bnb_4bit_compute_dtype&quot;:torch.bfloat16,
     20         &quot;load_in_4bit&quot;: True}
     21 )

File /opt/conda/lib/python3.10/site-packages/llama_index/llms/huggingface/base.py:161, in HuggingFaceLLM.__init__(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)
    159 &quot;&quot;&quot;Initialize params.&quot;&quot;&quot;
    160 model_kwargs = model_kwargs or {}
--&gt; 161 self._model = model or AutoModelForCausalLM.from_pretrained(
    162     model_name, device_map=device_map, **model_kwargs
    163 )
    165 # check context_window
    166 config_dict = self._model.config.to_dict()

File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561, in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    557         cls.register(config.__class__, model_class, exist_ok=True)
    558     return model_class.from_pretrained(
    559         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    560     )
--&gt; 561 elif type(config) in cls._model_mapping.keys():
    562     model_class = _get_model_class(config, cls._model_mapping)
    563     return model_class.from_pretrained(
    564         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    565     )

File /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3024, in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3005     config_path = config if config is not None else pretrained_model_name_or_path
   3006     config, model_kwargs = cls.config_class.from_pretrained(
   3007         config_path,
   3008         cache_dir=cache_dir,
   (...)
   3019         **kwargs,
   3020     )
   3021 else:
   3022     # In case one passes a config to `from_pretrained` + &quot;attn_implementation&quot;
   3023     # override the `_attn_implementation` attribute to `attn_implementation` of the kwargs
-&gt; 3024     # Please see: https://github.com/huggingface/transformers/issues/28038
   3025 
   3026     # Overwrite `config._attn_implementation` by the one from the kwargs --&gt; in auto-factory
   3027     # we pop attn_implementation from the kwargs but this handles the case where users
   3028     # passes manually the config to `from_pretrained`.
   3029     config = copy.deepcopy(config)
   3031     kwarg_attn_imp = kwargs.pop(&quot;attn_implementation&quot;, None)

File /opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62, in Bnb4BitHfQuantizer.validate_environment(self, *args, **kwargs)
     60 def validate_environment(self, *args, **kwargs):
     61     if not (is_accelerate_available() and is_bitsandbytes_available()):
---&gt; 62         raise ImportError(
     63             &quot;Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` &quot;
     64             &quot;and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`&quot;
     65         )
     67     if kwargs.get(&quot;from_tf&quot;, False) or kwargs.get(&quot;from_flax&quot;, False):
     68         raise ValueError(
     69             &quot;Converting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make&quot;
     70             &quot; sure the weights are in PyTorch format.&quot;
     71         )
</code></pre>
<p>ImportError: Using <code>bitsandbytes</code> 8-bit quantization requires Accelerate: <code>pip install accelerate</code> and the latest version of bitsandbytes: <code>pip install -i https://pypi.org/simple/ bitsandbytes</code></p>
<p>what is the issue of this</p>
","5","Question"
"78254412","","<p>I want to extract the two graphs on the right hand side together nor seperately both can be extracted as one (<a href=""https://i.sstatic.net/RqwkB.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/RqwkB.jpg</a>)</p>
<p>Im not sure what to try i am very new to this. I am using python and i hae extracted the text from the image and saved it in a csv</p>
<pre><code>import cv2
import numpy as np

# Load the image
image_path = r'C:\Prarthana\PROJECTS\GitHub\MajorProject\Images\1.jpg'
image = cv2.imread(image_path)

# Convert the image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Apply Gaussian blur to reduce noise
blurred = cv2.GaussianBlur(gray, (5, 5), 0)

# Apply Canny edge detection
edges = cv2.Canny(blurred, 50, 150)

# Find contours in the edge-detected image
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Filter contours based on area to find the largest contour (assuming the graph is the largest area)
contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]

# Create a mask to extract the graph region
mask = np.zeros_like(gray)
cv2.drawContours(mask, contours, -1, (255, 255, 255), thickness=cv2.FILLED)

# Apply the mask to the original image to extract the graph
graph = cv2.bitwise_and(image, image, mask=mask)

# Save the extracted graph image
cv2.imwrite(r'C:\Prarthana\PROJECTS\GitHub\MajorProject\Images\output\extracted_graph.jpg', graph)
</code></pre>
","-2","Question"
"78255277","","<p>I am very new and fresh to machine learning and this is my first project I am working on as part of a college course. I chose UK football (soccer) matches. I have chosen to use a Random Forest.</p>
<p>Using different sources I have managed to get 20 years worth of data on said matches, clean the data and build my model.</p>
<p>However, I am stuck. How do I actually get the model to make predictions for future matches?</p>
<p>Thanks</p>
<p>I've tried to load the model and then use a CSV file with only the Date, Home_Team and Away_Team columns populated, leaving the other columns blank for the model to predict these values - is this the correct way to do this?</p>
<p>Update:</p>
<p>Thanks - please see the code used to build the model;</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
train = matches[matches[&quot;Date&quot;] &lt; '2012-06-01']
test = matches[matches[&quot;Date&quot;] &gt; '2012-06-01']
predictors = ['Home_Team', 'Away_Team',     'HT_Winner', 'FT_Winner', 'match_result', 'ht_match_result',     'HomeShots',     'AwayShots', 'HomeCorners', 'AwayCorners']
rf.fit(train[predictors], train[&quot;FT_Winner&quot;])
preds = rf.predict(test[predictors])
</code></pre>
<p>New CSV for future predictions:</p>
<pre><code>import pandas as pd

new_data_df = pd.read_csv(..)
predictions = model.predict(new_data_df)
</code></pre>
<p>Updated CSV contains all of the same columns (with only Date, Home_Team and Away_Team columns populated as this is the only info currently available and the other columns wanting the model to make predictions for. But when attempting to get predictions for the new CSV, I get the following;</p>
<p>&quot;Feature names must be in the same order as they were in fit.\n&quot;</p>
<p>ValueError(message)</p>
<pre><code>ValueError: The feature names should match those that were passed during fit.
</code></pre>
","1","Question"
"78257033","","<p>I am trying to tune hyperparameters with scikeras.wrappers.KerasRegressor and I am getting the problem described below:</p>
<p>Code:</p>
<pre><code>    # define a func to create an instance of lstm_model
def create_lstm_model():
    
    model = Sequential([
            LSTM(5, input_shape = (Xtrain.shape[1], Xtrain.shape[2]), dropout = 0.1, activation = 'tanh', return_sequences = True),
            LSTM(10, dropout = 0.05, activation = 'tanh'),
            Dense(5, activation = 'relu'),
            Dense(1)
        ])
    model.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.MeanSquaredError(), metrics = [keras.metrics.MeanSquaredError()])
    
    return model

#create the sklearn model for the network
model = KerasRegressor(build_fn = create_lstm_model, verbose = 1)

#param grid
batches = [16, 32]
epochs = [3, 4]

param_grid = dict(batch_size = batches, epochs = epochs)

grid = GridSearchCV(estimator = model,
                    param_grid = param_grid,
                    cv = 3)
grid.fit(Xtrain, ytrain, validation_data = (Xvalidation, yvalidation))
</code></pre>
<p>Error:</p>
<pre><code>    fn_or_cls = keras_metric_get(metric)
  File &quot;/home/aaa/Desktop/aaa/aaa/2024-gold-price-prediction-with-lstm-model/.venv/lib/python3.10/site-packages/keras/src/metrics/__init__.py&quot;, line 204, in get
    raise ValueError(f&quot;Could not interpret metric identifier: {identifier}&quot;)
ValueError: Could not interpret metric identifier: loss
</code></pre>
<p>I started with much more complex code and simplify it to minimum during the trouble shooting. I was even trying to remove metrics from the model func.</p>
<p>Do you have any suggestion what's wrong with my code?</p>
","0","Question"
"78257038","","<p>In gradient boosting different loss functions can be used. For example, in sklearn's GradientBoostingRegressor possible loss functions are: ‘squared_error’, ‘absolute_error’, ‘huber’, and ‘quantile’ loss functions.</p>
<p>I understand the effect of loss functions in gradient descent (not gradient boosting). For example, squared error loss function penalizes large errors more compared to absolute error loss function. Can we say similar things in the case of gradient boosting?</p>
","0","Question"
"78257853","","<p>I am trying to predict a variable (Y) using LightGBM Regression. However my predicted values are all the same (i.e. constant). Can someone help out in detecting the problem.</p>
<pre><code>data_x = [[2021,5,368.92],[2023,11,356.82],[2022,10,352.49],[2023,5,343.63],[2023,10,324.91],[2022,12,352.02],[2021,6,370.79],[2022,5,386.59],[2019,2,301.56],[2021,4,353.7],[2021,1,303.93],[2021,9,371.94],[2019,4,310.77],[2021,3,345.3],[2020,5,249.63],[2022,4,381.16],[2023,4,363.14],[2019,7,304.19],[2020,7,258.43],[2022,2,412.47],[2022,8,353.43],[2019,6,302.34],[2020,1,319.88],[2022,7,361.66],[2020,9,265.39],[2022,3,408.72],[2022,1,417.47],[2022,6,351.92],[2022,9,344.06],[2022,11,373.75],[2019,9,314.97],[2019,11,324.14],[2023,2,377.23],[2021,11,380.83],[2021,12,403.12],[2023,7,368.73],[2023,1,379.76],[2019,5,295.02],[2023,9,343.78],[2020,4,248.54],[2019,10,314.79],[2019,8,295.92],[2023,3,354.09],[2023,6,357.35],[2021,2,324.31],[2020,3,246.26],[2019,3,295.36],[2020,12,306.27],[2021,8,376.54],[2020,6,258.21],[2023,8,352.35],[2021,7,370.21],[2020,10,259.13],[2020,8,275.66],[2019,12,315.47],[2020,11,301.27],[2021,10,389.23],[2019,1,291.94],[2020,2,302.38]]

df_x = pd.DataFrame(data_x, columns=['Year', 'Month', 'Close'])

data_y = [[1479.42],[1654.53],[1537.76],[1621.22],[1567.62],[1528.39],[1444.63],[1562.17],[1356.81],[1463.48],[1558.9],[1463.96],[1362.03],[1432.7],[1502.46],[1524.71],[1592.68],[1342.74],[1467.48],[1553.66],[1609.19],[1349.1],[1379.39],[1496.12],[1448.08],[1562.96],[1525.25],[1575.06],[1591.15],[1544.66],[1319.9],[1366.73],[1482.72],[1520.73],[1557.03],[1577.37],[1624.74],[1402.05],[1614.94],[1482.28],[1338.88],[1354.6],[1553.65],[1606.36],[1510.78],[1348.05],[1323.39],[1542.95],[1411.64],[1493.44],[1563.53],[1414.8],[1452.67],[1491.7],[1451.43],[1467.23],[1477.13],[1360.29],[1386.48]]

df_y = pd.DataFrame(data_y, columns=['Value'])

X_df_earn_ind_fin_train, X_df_earn_ind_fin_test, y_df_earn_ind_fin_train, y_df_earn_ind_fin_test = train_test_split(df_x, df_y, test_size=0.3, random_state=21)

hyper_params = {
    'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': ['mape', 'auc'],
    'learning_rate': 0.01,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.7,
    'bagging_freq': 10,
    'verbose': 0,
    'verbose_eval': -1,
    &quot;max_depth&quot;: 10,
    &quot;num_leaves&quot;: 96,  
    &quot;max_bin&quot;: 256,
    &quot;num_iterations&quot;: 1000,
    &quot;n_estimators&quot;: 250
}

gbm = lgm.LGBMRegressor(**hyper_params)
gbm.fit(X_df_earn_ind_fin_train, y_df_earn_ind_fin_train,
        eval_set=[(X_df_earn_ind_fin_test, y_df_earn_ind_fin_test)],
        eval_metric='mape')

y_pred_df_earn_ind_test = gbm.predict(X_df_earn_ind_fin_test)

</code></pre>
<p>However my output is just an array of a constant value</p>
<pre><code>y_pred_df_earn_ind_test = 
array([1497.21170863, 1497.21170863, 1497.21170863, 1497.21170863,
       1497.21170863, 1497.21170863, 1497.21170863, 1497.21170863,
       1497.21170863, 1497.21170863, 1497.21170863, 1497.21170863,
       1497.21170863, 1497.21170863, 1497.21170863, 1497.21170863,
       1497.21170863, 1497.21170863])

</code></pre>
<p>How do I rectify this?</p>
","0","Question"
"78259907","","<p>I am trying to do transfer learning on Pytorch pretrained models with custom dataset.
Presently, I am getting an error as
<code>mat1 and mat2 shapes cannot be multiplied (212992x13 and 1280x3) </code>
during training the custom model.</p>
<p>When I try using efficient net, the below code works and it trains successfully, but when I use models like squeeze net I get an error</p>
<p>Works:</p>
<pre><code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
model = torchvision.models.efficientnet_b0(weights=weights).to(device)
</code></pre>
<p>Does not work:</p>
<pre><code>weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
</code></pre>
<p>Train:</p>
<pre><code>auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=auto_transforms, batch_size=32)

for param in model.features.parameters():
    param.requires_grad = False #Freeze layers

torch.manual_seed(42)
output_shape = len(class_names)
model.classifier = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    torch.nn.Linear(in_features=1280,
                    out_features=output_shape,
                    bias=True)).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

#ERROR DURING TRAIN
results = engine.train(model=model, train_dataloader=train_dataloader, test_dataloader=test_dataloader, optimizer=optimizer, loss_fn=loss_fn, epochs=100, device=device)
</code></pre>
<p>The training image size is 512x512</p>
<p>To make sure that this is not a problem of transforms, I have used autotransforms but still the problem persists.</p>
<p>Although there exists a similar topic <a href=""https://stackoverflow.com/questions/72724452/mat1-and-mat2-shapes-cannot-be-multiplied-128x4-and-128x64"">mat1 and mat2 shapes cannot be multiplied (128x4 and 128x64)</a>, it is based completely on creating a new Sequential model, whereas I am trying to use transfer learning on pretrained model.</p>
","-1","Question"
"78263004","","<p>I am trying to use mixtral-8x7b with my own data with no luck. Here is my code</p>
<pre><code>import torch
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt
llm = LlamaCPP(
    model_url=None, # We'll load locally.
    model_path='./Models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf', # 4-bit model
    temperature=0.1,
    max_new_tokens=1024, # Increasing to support longer responses
    context_window=8192, # Mistral7B has an 8K context-window
    generate_kwargs={},
    # set to at least 1 to use GPU
    model_kwargs={&quot;n_gpu_layers&quot;: 40}, # 40 was a good amount of layers for the RTX 3090, you may need to decrease yours if you have less VRAM than 24GB
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    verbose=True
)
</code></pre>
<p>Which gives error No module named 'llama_index.llms.llama_cpp'.</p>
<p>I have installed llama_index, used my MAC Mini and also Google Colab's GPUs</p>
<p>Any suggestions?</p>
","2","Question"
"78265338","","<p>I am working on time series classification. I have used two preprocessing steps given below</p>
<ol>
<li><p>Time series Dataset --&gt; Slicing Time Series ---&gt; Train-Validation-Test-Split --&gt; Model Training
Accuracy -- 90%</p>
</li>
<li><p>Time series Dataset --&gt; Train-Validation-Test-Split --&gt; Slicing Train/Test/Validation independently --&gt; Model Training
Accuracy -- 64%</p>
</li>
</ol>
<p>I am currently working on obtaining a sequential probability distribution for my project and have moved on to the second step. However, I am getting a lower accuracy result than expected, even lower than first step. I have already tuned hyperparameters and addressed the issue of class imbalance to eliminate bias, but the accuracy not increasing beyond 64%. Can anyone provide some insight into why this may be occurring?</p>
<p>For the train validation test split, I am using sklearn</p>
<p>x_main, x_test, y_main, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y,shuffle=True)</p>
<p>x_train, x_val, y_train, y_val = train_test_split(x_main, y_main, test_size= 0.1, random_state=42,stratify=y_main,shuffle=True)</p>
<p>At least I am expecting a close result of 80-85% with the second step</p>
","-1","Question"
"78266509","","<p>I'm trying to serve a model locally as a REST endpoint using MLFlow CLI. MLflow version is 2.11.3. Following are the arguments that I'm using.</p>
<pre><code>mlflow models serve -m &quot;runs:/7782c4a700cc49c1a04f9ce608d90358/knnmodel&quot; --env-manager local --port 5000
</code></pre>
<p>Following is the exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/vagrant/.local/bin/mlflow&quot;, line 8, in &lt;module&gt;
    sys.exit(cli())
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/click/core.py&quot;, line 1157, in __call__
    return self.main(*args, **kwargs)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/click/core.py&quot;, line 1078, in main
    rv = self.invoke(ctx)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/click/core.py&quot;, line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/click/core.py&quot;, line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/click/core.py&quot;, line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/click/core.py&quot;, line 783, in invoke
    return __callback(*args, **kwargs)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/cli.py&quot;, line 104, in serve
    return get_flavor_backend(
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/flavor_backend_registry.py&quot;, line 44, in get_flavor_backend
    local_path = _download_artifact_from_uri(
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py&quot;, line 105, in _download_artifact_from_uri
    return get_artifact_repository(artifact_uri=root_uri).download_artifacts(
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;, line 124, in get_artifact_repository
    return _artifact_repository_registry.get_artifact_repository(artifact_uri)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;, line 77, in get_artifact_repository
    return repository(artifact_uri)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/runs_artifact_repo.py&quot;, line 27, in __init__
    self.repo = get_artifact_repository(uri)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;, line 124, in get_artifact_repository
    return _artifact_repository_registry.get_artifact_repository(artifact_uri)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;, line 77, in get_artifact_repository
    return repository(artifact_uri)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;, line 45, in __init__
    super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;, line 59, in resolve_uri
    _validate_uri_scheme(track_parse.scheme)
  File &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;, line 35, in _validate_uri_scheme
    raise MlflowException(
mlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'https', 'http'}
</code></pre>
<p>It seems like MLflow the artefact cannot be accessed from the MLflow Webserver. Any fixed for this?</p>
","0","Question"
"78266928","","<p>my code defines a ROS node for object detection in images using the YOLO algorithm, specifically with a YOLO model loaded from a specified file path. The node subscribes to a ROS topic (/camera/color/image_raw) that broadcasts images. Upon receiving an image, the callback function is triggered, converting the ROS image message to an OpenCV image format using CvBridge. This image is then processed by the YOLO model to detect objects. The results, including detected object boxes, are printed to the console. To prevent continuous processing and reprocessing of images, a flag (image_processed) is set after the first image is processed. Additionally, the processed image is saved to a specified folder (temporary). The node remains active, listening for new images, until it is manually shut down.</p>
<pre><code>#!/usr/bin/python3
import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import os
from ultralytics import YOLO

class Node(object):
    def __init__(self):
        self.yolo_model = YOLO('/home/user/catkin_ws/src/run_folder/content/runs/obb/train/weights/best.pt')
        self.br = CvBridge()
        self.image_processed = False  # Flag to indicate if the image has been processed

        # Subscribe to the ROS topic that publishes images
        rospy.Subscriber(&quot;/camera/color/image_raw&quot;, Image, self.callback)

    def callback(self, msg):
        if not self.image_processed:  # Process only if no image has been processed yet
            image = self.br.imgmsg_to_cv2(msg)  # Convert ROS Image message to OpenCV image

            # Perform object detection with YOLO
            results = self.yolo_model.predict(image, show=True)
            for r in results[0]:
                print(&quot;---------------------------&quot;)
                print(r.boxes)

            # Save the image after processing it for the first time
            self.save_image(image, 'temporary')
            self.image_processed = True  # Set the flag to True to avoid re-processing

    def save_image(self, image, folder_name):
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
        
        file_path = os.path.join(folder_name, 'image.jpg')
        cv2.imwrite(file_path, image)
        print(f&quot;Image saved at {file_path}&quot;)

if __name__ == '__main__':
    rospy.init_node(&quot;image_processor_with_yolo&quot;, anonymous=True)
    node = Node()
    rospy.spin()  # Keep the node running until shutdown

</code></pre>
<p>when i try to extract the boundary boxes i get &quot;none&quot; in the output.</p>
","0","Question"
"78266973","","<p>I am using the random forest classification from sklearn and I am getting decent results in everything except the confusion matrix here are the codes and results</p>
<p><a href=""https://i.sstatic.net/a43TZ.png"" rel=""nofollow noreferrer"">The label distribution for the training and testing</a></p>
<p><a href=""https://i.sstatic.net/1w4QW.png"" rel=""nofollow noreferrer"">The size of the train set</a></p>
<p><a href=""https://i.sstatic.net/qkyeT.png"" rel=""nofollow noreferrer"">The model</a></p>
<p><a href=""https://i.sstatic.net/VWA45.png"" rel=""nofollow noreferrer"">Scores for the training model</a></p>
<p><a href=""https://i.sstatic.net/XMyHE.png"" rel=""nofollow noreferrer"">Here is the issue</a></p>
<p>This is not what i was expecting especially since that the amount of the training was only 1/3 of what it should have been i have 677k in the training dataset, but in the confusion matrix it's only doing all the label 0's.</p>
<p><strong>The model:</strong></p>
<pre><code>import time
# Record the starting time
start_time = time.time()

# Random Forest classifier
rf = RandomForestClassifier()

# Define the parameter grid
rf_param_grid = {'n_estimators': [45], 'criterion': ['entropy'], 'max_depth': [30]}

# Grid search
rf_cv = GridSearchCV(rf, rf_param_grid, cv=7)
rf_cv.fit(X_train, y_train)

# Record the ending time
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time

# Print the results
print(&quot;Best Score:&quot;, rf_cv.best_score_)
print(&quot;Best Parameters:&quot;, rf_cv.best_params_)
print(&quot;Elapsed Time:&quot;, elapsed_time, &quot;seconds&quot;)
</code></pre>
<p><strong>I got good results for each class here over 98% for each:</strong></p>
<pre><code># Make predictions on the training data
y_train_pred = rf_cv.predict(X_train)

# Compute accuracy
accuracy = accuracy_score(y_train, y_train_pred)

# Compute precision, recall, and F1-score for each class
precision = precision_score(y_train, y_train_pred, average=None)
recall = recall_score(y_train, y_train_pred, average=None)
f1 = f1_score(y_train, y_train_pred, average=None)

# Compute macro-averaged precision, recall, and F1-score
macro_precision = precision_score(y_train, y_train_pred, average='macro')
macro_recall = recall_score(y_train, y_train_pred, average='macro')
macro_f1 = f1_score(y_train, y_train_pred, average='macro')

# Print the evaluation metrics
print(&quot;Accuracy:&quot;, accuracy)
print(&quot;Precision (Class 0, 1, 2):&quot;, precision)
print(&quot;Recall (Class 0, 1, 2):&quot;, recall)
print(&quot;F1-score (Class 0, 1, 2):&quot;, f1)
print(&quot;Macro-averaged Precision:&quot;, macro_precision)
print(&quot;Macro-averaged Recall:&quot;, macro_recall)
print(&quot;Macro-averaged F1-score:&quot;, macro_f1)
</code></pre>
<p><strong>The confusion matrix where it doesn't show all the labels except class 0</strong></p>
<pre><code># Generate the confusion matrix
conf_matrix = confusion_matrix(y_train, y_train_pred)

# Define class labels
class_labels = ['Class 0', 'Class 1', 'Class 2']

# Visualize the confusion matrix with class labels
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;, xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
</code></pre>
","-2","Question"
"78268793","","<p>The error is this:
I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable <code>TF_ENABLE_ONEDNN_OPTS=0</code>.
2024-04-03 20:51:40.389067: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable <code>TF_ENABLE_ONEDNN_OPTS=0</code>.
WARNING:tensorflow:From C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.</p>
<p>2024-04-03 20:51:47.709342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
File &quot;D:\image caption\testing_caption_generator.py&quot;, line 68, in 
model = load_model(
ValueError: Unknown layer: 'NotEqual'. Please ensure you are using a <code>keras.utils.custom_object_scope</code> and that this object is included in the scope. See <a href=""https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object</a> for details.</p>
<p>This is my code:</p>
<pre><code>from PIL import Image
import matplotlib.pyplot as plt
import argparse
import pickle
from keras.models import load_model
from keras.layers import Lambda
from keras.applications.xception import Xception
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from pickle import load  # Import load function from pickle module
import tensorflow_hub as hub

# Define the custom layer
def NotEqual(x, y):
    import tensorflow as tf
    return tf.math.not_equal(x, y)

# Define functions for extracting features, generating descriptions, and other necessary utilities
def extract_features(filename, model):
    try:
        image = Image.open(filename)
    except:
        print(&quot;ERROR: Couldn't open image! Make sure the image path and extension are correct&quot;)
    image = image.resize((299,299))
    image = np.array(image)
    if image.shape[2] == 4: 
        image = image[..., :3]
    image = np.expand_dims(image, axis=0)
    image = image/127.5
    image = image - 1.0
    feature = model.predict(image)
    return feature

def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        pred = model.predict([photo, sequence], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text

ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help=&quot;Image Path&quot;)
args = vars(ap.parse_args())
img_path = args['image']




#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'
max_length = 32
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;,&quot;rb&quot;))
path='models/model_9.h5'
model = load_model(
       (path),
       custom_objects={'KerasLayer':hub.KerasLayer}
)
xception_model = Xception(include_top=False, pooling=&quot;avg&quot;)

photo = extract_features(img_path, xception_model)
img = Image.open(img_path)

description = generate_desc(model, tokenizer, photo, max_length)
print(&quot;\n\n&quot;)
print(description)
plt.imshow(img)```
</code></pre>
","0","Question"
"78269213","","<p>I am using this CNN to detect information in eeg scans. It is gaining accuracy really slowly and I am wondering if I am mmissing anything in any of the layers or am doing anything wrong</p>
<pre><code>class Net(Module):
    def __init__(self):
        super(Net, self).__init__()
        self.cnn_layers = Sequential(
            Conv1d(1,14, kernel_size=5, padding=1),
            BatchNorm1d(14),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),

        )
        self.cnn_layer2 = Sequential(
            Conv1d(14, 10,kernel_size=5, padding=1),
            BatchNorm1d(10),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),
            Dropout(0.2),
        )
        self.cnn_layer3 = Sequential(
            Conv1d(10, 10, kernel_size=5, padding=1),
            BatchNorm1d(10),
            LeakyReLU(0.1),
            MaxPool1d(kernel_size=5, stride=1),
            Dropout(0.2),
        )
        self.linear_layer1 = Sequential(
            Linear(in_features=35660,out_features=3500),
            BatchNorm1d(3500),
            LeakyReLU(0.1),
            Dropout(0.2)

        )
        self.linear_layer2 = Sequential(
            Linear(in_features=3500,out_features=2500),
            BatchNorm1d(2500),
            LeakyReLU(0.1),
            Dropout(0.2)
        )
        self.linear_layer3 = Sequential(
            Linear(in_features=2500, out_features=250),
            BatchNorm1d(250),
            LeakyReLU(0.1),
            Dropout(0.2)
        )
        self.linear_layer4 = Sequential(
            Linear(in_features=250, out_features=10)
        )
        self.logsoft = Sequential(
            LogSoftmax(dim=1)
        )
        self.flatten = Sequential(
            Flatten() # probably has to be changed
        )

    def forward(self, x):
        x = self.cnn_layers(x)

        x = self.cnn_layer2(x)

        x = self.cnn_layer3(x)

        x = self.flatten(x)
        x = self.linear_layer1(x)
        x = self.linear_layer2(x)
        x = self.linear_layer3(x)
        x = self.linear_layer4(x)
        x = self.logsoft(x)
        return x



model = Net()

#Build a dataset by taking 255 columns and grouping them into 14 * 255 channels
class CustomDataSet():
    def __init__(self, csv_file, label,  transform=None):
        self.df = csv_file
        self.transform = transform
        self.label = label

    def __len__(self):
        return self.df.shape[0]

    def __getitem__(self, index):
        scan = (self.df[index])
        label = self.label[index]
        if self.transform:
            scan = self.transform(scan)

        return scan, label

train_dataset = rows
print(train_dataset.shape)

train_dataset = CustomDataSet(csv_file=rows, label=(labels))

optimizer = SGD(model.parameters(), lr=0.001, weight_decay=5.0e-5)
criterion = CrossEntropyLoss()
num_epochs = 500
train_loss_list = []
batch_size = 500
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_loss = 0

    # Iterating over the training dataset in batches
    total_correct = 0
    total_samples = 0
    model.train()
    for i, (scan, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated



        # Calculating the model output and the cross entropy loss

        outputs = model(scan)
        print(outputs.shape)

        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)

        total_correct += (predicted == labels).sum().item()
        total_samples += labels.size(0)
        # Printing loss for each epoch
    accuracy = 100 * total_correct / total_samples
    print(&quot;Accuracy: &quot;, accuracy)
    train_loss_list.append(train_loss / len(train_loader))
    print(f&quot;Training loss = {train_loss_list[-1]}&quot;)
</code></pre>
<p>I have tried adding batch normalizations on each layer and dropout layers. Each epoch is being trained on 20,000 scans but I have access to 51,000 so I might try it with more data. After 100 epochs it only reached 13 percent accuracy. Is this normal or have I made a mistake?</p>
","0","Question"
"78270055","","<p>I'm trying to build a SARIMA (Seasonal Autoregressive Integrated Moving Average) model for forecasting PM10 concentrations based on five years of data. However, when I set the seasonal parameter m to 365, my code doesn't seem to run.</p>
<p>Could someone please explain why my code is not running with m=365 and suggest a potential solution?</p>
<p>Thanks in advance!</p>
<pre class=""lang-py prettyprint-override""><code># Here's a snippet of my code:
## Split the dataset into train and test sets

    `train_size = int(len(Alipur_df) * 0.8)  # 80% train, 20% test`
    `train, test = Alipur_df[:train_size], Alipur_df[train_size:]`

## Convert train DataFrame to a numpy array

    `train_values = train['Alipur'].values`
    `test_values = test['Alipur'].values`

## Use auto_arima to find the best parameters for SARIMA

    <span class=""math-container"">`</span>auto_model = auto_arima(train['Alipur'], seasonal=True, stationary=True, m= 365, trac
</code></pre>
","6","Question"
"78271090","","<p>I tried to run the code below, taken from <a href=""https://cdn.cs50.net/ai/2020/spring/lectures/5/src5/banknotes/"" rel=""noreferrer"">CS50's AI course</a>:</p>
<pre><code>import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Read data in from file
with open(&quot;banknotes.csv&quot;) as f:
    reader = csv.reader(f)
    next(reader)

    data = []
    for row in reader:
        data.append(
            {
                &quot;evidence&quot;: [float(cell) for cell in row[:4]],
                &quot;label&quot;: 1 if row[4] == &quot;0&quot; else 0,
            }
        )

# Separate data into training and testing groups
evidence = [row[&quot;evidence&quot;] for row in data]
labels = [row[&quot;label&quot;] for row in data]
X_training, X_testing, y_training, y_testing = train_test_split(
    evidence, labels, test_size=0.4
)

# Create a neural network
model = tf.keras.models.Sequential()

# Add a hidden layer with 8 units, with ReLU activation
model.add(tf.keras.layers.Dense(8, input_shape=(4,), activation=&quot;relu&quot;))

# Add output layer with 1 unit, with sigmoid activation
model.add(tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;))

# Train neural network
model.compile(
    optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)
model.fit(X_training, y_training, epochs=20)

# Evaluate how well model performs
model.evaluate(X_testing, y_testing, verbose=2)
</code></pre>
<p>However, I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Eric\Desktop\coding\cs50\ai\lectures\lecture5\banknotes\banknotes.py&quot;, line 41, in &lt;module&gt;
    model.fit(X_training, y_training, epochs=20)
  File &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;, line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\trainers\data_adapters\__init__.py&quot;, line 113, in get_data_adapter
    raise ValueError(f&quot;Unrecognized data type: x={x} (of type {type(x)})&quot;)
ValueError: Unrecognized data type: x=[...] (of type &lt;class 'list'&gt;)
</code></pre>
<p>where &quot;...&quot; is the training data.</p>
<p>Any idea what went wrong? I'm using Python version 3.11.8 and TensorFlow version 2.16.1 on a Windows computer.</p>
<p>I tried running the same code in a Google Colab notebook, and it works: the problem only occurs on my local machine. This is the output I'm expecting:</p>
<pre><code>Epoch 1/20
26/26 [==============================] - 1s 2ms/step - loss: 1.1008 - accuracy: 0.5055
Epoch 2/20
26/26 [==============================] - 0s 2ms/step - loss: 0.8588 - accuracy: 0.5334
Epoch 3/20
26/26 [==============================] - 0s 2ms/step - loss: 0.6946 - accuracy: 0.5917
Epoch 4/20
26/26 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6683
Epoch 5/20
26/26 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7120
Epoch 6/20
26/26 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.7655
Epoch 7/20
26/26 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8177
Epoch 8/20
26/26 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8433
Epoch 9/20
26/26 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8615
Epoch 10/20
26/26 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8870
Epoch 11/20
26/26 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.9028
Epoch 12/20
26/26 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.9125
Epoch 13/20
26/26 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9283
Epoch 14/20
26/26 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9514
Epoch 15/20
26/26 [==============================] - 0s 3ms/step - loss: 0.2124 - accuracy: 0.9660
Epoch 16/20
26/26 [==============================] - 0s 2ms/step - loss: 0.1961 - accuracy: 0.9769
Epoch 17/20
26/26 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9781
Epoch 18/20
26/26 [==============================] - 0s 2ms/step - loss: 0.1681 - accuracy: 0.9793
Epoch 19/20
26/26 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9793
Epoch 20/20
26/26 [==============================] - 0s 2ms/step - loss: 0.1452 - accuracy: 0.9830
18/18 - 0s - loss: 0.1407 - accuracy: 0.9891 - 187ms/epoch - 10ms/step
[0.14066053926944733, 0.9890710115432739]
</code></pre>
","6","Question"
"78274833","","<p>I am trying to migrate a pipeline from Microsoft machine learning studio classic to Azure machine learning.  My pipeline which is actually running on Machine Learning Studio Classic and is composed by the following components which are classic and custom. The input data are six and are collected from a blob storage account using the import data component. They are mainly csv files and i use a custom component which is an R script to clean the input data. This is because the “Execute R Script“ allows only 2 inputs but i need 6 as i have 6 input files. After that, the csv files are written into the same blob storage account but in another folder using a component called export data. I want to build the same pipeline into azure machine learning can I use custom and default components in the same pipeline?</p>
<p>If I try to create a pipeline with classic components apparently I can‘t use my custom component (A.K.A. R Script) and when I try to create a custom pipeline I can‘t use the classic components and apparently there is no way to export my data when creating a custom pipeline after my custom component cleans the input data.</p>
","0","Question"
"78274904","","<p>When converting categorical data in first column of my dataframe I am getting strange behavior of ColumnTransformer with OneHotEncoder. the behavior occurs when I add one row to my csv file.</p>
<p>the initial data is:</p>
<pre><code>title,dailygross,theaters,DayInYear
ACatinParis,307,5,257
ALettertoMomo,307,5,257
AnotherDayofLife,307,5,257
ApprovedforAdoption,307,5,257
AprilandtheExtraordinaryWorld,307,5,257
Belle,307,5,257
BirdboyTheForgottenChildren,307,5,257
ChicoRita,307,5,257
</code></pre>
<p>when running code</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv('../data/GKIDS_DayNum_test_names.csv')
dataset['title'].str.strip()
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

title_column_index = dataset.columns.get_loc('title')
print('title index:', title_column_index)
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [title_column_index])], remainder='passthrough')
X_Encoded = np.array(ct.fit_transform(X))
print(X_Encoded)
</code></pre>
<p>the result is correct:</p>
<pre><code>[[1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 307 5]]
</code></pre>
<p>However when I add additional row: <code>BlueGiant,307,5,257</code>
to the file and re run the code I am getting weird output:</p>
<pre><code>  (0, 0)    1.0
  (0, 9)    307.0
  (0, 10)   5.0
  (1, 1)    1.0
  (1, 9)    307.0
  (1, 10)   5.0
  (2, 2)    1.0
  (2, 9)    307.0
  (2, 10)   5.0
  (3, 3)    1.0
  (3, 9)    307.0
  (3, 10)   5.0
  (4, 4)    1.0
  (4, 9)    307.0
  (4, 10)   5.0
  (5, 5)    1.0
  (5, 9)    307.0
  (5, 10)   5.0
  (6, 6)    1.0
  (6, 9)    307.0
  (6, 10)   5.0
  (7, 8)    1.0
  (7, 9)    307.0
  (7, 10)   5.0
  (8, 7)    1.0
  (8, 9)    307.0
  (8, 10)   5.0
</code></pre>
<p>I do not understand why it is.</p>
<p>Please help.</p>
","1","Question"
"78275121","","<p>I Have Sample House Price Data And Simple Code :</p>
<pre><code>import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

data = pd.read_csv('house_price_4.csv')
df = pd.DataFrame(data)
df['Area'] = df['Area'].str.replace(',', '')
df = df.dropna()

# Encoding the categorical feature 'Address'
df['Address'] = df['Address'].astype('category').cat.codes
df['Parking'] = df['Parking'].replace({True: 1, False: 0})
df['Warehouse'] = df['Warehouse'].replace({True: 1, False: 0})
df['Elevator'] = df['Elevator'].replace({True: 1, False: 0})

X = df.drop(columns=['Price(USD)','Price'])
y = df['Price']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r_squared = r2_score(y_test, y_pred)
print(f'R^2 Score: {r_squared:.4f}')   

                                                                  
</code></pre>
<p>My R2 Score is Very Low : 0.34</p>
<p>How Can I Get Higher R2 Score?</p>
<p>This is My Sample Data :    <a href=""https://drive.google.com/file/d/14Se90XbGJivftq3_VrtgRSAlkCplduVX/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/14Se90XbGJivftq3_VrtgRSAlkCplduVX/view?usp=sharing</a></p>
","1","Question"
"78275238","","<p><strong>tl;dr</strong>: I have a pipeline that does not work as expected. I am probably making some mistake in <code>OneHoteEncode</code>. The question is a little long because of the codes but it is probably very simple and straightforward. I can provide the full code and data upon request, but I don't think it will be needed.</p>
<p>The question:</p>
<p>I have a transformation pipeline that goes as follows:</p>
<pre><code>#from sklearn import set_config
#set_config(transform_output='pandas')

class Preprocessor(TransformerMixin):
    def __init__(self):
        self._cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
        #pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        #self._cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
        preprocessing = self._preprocessing()
        return preprocessing.fit_transform(X)

    def _column_ratio(self, X):
        return X[:, [0]] / X[:, [1]]

    def _ratio_name(self, function_transformer, feature_names_in):
        return [&quot;ratio&quot;]

    def _ratio_pipeline(self):
        return make_pipeline(
            SimpleImputer(strategy=&quot;median&quot;),
            FunctionTransformer(self._column_ratio, feature_names_out=self._ratio_name),
        StandardScaler()
    )

    def _log_pipeline(self):
        return make_pipeline(
            SimpleImputer(strategy=&quot;median&quot;),
            FunctionTransformer(np.log, feature_names_out=&quot;one-to-one&quot;),
            StandardScaler()
        )

    def _cat_pipeline(self):
        return make_pipeline(
            SimpleImputer(strategy=&quot;most_frequent&quot;),
            OneHotEncoder(handle_unknown=&quot;ignore&quot;)
        )

    def _default_num_pipeline(self):
        return make_pipeline(SimpleImputer(strategy=&quot;median&quot;),
                             StandardScaler()
        )

    def _preprocessing(self):
        return ColumnTransformer([
            (&quot;bedrooms&quot;, self._ratio_pipeline(), [&quot;total_bedrooms&quot;, &quot;total_rooms&quot;]),
            (&quot;rooms_per_house&quot;, self._ratio_pipeline(), [&quot;total_rooms&quot;, &quot;households&quot;]),
            (&quot;people_per_house&quot;, self._ratio_pipeline(), [&quot;population&quot;, &quot;households&quot;]),
            (&quot;log&quot;, self._log_pipeline(), [&quot;total_bedrooms&quot;, &quot;total_rooms&quot;, &quot;population&quot;,
                                &quot;households&quot;, &quot;median_income&quot;]),
            (&quot;geo&quot;, self._cluster_simil, [&quot;latitude&quot;, &quot;longitude&quot;]),
            (&quot;cat&quot;, self._cat_pipeline(), make_column_selector(dtype_include=object)),
    ],
    remainder=self._default_num_pipeline())  # one column remaining: housing_median_age

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, n_init=10,
                              random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self  # always return self!

    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)

    def get_feature_names_out(self, names=None):
        return [f&quot;Cluster {i} similarity&quot; for i in range(self.n_clusters)]
</code></pre>
<p>But it is not working properly. When I call just for test:</p>
<pre><code>preprocessor = Preprocessor()
X_train = preprocessor.fit_transform(housing)
</code></pre>
<p>the output for <code>X_train.info()</code> is exactly what it should be. But when I try a <code>gridSearch</code> with:</p>
<pre><code>svr_pipeline = Pipeline([(&quot;preprocessing&quot;, preprocessor), (&quot;svr&quot;, SVR())])
grid_search = GridSearchCV(svr_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
                       
grid_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])
</code></pre>
<p>it outputs a warning:</p>
<pre><code>ValueError: The feature names should match those that were passed during fit.
Feature names seen at fit time, yet now missing:
- cat__ocean_proximity_ISLAND

UserWarning: One or more of the test scores are non-finite:
[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan]
</code></pre>
<p><strong>The problem is probabily here:</strong></p>
<p><code>ValueError: X has 23 features, but SVR is expecting 24 features as input.</code></p>
<p>The shape of the correct <code>X</code> after passing the pipelin is <code>X.shape</code>: <code>(16521, 24)</code>. As it should be. This is the correct shape expected after the transformation.</p>
<p>That is, I have 24 features after the transformation pipeline. But somehow, when calling the <code>Preprocessor</code> class SVR is only seeing 23 features, not all of the 24: the one that is missing is the <code>ocean_proximity_ISLAND</code> which has only few values in the dataset. That is why when running the gridsearch in only the first 100 or 1000 lines of the dataset it gives no problems, but when running on a sufficient number of lines that the <code>ocean_proximity_ISLAND</code> is seen, it raises this problem.</p>
<p>This warning repeats at every step of the <code>GridSearch</code> and the column it is pointing to is always the same <code>cat__ocean_proximity_ISLAND</code> which comes from the <code>def _cat_pipeline(self):</code> part of the pipeline and is a result of using <code>OneHotEncode</code></p>
<p>Is the problem really on <code>OneHotEncode</code>? If so, why does it only raises error after the <code>gridSearch</code>? How to fix it and avoid such an error in the future?emphasized text</p>
","0","Question"
"78275777","","<p>From a simple vanilla GAN code I look from <a href=""https://github.com/shinyflight/Simple-GAN-TensorFlow-2.0/blob/master/GAN.py"" rel=""nofollow noreferrer"">GitHub</a></p>
<p>I saw this generator model with activation <code>sigmoid</code>:</p>
<pre><code># Generator
G = tf.keras.models.Sequential([
  tf.keras.layers.Dense(28*28 // 2, input_shape = (z_dim,), activation='relu'),
  tf.keras.layers.Dense(28*28, activation='sigmoid'),
  tf.keras.layers.Reshape((28, 28))])
</code></pre>
<p>The <code>G</code>'s loss is defined like this where <code>from_logits</code> is enabled:</p>
<pre><code>cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)
def G_loss(D, x_fake):
  return cross_entropy(tf.ones_like(D(x_fake)), D(x_fake))
</code></pre>
<p>As far as I know, <code>from_logits=True</code> is intended to make loss function accept <code>y_pred</code> value that has range between <code>-infinity</code> to <code>infinity</code>. Opposite to <code>from_logits=False</code> where the loss function assume the value is range between <code>0</code> until <code>1</code>.</p>
<p>As you see, the output layer of <code>G</code> model already has <code>sigmoid</code> activation where it's ranging between <code>0</code> until <code>1</code>.</p>
<p>But, why the author still using <code>from_logits=True</code>?</p>
","0","Question"
"78276066","","<p>I'm attempting to adjust the linear regression with a learning rate of 0.01. However, I'm encountering an issue where the line doesn't seem to adjust properly. Instead of following the expected path dictated by the learning rate, the line seems to randomly shift to different positions without proper alignment. I'm sure its not because of the learning rate, but rather the equation.</p>
<p>This is what I tried</p>
<pre><code>from sklearn.datasets import make_regression, make_classification, make_blobs
import matplotlib.pyplot as plt
import numpy as np
import random as rd

# 1. make_regression
x, y = make_regression(
  n_samples=100,
  n_features=1,
  noise=30
)

# 2. make_classification
train_x = x[:-20]
test_x = x[-20:]

train_y = y[:-20]
test_y = y[-20:]

# 3. make_blobs
def graph_linear_regression():
  # Plot the data
  plt.scatter(train_x, train_y, color='blue', label='Data')

  # Plot the linear regression line
  plt.plot(train_x, train_x, color='red', label='Linear Regression')

  # Add labels and title
  plt.xlabel('x')
  plt.ylabel('y')
  plt.title('Linear Regression')

  # Display the plot
  plt.legend()
  plt.show()

# 4. Calculate the error
def calculate_update_error():
  global m
  global b
  learning_rate = 0.01
  # Calculate Error
  # M = 1/n * sum(f(x) - y)
  # B = 1/n * sum(f(x) - y) * x

  M = 1 / n * np.sum(f(train_x) - train_y)
  B = 1 / n * np.sum(np.sum(f(train_x) - train_y) * train_x)

  m = m - learning_rate * M
  b = b - learning_rate * B

# Calculate linear regresion
# Step 1 - initialize

m = rd.randint(1, 100)
b = rd.randint(1, 100)
n = len(train_x)
learning_rate = 0.01

print(f&quot;n: {n}&quot;)
print(f&quot;m: {m}&quot;)
print(f&quot;b: {b}&quot;)
print(f&quot;learning_rate: {learning_rate}&quot;)

def f(x):
  return m * x + b

# Problem?
for i in range(n):
  calculate_update_error()
  print()

graph_linear_regression()
</code></pre>
<p>This is what im getting</p>
<p><a href=""https://i.sstatic.net/o0sn9.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Now, this is what im trying to obtain, this is using minimum square
<a href=""https://i.sstatic.net/0Dc6A.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","0","Question"
"78277279","","<p>When running this code:</p>
<pre><code>def scalar_summary(self, tag, value, step):
        &quot;&quot;&quot;Log a scalar variable.&quot;&quot;&quot;
        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])
        self.writer.add_summary(summary, step)
</code></pre>
<p>I get the error message:
<code>AttributeError: module 'tensorflow' has no attribute 'Summary'. Did you mean: 'summary'?</code></p>
<p>I am trying to run the code from <a href=""https://github.com/InhwanBae/ENet-SAD_Pytorch/blob/master/utils/tensorboard.py"" rel=""nofollow noreferrer"">https://github.com/InhwanBae/ENet-SAD_Pytorch/blob/master/utils/tensorboard.py</a> on Google colabs. I am trying to train the ENet-SAD model using the CULane dataset.</p>
","0","Question"
"78282862","","<p>I'm trying to run this block of code in my local LLM.</p>
<pre class=""lang-py prettyprint-override""><code>if compile:
    print(&quot;compiling the model&quot;)
    unoptimized_model = model
    model = torch.compile(model)
</code></pre>
<p>And this is the error i get:</p>
<p>Pls help me fix this</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
File &quot;c:\\Users\\abul4\\OneDrive\\Desktop\\LLM\\train.py&quot;, line 180, in \&lt;module\&gt;
model = torch.compile(model)
^^^^^^^^^^^^^^^^^^^^
File &quot;C:\\Users\\abul4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\__init_\_.py&quot;, line 1801, in compile
raise RuntimeError(&quot;Dynamo is not supported on Python 3.12+&quot;)
RuntimeError: Dynamo is not supported on Python 3.12+
</code></pre>
","1","Question"
"78284197","","<p>My goal is to finetune Mistral 7b to write short streams of consciousness (text completion, not instruction following).</p>
<p>I have a large database (1m rows) of short texts scraped from the internet. I manually labeled 15k rows into <code>good</code> (1k) and <code>bad</code> (the rest, 14k) examples. My plan is to train an <a href=""https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification"" rel=""nofollow noreferrer""><code>AutoModelForSequenceClassification</code></a> on these examples to label the other 985k rows.</p>
<p>In this way I hope to collect around ~20k good examples of streams of consciousness to finetune Mistral 7b on.</p>
<p>But only finetuning on the <code>good</code> examples does not use the information in the <code>bad</code> examples, which are far greater in number. Therefore I was thinking to use Mistral 7b as base model for the <code>AutoModelForSequenceClassification</code> (following <a href=""https://medium.com/@lukas.hauzenberger/multilabel-classification-using-mistral-7b-on-a-single-gpu-with-quantization-and-lora-8f848b5237f3"" rel=""nofollow noreferrer"">this Medium post</a>) and then retrain the resulting <code>AutoModelForSequenceClassification</code> for text completion. This entails removing the classification head and adding new/retraining LoRA components.</p>
<p>Do you think this at all feasible? Would this cripple the model (eg. needing to relearn grammar) or could this be an effective way to incorporate the information of the <code>bad</code> counterexamples into the text generation? Or at the very least provide a good initialization point for the LoRA finetuning to text generation?</p>
","2","Question"
"78284460","","<p>I am currently working on a project where I need to use BatchNormalization in TensorFlow Keras. I have imported layers from tensorflow.python.keras, but when I try to use BatchNormalization, I encounter the following error:</p>
<p><code>AttributeError: module 'tensorflow.python.keras.layers' has no attribute 'BatchNormalization'</code></p>
<p>Here's a snippet of my code:</p>
<pre><code>from tensorflow.python.keras import Model, Input
from tensorflow.python.keras.optimizers import adam_v2 as Adam
from tensorflow.python.keras import layers
class InceptionBlock(layers.Layer):
    def __init__(self, f, pooling=True):
        super(InceptionBlock, self).__init__()
        
        self.f = f
        self.pooling = pooling
   
        self.conva0 = layers.Conv2D(self.f, (1, 1), activation='relu', padding='same')
        self.batch_norma0 = layers.BatchNormalization()
        self.conva1 = layers.Conv2D(self.f, (3, 3), activation='relu', padding='same')
        self.batch_norma1 = layers.BatchNormalization()
        self.conva2 = layers.Conv2D(self.f, (1, 3), activation='relu', padding='same')
        self.batch_norma2 = layers.BatchNormalization()
        self.poola = layers.MaxPooling2D(pool_size=(2, 2))
        self.conva3 = layers.Conv2D(self.f, (3, 1), activation='relu', padding='same')
        self.batch_norma3 = layers.BatchNormalization()
            
</code></pre>
<p>And my tensorflow version is 2.16.1 and python version is 3.11.9. I've checked the documentation and it seems that BatchNormalization should be available in the layers module. Could anyone shed some light on why I might be encountering this error? Is there a different method I should be using to import BatchNormalization in TensorFlow Keras?</p>
<p>Any help would be appreciated.</p>
<p>Thank you!</p>
<p>i am trying to use BatchNormalization in my InceptionBlock path.</p>
","2","Question"
"78285377","","<p>This is my code on google collab</p>
<pre><code># Import library 
from pycaret.classification import *


exp_clf = setup(data, target='target', normalize=True)


best_model = compare_models()
</code></pre>
<p>And this is the result</p>
<p><a href=""https://i.sstatic.net/UWXt7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UWXt7.png"" alt=""enter image description here"" /></a></p>
<p>this code will show the best model parameter</p>
<pre><code>display(best_model)
</code></pre>
<p>and this is the result</p>
<p><a href=""https://i.sstatic.net/jL73i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jL73i.png"" alt=""enter image description here"" /></a></p>
<p>I want to know the other model parameter instead of the best model (Light Gradient Boosting Machine) since the AUC is not high enough for me. How do i display the parameter for random forest classifier?</p>
<p>I tried this, hoping it will show me the parameter of the second best model (random forest)</p>
<pre><code> display(best_model[1])
</code></pre>
<p>and this is the result</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-20-63c1d322b8ae&gt; in &lt;cell line: 1&gt;()
----&gt; 1 display(best_model[1])

TypeError: 'LGBMClassifier' object is not subscriptable
</code></pre>
","0","Question"
"78285447","","<p>I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?</p>
","0","Question"
"78286410","","<p>I've developed a basic recommendation system using Python and the cosine similarity algorithm. Now, I'm interested in creating a Laravel application to integrate this recommendation system. However, I'm unsure about how to establish the connection between the two.
Any assistance would be greatly appreciated!</p>
<p>i did not find anything to start with !!</p>
","-2","Question"
"78286577","","<p>So I've been working on machine learning projects, and needed to train a custom dataset using opencv_traincascade. but whenever i tried installing it, it would never work. I got other things like opencv_annotation and stuff to work, but traincascade or event createsamples wouldnt work. Do i have to manually build these?</p>
<p>I downloaded mingw-gcc, cmake and couldnt find a feasible solution online. I have opencv 4.9.0 by the way, installed both on anaconda and in my C: drive manually. I have tried looking for some third parties too, who had the entirety of opencv installed and could be replicated, but no luck. Any help would be severely appreciated, thank you!</p>
","-2","Question"
"78288109","","<p>I've tried to implement a gradient descent algorithm in Python for a machine learning problem. The dataset I'm working with has been preprocessed, and I observed an unexpected behavior in the runtime when comparing two datasets one with the original features and another with reduced dimensions of the former set using Singular Value Decomposition (SVD).I'm consistently observing that the runtime for the gradient descent algorithm is lower for the larger original dataset compared to the reduced one, contrary to my expectations. Shouldn't the runtime be shorter for the reduced dataset given its smaller size? I'm trying to understand why this is happening.
Here's the relevant code snippets:</p>
<pre><code>import time
import numpy as np
import matplotlib.pyplot as plt

def h_theta(X1, theta1):
    # Implementation of hypothesis function
    return np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
    # Implementation of cost function
    return np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, theta):
    # Calculation of gradient
    gradient = np.dot(X1.T, h_theta(X1, theta) - y1) / len(y1)
    return gradient

def gradient_descent(X1, y1):
    theta_initial = np.zeros(X1.shape[1])  # Initialize theta with zeros

    num_iterations = 1000
    learning_rates = [0.1, 0.01, 0.001]  
    cost_iterations = []
    theta_values = []
    start = time.time()
    for alpha in learning_rates:
        theta = theta_initial.copy()
        cost_history = []
        for i in range(num_iterations):
            gradient = grad(X1, y1, theta)
            theta = theta - np.dot(alpha, gradient)
            cost = j_theta(X1, y1, theta)
            cost_history.append(cost)
        cost_iterations.append(cost_history)
        theta_values.append(theta)
    end = time.time()
    print(f&quot;Time taken: {end - start} seconds&quot;)
    fig, axs = plt.subplots(len(learning_rates), figsize=(8, 15))  
    for i, alpha in enumerate(learning_rates):
        axs[i].plot(range(num_iterations), cost_iterations[i], label=f'alpha = {alpha}')  
        axs[i].set_title(f'Learning Rate: {alpha}')
        axs[i].set_ylabel('Cost J')
        axs[i].set_xlabel('Number of Iterations')  
        axs[i].legend()
    plt.tight_layout()  
    plt.show()

# code to reduce X to 3 features (columns) using SVD:
# Perform Singular Value decomposition on X and reduce it to 3 columns
U, S, Vt = np.linalg.svd(X_normalized)
# Reduce X to 3 columns
X_reduced = np.dot(X_normalized, Vt[:3].T)

# print the first 5 rows of X_reduced
print(&quot;First 5 rows of X_reduced:&quot;)
# Normalize X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;the means and stds of X after being reduced and normalized:\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# Print the shape of the reduced X to confirm it has only 3 features
print(&quot;Shape of X_reduced:&quot;, X_reduced.shape)

# Adding the intercept column to X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))


# Example usage
# X_normalized_with_intercept and y_normalized represent the original dataset
# X_reduced_with_intercept and y_normalized represent the reduced dataset

# Performing gradient descent on the original dataset
gradient_descent(X_normalized_with_intercept, y_normalized)

# Performing gradient descent on the reduced dataset
gradient_descent(X_reduced_with_intercept, y_normalized)
</code></pre>
<p>What could be causing the reduced dataset to consistently have a longer runtime compared to the full dataset in my gradient descent implementation? Any insights or suggestions for troubleshooting would be greatly appreciated.</p>
<p>I've tried rewriting and reviewing my implementations, but it seems that for most learning rates, and for an increased amount of iterations, the running time of the larger feature set is lower than it's SVD subset.</p>
","1","Question"
"78288542","","<p>I try to create hybrid CNN and LSTM model. I had issue related to the shape of the architecture. This lead to the epoch could not run through the data 200 times.</p>
<p>My datasize is (96,2)</p>
<p>ERROR:</p>
<pre><code>Epoch 1/200
    178/Unknown 9s 34ms/step - loss: 1.2366 - mse: 5.4560
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[40], line 4
      2 is_train = True
      3 if is_train:
----&gt; 4    model_create.fit(train_dataset, epochs=200 , batch_size=128)

Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [78,2], [batch]: [96,2]
     [[{{node IteratorGetNext}}]] [Op:__inference_one_step_on_iterator_23678]
</code></pre>
<p>CNN-LSTM model :</p>
<pre><code>def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D(filters=64,
                               kernel_size=3,
                               activation='relu',
                               input_shape=input_data_shape),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=&quot;same&quot;),
        tf.keras.layers.Conv1D(filters=64,
                               kernel_size=3,
                               activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=&quot;same&quot;),    
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model_cnn

</code></pre>
<p>Compile model</p>
<pre><code>def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  metrics=[&quot;mse&quot;])
    return model_create

model_create = create_model()

model_create.summary()

model_create.fit(train_dataset, epochs=200 , batch_size=128)

</code></pre>
<p>I had try to add reshape before the flatten () function to change the shape. I also decrease the batch size and epoch size. None of this work. How could fit my model with train_data?</p>
","0","Question"
"78288786","","<p>I am running notebooks on paperspace's gradient. When I book a machine for 4 hours and start running a jupyter notebook in it, if I close my browser the execution stops. How can I change this behaviour? I did not have this issue on google collab pro</p>
<p>I tried free and paid GPU machines and I tried upgrading to Pro account</p>
","0","Question"
"78289756","","<p>I am attempting to write a Reinforcement learning model using Box2D and TensorFlow in Google Colab. I have a simple one line install command for everything as I've found that Colab breaks whenever I try to do multi line installs, and even the main Gymnasium page has <code>pip install Box2D</code> as the correct way to install it.</p>
<p>Whenever I try to install it, I get this error:</p>
<pre><code>!pip install tensorflow[and-cuda] stable-baselines gym Box2D
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Building wheel for Box2D (setup.py) ... error
  ERROR: Failed building wheel for Box2D
  Running setup.py clean for Box2D
Failed to build Box2D
ERROR: Could not build wheels for Box2D, which is required to install pyproject.toml-based projects
</code></pre>
<p>Every solution I've found online hasn't worked, and yet all seem to have the same problem. How can I fix it?</p>
","2","Question"
"78289901","","<p>I have a popular .onnx ML model for weather forecasting and am trying to convert this to PyTorch for finetuning. I use the following code to convert this:</p>
<pre><code>import os
import numpy as np
import onnx
from onnx import numpy_helper
import onnxruntime as ort
from onnx2torch import convert

model_24 = onnx.load('pangu_weather_24.onnx')
tm = convert(model_24) #Convert onnx model to torch
</code></pre>
<p>From here, I want to access the weights of the model in the 'tm' object but I can't seem to find any resources online for this.</p>
<p>Trying to subscript it with tm[0] reveals the following error:</p>
<pre><code>TypeError: 'GraphModule' object is not subscriptable
</code></pre>
<p>and the dict for this object via 'tm.<strong>dict</strong>' is even more confusing (pasted in the image).</p>
<p>The normal methods online for accessing a PyTorch weight matrix also reveal the same error that the graphmodule is not subscriptable</p>
","0","Question"
"78291576","","<p>I'm tring to install tensorflow 2.6.0 on WSL2. I follow the guide (<a href=""https://www.tensorflow.org/install/source?hl=pl"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/source?hl=pl</a>) and after:</p>
<pre><code>bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda
</code></pre>
<p>I have an error:</p>
<pre><code>Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
ERROR: Skipping '//tensorflow/tools/pip_package:wheel': no such target '//tensorflow/tools/pip_package:wheel': target 'wheel' not declared in package 'tensorflow/tools/pip_package' defined by /home/jakmacc/tensorflow/tensorflow/tools/pip_package/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/tools/pip_package:wheel': target 'wheel' not declared in package 'tensorflow/tools/pip_package' defined by /home/jakmacc/tensorflow/tensorflow/tools/pip_package/BUILD
INFO: Elapsed time: 10.827s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded)
</code></pre>
<p>I don't know how to solve it.</p>
<p>I want to solve that mistake.</p>
<p>I tried also that command:</p>
<pre><code>bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu
</code></pre>
","1","Question"
"78293078","","<p>I use YOLOv8X for object detection in photos, and recently I've been wondering about the impact of image size on detection quality. I usually use datasets from Roboflow and apply a resize to 1280x1280 (which results in significantly better performance than the standard resize to 640x640), but the original photos often have aspect ratios far from square, and resizing changes the proportions of objects in the photos. The annotation in the dataset is recalculated for the new photo size, but can changing proportions degrade detection quality? Can I train the model without resizing the input photos? Is it necessary to resize on Roboflow if I can simply specify the parameter imgsz=1280 during training?</p>
<p>I have trained the model both with and without applying resize on Roboflow, and the quality metrics were similar. Some photos from the dataset appear unnaturally stretched after resizing to 1280x1280, which may degrade the model's ability to recognize objects due to changes in their shapes.</p>
","1","Question"
"78293441","","<p>I am currently doing a project where I am attempting to correctly classify images of breast cancer tumors to predict if they are benign (ok) or malignant (dangerous) for which I am using Pytorch. I have set up my network which extends the ResNet18 model and added a few more layers. For the hyperparameters, I originally set them up myself but wanted to use a method such as random search to find the best hyperparameters. However, upon running the validation loop pipeline to get the accuracy, I find it is going over 100%?</p>
<p>I load in my data like so:</p>
<pre><code>
import torch.utils
import torch.utils.data
from torchvision.transforms import transforms
import torch.utils.data as data

transformed_data = transforms.Compose([ # using Compose() to chain data transformations
    transforms.ToTensor()
])



training_dataset = DataSetClass(split=&quot;train&quot;, transform=transformed_data, download=download)
# testing_dataset = DataSetClass(split=&quot;test&quot;, transform=transformed_data, download=download)

train_size = int(training_ratio * len(training_dataset))
validation_size = int(validation_ratio * len(training_dataset))
testing_size = len(training_dataset) - train_size - validation_size

train_set, validation_set, testing_dataset = torch.utils.data.random_split(
    training_dataset, [train_size, validation_size, testing_size]
)

# convert data into dataloader form
train_loader = data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)
validation_loader = data.DataLoader(dataset=validation_set, batch_size=2*BATCH_SIZE, shuffle=False)
test_loader = data.DataLoader(dataset=testing_dataset, batch_size=BATCH_SIZE, shuffle=False)


print(training_dataset)
print(&quot;=======================&quot;)
print(testing_dataset)
</code></pre>
<p>I attempted to create a list storing different values for each hyperparameter as shown below along with performing the validation loop:</p>
<pre><code>import torch.optim as optim
import numpy as np

# Define your hyperparameters space
lr_space = [0.01, 0.02, 0.03, 0.04, 0.05]
epochs_space = [10, 20, 30, 40, 50]
batch_size_space = [32, 64, 128, 256]

# Initialize your network
network = ExtendedNetwork(resnet18)
network.to(device=device)

# Define your loss function
loss_function = nn.BCEWithLogitsLoss()

network.eval()

best_accuracy = 0
best_hyperparameters = None
validation_accuracy = 0


# Perform random search
for _ in range(100): 
    lr = np.random.choice(lr_space)
    epochs = np.random.choice(epochs_space)
    batch_size = np.random.choice(batch_size_space)

    # Use the selected hyperparameters to train your model
    optimizer = optim.Adam(network.parameters(), lr=lr)
    
    for epoch in range(epochs):
        with torch.no_grad():
            for inputs, targets in validation_loader:
                inputs, targets = inputs.to(device), targets.to(device)

                # Forward
                output = network(inputs)

                # Calculate and accumulate accuracy
                validation_accuracy += accuracy(output, targets)

        # Calculate average accuracy over all validation batches
        validation_accuracy /= len(validation_loader)

        print('Validation accuracy:', validation_accuracy)

        # If the current model is better than all previous models, update the best accuracy and best hyperparameters
        if validation_accuracy &gt; best_accuracy:
            best_accuracy = validation_accuracy
            best_hyperparameters = {'lr': lr, 'epochs': epochs, batch_size': batch_size}

print('Best accuracy:', best_accuracy)
print('Best hyperparameters:', best_hyperparameters)
</code></pre>
<p>This leads to results:</p>
<pre><code>Validation accuracy: 0.045871559633027525
Validation accuracy: 0.09174311926605505
...
Validation accuracy: 135.73394495412717
Validation accuracy: 135.77981651376018

Best accuracy: 135.77981651376018
Best hyperparameters: {'lr': 0.03, 'epochs': 40, 'batch_size': 256}
</code></pre>
<p>But the accuracy should be capped at 100% ? I do not understand if my error is coming from the way I am loading my data or the validation loop pipeline</p>
<p>Edits:</p>
<ol>
<li><p>Moved the validation_accuracy inside the epoch (which I did before but am still getting a constant accuracy which is neither increasing or decreasing, something like &quot;Validation accuracy: 0.3761467889908257&quot;</p>
</li>
<li><p>Could it be due to the way I am using the batch size I set myself and using it in the loader variables such as <code>train_loader, validation_loader, test_loader</code> but then I am wanting to use a list of batch sizes? Should i be using the values from <code>batch_size_space</code> inside the loaders instead?</p>
</li>
</ol>
","0","Question"
"78297772","","<p>I need to create a system for detecting the time that elapses between the start of booting the multimedia system in the car until the first appearance of the sidebar on the screen. Sidebar has a rectangular shape and is always located along one of the edges of the screen. Depending on the car manufacturer, the appearance of multimedia also differs. I am wondering what image detection algorithm would be most suitable if I don't care about execution speed or real-time work.</p>
<p>SSD, Faster R-CNN, YOLO or something else?</p>
","0","Question"
"78298357","","<p>I am working on hyperparameter tuning in TensorFlow and have set up an experiment using the HParams plugin in TensorBoard to log different configurations. My models are being trained with variations in dropout and learning rate, and I'm logging these parameters along with the model's accuracy. However, when I open TensorBoard and navigate to the HParams dashboard, the accuracy metrics associated with each trial are not displayed. The table shows the hyperparameters correctly, but the 'Accuracy' column is empty, even though my code compiles the model with 'accuracy' as a metric and uses the hp.KerasCallback for logging. I've verified that the models are training correctly, and other TensorBoard functionalities like the Scalars dashboard are showing the accuracy trends over epochs. I'm looking for assistance in understanding why the accuracy isn't showing up in the HParams table and how to fix this issue.<br />
<a href=""https://i.sstatic.net/sVWGl.png"" rel=""nofollow noreferrer"">Picture: Missing Values in Accuracy Column</a><br />
My Code for Hyperparameter Tuning with TensorBoard's HParams:</p>
<pre><code>from tensorboard.plugins.hparams import api as hp
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout

# Define hyperparameters
HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-2, 1e-3]))

# Set up logging
log_dir = './tensorboard/nn_1'
with tf.summary.create_file_writer(log_dir).as_default():
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        metrics=[hp.Metric('accuracy', display_name='Accuracy')]
    )

# Training function
def train_test_model(hparams, session_num):
    model_name = f&quot;model_1_session_{session_num}&quot;
    print(f&quot;Training {model_name} with hyperparameters {hparams}...&quot;)
    model = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), activation='elu'),
        Dropout(hparams[HP_DROPOUT]),
        Conv2D(32, kernel_size=(3, 3), activation='elu'),
        Dropout(hparams[HP_DROPOUT]),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(10, activation='softmax')
    ])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        metrics=['accuracy']
    )

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{log_dir}/{model_name}')
    hparams_callback = hp.KerasCallback(writer=f'{log_dir}/{model_name}', hparams=hparams)

    model.fit(
        x_train_reshaped, y_train_,
        epochs=3,
        validation_data=(x_val_reshaped, y_val),
        callbacks=[hparams_callback, tensorboard_callback]
    )

# Run the training for each set of hyperparameters
session_num = 0
for dropout_rate in HP_DROPOUT.domain.values:
    for learning_rate in HP_LEARNING_RATE.domain.values:
        hparams = {
            HP_DROPOUT: dropout_rate,
            HP_LEARNING_RATE: learning_rate,
        }
        train_test_model(hparams, session_num)
        session_num += 1

</code></pre>
","3","Question"
"78298624","","<p>Unable to import deepface in Python
I am currently using Pycharm on macbook.</p>
<p>Virtual environment is already activated and still getting the below error.
Can anyone please help me here.</p>
<p>Python 3.9
deepface 0.0.89 (latest)
tensorflow version 2.14.1</p>
<p>Virtual environment is already activated.
Everything is installed and still unable to resolve the import error.</p>
","0","Question"
"78299375","","<pre><code>prediction = &quot;I am ABC. I have completed my bachelor's degree in computer application at XYZ University and I am currently pursuing my master's degree in computer application through distance education.&quot;


reference = &quot;I'm ABC. I have finished my four-year certification in PC application at XYZ and I'm currently pursuing my graduate degree in PC application through distance training.&quot;

from nltk.translate.bleu_score import sentence_bleu

# Tokenize the sentences
prediction_tokens = prediction.split()
reference_tokens = reference.split()

# Calculate BLEU score
bleu_score = sentence_bleu([reference_tokens], prediction_tokens)

# Print the BLEU score
print(f&quot;BLEU score: {bleu_score:.4f}&quot;)
</code></pre>
<p>I am getting BLEU score as 0. I think I am making mistake somwhere. But not sure where.</p>
","-1","Question"
"78302444","","<p>I am calculating BLEU score between 2 sentences which seem very similar to me but I am getting BLEU score as very low. Is it supposed to happen?</p>
<pre><code>prediction = &quot;I am ABC.&quot;
reference = &quot;I'm ABC.&quot;

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu
from nltk.translate.bleu_score import SmoothingFunction
# Tokenize the sentences
prediction_tokens = prediction.split()
reference_tokens = reference.split()
   
# Calculate BLEU score
bleu_score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=SmoothingFunction().method4)

# Print the BLEU score
print(f&quot;BLEU score: {bleu_score:.4f}&quot;)

Output is 0.0725
</code></pre>
","1","Question"
"78303940","","<p>I wanted to build a model / train the model using deeplearning4j.
This model should understand all the employee related jargon for example, it should understand the words Employee, Worker, Associate as same words.</p>
<p>I was using below code snippet to create Data Set and have the iterator</p>
<pre><code>    DataSet allData;
    try (RecordReader recordReader = new CSVRecordReader(0, ',')) {
        recordReader.initialize(new StringSplit(&quot;Employee ID,0&quot;));

        DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader, 150, FEATURES_COUNT, CLASSES_COUNT);
        allData = iterator.next();
    }

    allData.shuffle(42);
</code></pre>
<p>and ending up with following error:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NumberFormatException: For input string: &quot;Employee ID&quot;
    at java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)
    at java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
    at java.base/java.lang.Double.parseDouble(Double.java:651)
    at org.datavec.api.writable.Text.toDouble(Text.java:590)
    at org.datavec.api.util.ndarray.RecordConverter.toMinibatchArray(RecordConverter.java:207)
    at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next(RecordReaderMultiDataSetIterator.java:153)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:346)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:421)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:53)
    at com.test.dl4jtest.dl4jtest.TrainModel.main(TrainModel.java:46)
</code></pre>
<p>Please suggest how do i create a model that should consider terms like worker, asociate, employee as an employee here.</p>
","-1","Question"
"78306767","","<p>I am performing a random forest analysis in RStudio and I can extract the confusion matrix using the code below. I can see how many samples are incorrectly classified, but what code can I use to identify which specific samples are being misclassified?</p>
<pre><code>library(randomForest)
library(rfPermute)

rfmetrics &lt;- randomForest(x, y, ntree=ntree, importance=T)
print(rfmetrics)

Call:
 randomForest(x = x, y = y, ntree = ntree, importance = T) 
               Type of random forest: classification
                     Number of trees: 1999
No. of variables tried at each split: 25

        OOB estimate of  error rate: 56.88%
Confusion matrix:
  1  2  3 4 class.error
1 8  7  7 4   0.6923077
2 4 19  2 4   0.3448276
3 3  3 15 6   0.4444444
4 3  9 10 5   0.8148148
</code></pre>
","0","Question"
"78308446","","<p><a href=""https://i.sstatic.net/nDOot.png"" rel=""nofollow noreferrer"">Example Data</a></p>
<p>I have an array of weather data by lat/lon in the following shape: (1038240,4) (See photo for sample example data)</p>
<p>I want to reshape this to the shape (4,721,1440) which would be four weather variables (&amp; lat/lon) over a 721 by 1440 image of the globe.</p>
<p>I have tried:</p>
<pre><code>newarr = t_new.reshape(4,721,1440) 
</code></pre>
<p>which puts this in the correct shape, but doesn't match this to the first two lat/lon coordinates to the preferred format below:</p>
<p>For the (6,4) example data in the photo above, this operation would look like a (2,3,2) array below:</p>
<p><a href=""https://i.sstatic.net/nur1x.png"" rel=""nofollow noreferrer"">Example Desired Output</a></p>
<pre><code>newarr = t_new.reshape(4,721,1440) 
</code></pre>
","-1","Question"
"78310990","","<p>Can I use LightGBM with Multi-Output Regression and also a custom Loss Function?
Problem is I have to use LightGBM.</p>
<p>I know that i can use MultiOutputRegression by sklearn to wrap LightGBM but this doesnt allow me to define a custom loss function, as I could do it with Keras.</p>
","-1","Question"
"78311513","","<p>I'm trying to train neural network to learn y = |x| function. As we know the absolute function has 2 different lines connecting with each other at point zero. So I'm trying to have following Sequential model:</p>
<p>Hidden Layer:
2 Dense Layer (activation relu)
Output Layer:
1 Dense Layer</p>
<p>after training the model,it only fits the half side of the function. Most of the time it is right hand side, sometimes it is the left side. As soon as I add 1 more Layer in the hidden layer, so instead of 2 I have 3, it perfectly fits the function. Can anyone explain why there is need an extra layer when the absolute function has only one cut ?</p>
<p>Here is the code:</p>
<pre><code>import numpy as np


X = np.linspace(-1000,1000,400)
np.random.shuffle(X)
Y = np.abs(X)

# Reshape data to fit the model input
X = X.reshape(-1, 1)
Y = Y.reshape(-1, 1)

import tensorflow as tf
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mse',metrics=['mae'])
model.fit(X, Y, epochs=1000)
# Predict using the model
Y_pred = model.predict(X)

# Plot the results
plt.scatter(X, Y, color='blue', label='Actual')
plt.scatter(X, Y_pred, color='red', label='Predicted')
plt.title('Actual vs Predicted')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
</code></pre>
<p>Plot for 2 Dense Layer:</p>
<p><a href=""https://i.sstatic.net/42zxJ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/42zxJ.png"" alt=""enter image description here"" /></a></p>
<p>Plot for 3 Dense Layer:
<a href=""https://i.sstatic.net/xlZ9Y.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/xlZ9Y.png"" alt=""enter image description here"" /></a></p>
","5","Question"
"78320892","","<p>I'm trying to perform a dummy classification on a dataset for a school project. The idea is to get an idea of the frequency in which different political parties give speeches. My idea is to write this code in the following way:</p>
<pre><code>from sklearn.dummy import DummyClassifier
import pandas as pd
import bz2


with bz2.open(&quot;data/ch3/speeches-201718.json.bz2&quot;) as source:
    speeches_201718 = pd.read_json(source)

with bz2.open(&quot;data/ch3/speeches-201819.json.bz2&quot;) as source:
    speeches_201819 = pd.read_json(source)


training_data, test_data = speeches_201718, speeches_201819

train_parties_count = training_data['party'].value_counts()
test_parties_count = test_data['party'].value_counts()
dummy_clf = DummyClassifier(strategy=&quot;most_frequent&quot;)

X = train_parties_count
y = train_parties_count.index
dummy_clf.fit(X.values, y)
print(X)
print(y)

test_parties_count.index = pd.CategoricalIndex(test_parties_count.index, categories=train_parties_count.index, ordered=True)
X_test = test_parties_count.sort_index()
print(X_test)
pred_mfc = dummy_clf.predict(X_test.values)

print(&quot;Urval av prediktioner [0-4]: &quot;, pred_mfc[:5])

</code></pre>
<p>I get the following output:
<a href=""https://i.sstatic.net/lIF2i.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>As you can see the prediction is C when it should be S, what can be incorrect?</p>
<p>I have tried defining the train and test data in multiple ways with no success.</p>
","0","Question"
"78322346","","<p>I need to split a large dataset into set proportions of training, validation, and test sets while ensuring the following:</p>
<ul>
<li>Keep unique IDs in each set. No ID can be in more than one set.</li>
<li>At each shuffling of data, there needs to be at least one occurrence of each level (&quot;hot&quot;, &quot;warm&quot;, &quot;cold) in the training, validation, and test sets.</li>
</ul>
<pre><code>data &lt;- data.frame(IDs = c(001, 001, 001, 
                           002, 002, 002, 002, 
                           003, 003, 003, 003, 
                           004, 004, 004, 004, 004, 004, 
                           005, 005, 005, 005, 005, 
                           006, 006, 006, 006,
                           007, 007, 007,
                           008, 008, 
                           009, 009, 
                           010, 010, 010),
                   var1 = c(0102, 0210, 0405, 
                            0318, 0629, 1201,0101, 
                            0923, 0702, 0710, 0801,
                            0203, 0501, 1204, 0516, 0112, 1005, 
                            1101, 1125, 1020, 0112, 0310,
                            0203, 0401, 0607, 0811,
                            1010, 1212, 0707,
                            0430, 0428,
                            1030, 1008,
                            0501, 0511, 0601),
                   var2 = c(&quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, 
                            &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;,
                            &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, 
                            &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;,   
                            &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;,
                            &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, 
                            &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;,
                            &quot;warm&quot;, &quot;warm&quot;,
                            &quot;hot&quot;, &quot;hot&quot;,
                            &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;))
</code></pre>
<p>I've tried using the data splitting packages caret(fx = createDataPartition()) and splitTools (fx = partition()), as well as dplyr sampling functions but the grouping they apply ensures that each ID appears in all sets.</p>
<p>It's okay to reduce the dataset. Here is one of many attempts guided by an existing Stack Overflow question:</p>
<pre><code>assignments &lt;- data %&gt;%
        select(IDs, var2) %&gt;%
        distinct(IDs) %&gt;%
        rowwise() %&gt;%
        mutate(Group= sample(c(&quot;validation&quot;, &quot;training&quot;, &quot;test&quot;), 1, 
                             prob = c(0.70, 0.20, 0.10)))
data %&gt;%
  left_join(assignments, data, by = &quot;IDs&quot;)
</code></pre>
<p>This attempt neglects that the probability argument is *not setting a proportion. It also doesn't ensure that all levels appear in training, validation, and test sets.</p>
","0","Question"
"78323867","","<p>I am preparing to train a Facebook ResNet DETR model on a custom dataset for detecting signatures in images (I have only 1 class in my dataset). I'm uncertain about the appropriate value to assign to the num_labels parameter in the model configuration. Given the context, should this value be set to 1 (since I'm only detecting signatures), or I should add the 2nd label for the cases where there is no any signatures?</p>
<p>Here is the code</p>
<p><code>model = DetrForObjectDetection.from_pretrained(pretrained_model_name_or_path=CHECKPOINT,num_labels=????,ignore_mismatched_sizes=True)</code></p>
","0","Question"
"78324647","","<p>I tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error:</p>
<blockquote>
<p>ValueError: Samples can not be a single string. The input must be an iterable over iterables of strings.</p>
</blockquote>
<pre><code>from sklearn.feature_extraction import FeatureHasher

hash_vector_size = 50
fh = FeatureHasher(n_features=hash_vector_size, input_type='string')
hashed_df = pd.DataFrame(fh.transform(X_train[&quot;Item_Identifier&quot;]).toarray(),
                         columns=['H'+str(i) for i in range (hash_vector_size)])
</code></pre>
<p>I was expecting a dataframe of 50 columns where the data would have been hashed in</p>
","2","Question"
"78325995","","<p>I am trying to implement human detection using SVM. I am using HOG feature extraction and then applying SVM on it. When I apply linear SVM I will get score of an image but in RBF kernal SVM I only get 0 and 1. Is there anyway i can get score of a detection? How can i see coefficients of RBF kernal like linear SVM?</p>
<p>I tried SVM libraries which are provided by python.</p>
","0","Question"
"78327535","","<p>I am trying to import linear model from scikit-learn into my python code in vscode and get an unexpected error message.</p>
<pre><code>import sklearn
from sklearn import linear_model
</code></pre>
<p>the error:</p>
<pre><code>cannot import name 'METRIC_MAPPING64' from 'sklearn.metrics._dist_metrics'
</code></pre>
<p>I am not trying to import these metrics, how to solve this?</p>
<p>The scikit-learn version used is 1.1.3.</p>
","1","Question"
"78328211","","<p>I am having trouble in applying a SARIMA model to my data set in Python - I am using store sales data of a department store and want to forecast the next year split into quarters. The data has stationarity and I have cleansed to split into quarters of the historical data. Data source goes to 31/12/2017.</p>
<p>See below the Python code and the output</p>
<pre><code>from statsmodels.tsa.statespace.sarimax import SARIMAX


model = SARIMAX(quarterly_sales, order=order, seasonal_order=seasonal_order)
results = model. Fit()

forecast = results.get_forecast(steps=4)
forecast_index = pd.date_range(start='2013-01-01', periods=4, freq='Q')
forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)


print(forecast_series)

# Plot the historical quarterly sales data
quarterly_sales.plot(kind='bar', figsize=(10, 6), label='Historical Quarterly Sales')

# Check if the forecast index aligns with the expected future quarters
print(forecast_series.index)

# Overlay the forecasted sales with more visibility
plt.plot(forecast_series.index, forecast_series, color='red', marker='o', linestyle='dashed', linewidth=2, label='Forecasted Quarterly Sales')


plt.ylim(0, max(quarterly_sales.max(), forecast_series.max()) * 1.1)


plt.title('Quarterly Sales with Forecast')
plt.xlabel('Quarter')
plt.ylabel('Sales')
plt.xticks(rotation=45)


plt.legend()

plt.show()


new_index = [f&quot;Q{date.quarter} {str(date.year)[-2:]}&quot; for date in forecast_series.index]
forecast_series.index = new_index

import matplotlib.pyplot as plt


historical_index = [f&quot;Q{date.quarter} {str(date.year)[-2:]}&quot; for date in quarterly_sales.index]
quarterly_sales.index = historical_index


quarterly_sales.plot(kind='bar', figsize=(10, 6), label='Historical Quarterly Sales')


plt.plot(forecast_series.index, forecast_series, color='red', marker='o', linestyle='dashed', label='Forecasted Quarterly Sales')

plt.title('Quarterly Sales with Forecast')
plt.xlabel('Quarter')
plt.ylabel('Sales')
plt.xticks(rotation=45)


plt.legend()

plt.show()
</code></pre>
<p>AS above is what I have tried but I cannot see any of my forecast on my visualisation graph as specified split into quarters for the upcoming 12 months despite it showing on the screenshot on the legend.</p>
","0","Question"
"78328539","","<p>I'm working with Huggingface in Python to make inference with specific LLM text generation models. So far I used pipelines like this to initialize the model, and then insert input from a user and retrieve the response:</p>
<pre><code>import torch
from transformers import pipeline
print(torch.cuda.is_available())

generator = pipeline('text-generation', model='gpt2', device=&quot;cuda&quot;)
#Inference code
</code></pre>
<p>However, when I change <code>gpt2</code> with <code>google/gemma-2b-it</code> or some other models, it might ask for authentication or directly it thwors an error indicating it´s not available from <code>pipeline()</code>.</p>
<p>I know some models need specific tokenizers and dependencies, but, is there any way to list all available models from <code>pipeline()</code>? And is there any way I can use other models inside <code>pipeline()</code> with all its dependencies without needing to import or use them inside the script?</p>
","0","Question"
"78332079","","<p>I have fetched Amazon Reviews for a product and now trying to train a logistic regression model on it to categorize customer reviews. It gives 100 percent accuracy. I am unable to understand the issue. Here is a sample from my dataset:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Stars</th>
<th>Title</th>
<th>Date</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dipam</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>A very good fragrance. Recommended Seller - Sun Fragrances</td>
</tr>
<tr>
<td>sanket shah</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>Yes</td>
</tr>
<tr>
<td>Manoranjidham</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>This perfume is ranked No 3 .. Good one :)</td>
</tr>
<tr>
<td>Moukthika</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>I was gifted Versace Bright on my 25th Birthday. Fragrance stays for at least for 24 hours. I love it. This is one of my best collections.</td>
</tr>
<tr>
<td>megh</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>I have this perfume but didn't get it online..the smell is just amazing.it stays atleast for 2 days even if you take bath or wash d cloth. I have got so many compliments..</td>
</tr>
<tr>
<td>riya</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>Bought it from somewhere else,awesome fragrance, pure rose kind of smell stays for long,my guy loves this purchase of mine n fragrance too.</td>
</tr>
<tr>
<td>manisha.chauhan0091</td>
<td>5</td>
<td>5.0 out of 5 stars</td>
<td>N/A</td>
<td>Its light n long lasting i like it</td>
</tr>
<tr>
<td>UPS</td>
<td>1</td>
<td>1.0 out of 5 stars</td>
<td>N/A</td>
<td>Absolutely fake. Fragrance barely lasts for 15 minutes. Extremely harsh on the skin as well.</td>
</tr>
<tr>
<td>sanaa</td>
<td>1</td>
<td>1.0 out of 5 stars</td>
<td>N/A</td>
<td>a con game. fake product. dont fall for it</td>
</tr>
<tr>
<td>Juliana Soares Ferreira</td>
<td>N/A</td>
<td>Ótimo produto</td>
<td>N/A</td>
<td>Produto verdadeiro, com cheio da riqueza, não fixa muito, mas é delicioso. Dura na minha pele umas 3 horas e depois fica um cheirinho leve...Super recomendo</td>
</tr>
</tbody>
</table></div>
<p>Here is my code</p>
<pre><code>import re
import nltk
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

# Ensure necessary NLTK datasets and models are downloaded
# nltk.download('punkt')
# nltk.download('vader_lexicon')

# Load the data
df = pd.read_csv(&quot;reviews.csv&quot;)  # Make sure to replace 'reviews.csv' with your actual file path

# Preprocess data
df['Stars'] = df['Stars'].fillna(3.0)  # Handle missing values
df['Title'] = df['Title'].str.lower()  # Standardize text formats
df['Description'] = df['Description'].str.lower()
df = df.drop(['Name', 'Date'], axis=1)  # Drop unnecessary columns
print(df)


# Categorize sentiment based on star ratings
def categorize_sentiment(stars):
    if stars &gt;= 4.0:
        return 'Positive'
    elif stars &lt;= 2.0:
        return 'Negative'
    else:
        return 'Neutral'


df['Sentiment'] = df['Stars'].apply(categorize_sentiment)


# Clean and tokenize text
def clean_text(text):
    text = BeautifulSoup(text, &quot;html.parser&quot;).get_text()
    letters_only = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, text)
    return letters_only.lower()


def tokenize(text):
    return word_tokenize(text)


df['Clean_Description'] = df['Description'].apply(clean_text)
df['Tokens'] = df['Clean_Description'].apply(tokenize)

# Apply NLTK's VADER for sentiment analysis
sia = SentimentIntensityAnalyzer()


def get_sentiment(text):
    score = sia.polarity_scores(text)
    if score['compound'] &gt;= 0.05:
        return 'Positive'
    elif score['compound'] &lt;= -0.05:
        return 'Negative'
    else:
        return 'Neutral'


df['NLTK_Sentiment'] = df['Clean_Description'].apply(get_sentiment)
print(&quot;df['NLTK_Sentiment'].value_counts()&quot;)
print(df['NLTK_Sentiment'].value_counts())

# Prepare data for machine learning
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(tokenizer=tokenize)
X = vectorizer.fit_transform(df['Clean_Description'])
y = df['NLTK_Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=80)

# Train a Logistic Regression model

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))
print(f&quot;class_weights_dict {class_weights_dict}&quot;)
# Apply to Logistic Regression
# model = LogisticRegression(class_weight=class_weights_dict)
model = LogisticRegression(C=0.001, penalty='l2', class_weight='balanced')

model.fit(X_train, y_train)

# Predict sentiments on the test set
predictions = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions, average='weighted')
recall = recall_score(y_test, predictions, average='weighted')
f1 = f1_score(y_test, predictions, average='weighted')

print(f&quot;Accuracy: {accuracy:.4f}&quot;)
print(f&quot;Precision: {precision:.4f}&quot;)
print(f&quot;Recall: {recall:.4f}&quot;)
print(f&quot;F1 Score: {f1:.4f}&quot;)
</code></pre>
<p>Here are the results of the print statements:</p>
<blockquote>
<p>NLTK_Sentiment<br />
Positive    8000<br />
Negative    2000<br />
Name: count, dtype: int64</p>
</blockquote>
<blockquote>
<p>class_weights_dict {0: 2.3696682464454977, 1: 0.6337135614702155}<br />
Accuracy: 1.0000<br />
Precision: 1.0000<br />
Recall: 1.0000<br />
F1 Score: 1.0000</p>
</blockquote>
<p>I am unable to find the reason why my model is always giving 100 percent accuracy.</p>
","0","Question"
"78333191","","<p>I've built a predictive model for predicting outcomes based on certain features in the supplied data.</p>
<p>The model is a tabular learner utilizing fastai.</p>
<p>The dataset consists of about 300 records split for training, validation and testing sets.</p>
<p>I've implemented techniques to address overfitting such as early stopping and weight decay, but the model still appears to be overfitting when evaluated on unseen data.</p>
<p>Furthermore, I've also tried to adjust hyperparameters such as learning rate and batch size without improvement. I suspect there might be some aspects of my model's architecture or preprocessing pipeline that could be contributing to the problem, but I'm not sure where to start investigating.</p>
<p>Given the sensitive nature of the project, I'm unable to provide specific details about the dataset or the prediction task, but I can share the preprocessing and structure of the model as it currently stands.</p>
<p>Here's output of the training:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.752707</td>
<td>0.579501</td>
<td>0.776119</td>
<td>00:00</td>
</tr>
<tr>
<td>1</td>
<td>0.699270</td>
<td>0.833771</td>
<td>0.776119</td>
<td>00:00</td>
</tr>
<tr>
<td>2</td>
<td>0.652438</td>
<td>0.598243</td>
<td>0.791045</td>
<td>00:00</td>
</tr>
<tr>
<td>3</td>
<td>0.621083</td>
<td>3.889398</td>
<td>0.776119</td>
<td>00:00</td>
</tr>
<tr>
<td>4</td>
<td>0.591348</td>
<td>0.632366</td>
<td>0.791045</td>
<td>00:00</td>
</tr>
<tr>
<td>5</td>
<td>0.580582</td>
<td>6.670314</td>
<td>0.791045</td>
<td>00:00</td>
</tr>
</tbody>
</table></div>
<blockquote>
<p>No improvement since epoch 2: early stopping</p>
</blockquote>
<p>Here's the code for preprocessing (after I've built features which I can't divulge).</p>
<p>The <code>features</code> list defines each feature including a valid value range and weights (<code>feature</code>, <code>range_</code>, and <code>weight</code> as used in the normalize function below).</p>
<pre><code>def custom_normalize(df, feature, range_, weight):
    df[feature] = normalize(df[feature], range_)
    df[feature] = df[feature] * weight
    return df

splits = RandomSplitter(valid_pct=0.2)(range_of(df))

procs = [Categorify, FillMissing]

for feature, info in features.items():
    # Determine a range within which to select values when training.
    procs.append(partial(custom_normalize, feature=feature, range_=info['range'], weight=info['weight']))
</code></pre>
<p>Building the model and training is pretty standard as far as I know:</p>
<pre><code>to = TabularPandas(df, procs=procs,
                   cat_names = cat_vars,
                   cont_names = cont_vars,
                   y_names=dep_var,
                   splits=splits)

dls = to.dataloaders(bs=64)

early_stop = EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=3)

learn = tabular_learner(dls, metrics=accuracy, wd=0.1)
learn.lr_find()

# Plot learning rate.
learn.recorder.plot_lr_find()

# Choose a learning rate based on the plot.
lr = learn.recorder.lrs[np.argmin(learn.recorder.losses)]

learn.fit_one_cycle(15, lr, cbs=early_stop)
learn.show_results()

# Only save model if none exists
# TODO wrap save in conditional that prevents saving if a model exists.
if not os.path.exists(model_fname):
    learn.save(model_fname)
</code></pre>
","1","Question"
"78334661","","<p>It might be duplicated from <a href=""https://stackoverflow.com/questions/43702514/how-to-get-each-individual-trees-prediction-in-xgboost"">How to get each individual tree&#39;s prediction in xgboost?</a> but the solution no longer works (possibly to changes on XGBoost library). My idea is to dump the model in a raw format <code>model.get_booster().get_dump()</code> and implement it in a different platform (prediction only). However, I'm first trying to implement it in python. Running the following code, making predictions using all the individual booster and combining them, does not return the same result as the <code>model.predict()</code> function.
Is there any way I can match the <code>model.predict()</code> with the combination of the boosters? What am I missing?</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import xgboost as xgb
from sklearn import datasets
from scipy.special import expit as sigmoid, logit as inverse_sigmoid

# Load data
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# Fit a model
model = xgb.XGBClassifier(
    n_estimators=10,
    max_depth=10,
    use_label_encoder=False,
    objective='binary:logistic'
)
model.fit(X, y)
booster_ = model.get_booster()

# Extract indivudual predictions
individual_preds = []
for tree_ in booster_:
    individual_preds.append(
        tree_.predict(xgb.DMatrix(X))
    )
individual_preds = np.vstack(individual_preds)

# Aggregated individual predictions to final predictions
indivudual_logits = inverse_sigmoid(individual_preds)
final_logits = indivudual_logits.sum(axis=0)
final_preds = sigmoid(final_logits)

# Verify correctness
xgb_preds = booster_.predict(xgb.DMatrix(X))
np.testing.assert_almost_equal(final_preds, xgb_preds)
</code></pre>
<blockquote>
<p>AssertionError: Arrays are not almost equal to 7 decimals
Mismatched elements: 150 / 150 (100%)
Max absolute difference: 0.90511334
Max relative difference: 0.99744916
x: array([7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,
7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,
7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,...
y: array([0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,...</p>
</blockquote>
","1","Question"
"78337383","","<p>I am doing a project where I am trying to predict user scores (reviews) of books by plotting the sentiment of the sentences in the book.</p>
<p><a href=""https://i.sstatic.net/KPok4.png"" rel=""nofollow noreferrer"">A graph to give you an idea:</a></p>
<p>Red is the average sentiment plot of the highest scoring 25% of books,
blue the worst.</p>
<p>As you can see the books start out pretty medium, drop before the end, and all have high sentiment in the very end. As you can also see, the best scoring 25% (red), reaches the highest high at the end.</p>
<p>What I want to do is use regression to predict the score of a book based on a vector containing the sentiment scores of every sentence in the book.</p>
<p>I have tried a few things, but nothing works.</p>
<p>My idea was to split all books into 100 parts, take the average of these 100 parts for every book, and train a Support Vector Regression model (with poly kernel) on this data. However it does not perform better than just predicting the mean score everytime.</p>
<p>So:</p>
<p>1 Independent variable = [avg_sentiment1,avg_sentiment2,...avg_sentiment100]
1 Dependendent variable = score (a number between 1 and 5, or more specifically, in our dataset, between ~3.200 and ~4.700)</p>
<p>So while fitting the sklearn SVR model using this setup does not give any errors, it just does not seem to learn (it performs worse than a dummy predictor that always predicts the mean). I have tried a few different Regression models (Ridge, SVR with different kernels, adding a SplineTransformer) with different parameters. None do better than random.</p>
<p>And all examples of regression I can find online seem to use either singular value predicting singular value (so independent variable age = 12, dependent variable height = 160 cm, stuff like that), or at most multiple variables (adding more singular values, like weight = 67 (kg), as independent variables).</p>
<p>Is my regression interpreting my 100 number vector as 100 unrelated variables? Does that matter?</p>
<p>Help, I am out of my depth here. What kind of technique would be most applicable here? Preferably something I can find on SKLearn, I am not an expert (as you can probably tell)</p>
","0","Question"
"78337397","","<p>I am attempting to build a chess bot which learns using Proximal Policy Optimization. I am currently using the python-chess library (<a href=""https://python-chess.readthedocs.io/en/latest/index.html#"" rel=""nofollow noreferrer"">https://python-chess.readthedocs.io/en/latest/index.html#</a>) as the environment where my agent plays games against itself and learns. The issue I am facing is extremely slow games for training. With a move limit of 200 per game, my bot can play 1 game against itself in about 1 second. This 1 second also includes the PPO part of the training which takes on average 0.01 seconds with a GPU.</p>
<p>I am using PyTorch, so I have already moved all tensors to the GPU. I have not found any other ways beyond this to speed up the execution time.</p>
<p>I would like to get the execution time down for playing the games to around 0.5 or less seconds per game, but I haven't been able to find a way to accomplish this.</p>
<p>I would greatly appreciate the feedback and help if anyone knows of a possible solution.</p>
","0","Question"
"78340023","","<p>I'm new to Pyspark and Databricks and trying to create a Logistic regression model (going through the Spark_DS&amp;ML_exercise provided by Databrticks itself). After fitting the model to my training data. I'm trying to get the f-measure from the summary by threshold.</p>
<p>I ran the following code:</p>
<pre><code>from pyspark.ml.classification import LogisticRegression

# Create initial LogisticRegression model
lr = LogisticRegression(labelCol=&quot;label&quot;, featuresCol=&quot;features&quot;, maxIter=10)

# set threshold for the probability above which to predict a 1
lr.setThreshold(train_positive_rate)
# lr.setThreshold(0.5) # could use this if knew you had balanced data

# Train model with Training Data
lrModel = lr.fit(train)

# get training summary used for eval metrics and other params
lrTrainingSummary = lrModel.summary

# Find the best model threshold if you would like to use that instead of the empirical positve rate
fMeasure = lrTrainingSummary.fMeasureByThreshold
</code></pre>
<p>But I'm getting this AttributeError:</p>
<pre><code>AttributeError: 'LogisticRegressionTrainingSummary' object has no attribute 'fMeasureByThreshold'
</code></pre>
<p>It seems the <code>fMeasureByThreshold</code> doesn't exist anymore. Is it the case?</p>
","0","Question"
"78340927","","<p>I'm Currently Facing a problem when it comes to use the keras-rl2 with tensorflow and i dont know why, I just search on the internet and the keras-rl2, tensorflow, and keras documentation and didn't found the solution.</p>
<p>Currently, I want to use the keras-rl2 with the newest version of tensorflow which is 2.16.1 and keras 3 and got some error like this when using</p>
<pre><code>from rl.agents import DKQAgent
</code></pre>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
Cell In[37], line 1
----&gt; 1 import rl.agents
      3 print(&quot;RL agents library version:&quot;, rl.agents.__version__)

File D:\Anaconda\Lib\site-packages\rl\agents\__init__.py:2
      1 from .dqn import DQNAgent, NAFAgent, ContinuousDQNAgent
----&gt; 2 from .ddpg import DDPGAgent
      3 from .cem import CEMAgent
      4 from .sarsa import SarsaAgent, SARSAAgent

File D:\Anaconda\Lib\site-packages\rl\agents\dqn.py:8
      5 from tensorflow.keras.layers import Lambda, Input, Layer, Dense
      7 from rl.core import Agent
----&gt; 8 from rl.policy import EpsGreedyQPolicy, GreedyQPolicy
      9 from rl.util import *
     12 def mean_q(y_true, y_pred):

File D:\Anaconda\Lib\site-packages\rl\core.py:8
      4 import numpy as np
      5 from tensorflow.keras.callbacks import History
      7 from rl.callbacks import (
----&gt; 8     CallbackList,
      9     TestLogger,
     10     TrainEpisodeLogger,
     11     TrainIntervalLogger,
     12     Visualizer
     13 )
     16 class Agent:
     17     &quot;&quot;&quot;Abstract base class for all implemented agents.
     18 
     19     Each agent interacts with the environment (as defined by the `Env` class) by first observing the
   (...)
     37         processor (`Processor` instance): See [Processor](#processor) for details.
     38     &quot;&quot;&quot;

File D:\Anaconda\Lib\site-packages\rl\callbacks.py:12
      9 from tensorflow.python.keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList
     10 from tensorflow.python.keras.utils.generic_utils import Progbar
---&gt; 12 class Callback(KerasCallback):
     13     def _set_env(self, env):
     14         self.env = env

ModuleNotFoundError: No module named 'keras.utils.generic_utils'
</code></pre>
<p>and when I thought I just need to downgrade it to some version like 2.13.0 and keras 2.13.0 it still error like this</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[18], line 1
----&gt; 1 from rl.agents.dqn import DQNAgent

File D:\Anaconda\envs\AI\Lib\site-packages\rl\agents\__init__.py:1
----&gt; 1 from .dqn import DQNAgent, NAFAgent, ContinuousDQNAgent
      2 from .ddpg import DDPGAgent
      3 from .cem import CEMAgent

File D:\Anaconda\envs\AI\Lib\site-packages\rl\agents\dqn.py:7
      4 from tensorflow.keras.models import Model
      5 from tensorflow.keras.layers import Lambda, Input, Layer, Dense
----&gt; 7 from rl.core import Agent
      8 from rl.policy import EpsGreedyQPolicy, GreedyQPolicy
      9 from rl.util import *

File D:\Anaconda\envs\AI\Lib\site-packages\rl\core.py:7
      4 import numpy as np
      5 from tensorflow.keras.callbacks import History
----&gt; 7 from rl.callbacks import (
      8     CallbackList,
      9     TestLogger,
     10     TrainEpisodeLogger,
     11     TrainIntervalLogger,
     12     Visualizer
     13 )
     16 class Agent:
     17     &quot;&quot;&quot;Abstract base class for all implemented agents.
     18 
     19     Each agent interacts with the environment (as defined by the `Env` class) by first observing the
   (...)
     37         processor (`Processor` instance): See [Processor](#processor) for details.
     38     &quot;&quot;&quot;

File D:\Anaconda\envs\AI\Lib\site-packages\rl\callbacks.py:8
      6 import numpy as np
      7 import tensorflow as tf
----&gt; 8 from tensorflow.keras import __version__ as KERAS_VERSION
      9 from tensorflow.python.keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList
     10 from tensorflow.python.keras.utils.generic_utils import Progbar

ImportError: cannot import name '__version__' from 'tensorflow.keras' (D:\Anaconda\envs\AI\Lib\site-packages\keras\api\_v2\keras\__init__.py)
</code></pre>
<p>Can Anyone Give me an Explanation or Solution why it is always Error?</p>
<p>Thank you for your concern</p>
","0","Question"
"78343238","","<p>I am building a model that can predict that a patient is diagnose with chronic kidney disease
Below are the steps I used;</p>
<ul>
<li>import the neccessary libraries for the training model and the dataset</li>
<li>impute the numerical columns with a random value and the categorical columns with the mode</li>
<li>perform oridinal encoding for the categorical columns</li>
<li>split the dataset into train and test values</li>
<li>there was a bias values in the dataset so I use SMOTE library to generate some dummy records to make the data balanced</li>
<li>I use decision tree classifier for the model</li>
</ul>
<p>Here is my code;</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
import warnings
warnings.filterwarnings(action='ignore')


df = pd.read_csv('kidney_disease.csv')
cat_cols = [col for col in df.columns if df[col].dtype == 'object']
num_cols = [col for col in df.columns if df[col].dtype != 'object']

from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()

df_copied = df.copy()
X_enc = enc.fit_transform(df_copied[cat_cols])
df_copied[cat_cols] = pd.DataFrame(X_enc, columns=cat_cols)

x = df_copied.drop(['classification'], axis=1)
y = df_copied['classification']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

dtc = grid_search_dtc.best_estimator_

# accuracy score, confusion matrix and classification report of decision tree

dtc_acc = accuracy_score(y_test, dtc.predict(X_test))

dtcClassifier = 'dtc_model.pkl'
with open(dtcClassifier, 'wb') as file:
    pickle.dump(dtc, file)

</code></pre>
<p>I use streamlit library to implement my model</p>
<pre><code>with open('dtc_model.pkl', 'rb') as file:  
    model = pickle.load(file)

# Define the column names
cols = [&quot;age&quot;, &quot;bp&quot;, &quot;sg&quot;, &quot;al&quot;, &quot;su&quot;, &quot;rbc&quot;, &quot;pc&quot;, &quot;pcc&quot;, &quot;ba&quot;, &quot;bgr&quot;, &quot;bu&quot;, &quot;sc&quot;, &quot;sod&quot;, &quot;pot&quot;, &quot;hemo&quot;, &quot;pcv&quot;, &quot;wc&quot;, &quot;rc&quot;, &quot;htn&quot;, &quot;dm&quot;, &quot;cad&quot;, &quot;appet&quot;, &quot;pe&quot;, &quot;ane&quot;]

def main():
    st.title(&quot;Kidney Disease Prediction Using Hybrid Model&quot;)
  
   
        
    html_temp = &quot;&quot;&quot;
    &lt;div style=&quot;background:#025246 ;padding:10px&quot;&gt;
    &lt;h2 style=&quot;color:white;text-align:center;&quot;&gt;KD Prediction App &lt;/h2&gt;
    &lt;/div&gt;
    &quot;&quot;&quot;
    st.markdown(html_temp, unsafe_allow_html = True)
    
    # Define input fields
    age = st.text_input(&quot;Age&quot;, 0)
    bp = st.text_input(&quot;Blood Pressure&quot;, 0)
    sg = st.selectbox(&quot;Specific Gravity&quot;, [0, 1.005, 1.010, 1.015, 1.020, 1.025])
    al = st.selectbox(&quot;Albumin&quot;, [0, 1, 2, 3, 4, 5])
    su = st.selectbox(&quot;Sugar&quot;, [0, 1, 2, 3, 4, 5])
    rbc = st.selectbox(&quot;Red Blood Cells&quot;, [&quot;&quot;, &quot;normal&quot;, &quot;abnormal&quot;])
    pc = st.selectbox(&quot;Pus Cell&quot;, [&quot;&quot;, &quot;normal&quot;, &quot;abnormal&quot;])
    pcc = st.selectbox(&quot;Pus Cell clumps&quot;, [&quot;&quot;, &quot;present&quot;, &quot;notpresent&quot;])
    ba = st.selectbox(&quot;Bacteria&quot;, [&quot;&quot;, &quot;present&quot;, &quot;notpresent&quot;])
    bgr = st.text_input(&quot;Blood Glucose Random&quot;, 0)
    bu = st.text_input(&quot;Blood Urea&quot;, 0)
    sc = st.text_input(&quot;Serum Creatinine&quot;, 0)
    sod = st.text_input(&quot;Sodium&quot;, 0)
    pot = st.text_input(&quot;Potassium&quot;, 0)
    hemo = st.text_input(&quot;Hemoglobin&quot;, 0)
    pcv = st.text_input(&quot;Packed Cell Volume&quot;, 0)
    wc = st.text_input(&quot;White Blood Cell Count&quot;, 0)
    rc = st.text_input(&quot;Red Blood Cell Count&quot;, 0)
    htn = st.selectbox(&quot;Hypertension&quot;, [&quot;&quot;, &quot;yes&quot;, &quot;no&quot;])
    dm = st.selectbox(&quot;Diabetes Mellitus&quot;, [&quot;&quot;, &quot;yes&quot;, &quot;no&quot;])
    cad = st.selectbox(&quot;Coronary Artery Disease&quot;, [&quot;&quot;, &quot;yes&quot;, &quot;no&quot;])
    appet = st.selectbox(&quot;Appetite&quot;, [&quot;&quot;, &quot;good&quot;, &quot;poor&quot;])
    pe = st.selectbox(&quot;Pedal Edema&quot;, [&quot;&quot;, &quot;yes&quot;, &quot;no&quot;])
    ane = st.selectbox(&quot;Anemia&quot;, [&quot;&quot;, &quot;yes&quot;, &quot;no&quot;])
    
   
    
    if st.button(&quot;Predict&quot;):
        
        # Convert data to DataFrame
        data = {
                'age': int(age), 
                'bp': float(bp), 
                'sg': sg, 
                'al': al, 
                'su': su, 
                'rbc': rbc, 
                'pc': pc,
                'pcc': pcc,
                'ba': ba, 
                'bgr': float(bgr), 
                'bu': float(bu), 
                'sc': float(sc), 
                'sod': float(sod), 
                'pot': float(pot), 
                'hemo': float(hemo), 
                'pcv': float(pcv), 
                'wc': float(wc), 
                'rc': float(rc), 
                'htn': htn, 
                'dm': dm, 
                'cad': cad, 
                'appet': appet, 
                'pe': pe, 
                'ane': ane
        }
        
        df = pd.DataFrame([data], columns=cols)
        # Check if all input fields are filled
        # Convert data to DataFrame
       
        
        
        cat_cols = [col for col in df.columns if df[col].dtype == 'object']
        enc = OrdinalEncoder()

        df_copied = df.copy()
        df_copied[cat_cols] = enc.fit_transform(df_copied[cat_cols])
        
        print(pd.DataFrame(df_copied))
        
        prediction = model.predict(df_copied)
        print(prediction[0])
        
        # Display prediction result
        if prediction[0] == 1:
                st.write(&quot;Positive&quot;)
        else:
                st.write(&quot;Negative&quot;)



if __name__ == '__main__':
    main()
</code></pre>
<p>When I print the df_copied in Dataframe format, I notice that the cat_cols is always 0</p>
<p>I tried LabelEncoder() but is the same</p>
","0","Question"
"78343327","","<p>I'm new to machine learning, but I have experience in C#, which is why I want to pick up Ml.net to familiarize myself with the topic.
As I grasp the concept, I have decided to make a movie recommendation model that will take previous reviews and recommend a movie with the same example as available on Microsoft ML.NET tutorials.
My issue is that when I build and train a model in the function below while creating a pipeline, I don't see a method call of Recommendation(). By the way, I am using VS2022 and .NET 8.0. Can anyone suggest why?</p>
<pre><code>using Microsoft.ML;
using Microsoft.ML.Trainers;
using Microsoft.ML.Data;
using Microsoft.ML.Transforms;

ITransformer BuildAndTrainModel(MLContext mlContext, IDataView trainingDataView)
{
    //STEP 3: Transform your data by encoding the two features userId and movieID. These encoded features will be provided as input
    //        to our MatrixFactorizationTrainer.
    var dataProcessingPipeline = mlContext.Transforms.Conversion.MapValueToKey(outputColumnName: &quot;userIdEncoded&quot;, inputColumnName: nameof(MovieRating.userId))
                   .Append(mlContext.Transforms.Conversion.MapValueToKey(outputColumnName: &quot;movieIdEncoded&quot;, inputColumnName: nameof(MovieRating.movieId)));

    //Specify the options for MatrixFactorization trainer            
    MatrixFactorizationTrainer.Options options = new MatrixFactorizationTrainer.Options();
    options.MatrixColumnIndexColumnName = &quot;userIdEncoded&quot;;
    options.MatrixRowIndexColumnName = &quot;movieIdEncoded&quot;;
    options.LabelColumnName = &quot;Label&quot;;
    options.NumberOfIterations = 20;
    options.ApproximationRank = 100;

    //STEP 4: Create the training pipeline 
    var trainingPipeLine = dataProcessingPipeline.Append(mlContext.Recommendation().Transforms.MatrixFactorization(options));

   

    Console.WriteLine(&quot;=============== Training the model ===============&quot;);
    ITransformer model = trainingPipeLine.Fit(trainingDataView);

    return model;
}

</code></pre>
<p>I did a few searches in Github code, but I am getting no pointers.</p>
","0","Question"
"78343980","","<p>I'm new to the general stack overflow community and especially new to the Tensorflow library (1 week as of now). So I'm positive I am missing something here, just can't quite figure it out myself.</p>
<p>I have done a few practices utilizing Tensorflow's example dataset and a few miscellaneous tutorials. I found myself attempting to apply one of the tutorial's code to my own dataset, this dataset is a csv and the model should return it's prediction of the &quot;Last Result&quot; column. This turned out to be a bit much because I quickly encountered a problem, the &quot;predict&quot; function always predicts identical values for every data entry.</p>
<p>[[0.6335701]
[0.6335701]
...
[0.6335701]
[0.6335701]]</p>
<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from keras.models import Sequential, load_model
from keras.layers import Dense
from sklearn.metrics import accuracy_score

df = pd.read_csv('*.csv')
x = pd.get_dummies(df.drop(['Last Result'], axis=1))
y = df['Last Result'].apply(lambda X: 1 if X == 'Registered' else 0)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)
x_train.head()
print(&quot;x train: \n&quot;, x_train)
y_train.head()
print(&quot;y train: \n&quot;, y_train)

# Below is code to create a new TensorFlow Model. If you need to call a saved model, see 'load model' below.

model = Sequential()
model.add(Dense(units=32, activation='relu', input_dim=len(x_train.columns)))
model.add(Dense(units=32, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=100, batch_size=128)

# model.save('*.keras')

# This is code to recall a previously saved TensorFlow Model

# model = load_model('*.keras')

y_hat = model.predict(x_test)
# y_hat = [0 if val &lt; 0.5 else 1 for val in y_hat]
print(y_hat)

# print(accuracy_score(y_test, y_hat))
</code></pre>
<p>I have tried other suggestions from similar posts to no avail. My best understanding of the issue is that the model is not learning anything and is instead just guessing all one value because one of the expected values is true ~60% of the time, thus the ~.60 predictions.</p>
<p>Any help on the matter would mean a great deal in assisting my understanding of the library. Thanks in advance and thank you for your time.</p>
","0","Question"
"78344537","","<p>I've been trying to construct a LightGBM Dataset in Python using a reference Dataset (called ref_dataset) in a streaming manner. I'm not sure how it's done, and it involves calling what appear to be non-public methods in the Dataset class.</p>
<p>I've tried:</p>
<pre><code>label_column = &quot;label&quot;
weight_column = &quot;weight&quot;
ref_dataset = lightgbm.Dataset(
   sample_df.drop(columns=[label_column, weight_column])
   label=sample_df[label_column],
   weight=sample_df[weight_column],
   params=config,
   **(ref_dataset_kwargs or {}),
)
ref_dataset.construct()
temp_dataset = lightgbm.Dataset(None, reference=ref_dataset, params=ref_dataset.get_params())
# train_filenames_and_part_infos is just a list of tuple[filename, part_info_dict]
estimated_num_rows = sum(
    part_info[&quot;num_rows&quot;] for _, part_info in train_filenames_and_part_infos
)
temp_dataset._init_from_ref_dataset(estimated_num_rows, ref_dataset._handle)

weights_list = []
labels_list = []
# This loop is not actually my code, which is more complicated, but basically what it does
for filename, _ in train_filenames_and_part_infos:
    tbl: pyarrow.Table = load_from_file(filename)
    labels = tbl[label_column].to_pandas().to_numpy()
    weights = tbl[weight_column].to_pandas().to_numpy()

    labels_list.append(labels)
    weights_list.append(weights)
    tbl = tbl.drop_columns([label_column, weight_column])
    np_array: np.ndarray = tbl.to_pandas().to_numpy()
    if temp_dataset._start_row + np_array.shape[0] &gt; temp_dataset.num_data():
        raise RuntimeError(&quot;Dataset is too small to fit the data&quot;)
    temp_dataset._push_rows(np_array)

all_weights = np.concatenate(weights_list)
all_labels = np.concatenate(labels_list)
actual_length = all_weights.shape[0]
# Unfortunately, the estimate is not exact for various reasons
extra_zeros_features = np.zeros(
     (estimated_num_rows - actual_length, temp_dataset.num_feature()), dtype=np.float32
)
temp_dataset._push_rows(extra_zeros_features)
_LIB.LGBM_DatasetMarkFinished(temp_dataset._handle)
extra_zeros = np.zeros(estimated_num_rows - actual_length, dtype=np.float32)
temp_dataset.set_weight(np.concatenate([all_weights, extra_zeros]))
temp_dataset.set_label(np.concatenate([all_labels, extra_zeros]))

lightgbm.train(
    params=config, # includes network parameters for distributed voting parallel training
    train_set=temp_dataset,
    num_boost_round=100,
    valid_sets=valid_sets, # initialized somewhere else
    valid_names=valid_names, # initialized somewhere else
    init_model=starting_model, # not really necessary
    **lightgbm_train_kwargs, # empty
)
</code></pre>
<p>Unfortunately, when I run this code, I get this console output (some of the lines might be out of order, because I'm actually running this on distributed, and the logs are aggregated; I've done some light editing to remove the intrusive lines):</p>
<pre><code>[LightGBM] [Info] Total Bins 137618
[LightGBM] [Info] Trying to bind port 50627...
[LightGBM] [Info] Binding port 50627 succeeded
[LightGBM] [Info] Listening...
[LightGBM] [Info] Number of data points in the train set: 3934363, number of used features: 1382
[LightGBM] [Info] Connected to rank 0
[LightGBM] [Info] Connected to rank 1
[LightGBM] [Info] Connected to rank 2
[LightGBM] [Info] Connected to rank 3
[LightGBM] [Info] Connected to rank 4
[LightGBM] [Info] Connected to rank 5
[LightGBM] [Info] Connected to rank 6
[LightGBM] [Info] Connected to rank 8
[LightGBM] [Info] Local rank: 7, total number of machines: 9
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 5.318313 seconds.
[LightGBM] [Info] Start training from score -0.000000
</code></pre>
<p>and then it just sits there with idle CPU and network. I don't see it making any progress for hours. I've checked that all of the ranks Is there something I'm doing wrong? How else do I construct with a given sample?</p>
<p>Some more information:
inspecting the stack traces of the idle Python processes reveals that the code is stuck in:</p>
<pre><code>update (lightgbm/basic.py:3891)
train (lightgbm/engine.py:276)
... my code ...
</code></pre>
<p>and for the version of LightGBM I'm using (4.3.0), this corresponds to the code:</p>
<pre><code>_safe_call(_LIB.LGBM_BoosterUpdateOneIter(
                self._handle,
                ctypes.byref(is_finished)))
</code></pre>
<p>Another update:
It appears that the number of bins differs between the workers; some have 137608, 137612, 137616. Is that a problem?</p>
","-1","Question"
"78345096","","<p>I am using OCI's machine learning feature to use association rule model for my data.
I have below settings.</p>
<p>setting = {'ASSO_MIN_SUPPORT':'0.04',
'ASSO_MIN_CONFIDENCE':'0.1',
'ASSO_MAX_RULE_LENGTH': '2',
'ODMS_ITEM_ID_COLUMN_NAME':'PRODUCT_NAME'}</p>
<p>ar_mod = oml.ar(**setting)
ar_mod = ar_mod.fit(SALES_TRANS_CUST, case_id = 'CUST_ID')</p>
<p>and i am getting below error:</p>
<p><code>oracledb.thick_impl._raise_from_info oracledb.exceptions.DatabaseError: ORA-40104: invalid training data for model build</code></p>
<p>I am trying to train a model using fit() method but its giving error.</p>
","-1","Question"
"78347872","","<p>How can I group by two columns using OML4Py oml.group_apply invocation?</p>
<p>For instance in sql I can do the following:</p>
<p>'''
select mgr, count(mgr), count(deptno), deptno from emp
group by mgr, deptno
order by deptno;</p>
<p>which returns
7782    1   1   10
7839    1   1   10
0   1   10
7566    2   2   20
7788    1   1   20
7839    1   1   20
7902    1   1   20
7698    5   5   30
7839    1   1   30
'''</p>
","0","Question"
"78348747","","<p>For example, the apply function works with the pandas DataFrame but but OML dataframes don't support the apply method. I get this error</p>
<p>AttributeError: 'DataFrame' object has no attribute 'apply'</p>
<p>How can I resolve this?</p>
<p>'''
%python</p>
<pre><code>import oml
import pandas as pd

inp = [{'VAL_1':10, 'VAL_2':1}, {'VAL_1':11,'VAL_2':10}, {'VAL_1':12,'VAL_2':0}]
df = pd.DataFrame(inp)
oml_df = oml.push(df)

# The apply function works with the pandas DataFrame:

%python

df['VAL_NEW'] = df.apply(lambda row, arg: row['VAL_1'] + row['VAL_2'] +100, axis=1, args=(100,))
print(df)

# But OML dataframes don't support the apply method:

%python

oml_df['VAL_NEW'] = oml_df.apply(lambda row, arg: row['VAL_1'] + row['VAL_2'] +100, axis=1, args=(100,))

# AttributeError: 'DataFrame' object has no attribute 'apply'
</code></pre>
<p>'''</p>
","0","Question"
"78348894","","<p>In a regression task I'm given the following data:</p>
<ol>
<li>Input vectors with a known label. MSE loss should be used between the precidtion and the label.</li>
<li>Pairs of input vectors without a label, for which it is known that the model should give similar results. MSE loss should be used between the two predictions.</li>
</ol>
<p>What's the right way to <code>fit</code> a Keras model with these two kinds of data simultaneously?</p>
<p>Ideally, I'd like the train loop to iterate the two kinds in an interleaved way - a superivsed (1) batch and then a self-supervised (2) batch, then supervised again etc.</p>
<p>If it matters, I'm using the Jax backend. Keras version 3.2.1.</p>
","3","Question"
"78349854","","<p>I have been experimenting with some very basic data for Alberta power markets, and trying to use a LMST model for time series data to try and predict prices. I do get &quot;possible&quot; results from my model, and it does seem to pick up some volatility that we can expect (just from my own market experience).</p>
<p>However, I am looking for a better understanding of maybe some of the pitfalls I am running into.</p>
<pre><code>from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Dense
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error,mean_squared_error
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Load the data
# Load the data

# Load the data
csv_file_path = 'Frankenstein.csv'  # Update with your actual file path
df = pd.read_csv(csv_file_path)

# Convert 'Date/Time' to datetime and extract components if present in your dataset
if 'Date/Time' in df.columns:
    df['Date'] = pd.to_datetime(df['Date/Time'])
    df['Year'] = df['Date'].dt.year
    df['Month'] = df['Date'].dt.month
    df['Day'] = df['Date'].dt.day
    df['Hour'] = df['Date'].dt.hour
    df.drop(['Date/Time', 'Date'], axis=1, inplace=True)

# Assuming 'Price' is the target variable
features = df.drop(['Price'], axis=1)
target = df['Price']

# Normalize features and target
scaler_features = MinMaxScaler()
features_scaled = scaler_features.fit_transform(features)
scaler_target = MinMaxScaler()
target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))

# Create sequences function
def create_sequences(features, target, time_steps=100):
    X, y = [], []
    for i in range(len(features) - time_steps):
        X.append(features[i:(i + time_steps)])
        y.append(target[i + time_steps])
    return np.array(X), np.array(y)

# Create sequences using the entire dataset
X, y = create_sequences(features_scaled, target_scaled.flatten())

# Model configuration
input_shape = (X.shape[1], X.shape[2])  # (time_steps, num_features)

# Define the LSTM model
model = Sequential([
    LSTM(units=100, return_sequences=True, input_shape=input_shape),
    Dropout(0.1),
    LSTM(units=100),
    Dropout(0.1),
    Dense(units=100, activation='elu'),
    Dense(1)  # Predicting a single value
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model on the entire dataset
history = model.fit(X, y, epochs=150, batch_size=20, validation_split=0.1)

# Plot training &amp; validation loss values
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Save the LSTM model
model_save_path = 'trained_lstm_model.h5'
model.save(model_save_path)
print(f&quot;Model saved to {model_save_path}&quot;)
joblib.dump(scaler_features, 'scaler_features.pkl')
joblib.dump(scaler_target, 'scaler_target.pkl')
</code></pre>
<p>Anyone out there that could give some advice to an absolute beginner? Mostly looking for a better understanding of how I should be setting this up. I have a dataset that is hourly, and is the past three years of historical generation and interchanges. I am looking for ways to make my model more reactive to changes in supply vs price.</p>
","-1","Question"
"78353335","","<p>I have deployed an MLFlow server using <code>mlflow server ...</code>. When visited its deployed URL <code>https://some.url</code> I would see a blank page and couldn't load. Oddly, this is not an issue locally on local host.</p>
<p>Investigating the browser's console I see a bunch of errors giving <code>404</code> for static (<code>.../mlflow/build</code>) files.</p>
<p>What am I doing wrong?</p>
","0","Question"
"78354052","","<p>For a specific use case, I had to deploy a custom model into OpenSearch, First of the all, I exported a HuggingFace Model into TorchScript and had registered successfully then I tried to deploy it but unfortunately deploy operation failed and returns the following error :</p>
<pre><code>    {
  &quot;model_id&quot;: &quot;Guem7I4BM4PGcXAkFYKV&quot;,
  &quot;task_type&quot;: &quot;DEPLOY_MODEL&quot;,
  &quot;function_name&quot;: &quot;TEXT_EMBEDDING&quot;,
  &quot;state&quot;: &quot;FAILED&quot;,
  &quot;worker_node&quot;: [
    &quot;d7zdcxaASDyFKGX7AhlG0g&quot;,
    &quot;5fc7S9yJQQCWarLbAnVESg&quot;,
    &quot;TPu-1KznRzCwj8tQAgHOMQ&quot;
  ],
  &quot;create_time&quot;: 1713367889874,
  &quot;last_update_time&quot;: 1713368035088,
  &quot;error&quot;: &quot;&quot;&quot;
                          {&quot;d7zdcxaASDyFKGX7AhlG0g&quot;:&quot;The following operation failed in the TorchScript interpreter.
                          \nTraceback of TorchScript, serialized code (most recent call last):
                          \n  File \&quot;code/__torch__.py\&quot;, line 12, in forward
                          \n    input_ids = inputs[\&quot;input_ids\&quot;]
                          \n    attention_mask = inputs[\&quot;attention_mask\&quot;]
                          \n    input = inputs[\&quot;token_type_ids\&quot;]
                          \n            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
                          \n    _0 = (model).forward(input_ids, attention_mask, input, )
                          \n    return {\&quot;sentence_embedding\&quot;: _0}
                          \n
                          \nTraceback of TorchScript, original code (most recent call last):
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(1074): trace_module
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(806): trace
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(43): export_to_torchscript
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(52): &lt;module&gt;
                          \nRuntimeError: KeyError: token_type_ids
                          \n&quot;,&quot;5fc7S9yJQQCWarLbAnVESg&quot;:&quot;The following operation failed in the TorchScript interpreter.
                          \nTraceback of TorchScript, serialized code (most recent call last):
                          \n  File \&quot;code/__torch__.py\&quot;, line 12, in forward
                          \n    input_ids = inputs[\&quot;input_ids\&quot;]
                          \n    attention_mask = inputs[\&quot;attention_mask\&quot;]
                          \n    input = inputs[\&quot;token_type_ids\&quot;]
                          \n            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
                          \n    _0 = (model).forward(input_ids, attention_mask, input, )
                          \n    return {\&quot;sentence_embedding\&quot;: _0}
                          \n
                          \nTraceback of TorchScript, original code (most recent call last):
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(1074): trace_module
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(806): trace
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(43): export_to_torchscript
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(52): &lt;module&gt;
                          \nRuntimeError: KeyError: token_type_ids
                          \n&quot;,&quot;TPu-1KznRzCwj8tQAgHOMQ&quot;:&quot;The following operation failed in the TorchScript interpreter.
                          \nTraceback of TorchScript, serialized code (most recent call last):
                          \n  File \&quot;code/__torch__.py\&quot;, line 12, in forward
                          \n    input_ids = inputs[\&quot;input_ids\&quot;]
                          \n    attention_mask = inputs[\&quot;attention_mask\&quot;]
                          \n    input = inputs[\&quot;token_type_ids\&quot;]
                          \n            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
                          \n    _0 = (model).forward(input_ids, attention_mask, input, )
                          \n    return {\&quot;sentence_embedding\&quot;: _0}
                          \n
                          \nTraceback of TorchScript, original code (most recent call last):
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(1074): trace_module
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(806): trace
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(43): export_to_torchscript
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(52): &lt;module&gt;
                          \nRuntimeError: KeyError: token_type_ids\n&quot;}
    &quot;&quot;&quot;,
  &quot;is_async&quot;: true
}
</code></pre>
<p>Here is python script I used to export the model into TorchScript</p>
<pre><code>import torch
from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer
from transformers.utils import PaddingStrategy
from sentence_transformers import SentenceTransformer


class TorchScriptWrapper(torch.nn.Module):
    def __init__(self, model):
        super(TorchScriptWrapper, self).__init__()
        self.model = model

    def forward(self, inputs: dict):
        with torch.no_grad():
            outputs = self.model(inputs)
        return {&quot;sentence_embedding&quot;: outputs['sentence_embedding']}


def export_to_torchscript(model_name: str, is_sentence_transformer: bool, output_path: str, max_seq_length: int = 128):
    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name)
    if is_sentence_transformer:
        model = SentenceTransformer(model_name, device=&quot;cpu&quot;)
    else:
        model = AutoModel.from_pretrained(model_name)
    model.eval()

    # Define example text
    text = &quot;This is a test string&quot;

    # Create inputs
    inputs = tokenizer(text, padding=PaddingStrategy.MAX_LENGTH, return_tensors=&quot;pt&quot;, max_length=max_seq_length)

    # Instantiate the wrapper class
    model_wrapper = TorchScriptWrapper(model)

    # Unpack HF batch encoding into a regular dict
    new_inputs = {}
    new_inputs[&quot;input_ids&quot;] = inputs[&quot;input_ids&quot;]
    new_inputs[&quot;attention_mask&quot;] = inputs[&quot;attention_mask&quot;]
    if inputs.get(&quot;token_type_ids&quot;, None) is not None:
        new_inputs[&quot;token_type_ids&quot;] = inputs[&quot;token_type_ids&quot;]

    # Trace the wrapper class
    traced_model = torch.jit.trace(model_wrapper, new_inputs, strict=False)

    # Save traced model to file
    traced_model.save(output_path)


if __name__ == &quot;__main__&quot;:
    # Load pre-trained model and tokenizer
    model_name = &quot;sentence-transformers/LaBSE&quot;
    export_to_torchscript(model_name, True, output_path=&quot;torchscript_labse.pt&quot;)
</code></pre>
<p>PS : The error occured even though the <strong>token_type_ids</strong> key is part of the inputs dictionary</p>
","0","Question"
"78356002","","<p>I have configured my AWS CLI to use profile using SSO. I have referred this doc <a href=""https://docs.aws.amazon.com/cli/latest/userguide/sso-configure-profile-token.html#sso-configure-profile-token-auto-sso"" rel=""nofollow noreferrer"">AWS Documentation</a></p>
<p>I want to use postman to call Sagemaker endpoint. Is there a way to do it. I tried giving my credentials in postman but it returned 404 error.</p>
<p>Some additional details -
The Sagemaker endpoint uses VPC.</p>
","0","Question"
"78356992","","<p>I have a low amount of data instances in my data set. So, I tried the &quot;resample&quot; filter in Weka to increase the data amount and thus enhance the model performance. Is it okay to set the sample size percentage to 200? Because at that point I am getting a good correlation coefficient on the cross-validation test.</p>
<p>I want to know if the Resample filter works fine when setting the sample size percentage to 200.
And after using this filter will my model predict accurately?
Are there any other augmentation methods to enhance my model's performance because I have a low amount of data?</p>
","-1","Question"
"78358516","","<p>Having a multiclass classification task. My goal is to solve this using the Local Classifier per Parent Node (LCPN) approach.</p>
<p>Let me explain using a MWE.</p>
<p>Say I have this dummy dataset:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.datasets import make_classification
from scipy.cluster import hierarchy

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_informative=4)
</code></pre>
<p>I came up with the distance matrix between these classes to be:</p>
<pre class=""lang-py prettyprint-override""><code>d = np.array(
[[  0.,    201.537, 197.294, 200.823, 194.517],
 [201.537,   0.,    199.449, 202.941, 196.703],
 [197.294, 199.449,   0.,    198.728, 192.354],
 [200.823, 202.941, 198.728,   0.,    195.972],
[[194.517, 196.703, 192.354, 195.972,   0.   ]]
)
</code></pre>
<p>So, I determined the class hierarchy like so:</p>
<pre class=""lang-py prettyprint-override""><code>hc = hierarchy.linkage(d, method='complete')
</code></pre>
<p>The dendrogram obtained is thus:</p>
<pre class=""lang-py prettyprint-override""><code>dendrogram = hierarchy.dendrogram(hc, labels=['A','B','C', 'D', 'F'])
dendrogram
</code></pre>
<p><a href=""https://i.sstatic.net/qWYUi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qWYUi.png"" alt=""enter image description here"" /></a></p>
<p>Which I illustrate in a tree-like structure, using <code>hierarchy.to_tree()</code> as:</p>
<p><a href=""https://i.sstatic.net/KiSRM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KiSRM.png"" alt=""enter image description here"" /></a></p>
<p><strong>My Question:</strong></p>
<p>How do I fit a classifier such as a <code>DecisionTreeClassifier</code> or <code>SVM</code> at each internal node (including the root), following the LCPN method, to proceed as in the tree illustration above?</p>
","4","Question"
"78360476","","<p>I can't get the predict working even while doing everything a tutorial indicate.</p>
<pre><code>from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=3, metric='euclidean')
knn_model.fit(train_val_process, merge_labels.label)
y_pred_knn = knn.predict(test_data_process)
</code></pre>
<p>The error is the following :</p>
<pre><code>TypeError: KNeighborsClassifier.predict() missing 1 required positional argument: 
'X'
</code></pre>
<p>I'm supposed to normalize the data mean 1 std 0. Is it good ?</p>
<pre><code>from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(pd.concat([train_data, val_data]))
train_data_process = pd.DataFrame(scaler.transform(train_data), columns=train_data.columns)
val_data_process = pd.DataFrame(scaler.transform(val_data), columns=val_data.columns)
train_val_process = pd.concat([train_data, val_data])
test_data_process = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)
y_test = test_labels.label
</code></pre>
","-3","Question"
"78360982","","<pre class=""lang-py prettyprint-override""><code>from PIL import Image
import matplotlib.pyplot as plt
import argparse
import pickle
import numpy
 as np
from tensorflow import keras
from keras.applications.xception import Xception
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf

# Define the custom layer using Lambda (without name argument)
class NotEqual(tf.keras.layers.Layer):
    def __init__(self, name=None):
        super(NotEqual, self).__init__(name=name)

    def call(self, x, y):  # Use keyword arguments 'x' and 'y'
        return tf.math.not_equal(x, y)


# Define functions for extracting features, generating descriptions, and other necessary utilities
def extract_features(filename, model):
    try:
        image = Image.open(filename)
    except:
        print(&quot;ERROR: Couldn't open image! Make sure the image path and extension are correct&quot;)
    image = image.resize((299, 299))
    image = np.array(image)
    if image.shape[2] == 4:
        image = image[..., :3]
    image = np.expand_dims(image, axis=0)
    image = image / 127.5
    image = image - 1.0
    feature = model.predict(image)
    return feature

def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        pred = model.predict({'image_input': photo, 'text_input': sequence})  # Pass inputs as a dictionary
        pred = np.argmax(pred)
        word = word_for_id(pred, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text




# Parse arguments
ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help=&quot;Image Path&quot;)
args = vars(ap.parse_args())
img_path = args['image']

# Load tokenizer
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;, &quot;rb&quot;))

# Define the path to the model
model_path = 'models/model_9.h5'
# Load the model with custom objects including the NotEqual layer
with keras.utils.custom_object_scope({'NotEqual': NotEqual}):
    model = tf.keras.models.load_model(model_path)
</code></pre>
<p>I tried to run this code in very ways but I am getting error:</p>
<pre class=""lang-py prettyprint-override""><code> model = tf.keras.models.load_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\saving\saving_api.py&quot;, line 183, in load_model
    return legacy_h5_format.load_model_from_hdf5(filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\legacy_h5_format.py&quot;, line 133, in load_model_from_hdf5
    model = saving_utils.model_from_config(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\saving_utils.py&quot;, line 85, in model_from_config
    return serialization.deserialize_keras_object(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\serialization.py&quot;, line 495, in deserialize_keras_object
    deserialized_obj = cls.from_config(
                       ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\model.py&quot;, line 528, in from_config
    return functional_from_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\functional.py&quot;, line 528, in functional_from_config
    process_node(layer, node_data)
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\functional.py&quot;, line 475, in process_node
    layer(*args, **kwargs)
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;, line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\layers\layer.py&quot;, line 721, in __call__
    raise ValueError(
ValueError: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 0 (of type &lt;class 'int'&gt;)
</code></pre>
","1","Question"
"78361038","","<p>I tried two ways to calculate AUC score after training a Random Forest classifier. The first one is obtaining the metric from the cross_validate function:</p>
<pre><code>numeric_transformer = make_pipeline(
    IterativeImputer(estimator=RandomForestRegressor(),random_state=0),
    StandardScaler()
)
preprocessor = make_column_transformer(
    (numeric_transformer, numeric_cols)
)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('clf', RandomForestClassifier())
])

scoring = {
    'AUC': 'roc_auc', 
    'ACCURACY': 'accuracy',
    'F1_SCORE': 'f1',
    'PRECISION': 'precision',
    'RECALL': 'recall'
}
print(scoring)

cv = StratifiedKFold(n_splits=5)
cv_scores_RF = cross_validate(pipe, X, y, cv=cv, scoring=scoring, return_estimator=True)

print(&quot;Random forest metrics&quot;)
print(f&quot;AUC: {abs(cv_scores_RF['test_AUC']).mean()}&quot;)
print(f&quot;ACCURACY: {cv_scores_RF['test_ACCURACY'].mean()}&quot;)
print(f&quot;F1 SCORE: {abs(cv_scores_RF['test_F1_SCORE']).mean()}&quot;)
print(f&quot;PRECISION: {abs(cv_scores_RF['test_PRECISION']).mean()}&quot;)
print(f&quot;RECALL: {abs(cv_scores_RF['test_RECALL']).mean()}&quot;)

</code></pre>
<p>With the code above I obtain an AUC of 0.72.</p>
<p>Then i plotted the ROC curve with the function RocCurveDisplay:</p>
<pre><code>import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay


plt.figure(figsize=(8, 6))


tprs = []
aucs = []


for i, estimator in enumerate(cv_scores_RF['estimator']):
    viz = RocCurveDisplay.from_estimator(estimator, X, y, ax=plt.gca(), name=f'ROC fold {i+1}')
    
    roc_auc = auc(viz.fpr, viz.tpr)
    aucs.append(roc_auc)

    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)

mean_tpr = np.mean(tprs, axis=0)
mean_auc = np.mean(aucs)

plt.plot(mean_fpr, mean_tpr, color='b', linestyle='--', lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) - RF')
plt.legend(loc='lower right')

plt.show()

print(f&quot;Mean AUC: {mean_auc:.2f}&quot;)

</code></pre>
<p>But plotting the curve the AUC I got in the graph is of 0.97. Why does this happen?</p>
<p>I expected to get similar values, for example I changed the classifier to SVM and got different values too. From the metrics obtained by cross_validate I got 0.53 AUC, but from RocCurveDisplay, 0.71. Also trying with Naive Bayes, in this case, i got very similar values, 0.66 and 0.68 respectively.</p>
","0","Question"
"78361630","","<p>The env is <code>python 3.10</code>, <code>stable-baseline3 2.3.0</code> and I'm trying TD3 Algorithm.</p>
<p>I'm keep getting same error for whatever I do.</p>
<p>As far as I know, the reset method has return as same as observation space defined</p>
<p>The environment I made has reset method like below</p>
<pre><code>def reset(self, seed=0):
    self.current_index = 0
    self.current_cash = self.start_cash
    self.done = False
    self.current_time = self.start_time

    # 초기 관찰 상태 계산
    initial_state = self.get_state()  # dict
    return initial_state
</code></pre>
<p>Its never been complicated whatsoever and define env, model is also fine</p>
<pre><code>from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3 import TD3

class CustomFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=5):
        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)
        self.model_alpha = ModelAlpha()
    
    def forward(self, observations):
        prices = observations['prices']
        position = observations['position']
        quantity = observations['quantity']
        pnr = observations['pnr']
        
        return self.model_alpha(prices, torch.cat([position, quantity, pnr]))
        
        
# 환경과 모델 설정
env = MarketEnvironment(candles, '2020-07-01 00:00:00', '2023-12-31 23:59:00') # 여러분의 환경 설정
policy_kwargs = dict(
    features_extractor_class=CustomFeatureExtractor,
    features_extractor_kwargs=dict(features_dim=5)
)

model = TD3(&quot;MultiInputPolicy&quot;, env, policy_kwargs=policy_kwargs, batch_size=128, verbose=1)
</code></pre>
<p>Jupyter prompt says that</p>
<p>Using cpu device
Wrapping the env with a <code>Monitor</code> wrapper
Wrapping the env in a DummyVecEnv.</p>
<p>and it runs fine until</p>
<pre><code>model.learn(total_timesteps=1, log_interval=10, progress_bar=True)
</code></pre>
<p>this code.</p>
<p>It keep saying that again and again no matter what I've done</p>
<pre><code>File ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\off_policy_algorithm.py:297, in OffPolicyAlgorithm._setup_learn(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)
    290 if (
    291     self.action_noise is not None
    292     and self.env.num_envs &gt; 1
    293     and not isinstance(self.action_noise, VectorizedActionNoise)
    294 ):
    295     self.action_noise = VectorizedActionNoise(self.action_noise, self.env.num_envs)
--&gt; 297 return super()._setup_learn(
    298     total_timesteps,
    299     callback,
    300     reset_num_timesteps,
    301     tb_log_name,
    302     progress_bar,
    303 )

File ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\base_class.py:425, in BaseAlgorithm._setup_learn(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)
    423 if reset_num_timesteps or self._last_obs is None:
    424     assert self.env is not None
--&gt; 425     self._last_obs = self.env.reset()  # type: ignore[assignment]
    426     self._last_episode_starts = np.ones((self.env.num_envs,), dtype=bool)
    427     # Retrieve unnormalized observation for saving into the buffer

File ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:77, in DummyVecEnv.reset(self)
     75 for env_idx in range(self.num_envs):
     76     maybe_options = {&quot;options&quot;: self._options[env_idx]} if self._options[env_idx] else {}
---&gt; 77     obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
     78     self._save_obs(env_idx, obs)
     79 # Seeds and options are only used once

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>What I know is that the reset() method of this error is in an abstract class named VecEnv</p>
<p>How to resolve this?</p>
","1","Question"
"78366460","","<p>I am using a dataset where each sample corresponds to 4 images taken at a known delay from each other and each set of 4 images has a target prediction that is a number (not classification). I currently have made the model below but it doesnt give good results at all. any advice ?</p>
<pre><code>class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout(p=0.25)
        
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout(p=0.25)
        
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(32)
        self.pool3 = nn.MaxPool2d(kernel_size=5, stride=2)
        self.dropout3 = nn.Dropout(p=0.25)
        
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28800, 512)
        self.dropout4 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(512, 1)  # Single output

    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.dropout1(x)
        
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.dropout2(x)
        
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        x = self.dropout3(x)
        
        x = self.flatten(x)
        x = torch.relu(self.fc1(x))
        x = self.dropout4(x)
        
        x = self.fc2(x)  # Output layer, no activation function for regression
        return x
</code></pre>
<p>Also, the target prediction value is often very small and sometimes much larger such from around 1e-9 to 1e2. i have applied a log scale to the target prediction to reduce this effect to attempt to improve learning but not sure how much it helped.</p>
","-1","Question"
"78367946","","<p><strong>I am working on a Flask-based web application where users can upload images to predict using a machine learning model. The uploaded images are stored in a local directory, and predictions are made with a pre-trained model. However, when I hit the predict button</strong>
What is causing this UnicodeEncodeError?
How can I resolve this issue to ensure my application works correctly with image uploads and predictions?
Are there best practices for dealing with character encoding in a Flask environment, especially on Windows?</p>
<h1>==app.py====</h1>
<pre><code>@app.route('/uploadimage', methods=['GET', 'POST'])
def upload_image():

        file = request.files['my_image']
        # Get the prediction
        predicted_label = predict_label(img_path)
        # Return the predicted label with a flash message
        flash(f&quot;Prediction: {predicted_label}&quot;, &quot;success&quot;)
        os.remove(img_path)  # Remove the temporary file after processing
    return render_template('uploadimage.html')  # For GET request, render the form

</code></pre>
<p>I am getting this error even i have set the environment variable &quot;UTF-8&quot;
<strong>Error</strong></p>
<p>File &quot;C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;, line 122, in error_handler
raise e.with_traceback(filtered_tb) from None
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py&quot;, line 19, in encode
return codecs.charmap_encode(input,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode characters in position 19-38: character maps to undefined.</p>
<p>============
Even i got a simplest code to test encoding
caption for that was &quot;To test if your console can handle UTF-8, try outputting text with special characters or Unicode characters:&quot;</p>
<pre><code>print(&quot;UTF-8 test: àéîöü — 中文 — العربية&quot;)

</code></pre>
<p>Error is same for it as well</p>
<p>print(&quot;UTF-8 test: ����� � \u4e2d\u6587 � \u0627\u0644\u0639\u0631\u0628\u064a\u0629&quot;)
File &quot;C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py&quot;, line 19, in encode
return codecs.charmap_encode(input,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode characters in position 20-21: character maps to </p>
","-1","Question"
"78370048","","<p>I have a ML code</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
import tensorflow_federated as tff

iris = load_iris()
df = pd.DataFrame(iris.data,columns=iris.feature_names)
df['Species']=iris.target

# Splitting the dataframe into input features and target variables

x = df.drop('Species',axis=1)
y = df['Species']
# Function to create client datasets (assuming data is pre-partitioned)
def create_tf_dataset(client_data):
  &quot;&quot;&quot;Creates a tf.data.Dataset from the provided client data (features, labels).&quot;&quot;&quot;
  features, labels = client_data
  return tf.data.Dataset.from_tensor_slices((features, labels))

# Split data into cliendatasets (simulating data partitioning)
client_datasets = []
num_clients = 5
for i in range(num_clients):
  start_index = int(i * (len(x) / num_clients))
  end_index = int((i + 1) * (len(x) / num_clients))
  client_features = x[start_index:end_index]
  client_labels = y[start_index:end_index]
  client_datasets.append(create_tf_dataset((client_features, client_labels)))
  # Define the model architecture (replace with your desired model complexity)
def model_fn(inputs):
   features, _ = inputs  # We only use features for classification
   dense1 = tf.keras.layers.Dense(10, activation='relu')(features)
   dense2 = tf.keras.layers.Dense(3, activation='softmax')(dense1)  # 3 units for 3 Iris classes
   return tf.keras.Model(inputs=features, outputs=dense2)

# Define the client optimizer
client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# Define the server optimizer (for server-sided aggregation)
server_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
fed_learning_model = tff.learning.build_federated_averaging_process(
     model_fn,
     client_optimizer_fn=client_optimizer,
     server_optimizer_fn=server_optimizer)
</code></pre>
<p>But I am getting this error consistently.</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-13-e5966e29fc79&gt; in &lt;cell line: 1&gt;()
----&gt; 1 fed_learning_model = tff.learning.build_federated_averaging_process(
      2      model_fn,
      3      client_optimizer_fn=client_optimizer,
      4      server_optimizer_fn=server_optimizer)

AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'build_federated_averaging_process'
</code></pre>
<p>I dont know whether my version suites. my tensorflow federated version is 0.76.0
my tensorflow version is 2.14.1 and python version is 3.10.12</p>
<p>When i search through the internet i saw that this code doesnot support for tensorflow version 0.21.0 onwards. but i dont know what to use in the latest version</p>
","1","Question"
"78370489","","<p>I have been trying to use <a href=""https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split"" rel=""nofollow noreferrer"">TimeSeriesSplit</a> for panel data. By panel data I mean yearly pictures of the population. I was interested in splitting the data along the years. That population is evolving and each year do not contain the same population size. Hence the direct use of the TimeSeriesSplit is not possible.</p>
<p>Basically I was trying to get the following CV scheme:</p>
<p><a href=""https://i.sstatic.net/PPWS0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PPWS0.png"" alt=""enter image description here"" /></a></p>
<p>I manage to do this withe following code:</p>
<pre><code>import pandas as pd, numpy as np
import seaborn as sns, matplotlib.pyplot as plt

from sklearn.datasets import make_regression
from sklearn.dummy import DummyRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

X_test, y_test = [], []

start_year = 2010
end_year = 2020

for year in np.arange(start_year, end_year+1):
    X_year, y_year = make_regression(n_samples=5+year-start_year, n_features=2, bias=100, noise=1, random_state=year)
    X_year = pd.DataFrame(X_year).rename(columns={0:'X1', 1:'X2'})
    X_year['year'] = year
    y_year = pd.Series(y_year)
    X_test.append(X_year)
    y_test.append(y_year)
    
X_test, y_test = pd.concat(X_test), pd.concat(y_test)

# modelling

X = X_test
y = y_test
years = np.unique(X_test['year'])

# modelisation
model = DummyRegressor(strategy=&quot;mean&quot;)
metric = mean_squared_error
cv = TimeSeriesSplit(n_splits=len(years)-1)

years_folds = []
res = []

for i, (train_year, test_year) in enumerate(cv.split(years)):
    
    print(f&quot;Fold {i}:&quot;)
    print(f&quot;  Train: index={years[train_year]}&quot;)
    print(f&quot;  Test:  index={years[test_year]}&quot;)
    
    years_folds.append((years[train_year], years[test_year]))
    
    train_filter = X['year'].isin(years[train_year])
    test_filter = X['year'].isin(years[test_year])
    
    X_train, y_train = X.loc[train_filter.values], y[train_filter.values]
    X_test, y_test = X.loc[test_filter.values], y[test_filter.values]
    
    model.fit(X_train, y_train)
    score = metric(model.predict(X_test), y_test)
    print(f' {score=:.3}')
    res.append((years[test_year][0], score))

plot_year_folds(years_folds)
    
folds_res = pd.DataFrame(res,columns=['test_year', metric.__name__])
folds_res.plot.scatter(x='test_year', y=metric.__name__, title=f'{metric.__name__} over test_year');
</code></pre>
<p>Note: I use a dummy dataset and a dummy model for the sake of providing a running example. It is not the main topic of my post.</p>
<p>As you can notice I have to use a trick: I split the years and not the data. I am wondering: is there a standard way to split the data with a sklearn cv object ? The goal is to use it in a cross_val_score function.</p>
","1","Question"
"78373040","","<p>I'm working on a machine learning project, and my dataset contains variables about social, demographic and economic aspects of 218 countries, ranging from 1960 to 2022. The target variable is a binary variable (Yes or No) that represents if the country has had at least one attempt of coup d'etat in a specific year.
My question is: what are the best classification models for multi-level data?</p>
<p>From consulting different sources, I've wrote down these models (no particular order):</p>
<ul>
<li>Random Forest</li>
<li>XGBoost</li>
<li>Logistic Classification</li>
<li>Decision Tree</li>
</ul>
<p>Are they the wrong ones? Are there more models I'm not aware of?
If not, do you know any source I can use to implement these models in R?</p>
","-2","Question"
"78374435","","<p>I am trying to overfit polynomial regression to a sine curve. As far as I've understood, when having <code>N</code> data samples and a polynomial degree of <code>N-1</code> then the curve should go through all the data points, however, in my example this doesn't happen.</p>
<p>My code is below:</p>
<pre><code>from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

N = 50
deg = 49

X = X = np.linspace(0, 2 * np.pi, N).reshape(-1, 1)
X = np.sort(X, axis=0)
y = np.sin(X) + np.random.randn(N, 1) * 0.2

poly_features = PolynomialFeatures(degree=deg, include_bias=False)

X_poly = poly_features.fit_transform(X)

reg = LinearRegression()
reg.fit(X_poly, y)

y_vals = reg.predict(X_poly)

plt.scatter(X, y)
plt.plot(X, y_vals, color='r')
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/Ntazj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ntazj.png"" alt=""Result of the code above"" /></a></p>
<p>Can you explain what I'm misunderstanding here?</p>
","3","Question"
"78376338","","<p>I am creating a plant disease identification model. I have a dataset of 38 diseases with around 2000 images for each disease. But while training the model, some epochs are getting skipped due to some OUT_OF_RANGE error. Can someone please help me to figure this out?</p>
<pre><code>import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input

train_dir = 'dataset/train'
valid_dir = 'dataset/valid'
batch_size = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

valid_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=batch_size,
    class_mode='categorical'
)

valid_generator = valid_datagen.flow_from_directory(
    valid_dir,
    target_size=(150, 150),
    batch_size=batch_size,
    class_mode='categorical'
)

model = Sequential([
    Input(shape=(150, 150, 3)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(38, activation='softmax')  # Adjust output units based on the number of disease classes
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=10,
    validation_data=valid_generator,
    validation_steps=valid_generator.samples // batch_size
)

model.save('plant_disease_model.h5')

class_indices = train_generator.class_indices
disease_names = list(class_indices.keys())
print(&quot;Mapping of Class Indices to Disease Names:&quot;, class_indices)
</code></pre>
<p>Terminal:</p>
<pre><code>Found 70295 images belonging to 38 classes.
Found 17572 images belonging to 38 classes.
2024-04-23 19:50:32.085744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instru
ctions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/10
\.venv\Lib\site-packages\keras\src\trainers\data_adapters\py_dataset_adapter.p
y:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_m
ultiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.
  self._warn_if_super_not_called()
←[1m2196/2196←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m905s←[0m 411ms/step - accuracy: 0.4608 - loss: 1.8737 - val_accuracy: 0.7432 - val_
loss: 0.8556
Epoch 2/10
←[1m   1/2196←[0m ←[37m━━━━━━━━━━━━━━━━━━━━←[0m ←[1m12:02←[0m 329ms/step - accuracy: 0.6875 - loss: 0.78202024-04-23 20:05:37.996528: W tensorfl
ow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
         [[{{node IteratorGetNext}}]]
C:\Users\Admin\AppData\Local\Programs\Python\Python311\Lib\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Ma
ke sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function wh
en building your dataset.
  self.gen.throw(typ, value, traceback)
2024-04-23 20:05:38.068817: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of
sequence
         [[{{node IteratorGetNext}}]]
←[1m2196/2196←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 49us/step - accuracy: 0.6875 - loss: 0.7820 - val_accuracy: 0.7500 - val_los
s: 0.2462
</code></pre>
<p>As you can see above, epoch 1 was successfully completed but epoch 2 was terminated due to some error. Similarly, epoch 3, 5, 7, 9 were successfully completed but epoch 4, 6, 8, 10 caused errors.</p>
","6","Question"
"78378193","","<p>I'm using Python and I have a dataset containing NaN values. To clean up these data, I replaced the NaN values with the mean or median of each column using the fillna() function from pandas. However, after this operation, some values in my dataset became &quot;inf&quot;. I don't understand why this is happening and how I can fix this issue.</p>
","0","Question"
"78378560","","<p>I use SVR to predict my data</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_absolute_error
import csv
from math import sqrt
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

filepath = '/content/drive/MyDrive/TESIS/DATA/'

pilihankolom = 35

X = pd.read_csv(filepath+'Data_Radiomic.csv').to_numpy()
Y = pd.read_csv(filepath+'Data_Dosiomic.csv', usecols=[pilihankolom])
print(X.shape)
Y_label = str(Y.columns)
Y = Y.to_numpy()

(187, 98)

model=SVR(kernel='rbf', C=10, epsilon=0.01)
kf = KFold(n_splits=4)

X_training, X_testing, Y_training, Y_testing = train_test_split(X, Y, test_size=0.2, random_state=0)
print(X_training.shape, X_testing.shape)

(149, 98) (38, 98)

prediction = []
mse_set = []
for train_index, test_index in kf.split(X_training):
  print(train_index, test_index)
  X_train, X_test = X_training[train_index], X_training[test_index]
  Y_train, Y_test = Y_training[train_index], Y_training[test_index]
  model.fit(X_train, Y_train)
  Y_pred = model.predict(X_test)
  mse_set.append(mean_squared_error(Y_test, Y_pred))
  prediction.extend(Y_pred)
</code></pre>
<p>this is the result</p>
<h2>[ 38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
146 147 148] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
24 25 26 27 28 29 30 31 32 33 34 35 36 37]</h2>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-62-98eca0acb6ac&gt; in &lt;cell line: 3&gt;()
      5   X_train, X_test = X_training[train_index], X_training[test_index]
      6   Y_train, Y_test = Y_training[train_index], Y_training[test_index]
----&gt; 7   model.fit(X_train, Y_train)
      8   Y_pred = model.predict(X_test)
      9   mse_set.append(mean_squared_error(Y_test, Y_pred))

4 frames
/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py in fit(self, X, y, sample_weight)
    190             check_consistent_length(X, y)
    191         else:
--&gt; 192             X, y = self._validate_data(
    193                 X,
    194                 y,

/usr/local/lib/python3.10/dist-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    582                 y = check_array(y, input_name=&quot;y&quot;, **check_y_params)
    583             else:
--&gt; 584                 X, y = check_X_y(X, y, **check_params)
    585             out = X, y
    586 

/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1104         )
   1105 
-&gt; 1106     X = check_array(
   1107         X,
   1108         accept_sparse=accept_sparse,

/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
    919 
    920         if force_all_finite:
--&gt; 921             _assert_all_finite(
    922                 array,
    923                 input_name=input_name,

/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)
    159                 &quot;#estimators-that-handle-nan-values&quot;
    160             )
--&gt; 161         raise ValueError(msg_err)
    162 
    163 
</code></pre>
<p>ValueError: Input X contains NaN.
SVR does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See <a href=""https://scikit-learn.org/stable/modules/impute.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/impute.html</a> You can find a list of all estimators that handle NaN values at the following page: <a href=""https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values</a></p>
<p>please  any advice to solve this error</p>
","0","Question"
"78379552","","<p>I am using the following query to create a model but the editor complains that ML.NGRAM is not supported within the transform clause.</p>
<pre><code>CREATE OR REPLACE MODEL
`singular-hub-291814.movie_sentiment.mymodel3`
TRANSFORM(ML.NGRAM(string_field_0,[1,2])OVER() as ngram )
OPTIONS
  ( model_type='LOGISTIC_REG',
    auto_class_weights=TRUE,
    data_split_method='RANDOM',
    DATA_SPLIT_EVAL_FRACTION = .10,
    input_label_cols=['review']
  ) AS

SELECT 
  string_field_0 , review from table;
</code></pre>
<p>Even though the same transformation can be used inside SELECT query.</p>
<pre><code>SELECT 
  ML.NGRAMS(words_array, [1,2]) as ngrams, 
  review
from table;
</code></pre>
<p>whereas the other transformation functions like bag_of_words, min_abs_scalar can be used inside transform. Why is this behavior different this way? Is there an explicit list of transformers that cannot be used in the TRANSFORM clause?</p>
","0","Question"
"78379820","","<p>In LLM studio, when I try to download any model, I am facing  following error:</p>
<p>Download Failed: unable to get local issuer certificate</p>
<p><a href=""https://i.sstatic.net/WkMZI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WkMZI.png"" alt=""enter image description here"" /></a></p>
","4","Question"
"78380376","","<p>How can I implement the FFT-accelerated Interpolation-based t-SNE (FIt-SNE) on my colab notebook?</p>
<p>I was trying to compute t-SNE on a kaggle dataset of <a href=""https://www.kaggle.com/datasets/danielshanbalico/dog-emotion/data"" rel=""nofollow noreferrer"">dog emotions</a>,
I tried first to take the first components.</p>
<pre><code>pc = 60
pca = decomposition.PCA(n_components=pc)
_ = pca.fit(images)
imgPCA = pca.transform(images)

tsne = TSNE(n_components=2)
Z = tsne.fit_transform(imgPCA)
plot_embedding(Z)
</code></pre>
<p>Then I tried the <a href=""https://github.com/DmitryUlyanov/Multicore-TSNE"" rel=""nofollow noreferrer"">multicore t-SNE</a> to raise the number of iterations but I still don't like it</p>
<pre><code>!pip install git+https://github.com/DmitryUlyanov/Multicore-TSNE.git

from MulticoreTSNE import MulticoreTSNE

Z = MulticoreTSNE(n_jobs=4, n_iter=10000).fit_transform(imgPCA)
plot_embedding(Z, show_axis = True)
</code></pre>
<p>Now I want to try to use the <a href=""https://github.com/KlugerLab/FIt-SNE"" rel=""nofollow noreferrer"">FIt-SNE</a>, but I don't know how to use it, can you help me?
Or maybe you can help to improve the previous snippets of code if you want.</p>
<p>There is the code to understand the format of the dataset:</p>
<pre><code>import pandas as pd
import cv2

img_size = (192,192,3)
num_px = img_size[0] * img_size[1] * img_size[2]

directory = '/content/drive/MyDrive/Colab Notebooks/ML/Dog_Emotion/' 
images = []
labels = []
labels_df = pd.read_csv(directory + &quot;labels.csv&quot;)
n_images = 0

for image in tqdm(labels_df.iloc, desc = &quot;loading images&quot;, unit = &quot;images&quot;, total = 4000):
  images.append(np.asarray(cv2.resize(cv2.imread(directory + image[2] + '/' + image[1], cv2.IMREAD_COLOR), img_size[0:2])[:, :, ::-1]))
  labels.append(image[2])

images, labels = np.array(images).reshape(4000, num_px), np.array(labels)

print(f'labels shape: {labels.shape}')
print(f'images shape: {images.shape}')
print(f'images size: {img_size}')

def plot_embedding(Z, show_axis=&quot;False&quot;):
  plt.figure(figsize=(10, 8))
  map = {label: i for i, label in enumerate(np.unique(labels))}
  color = np.array([map[l] for l in labels])
  plt.scatter(Z[:, 0], Z[:, 1], c = color, cmap = &quot;jet&quot;)
  plt.colorbar()
  plt.title('2d t-SNE Visualization')
  if not show_axis:
    plt.axis(&quot;off&quot;)
  plt.axis(&quot;equal&quot;)
  plt.show()
</code></pre>
<p>labels shape: (4000,)
images shape: (4000, 110592)
images size: (192, 192, 3)</p>
","0","Question"
"78383583","","<p>I want develop a chatbot that is able to perform actions and answer questions in a given Smart Home environment.</p>
<p>I am curious how to do this with an LLM. How can I customize/train a model to execute code?
Just a simple example: When I tell the chatbot &quot;turn on the light in the living room&quot; it should answer &quot;I will turn on the light in the living room&quot; and at the same time turn it on in the background (assuming I have an API / code that I am able to call).</p>
<p>Are there some resources or even examples you can share to learn about the process?</p>
<p>I know a bit about customizing models like adding a System Message or adjusting the temperature of the model and I also trained an LLM to generate software requirements before. But I don't know how to train a model to perform an action like turning a smart device on or off.</p>
<p>I am currently using Ollama to manage and customize models.</p>
","-1","Question"
"78383840","","<p>The Following are the <a href=""https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.cv.html#lightgbm-cv"" rel=""nofollow noreferrer"">.cv</a> APIs of lightgbm</p>
<blockquote>
<p>lightgbm.cv(params, train_set, num_boost_round=100, folds=None, nfold=5, stratified=True, shuffle=True, metrics=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto', fpreproc=None, seed=0, callbacks=None, eval_train_metric=False, return_cvbooster=False)</p>
</blockquote>
<p>There is a parameter <code>cateogrical_feature</code></p>
<blockquote>
<p>Categorical features. If list of int, interpreted as indices. If list of str, interpreted as feature names (need to specify feature_name as well).</p>
</blockquote>
<p>Now the <a href=""https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html#lightgbm-train"" rel=""nofollow noreferrer"">.train</a> API</p>
<blockquote>
<p>lightgbm.train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto', keep_training_booster=False, callbacks=None)</p>
</blockquote>
<p>Here also there is a <code>categorical_feature</code> parameter. The documentation for this is the same as above</p>
<p>Now, as you notice both the APIs consume the <a href=""https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html#lightgbm-dataset"" rel=""nofollow noreferrer"">lightgbm dataset</a> which, itself takes a <code>categorical_feature</code> parameter. The documentation is exactly the same</p>
<p>Questions:</p>
<ol>
<li>If both are specified which one takes precedence?</li>
<li>Which one is the suggested place to specify the categorical_feature?</li>
<li>Are the two choices in any way different internally to the working of the lightgbm pipeline?</li>
</ol>
","1","Question"
"78383883","","<p>I am trying to model nitrate concentrations in the streams in Bavaria in Germany using Random Forest model. I am using Python and primarily sklearn for the same. I have data from 490 water quality stations. I am following the methodology in the paper from LongzhuQ.Shen et al which can be found here: <a href=""https://www.nature.com/articles/s41597-020-0478-7"" rel=""nofollow noreferrer"">https://www.nature.com/articles/s41597-020-0478-7</a></p>
<p>I want to split my dataset into training and testing set such that the spatial distribution of data in both sets is identical. The idea is that if data splitting ignores the spatial distribution, there is a risk that the training set might end up with a concentration of points from densely populated areas, leaving out sparser areas. This can skew the model's learning process, making it less accurate or generalizable across the entire area of interest. sklearn train_test_split just randomly divides the data into training and testing sets and it does not consider the spatial patterns in the data.</p>
<p>The paper I mentioned above follows this methodology: &quot;We split the full dataset into two sub-datasets, training and testing respectively. To consider the heterogeneity of the spatial distribution of the gauge stations, we employed the spatial density estimation technique in the data splitting step by building a density surface using Gaussian kernels with a bandwidth of 50 km (using v.kernel available in GRASS GIS33) for each species and season. The pixel values of the resultant density surface were used as weighting factors to split the data into training and testing subsets that possess identical spatial distributions.&quot;</p>
<p>I want to follow the same methodology but instead of using grass GIS, I am just building the density surface myself in Python. I have also extracted the probability density values and the weights for the stations. (attached figure)</p>
<p>Now the only problem I am facing is how do I use these weights to split the data into training and testing sets? I checked there is no keyword in the sklearn train_test_split function that can consider the weights. I also went back and forth with chat GPT 4 but it is also not able to give me a clear answer. Neither did I find anything concrete on the internet about this. Maybe I am missing something.</p>
<p>Is there any other function I can use to do this? Or will I have to write my own algorithm to do the splitting? In case of the latter, can you please suggest me the approach so I can code it myself?</p>
<p>In the attached figure you can see the location of the stations and the probability density surface generated using the kernel density estimation method (using Gaussian kernels).</p>
<p>Also attaching a screenshot of my dataframe to give you some idea of the data structure. (all columns after longitude ('lon') column are used as features. the NO3 column is used as the target variable.)</p>
<p>Please find the attached images for reference.</p>
<p><a href=""https://i.sstatic.net/uSnkK.png"" rel=""nofollow noreferrer"">Probability density surface generated using the kernel density estimation method with gaussian kernels.</a></p>
<p><a href=""https://i.sstatic.net/zl9aE.png"" rel=""nofollow noreferrer"">the dataset I am using to model the nitrate concentrations</a></p>
","-2","Question"
"78387055","","<p>I have some images from an experiment with plastics particles and water waves. The goal is to identify the plastics particles automatically. They sometimes overlap, and I don't need to find individual particles, it is enough to find those pixels that contain plastics.</p>
<p>Since the particles are red, and the background is mostly white or black, I thought I could go for a simple thresholding, saying that pixels are plastics if <code>R &gt; 5*B</code> and <code>R &gt; 0.25</code>, where <code>R</code> and <code>B</code> are the red and blue channels. However the exposure varies quite a bit between experiments, and sometimes within an experiment when part of the surface is covered by water, so my approach doesn't work very consistently, and sometimes mis-identifies the dark cracks along the sides.</p>
<p>I'm wondering what could be other options. I have limited experience with neural networks, so I'm not sure if that would work (with a reasonable amount of effort). In particular, I don't think shape is going to be much help, since the particles are close together and partially overlapping with poor contrast between them, but maybe colour is enough?</p>
<p>Example images:</p>
<p><a href=""https://i.sstatic.net/ArZDBn8J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ArZDBn8J.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/iVWta6Aj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iVWta6Aj.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/UyxGmmED.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UyxGmmED.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"78389349","","<p>I have a dataset consisting of 5 features and 1 target. I am using a spline transformer to transform the features and then fit it using linear regression. I am fitting the data with 1st degree and 5 knots of spline transformed features. Now I want to represent the model fit in a mathematical equation that will include the individual terms as well as the interaction terms.</p>
<p>How can I do that?</p>
","-1","Question"
"78389839","","<p>I have some images. In that some images are cropped versions.
like here is the original image <a href=""https://i.sstatic.net/vzb5KJo7.jpg"" rel=""nofollow noreferrer"">big image</a>
and the cropped image <a href=""https://i.sstatic.net/yNNZRy0w.jpg"" rel=""nofollow noreferrer"">small image</a>.</p>
<p>Note that the shape(resolution) of images are not same.</p>
<p>I have a few pairs like this. The original images are kept in a folder and the cropped images are kept in another folder.</p>
<p>Ultimately I want to find the pairs of original image and cropped image from these images.</p>
<p>So I want to iterate to the images from both folders and check if the cropped image is a part of bigger image or not.</p>
<p>But I can't find any algorithm that giving such results with images of different shapes(resolutions).</p>
<p>I have already tried <code>cv2.matchTemplate</code> and <code>skimage.metrics.structural_similarity</code>
but they are only working for images of similar shape(resolution).</p>
","-2","Question"
"78392429","","<p>I want to implement a method to monitor in Tensorboard the update-to-data ratio during training with PyTorch, following an idea mentioned in Karpathy's video. I've come up with a solution, but I'm looking for a more elegant and configurable approach.</p>
<p>The current implementation directly modifies the training loop as follows:</p>
<pre><code>for step, batch in data_loader:
    x, y = batch
    optimizer.zero_grad()
    for name, param in model.named_parameters():
        if param.requires_grad and &quot;weight&quot; in name:
            param.data_before_step = param.data.clone()
    output = model(x)
    loss = loss_fn(output, y)
    loss.backward()
    optimizer.step()
    lr_scheduler.step()
    for name, param in model.named_parameters():
        if hasattr(param, &quot;data_before_step&quot;):
            update = param.data - param.data_before_step
            update_to_data = (update.std() / param.data_before_step.std()).log10().item()
            summary_writer.add_scalar(f&quot;Update:data ratio {name}&quot;, update_to_data, epoch * len(data_loader) + step)
            param.data_before_step = param.data.clone()
</code></pre>
<p>However, this approach adds code directly within the training loop, which can clutter the code and if we want to make it configurable, if-else statements are needed which clutter the code even more.</p>
<p>I've also explored using PyTorch hooks to achieve this. I've successfully implemented a hook to track gradients:</p>
<pre><code>class GradToDataRatioHook:
    def __init__(self, name, param, start_step, summary_writer):
        self.name = name
        self.param = param
        self.summary_writer = summary_writer
        self.grads = []
        self.grads_to_data = []
        self.param.update_step = start_step

    def __call__(self, grad):
        self.grads.append(grad.std().item())
        self.grads_to_data.append((grad.std() / (self.param.data.std() + 1e-5)).log10().item())
        self.summary_writer.add_scalar(f&quot;Grad {self.name}&quot;, self.grads[-1], self.param.update_step)
        self.summary_writer.add_scalar(f&quot;Grad:data ratio {self.name}&quot;, self.grads_to_data[-1], self.param.update_step)
        self.param.update_step += 1
</code></pre>
<p>However, implementing a similar hook to capture updates seems tricky. As far as I understand, <code>param.register_hook(...)</code> registers the hook, which is called when the gradient is calculated, i.e., before <code>optimizer.step()</code> is called. While the gradient and learning rate provide a direct value of an update for standard SGD, modern optimizers like Adam make the update process a bit more complicated. I'm seeking a solution that captures updates in an optimizer-agnostic way, preferably with PyTorch hooks. However, any suggestions or alternative approaches would be also greatly appreciated.</p>
","1","Question"
"78393841","","<p>I have a bunch of photos of energy meters. In every photo there is info written on the meter and also a seven segment display which has reading in kWh. Also, there is logo of meter manufacturer. I want to extract information like meter serial number, meter manufacturer, meter reading in kWh(7 segmented display).</p>
<p>I have used easyocr module to detect text and using that I have achieved to extract meter serial numbers but for kWh reading easyocr fails to detect the digital digits.</p>
<p>Currently I have 2 requirements, one is to detect digits from 7 segment display and also detect manufacturer from logo symbol in meter. Only 2-3 different manufacturers are there so I need to train based on only 2-3 logos. But I don't know how to do it.</p>
<p>Any help or guidance with steps kr procedure will be really appreciated.</p>
<p>I tried googling, searched on YouTube but not getting any roadmap for the same. I have tried other ocr modules like tesseract but it also fails.</p>
","-2","Question"
"78395647","","<p>I've tried to make a onvsrest classiffication model using <code>decision_function_shape= ovr</code>, but when i changed it to <code>decision_function_shape= ovo</code>, it gave me the same result as ovr. Turn out i read that svc() is using ovo as it base wheter it initiated as ovr or ovo. So how can i turn my code so it give me an ovr result?</p>
<pre><code>model3 = SVC(kernel = 'rbf', decision_function_shape='ovr')
model3.fit(X_train, Y_train)
model3_predictions = model3.predict(X_test)
</code></pre>
<p>Ive tried using OneVsRestClassifier() but idk how to give the output of all of this one bellow, it always error and say OneVsRestClassifier dont have those commands. is there anyway to get the cm, sm, sv, beta, and intercept with OneVsRestClassifier?</p>
<pre><code>cm3 = confusion_matrix(Y_test, model3_predictions, labels=[-1,0,1])
sm3 = classification_report(Y_test, model3_predictions)
support_vector3 = model3.support_
n_sv_model3 = model3.n_support_
alpha_model3 = pd.DataFrame(model3.dual_coef_)
b_model3 = pd.DataFrame(model3.intercept_)
</code></pre>
<p>Hope someone can help me, thanks in advance</p>
","2","Question"
"78396068","","<p>I am using XGBoost with SHAP to analyze feature importance in a multiclass classification problem and need help plotting the SHAP summary plots for all classes at once. Currently, I can only generate plots one class at a time.</p>
<pre class=""lang-py prettyprint-override""><code>SHAP version: 0.45.0
Python version: 3.10.12
</code></pre>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import xgboost as xgb
import shap
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

# Generate synthetic data
X, y = make_classification(n_samples=500, n_features=20, n_informative=4, n_classes=6, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Train a XGBoost model for multiclass classification
model = xgb.XGBClassifier(objective=&quot;multi:softprob&quot;, random_state=42)
model.fit(X_train, y_train)
</code></pre>
<p>I then tried to plot the shape values:</p>
<pre class=""lang-py prettyprint-override""><code># Create a SHAP TreeExplainer
explainer = shap.TreeExplainer(model)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)

# Attempt to plot summary for all classes
shap.summary_plot(shap_values, X_test, plot_type=&quot;bar&quot;)
</code></pre>
<p>I got this interaction plot instead:</p>
<p><a href=""https://i.sstatic.net/nS15e6YP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nS15e6YP.png"" alt=""enter image description here"" /></a></p>
<p>I remedied the problem with help from <a href=""https://stackoverflow.com/questions/78368073/incorrect-array-shapes-for-shap-values-in-binary-classification-when-trying-to-c/78368951#78368951"">this</a> post:</p>
<pre class=""lang-py prettyprint-override""><code>shap.summary_plot(shap_values[:,:,0], X_test, plot_type=&quot;bar&quot;)
</code></pre>
<p>which gives a normal bar plot for class 0:</p>
<p><a href=""https://i.sstatic.net/BHbGh93z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHbGh93z.png"" alt=""enter image description here"" /></a></p>
<p>I can then do the same with classes 1, 2, 3, etc.</p>
<p>The question is, how can you make a summary plot for all the classes? I.e., a single plot showing the contribution of a feature to each class?</p>
","3","Question"
"78398017","","<p>I have a question about make_classification from scikit-learn. I have created a dataset with make_classification (binary classification task) and the aim is to test how well different models can distinguish important features from less important features.</p>
<p><strong>How can I set an experiment in which I can evaluate whether a model is able to identify the variables that have an influence?</strong></p>
<p>I have looked at the documentation of make_classification, but unfortunately I did not get any further.</p>
<p>I have set the following:</p>
<pre><code>X,y = make_classification(n_samples=50000, n_features=10, n_informative=5, 
                    n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2,
                          class_sep=1,
                   flip_y=0.01, weights=[0.9,0.1], shuffle=True, random_state=42)
</code></pre>
<p>How can I display the - in this case - 5 informative varibles? Can I shape the importance of the features when generating data with make_classification? Which features are intended to be important by make_classification? And then in a next step, I would use some freature_importance methods to verify (or not) how well a model detects the &quot;pre-setted&quot; feature importance/ the variables with influence.</p>
<p>Thank you, any ideas or advice are highly appreciated.</p>
","0","Question"
"78400223","","<p>I'm trying to use GPU Time sharing from <a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/timesharing-gpus"" rel=""nofollow noreferrer"">instructions here</a>, however my workloads will not run on the time-sharing enabled nodes.</p>
<p>I have a node pool with GPU configuration, GPU sharing enabled with Time-sharing for strategy and &quot;Max shared clients per GPU&quot; as 48. The node(s) run fine but I'm unable to run workloads on them using the documented <code>nodeSelector</code> config for my workload, e.g.</p>
<pre><code>nodeSelector:
  cloud.google.com/gke-accelerator: &quot;nvidia-tesla-t4&quot;
  cloud.google.com/gke-max-shared-clients-per-gpu: &quot;48&quot;
  cloud.google.com/gke-gpu-sharing-strategy: time-sharing
</code></pre>
<p>With this my pods get stuck in pending status with message <code>x nodes didn't match Pod's node affinity/selector</code>. If I remove the <code>gke-max-shared-clients-per-gpu</code> and <code>gke-gpu-sharing-strategy</code> key pairs, the pod schedules and runs fine.</p>
<p>When I check the kubernetes labels on the nodes in the gpu time sharing node pool, they do NOT include these labels and I can't add them manually because GCP prevents it.</p>
<p>Any suggestions?</p>
","0","Question"
"78400254","","<p>I have a dataset with 41 features, out of which 4 are text features. I've been given &quot;Bag of Words&quot; numpy arrays (npz) for these four features, which I combined with the other numerical features to train an SVM model. There are a total of 100000 records and 41 features, 4 of which are vectorized as mentioned.</p>
<p>This model has been training for 45 minutes now :). Is there a way to decrease training time? Is there anything wrong with how I pre-process the dataset (particularly combine npz and existing numerical features)? Any other options that I could explore?</p>
<pre><code>title_feature = load_npz('train_title_bow.npz')
overview_feature = load_npz('train_overview_bow.npz')
tagline_feature = load_npz('train_tagline_bow.npz')
production_companies_feature = load_npz('train_production_companies_bow.npz')

numerical_features = df_train[df_train.columns.difference(['title', 'overview', 'tagline', 'production_companies', 'rate_category', 'average_rate', 'original_language'])]
text_features = np.hstack([title_feature.toarray(), overview_feature.toarray(), tagline_feature.toarray(), production_companies_feature.toarray()])
svm_X_train = np.hstack([numerical_features, text_features])
svm_y_train = df_train['rate_category']

svm_classifier = SVC(kernel='linear')  # Linear kernel is used, you can choose other kernels too
svm_classifier.fit(svm_X_train, svm_y_train)
</code></pre>
","-2","Question"
"78401636","","<p>I am trying to use an already trained model to transfer the learning to a model I will create and  modifying only the last few layers. The goal of this is to used the already trained model (trained on millions of images already) to help my model classify food item recognition. I am pretty new to Keras and I am facing an issue that I am now starting understand to  but don't know how to resolve</p>
<pre><code># Load the model from TensorFlow Hub
model_url = &quot;https://www.kaggle.com/models/tensorflow/resnet-50/TensorFlow2/classification/1&quot;
hub_layer = hub.KerasLayer(model_url, input_shape=(224, 224, 3))

# Create a Sequential model
model = tf.keras.Sequential()

# Add the TensorFlow Hub layer to the Sequential model
model.add(hub_layer)

# Build the Sequential model
model.build((None, 224, 224, 3))

# Summary of the model
model.summary()
</code></pre>
<p>ERROR:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[56], line 9
      6 model = tf.keras.Sequential()
      8 # Add the TensorFlow Hub layer to the Sequential model
----&gt; 9 model.add(hub_layer)
     11 # Build the Sequential model
     12 model.build((None, 224, 224, 3))

File c:\Users\Karim\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\sequential.py:95, in Sequential.add(self, layer, rebuild)
     93         layer = origin_layer
     94 if not isinstance(layer, Layer):
---&gt; 95     raise ValueError(
     96         &quot;Only instances of `keras.Layer` can be &quot;
     97         f&quot;added to a Sequential model. Received: {layer} &quot;
     98         f&quot;(of type {type(layer)})&quot;
     99     )
    100 if not self._is_layer_name_unique(layer):
    101     raise ValueError(
    102         &quot;All layers added to a Sequential model &quot;
    103         f&quot;should have unique names. Name '{layer.name}' is already &quot;
    104         &quot;the name of a layer in this model. Update the `name` argument &quot;
    105         &quot;to pass a unique name.&quot;
    106     )

ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: &lt;tensorflow_hub.keras_layer.KerasLayer object at 0x00000190C6B8AD20&gt; (of type &lt;class 'tensorflow_hub.keras_layer.KerasLayer'&gt;)
</code></pre>
","0","Question"
"78402507","","<p>I am using random forest in <code>sklearn</code>, and my dataset is fairly unbalanced (20% positive class, 80% other class). Is there a way to make it train (optimize) for some metric that takes this into consideration, like AUC score or F1-score? Are there any tricks that I can use to nudge it in this direction?
So far, only approach I have thought of / tried is using different class weights.</p>
<p>Alternatively, is there another implementation (or another model, e.g. xgboost) that would allow me such custom metric?</p>
","-2","Question"
"78403537","","<p>I know I can create custom code modules in Azure Designer, but is there a way to connect my ML model I created in AutoML natively?</p>
<p>The AutoML model is using XGBoost, which doesn't appear to be an option under the ML Components feature of Designer. My goal was to create a low code solution, so I'd prefer not to use the custom Python code component.</p>
<p>Any ideas?</p>
<p>Built model using AutoML, need to connect to existing data pipeline in Designer</p>
","0","Question"
"78404705","","<p>I have the following input matrix</p>
<pre><code>inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.7980, 0.0000],
        [1.0000, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.0000, 1.0000]])
</code></pre>
<p>and indices of the elements that I want to exclude (<strong>in this example they are the zero elements, but they can be any value</strong>)</p>
<pre><code>mask_indices = torch.tensor(
[[7, 2],
[2, 6]])
</code></pre>
<p>How can I exclude these elements from the multiplication with the following matrix:</p>
<pre><code>my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696],
        [0.4414, 0.2969, 0.8317]])
</code></pre>
<p>That is, instead of multiplying it including these values (zeros in this example):</p>
<pre><code>a = torch.mm(inp_tensor, my_tensor)
print(a)
tensor([[1.7866, 2.5468, 1.6330],
        [2.2041, 2.5388, 2.3315]])
</code></pre>
<p>I want to exclude the (zero) elements (and the corresponding rows of <code>my_tensor</code>), <strong>so they will not participate in the computational graph</strong>:</p>
<pre><code>inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.6524, 0.6057, 0.3725, 0.7980]]) # remove the elements based on the indices (the zeros here)

my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696]]) # remove the corresponding zero elements rows

b = torch.mm(inp_tensor, my_tensor)
print(b)
&gt;&gt;&gt; tensor([[1.7866, 2.5468, 1.6330]])

inp_tensor = torch.tensor([[1.0000, 0.1115, 0.6524, 0.6057, 0.3725, 1.0000]]) # remove the elements based on the indices (the zeros here)

my_tensor = torch.tensor(
        [
        [0.8823, 0.9150, 0.3829],                
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.4414, 0.2969, 0.8317]])  # remove the corresponding zero elements rows

c = torch.mm(inp_tensor, my_tensor)
print(c)
&gt;&gt;&gt; tensor([[2.2041, 2.5388, 2.3315]])
print(torch.cat([b,c]))
&gt;&gt;&gt; tensor([[1.7866, 2.5468, 1.6330],
        [2.2041, 2.5388, 2.3315]])
</code></pre>
<p>I need this to be efficient (i.e., no <code>for loops</code>), as my tensors are quite large, and also to maintain the gradient (i.e., if I call <code>optimizer.backward()</code> that the relevant parameters from the computational graph be updated)</p>
<p>Note that each of the rows of <code>inp_tensor</code> have the same number of elements to be removed (e.g., zero elements in this example). Hence, each of the rows of <code>mask_indices</code> will also have the same number of elements (e.g., 2 in this example).</p>
","2","Question"
"78404768","","<p>I'm basically trying to fine-tune my model with Neftune. Model is based on Turkish Language. But there I'm receiving zero training lose. I've tried to another model like <a href=""https://huggingface.co/ytu-ce-cosmos/turkish-gpt2-large"" rel=""nofollow noreferrer"">Turkish-GPT2</a> there is no issue everything is okay. I think probably there is problem with model. I don't know how to handle with this issue.</p>
<p>Loading Model:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;asafaya/kanarya-750m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;asafaya/kanarya-750m&quot;)

v3_prompt = &quot;&quot;&quot;Aşağıda, daha fazla bağlam sağlayan bir girdiyle eşleştirilmiş, bir görevi açıklayan bir talimat bulunmaktadır. İsteği uygun şekilde tamamlayan bir yanıt yazın.

### Input:
{}

### Instructions:
{}

### Response:
{}
&quot;&quot;&quot;
</code></pre>
<p>Changing format prompts:</p>
<pre><code>EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    inputs       = examples[&quot;input&quot;]
    instructions = examples['instructions']
    outputs      = examples[&quot;response&quot;]
    texts = []
    for input, instructions, output in zip(inputs, instructions, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = v3_prompt.format(input, instructions, output) + EOS_TOKEN
        texts.append(text)
    return { &quot;text&quot; : texts, }
pass

from datasets import Dataset

dataset = Dataset.from_pandas(data[:40000])
dataset = dataset.map(formatting_prompts_func, batched=True)
</code></pre>
<p>Neftune:</p>
<pre><code>from trl import SFTTrainer
from transformers import TrainingArguments

trainer_2 = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=512,
    neftune_noise_alpha=5,
    packing=False,
    args = TrainingArguments(
        per_device_train_batch_size = 1, #  batch size
        gradient_accumulation_steps = 2, #  gradient accumulation steps
        warmup_steps = 5,
        max_steps = 80,
        learning_rate = 2e-4,
        fp16 = False,
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = &quot;adamw_8bit&quot;,
        weight_decay = 0.01,
        lr_scheduler_type = &quot;linear&quot;,
        seed = 3407,
        output_dir = &quot;outputs&quot;,
    ),
)
trainer_2.train()
</code></pre>
<p>Output:</p>
<pre><code> [80/80 00:25, Epoch 0/1]
Step    Training Loss
1   0.000000
2   0.000000
3   0.000000
4   0.000000
5   0.000000
6   0.000000
7   0.000000
8   0.000000
9   0.000000
10  0.000000
11  0.000000
12  0.000000
13  0.000000
14  0.000000
15  0.000000
16  0.000000
17  0.000000
18  0.000000
19  0.000000
20  0.000000
21  0.000000
22  0.000000
23  0.000000
24  0.000000
25  0.000000
26  0.000000
27  0.000000
28  0.000000
29  0.000000
30  0.000000
31  0.000000
32  0.000000
33  0.000000
34  0.000000
35  0.000000
36  0.000000
37  0.000000
38  0.000000
39  0.000000
40  0.000000
41  0.000000
42  0.000000
43  0.000000
44  0.000000
45  0.000000
46  0.000000
47  0.000000
48  0.000000
49  0.000000
50  0.000000
51  0.000000
52  0.000000
53  0.000000
54  0.000000
55  0.000000
56  0.000000
57  0.000000
58  0.000000
59  0.000000
60  0.000000
61  0.000000
62  0.000000
63  0.000000
64  0.000000
65  0.000000
66  0.000000
67  0.000000
68  0.000000
69  0.000000
70  0.000000
71  0.000000
72  0.000000
73  0.000000
74  0.000000
75  0.000000
76  0.000000
77  0.000000
78  0.000000
79  0.000000
80  0.000000
TrainOutput(global_step=80, training_loss=0.0, metrics={'train_runtime': 25.3789, 'train_samples_per_second': 6.304, 'train_steps_per_second': 3.152, 'total_flos': 117937578909696.0, 'train_loss': 0.0, 'epoch': 0.004})
</code></pre>
","0","Question"
"78405026","","<p>How can I improve the RMSE in my car price estimate?</p>
<ol>
<li>First, I will fill in the missing condition values ​​by estimating it based on the number of kilometers driven.</li>
</ol>
<pre><code>`
new_condition_df = df[df['condition'].map(condition_mapping) == 2]
top_1000_highest_mileage = new_condition_df.nlargest(1000, 'mileage')['mileage']
average_top_1000_highest_mileage = top_1000_highest_mileage.mean()

# Filter the DataFrame for rows where condition is null or unspecified
null_condition_df = df[df['condition'].isnull() | (df['condition'] == '')]

# Update 'condition' based on mileage condition
null_condition_df.loc[null_condition_df['mileage'] &gt;= average_top_1000_highest_mileage, 'condition'] = 'CONDITION_USED'
null_condition_df.loc[null_condition_df['mileage'] &lt; average_top_1000_highest_mileage, 'condition'] = 'CONDITION_NEW'

# Update the original DataFrame with the modified rows
df.update(null_condition_df)
`
</code></pre>
<ol start=""2"">
<li>Deleting a few empty lines</li>
</ol>
<pre><code>columns_with_null = ['color', 'vat_reclaimable', 'cubic_capacity', 'seller_country', 'feature']
df.dropna(subset=columns_with_null, inplace=True)

df['air_conditioning'].fillna('AIRCONDITIONING_NONE', inplace=True)
df['parking_camera'].fillna('PARKINGCAMERA_NONE', inplace=True)
df['parking_sensors'].fillna('PARKINGSENZOR_NONE', inplace=True)
</code></pre>
<ol start=""3"">
<li>Here I am trying to estimate the missing values ​​for the drive column, which contains information about whether the car is 4x4 or 4x2, drive contains a large number of null values, that is why I estimate it in such a complicated way</li>
</ol>
<pre><code>features = ['mileage', 'cubic_capacity', 'power', 'year'] + list(df.columns[df.columns.str.startswith('car_style_')]) + list(df.columns[df.columns.str.startswith('transmission_')]) + list(df.columns[df.columns.str.startswith('fuel_type_')])

train_data = df.dropna(subset=['drive'])  # Odstranění řádků s chybějícími hodnotami sloupce 'drive'
X_train, X_test, y_train, y_test = train_test_split(train_data[features], pd.get_dummies(train_data['drive']), test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

missing_data = df[df['drive'].isnull()]
X_missing = missing_data[features]
predicted_values = model.predict(X_missing)


df_imputed = df.copy()
predicted_df = pd.DataFrame(predicted_values, columns=y_train.columns, index=missing_data.index)
df_imputed.loc[df_imputed['drive'].isnull(), y_train.columns] = predicted_df.values

predicted_df_encoded = pd.DataFrame(predicted_values, columns=y_train.columns, index=missing_data.index)
predicted_df_encoded = (predicted_df_encoded &gt; 0.5).astype(int)

for column in predicted_df_encoded.columns:
    df_imputed[column] = 0  # Přidání sloupce se všemi hodnotami 0
    df_imputed.loc[predicted_df_encoded.index, column] = predicted_df_encoded[column].values  

unique_values_imputed_encoded = df_imputed['drive'].unique()
df = df_imputed
df.drop(columns=['drive'], inplace=True)
</code></pre>
<ol start=""4"">
<li>Here I encode the feature field</li>
</ol>
<pre><code>from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
df = df.join(pd.DataFrame(mlb.fit_transform(df['feature']),columns=mlb.classes_))
df.fillna(0, inplace=True)

df = df.drop(columns=['feature'])
</code></pre>
<ol start=""5"">
<li>The training itself</li>
</ol>
<pre><code>df_encoded = pd.get_dummies(df)

X_train, X_test, y_train, y_test = train_test_split(df_encoded.drop(columns=['price_with_vat_czk']), df_encoded['price_with_vat_czk'], test_size=0.25, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

</code></pre>
<p>All program: <a href=""https://onecompiler.com/python/42brp9a4r"" rel=""nofollow noreferrer"">https://onecompiler.com/python/42brp9a4r</a>
Dataset <a href=""https://filetransfer.io/data-package/a0mFEfg4#link"" rel=""nofollow noreferrer"">https://filetransfer.io/data-package/a0mFEfg4#link</a></p>
<p>My RMSE is something about 64k</p>
","-1","Question"
"78405164","","<p>My name is Lucas, and I'm relatively new to the field of machine learning. I've written this code with the help of some online documentation and tutorials. However, I'd like some assistance in understanding if the integration of RFE() with GBM() is correct.</p>
<pre><code>def evaluateAlgorithm(X_train, X_test, y_train, y_test, dataset):
    Kfold = StratifiedKFold(n_splits=20, shuffle=True)

    GBM = GradientBoostingClassifier(
        loss='log_loss', learning_rate=0.01,
        n_estimators=1000, subsample=0.9,
        min_samples_split=2, min_samples_leaf=1,
        min_weight_fraction_leaf=0.0, max_depth=8,
        init=None, random_state=None,
        max_features=None, verbose=0,
        max_leaf_nodes=None, warm_start=False)

    pipeline = Pipeline(steps=[['feature_selection', RFE(GBM)], ['model', GBM]])

    parameters = {'model__learning_rate': [0.01, 0.02, 0.03],
                  'model__subsample': [0.9, 0.5, 0.3, 0.1],
                  'model__n_estimators': [100, 500, 1000],
                  'model__max_depth': [1, 2, 3],
                  'feature_selection__n_features_to_select': [7, 14, 27]}

    grid_GBM = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=Kfold,
                            verbose=1, n_jobs=-1, refit=True, scoring='accuracy')
    grid_GBM.fit(X_train, y_train)

    print(&quot;\n=========================================================================&quot;)
    print(&quot; Results from Grid Search Gradient Boosting&quot;)
    print(&quot;=========================================================================&quot;)
    print(&quot;\n The best estimator across ALL searched params: \n&quot;, grid_GBM.best_estimator_)
    print(&quot;\n The best score across ALL searched params: \n&quot;, grid_GBM.best_score_)
    print(&quot;\n The best parameters across ALL searched params: \n&quot;, grid_GBM.best_params_)
    print(&quot;\n=========================================================================&quot;)

    # Obtain features selected by RFE
    rfe_selected_features_indices = grid_GBM.best_estimator_['feature_selection'].support_
    rfe_selected_features_names = X_train.columns[rfe_selected_features_indices]
    print(&quot;Features selected by RFE:&quot;, rfe_selected_features_names)

    model_GBM = grid_GBM.best_estimator_

    # Cross-validation
    cv_results_GBM = cross_val_score(model_GBM, X_train, y_train, cv=Kfold, scoring='accuracy', n_jobs=-1, verbose=0)

    print()
    print(&quot;Cross Validation results Gradient Boosting: &quot;, cv_results_GBM)
    prt_string = &quot;CV Mean accuracy: %f (Std: %f)&quot; % (cv_results_GBM.mean(), cv_results_GBM.std())
    print(prt_string)

    trained_Model_GBM = model_GBM.fit(X_train, y_train)

    print();
    print('========================================================')
    print();
    print(trained_Model_GBM.get_params(deep=True))
    print();
    print('=========================================================')

    # Make predictions on the test set
    pred_Labels_GBM = trained_Model_GBM.predict(X_test)
    pred_proba_GBM = trained_Model_GBM.predict_proba(X_test)

    # Evaluate performance
    print();
    print('Evaluation of the trained model Gradient Boosting: ')
    accuracy = accuracy_score(y_test, pred_Labels_GBM)
    print();
    print('Accuracy Gradient Boosting: ', accuracy)
    precision = precision_score(y_test, pred_Labels_GBM, pos_label='positive')
    print();
    print('Precision Gradient Boosting: ', precision)
    recall = recall_score(y_test, pred_Labels_GBM, pos_label='positive')
    print();
    print('Recall Score Gradient Boosting: ', recall)
    f1 = f1_score(y_test, pred_Labels_GBM, pos_label='positive')
    print();
    print('f1 Score Gradient Boosting: ', f1)
    confusion_mat = confusion_matrix(y_test, pred_Labels_GBM)
    classReport = classification_report(y_test, pred_Labels_GBM)
    print();
    print('Classification Report Gradient Boosting: \n', classReport)
    kappa_score = cohen_kappa_score(y_test, pred_Labels_GBM)
    print();
    print('Kappa Score Gradient Boosting: ', kappa_score)

    skplt.estimators.plot_learning_curve(model_GBM, X_train, y_train, figsize=(8, 6))
    plt.show()

    skplt.metrics.plot_roc(y_test, pred_proba_GBM, figsize=(8, 6));
    plt.show()

    skplt.metrics.plot_confusion_matrix(y_test, pred_Labels_GBM, figsize=(8, 6));
    plt.show()

    skplt.metrics.plot_precision_recall(y_test, pred_proba_GBM,
                                        title='Precision-Recall Curve', plot_micro=True,
                                        classes_to_plot=None, ax=None, figsize=(8, 6),
                                        cmap='nipy_spectral', title_fontsize='large',
                                        text_fontsize='medium');
    plt.show()


evaluateAlgorithm(X_train, X_test, y_train, y_test, dataset)

</code></pre>
<p>My goal is to use RFE to find the best combination of features alongside the grid search of GBM for the best hyperparameters. However, it seems that RFE is only finding the best features before the hyperparameters of the grid search. How can I resolve this so that both processes occur simultaneously? The idea is to achieve the best combination of both criteria. Additionally, do you have any suggestions for improving this code?</p>
<p><em><strong>Based on Ben Reiniger's response, I have arrived at the following result.</strong></em></p>
<pre><code>    pipeline = Pipeline(steps=[['RFE', RFE(estimator=GBM)]])

parameters = {
    'RFE__estimator__learning_rate': [0.001, 0.01, 0.05],#0.01, 0.02, 0.03, 0.05
    'RFE__estimator__subsample': [0.9, 0.5, 0.1], #0.7,  0.3,
    'RFE__estimator__n_estimators': [500, 1000, 2000], #1500
    'RFE__estimator__max_depth': [4, 5, 6,], #3, 7, 8
    #'RFE__estimator__max_features': ['auto', 'sqrt', 'log2', None],
    #'RFE__estimator__min_samples_split': [2, 5, 10],
    #'RFE__estimator__min_samples_leaf': [1, 2, 4],
    #'RFE__estimator__max_leaf_nodes': [None, 5, 10, 20],
    'RFE__n_features_to_select': [5, 10, 20]
}
</code></pre>
","0","Question"
"78406265","","<p>I have developed machine learning model (XGBClassifier) using different sets of variables, and both sets achieve an accuracy score of around 80%. However, one model uses 17 variables, while the other model uses 18 variables. I'm unsure which model to choose for deployment. Here are some factors I'm considering:</p>
<ol>
<li><p>Interpretability: I understand that simpler models with fewer variables are generally preferred for ease of interpretation. Does this mean I should favor the model with 17 variables?</p>
</li>
<li><p>Overfitting: Would the model with 18 variables be more prone to overfitting due to its higher complexity, even though using both variables sets have similar accuracy scores?</p>
</li>
<li><p>Computational Efficiency: Would the model with fewer variables (17) be more computationally efficient in terms of training and prediction time?</p>
</li>
<li><p>Feature Importance: How can I assess the importance of the additional variable in the model with 18 variables? Is there a way to determine whether it provides meaningful insights or improves performance?</p>
</li>
<li><p>Data Quality: Should I consider the quality and dimensionality of my dataset in making this decision? Are there any risks associated with adding an extra variable, such as increased susceptibility to noise or overfitting?</p>
</li>
<li><p>Information: Does more variables gives the model more information, which leads to better decision?</p>
</li>
<li><p>Consideration: I'm on Kaggle Private Competition, so evaluation is considered more.</p>
</li>
</ol>
<p>Given these considerations, how should I approach the decision of choosing between these two sets with similar accuracy but different numbers of variables? Are there any best practices or guidelines I should follow in such cases?</p>
<p>Any insights or advice would be greatly appreciated. Thank you!</p>
","0","Question"
"78407603","","<p>I am trying to make <code>XGBoost</code> work with the hierarchical classifier package available <a href=""https://github.com/globality-corp/sklearn-hierarchical-classification/tree/develop"" rel=""nofollow noreferrer"">here</a> (repo archived).</p>
<p>I can confirm the module works fine with sklearn's random forest classifier (and other sklearn modules that I checked with). But I cannot get it work with <code>XGBoost</code>. I understand there's some modification needed for the hierarchical classifier to work, but I cannot figure out this modification.</p>
<p>I give below, a <code>MWE</code> to reproduce the issue (assuming the library is installed via - <code>pip install sklearn-hierarchical-classification</code>):</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import  load_digits
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn_hierarchical_classification.classifier import HierarchicalClassifier
from sklearn_hierarchical_classification.constants import ROOT
from sklearn_hierarchical_classification.metrics import h_fbeta_score, multi_labeled
#from sklearn_hierarchical_classification.tests.fixtures import make_digits_dataset
</code></pre>
<p>We want to build the following class hierarchy along with data from the handwritten digits dataset:</p>
<pre><code>         &lt;ROOT&gt;
          /   \
         A     B
       /  \   /  \
      1   7  C    9
            / \
           3   8
</code></pre>
<p>Like so:</p>
<pre class=""lang-py prettyprint-override""><code>def make_digits_dataset(targets=None, as_str=True):
    &quot;&quot;&quot;Helper function: from sklearn_hierarchical_classification.tests.fixtures module &quot;&quot;&quot;
    X, y = load_digits(return_X_y=True)
    if targets:
        ix = np.isin(y, targets)
        X, y = X[np.where(ix)], y[np.where(ix)]

    if as_str:
        # Convert targets (classes) to strings
        y = y.astype(str)

    return X, y

class_hierarchy = {
    ROOT: [&quot;A&quot;, &quot;B&quot;],
    &quot;A&quot;: [&quot;1&quot;, &quot;7&quot;],
    &quot;B&quot;: [&quot;C&quot;, &quot;9&quot;],
    &quot;C&quot;: [&quot;3&quot;, &quot;8&quot;],
    }

</code></pre>
<p>So that:</p>
<pre class=""lang-py prettyprint-override""><code>base1 = RandomForestClassifier()
base2 = XGBClassifier()

clf = HierarchicalClassifier(
    base_estimator=base1,
    class_hierarchy=class_hierarchy,
    )

X, y = make_digits_dataset(targets=[1, 7, 3, 8, 9],
                            as_str=False, )
y = y.astype(str)

X_train, X_test, y_train, y_test = train_test_split(
     X, y, test_size=0.2, random_state=RANDOM_STATE, )

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

with multi_labeled(y_test, y_pred, clf.graph_) as (y_test_, y_pred_, graph_):
  h_fbeta = h_fbeta_score(
      y_test_, y_pred_, graph_, )

print(&quot;h_fbeta_score: &quot;, h_fbeta)
h_fbeta_score:  0.9690011481056257
</code></pre>
<p>Works fine. But with <code>XGBClassifier</code> (<code>base2</code>) raises the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;~/hierarchical-classification.py&quot;, line 62, in &lt;module&gt;
    clf.fit(X_train, y_train)
  File &quot;~/venv/lib/python3.10/site-packages/sklearn_hierarchical_classification/classifier.py&quot;, line 206, in fit
    self._recursive_train_local_classifiers(X, y, node_id=self.root, progress=progress)
  File &quot;~/venv/lib/python3.10/site-packages/sklearn_hierarchical_classification/classifier.py&quot;, line 384, in _recursive_train_local_classifiers
    self._train_local_classifier(X, y, node_id)
  File &quot;~/venv/lib/python3.10/site-packages/sklearn_hierarchical_classification/classifier.py&quot;, line 453, in _train_local_classifier
    clf.fit(X=X_, y=y_)
  File &quot;~/venv/lib/python3.10/site-packages/xgboost/core.py&quot;, line 620, in inner_f
    return func(**kwargs)
  File &quot;~/venv/lib/python3.10/site-packages/xgboost/sklearn.py&quot;, line 1438, in fit
    or not (self.classes_ == expected_classes).all()
AttributeError: 'bool' object has no attribute 'all'

</code></pre>
<p>I understand this error got to do with this section of the call to the <code>fit()</code> method in <code>xgboost.sklearn.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code>1436            if (
1437                self.classes_.shape != expected_classes.shape
1438                or not (self.classes_ == expected_classes).all()
1439            ):
1440                raise ValueError(
1441                    f&quot;Invalid classes inferred from unique values of `y`.  &quot;
1442                    f&quot;Expected: {expected_classes}, got {self.classes_}&quot;
1443                )
</code></pre>
<p>Expected value of <code>y</code>:  <code>[0 1]</code>, but got <code>['A' 'B']</code> (internal nodes).
There must be a way to modify the class <code>sklearn_hierarchical_classification.classifier.HierarchicalClassifier.py</code> so that it works fine with <code>xgboost</code>.</p>
<p>What's the fix to this?</p>
","1","Question"
"78407668","","<p>I am running some Machine Learning algorithms in order to train a model.</p>
<p>Until now I've been doing a correlation matrix in order to select the characteristics with highest correlaction to my target variable.</p>
<p>I read online that doing this selection is not necessary unless I am running Logistic Regression. Is this true?</p>
<p>The algorithms that I am running are Logistic Regression, Decision Tree, SVM, KNN and Naive Bayes.</p>
<p>Should I use my training set with all the characteristics for all the algorithms except Logistic Regression and another version with only the most correlated variables for Logistic Regression?</p>
","0","Question"
"78408313","","<p>I'm using tensorflow and Keras to do transfer learning using Resnet_50. The issue I am having is that my model seems to be doing good on accuracy but my val_loss is extremely high as well as when i try to make predictitions the accuracy is very low.</p>
<p>Here are the relevent parts of code:</p>
<pre><code># Create an ImageDataGenerator with data augmentation for training
data_generator = keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    validation_split=0.5,
    rotation_range=30,  # Random rotations
    width_shift_range=0.2,  # Horizontal shifts
    height_shift_range=0.2,  # Vertical shifts
    shear_range=0.2,  # Shear transformations
    zoom_range=0.2,  # Zoom
    horizontal_flip=True,  # Horizontal flips
)

# Load and preprocess training data
train_data_flow = data_generator.flow_from_directory(
    dataset_path,
    target_size=(224, 224),  # Resize images to 224x224
    batch_size=32,
    class_mode='categorical',
    subset='training'  # Use training subset
)

# Load and preprocess validation data
val_data_flow = data_generator.flow_from_directory(
    dataset_path,
    target_size=(224, 224),  # Resize images to 224x224
    batch_size=32,
    class_mode='categorical',
    subset='validation'  # Use validation subset
)
</code></pre>
<p>Loading The Model</p>
<pre><code># Load the model from TensorFlow Hub
model_url = &quot;https://tfhub.dev/tensorflow/resnet_50/feature_vector/1&quot;
hub_layer = hub.KerasLayer(model_url, input_shape=(224, 224, 3) , trainable=False)

# Create a Sequential model with dropout and batch normalization
model = keras.Sequential([
    hub_layer,
    layers.Dropout(0.2),  # Lower dropout rate
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),  # Batch normalization
    layers.Dropout(0.5),  # Dropout
    layers.Dense(9, activation='softmax')
])

# Build the Sequential model
model.build((None, 224, 224, 3))

# Summary of the model
model.summary()
</code></pre>
<p>I then compile :</p>
<pre><code># Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
</code></pre>
<p>And Finally :</p>
<pre><code># Fit the model with early stopping
history = model.fit(
    train_data_flow,
    validation_data=val_data_flow,
    epochs=15,  # Number of epochs,
)
</code></pre>
<p>Here are the epochs:</p>
<p>Epoch 1/15</p>
<pre><code>81/81 [==============================] - 489s 6s/step - loss: 0.3773 - accuracy: 0.8832 - val_loss: 0.7994 - val_accuracy: 0.7476
Epoch 2/15
81/81 [==============================] - 489s 6s/step - loss: 0.3316 - accuracy: 0.8980 - val_loss: 0.8229 - val_accuracy: 0.7378
Epoch 3/15
81/81 [==============================] - 488s 6s/step - loss: 0.3468 - accuracy: 0.8879 - val_loss: 0.8221 - val_accuracy: 0.7362
Epoch 4/15
81/81 [==============================] - 489s 6s/step - loss: 0.3148 - accuracy: 0.9011 - val_loss: 0.8380 - val_accuracy: 0.7362
Epoch 5/15
81/81 [==============================] - 488s 6s/step - loss: 0.3250 - accuracy: 0.8972 - val_loss: 0.7680 - val_accuracy: 0.7409
Epoch 6/15
81/81 [==============================] - 491s 6s/step - loss: 0.3100 - accuracy: 0.9026 - val_loss: 0.7220 - val_accuracy: 0.7616
Epoch 7/15
81/81 [==============================] - 491s 6s/step - loss: 0.2844 - accuracy: 0.9120 - val_loss: 0.7259 - val_accuracy: 0.7651
Epoch 8/15
81/81 [==============================] - 490s 6s/step - loss: 0.2811 - accuracy: 0.9007 - val_loss: 0.7722 - val_accuracy: 0.7511
Epoch 9/15
81/81 [==============================] - 490s 6s/step - loss: 0.2689 - accuracy: 0.9167 - val_loss: 0.7943 - val_accuracy: 0.7433
Epoch 10/15
81/81 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9182
</code></pre>
<p>Predicting:</p>
<pre><code>Model Accuracy on Test Set: 0.2021069059695669
</code></pre>
","-1","Question"
"78409561","","<p>I am building a sales prediction model which consists of &quot;Year&quot;, &quot;Month&quot;, &quot;Economy Indicator&quot;, &quot;Customer_Id&quot;, &quot;Product_Id&quot;, &quot;Quantity&quot;, &quot;Sales&quot;, &quot;Margin&quot;.</p>
<p>The cleaned dataset contains about 1.5 millions of rows with the above 8 columns, which was the sales per month per customer per product for the past 6 years. My end goal is being able to predict the sales for the upcoming months for the entire up coming year, but more precisely, the prediction will be on product per customer level, which is a very detailed level.</p>
<p>However, Since my Customer_Id and Product_Id are TEXT, such as &quot;A77BC&quot;, and there are over 100000 unique product_id and 6000 unique customer_id, if I use one hot encoding to label them, the dimentionality will be too high for my device to handle, (for example, my laptop has 16G ram but label the customer_id already requires 24G ram) And I believe there must be a better way of handling such situation, but I am very new to machine learning.</p>
","-1","Question"
"78411015","","<p><a href=""https://i.sstatic.net/24nEZCM6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/24nEZCM6.png"" alt=""enter image description here"" /></a></p>
<p>Does the learning curve suggest overfitting or an acceptable level of model performance? The results are based on xgboost. Do I need to re-tune the hyperparamters? If so, how to tune the hyperparameters? Currently, I use we used BayesSearchCV from scikit-optimize to automatically tune the hyperparameters. My search space is</p>
<pre><code>from skopt.space import Real, Integer
search_spaces = {'learning_rate': Real(0.0001, 0.04, 'uniform'), 
                 'max_depth': Integer(2, 20),
                 'subsample': Real(0.1, 1.0, 'uniform'),
                 'colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns by tree
                 'reg_lambda': Real(1e-9, 100., 'uniform'), # L2 regularization, default = 0
                 'reg_alpha': Real(1e-9, 100., 'uniform'), # L1 regularization, default = 0
                 'n_estimators': Integer(100, 3000), # number of boosting rounds or the number of decision trees
                 'min_child_weight': Real(2, 8, 'uniform'),
                 'gamma': Real(0.1, 0.9, 'uniform') 
}
</code></pre>
","-1","Question"
"78412765","","<p>I am having issues with transfer learning the Mobilenetv2 for image classification of food items. I had initially used resnet but was facing the same overfitting issue but mobilenet runs faster so i decided to stick with that. I've posted code snippits below:
RELEVENT CODE:</p>
<pre><code># Create an ImageDataGenerator with data augmentation for training
data_generator = keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=keras.applications.mobilenet.preprocess_input,
    # rescale=1./255,
    validation_split=0.3,
    rotation_range=30,  # Random rotations
    width_shift_range=0.2,  # Horizontal shifts
    height_shift_range=0.2,  # Vertical shifts
    shear_range=0.2,  # Shear transformations
    zoom_range=0.2,  # Zoom
    horizontal_flip=True,  # Horizontal flips
)

# Load and preprocess training data
train_data_flow = data_generator.flow_from_directory(
    dataset_path,
    target_size=(128, 128),  # Resize images to 224x224
    batch_size=32,
    class_mode='categorical',
    subset='training'  # Use training subset
)



# Load and preprocess validation data
val_data_flow = data_generator.flow_from_directory(
    dataset_path,
    target_size=(128, 128),  # Resize images to 224x224
    batch_size=32,
    class_mode='categorical',
    subset='validation'  # Use validation subset
)
</code></pre>
<p>Creating the model:</p>
<pre><code># Load the model from TensorFlow Hub
    model_url = &quot;https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/035-128-classification/2&quot;
    hub_layer = hub.KerasLayer(model_url, input_shape=(128, 128, 3) ,  trainable=True, arguments=dict(batch_norm_momentum=0.997))

# Create a Sequential model with dropout and batch normalization
model = keras.Sequential([
    hub_layer,
    layers.Dropout(0.2),  # Lower dropout rate
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),  # Batch normalization
    layers.Dropout(0.2),  # Dropout
    layers.Dense(9, activation='softmax')
])

# Build the Sequential model
model.build((None, 128, 128, 3))

# Summary of the model
model.summary()
</code></pre>
<p>Fitting the model:</p>
<pre><code># Define the EarlyStopping callback
early_stopping_callback = keras.callbacks.EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=20,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
)

# Fit the model with early stopping
history = model.fit(
    train_data_flow,
    validation_data=val_data_flow,
    epochs=100,
    callbacks=[early_stopping_callback],  # Pass the EarlyStopping callback as a list
    shuffle= True
)
</code></pre>
<p>Typical case of overfitting. I've tried data augmentation, different splits for training and validation , shuffling etc. Any Ideas?</p>
<p>Here are some of the epochs:</p>
<pre><code>Epoch 1/100
113/113 [==============================] - 73s 641ms/step - loss: 0.5302 - accuracy: 0.8623 - val_loss: 3.2597 - val_accuracy: 0.4723
Epoch 2/100
113/113 [==============================] - 70s 618ms/step - loss: 0.4798 - accuracy: 0.8751 - val_loss: 2.4527 - val_accuracy: 0.6070
Epoch 3/100
113/113 [==============================] - 70s 621ms/step - loss: 0.4891 - accuracy: 0.8695 - val_loss: 2.0448 - val_accuracy: 0.6018
Epoch 4/100
113/113 [==============================] - 70s 624ms/step - loss: 0.4618 - accuracy: 0.8851 - val_loss: 2.0285 - val_accuracy: 0.5764
Epoch 5/100
113/113 [==============================] - 70s 618ms/step - loss: 0.4320 - accuracy: 0.8948 - val_loss: 1.7363 - val_accuracy: 0.6344
Epoch 6/100
113/113 [==============================] - 70s 619ms/step - loss: 0.3853 - accuracy: 0.9051 - val_loss: 1.9907 - val_accuracy: 0.5901
Epoch 7/100
113/113 [==============================] - 70s 618ms/step - loss: 0.4188 - accuracy: 0.8909 - val_loss: 2.0851 - val_accuracy: 0.5777
Epoch 8/100
113/113 [==============================] - 71s 625ms/step - loss: 0.3877 - accuracy: 0.9062 - val_loss: 1.4624 - val_accuracy: 0.6630
Epoch 9/100
113/113 [==============================] - 70s 621ms/step - loss: 0.3589 - accuracy: 0.9115 - val_loss: 1.6237 - val_accuracy: 0.6376
Epoch 10/100
113/113 [==============================] - 70s 618ms/step - loss: 0.3782 - accuracy: 0.8998 - val_loss: 1.6615 - val_accuracy: 0.6207
Epoch 11/100
113/113 [==============================] - 70s 621ms/step - loss: 0.3714 - accuracy: 0.9040 - val_loss: 2.5165 - val_accuracy: 0.4782
Epoch 12/100
113/113 [==============================] - 71s 629ms/step - loss: 0.3514 - accuracy: 0.9149 - val_loss: 1.9595 - val_accuracy: 0.5901
Epoch 13/100
...
113/113 [==============================] - 76s 669ms/step - loss: 0.2445 - accuracy: 0.9466 - val_loss: 1.3978 - val_accuracy: 0.6630
Epoch 44/100
113/113 [==============================] - 71s 632ms/step - loss: 0.2272 - accuracy: 0.9527 - val_loss: 1.3801 - val_accuracy: 0.6760
</code></pre>
<p>FOR PREDICITONS:</p>
<pre><code>from sklearn.metrics import classification_report


predictions = model.predict(val_data_flow)

# Convert predictions to class labels
predicted_classes = np.argmax(predictions, axis=1)

# Step 4: Evaluate the Model

# Get the true labels from the test data
true_labels = val_data_flow.classes  

# Calculate accuracy
accuracy = np.mean(predicted_classes == true_labels)  

# Accuracy as the score
print(&quot;Model Accuracy on Test Set:&quot;, accuracy)

# sklearn's classification_report for more detailed metrics
print(&quot;Classification Report:&quot;)
print(classification_report(true_labels, predicted_classes, target_names=list(val_data_flow.class_indices.keys())))
</code></pre>
<p>Results:</p>
<pre><code>Model Accuracy on Test Set: 0.19388418998048146
Classification Report:
              precision    recall  f1-score   support

           1       0.32      0.30      0.31       495
           2       0.00      0.00      0.00        21
           3       0.17      0.21      0.19       199
           4       0.13      0.09      0.11       154
           5       0.00      0.00      0.00         8
           6       0.11      0.06      0.08        32
           7       0.11      0.07      0.08       199
           8       0.16      0.19      0.17       231
           9       0.13      0.18      0.15       198

    accuracy                           0.19      1537
   macro avg       0.12      0.12      0.12      1537
weighted avg       0.19      0.19      0.19      1537
</code></pre>
","-1","Question"
"78415660","","<p>I am creating CNN model to recognize dogs and cats. I trained it and when I evaluate accuracy of it by hand it has 80-85% accuracy on an unseen data.</p>
<p>But, when I try to use library torchmetrics.accuracy to calculate my accuracy then for some reason I get wrong accuracy calculations. Let me explain:</p>
<p>The code of the model(I use python, torch, lightning to optimize the model and code):</p>
<pre><code>import lightning as L
import torch
import torchmetrics
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
from torchvision.transforms import ToTensor

from CustomDataset import CustomDataset


class Model(L.LightningModule):
    def __init__(self, batch_size, learning_rate, num_classes):
        super(Model, self).__init__()
        self.save_hyperparameters()
        ## HERE GOES MODEL LAYERS CRITERION etc

        self.accuracy = torchmetrics.Accuracy(num_classes=2, average='macro', task='multiclass')

        self.test_transform = transforms.Compose([
            transforms.Resize((200, 200)),  # Resize images to 256x256
            transforms.ToTensor(),  # Convert images to PyTorch tensors
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images
        ])
        self.transform = transforms.Compose([
            transforms.RandomResizedCrop(200),  # Randomly crops and resizes images to 224x224
            transforms.RandomHorizontalFlip(p=0.5),  # Randomly flips images horizontally
            transforms.RandomRotation(15),  # Resize images to 256x256
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),  # Convert images to PyTorch tensors
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images
        ])

    def forward(self, image):
        image = F.relu(self.conv1(image))
        image = self.pool(image)
        image = F.relu(self.conv2(image))
        image = self.pool(image)
        image = F.relu(self.conv3(image))
        image = self.pool(image)  # Output is now (128, 25, 25)
        image = torch.flatten(image, 1)  # Flatten the output
        image = F.relu(self.fc1(image))
        image = self.fc2(image)
        return image

    def training_step(self, batch, batch_idx):
        images, labels = batch
        predictions = self(images)  # Forward pass
        loss = self.criterion(predictions, labels)  # Compute the loss
        predicted_classes = torch.argmax(F.softmax(predictions, dim=1), dim=1)
        predictions_softmax = F.softmax(predictions, dim=1)
        acc = self.accuracy(predictions_softmax, labels)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)
        return loss  # Returning the loss for backpropagation

    def validation_step(self, batch, batch_idx):
        images, labels = batch
        predictions = self(images)
        loss = self.criterion(predictions, labels)
        predicted_classes = torch.argmax(F.softmax(predictions, dim=1), dim=1)
        predictions_softmax = F.softmax(predictions, dim=1)
        acc = self.accuracy(predictions_softmax, labels)
        self.log('val_loss', loss, prog_bar=True)
        self.log('val_acc', acc, prog_bar=True)
        return loss

    def test_step(self, batch, batch_idx):
        images, labels = batch
        predictions = self(images)  # Forward pass
        loss = self.criterion(predictions, labels)  # Compute the loss
        predicted_classes = torch.argmax(F.softmax(predictions, dim=1), dim=1)
        predictions_softmax = F.softmax(predictions, dim=1)
        acc = self.accuracy(predictions_softmax, labels)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)
        return loss  # Returning the loss for backpropagation
        # images, labels = batch
        # predictions = self(images)
        # loss = self.criterion(predictions, labels)
        # predicted_classes = torch.argmax(F.softmax(predictions, dim=1), dim=1)
        # predictions_softmax = F.softmax(predictions, dim=1)
        # acc = self.accuracy(predictions_softmax, labels)
        # real_step_acc = (labels == predicted_classes).sum() / self.batch_size
        # self.log('test_loss', loss, prog_bar=True)
        # self.log('real_test_acc', real_step_acc, prog_bar=True)
        # self.log('test_acc', acc, prog_bar=True)
        # return loss

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.9)
        return optimizer

    def train_dataloader(self):
        # Set up and return the training DataLoader
        filepath_train = &quot;dataset/test/&quot;

        train_dataset = datasets.ImageFolder(root=filepath_train, transform=self.transform)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=16)

        return train_loader

    def test_dataloader(self):
        # Set up and return the training DataLoader
        filepath_train = &quot;dataset/test/&quot;

        test_dataset = datasets.ImageFolder(root=filepath_train, transform=self.transform)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True, num_workers=16)

        return test_loader

    def val_dataloader(self):
        # Set up and return the validation DataLoader
        filepath_train = &quot;dataset/val/&quot;

        val_dataset = datasets.ImageFolder(root=filepath_train, transform=self.test_transform)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=16)

        return val_loader
</code></pre>
<p>Output is like this:
train_acc_epoch        0.7635096907615662
real_test_acc         0.7901701927185059
test_acc            0.39825108647346497</p>
<p>Real test accuracy I compute like this:</p>
<pre><code>predictions_softmax = F.softmax(predictions, dim=1)
acc = self.accuracy(predictions_softmax, labels)
real_step_acc = (labels == predicted_classes).sum() / self.batch_size
</code></pre>
<p>So the problem is:
When I run the testing then the test accuracy inside test_step method is 40% but the real test accuracy that I compute myself is 80-85%.
so what I tried:
When I enable shuffling on test data(I know it is bad practice but it was part of the debugging), torchmetrics.accuracy becomes correct! It outputs 80-85% accuracy.</p>
<p>So why the shuffling changes the thing? Is this some kind of bug?</p>
","0","Question"
"78416214","","<p>I have code that looks like this</p>
<pre><code>clf = lgb.LGBMClassifier(max_depth=3, verbosity=-1, n_estimators=3)
clf.fit(train_data[features], train_data['y'], sample_weight=train_data['weight'])
print (f&quot;I have {clf.n_estimators_} estimators&quot;)
fig, ax = plt.subplots(nrows=4, figsize=(50,36), sharex=True)
lgb.plot_tree(clf, tree_index=7, dpi=600, ax=ax[0]) # why does it have 7th tree?
lgb.plot_tree(clf, tree_index=8, dpi=600, ax=ax[1]) # why does it have 8th tree?
#lgb.plot_tree(clf, tree_index=9, dpi=600, ax=ax[2]) # crashes
#lgb.plot_tree(clf, tree_index=10, dpi=600, ax=ax[3]) # crashes
</code></pre>
<p>I am surprised that despite <code>n_estimators=3</code>, I seem to have 9 trees? How do I actually set the number of trees, and related to that, what does <code>n_estimators</code> do? I've read the docs, and I thought it would be the number of trees, but it seems to be something else.</p>
<p>Separately, how do I interpret the separate trees, with their ordering, 0, 1, 2, etc. I know random forest, and how there every tree is equally important. In boosting, the first tree is most important, the next one significantly less, the next significantly less. So in my head, when I look at the tree diagrams, how can I &quot;simulate&quot; the LightGBM inference process?</p>
","0","Question"
"78418098","","<p>We are developing an iOS app that lets users send customizable digital cards. Users can choose from various card templates, enter their own text, and make edits to the card as they like. We also have a feature where users can provide a short message, like &quot;happy birthday mom,&quot; and receive an expanded version of the text, such as &quot;happy birthday to my special mother! I love you and hope you have an amazing day.&quot;</p>
<p>I'm conducting research to figure out how to accomplish this and planning to create a model using Natural Language Processing (NLP) and CoreML. However, I've encountered an issue in finding a suitable dataset for this particular task. As a result, I'm interested in building an accurate dataset specifically tailored to this purpose. However, I'm unsure where I can obtain the necessary data or if there are alternative data sources available for quick use.</p>
<p>If you have any insights or alternative methods to achieve this feature, please share them.</p>
","1","Question"
"78418741","","<p>I want to insert the string into the machine learning model, but it keeps saying this:</p>
<pre><code>    prediction = model.predict(email_features_array)
                 ^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'predict'
</code></pre>
<p>I don't know what the problem is, I changed the decoding method several times.</p>
<p>This is the function I'm using after I do vectorzation:</p>
<pre><code>def predict():
    email_text =&quot;hello world&quot;
    cv = CountVectorizer()
    email_features_array = cv.fit_transform([email_text])
    # email_features_array = fit_count_vectorizer(email_text)
    print(email_features_array)
    # Assume `model` is already defined and trained
    prediction = model.predict(email_features_array)

    # Apply the pre-trained model to predict the probability of the email being phishing
    probability = model.predict_proba(email_features_array)
    if prediction[0] == 1:
        result = 'Phishing'
        probability_score = probability[0][1] * 100
        print(result, probability_score)
    else:
        result = 'Legitimate'
        probability_score = probability[0][0] * 100
        print(result, probability_score)
</code></pre>
<p>I expect that there is a problem with how I decode it but not sure.</p>
","0","Question"
"78419199","","<p>I have this dataset:</p>
<pre><code>(26.5625,0)
(29.5625,0)
(30.390625,0)
(18.640625,0)
(27.984375,0)
(26.984375,0)
(25.703125,0)
(25.78125,0)
(32.09375,0)
(25.59375,0)
(27.703125,0)
(30.828125,0)
(23.578125,0)
(21.890625,0)
(25.734375,0)
(24.65625,0)
(27.46875,0)
(31.640625,0)
(26.53125,0)
(25.078125,0)
(30.65625,0)
(24.515625,0)
(25.21875,0)
(21.78125,0)
(28.984375,0)
(29.765625,0)
(27.171875,1)
(30.46875,1)
(35.3125,1)
(27.90625,1)
(34.9375,1)
(33.4375,1)
(30.90625,1)
(31.671875,1)
(32.40625,1)
(26.078125,1)
(31.171875,1)
(36.21875,1)
(35.0625,1)
(35.65625,1)
(36.65625,1)
(37.96875,1)
(31.953125,1)
(33.15625,1)
(37.34375,1)
</code></pre>
<p>And the ordering with the corresponding precision is:</p>
<pre><code>ordered_labels: [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

average precision: 0.7338
</code></pre>
<p>I'm trying to find the threshold (for example 27.0) which returns the highest precision (0.7338 in that case), which means as many as possible the '1' are on the left, and as low as possible the '0' on the right. (For example: [1, 1, 1, 0, 0, 0] has a precision of 1.0) I've tried logistic regression but I'm getting back threshold as '0.7' instead of a number such as 27.0.
Should I use linear regression or svm for that dataset?</p>
<p>My outputs: (code below)</p>
<pre><code>Precision: [0.33333333 0.         0.         1.        ]
Recall: [1. 0. 0. 0.]
Threshold: [0.13154558 0.7006058  0.72969373]
</code></pre>
<p>This is the code I am using:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from ast import literal_eval

# Create a simple dataset
scores_labels_path = 'data.txt'
X, y = [], []
with open(scores_labels_path) as file:
    for line in file:
        line = literal_eval(line.rstrip())
        X.append(line[0])
        y.append(line[1])

X = np.array(X).reshape(-1, 1)
y = np.array(y)
# X1, y1 = make_classification(n_samples=1000, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)
y_scores = lr.predict_proba(X_test)
precision, recall, threshold = precision_recall_curve(y_test, y_scores[:, 1])

print(&quot;Precision: {}&quot;.format(precision))
print(&quot;Recall: {}&quot;.format(recall))
print(&quot;Threshold: {}&quot;.format(threshold))
</code></pre>
","0","Question"
"78419755","","<p>I'm encountering an issue with PyLance in Visual Studio Code where the tensorflow.keras namespace isn't being recognized, leading to incomplete IntelliSense and auto-completion. It appears that this problem stems from the lazy loading functionality (_KerasLazyLoader) utilized by TensorFlow to defer loading unnecessary packages until they are needed.</p>
<p>I've searched through existing questions on this topic, but the most recent discussions I found date back to late 2023. Additionally, the proposed solutions primarily involve workarounds that may not be reliable in the long term.</p>
<p>Is there an official solution to ensure that PyLance properly recognizes the tensorflow.keras namespace? I can't imagine lazy loading in general will break VSCode intellisense (although it is certainly possible I am wrong). Alternatively, are there any robust workarounds that are less likely to break in future updates? Any insights or guidance would be greatly appreciated.</p>
<p>Software:
MacOS Sonoma 14.1.2
Python 3.12
tensorflow 2.16.1</p>
<p>References to previous questions/issues being posted:</p>
<ol>
<li><a href=""https://github.com/microsoft/pylance-release/issues/3249"" rel=""nofollow noreferrer"">https://github.com/microsoft/pylance-release/issues/3249</a></li>
<li><a href=""https://stackoverflow.com/questions/73496946/vscode-autocomplete-and-suggestion-intellisense-doesnt-work-for-tensorflow-an"">VSCode Autocomplete And Suggestion (IntelliSense) Doesn&#39;t Work For Tensorflow And Keras Libraries?</a></li>
<li><a href=""https://community.deeplearning.ai/t/unable-to-import-tensorflow-keras-pylinte0401-import-error-in-visual-studio-code/512586"" rel=""nofollow noreferrer"">https://community.deeplearning.ai/t/unable-to-import-tensorflow-keras-pylinte0401-import-error-in-visual-studio-code/512586</a></li>
<li><a href=""https://jagaimox.wordpress.com/2020/12/28/configure-python-intellisense-on-vscode-for-tensorflow-1-14-or-1-15/"" rel=""nofollow noreferrer"">https://jagaimox.wordpress.com/2020/12/28/configure-python-intellisense-on-vscode-for-tensorflow-1-14-or-1-15/</a> (tensorflow 1.14/1.15)</li>
</ol>
<p>I tried a couple of solutions in the above links and they didn't work for me. It is possible some of them require some additional configuration that I would need to do which I didn't (for instance, one referenced _typing in tensorflow.<strong>init</strong> but no _typing was already defined - could be outdated).</p>
","1","Question"
"78420048","","<pre><code>  prediction = model.predict(email_features_array)
                 ^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'predict'

</code></pre>
<p>i have already built and trained ML Model i only wanna use it through flask, but whenever it's about enter the mode.predict it gives me this issue</p>
<p>this is my code:</p>
<p>these are my imports</p>
<pre><code>import pickle
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np  # Import NumPy
</code></pre>
<h1>Load the pre-trained model</h1>
<pre><code>
model = pickle.load(open('c:/Users/7rbe2/OneDrive/سطح المكتب/Main projects/Grad project/Phishward/PhishWarden/app/Python/logistic_regression_model.pkl', 'rb'))

# Initialize CountVectorizer
cv = CountVectorizer()
</code></pre>
<p>here are functions for cleaning</p>
<pre><code>def remove_special_characters(word):
    return word.translate(str.maketrans('', '', string.punctuation))

def remove_stop_words(words):
    stop_words = set(stopwords.words('english'))
    return [word for word in words if word not in stop_words]

def remove_hyperlink(word):
    return re.sub(r&quot;http\S+&quot;, &quot;&quot;, word)

def fit_count_vectorizer(text):
    # Clean and tokenize the text
    cleaned_text = remove_special_characters(text)
    cleaned_text = remove_hyperlink(cleaned_text)
    tokens = word_tokenize(cleaned_text)
    tokens = remove_stop_words(tokens)
    cleaned_text = ' '.join(tokens)
    return cleaned_text  # Return the cleaned text
</code></pre>
<p>here are the prediciton where th problem occurs</p>
<pre><code>def predict():
    email_text =&quot;hello world&quot;
    cleaned_text = fit_count_vectorizer(email_text)
    cv.fit([cleaned_text])  # Fit CountVectorizer on the cleaned text
    email_features_array = cv.transform([cleaned_text])  # Use transform instead of fit_transform
    # Assume `model` is already defined and trained
    prediction = model.predict(email_features_array)

    # Apply the pre-trained model to predict the probability of the email being phishing
    probability = model.predict_proba(email_features_array)
    if prediction[0] == 1:
        result = 'Phishing'
        probability_score = probability[0][1] * 100
        print(result, probability_score)
    else:
        result = 'Legitimate'
        probability_score = probability[0][0] * 100
        print(result, probability_score)


# Example usage
predict()
</code></pre>
<p>i checked the type of my ML Model and got this:</p>
<pre><code>&lt;class 'numpy.ndarray'&gt;
</code></pre>
<p>, idk what i could do to solve this i tried almost every way possible</p>
","-1","Question"
"78420289","","<p>I am trying to train a GAN model to detect Diabetic retinopathic images but it's throwing error. Kindly help.
The images dataset is not empty I have tried looking into it
Error is:-</p>
<pre><code>Epoch 1/50
Traceback (most recent call last):
  File &quot;C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py&quot;, line 65, in &lt;module&gt;
    classifier.fit(X, Y, batch_size=32, epochs=50)
  File &quot;C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py&quot;, line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py&quot;, line 553, in categorical_crossentropy
    raise ValueError(
ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 3), output.shape=(None, 5)
</code></pre>
<p>The code for the train model file is:</p>
<pre><code>import numpy as np
import imutils
import sys
import cv2
import os
from tensorflow.keras.utils import to_categorical
from keras.models import model_from_json
from keras.layers import MaxPooling2D
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D
from keras.models import Sequential 

images = []
image_labels  = []
directory = 'dataset'
list_of_files = os.listdir(directory)
index = 0
for file in list_of_files:
    subfiles = os.listdir(directory+'/'+file)
    for sub in subfiles:
        path = directory+'/'+file+'/'+sub
        img = cv2.imread(path)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        if img is None:
          print('Wrong path:', path)
        else:
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         images.append(im2arr)
         image_labels.append(file)
    print(file)    

X = np.asarray(images)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow('ff',cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;shape == &quot;+str(X.shape))
print(&quot;shape == &quot;+str(Y.shape))
print(Y)
X = X.astype('float32')
X = X/255

np.save(&quot;model/img_data.txt&quot;,X)
np.save(&quot;model/img_label.txt&quot;,Y)

X = np.load('model/img_data.txt.npy')
Y = np.load('model/img_label.txt.npy')
print(Y)
img = X[20].reshape(32,32,3)
cv2.imshow('ff',cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet transfer learning code here
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), activation = 'relu'))
classifier.add(MaxPooling2D((2, 2) , padding='same'))
classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))
classifier.add(MaxPooling2D((2, 2) , padding='same'))
classifier.add(Flatten())
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 5, activation = 'softmax'))
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
classifier.fit(X, Y, batch_size=32, epochs=50)
</code></pre>
<p>I have tried changing dimensions but it isn't working I am not able to understand is it a version error or code error so to address the same kindly give the solutions.</p>
","0","Question"
"78420559","","<p>I am trying to make a classifier as parsimonious as possible. To do this, I am recursively dropping features using cross validation.</p>
<p>In the context of my model, precision is the most important metric. However, I would also like to see how other metrics evolve as less features are fed to the model.</p>
<p>In particular, I would like to evaluate the models' recall and F1 scores.</p>
<pre class=""lang-py prettyprint-override""><code>rfe = RFECV(
    estimator=clf,  # An XGBClassifier instance
    step=1,
    min_features_to_select=1,
    cv=cv,  # A StratifiedKFold instance
    scoring='precision',
    # scoring=['f1', 'precision', 'recall'],  # throws error
    verbose=1,
    n_jobs=1
)
</code></pre>
<p>I commented-out the line that passes a list of metrics to the <code>scoring</code> argument because it throws an <code>InvalidParameterError</code> (that is, it's not happy that I passed it a list).</p>
<p>Is there a way to pass multiple metrics to an RFECV instance?</p>
","2","Question"
"78420651","","<p>I have a JSON file that'll be used as data for a NER model.
It has a sentence and the relevant entities in that specific sentence.
I want to create a function that will generate a BIO-labeled string for each sentence according to the entities</p>
<p>for example the following object from the JSON file</p>
<pre class=""lang-js prettyprint-override""><code>{
      &quot;request&quot;: &quot;I want to fly to New York on the 13.3&quot;,
      &quot;entities&quot;: [
        {&quot;start&quot;: 16, &quot;end&quot;: 23, &quot;text&quot;: &quot;New York&quot;, &quot;category&quot;: &quot;DESTINATION&quot;},
        {&quot;start&quot;: 32, &quot;end&quot;: 35, &quot;text&quot;: &quot;13.3&quot;, &quot;category&quot;: &quot;DATE&quot;}
      ]
} 
</code></pre>
<p>&quot;I want to fly to New York on the 13.3&quot;
The corresponding BIO label will be
&quot;O O O O O B-DESTINATION I-DESTINATION O O B-DATE&quot;
where B-category is the beginning of that category
I-category stands for inside and O for outside.</p>
<p>I'm looking for a Python code to iterate on each object in the JSON file that will generate a BIO-label for it.</p>
<p>change the JSON format if necessary</p>
","0","Question"
"78423313","","<p>I am trying to build a model for diabetes prediction by usage of Diabetes Health Indicators Dataset (<a href=""https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset</a>). I use Azure Machine Learning Studio (classic) where I tried both &quot;Train Model&quot; and &quot;Tune Model Hyperparameters&quot; (Random sweep) approach, but I cannot reach accuracy and/or other metrics above cca 72%. Dataset is balanced (35346 instances for 0 and 35346 instances for 1) and most of its features are just 0 or 1. I removed some of them they are not important for diabetes prediction (CholCheck, AnyHealthCare, NoDocbcCost, Education, Income).</p>
<p><a href=""https://i.ibb.co/Wn5cSY9/Bez-naslova.png"" rel=""nofollow noreferrer"">https://i.ibb.co/Wn5cSY9/Bez-naslova.png</a></p>
<p>Is this dataset suitable for accurate prediction or I have to change approach to problem?</p>
","-1","Question"
"78423364","","<p>I have a categorical column in Polars with 3 categories <code>'A'</code>, <code>'B'</code> and <code>'C'</code>, inferred from a training dataset. The column also contains some nulls.</p>
<p>Now suppose there exists a previously-unseen category <code>'X'</code> in my test set. I want to handle it like this:</p>
<ol>
<li>Add an <code>'Unknown'</code> category to the training column</li>
<li>For any unseen categories in test set (e.g. <code>'X'</code>), set them to <code>'Unknown'</code>.</li>
<li>Keep all nulls in train and test sets as-is (or as a dedicated category), since they may have special meaning.</li>
</ol>
<p>How to accomplish this?</p>
","3","Question"
"78425666","","<p>How <code>N_u</code> units of <code>LSTM</code> works on a data of <code>N_x</code> length? I know that there are many similar questions asked before but the answers are full of contradictions and confusions. Therefore I am trying to clear my doubts by asking specific questions. I am following the simple blog here:
<a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>Q0) Is <code>keras</code> implementation consistent with the above blog?
Please consider the following code.</p>
<pre><code>import tensorflow as tf
N_u,N_x=1,1
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(N_u, stateful=True, batch_input_shape=(32, 1, N_x))
])
model.summary()
</code></pre>
<p>For simplicity, my input data here is just a scalar and I have one time step to keep things simple. The output shape is <code>(32,1)</code>. No. of parameter is 12.<br />
Q1) I have one <code>LSTM</code> unit or cell, right? The following represent a cell, right?
<a href=""https://i.sstatic.net/KbmAUHGy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KbmAUHGy.png"" alt=""enter image description here"" /></a>
I understand from the picture that there would be 12 parameters : forget gate=2 weights+1 bias; input_gate=2*(2 weights+1 bias); output gate=(2 weights+1 bias). So everything is fine up to this point.</p>
<p>Q2) Now let us set <code>N_u,N_x=1,2</code>. I expect the same cell will be applied to the two elements of <code>x</code>. But I found that the total number of parameters now is 16! Why? Is it because I get 4 additional weight parameters corresponding to the <code>LSTM</code> connection between the <code>x_2</code> and the <code>LSTM</code> unit?</p>
<p>Q3) Now let us set <code>N_u,N_x=2,1</code>. I have now two units of <code>LSTM</code>. My understanding was the two cells will operate parallelly on the same data (a scalar number in this case). Are these two units completely independent or do they influence each other? I expected the parameters number would be 2*12=24, but I in reality got 32 instead. Why 32?</p>
<p>Q4) If I set <code>N_u,N_x=2,2</code>, number of parameter is 40. I think I can get it if I understand the above two points.</p>
<p>Q5) Finally, is there a documentation/paper which the keras implementation is based on?</p>
","0","Question"
"78427741","","<p>I'm new to Python and machine learning and I have the following problem: I have annotated data in the COCO <code>.json</code> format. In this case, it is the surface area of corals on underwater photos that are alive and parts of corals that are dead. Sometimes, the masks overlap.</p>
<p>I want to subtract the area I annotated as &quot;dead&quot; from the areas I annotated as &quot;alive&quot;. On each photo, only one coral is annotated, sometimes I did this in multiple parts, therefore I also want to merge the masks of each class before the subtraction. The annotation class &quot;dead&quot; only occurs in a subset of the images.</p>
<p>Could somebody point me in the right direction on how to do this?</p>
<p>I added an example photo: In yellow is the class &quot;alive&quot;, in green the class &quot;dead&quot;.</p>
<p><a href=""https://i.sstatic.net/jtE680lF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jtE680lF.png"" alt=""enter image description here"" /></a></p>
<p>Thanks a lot!</p>
","0","Question"
"78429387","","<p>When I tried to save the model as h5</p>
<pre class=""lang-py prettyprint-override""><code>caption_model.save(&quot;/kaggle/working/mymodel.h5&quot;)
</code></pre>
<p>This erorr appeared to me</p>
<pre><code>ValueError                                Traceback (most recent call last)
Cell In[19], line 1
----&gt; 1 caption_model.save(&quot;/kaggle/working/mymodel.h5&quot;)

File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183, in Group.create_dataset(self, name, shape, dtype, data, **kwds)
    180         parent_path, name = name.rsplit(b'/', 1)
    181         group = self.require_group(parent_path)
--&gt; 183 dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)
    184 dset = dataset.Dataset(dsid)
    185 return dset

File /opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163, in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)
    160 else:
    161     sid = h5s.create_simple(shape, maxshape)
--&gt; 163 dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
    165 if (data is not None) and (not isinstance(data, Empty)):
    166     dset_id.write(h5s.ALL, h5s.ALL, data)

File h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper()

File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()

File h5py/h5d.pyx:137, in h5py.h5d.create()

ValueError: Unable to synchronously create dataset (name already exists)
</code></pre>
<p>Has any of you encountered this problem before or knows how to solve it?</p>
<p>Thank you</p>
","0","Question"
"78432197","","<p>I used RAG with LLAMA3 for AI bot. I find RAG with chromadb is much slower than call LLM itself.
Following the test result, with just one simple web page about 1000 words, it takes more than 2 seconds for retrieving:</p>
<pre><code>Time used for retrieving: 2.245511054992676
Time used for LLM: 2.1182022094726562
</code></pre>
<p>Here is my simple code:</p>
<pre><code>embeddings = OllamaEmbeddings(model=&quot;llama3&quot;)
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
retriever = vectorstore.as_retriever()
question = &quot;What is COCONut?&quot;
start = time.time()
retrieved_docs = retriever.invoke(question)
formatted_context = combine_docs(retrieved_docs)
end = time.time()
print(f&quot;Time used for retrieving: {end - start}&quot;)

start = time.time()
answer = ollama_llm(question, formatted_context)
end = time.time()
print(f&quot;Time used for LLM: {end - start}&quot;)
</code></pre>
<p>I found when my chromaDB size just about 1.4M, it takes more than 20 seconds for retrieving and still only takes about 3 or 4 seconds for LLM. Is there anything I missing? or RAG tech itself is so slow?</p>
","-1","Question"
"78433029","","<p>I built a custom Estimator using sklearn BaseEstimator and ClassifierMixin. But when it comes to cross validation, GridSearchCV gives me nan values on the score.
Here is the code of the estimator :</p>
<pre><code>class RegressionClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, regressor=RidgeCV(cv=10, fit_intercept=True), alpha=0, n_components=1):
        self.alpha = alpha
        self.n_components = n_components
        self.regressor = regressor
        self.estimator = None

    def fit(self, X, y):
        pipe = Pipeline(steps=[
            ('imputate', SimpleImputer(missing_values=np.nan, strategy=&quot;median&quot;)),
            ('scale', StandardScaler()),
            ('reduce', PCA(self.n_components)),
            ('regress', self.regressor)
        ])
        self.estimator = pipe.fit(X, y)
        return self
        
    def predict(self, X):
        predictions = self.estimator.predict(X)
        converter = [
        predictions &lt; -self.alpha,
        (-self.alpha &lt;= predictions) &amp; (predictions &lt; self.alpha),
        predictions &gt;= self.alpha
         ]
        classes = [2, 0, 0]
        predicted_class = np.select(converter, classes)
        return predicted_class

    def score(self, X, y):
        y_true = train_new_y[y.index]
        return accuracy_score(y_true, y_true)    

</code></pre>
<p>This is supposed to return 1 as score since I compute the accuracy of to identical predictions.</p>
<p>The Estimator works as follow :</p>
<ol>
<li><p>Pipeline (which does a linear regression) -&gt; output (regression results vector)</p>
</li>
<li><p>Converter (which take the pipeline results) -&gt; output (classes by some magical operations on regression results)</p>
</li>
<li><p>Score should take generated classes and output accuracy between those classes and a target vector</p>
</li>
</ol>
<p>Results of the gridsearch:</p>
<pre><code>Fitting 5 folds for each of 100 candidates, totalling 500 fits
[CV 1/5] END ........alpha=0.0, n_components=50.0;, score=nan total time=   0.2s
[CV 2/5] END ........alpha=0.0, n_components=50.0;, score=nan total time=   0.1s
[CV 3/5] END ........alpha=0.0, n_components=50.0;, score=nan total time=   0.2s
</code></pre>
","0","Question"
"78433332","","<p>I have a python list of torch_geometric.data.Data objects (each one representing a graph). There is no easy way for me to access original raw files for this data: I just have the list. I need to turn this list of Data objects into a torch_geometric.data.InMemoryDataset or torch_geometric.data.Dataset object in order to integrate it with a larger code base which I did not write. How do I do this?</p>
<p>To be clear, I know that one can use a list of Data objects to make a torch_geometric.data.DataLoader object. But, I specifically need a Dataset object, <em>not a DataLoader object</em>, as the larger code base does some additional processing steps on Dataset objects before turning them into loaders.</p>
<p>I don't understand why PyG makes this so difficult. Should there not be a very easy way to do this?</p>
<p>I tried using a simple CustomDataset class</p>
<pre><code>class CustomDataset(InMemoryDataset):
    def __init__(self, data):
        super().__init__()
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        return sample
</code></pre>
<p>and it gave me a KeyIndex error when trying to get the Data object at index 0. I also tried a version of the above code where the super class is Dataset as opposed to InMemoryDataset but I couldn't figure out how to make the collate method work.</p>
","0","Question"
"78434421","","<p>Why doesn't one-hot encoding use bit-based encoding? Wouldn't it take much less memory? What I mean is when you encode for example four cities you can do it like what one-hot encoder does by expanding one column to 4 or like this in one column:</p>
<pre><code>    1st city = 0(base10) = 00000000
    2nd city = 1(base10) = 00000001
    3rd city = 2(base10) = 00000010
    4th city = 3(base10) = 00000011    
</code></pre>
<p>Isn't this just more efficient in terms of memory cost? Or does the encoding technique force it to take more space?</p>
","1","Question"
"78434627","","<p>I have a python notebook that contains several functions such as pre_process,.. and it contains my model which I trained. Now I'm supposed to submit a python notebook that contains my trained model for further testing. How can I do that?</p>
<p>I know that when running a cell it saves the variables for all cells to use, can I maybe link the two files and call the model from there?</p>
","0","Question"
"78435504","","<p>I need help in defining a torch model for my data. I have tried various methods but nothing seems to be working out. Error after error related to input size and shaping. How can I resolve these issues?</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as f

# Load data from .npy file
data = np.load(&quot;other py files/project_files/data/train/data.npy&quot;)
print(&quot;Data Shape: &quot;, data.shape)  # (401, 701, 255)

data_size = data.shape[0] * data.shape[1] * data.shape[2]
print(&quot;Data Size:&quot;, data_size)  # 71680755

# Load labeling data from .npy file
labels = np.load(&quot;other py files/project_files/data/train/label.npy&quot;)
print(&quot;Label Data Shape: &quot;, labels.shape)  # (401, 701, 255)

# Convert numpy arrays to PyTorch tensors
data_tensor = torch.Tensor(data)
labels_tensor = torch.Tensor(labels)


class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def forward(self, x):
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        return x

model = MyModel()
print(model)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

dataset = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

num_epochs = 10
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(dataloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs.unsqueeze(1))  # channel dimension
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:
            print(f&quot;[{epoch + 1}, {i + 1}] loss: {running_loss / 100}&quot;)
            running_loss = 0.0


with torch.no_grad():
    predictions = model(data_tensor.unsqueeze(1))  # channel dimension
</code></pre>
<p>Console Output:</p>
<pre><code>Connected to pydev debugger (build 223.8836.43)
Data Shape:  (401, 701, 255)
Data Size: 71680755
Label Data Shape:  (401, 701, 255)
MyModel(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fc): Linear(in_features=71680755, out_features=2, bias=True)
)

File &quot;C:\Users\PC1\PycharmProjects\Project1\newmodel2.py&quot;, line 36, in forward
    x = x.view(-1, self.fc_input_size)
RuntimeError: shape '[-1, 71680755]' is invalid for input of size 22579200
python-BaseException
</code></pre>
","0","Question"
"78436672","","<p>I have 2 columns, one is independent which signifies PART_NO and other is dependent which is QUANTITY.
I am trying to predict the quantity of a particular machinery on the basis of which machinery it is. Now the problem is my PART_NO has alphanumeric characters like 01232COM002-222, ABCD/235, GS.612.90-19, 123456.</p>
<p>When I am fitting the model using RandomForestRegressor, I am getting</p>
<blockquote>
<p>error: could not convert string to float,</p>
</blockquote>
<p>I cannot replace any slash or comma with anything as PART_NO is unique.</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import LinearSVR
from sklearn.linear_model import LinearRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder

file = 'testData.xlsx'
df = pd.read_excel(file)
df.head()

print(f&quot;Number of rows = {df.shape[0]}&quot;)
print(f&quot;Number of columns = {df.shape[1]}&quot;)

print(&quot;Data types of columns: \n&quot;, df.dtypes)

print(&quot;X-Features and y-Labels&quot;)
X = df[['PART_NO']]
y = df['QUANTITY']
print(&quot;X-Features\n&quot;)
print(X.head())
print(&quot;&quot;)
print(&quot;y-Labels&quot;)
print(y.head())


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
print(&quot;Training size = &quot;, X_train.shape)
print(&quot;Testing size = &quot;, X_test.shape)
model = RandomForestRegressor()
model.fit(X_train, y_train)
</code></pre>
<p>I tried LabelEncoding, OneHotEncoding, did not work, I tried string replace, did not work.</p>
","-1","Question"
"78437477","","<p>There are various machine learning models (Claude, chatGPT, etc) which can be used to extract machine-readable information from images. Has anyone seen cases of successfully extracting Newick format data (or equivalent) from published images of phylogenetic trees (of which there are many on the internet)?</p>
","1","Question"
"78439640","","<p>could someone help me out with my Pytorch installation? My device currently uses Windows OS and an AMD GPU. However, the Pytorch installation does not support Windows OS with ROCm combination. Only when Linux OS is chosen will the ROCm option be available.</p>
<p>Can I use CUDA toolkit in replacement of ROCm? Or do I somehow change my OS to Linux? Is there some way to by pass all of these and still be able to use Pytorch?</p>
<p>Any advice will be greatly appreciated!</p>
<p>I have tried looking for installation tutorials on youtube but they do not have the same OS and GPU combination as I do. (That is Windows OS and AMD GPU)</p>
","1","Question"
"78439701","","<h2>How to programmatically set alerts for failure in a W&amp;B run?</h2>
<p>I'm trying to set up alerts in my W&amp;B (Weights &amp; Biases) project to notify me if a run fails. I've been testing several functions I thought would work based on my research, but none seem to be implemented in the W&amp;B API. Here's the code snippet where I attempt to set up these notifications:</p>
<pre class=""lang-py prettyprint-override""><code>import wandb

mode = 'dryrun'
run_name = 'my_run'
num_batches = 50
path = '/data'
name = 'experiment1'
today = '2023-08-01'
probabilities = [0.1, 0.9]
batch_size = 32
data_mixture_name = 'mix1'

debug = mode == 'dryrun'
run = wandb.init(mode=mode, project=&quot;beyond-scale&quot;, name=run_name, save_code=True)
wandb.config.update({
    &quot;num_batches&quot;: num_batches, 
    &quot;path&quot;: path, 
    &quot;name&quot;: name, 
    &quot;today&quot;: today, 
    'probabilities': probabilities, 
    'batch_size': batch_size, 
    'debug': debug, 
    'data_mixture_name': data_mixture_name
})

# Attempts to set notifications
run.notify_on_failure()
run.notify_on_crash()
run.notify_on_exit()
run.notify_on_heartbeat()
run.notify_on_abort()
</code></pre>
<p>Each of these attempts results in an <code>AttributeError</code>, stating that the 'Run' object has no such attribute. For example:</p>
<pre><code>AttributeError: 'Run' object has no attribute 'notify_on_failure'
</code></pre>
<p>Is there a correct method to set up failure or other alerts in W&amp;B? If so, how should I modify my approach?</p>
<p>ref: <a href=""https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891</a></p>
","0","Question"
"78439767","","<p>I'm trying to convert existing <a href=""https://github.com/LiheYoung/Depth-Anything"" rel=""nofollow noreferrer"">depth-anything</a> PyTorch model to CoreML format. I decided to use Google Colab and took the <a href=""https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/depth-anything/depth-anything.ipynb"" rel=""nofollow noreferrer"">following </a>note for inferencing depth-anything model. However, I meet some exception while trying to import it on iOS side. Here is my code snippet for converting:</p>
<pre class=""lang-py prettyprint-override""><code># Installing all needed extensions
!pip install coremltools
# ...

import coremltools as ct
import torch

# Convert the PyTorch model to TorchScript
traced_model = torch.jit.trace(depth_anything, torch.rand(1, 3, 518, 518))

# Convert the TorchScript model to CoreML
model_coreml = ct.convert(
    traced_model,
    inputs=[ct.ImageType(name=&quot;input_1&quot;, shape=(1, 3, 518, 518), scale=1/255.0)]
)

output = model_coreml._spec.description.output[0]
output.type.imageType.colorSpace = ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('RGB')
output.type.imageType.width = 518
output.type.imageType.height = 518

# Save the modified CoreML model
print(model_coreml)
model_coreml.save('/content/drive/MyDrive/trained_models/depth9.mlpackage')
</code></pre>
<p>I've tried to specify input parameters straight as I do for the output one like this:</p>
<pre class=""lang-py prettyprint-override""><code># Create a dictionary for the input schema
input_schema = {'input_name': 'input', 'input_type': ct.TensorType(shape=(1, 3, 518, 518))}

# Add the input schema to the model's metadata
model_coreml.user_defined_metadata['inputSchema'] = str(input_schema)
</code></pre>
<p>Or to use <code>convert_to</code> option with setting up <code>neuralnetwork</code> like this:</p>
<pre class=""lang-py prettyprint-override""><code>model_coreml = ct.convert(
    traced_model,
    inputs=[ct.ImageType(name=&quot;input_1&quot;, shape=(1, 3, 518, 518), scale=1/255.0)],
    convert_to='neuralnetwork'
)
</code></pre>
<p>Or to set <code>ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('RGB')</code> with <code>BGR</code>/<code>GRAYSCALE</code><br />
Nothing helps.</p>
<p>If I try to import the model with <code>neuralnetwork</code> backend I just receive an infinite loading. If I try to import the model with <code>mlprogram</code> backend (default, if not specified) I receive the following:<br />
<a href=""https://i.sstatic.net/zOriEIc5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zOriEIc5.png"" alt=""Exception while importing mlprogram backend"" /></a></p>
<p>I look forward for any advices and help since all I need is just to convert existing <code>depth-anything</code> model with no adjustments or changes to CoreML format. Thanks!</p>
","0","Question"
"78440449","","<pre><code>from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# Define features and target
X = df.drop('infected', axis=1)
y = df['infected']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the resampling strategy
over = SMOTE(sampling_strategy=0.5)  # Oversample the minority class to 50% of the majority class
under = RandomUnderSampler(sampling_strategy=0.8)  # Undersample the majority class to 80% of its original size

pipeline = Pipeline(steps=[('o', over), ('u', under)])

# Apply the resampling
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# Show the new class distribution
print(&quot;Resampled class distribution:&quot;, pd.Series(y_resampled).value_counts())
</code></pre>
<p>This is my code</p>
<p>And this is the error I am getting</p>
<pre><code>AttributeError                            Traceback (most recent call last)
Cell In[7], line 19
     16 pipeline = Pipeline(steps=[('o', over), ('u', under)])
     18 # Apply the resampling
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
     21 # Show the new class distribution
     22 print(&quot;Resampled class distribution:&quot;, pd.Series(y_resampled).value_counts())

File ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372, in Pipeline.fit_resample(self, X, y, **fit_params)
    342 &quot;&quot;&quot;Fit the model and sample with the final estimator.
    343 
    344 Fits all the transformers/samplers one after the other and
   (...)
    369     Transformed target.
    370 &quot;&quot;&quot;
    371 self._validate_params()
--&gt; 372 fit_params_steps = self._check_fit_params(**fit_params)
    373 Xt, yt = self._fit(X, y, **fit_params_steps)
    374 last_step = self._final_estimator

AttributeError: 'Pipeline' object has no attribute '_check_fit_params'
</code></pre>
<p>I have tried everything. All my packages are updated. And all the methods I have tried to use looking at sklearn and imblearn both the websites.</p>
","1","Question"
"78441794","","<p><strong>Description:</strong></p>
<p>I'm encountering an issue with a LightGBM regressor and classifier when using PySpark in Python 3.10.0.</p>
<p><strong>Environment:</strong></p>
<p>PySpark version: 3.2.1
Python version: 3.10.0
Py4j version: 0.10.9.5
Spark jars packages: com.microsoft.azure:synapseml_2.12:0.11.0</p>
<p><strong>Error Message:</strong></p>
<p>java.lang.UnsatisfiedLinkError: Can't load library: /var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib</p>
<p><strong>Steps to Reproduce:</strong></p>
<ul>
<li>Use LightGBM regressor or classifier with PySpark DataFrame in Python
3.10.0.</li>
<li>Encounter the error mentioned above.</li>
</ul>
<p><strong>Attempts Made:</strong>
I followed the solution provided <a href=""https://stackoverflow.com/a/70371953/20911151"">here</a> and symlinked the installed libomp, but the issue persists.</p>
<p><strong>Detailed Error Stack:</strong></p>
<p>py4j.protocol.Py4JJavaError: An error occurred while calling o5147.fit.
E                   : java.lang.UnsatisfiedLinkError: Can't load library: /var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
E                       at java.base/java.lang.ClassLoader.loadLibrary(Unknown Source)
E                       at java.base/java.lang.Runtime.load0(Unknown Source)
E                       at java.base/java.lang.System.load(Unknown Source)
E                       at com.microsoft.azure.synapse.ml.core.env.NativeLoader.loadLibraryByName(NativeLoader.java:66)
E                       at com.microsoft.azure.synapse.ml.lightgbm.LightGBMUtils$.initializeNativeLibrary(LightGBMUtils.scala:33)
E                       at com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train(LightGBMBase.scala:37)
E                       at com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train$(LightGBMBase.scala:36)
E                       at com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E                       at com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E                       at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
E                       at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
E                       at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E                       at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
E                       at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
E                       at java.base/java.lang.reflect.Method.invoke(Unknown Source)
E                       at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E                       at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E                       at py4j.Gateway.invoke(Gateway.java:282)
E                       at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E                       at py4j.commands.CallCommand.execute(CallCommand.java:79)
E                       at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E                       at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E                       at java.base/java.lang.Thread.run(Unknown Source)</p>
<p><strong>Additional Notes:</strong></p>
<ul>
<li>The issue seems to be related to loading the LightGBM library in Python 3.10.0</li>
<li>I've checked that lib_lightgbm.dylib exists in the specified path.</li>
<li>Any insights or suggestions on resolving this issue would be greatly appreciated.</li>
</ul>
","0","Question"
"78442079","","<p>I have a model(3DUnet, Regression problem) that predicts values PD and T1, where PD and T1 are the qMRI outputs based on the input. From these predictions, I calculate T1_Weighted_image using the formula: <strong><em>Weighted_images  = PD</em> (1 - exp(-1 / (T1 + epsilon)))</strong>*, where epsilon is a small value to prevent division by zero and T1=&gt;0 . During training, my ground truth for loss computation is T1_Weighted_groundtruth, but I also have ground truth values for PD and T1, although they are not directly used for loss computation. They serve to ensure the correctness of predicted values for PD and T1. The loss is computed using a loss function between T1_Weighted_predict and T1_Weighted_groundtruth.</p>
<p>However, there exist various combinations of PD or T1 that can yield similar results for T1_Weighted. For instance, instead of predicting high values for T1 (which is the correct answer), my model might predict very low values for PD (for example in CSF as an obvious example). Is there a method to compel my model to predict the correct values, or at least to predict (any) possible combinations?</p>
","0","Question"
"78442377","","<p>I have a python application which uses subprocesses to run a bash script. The bash script in turn runs a python file which has some imports (retina face, deep face libraries). The application takes a lot of time to run because each time the subprocess is run, it takes 20-30 secs to load/import retina/deep face modules. Is there a way by which this can be sped up?</p>
<p>Note: Changing the set-up is not possible i.e. I can't call the python code directly from the original python code.</p>
<p>I am unsure on how to go about solving this, any help is appreciated. Thank you.</p>
<p>I tried using cached version of sys modules but that did not work. Although, I am unsure on how to correctly utilize it.</p>
","0","Question"
"78443975","","<p>I am new with tensorflow. I am trying to read values from a CSV file and load it as tensorflow dataset. However, when I try to run model.fit, it gives following error-
Missing data for input &quot;input_39&quot;. You passed a data dictionary with keys ['Age', 'Number', 'Start']. Expected the following keys: ['input_39']</p>
<p>Here is my code-</p>
<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf

input_file='kyphosis.csv'

all_dataset = tf.data.experimental.make_csv_dataset(input_file, batch_size=1,label_name=&quot;Kyphosis&quot;,num_epochs=1)

model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(3))
model.add(tf.keras.layers.Dense(10))
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

model.compile(optimizer='adam',loss='binary_crossentropy',run_eagerly=True)

model.fit(all_dataset,epochs=10)
</code></pre>
<p>Please let me know what I am doing wrong here. Tensorflow version is 2.11.0.</p>
<p>I tried with tf.data.Dataset.from_tensor_slices but getting the same error-</p>
<pre><code>df=pd.read_csv('kyphosis.csv')
X=df.drop('Kyphosis',axis=1)
y=df['Kyphosis']

all_dataset=tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'),y))
all_dataset = all_dataset.batch(1)

model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(3))
model.add(tf.keras.layers.Dense(10))
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

model.compile(optimizer='adam',loss='binary_crossentropy')
model.fit(all_dataset,epochs=3)
</code></pre>
<p>Error-
ValueError: Missing data for input &quot;input_41&quot;. You passed a data dictionary with keys ['Age', 'Number', 'Start']. Expected the following keys: ['input_41']</p>
","0","Question"
"78444040","","<p>I have a random forest model that I'm trying to understand better.</p>
<p>For the sake of the example, lets say we have a grove of blueberry bushes. What we're interested in is predicting the production of rotten blueberries on a specific bush, among harvest of all blueberries of the individual bushes.</p>
<p>Each bush has an identifying name: <strong>bush_name</strong>, such as <strong>'bush001'</strong>, and we want predictions based on each individual bush. For example, I want to know if bush025 produced a rotten berry on 2/2/22.</p>
<p>Inputs are in a df with the following dummy structure for the sake of this example:</p>
<pre><code>train_data &lt;- data.frame(date = c(&quot;2022-01-01&quot;, &quot;2022-01-07&quot;, &quot;2022-02-09&quot;, &quot;2022-05-01&quot;, &quot;2022-11-01&quot;, &quot;2022-11-02&quot;),
                   bush_name = c(&quot;bush001&quot;, &quot;bush001&quot;, &quot;bush001&quot;, &quot;bush043&quot;, &quot;bush043&quot;, &quot;bush043&quot;),
                   bugs = c(2, 0, 1, 0, 3, 1),
                   has_rotten_berry = c(1, 0, 0, 1, 1, 0),
                   berry_count = c(12, 1, 7, 100, 14, 4),
                   weather = c(1, 0, 2, 0, 1, 1))
</code></pre>
<p>I've got a random forest model that I have set up with the following high level set up:</p>
<pre><code>library(agua)
library(parsnip)
library(h2o)

h2o.init(nthreads = -1)

model_fit &lt;- rand_forest(mtry = 10,  trees = 100) %&gt;%
  set_engine(&quot;h2o&quot;) %&gt;%
  set_mode(&quot;classification&quot;) %&gt;%
  fit(has_rotten_berry ~  .,
      data = train_data) %&gt;% 
  step_dummy(bush_name) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_normalize(all_predictors())
</code></pre>
<p>I do get this message after training:</p>
<pre><code>Warning message:
In .h2o.processResponseWarnings(res) :
  Dropping bad and constant columns: [bush_name].
</code></pre>
<p><strong>What I want to know is:</strong></p>
<p>When I try to then predict on new data in the trained model, it seems that I am only able to input new test data with the bush_names of bushes I already trained on. <strong>Am I correct in assuming this model is creating bush-specific predictions? And therefore would have to input new bush information in the training in order to output a future prediction for those new bushes?</strong></p>
<p>Example: I plant a new bush, bush700, and it was not present in the original training data set. If I try to predict with the new bush data without it being present in the training data, is giving me a message that there are new levels in the data. So <strong>I'm assuming that because it seems these predictions are bush-specific, and we can't get any new bush predictions for newly-added bushes.</strong></p>
<p><strong>Is this correct to assume?</strong></p>
","0","Question"
"78448470","","<p>I'm encountering an issue with my YOLO model.</p>
<p>Initially, I trained it with 7 classes. Now, I want to add 4 new classes to the model. However, when I combine the data for the original 7 classes with the new 4 classes, the training time and associated cloud costs significantly increase. What's a good solution to efficiently incorporate these additional classes into the model without inflating training time and costs?</p>
<p>My expecting is reduce the cost and training time in incremental learnng.</p>
","0","Question"
"78450279","","<p>I have data for train a binary classification model.</p>
<pre><code>set.seed(1)
n &lt;- 20
dat &lt;- cbind.data.frame(target=as.factor(sample(0:1,n,T)),
                        price=round(rnorm(n)+1000,2),
                        var1=sample(1:n),
                        var2=round(rnorm(n),2),
                        var3=round(rnorm(n),2)) 
</code></pre>
<p>.</p>
<pre><code>   target   price var1  var2  var3
1       1 1001.03    5 -0.95  1.16
2       0 1000.21   13 -0.02 -0.45
3       0  999.49   11  1.20  1.95
4       0 1000.95    7  0.71  0.86
5       1 1001.49   20 -0.44  1.07
6       0  999.45   10  0.78 -1.76
7       1  998.78   12 -1.64 -0.77
8       1  998.23    8  0.67  0.12
</code></pre>
<p>I would like to force my model to take into account the variable <code>price</code> that I specify in every decision it makes.</p>
<p>I'll explain with an example what I mean.
Let's say I'm using a random forest model made up of rules, in which case I would like each rule to have a <code>price</code> variable within it.</p>
<p>These are good rules because each rule has a variable <code>price</code></p>
<pre><code> price&lt;=1001.49 &amp; var1&gt;0.105                           
 var2&lt;=0.12 &amp; price&gt;1001.03 &amp; var3&gt;-0.025             
 price&lt;=998.23 &amp; price&gt;=997.23
</code></pre>
<p>These are bad rules, I wish there weren’t such rules in the model</p>
<pre><code>var3&lt;=0.57 &amp; var3&gt;0.105                            
var2&lt;=0.12 &amp; var2&gt;-1.005 &amp; var3&gt;-0.025               
var1&lt;=6.5
</code></pre>
<p>I understand that I cannot influence the model itself, but perhaps I can somehow change the variables in the dataset to ultimately force the model to force the use of the <code>price</code> variable in every decision.</p>
<p>As a test, you can use this code that extracts rules from a trained random forest model.</p>
<pre><code>library(inTrees)
library(randomForest)
rules &lt;- randomForest(target~., dat, ntree=20) |&gt; 
          RF2List() |&gt; 
          extractRules(dat) |&gt; 
          unique() |&gt; 
          getRuleMetric(dat[,-1], dat$target) |&gt; 
          pruneRule(dat[,-1], dat$target) |&gt; 
          buildLearner(dat[,-1], dat$target)
</code></pre>
<p>.</p>
<pre><code>presentRules(rules, colnames(dat[,-1]))

    len freq   err condition                                pred
[1,] &quot;2&quot; &quot;0.35&quot; &quot;0&quot; &quot;var1&lt;=15.5 &amp; var2&lt;=-0.05&quot;               &quot;1&quot; 
[2,] &quot;3&quot; &quot;0.2&quot;  &quot;0&quot; &quot;var1&lt;=13.5 &amp; var2&gt;-0.05 &amp; var3&lt;=-0.315&quot; &quot;0&quot; 
[3,] &quot;1&quot; &quot;0.1&quot;  &quot;0&quot; &quot;var2&gt;1.255&quot;                             &quot;1&quot; 
[4,] &quot;2&quot; &quot;0.1&quot;  &quot;0&quot; &quot;var1&gt;17 &amp; var2&gt;-1.55&quot;                   &quot;1&quot; 
[5,] &quot;1&quot; &quot;0.1&quot;  &quot;0&quot; &quot;var3&lt;=-0.16&quot;                            &quot;0&quot; 
[6,] &quot;2&quot; &quot;0.1&quot;  &quot;0&quot; &quot;var2&gt;-0.05 &amp; var3&gt;0.49&quot;                 &quot;0&quot; 
[7,] &quot;1&quot; &quot;0.05&quot; &quot;0&quot; &quot;Else&quot;                                   &quot;1&quot; 
</code></pre>
<p>As you can see at this stage, the model refuses to use variable <code>price</code> in its rules. But the specifics of my task mean that I don’t need rules that don’t use <code>price</code>.</p>
<p>To summarize, my question sounds like this:
How can I change my dataset to force the model to use <code>price</code> in each of its rules?</p>
","0","Question"
"78451297","","<p>I've tried using all the variables vs also selecting certain variables, however the MSE stays high. I'm wondering if its a mistake in my code instead. I'm also was trying to add some feature engineering but commented it out for now as it made my MSE worse.</p>
<pre><code> #kaggle : House Prices - Advanced Regression Techniques   
    import pandas as pd
    import numpy as np
    import sklearn.model_selection
    import matplotlib.pyplot as plt
    import sklearn as sk
    import sklearn.tree
    import sklearn.ensemble
    
    df = pd.read_csv('/Users/andrewhashoush/Downloads/house-prices-advanced-regression-techniques/train.csv')
    df_test = pd.read_csv('/Users/andrewhashoush/Downloads/house-prices-advanced-regression-techniques/test.csv')
    
    print(df.head())
    print(df.info())
    
    #selected_features = ['OverallQual', 'YearBuilt', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea',
    #                     'GarageCars', 'GarageArea', 'MSZoning', 'Neighborhood',
    #                     'KitchenQual', 'CentralAir', 'LotArea', 'MSSubClass', 'LotFrontage', 
    #                     'Street', 'LandContour', 'Utilities', 'OverallCond', 'RoofStyle',
    #                     'RoofMatl', 'BsmtQual','SaleCondition', 'SaleType', 'YrSold', 'MoSold', 
    #                     'PoolArea']
    
    selected_features = [
        'LotFrontage', 'OverallQual', 'OverallCond', 'MasVnrArea', 'HalfBath',
        'BedroomAbvGr', 'KitchenAbvGr', 'GarageCars', 'WoodDeckSF',
        'OpenPorchSF', 'MoSold', 'YrSold', 'MSZoning', 'Alley',
        'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood',
        'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st',
        'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',
        'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
        'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',
        'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',
        'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',
        'SaleType', 'SaleCondition'
    ]
    
    #feature engineering
    #df['Quality_Condition'] = df['OverallQual'] * df['OverallCond']
    #df['Age_at_Sale'] = df['YrSold'] - df['YearBuilt']
    #selected_features += ['Quality_Condition', 'Age_at_Sale']
    
    X = df[selected_features]
    print(X.head())
    
    for column in X.columns:
        missing_data = df[column].isnull().sum()
        print(f&quot;{column}: {missing_data}&quot;)
      
    categorical_vars = X.select_dtypes(include='object').columns.tolist()
    numerical_vars = X.select_dtypes(exclude='object').columns.tolist()
    
    print(&quot;Categorical Variables:&quot;, categorical_vars)
    print()
    print(&quot;Numerical Variables:&quot;, numerical_vars)
    
    
    #%%
    
    #filled the missing categorical values with mode
    for var in categorical_vars:
        mode_value = X[var].mode()[0] 
        # X[var] = X[var].fillna(mode_value) 
        # X.loc[:, var] = X.loc[:, var].fillna(mode_value)
        X.loc[:, var] = X[var].fillna(mode_value)
    
    #filled the missing numerical values with median
    for var in numerical_vars:
        median_value = X[var].median()
        X.loc[:, var] = X[var].fillna(median_value)
    
    #checking it
    for column in X.columns:
        missing_data = X[column].isnull().sum()
        print(f&quot;{column}: {missing_data}&quot;)
    
    #one hot encoding 
    X = pd.get_dummies(X, columns=categorical_vars)
    y = df['SalePrice']
    
    #split the data
    X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X,y, train_size =.8, random_state= 123)
    
    dt_model = sklearn.tree.DecisionTreeRegressor(max_depth=5, random_state=123)
    dt_model.fit(X_train, y_train)
    y_pred = dt_model.predict(X_val)  
    mse_val = np.mean((y_pred - y_val)**2)
    print(f&quot;MSE: {mse_val}&quot;)
    
    # Random Forest Regressor
    rf_model = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=123)
    rf_model.fit(X_train, y_train)
    y_pred1 = rf_model.predict(X_val)
    mse_val1 = np.mean((y_pred1 - y_val)**2)
    print(f&quot;MSE: {mse_val1}&quot;)
    
    # Gradient Boosting Regressor
    gb_model = sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=123)
    gb_model.fit(X_train, y_train)
    y_pred2 = gb_model.predict(X_val)
    mse_val2 = np.mean((y_pred2 - y_val)**2)
    print(f&quot;MSE: {mse_val2}&quot;)
</code></pre>
<pre><code>MSE: 1451852149.6361678
MSE: 944388532.5714014
MSE: 755815420.2686024
</code></pre>
","-3","Question"
"78452655","","<p>KL divergence loss too high</p>
<p>I'm trying to perform knowledge distillation . For my student loss I have used cross entropy loss and for my knowledge distillation loss I am trying to use KL divergence loss.</p>
<p>Here is the code that I used for my KL divergence loss.</p>
<pre><code>class KLDivLoss(nn.Module):
    def __init__(self,ignore_index=-1, reduction=&quot;batchmean&quot;, log_target=False):
        super(KLDivLoss, self).__init__()
        self.reduction = reduction
        self.log_target = log_target
        self.ignore_index = ignore_index

    def forward(self, preds_S, preds_T, T =1.0, alpha = 1.0):
        preds_T[0] = preds_T[0].detach()  # Detach teacher predictions
        pred_1 = torch.sigmoid(preds_T[0]/T) # white
        pred_0 = 1 - pred_1
        preds_teacher = torch.cat((pred_0, pred_1), dim=1)
        assert preds_S[0].shape == preds_teacher.shape, &quot;Input and target shapes must match for KLDivLoss&quot;
        stu_prob = F.log_softmax(preds_S[0]/T, dim=1)
        kd_loss = F.kl_div(stu_prob, 
                           preds_teacher, 
                           reduction='batchmean',
                           ) * T * T
        return {'loss': kd_loss}
</code></pre>
<p>The values that I am getting from this are extremely huge. I am simply adding my knowledge distillation loss and cross entropy loss from student model. Since my CE loss is very small this is all from the KLdiv loss. Could you tell me how to reduce the loss? Or if I am doing something wrong.</p>
<p><a href=""https://i.sstatic.net/6tKJOOBM.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried using the KL div loss with temperature =1
my teacher model gave the output in the form of tensor [8,1,224,224] as it was used for binary prediction of pixel while my student model gave output in the form [8,2,224,224] where 0 belongs to class black and 1 to white.</p>
<p>so in order to match them up for KL div loss I used sigmoid function to get probabilities for class white and 1 - white probability for black. and then concatenated them to form a tensor of size [8,2,224,224] which would be similar to the student tensor.</p>
<p>and then I tried performing the KL divergence. the losses I got were extremely high</p>
","1","Question"
"78454026","","<p><code>xgboost</code> has a <a href=""https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster"" rel=""nofollow noreferrer"">parameter</a> <code>feature_weights</code> that should influence the probability of selecting a feature for the model, that is, we can give more or less weight to each feature, but it seems that the parameter does not work or am I doing something wrong?</p>
<pre><code>X &lt;- as.matrix(iris[,-5])
Y &lt;- ifelse(iris$Species==&quot;setosa&quot;, 1, 0)

library(xgboost)
dm1 &lt;- xgb.DMatrix(X, label = Y)
#I set different probabilities for each feature
dm2 &lt;- xgb.DMatrix(X, label = Y, feature_weights = c(1, 0, 0, 0.01))
params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;logloss&quot;)

set.seed(1)



xgb1 &lt;- xgboost(data = dm1, params = params, nrounds = 10, print_every_n = 5)

[1] train-logloss:0.448305 
[6] train-logloss:0.090220 
[10]train-logloss:0.033148 



xgb2 &lt;- xgboost(data = dm2, params = params, nrounds = 10, print_every_n = 5)

[1] train-logloss:0.448305 
[6] train-logloss:0.090220 
[10]train-logloss:0.033148 
</code></pre>
<p>But the models behave absolutely the same, it seems that the parameter <code>feature_weights</code> is simply ignored</p>
","1","Question"
"78456621","","<p>I am analyzing data on the <a href=""https://data.cms.gov/resources/medicaid-spending-by-drug-data-dictionary"" rel=""nofollow noreferrer"">Medicaid Spending by Drug Data Dictionary </a> dataset. Specifically, I want to perform a logistic regression, where y should be the CAGR_Avg_Spnd_Per_Dsg_Unt_18_22.</p>
<p>Unfortunately, the class and mode remain as characters based on my code.</p>
<p>My inspiration for the &quot;Up&quot; and &quot;Down&quot; approach comes from the following:</p>
<pre class=""lang-r prettyprint-override""><code># The library comes from Introduction to Statistical Learning: With Applications in R

library(ISLR)
attach(Smarket)
summary(Smarket)
# desired output:
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
            family=binomial,data=Smarket)
contrasts(Direction)
</code></pre>
<p>By using <code>glm.fit,</code> I can perform predictions, create a confusion matrix, and more.</p>
<p>However, when examining</p>
<pre class=""lang-r prettyprint-override""><code>summary(drug.spending)
</code></pre>
<p>My &quot;Up&quot; and &quot;Down&quot; are characters, while the authors to ISLR's &quot;Up&quot; and &quot;Down&quot; appear to be numerically counted. The authors never offered a code for doing that with their data frame's &quot;Up&quot; and &quot;Down&quot; observations!</p>
<p>Here is my code:</p>
<pre class=""lang-r prettyprint-override""><code>library(dplyr)
library(tidyr)
library(psych)
library(leaps)
set.seed(1)

spending &lt;- read.csv(&quot;medicaid_spending_by_drug_data_dictionary.csv&quot;)
drug.spending &lt;-  spending %&gt;%
  na.omit(spending) %&gt;%
  filter(Mftr_Name == &quot;Overall&quot;) %&gt;%
  arrange(desc(Tot_Mftr)) %&gt;%
  filter(duplicated(Gnrc_Name))
drug.spending &lt;- drug.spending[!duplicated(drug.spending$Gnrc_Name),]
attach(drug.spending)



drug.spending &lt;- drug.spending %&gt;%
  mutate(CAGR_Direction = ifelse(CAGR_Avg_Spnd_Per_Dsg_Unt_18_22 &gt; 0, 'Up', 'Down'))
drug.spending$CAGR_Direction &lt;- factor(drug.spending$CAGR_Direction, levels = c('Down', 'Up')) # Update #1

summary(drug.spending)
contrasts(CAGR_Direction) #gives an error
</code></pre>
<p>I have used different coercions, such as <code>as.numeric()</code> and <code>as.integer()</code>. I am not exactly sure where I am going wrong...</p>
","0","Question"
"78457370","","<p>I'm studying IA using PyTorch and implementing some toy examples.
First, I created a one-dimensional tensor (X) and a second tensor (y), derived from the first one:</p>
<pre class=""lang-py prettyprint-override""><code>X = torch.arange(0, 100, 1.0).unsqueeze(dim=1)
y = X * 2
</code></pre>
<p>So I have something like</p>
<pre><code>X = tensor([[0.], [1.], [2.], [3.], [4.], [5.], ...
y = tensor([[ 0.], [ 2.], [ 4.], [ 6.], [ 8.], [10.], ...
</code></pre>
<p>Then, I trained a model to predict y and it was working fine.</p>
<p>Now, I would like something different. The X will be 2D and y 1D. y is calculated by an operation in the elements of X:
<code>If x[0] + x[1] &gt;0? y = 10: y -10 </code></p>
<pre><code>X = tensor([[ 55.5348, -97.7608],
            [ 29.0493, -52.1908],
            [ 47.1722, -43.1151],
            [ 11.1242, -62.8652],
            [ 44.8067,  80.8335],...
y = tensor([[-10.], [-10.], [ 10.], [-10.], [ 10.],...
</code></pre>
<p>First question, Is it make sense in terms of Machine Learning?</p>
<p>Second one...
I'm generating the tensors using numpy. Could I do it in a smarter way?</p>
<pre class=""lang-py prettyprint-override""><code># Criar X valores de entrada para testes
X_numpy = np.random.uniform(low=-100, high=100, size=(1000,2))
print(&quot;X&quot;, X_numpy)

#y_numpy = np.array([[ (n[0]+n[1]) &gt;= 0 ? 10:-10] for n in X_numpy])
y_numpy = np.empty(shape=[0, 1])
for n in X_numpy:
    if n[0] + n[1] &gt;= 0:
        y_numpy = np.append(y_numpy, [[10.]], axis=0)
    elif n[0] + n[1] &lt; 0:
        y_numpy = np.append(y_numpy, [[-10.]], axis=0)
</code></pre>
","-2","Question"
"78460776","","<pre><code>import numpy as np
from numpy.linalg import inv
from scipy.linalg import pinv

# Define necessary functions
def create_laplacian_from_adjacency(adj_matrix):
    degree_matrix = np.diag(adj_matrix.sum(axis=1))

    laplacian_matrix = degree_matrix - adj_matrix
    
    return laplacian_matrix

def dirichlet_energy(L, X):
    &quot;&quot;&quot;Dirichlet energy for smoothness quantification.&quot;&quot;&quot;
    return np.trace(X.T @ L @ X)

def update_C(X, X_tilde, L, C, gamma, alpha, lam, J):
    &quot;&quot;&quot;Updating C using gradient descent with a majorized function approximation.&quot;&quot;&quot;
    p, k = C.shape
    C_old = np.copy(C)
    
    gradient_f = (-2 * gamma * L @ C_old @ inv(C_old.T @ L @ C_old + J) +
                  alpha * (C_old @ X_tilde - X) @ X_tilde.T +
                  2 * L @ C_old @ X_tilde @ X_tilde.T + lam * C_old @ np.ones((k, k)))
    
    # Majorized function optimization step (simplified approach)
    t = 0.01  # Learning rate, needs tuning based on actual application
    C_new = pinv(C_old - t * gradient_f)
    C_new = np.maximum(C_new, 0)  # Enforce non-negativity
    return C_new

def update_X(X, L, C, alpha):
    &quot;&quot;&quot;Update X(tilda) based on the updated C.&quot;&quot;&quot;
    inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C)
    X_tilde_new = inv_matrix @ C.T @ X
    return X_tilde_new

def fgc_algorithm(X, L, alpha, gamma, lam, iterations=5):
    &quot;&quot;&quot;Perform the Featured Graph Coarsening algorithm.&quot;&quot;&quot;
    p, n = X.shape
    k = 3  # Assume a reduced dimension of 3 for coarsening
    C = np.random.rand(p, k)*0.1
    J = np.full((k, k), 1/k)
    X_tilde = pinv(C)@X

    for i in range(iterations):
        C = update_C(X, X_tilde, L, C, gamma, alpha, lam, J)
        X_tilde = update_X(X, L, C, alpha)
        current_energy = dirichlet_energy(L, X_tilde)
        print(f&quot;Iteration {i}: Dirichlet Energy = {current_energy}&quot;)

    L_c = C.T @ L @ C
    return C, L_c, X_tilde

# Example:
X = np.random.rand(5, 7)  # Random features for 10 nodes
adj_matrix = np.array([
    [0, 1, 0, 0, 0],
    [1, 0, 1, 1, 1],
    [0, 1, 0, 1, 0],
    [0, 1, 1, 0, 1],
    [1, 1, 0, 1, 0]
])
L = create_laplacian_from_adjacency(adj_matrix)  # Create a sample Laplacian matrix
alpha, gamma, lam = 0.1, 1, 0.5  # Regularization parameters

C, L_c, X_tilde = fgc_algorithm(X, L, alpha, gamma, lam)
print(&quot;Updated C matrix:\n&quot;, C)
print(&quot;The L_C matrix:\n&quot;, L_c)
print(&quot;Updated feature matrix X(tilda):\n&quot;, X_tilde)

</code></pre>
<p>I am trying to implement the Featured Coarsened Graph Algorithm but everytime the code reaches line 34: inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C),
the mentioned error is raised.</p>
<p>I have tried checking everyting but according to me the dimensions of the matrices are correct, so I dont really understand why there is this problem?</p>
<p>If you happen to understand this please help.</p>
","0","Question"
"78460997","","<p>I'm currently learning about neural networks and I want to use the <strong>train_images()</strong> function, but I'm unable to do so. If I run the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import mnist

images = mnist.train_images()
</code></pre>
<p>, I'll get:</p>
<pre class=""lang-py prettyprint-override""><code>runfile('C:/Users/deriv/untitled0.py', wdir='C:/Users/deriv')
Traceback (most recent call last):

  File ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
    exec(code, globals, locals)

  File c:\users\deriv\untitled0.py:3
    images = mnist.train_images()

  File ~\anaconda3\Lib\site-packages\mnist\__init__.py:161 in train_images
    return download_and_parse_mnist_file('train-images-idx3-ubyte.gz')

  File ~\anaconda3\Lib\site-packages\mnist\__init__.py:143 in download_and_parse_mnist_file
    fname = download_file(fname, target_dir=target_dir, force=force)

  File ~\anaconda3\Lib\site-packages\mnist\__init__.py:59 in download_file
    urlretrieve(url, target_fname)

  File ~\anaconda3\Lib\urllib\request.py:241 in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:

  File ~\anaconda3\Lib\urllib\request.py:216 in urlopen
    return opener.open(url, data, timeout)

  File ~\anaconda3\Lib\urllib\request.py:525 in open
    response = meth(req, response)

  File ~\anaconda3\Lib\urllib\request.py:634 in http_response
    response = self.parent.error(

  File ~\anaconda3\Lib\urllib\request.py:563 in error
    return self._call_chain(*args)

  File ~\anaconda3\Lib\urllib\request.py:496 in _call_chain
    result = func(*args)

  File ~\anaconda3\Lib\urllib\request.py:643 in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)

HTTPError: Forbidden
</code></pre>
<p>I installed <strong>mnist</strong> correctly using <strong>pip install</strong> but, I don't know why** mnist.train_images()** causes the error. Sorry if this is a simple question but, it will help me a lot.</p>
<p>I don't know wheter or not I'm supposed to download files straightforward from <a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow noreferrer"">http://yann.lecun.com/exdb/mnist/</a>. However I'm not able to do so because I don't have a permission to access this resources.</p>
","2","Question"
"78461070","","<p>I am learning about Ordered logit regression and I was wondering how the prediction works mathematically and how can I do it in python by myself. I know that in python i can just simply use predict but I was wondering on how can I make a prediction with only coefs from model.summary().</p>
<pre><code>import pandas as pd
from statsmodels.miscmodels.ordinal_model import  OrderedModel


data = pd.DataFrame({
    'score': [3.2, 4.5, 5.6, 6.7, 7.8, 8.9, 9.1],
    'rating': [1,2,3,4,5,6,6]  
})

X = data[['score']]
y = data['rating']


ordinal_model = OrderedModel(y, X, distr='logit')


ordinal_results = ordinal_model.fit(method='bfgs')


print(ordinal_results.summary())

</code></pre>
<p>The outcome is:</p>
<pre><code>Time:                        17:05:52                                         
No. Observations:                   7                                         
Df Residuals:                       1                                         
Df Model:                           1                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
score         66.3902   5669.125      0.012      0.991    -1.1e+04    1.12e+04
1/2          285.5835   2.56e+04      0.011      0.991   -4.98e+04    5.04e+04
2/3            4.2698     88.656      0.048      0.962    -169.493     178.032
3/4            4.1879    155.834      0.027      0.979    -301.241     309.617
4/5            4.3867    136.765      0.032      0.974    -263.668     272.442
5/6            3.4706    220.734      0.016      0.987    -429.161     436.102
==============================================================================
</code></pre>
<p>Using coef vector how do i get the same output as in</p>
<pre><code>ordinal_results.model.predict(ordinal_results.params, exog = (4.3))
</code></pre>
<pre><code>[[0.5264086 0.4735914 0.        0.        0.        0.       ]]

</code></pre>
<p>I thought that I simply should use softmax on linear sum of coef and new data but that didn't work</p>
","1","Question"
"78461828","","<p>I want to create a model architecture to predict future stock price movement as such:
<a href=""https://i.sstatic.net/yve3Hc0w.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yve3Hc0w.png"" alt=""enter image description here"" /></a></p>
<p>The Goal of this model is to predict if the price will go UP or DOWN within the next 3 months.</p>
<p>I have tried a few models such as Logistic Regression, a Neural network, XGBoost, etc. Ive received some decent results. Through using a Random Forest Classifier I have received the best results so far. How can I encode data through using a neural network then pass those values to a Random Forest Classifier to make the classification rather than using a final output layer using a sigmoid function as shown in the picture(Using Python, Keras, and SKlearn).</p>
<p>I am not the most well versed in Keras so I want to know if it is even possible to train a neural network that feeds into a separate classifier and if so how would it be done.</p>
","0","Question"
"78462277","","<pre><code>model = Sequential()
model.add(Embedding(283, 100, input_length=56))
model.add(LSTM(150))
model.add(LSTM(150))
model.add(Dense(283, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

model.summary()
</code></pre>
<p>Tensorflow Version: 2.16.1,<br />
Keras Version: 3.3.3 ,<br />
Device - M3 pro macbook</p>
<p>I tried to build a LSTM model for text generation using a dummy dataset (had 282 unique words, checked with tokenizer), expected non zero params but the output is 0 params for each layer.</p>
","0","Question"
"78463519","","<p>I'm working on a kernel logistic regression functions except they are not returning the correct expected predictions.</p>
<p>It is also throwing exponential overflow warning, which is suppressed for now</p>
<pre><code>
    import numpy as np
    import warnings

    warnings.filterwarnings('ignore')

    def monomial_kernel(d):
        def k(x, y, d=d):
            phi_x_y = 0
            prod_xy = np.dot(x.T,y)

            for n in range(d+1):
                phi_x_y += (prod_xy ** n)

            return phi_x_y
            
        return k

    def rbf_kernel(sigma):
        def k(x, y, sigma=sigma):
            numerator = np.linalg.norm(x - y) **2
            denominator = 2 * (sigma ** 2)
            
            return np.exp(-numerator / denominator)

        return k

    def sigmoid(z):
        if type(z) == np.ndarray:
            z = z[0]
        try:
            return 1 / (1 + np.exp(-z)) 
        except:
            print(z)

    def logistic_regression_with_kernel(X, y, k, alpha, iterations):

        n_samples, _ = X.shape
        bias = 0
        kernel_matrix = np.zeros((n_samples, n_samples))
        beta = np.zeros(n_samples)

        #create kernel matrix
        for i in range(n_samples):
            for j in range(n_samples):
                kernel_matrix[i][j] = k(X[i], X[j])
        
        for _ in range(iterations):
            for i in range(n_samples):
                total = 0
                for j in range(n_samples):
                    total += beta[j] * kernel_matrix[i][j]
                total += bias
                sigmoid_value = sigmoid(total)
                t = y[i]

                beta += kernel_matrix[i] * alpha * (t - sigmoid_value)
                        
                bias += (alpha * (t - (sigmoid_value))) 

        def model(x, beta=beta, bias=bias, k=k, ref=X):
            z = sum([k(ref[i], x) * beta[i] for i in range(ref.shape[0])]) + bias 

            sig = sigmoid(z)
            # print(sig)
            return round(sig)
        return model


</code></pre>
<p>For some reason it's not correctly learning the following test case:</p>
<pre><code>    def test4():
        
    f = lambda x, y, z, w:  int(x*y*z - y**2*z*w/4 + x**4*w**3/8- y*w/2 &gt;= 0)

    training_examples = [
        ([0.254, 0.782, 0.254, 0.569], 0),
        ([0.237, 0.026, 0.237, 0.638], 0),
        ([0.814, 0.18, 0.814, 0.707], 1),
        ([0.855, 0.117, 0.855, 0.669], 1),
        ([0.776, 0.643, 0.776, 0.628], 1),
        ([0.701, 0.71, 0.701, 0.982], 0),
        ([0.443, 0.039, 0.443, 0.356], 1),
        ([0.278, 0.105, 0.278, 0.158], 0),
        ([0.394, 0.203, 0.394, 0.909], 0),
        ([0.83, 0.197, 0.83, 0.779], 1),
        ([0.277, 0.415, 0.277, 0.357], 0),
        ([0.683, 0.117, 0.683, 0.455], 1),
        ([0.421, 0.631, 0.421, 0.015], 1)
    ]

    X, y = map(np.array, zip(*training_examples))

    h = logistic_regression_with_kernel(X, y, monomial_kernel(10), 0.01, 500)

    test_examples = [
        ([0.157, 0.715, 0.787, 0.644], 0),
        ([0.79, 0.279, 0.761, 0.886], 1),
        ([0.903, 0.544, 0.138, 0.925], 0),
        ([0.129, 0.01, 0.493, 0.658], 0),
        ([0.673, 0.526, 0.672, 0.489], 1),
        ([0.703, 0.716, 0.088, 0.674], 0),
        ([0.276, 0.174, 0.69, 0.358], 1),
        ([0.199, 0.812, 0.825, 0.653], 0),
        ([0.332, 0.721, 0.148, 0.541], 0),
        ([0.51, 0.956, 0.023, 0.249], 0)
    ]
    print(f&quot;{'x' : ^30}{'prediction' : ^11}{'true' : ^6}&quot;)
    for x, y in test_examples:
        print(f&quot;{str(x) : ^30}{int(h(x)) : ^11}{y : ^6}&quot;)
    #             x               prediction  true
    # [0.157, 0.715, 0.787, 0.644]      0       0
    # [0.79, 0.279, 0.761, 0.886]       1       1
    # [0.903, 0.544, 0.138, 0.925]      0       0
    # [0.129, 0.01, 0.493, 0.658]       0       0
    # [0.673, 0.526, 0.672, 0.489]      1       1
    # [0.703, 0.716, 0.088, 0.674]      0       0
    # [0.276, 0.174, 0.69, 0.358]       1       1
    # [0.199, 0.812, 0.825, 0.653]      0       0
    # [0.332, 0.721, 0.148, 0.541]      0       0
    # [0.51, 0.956, 0.023, 0.249]       0       0
</code></pre>
<p>My results are:</p>
<p><a href=""https://i.sstatic.net/H3xbWr7O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H3xbWr7O.png"" alt=""enter image description here"" /></a></p>
<p>I've tried increasing the iteration up to 4000 and it still didn't produce the correct result</p>
","1","Question"
"78466376","","<p>I'm using the HuggingFace Transformers Pipeline library to generate multiple text completions for a given prompt. My goal is to utilize a model like GPT-2 to generate <strong>different possible completions like the defaults in vLLM</strong>. However, I am encountering an issue with unused model_kwargs when I attempt to specify parameters like max_length and num_return_sequences.</p>
<p>Here is the code snippet I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>Copy code
from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline
from typing import List, Dict

def process_prompts(prompts: List[str], model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, num_completions: int = 3) -&gt; List[List[str]]:
    device = 0 if model.device.type == 'cuda' else -1
    text_generator = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer, device=device)
    outputs = []

    for prompt in prompts:
        try:
            results = text_generator(prompt, max_length=50, num_return_sequences=num_completions, num_beams=num_completions)
            completions = [result['generated_text'] for result in results]
            outputs.append(completions)
        except Exception as e:
            print(f&quot;Error processing prompt {prompt}: {str(e)}&quot;)

    return outputs

if __name__ == &quot;__main__&quot;:
    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
    model.to(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    example_prompts = [&quot;Hello, how are you?&quot;]
    processed_outputs = process_prompts(example_prompts, model, tokenizer, num_completions=3)
    for output in processed_outputs:
        print(output)
</code></pre>
<p>and also:</p>
<pre class=""lang-py prettyprint-override""><code>            results = text_generator(prompt, max_length=50, num_return_sequences=num_completions)
</code></pre>
<p>When I run this, I get the following error:</p>
<pre><code>The following `model_kwargs` are not used by the model: ['max_len']
Note: I am aware that typos in the generate arguments can also trigger this warning, but I've checked and rechecked the arguments names.
</code></pre>
<p>and</p>
<pre><code>   raise ValueError(
ValueError: Greedy methods without beam search do not support `num_return_sequences` different than 1 (got 4).
</code></pre>
<p>What could be causing this error, and how can I fix it to generate multiple completions effectively using the model?</p>
<p>cross: <a href=""https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without-triggering-an-error/86297"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without-triggering-an-error/86297</a></p>
","1","Question"
"78466923","","<pre><code>def ans(n, depth):
    qc = QuantumCircuit(n)
    for j in range(depth):
        for i in range(n):
            param_name = f'theta_{j}_{i}'
            theta_param = Parameter(param_name)
            qc.rx(theta_param, i)
            qc.ry(theta_param, i)
            qc.rz(theta_param, i)
    for i in range(n):
        if i == n-1:
            qc.cnot(i, 0)
        else:
            qc.cnot(i, i+1)
    return qc
</code></pre>
<p>This method seems to be deprecated and removed. I got this error while trying to replicate the stock price prediction using Quantum circuits from <a href=""https://medium.com/datadriveninvestor/stock-price-prediction-with-quantum-machine-learning-in-python-54948a3da389"" rel=""nofollow noreferrer"">THIS</a> post.</p>
<p>I want an alternate function that has been given in the new version of qiskit</p>
","0","Question"
"78467998","","<p>I am doing a university project on index/stock price prediction. I plan to use a combined cnn-lstm model, and I have several different types of data: Open High Low Close Volume, values, fundamental data such as unemployment and various rates, technical indicators like RSI, MACD and others, and moving averages like SMA, EMA, WMA and etc. What is the best way to prepare data for the network?</p>
<p>At this moment I am using the following transformations
for OHLC - simple differentiation
for fundamental data - logarithmization
for moving averages - subtract the candle opening value from the value of this moving average
indicator values unchanged
Then I use StandardizeNormalizer for all dataset. I also tried normalizing (robust scaling, standardization, minmax scaling too) each sequence separately, and differentiating all the data, but it was not effective</p>
","1","Question"
"78468674","","<p>I am new in Machine Learning and I have a task that required me to perform unsupervised learning, so I decided to use K-Means.</p>
<p>I using Python to code. I have imported the data (my data is from a csv file) into Google Colab. My data has 7 features, and I need to plot the clusters, but I get the error: scatter() got multiple values for argument 'c'.</p>
<p>This is my code:</p>
<p>This part is how I decide my k value. I use Elbow Method.</p>
<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
import pandas as pd

from sklearn.cluster import KMeans

DATA = &quot;/content/information.csv&quot;
df = pd.read_csv (DATA, header=0)
data = list(zip(x_train[&quot;date&quot;], x_train[&quot;a&quot;], x_train[&quot;b&quot;], x_train[&quot;c&quot;], x_train[&quot;d&quot;], x_train[&quot;e&quot;], x_train[&quot;f&quot;]))
print(data)

inertias = []

for i in range(1,40):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data) 
    inertias.append(kmeans.inertia_)

plt.plot(range(1,40), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
</code></pre>
<p>And this is where it goes wrong:</p>
<pre><code>kmeans = KMeans(n_clusters=5) 
kmeans.fit(data)  
plt.scatter(x_train[&quot;date&quot;], x_train[&quot;a&quot;], x_train[&quot;b&quot;], x_train[&quot;c&quot;], x_train[&quot;d&quot;], x_train[&quot;e&quot;], x_train[&quot;f&quot;], c=kmeans.labels_) 
plt.show()  
</code></pre>
<p>The error seems to indicated that the part that goes wrong is at the plt.scatter() line.</p>
<p>I tried with 2 features, it works, but when it comes to 7 features, I get error message. What could have gone wrong?</p>
","2","Question"
"78469183","","<p>So I was trying to look into models that could help determine if a company name is the same. So basically I have a dataset that lists a bunch of company names that different investment funds hold but every investment fund may have a different naming convention for the company name such as Apple Inc or Apple Common stock. Is there any model that can help determine if they are the same company name.  I Know this might just be beyond something I can do but I know chatgpt can do this really well I can pass it like 20 company names and it will tell me if they are all the same or not but obviously my dataset has 500,000 rows so I don't know anyway I can just pass that data to chatgpt and I have heard it fails on longer computations.</p>
<p>I looked into levenshtein distance and fuzzy matching but I feel like it fails when the string has stuff like common stock in the name even though as a human it is clear Apple inc and Apple Us common stock would refer to the same company.So any machine learning algorithm or models that could help me tackle this problem would be greatly appreciated. I'm happy to look into them myself I just want a starting point as fuzzy matching and the levenshtein distance has not been very helpful unless I potentially find a way to remove common filler words such as common stock, ptd, lty etc. Thanks for any help :))</p>
<p>Edit Again: Here is a smaller demo.</p>
<pre><code>import pandas as pd
from fuzzywuzzy import fuzz 

data = [
    &quot;29METALS LIMITED&quot;,
    &quot;29METALS LIMITED&quot;,
    &quot;29Metals Limited&quot;,
    &quot;29METALS LTD&quot;,
    &quot;29METALS LTD.&quot;,
    &quot;29METENTY LTD&quot;,
    &quot;29 METALS COMMON STOCK&quot;,
    &quot;29 METAL PTD&quot;,
    &quot;29METALS&quot;,
]

df = pd.DataFrame(data, columns=[&quot;Company&quot;])

df[&quot;Cleaned_Company&quot;] = df[&quot;Company&quot;].str.replace(&quot;[^\w\s]&quot;, &quot;&quot;).str.lower()


def match_names(name, group):
    for group_name in group:
        if fuzz.ratio(name, group_name) &gt; 80:  
            return group_name
    return name

groups = {}
for name in df[&quot;Cleaned_Company&quot;].unique():
    matched_name = match_names(name, groups.keys())
    if matched_name in groups:
        groups[matched_name].append(name)
    else:
        groups[matched_name] = [name]


group_number = 1
for group_name, group_values in groups.items():
    df.loc[df[&quot;Cleaned_Company&quot;].isin(group_values), &quot;Group&quot;] = group_number
    group_number += 1

print(df)
</code></pre>
<p>Now the output is:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Company</th>
<th>Cleaned_Company</th>
<th>Group</th>
</tr>
</thead>
<tbody>
<tr>
<td>29METALS LIMITED</td>
<td>29metals limited</td>
<td>1.0</td>
</tr>
<tr>
<td>29METALS LIMITED</td>
<td>29metals limited</td>
<td>1.0</td>
</tr>
<tr>
<td>29Metals Limited</td>
<td>29metals limited</td>
<td>1.0</td>
</tr>
<tr>
<td>29METALS LTD</td>
<td>29metals ltd</td>
<td>1.0</td>
</tr>
<tr>
<td>29METALS LTD.</td>
<td>29metals ltd</td>
<td>1.0</td>
</tr>
<tr>
<td>29METENTY LTD</td>
<td>29metenty ltd</td>
<td>2.0</td>
</tr>
<tr>
<td>29 METALS COMMON STOCK</td>
<td>29 metals common stock</td>
<td>3.0</td>
</tr>
<tr>
<td>29 METAL PTD</td>
<td>29 metal ptd</td>
<td>4.0</td>
</tr>
<tr>
<td>29METALS</td>
<td>29metals</td>
<td>5.0</td>
</tr>
</tbody>
</table></div>
<p>From this we can see it identified 5 groups in actuality they are all the same company except the 29Mententy LTD. I hope this makes my post more clear, thanks :)</p>
<p>Edit 3:
Here's another example this is a snippet of the data from the input file:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Company</th>
</tr>
</thead>
<tbody>
<tr>
<td>MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125</td>
</tr>
<tr>
<td>MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125</td>
</tr>
<tr>
<td>MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125</td>
</tr>
<tr>
<td>MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125</td>
</tr>
<tr>
<td>MOL HUNGARIAN OIL AND GAS PL</td>
</tr>
<tr>
<td>MOL HUNGARIAN OIL AND GAS PL</td>
</tr>
<tr>
<td>MOL HUNGARIAN OIL AND GAS PL</td>
</tr>
<tr>
<td>MOL MAGYAR OLAJ GAZIPARI</td>
</tr>
<tr>
<td>Mol Magyar Olajes Gazipari Nyrt</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>Molina Healthcare Inc</td>
</tr>
<tr>
<td>Molina Healthcare Inc</td>
</tr>
<tr>
<td>MOLINA HEALTHCARE INC</td>
</tr>
<tr>
<td>Molina Healthcare Inc</td>
</tr>
</tbody>
</table></div>
<p>The output should then be these company names and a group number assigned for example the first 7 entries should have the same group number because they are the same company and the next 2 would have another group and the rest would have a different group number assigned to them.</p>
","0","Question"
"78469835","","<p>I have 10 datasets (.csv) each with 100,000 rows, with each row containing 5 inputs ( -4.0f to +4.0f) and a output column (0/1). I want to train a Neural Network using this and predict the test dataset that I was given (which has 100,000 rows too, but without output column filled).</p>
<p>I thought of creating a 5--(reLU)--&gt; 32 --(reLU)--&gt; 32 --(sigmoid) --&gt; 1 Neural Network and train it with reward system like this [if (expec.op  ==0) reward=1- o/pfromNN; if (expec.op  ==1) reward= o/pfromNN].</p>
<p>How can I adjust weights of NN using this or How can I proceed further? I am a newbie to NN.</p>
<p>I thought of doing this like lunar lander module in gymnasium but since there are no states involved here I am confused</p>
","-2","Question"
"78470290","","<p>Here is my code:</p>
<pre><code>X = store1.drop(['Store','Date', 'Holiday_Flag','Days','Temperature'], axis=1)
y = store1['Weekly_Sales']

# scaling the predictor data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_sc = sc.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=0.2, random_state=21)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred = lin_reg.predict(X_test)

print(&quot;MAE: {}&quot; .format(mean_absolute_error(y_test, y_pred)))
print(&quot;RMSE: {}&quot; .format(mean_squared_error(y_test, y_pred)))
</code></pre>
<p>pic of notebook: <a href=""https://i.sstatic.net/Z48nX8dm.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/Z48nX8dm.png</a></p>
<p>It seems correct but this much of accuracy is really a concern i seek for a knowledgeable guidance.</p>
","0","Question"
"78471070","","<p>I need to tag a series of images for sewing detection on fabric. I work with the YOLOv5 algorithm.</p>
<p>The problem I have is that it is not clear to me what should be the optimal way to label these sewings.</p>
<p>The image below shows a sewing in the fabric.</p>
<p><a href=""https://i.sstatic.net/6wJljjBM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6wJljjBM.jpg"" alt=""enter image description here"" /></a></p>
<p>As you can see in the image, the sewings are always going to occupy the whole width of the fabric. Initially I had thought of labelling several sections/portions of the sewing (the number of sewings detected is not really important, what really matters to me is that it detects that there is at least one sewing). The image below shows this idea:</p>
<p><a href=""https://i.sstatic.net/zOMsZTu5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zOMsZTu5.png"" alt=""enter image description here"" /></a></p>
<p>However it is not clear to me if this would be a correct method (optimal, rather) or it should be to create a single label that completely encloses the sewing.</p>
<p>On the other hand, according to the labelling tips given in the documentation, the label should exactly enclose the object to be detected, leaving as little space as possible between the object and the bounding box of the label.</p>
<blockquote>
<p>Label accuracy. Labels must closely enclose each object. No space
should exist between an object and it's bounding box. No objects
should be missing a label.</p>
</blockquote>
<p><a href=""https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/"" rel=""nofollow noreferrer"">Tips for Best Training Results</a></p>
<p>In this particular case, and given that the sewings will always appear in a very similar way (they will always have a horizontal orientation), these labels would be very thin (with very little height), so it is not clear to me that the algorithm will be able to detect them. In my humble opinion, I believe that slightly increasing the height of the labels would allow the algorithm to detect the sewings more efficiently, since the fabrics that are joined through these sewings may be of the same colour. (the second of the images shows the idea I am talking about).</p>
<p>I would be grateful if you could help me and tell me the best way to do this labelling.</p>
<p>Thank you very much in advance.</p>
","0","Question"
"78473057","","<p>With the following code:</p>
<pre><code>def __init__(self, model, num_folds=5, batch_size=32, epochs=10, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
    &quot;&quot;&quot;
    Initialize the ModelTrainer with specified parameters.

    Args:
        model (torch.nn.Module): The PyTorch model to be trained and validated.
        num_folds (int): The number of folds for stratified k-fold cross-validation. Default is 5.
        batch_size (int): Batch size for training and validation. Default is 32.
        epochs (int): Number of epochs for training. Default is 10.
        lr (float): Learning rate for the optimizer. Default is 0.001.
        betas (tuple): Coefficients used for computing running averages of gradient and its square.
                       Default is (0.9, 0.999).
        eps (float): Term added to the denominator to improve numerical stability in the optimizer.
                     Default is 1e-8.
    &quot;&quot;&quot;
    self.model = model
    self.num_folds = num_folds
    self.batch_size = batch_size
    self.epochs = epochs
    self.lr = lr
    self.betas = betas
    self.eps = eps

def train_and_validate(self, data, labels):
    &quot;&quot;&quot;
    Train and validate the model using stratified k-fold cross-validation.

    Args:
        data (torch.Tensor): The input data for training and validation.
        labels (torch.Tensor): The labels corresponding to the input data.

    Returns:
        auc_scores (list): A list of AUC scores for each fold.
        sensitivities (list): Sensitivities at 100% specificity for each fold.
        mean_auc (float): Mean AUC score across all folds.
        mean_sensitivity (float): Mean sensitivity at 100% specificity across all folds.
    &quot;&quot;&quot;
    # Initialize StratifiedKFold
    skf = StratifiedKFold(n_splits=self.num_folds, shuffle=True, random_state=42)

    # Lists to store AUC scores and sensitivities for each fold
    auc_scores = []
    sensitivities = []

    # Lists to store ROC curve data for each fold
    tprs = []
    mean_fpr = np.linspace(0, 1, 100)

    # Iterate over folds
    for fold, (train_index, val_index) in enumerate(skf.split(data, labels)):
        # Get the data for this fold
        X_train_fold, X_val_fold = data[train_index], data[val_index]
        y_train_fold, y_val_fold = labels[train_index], labels[val_index]

        # Create PyTorch datasets and data loaders for this fold
        train_dataset = TensorDataset(X_train_fold, y_train_fold)
        val_dataset = TensorDataset(X_val_fold, y_val_fold)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)

        # Define loss function and optimizer
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas, eps=self.eps)

        # Training loop for this fold
        for epoch in range(self.epochs):
            self.model.train()
            for inputs, labels in train_loader:
                optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

        # Validation loop for this fold
        self.model.eval()
        val_outputs_list = []
        val_labels_list = []
        for inputs, labels in val_loader:
            with torch.no_grad():
                val_outputs = self.model(inputs)
                val_outputs_list.append(val_outputs.numpy())
                val_labels_list.append(labels.numpy())
        val_outputs_np = np.concatenate(val_outputs_list)
        val_labels_np = np.concatenate(val_labels_list)

        # Calculate ROC curve for this fold
        fpr, tpr, thresholds = roc_curve(val_labels_np, val_outputs_np)
        roc_auc = auc(fpr, tpr)
        auc_scores.append(roc_auc)

        # Calculate sensitivity at 100% specificity
        sensitivity = np.interp(1e-3, fpr, tpr)
        sensitivities.append(sensitivity)

        # Store ROC curve data for this fold
        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0

        # Plot ROC curve for this fold
        plt.plot(fpr, tpr, lw=1, alpha=0.3)

    # Plot settings
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.show()

    # Calculate mean ROC curve
    mean_tpr = np.mean(tprs, axis=0)
    mean_auc = auc(mean_fpr, mean_tpr)

    # Plot mean ROC curve with thicker line
    plt.plot(mean_fpr, mean_tpr, color='black', lw=2, linestyle='--', label=f'Mean ROC (AUC = {mean_auc:.2f})')
    plt.legend(loc='lower right')
    plt.show()

    # Calculate mean sensitivity at 100% specificity
    mean_sensitivity = np.mean(sensitivities)

    return auc_scores, sensitivities, mean_auc, mean_sensitivity
</code></pre>
<p>I constantly get the index out of bounds error in y_train_fold, y_val_fold = labels[train_index], labels[val_index] when the second fold wants to begin. The first fold runs. My data is shaped
(864,19,500) and my binary labels (864). Can anyone spot something wrong with this code that might trigger the error?</p>
<p>I tried balancing the class distributions with SMOTE, checking the length and shapes of the labels but I could not get it working. During the first fold, the length of labels is 864, but during the second fold, the shape becomes 24 when I use num_folds=4, resulting in the out of bounds error at index 24 for dimension 0 with size 24</p>
","0","Question"
"78474448","","<p>I want to load a huggingface model. <a href=""https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"" rel=""nofollow noreferrer"">The model</a> I want to load has about 150K downloads so I don't think there is any problem with the model itself.</p>
<p>With the both loading codes below I get the same error:</p>
<pre><code>from transformers import AutoModel
AutoModel.from_pretrained(&quot;laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup&quot;)
</code></pre>
<p>And</p>
<pre><code>from transformers import CLIPProcessor, CLIPModel
model_id = &quot;laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup&quot;
processor = CLIPProcessor.from_pretrained(model_id)
model = CLIPModel.from_pretrained(model_id)
</code></pre>
<p>With both I get:</p>
<pre><code>OSError: laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/main' for available files.
</code></pre>
<p>Any help to load the model would be appreciated.</p>
","0","Question"
"78475573","","<p>I'm building a restaurant recommendation system in Python using user preferences (cuisine, price range) and cosine similarity. I'm encountering a ValueError: User preference vector and restaurant feature vectors must have the same number of dimensions. Error under two scenarios:</p>
<p>Scenario 1:</p>
<p>I'm using one-hot encoding for cuisine types in the restaurant data.
When building the user preference vector, the get_user_preferences function considers both cuisine and price range (if provided).
The error occurs when the user selects &quot;all&quot; cuisines (one-hot encoded vector with all features as 1).
Scenario 2:</p>
<p>The error occurs even when the user selects a specific cuisine (one-hot encoded vector with a single feature as 1).
I suspect there might be a mismatch in how cuisines are represented in the user preference vector and the restaurant data (e.g., single cuisine vs. list of cuisines).</p>
<h1>2. Define User Preference Function</h1>
<p>def get_user_preferences(cuisine=None, price_range=None):</p>
<h1>Initialize an empty preference vector</h1>
<p>user_pref = [0] * len(encoder.get_feature_names_out())</p>
<h1>Set cuisine preference (one-hot encoding)</h1>
<p>if cuisine:
cuisine_index = encoder.get_feature_names_out().tolist().index(cuisine)
user_pref[cuisine_index] = 1</p>
<h1>Set price range</h1>
<p>if price_range:
user_pref.append(price_range)  # Assuming price range is a numerical value</p>
<p>return user_pref</p>
<p>def get_user_preferences(cuisine=None, price_range=None):
ist: User preference vector with one-hot encoded cuisine and price range.
&quot;&quot;&quot;
# Initialize an empty preference vector with an equal preference for all cuisines
num_cuisines = len(encoder.get_feature_names_out())
user_pref = [1 / num_cuisines] * num_cuisines</p>
<pre><code># Set cuisine preference (one-hot encoding)
if cuisine:
    cuisine_index = encoder.get_feature_names_out().tolist().index(cuisine)
    user_pref[cuisine_index] = 1

# Set price range
if price_range:
    user_pref.append(price_range)  # Assuming price range is a numerical value

return user_pref
</code></pre>
<p>#3</p>
<p>def cosine_similarity(user_pref, restaurant_vec):
# Extract cuisine vectors from user preference and restaurant vectors
user_pref_cuisine = user_pref[:-1] if len(user_pref) &gt; 1 else user_pref  # Exclude price range if available
restaurant_vec_cuisine = restaurant_vec[:-1] if len(restaurant_vec) &gt; 1 else restaurant_vec</p>
<h1>Check that the user_pref_cuisine and restaurant_vec_cuisine vectors have the same length</h1>
<pre><code>if len(user_pref_cuisine) != len(restaurant_vec_cuisine):
    raise ValueError(&quot;User preference cuisine vector and restaurant cuisine vector must have the same number of dimensions.&quot;)

# Calculate cosine similarity
return np.dot(user_pref_cuisine, restaurant_vec_cuisine) / (np.linalg.norm(user_pref_cuisine) * np.linalg.norm(restaurant_vec_cuisine))
</code></pre>
<p>def get_user_preferences_from_user():
&quot;&quot;&quot;
This function prompts the user to choose their preferred cuisine and price range.</p>
<pre><code>Returns:
    List: User preference vector with one-hot encoded cuisine and price range.
&quot;&quot;&quot;
# Get user input for cuisine
available_cuisines = list(df['Cuisines'].unique())  # Assuming unique cuisines are in a column
while True:
    cuisine = input(&quot;Enter your preferred cuisine (or 'all' for any): &quot;).lower().strip()
    if cuisine in available_cuisines or cuisine == 'all':
        break
    else:
        print(f&quot;Invalid cuisine. Available options are: {', '.join(available_cuisines)}&quot;)

# Get user input for the price range
price_ranges = {
    &quot;1&quot;: &quot;Cheap&quot;,
    &quot;2&quot;: &quot;Moderate&quot;,
    &quot;3&quot;: &quot;Fine Dining&quot;
}
while True:
    price_range = input(&quot;Enter your preferred price range (1 - Cheap, 2 - Moderate, 3 - Fine Dining): &quot;)
    if price_range in price_ranges:
        break
    else:
        print(f&quot;Invalid price range. Please enter 1, 2, or 3.&quot;)

# Encode cuisine (if not 'all')
if cuisine != 'all':
    cuisine_index = encoder.get_feature_names_out().tolist().index(cuisine)
    user_pref = [0] * len(encoder.get_feature_names_out())
    user_pref[cuisine_index] = 1
else:
    user_pref = [1] * len(encoder.get_feature_names_out())  # One-hot encode for all cuisines

# Add price range if specified
if price_range:
    user_pref.append(int(price_range))

# Ensure that the user preference vector has the same number of features as the restaurant feature vectors
if len(user_pref) != len(df.drop(columns=['Restaurant Name']).values[0]):
    raise ValueError(&quot;User preference vector and restaurant feature vectors must have the same number of dimensions.&quot;)

return user_pref
</code></pre>
<h1>4. Recommend Restaurants</h1>
<p>def recommend_restaurants(user_pref, df=df, top_n=5):
&quot;&quot;&quot;
This function recommends restaurants based on user preferences and similarity scores.
Args:
user_pref (list): User preference vector.
df (pandas.DataFrame, optional): Preprocessed restaurant data. Defaults to df.
top_n (int, optional): Number of top recommendations to return. Defaults to 5.
Returns:
pandas.DataFrame: Top N recommended restaurants with details.
&quot;&quot;&quot;
# Construct restaurant feature vectors including encoded cuisines
restaurant_feature_vectors = df.drop(columns=['Restaurant Name']).values</p>
<pre><code># Check if the user preference vector and restaurant feature vectors have the same number of dimensions
if len(user_pref) != len(restaurant_feature_vectors[0]):
    raise ValueError(&quot;User preference vector and restaurant feature vectors must have the same number of dimensions.&quot;)

# Calculate similarity scores for each restaurant
similarities = np.array([cosine_similarity(user_pref, vec) for vec in restaurant_feature_vectors])

# Sort restaurants by similarity score (descending)
df_sorted = df.assign(similarity=similarities).sort_values(by='similarity', ascending=False)

# Return top N recommendations
return df_sorted.head(top_n).drop('similarity', axis=1)
</code></pre>
<h1>Assuming you have a function to get user input for cuisine and price range</h1>
<p>user_pref = get_user_preferences_from_user()  # Replace with your function</p>
<h1>Call the recommend_restaurants function with your data and user preferences</h1>
<p>recommendations = recommend_restaurants(user_pref, df=df)  # Use your data in df</p>
<p>print(&quot;Top Restaurant Recommendations:&quot;)
print(recommendations)</p>
<h2>Enter your preferred cuisine (or 'all' for any): all
Enter your preferred price range (1 - Cheap, 2 - Moderate, 3 - Fine Dining): 4
Invalid price range. Please enter 1, 2, or 3.
Enter your preferred price range (1 - Cheap, 2 - Moderate, 3 - Fine Dining): 1</h2>
<p>ValueError                                Traceback (most recent call last)
 in &lt;cell line: 2&gt;()
1 # Assuming you have a function to get user input for cuisine and price range
----&gt; 2 user_pref = get_user_preferences_from_user()  # Replace with your function
3
4 # Call the recommend_restaurants function with your data and user preferences
5 recommendations = recommend_restaurants(user_pref, df=df)  # Use your data in df</p>
<p> in get_user_preferences_from_user()
42     # Ensure that the user preference vector has the same number of features as the restaurant feature vectors
43     if len(user_pref) != len(df.drop(columns=['Restaurant Name']).values[0]):
---&gt; 44         raise ValueError(&quot;User preference vector and restaurant feature vectors must have the same number of dimensions.&quot;)
45
46     return user_pref</p>
<p>ValueError: User preference vector and restaurant feature vectors must have the same number of dimensions.</p>
","0","Question"
"78475975","","<p>I have been trying to reproduce the results of this repo-
<a href=""https://github.com/sefcom/VarBERT/tree/main"" rel=""nofollow noreferrer"">https://github.com/sefcom/VarBERT/tree/main</a></p>
<p>I was able to train the BERT model for MLM objective. But in the Constrained Masked Language Model training I'm consistently facing an error -</p>
<pre><code>`/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/14/2024 06:58:30 - INFO - __main__ -   ***** Running training *****
05/14/2024 06:58:30 - INFO - __main__ -     Num examples = 4509495
05/14/2024 06:58:30 - INFO - __main__ -     Num Epochs = 30
05/14/2024 06:58:30 - INFO - __main__ -     Instantaneous batch size per GPU = 32
05/14/2024 06:58:30 - INFO - __main__ -     Total train batch size (w. parallel, distributed &amp; accumulation) = 32
05/14/2024 06:58:30 - INFO - __main__ -     Gradient Accumulation steps = 1
05/14/2024 06:58:30 - INFO - __main__ -     Total optimization steps = 4227660
05/14/2024 06:58:30 - INFO - __main__ -     Starting fine-tuning.
Epoch:   0%|                                                                                                                                                                           | 0/30 [00:00&lt;?, ?it/s/opt/conda/conda-bld/pytorch_1711403408687/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.t/s]
Iteration:   0%|                                                                                                                                                                   | 0/140922 [00:01&lt;?, ?it/s]
Epoch:   0%|                                                                                                                                                                           | 0/30 [00:01&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/home/hprakash/VarBERT/varbert/cmlm/training.py&quot;, line 944, in &lt;module&gt;
    main()
  File &quot;/home/hprakash/VarBERT/varbert/cmlm/training.py&quot;, line 892, in main
    global_step, tr_loss = train(args, train_dataset, model, tokenizer)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/VarBERT/varbert/cmlm/training.py&quot;, line 488, in train
    outputs = model(inputs,labels=labels)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/VarBERT/varbert/cmlm/training.py&quot;, line 139, in forward
    masked_lm_loss = loss_fct(prediction_scores.view(-1, vocab_size), labels.view(-1))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/torch/nn/modules/loss.py&quot;, line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/hprakash/.conda/envs/HP/lib/python3.11/site-packages/torch/nn/functional.py&quot;, line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.`
</code></pre>
<p>I ran the training file on the CPU and got this error-</p>
<pre><code>IndexError: Target 50001 is out of bounds.

</code></pre>
<p>I found some articles online where it was said this is caused due to discrepancies between the model's expected vocabulary size and the size defined in tokenizers config. I made changes accordingly but the error still persists. I need to figure this issue out to be able to further train the model.</p>
","0","Question"
"78477344","","<p>I'm using a preprocessed, z-score normalized list as the source for my dataset.</p>
<p>Here's a collage of images augmented by Albumentations:</p>
<p><a href=""https://i.sstatic.net/XIA8y5zc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Here's my <code>Compose</code>:</p>
<pre><code>augmentation = A.Compose([
    A.HorizontalFlip(),
    A.RandomBrightnessContrast(brightness_limit=(-0.0001, 0.0001), contrast_limit=(-0.01, 0.01)),
    A.CoarseDropout(8, 0.1, 0.1),
    A.Rotate(limit=15),
    A.Affine(shear=(-2, 2), scale=(0.95, 1.05)),
&gt;! ToTensorV2()
])
</code></pre>
<p>On the 50% of the images that RandBrightnessContrast is applied even with very small parameters, the whole distribution of the image is squashed to [0, 1] (from ~-2,~2 as expected for z-score normalized images).
Any way around this?</p>
<p>Maybe I should perform z-score normalization after these, but my original intent was to separate all deterministic steps (resize, normalize etc.) from the augmentation steps for efficiency.</p>
","1","Question"
"78481612","","<p>The minimum example is</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.losses import huber

# create dataset
x = np.random.rand(10, 1)
y = 2 * x + np.random.randn(10, 1) * 0.1

# define model
model = keras.Sequential([
    keras.layers.Dense(1, input_shape=[1])
])

# compile model
# model.compile(loss=huber, optimizer='adam')  # works
# model.compile(loss='huber', optimizer='adam')  # works
model.compile(loss=huber(delta=0.1), optimizer='adam')

# training
model.fit(x, y, epochs=5)
</code></pre>
<p>when I use huber loss in model.compile(), these two ways work.</p>
<pre class=""lang-py prettyprint-override""><code>from keras.losses import huber
</code></pre>
<pre class=""lang-py prettyprint-override""><code>model.compile(loss=&quot;huber&quot;, optimizer=optimizer='adam')

or 

model.compile(loss=huber, optimizer=optimizer='adam')

</code></pre>
<p>But if I want to add delta, there will be a TypeError.
What's the right way to add delta?
Thank you for advance.</p>
<pre><code>
---&gt; 18 model.compile(loss=huber(delta=delta), optimizer='adam')
     

File ~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    151 except Exception as e:
...
-&gt; 1170 result = api_dispatcher.Dispatch(args, kwargs)
   1171 if result is not NotImplemented:
   1172   return result

TypeError: Missing required positional argument
</code></pre>
","1","Question"
"78483192","","<p>I was trying to install Rasa, using the command <code>pip install rasa</code> in my windows 10 laptop.
I have Python version 3.11 installed.
But am getting the below errors:</p>
<pre><code>Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [20 lines of output]
      Traceback (most recent call last):
        File &quot;C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line   
     353, in &lt;module&gt;    
          main()
    File &quot;C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 335,   
    in main        
          json_out['return_val'] = hook(**hook_input['kwargs'])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</code></pre>
<p>What am I missing? Previously, i was trying to install chatterbot module and i was stuck with similar error as well.</p>
<p>Also, I tried with <code>pip install chatterbot</code> as well, but still got error.</p>
","0","Question"
"78486992","","<p>I am trying to run training using Sagemaker's training jobs and the Sagemaker Python SDK, the training script relies on some custom libraries. From my understanding, because of the custom script, I need to generate a custom image using a docker container that's been registered to ECR (Elastic Container Registry). The environment below is a Sagemaker Studio Code Editor.</p>
<p>The error I get is <code>Failed to parse hyperparameter</code>. See below for my set up and what I've tried as a solution.</p>
<p>Directory</p>
<pre><code>working directory
    —Dockerfile
    —train.py
    —requirements.txt 
</code></pre>
<p>Dockerfile</p>
<pre><code># Use python image as base
FROM python:3.10

# Install system dependencies
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends \
        libpq-dev \
        gcc \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Set working directory in container
COPY code /opt/program
WORKDIR /code

# Install Python dependencies
COPY requirements.txt /code/
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install sagemaker-training

# Copies the training code inside the container
COPY train.py /opt/ml/code/train.py

# Defines train.py as script entrypoint
ENV SAGEMAKER_PROGRAM train.py

# Set environment variables
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
ENV PATH=&quot;/opt/program:${PATH}&quot;
</code></pre>
<p>requirements.txt</p>
<pre><code>simpletransformers==0.70.0
pandas==2.1.1
numpy==1.26.0
torch==2.2.1
sklearn-deap==0.3.0
sklearn-genetic-opt==0.10.1
boto3==1.33.3
sagemaker
</code></pre>
<p>train.py</p>
<pre><code>import argparse
import os
import logging
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from simpletransformers.classification import ClassificationModel
import torch

from sagemaker_pytorch_estimator.pytorch_estimator import PyTorchModel
from sagemaker_containers.data_instances.data_buffer import BufferDataset, BufferedShuffledDataset

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.StreamHandler())

if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()

    parser.add_argument(&quot;--batch_size&quot;, type=int, default=32)
    parser.add_argument(&quot;--test_size&quot;, type=float, default=0.2)
    parser.add_argument(&quot;--target_column&quot;, type=str, default=&quot;annotation&quot;)
    parser.add_argument(&quot;--vertical&quot;, type=str, default=&quot;some_category&quot;)
    parser.add_argument(&quot;--model_dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;))
    parser.add_argument(&quot;--train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;))
    parser.add_argument(&quot;--val&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_VAL&quot;))
    parser.add_argument(&quot;--test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;))

    args, _ = parser.parse_known_args()

    model_data = None
    role = None
    entry_point = None

    ....(script continues)
</code></pre>
<p>Launching script:</p>
<pre><code>import sagemaker
from sagemaker.session import TrainingInput
from sagemaker.estimator import Estimator

vertical = 'some_category'
s3_bucket = 'some_bucker'
prefix = 'classification'
instance_type = 'ml.m4.xlarge' 
print(&quot;Instance Type: {}&quot;.format(instance_type))

region = sagemaker.Session().boto_region_name
print(&quot;AWS Region: {}&quot;.format(region))

role = sagemaker.get_execution_role()
print(&quot;RoleArn: {}&quot;.format(role))

s3_output_location='s3://{}/{}/{}'.format(s3_bucket, prefix, 'classifier')
container = '############.###.###.##-####-#.amazonaws.com/some-name/ml-training:latest'
print(&quot;Image Container: {}&quot;.format(container))

estimator = Estimator(
    image_uri=container,
    role=role,
    instance_count=1,
    instance_type=instance_type,
    volume_size=10,
    output_path=s3_output_location,
    sagemaker_session=sagemaker.Session()
)

estimator.set_hyperparameters(vertical=vertical,
                              s3_bucket=s3_bucket,
                              target_column='annotation',
                              test_size=0.2)

estimator.fit()
</code></pre>
<p>Error</p>
<pre><code>Failed to parse hyperparameter 
</code></pre>
<p>What I've tried as a solution:</p>
<ol>
<li>Seems to be an <a href=""https://github.com/aws/sagemaker-training-toolkit/issues/66"" rel=""nofollow noreferrer"">open issue</a> for the <code>sagemaker-training</code> library. The only suggestion there was to wrap the hyperparameters around a function some user suggested but that doesn't seem to be a current working solution (got this error: <code>TypeError: Estimator.set_hyperparameters() takes 1 positional argument but 2 were given</code>)</li>
<li>There are some <a href=""https://github.com/aws/sagemaker-python-sdk/issues/613"" rel=""nofollow noreferrer"">suggestions</a> here but I could not figure out how to apply them for my situation.</li>
<li>This <a href=""https://stackoverflow.com/questions/76326776/cant-get-hyperparameters-to-pass-in-as-arguments-to-a-sagemaker-estimator-when"">SO post</a> claims <code>argparse</code> is not compatible with Sagemaker (all the official aws sagemaker documentation uses <code>argparse</code>). Their suggested solution is unclear to me.</li>
</ol>
","0","Question"
"78490240","","<p>I am working on a machine learning project where I have a dataset with a combination of numeric columns and columns containing arrays. The numeric columns (eg. Mean) contain single values, while the columns with arrays (eg. Gradient) can have a variable number of elements per row.</p>
<p>What are the best practices for dealing with this type of input? Can I use numeric columns and columns with arrays simultaneously in a machine learning model? If so, what are the most common strategies to handle this heterogeneity of data during the preprocessing and training phase of the model?</p>
<p>I would greatly appreciate any suggestions or resources that can help me better understand how to deal with this challenge.</p>
<p>Example:</p>
<p><strong>Input of the model:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Mean</th>
<th>Gradient</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.5</td>
<td>[1,2,3,45,0.2]</td>
</tr>
<tr>
<td>1</td>
<td>[2,5,1.2,5,0]</td>
</tr>
</tbody>
</table></div>
","-1","Question"
"78490506","","<p>I'm looking for the best practice to avoid data leakage. I have 1 feature that requires mode imputation. The model is XGBoost Classifier.</p>
<p>These are the steps that I planned:</p>
<ol>
<li>Split data in random 80% training set - 20% test set</li>
<li>Apply mode imputation on training and test set independently</li>
<li>Execute RandomSearchCV on the training set for hyperparameter tuning</li>
<li>Train model on the whole training set using the best parameter set found</li>
<li>Evaluate model on unseen test set</li>
</ol>
<p>Now, my doubt is: is it okay to perform the mode imputation on the whole training set and then perform RandomSearchCV? I thought that data leakage applies only when evaluating with the unseen test set.
Or should I perform the imputation in each fold of RandomSearchCV to avoid data leakage? if so, how can I do it? I saw <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow noreferrer"">Sklearn pipelines</a>, but I cannot figure out how to apply the mode imputation only to the specific feature I need</p>
<p>Thank you in advance!</p>
<p>P.s. feedback on all the steps are welcome too if anything is wrong!</p>
","0","Question"
"78491587","","<p>I'm trying to use a UNet in Pytorch to extract prediction masks from multidimensional (8 band) satellite images. I'm having trouble getting the prediction masks to look somewhat expected/coherent. I'm not sure if the issue is the way my training data is formatted, my training code, or the code I'm using to make predictions. My suspicion is that it is the way my training data is being fed to the model. I have 8 band satellite images and single band masks with values ranging 0-n number of classes with 0 being background and 1-n being target labels like this:</p>
<p><a href=""https://i.sstatic.net/BHbMgmvz.png?"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHbMgmvz.png?"" alt=""enter image description here"" /></a></p>
<p>With the image shape being (8, 512, 512) and the mask shape being (512, 512) in the case of the single channel example, (512, 512, 8) in the OHE case, and (512, 512, 3) in the stacked case.</p>
<p>Some masks may contain all class labels, some may only have a couple or be background labels only. I've tried using these single channel masks, I've also converted them into 3 channel masks with the first channel being all the labels for a given image, and I've also tried one hot encoding them such that each mask is 0-n dimensions and each channel a different label with binary 0-1 for background/target.</p>
<p><strong>EDIT</strong>
After changing the softmax <code>dim=2</code>, the outputs started looking a little better. However, it appears the model is not learning at all after the first few warmup epochs as the training loss decreases initially but then immediately plateaus or increases and the prediction masks stop making sense (either all black or random blobs). I suspect there is an issue with my training pipeline (below) or possibly due to the class imbalance with class 0 (background).</p>
<pre class=""lang-py prettyprint-override""><code>import os
import torch
import numpy as np
from skimage import io
from tqdm import tqdm
import torch.nn as nn
import torch.optim as optim
import segmentation_models_pytorch as smp

image_dir = r'test_segmentation\images'
mask_dir = r'test_segmentation\masks'

data_dir=r'unet_training'
os.makedirs(data_dir, exist_ok=True)

model_dir = os.path.join(data_dir, 'models')
os.makedirs(model_dir, exist_ok=True)

pred_dir = os.path.join(data_dir, 'predictions')
os.makedirs(pred_dir, exist_ok=True)

num_bands = 8
num_classes = 9
epochs = 10
learning_rate = 0.001
weight_decay = 0
encoder = 'resnet50'
encoder_weights = 'imagenet'

model = smp.Unet(in_channels=num_bands, encoder_name=encoder, encoder_weights=encoder_weights, classes=num_classes).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
loss_function = nn.CrossEntropyLoss() if num_classes &gt; 1 else nn.BCEWithLogitsLoss()

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

for epoch in range(1, epochs + 1):
    train_loss = 0
    val_loss = 0 

    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f&quot;Epoch {epoch} Training&quot;)
    
    model.train()
    
    for batch_idx, (data, targets) in train_loop:
        optimizer.zero_grad()
        
        data = data.float().to(device)
        targets = targets.long().to(device)
        predictions = model(data)
        loss = loss_function(predictions, targets)
        
        train_loss += loss.item()

        loss.backward()
        optimizer.step()
    
        train_loop.set_postfix(loss=train_loss)

    val_loop = tqdm(enumerate(val_loader), total=len(val_loader), desc=f&quot;Epoch {epoch} Validation&quot;)

    model.eval()
    
    for batch_idx, (data, targets) in val_loop:
        data, targets = data.to(device).float(), targets.to(device).long()
        preds = model(data)

        val_loss = loss_function(preds, targets).item()

        softmax = torch.nn.Softmax(dim=2)
        preds = torch.argmax(softmax(preds), dim=1).cpu().numpy()
        preds = np.array(preds[0, :, :], dtype=np.uint8)
        labels = np.array(targets.cpu().numpy()[0, :, :], dtype=np.uint8)

        #save prediction and label mask
        pred_path = os.path.join(pred_dir, f&quot;{epoch}_{batch_idx}_pred.png&quot;)
        label_path = os.path.join(pred_dir, f&quot;{epoch}_{batch_idx}_label.png&quot;)
        io.imsave(pred_path, preds)
        io.imsave(label_path, labels)

        val_loop.set_postfix(loss=val_loss)
    
    avg_train_loss = train_loss / (batch_idx + 1)
    avg_val_loss = val_loss/ (batch_idx + 1)

    print(f&quot;\nEpoch {epoch} Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}&quot;)

    checkpoint_name = os.path.join(model_dir, f&quot;{modeltype}_bands{num_bands}_classes{num_classes}_{encoder}_{learning_rate}_{epoch}.pt&quot;)
    
    if epoch == 1:
        torch.save(model.state_dict(), checkpoint_name)
    elif epoch % 10 == 0:
        torch.save(model.state_dict(), checkpoint_name)
    elif epoch == epochs:
        torch.save(model.state_dict(), checkpoint_name)
    else:
        pass
</code></pre>
","0","Question"
"78493742","","<p>i have been encountering this error while trying to run a machine running model, the model is supposed to power a driver drowsiness detection project</p>
<pre><code>W0000 00:00:1715924294.765512    2256 inference_feedback_manager.cc:114] 
Feedback manager requires a model with a single signature inference. 
Disabling support for feedback tensors.
</code></pre>
<p>the model architecture is as follows:</p>
<pre><code>#**MODEL**
from keras.layers import BatchNormalization
model = tf.keras.models.Sequential()
# the input shape is the desired size of the image 145 x 145 with 3 bytes color

#This is the first convolution
   model.add(Conv2D(16, 3, activation='relu', input_shape=X_train.shape[1:]))
   model.add(BatchNormalization())
   model.add(MaxPooling2D())
   tf.keras.layers.Dropout(0.3)

# The second convolution
   model.add(Conv2D(32, 5, activation='relu'))
   model.add(BatchNormalization())
   model.add(MaxPooling2D())
   tf.keras.layers.Dropout(0.3)

# The third convolution
  model.add(Conv2D(64, 10, activation='relu'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())
  tf.keras.layers.Dropout(0.3)

# The fourth convolution
  model.add(Conv2D(128, 12, activation='relu'))
  model.add(BatchNormalization())

# Flatten the results to feed into a DNN
  model.add(Flatten())
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.25))
  model.add(Dense(64, activation='relu'))
# Only 1 output neuron.
  model.add(Dense(1, activation='sigmoid'))

  model.compile(loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;], optimizer=Adam(lr=0.001))
  history = model.fit(train_generator, epochs=10, batch_size=32, validation_data=test_generator)

# Defining the serving signature
  input_signature = [
      tf.TensorSpec(shape=[None, 145, 145, 3], dtype=tf.float32, name='input_tensor')
  ]

@tf.function(input_signature=input_signature)
def serving_fn(inputs):
    return model(inputs)

export_dir = 'E:\system project\project'  
tf.saved_model.save(serving_fn, export_dir)

# Loading the model for inference
loaded_model = tf.saved_model.load('my_model.keras')
</code></pre>
","5","Question"
"78493918","","<p>My model accuracy is pretty bad. This data is taken from <a href=""https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original</a> which shows logistics regression model accuracy of 96%, so really the problem is in my model. I have build the following model:</p>
<pre class=""lang-r prettyprint-override""><code># Importing dataset 
tumor_study &lt;- read.csv(&quot;breast-cancer-wisconsin.data&quot;, header = FALSE, na.strings = &quot;NA&quot;)

# Adding column names
features &lt;- c(&quot;id_number&quot;, &quot;ClumpThickness&quot;, &quot;Uniformity_CellSize&quot;,
              &quot;Uniformity_CellShape&quot;, &quot;MarginalAdhesion&quot;,
              &quot;SingleEpithelial_CellSize&quot;, &quot;BareNuclei&quot;, &quot;Bland_Chromatin&quot;,
              &quot;Normal_Nucleoli&quot;, &quot;Mitoses&quot;, &quot;Class&quot;)

colnames(tumor_study) &lt;- features 

# Cleaning data
# Remove the 1st column (id_number)
tumor_study &lt;- tumor_study[,-1]

# Convert &quot;?&quot; to NA in BareNuclei column and then to numeric
tumor_study$BareNuclei[tumor_study$BareNuclei == &quot;?&quot;] &lt;- NA
tumor_study$BareNuclei &lt;- as.numeric(tumor_study$BareNuclei)

# Remove rows with missing values in BareNuclei
tumor_study &lt;- tumor_study[!is.na(tumor_study$BareNuclei),]

# Convert Class to a factor
tumor_study$Class &lt;- factor(tumor_study$Class, levels = c(2, 4), labels = c(&quot;Benign&quot;, &quot;Malignant&quot;))

# Splitting the dataset into training and test sets
library(caTools)
set.seed(123)
split &lt;- sample.split(tumor_study$Class, SplitRatio = 0.8)
training_set &lt;- tumor_study[split == TRUE,]
test_set &lt;- tumor_study[split == FALSE,]

# Applying feature scaling
training_set[, 1:9] &lt;- scale(training_set[, 1:9])
test_set[, 1:9] &lt;- scale(test_set[, 1:9])

# Building the logistic regression model
classifier &lt;- glm(formula = Class ~ ., family = binomial, data = training_set)

# Predicting probabilities for the training set
prob_y_train &lt;- predict(classifier, type = 'response', newdata = training_set[,-10])
predicted_y_training &lt;- ifelse(prob_y_train &gt;= 0.5, &quot;Benign&quot;, &quot;Malignant&quot;)

# prediction using test_set
prob_y_test &lt;- predict(classifier, type = 'response', newdata = test_set[,-10])
predicted_y_test &lt;- ifelse(prob_y_test &gt;= 0.5, &quot;Benign&quot;, &quot;Malignant&quot;)

# Checking the accuracy with confusion matrix
cm_test &lt;- table(test_set[,10], predicted_y_test)
print(cm_test)
</code></pre>
<p>But my accuracy is close to 2%.</p>
<p>How can I figure out the problem in my model?</p>
","-1","Question"
"78496983","","<pre><code>def make_prediction(x0,t0):
    inputs = torch.vstack([x0,t0])
    layer_1 = torch.matmul(w0,inputs)
    return layer_1

loss1 = nn.MSELoss()
def loss_function():
            u_t=(make_prediction(x,t+inf_s)-make_prediction(x,t))/inf_s
            u_x=(make_prediction(x+inf_s,t)-make_prediction(x,t))/inf_s
            u_xx=(make_prediction(x+inf_s,t)-2*make_prediction(x,t)+make_prediction(x-inf_s,t))/inf_s**2
            return (1/N_i)*(loss1(make_prediction(x0IC,t0IC), u0IC))+(1/N_b)*(loss1(make_prediction(x0BC1,t0BC1), u0BC1))
            +(1/N_b)*(loss1(make_prediction(x0BC2,t0BC2), u0BC2))+(1/N_f)*(np.pi/0.01)*(loss1(u_xx-u_t-make_prediction(x,t)*u_x, 0))

def train_step(w,b, learning_rate):
    trainable_variables = [w,b]
    optimizer = torch.optim.SGD(trainable_variables, lr=learning_rate,momentum=0.9)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.01)
    loss = loss_function()
    loss.backward()
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        w.grad.zero_()
        b.grad.zero_()
    optimizer.step()
    scheduler.step()
train_step(w,bias,learning_rate)
</code></pre>
<p>I run this code (by scheduler.ExponentialLR), but there is no change in the learning rate.
Where do you think the problem comes from?
I write the full code...thanks from your help</p>
","1","Question"
"78497575","","<p>I am training the model using the Yolov3 model with the data set I received from Kaggle. Model training is completed and I add the new weights to the backup folder. I run one of the fruits I trained with for testing, but object detection does not occur. The same image appears as Prediction.jpg. The training seems to be decent, but I don't understand why it can't detect objects. Please help me.</p>
<p>Train Terminal Code:</p>
<pre><code>./darknet detector train /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights
</code></pre>
<p>Test Terminal Code:</p>
<pre><code>./darknet detector test /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out predictions.jpg
</code></pre>
<p>I set up and edited obj.data and obj.names and yolov3.cfg files.</p>
<p>I have 3 classes: apple, banana and orange. I have properly set the values ​​such as filter and class values ​​in the cfg file according to the 3 classes.</p>
<pre><code>cfg file 
[net]
# Testing
batch=64
subdivisions=1
# Training
subdivisions=16
width= 608
height=608
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=0.3

learning_rate=0.001
burn_in=1000
max_batches = 6000 # classnum * 2000
policy=steps
steps=3600,4800 # max_batches num %80, %90 
scales=.1,.1
</code></pre>
<p>In addition to the .jpg images in the data set, there are .txt files with the same name in yolo format.</p>
<p>File image:
<a href=""https://i.sstatic.net/Ed1BBcZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ed1BBcZP.png"" alt=""file image"" /></a></p>
<p>The train.txt and test.txt files containing the paths to all the images are also ready.</p>
<p>When I run the test command in the terminal, it works, but the picture looks the same, there are no bounding boxes that detect objects. I'm sure I have Opencv installed. I am using macOS. Why is it not detecting it? Someone please help. I cleaned the darknet many times by saying make clean and ran it by saying make opencv = 1, but the result does not change.</p>
<pre><code>[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
Total BFLOPS 137.613 
avg_outputs = 1052318 
Loading weights from /Users/melisabagcivan/darknet/backup/yolov3_final.weights...
 seen 64, trained: 32013 K-images (500 Kilo-batches_64) 
Done! Loaded 107 layers from weights-file 
Enter Image Path: /Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 Detection layer: 82 - type = 28 
 Detection layer: 94 - type = 28 
 Detection layer: 106 - type = 28 
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg: Predicted in 6738.129000 milli-seconds.
</code></pre>
<p>I tried it with many images and it doesn't draw the box in any of them. I don't understand whether it can't detect it or whether I'm making a mistake while testing it.</p>
","1","Question"
"78497836","","<p>I successfully launched a training job in sagemaker. However, when I try to use the model to run inference, sagemaker is unable to find the model.</p>
<pre><code>import sagemaker
from sagemaker.transformer import Transformer
from sagemaker.model import Model

# Set session parameters
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

# Input paths
model_s3_path = 's3://sagemaker-us-west-1-6584743930/pytorch-training-2024-05-16-15-18-34-042/source/sourcedir.tar.gz'
input_s3_path = 's3://some-bucket/inference/beauty_annotations_simple_transformer.csv' 
output_s3_path = 's3://some-bucket/inference/ouput/'

# Define instance type
instance_type = 'ml.m5.large'

# Model
model = Model(
    model_data=model_s3_path,
    role=role,
    framework_version='2.0',
    entry_point='inference.py',
    source_dir='./source_dir'
)

# Transformer
transformer = Transformer(
    model_name=model.name,
    instance_count=1,
    instance_type=instance_type,
    output_path=output_s3_path,
    assemble_with='Line',  # method of output assembled
    accept='application/jsonlines',  # output format
)

# Launch batch transform job
transformer.transform(
    data=input_s3_path,
    content_type='text/csv',  # input format
    split_type='Line',  # output split method
)

# Job wait
transformer.wait()
</code></pre>
<p><strong>Error</strong></p>
<blockquote>
<p>ValueError: Failed to fetch model information for
pytorch-training-2024-05-16-15-18-34-042. Please ensure that the model
exists. Local instance types require locally created models.</p>
</blockquote>
","0","Question"
"78497891","","<p>If I cleanse the data and impute median value into NaN values, am I supposed to somehow incorporate this into my model that will be used on the test data?  In other words, doesn't my test data need to be cleansed and imputed as well, or will the training take care of this?</p>
<p>I want to say it needs to be incorporated, because otherwise the NaN values break the model, plus any skewness wouldn't have been addressed.</p>
<p>In particular:</p>
<p>Replace NaN with median:</p>
<pre><code>data = data.fillna(data.median())
</code></pre>
<p>Deal with skewness using Quantile Transformation to follow a normal distribution for each feature (the following is just for one).</p>
<pre><code>qualtile_transformer = QuantileTransformer(output_distribution='normal', random_state=0')
data['feat_0'] = quantile_transformer.fit_transform(data['feat_0'].values.reshape(-1,1)).flatten()
</code></pre>
<p>Model:</p>
<pre><code>from sklearn.linear_model import LinearRegression
linear_regr = LinearRegression()
linear_regr.fit(Xtrain,Ytrain)
</code></pre>
<p>Prediction:</p>
<pre><code># make prediction using the testing set
Ypred = linear_regr.predict(Xtest)
</code></pre>
<p>So, ultimately, if I were to take my model and use it on a similar but different data, how can I be sure it doesn't fail and the NaNs and Quantile transformation will be taken care of with the new data before the prediction is implemented so it won't fail?</p>
","-2","Question"
"78501325","","<p>I wrote a code that will fine-tine my QnA model using huggingface. The code will import questions and answers from a json file and then fine tune a HF Question-Answer model with the data from the json file. However, things went awry from then onwards, when I get this traceback:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Philip Chen\OneDrive\Documents\ML\machineLearning.py&quot;, line 45, in &lt;module&gt;
    trainer.train()
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py&quot;, line 1859, in train
    return inner_training_loop(
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py&quot;, line 2165, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\accelerate\data_loader.py&quot;, line 454, in __iter__
    current_batch = next(dataloader_iter)
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py&quot;, line 631, in __next__
    data = self._next_data()
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py&quot;, line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 51, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\dataset_dict.py&quot;, line 81, in __getitem__
    raise KeyError(
KeyError: &quot;Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']&quot;
</code></pre>
<p>from the offending line:</p>
<pre><code>trainer.train()
</code></pre>
<p>And this is my entire code:</p>
<pre><code># https://www.mlexpert.io/blog/alpaca-fine-tuning
# https://wellsr.com/python/fine-tuning-huggingface-models-in-tensorflow-keras/
# https://learnopencv.com/fine-tuning-bert/
# https://medium.com/@karary/nlp-fine-tune-question-answering-model-%E5%AF%A6%E4%BD%9C-3-model-training-%E5%84%B2%E5%AD%98%E8%88%87-inference-13d2a5bf5c32
import transformers as tf
import datasets as ds
import pandas as pd
import numpy as np
import torch
import json
 
############## Check if CUDA is enabled. ################
hasCUDA=torch.cuda.is_available()
print(f&quot;CUDA Enabled? {hasCUDA}&quot;)
device=&quot;cuda&quot; if hasCUDA else &quot;cpu&quot;      
 
############## Loading file and populating data ################
fileName=&quot;qna.json&quot;
trainDS=ds.load_dataset(&quot;json&quot;, data_files=fileName)
evalDS=ds.load_dataset(&quot;json&quot;, data_files=fileName)
# rawDS=ds.load_dataset('squad')
############## Model ##########################################
modelName=&quot;./distilbert-base-cased&quot;     #or replace the model name with whatever you feel like.
config=tf.AutoConfig.from_pretrained(modelName+&quot;/config.json&quot;)
model=tf.AutoModelForQuestionAnswering.from_pretrained(modelName,config=config)
tokenizer=tf.AutoTokenizer.from_pretrained(modelName)
############## Training #######################################
trnArgs=tf.TrainingArguments(
    output_dir=&quot;./&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=3,
    remove_unused_columns=False,
    fp16=True
)
 
trainer=tf.Trainer(
    model=model,
    args=trnArgs,
    train_dataset=trainDS,
    eval_dataset=evalDS,
    tokenizer=tokenizer
)
trainer.train()
</code></pre>
<p>This is the JSON file that contains the questions:</p>
<pre><code>{&quot;text&quot;: &quot;Who wrote Charlie and the Chocolate Factory?&quot;, &quot;label&quot;: &quot;Roald Dahl&quot;}
{&quot;text&quot;: &quot;Name a few ways to treat constipation naturally.&quot;, &quot;label&quot;: &quot;Exercise regularly, eat more fibers, and drink more water.&quot;}
{&quot;text&quot;: &quot;Where is the longest roller coaster located?&quot;, &quot;label&quot;: &quot;Nagashima, Japan. The name of the coaster is Steel Dragon 2000.&quot;}
{&quot;text&quot;: &quot;What are the 11 herbs and spices that Colonel Sanders used in KFC?&quot;, &quot;label&quot;: &quot;Nobody knows, as it's a secret.&quot;}
{&quot;text&quot;: &quot;Who wrote Les Miserables?&quot;, &quot;label&quot;: &quot;Victor Hugo&quot;}
{&quot;text&quot;: &quot;What is the Watergate Scandal?&quot;, &quot;label&quot;: &quot;The Watergate scandal was a significant political controversy in the United States during the presidency of Richard Nixon from 1972 to 1974, ultimately resulting in Nixon's resignation. It originated from attempts by the Nixon administration to conceal its involvement in the June 17, 1972, break-in at the Democratic National Committee headquarters located in the Watergate Office Building in Washington, D.C.&quot;}
{&quot;text&quot;: &quot;What is Obama's most famous quote?&quot;, &quot;label&quot;: &quot;'Yes we can!'&quot;}
</code></pre>
<p>Any suggestions on how I can fix this thing?</p>
","1","Question"
"78506114","","<p>I am training a SpaCy pipeline with <code>['transformer', 'ner']</code> components, ner trains well, but transformer is stuck on 0 loss, and, I am assuming, is not training.</p>
<p>Here is my config:</p>
<pre class=""lang-ini prettyprint-override""><code>[paths]
vectors = &quot;en_core_web_trf&quot;
init_tok2vec = null
train = &quot;/home/sxdadmin/spacy/input/train.spacy&quot;
dev = &quot;/home/sxdadmin/spacy/input/dev.spacy&quot;

[system]
gpu_allocator = &quot;pytorch&quot;
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;transformer&quot;, &quot;ner&quot;]
batch_size = 512
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}
vectors = {&quot;@vectors&quot;:&quot;spacy.Vectors.v1&quot;}

######################################################################
[components]
######################################################################

[components.transformer]
factory = &quot;transformer&quot;
max_batch_items = 4096

[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v1&quot;
name = &quot;bert-base-cased&quot;
tokenizer_config = {&quot;use_fast&quot;: true}

[components.transformer.model.get_spans]
@span_getters = &quot;spacy-transformers.doc_spans.v1&quot;

[components.transformer.set_extra_annotations]
@annotation_setters = &quot;spacy-transformers.null_annotation_setter.v1&quot;

######################################################################

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

######################################################################
[corpora]
######################################################################

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 3000
gold_preproc = false
limit = 0
augmenter = null

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 3000
gold_preproc = false
limit = 0
augmenter = null

######################################################################
[training]
######################################################################

dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = 0
gpu_allocator = &quot;pytorch&quot;
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null

######################################################################

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 64
stop = 512
compound = 1.001
t = 0.0

######################################################################

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

######################################################################
[pretraining]
######################################################################

[initialize]
vectors = &quot;en_core_web_lg&quot;
init_tok2vec = null
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]
[initialize.components.transformer]
[initialize.tokenizer]
</code></pre>
<p>and the output:</p>
<p><a href=""https://i.sstatic.net/MWlXbDpB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MWlXbDpB.jpg"" alt=""enter image description here"" /></a></p>
<p>All warnings are met, the famous Bert's max_length of 512 tokens is achieved by text segmentation. Data was previously tested on <code>[tok2vec, ner]</code> setup.</p>
<p>Please help.</p>
","0","Question"
"78507382","","<p>I'm new to machine learning so I have been playing around with some of the models trying to get a better understanding.</p>
<p>When I create a matrix of features:</p>
<pre><code>X_Poly3 (X_Poly3 = PolynomialFeatures(3))
</code></pre>
<p>where X was a matrix with 2 columns, X_Poly3 was generated containing 10 columns:</p>
<p>X1, X2, X1^2, X1.X2, X2^2, X1^3, X1^2.X2, X2^2.X1, X2^3 plus a &quot;bias&quot; column of 1's.</p>
<p>When I fit LinearRegression() to this matrix, I then end up with 10 coefficients PLUS a y-intercept variable.</p>
<p>I thought that the bias column of 1's would act as a multiplier to create the y-intercept but if LinearRegression creates the y-intercept as standard, is the bias column required?</p>
<p>I created a polynomial linear regression model but I ended up with what looks to be 2 variables related to the y-intercept.</p>
<pre><code>import numpy as np
from sklearn.preprocessing import PolynomialFeatures

X = np.arange(6).reshape(3, 2)

poly = PolynomialFeatures(3)
X_Poly3 = poly.fit_transform(X)

from sklearn.linear_model import LinearRegression
y_train = np.arange(3).reshape(3, 1)

regressor = LinearRegression()
regressor.fit(X_Poly3, y_train)

print(regressor.intercept_)
print(regressor.coef_)
</code></pre>
","0","Question"
"78513688","","<p>I am trying to understand the difference between discriminative models and generative models. One of the helpful answers at Stack Overflow is here: <a href=""https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm/879591#879591"">What is the difference between a generative and a discriminative algorithm?</a></p>
<p>In the top answer (see the above link), there was a simple example where there are only four data points of the form <code>(x,y)</code>. The author of the answer said the following: The distribution <code>p(y|x)</code> is the natural distribution for classifying a given example <code>x</code> into a class <code>y</code>, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model <code>p(x,y)</code>, which can be transformed into <code>p(y|x)</code> by applying Bayes rule and then used for classification. However, the distribution <code>p(x,y)</code> can also be used for other purposes. For example, you could use <code>p(x,y)</code> to <em>generate</em> likely <code>(x,y)</code> pairs.</p>
<p>I don't quite understand how one could use <code>p(x,y)</code> to <em>generate</em> likely <code>(x,y)</code> pairs. I would be interested to see an example of <code>(x,y)</code> pair that is generated by using the joint probability distribution <code>p(x,y)</code>? Also, why can the conditional probability distribution <code>p(y|x)</code> not be used to generate new pairs?</p>
","0","Question"
"78514097","","<p>I'm working on a project where I need to model a non-linear relationship using a neural network. The relationship is ( y = 3x_1^2x_2^3 ). The network setup is as follows:</p>
<ul>
<li><strong>Preprocessing:</strong> Natural logarithm of inputs</li>
<li><strong>Network Design:</strong> Single layer with one neuron</li>
<li><strong>Activation Function:</strong> Exponential</li>
<li><strong>Loss Function:</strong> MAE (Mean Absolute Error)</li>
<li><strong>Optimizer:</strong> Adam</li>
<li><strong>Epochs:</strong> 50</li>
<li><strong>Batch Size:</strong> 32</li>
</ul>
<p><strong>Input and Expected Output:</strong></p>
<ul>
<li>Input: ([x1, x2])</li>
<li>Correct weights: ([2, 3])</li>
<li>Correct bias: (\ln 3)</li>
</ul>
<p>Despite these settings, I am not able to achieve 100% accuracy. I've tried initializing weights and biases randomly as well as with specific values.</p>
<p>Here is the code:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Generate data
x1 = np.random.randint(1, 21, size=(1000, 1))
x2 = np.random.randint(1, 21, size=(1000, 1))
y = 3 * (x1 ** 2) * (x2 ** 3)

# Preprocess data
log_x1 = np.log(x1)
log_x2 = np.log(x2)
log_inputs = np.hstack((log_x1, log_x2))

# Define model
model = Sequential()
model.add(Dense(1, input_dim=2, activation='exponential', kernel_initializer='ones', bias_initializer='zeros'))

# Compile model
model.compile(optimizer=Adam(learning_rate=0.01), loss='mae')

# Train model
model.fit(log_inputs, np.log(y), epochs=50, batch_size=32)

# Evaluate model
test_x1 = np.array([[2], [4], [5]])
test_x2 = np.array([[3], [7], [19]])
test_inputs = np.hstack((np.log(test_x1), np.log(test_x2)))
predicted = model.predict(test_inputs)
print(np.exp(predicted))
</code></pre>
<p>Does anyone have suggestions on how to improve the accuracy of this model?</p>
","0","Question"
"78518695","","<p>in machine learning, all models have the equation of accuracy while in the FastText model, we don't have please support.</p>
","-2","Question"
"78521104","","<p>I am trying to run hyperparameter sweeps using Weights and Biases (W&amp;B) and would like to leverage multiprocessing to parallelize my experiments as much as possible. I want to ensure that each set of hyperparameters is evaluated only once and that I can run multiple experiments simultaneously without repeating hyperparameters.</p>
<p>Here is a simplified version of my training and evaluation script:</p>
<pre class=""lang-py prettyprint-override""><code>import random
import numpy as np
import wandb
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Tuple, Dict

def train_one_epoch(epoch: int, lr: float, bs: int) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;
    Simulate training for one epoch.
    &quot;&quot;&quot;
    acc = 0.25 + ((epoch / 30) + (random.random() / 10))
    loss = 0.2 + (1 - ((epoch - 1) / 10 + random.random() / 5))
    return acc, loss

def evaluate_one_epoch(epoch: int) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;
    Simulate evaluation for one epoch.
    &quot;&quot;&quot;
    acc = 0.1 + ((epoch / 20) + (random.random() / 10))
    loss = 0.25 + (1 - ((epoch - 1) / 10 + random.random() / 6))
    return acc, loss

config: Dict[str, float] = {&quot;lr&quot;: 0.0001, &quot;bs&quot;: 16, &quot;epochs&quot;: 5}

def run_epoch(epoch: int, lr: float, bs: int) -&gt; Tuple[int, float, float, float, float]:
    &quot;&quot;&quot;
    Run training and evaluation for one epoch.
    &quot;&quot;&quot;
    train_acc, train_loss = train_one_epoch(epoch, lr, bs)
    val_acc, val_loss = evaluate_one_epoch(epoch)
    return epoch, train_acc, train_loss, val_acc, val_loss

def main() -&gt; None:
    &quot;&quot;&quot;
    Main function to run the training and evaluation in parallel using multiprocessing.
    &quot;&quot;&quot;
    lr = config[&quot;lr&quot;]
    bs = config[&quot;bs&quot;]
    epochs = config[&quot;epochs&quot;]

    # Initialize Weights and Biases
    wandb.init(project=&quot;my_project&quot;, config=config)

    with ProcessPoolExecutor() as executor:
        futures = [executor.submit(run_epoch, epoch, lr, bs) for epoch in np.arange(1, epochs)]
        for future in as_completed(futures):
            epoch, train_acc, train_loss, val_acc, val_loss = future.result()
            wandb.log({
                &quot;epoch&quot;: epoch,
                &quot;train_acc&quot;: train_acc,
                &quot;train_loss&quot;: train_loss,
                &quot;val_acc&quot;: val_acc,
                &quot;val_loss&quot;: val_loss
            })
            print(f&quot;epoch: {epoch}&quot;)
            print(f&quot;training accuracy: {train_acc}, training loss: {train_loss}&quot;)
            print(f&quot;validation accuracy: {val_acc}, validation loss: {val_loss}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h3>Questions:</h3>
<ol>
<li><strong>Role of <code>counts</code> in W&amp;B multiprocessing</strong>: How do I properly configure the <code>counts</code> parameter (if it exists) or any other relevant parameter in W&amp;B to maximize parallelization without repeating hyperparameters?</li>
<li><strong>Ensuring unique hyperparameters</strong>: How can I ensure that each process picks a unique set of hyperparameters when running the sweep? Should I handle this in my Python code or is there a feature in W&amp;B that manages this?</li>
<li><strong>Parallelization best practices</strong>: Are there best practices for implementing parallelization with W&amp;B to get the most efficient use of resources? For example, is there a preferred way to structure the <code>ProcessPoolExecutor</code> or any other method to maximize parallel runs?</li>
</ol>
<p>I am interested in both</p>
<ol>
<li>parallelizability with only <strong>CPU</strong> multi core option</li>
<li>parallelizability with multiple <strong>GPU</strong>  option</li>
</ol>
<p>Any insights, code examples, or references to the W&amp;B documentation would be greatly appreciated!</p>
<hr />
<p>Extension and comments to how to do it in the CLI/Bash option are also welcomed!</p>
<hr />
<p>Refs:</p>
<ul>
<li>mp with wandb SO: <a href=""https://stackoverflow.com/questions/75100914/can-we-use-torch-multiprocessing-spawn-with-wandb-sweep-hyper-parameter-tuning"">Can we use torch.multiprocessing.spawn with wandb sweep hyper-parameter tuning?</a></li>
<li>multithreading support for sweeps: <a href=""https://community.wandb.ai/t/multithreading-support-for-sweeps/5212"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/multithreading-support-for-sweeps/5212</a></li>
<li>examples of sweeps (from ultimate utils): <a href=""https://github.com/brando90/ultimate-utils/tree/master/tutorials_for_myself/my_wandb_uu/my_wandb_sweeps_uu"" rel=""nofollow noreferrer"">https://github.com/brando90/ultimate-utils/tree/master/tutorials_for_myself/my_wandb_uu/my_wandb_sweeps_uu</a></li>
<li>hf + wandb + distributed training: <a href=""https://stackoverflow.com/questions/76585219/what-is-the-official-way-to-run-a-wandb-sweep-with-hugging-face-hf-transformer?noredirect=1&amp;lq=1"">What is the official way to run a wandb sweep with hugging face (HF) transformers so that all the HF features work e.g. distributed training?</a></li>
<li>cross posted: <a href=""https://community.wandb.ai/t/multiprocessing-mp-wandb-sweeps-and-the-count-parameter-how-to-do-sweeps-with-mp/6763"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/multiprocessing-mp-wandb-sweeps-and-the-count-parameter-how-to-do-sweeps-with-mp/6763</a></li>
</ul>
","1","Question"
"78522177","","<p>This question has recently popped up in my mind. I've asked GPT and a couple of other models about the importance of bias term in convolutional networks. All of them responded differently and very superficially. I also occasionally see kaggle notebooks, where people set 'bias=False' or 'bias=True' in conv / dense layers, when train their models. Can you share insights about why bias term might be important and when to consider enabling / disabling it? Thanks.</p>
","0","Question"
"78523154","","<p>I want to rum mpnn with a lightning trainer on my mac.
These are my trainer settings:</p>
<pre><code>trainer = pl.Trainer(
    logger=False,
    enable_checkpointing=True, #
    enable_progress_bar=True,
    accelerator=&quot;mps&quot;,
    devices= 1,
    max_epochs=20, # number of epochs to train for
)
</code></pre>
<p>I also changed my environment varibale as followed:</p>
<pre><code>import os
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
</code></pre>
<p>still when I start my training:</p>
<pre><code>
trainer.fit(mpnn, train_loader)
</code></pre>
<p>I get following error message:</p>
<p>NotImplementedError: The operator 'aten::scatter_reduce.two_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on <a href=""https://github.com/pytorch/pytorch/issues/77764"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/issues/77764</a>. As a temporary fix, you can set the environment variable <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code> to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...</p>
<p>I tried following steps to fix the problem:</p>
<p>changed setup in trainer from accelerator ='auto', to accellerator = 'mps'</p>
<p>changed the environment variable as suggested in the error.</p>
<p>tried:</p>
<pre><code>
if torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')
</code></pre>
","1","Question"
"78524575","","<p>I am getting an import Error when trying to import imblearn.over_sampling for RandomOverSampler. I believe the issue is not with my code but with the libraries clashing, I'm not sure though.</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler    #actually scikit-learn
from imblearn.over_sampling import RandomOverSampler
</code></pre>
<p>code that's using StandardScaler and RandomOverSampler:</p>
<pre><code>def scale_dataset(dataframe, oversample=False):
    X = dataframe[dataframe.columns[:-1]].values
    Y = dataframe[dataframe.columns[-1]].values

    scaler = StandardScaler() 
    X = scaler.fit_transform(X) 

    if oversample:
        ros = RandomOverSampler()
        X, Y = ros.fit_resample(X,Y) 
    data = np.hstack((X, np.reshape(Y, (-1, 1))))
    return data, X, Y

print(len(train[train[&quot;class&quot;]==1]))
print(len(train[train[&quot;class&quot;]==0]))

train, X_train, Y_train = scale_dataset(train, True)
</code></pre>
<p>I tried fully importing sklearn, uninstalled and reinstalled scipi and sklearn (as scikit-learn), installing Tensorflow.
I do have numpy, scipy, pandas and other dependent libraries installed.</p>
","3","Question"
"78527509","","<pre><code>def train_model(x_train, y_train, dropout_prob, lr, batch_size, epochs):
    nn_model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(1, activation='sigmoid')
        ])

    nn_model.compile(keras.optimizers.Adam(lr), loss='binary_crossentropy', metrics=['accuracy'])

    history = nn_model.fit(
        x_train,y_train,epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0
    )

    plot_history(history)

    return nn_model, history


least_loss_model = train_model(x_train, y_train, 0.2, 0.005, 128, 100)
predicted = least_loss_model.predict(x_test)
print(predicted)
</code></pre>
<p>This is giving the following attribute error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\~\ai.py&quot;, line 162, in &lt;module&gt;
    predicted = least_loss_model.predict(x_test)
                ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'tuple' object has no attribute 'predict'
</code></pre>
<p>I have already tried <code>predicted = least_loss_model.predict_proba(x_test)</code>.</p>
","-2","Question"
"78527569","","<p>I am developing TensorFlow decision forest regression model for incremental learning, So I have developed the model and have saved the model. When I retrain with new data the error is coming like &quot;ValueError: The model's <code>task</code> attribute (CLASSIFICATION) does not match the <code>task</code> attribute passed to <code>pd_dataframe_to_tf_dataset</code> (REGRESSION).&quot;</p>
<p>I have tried saving it in different format, but still getting the error.</p>
","1","Question"
"78528640","","<p>I have input data as <code>List&lt;Dictionary&lt;string,dynamic&gt;&gt;</code>
I want to load this data as Enumerable in MLContext</p>
<pre><code>var mlContext = new MLContext();

mlContext.Data.LoadFromEnumerable(inputData);
</code></pre>
<p>I also have no way of predefining a strong static class for input data as everytime data columns will change</p>
<p>in dataView i get column schema as 0 when loaded</p>
<p>while transformation I get no column found exception</p>
<p>I have tried creating dynamic strong typed class
Genereated schema defination and loading the data</p>
<p>I wanted to transform column using loaded data</p>
","0","Question"
"78530486","","<p>I'm attempting to implement PPO to beat cartpole-v2, I manage to get it working if I keep things as A2C (That is, without clipped loss and a single epoch), when I use clipped loss and more than one epoch it doesn't learn, have been trying to find the issue in my implementation for about a week but I can't find what's wrong.</p>
<p><a href=""https://github.com/yanis-falaki/RL-Gym/blob/main/cartpole-ppo.ipynb"" rel=""nofollow noreferrer"">Full Code</a></p>
<p>Here is the function responsible for optimizing:</p>
<pre class=""lang-py prettyprint-override""><code>def finish_episode():
    # Calculating losses and performing backprop
    R = 0
    saved_actions = actor.saved_actions
    returns = []
    epsilon = 0.3
    num_epochs = 1 # When num_epochs is greater than one my network won't learn

    for r in actor.rewards[::-1]:
        R = r + 0.99 * R # Gamma is 0.99
        returns.insert(0, R)
    returns = torch.tensor(returns, device=device)
    returns = (returns - returns.mean()) / (returns.std() + eps)

    old_probs, state_values, states, actions = zip(*saved_actions)

    old_probs = torch.stack(old_probs).to(device)
    state_values = torch.stack(state_values).to(device)
    states = torch.stack(states).to(device)
    actions = torch.stack(actions).to(device)

    advantages = returns - state_values.squeeze()

    for epoch in range(num_epochs):

        new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()

        ratios = new_probs / old_probs

        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages

        #actor_loss = -torch.min(surr1, surr2).mean() # When using this (clipped) loss my network won't learn
        actor_loss = -surr1.mean()

        actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)
        actor_optimizer.step()

        if epoch == num_epochs - 1:
            critic_loss = F.smooth_l1_loss(state_values.squeeze(), returns)
            
            critic_optimizer.zero_grad()
            critic_loss.backward(retain_graph=False)
            critic_optimizer.step()

    del actor.rewards[:]
    del actor.saved_actions[:]
</code></pre>
<p>Tried different hyperparameters, using gae as opposed to full monte carlo retuns/advantages, in combing through my code I can't see what's wrong.</p>
","0","Question"
"78530745","","<p>In a RAG project, I am using <code>langchain</code>. When I run the QA chain with query input, this error keep showing up:</p>
<pre class=""lang-none prettyprint-override""><code>----&gt; result = qa_chain({'query': question})
ValueError: Missing some input keys: {'query'}
</code></pre>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Build prompt
template = &quot;&quot;&quot;Given the following context answer the question.
    Context:
    {context}
    ------------------
    Question: {query}
    Answer:&quot;&quot;&quot;

# LLM chain
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={&quot;prompt&quot;: QA_CHAIN_PROMPT}
)

question = &quot;What methodology was used in this research paper?&quot;

result = qa_chain({'query': question})

# Check the result of the query
result[&quot;result&quot;]
# Check the source document from where we 
result[&quot;source_documents&quot;][0]
</code></pre>
","0","Question"
"78531747","","<p><strong># I'm running this python code in my pc windows 10 on PyCharm 2024 version in Virtual enviroment-----: -</strong></p>
<pre><code>`import os
import numpy as np
import librosa
import soundfile as sf
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense, Masking, LSTM, TimeDistributed
from tensorflow.keras.callbacks import ModelCheckpoint

def extract_features(file_path, sample_rate=22050, duration=4):
    try:
        audio, _ = librosa.load(file_path, sr=sample_rate, duration=duration, mono=True)
        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate)
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        return mel_spec
    except Exception as e:
        print(f&quot;Error processing {file_path}: {e}&quot;)
        return None

def load_dataset(directory, sample_rate=22050, duration=4):
    audio_extensions = ('.wav', '.mp3', '.flac', '.aac', '.m4a', '.ogg')
    features = []
    for root, _, files in os.walk(directory):
        for filename in files:
            if filename.lower().endswith(audio_extensions):
                file_path = os.path.join(root, filename)
                print(f&quot;Processing file: {file_path}&quot;)
                mel_spec = extract_features(file_path, sample_rate, duration)
                if mel_spec is not None:
                    features.append(mel_spec)
                else:
                    print(f&quot;Failed to extract features from {file_path}&quot;)
    if len(features) == 0:
        print(&quot;No valid audio files found in the directory.&quot;)
    return features

def pad_sequences(sequences, maxlen=None):
    if maxlen is None:
        maxlen = max(seq.shape[1] for seq in sequences)
    padded sequences = []
    for seq in sequences:
        if seq.shape[1] &lt; maxlen:
            pad_width = maxlen - seq.shape[1]
            padded_seq = np.pad(seq, ((0, 0), (0, pad_width)), mode='constant')
        else:
            padded_seq = seq[:, :maxlen]
        padded_sequences.append(padded_seq)
    return np.array(padded_sequences)

def create_sequence_autoencoder(input_shape):
    input_layer = Input(shape=input_shape)
    masked = Masking(mask_value=0.0)(input_layer)
    encoded = LSTM(128, activation='relu', return_sequences=True)(masked)
    encoded = LSTM(64, activation='relu', return_sequences=False)(encoded)
    repeated = tf.keras.layers.RepeatVector(input_shape[0])(encoded)
    decoded = LSTM(64, activation='relu', return_sequences=True)(repeated)
    decoded = LSTM(128, activation='relu', return_sequences=True)(decoded)
    decoded = TimeDistributed(Dense(input_shape[1], activation='sigmoid'))(decoded)
    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')
    return autoencoder

# Training the model
qari_dataset_directory = r&quot;E:\quran\Hindi\Hindi_Translation_Splitter\pythonProject1\pythonProject1\qari_voice\qari-dataset&quot;  # Adjust the path as needed
X = load_dataset(qari_dataset_directory)

print(&quot;Loaded dataset shape:&quot;, [x.shape for x in X])

if len(X) &gt; 0:
    max_length = max(x.shape[1] for x in X)
    X_padded = pad_sequences(X, maxlen=max_length)
    input_shape = (X_padded.shape[1], X_padded.shape[2])
    autoencoder = create_sequence_autoencoder(input_shape)

    # Save the best model
    while True:
        try:
            checkpoint_path = input(&quot;Enter the path to save the model checkpoint (e.g., qari_autoencoder.keras): &quot;)
            checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')
            autoencoder.fit(X_padded, X_padded, epochs=10, batch_size=16, validation_split=0.2, callbacks=[checkpoint])
            if os.path.exists(checkpoint_path):
                print(f&quot;Model checkpoint saved at: {checkpoint_path}&quot;)
                break
            else:
                raise Exception(&quot;Checkpoint not saved.&quot;)
        except Exception as e:
            print(f&quot;Failed to save the model checkpoint at: {checkpoint_path}, error: {e}&quot;)

# Load the trained model
if os.path.exists(checkpoint_path):
    autoencoder = load_model(checkpoint_path)
    print(&quot;Model loaded successfully.&quot;)
else:
    print(f&quot;Model checkpoint not found at: {checkpoint_path}&quot;)
    exit(1)

def preprocess_audio(file_path, sample_rate=22050, duration=4):
    mel_spec = extract_features(file_path, sample_rate, duration)
    if mel_spec is None:
        raise ValueError(f&quot;Failed to extract features from {file_path}&quot;)
    return mel_spec

def pad_and_reshape(mel_spec, max_length):
    if mel_spec.shape[1] &lt; max_length:
        pad_width = max_length - mel_spec.shape[1]
        mel_spec_padded = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')
    else:
        mel_spec_padded = mel_spec[:, :max_length]
    return np.expand_dims(mel_spec_padded, axis=0)  # Reshape to match model input shape

# Example file to process
audio_file_path = r&quot;E:\quran\Hindi\Hindi_Translation_Splitter\Output\114 MSTR.wav&quot;

# Preprocess the audio
mel_spec = preprocess_audio(audio_file_path)
max_length = autoencoder.input_shape[1]
mel_spec_padded = pad_and_reshape(mel_spec, max_length)

# Predict using the autoencoder
output = autoencoder.predict(mel_spec_padded)

# Reshape and convert the output back to the original shape
output_mel_spec = output[0]

# Convert mel spectrogram back to audio
def mel_spec_to_audio(mel_spec, sample_rate=22050):
    mel_spec = librosa.db_to_power(mel_spec)
    audio = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sample_rate)
    return audio

# Convert the output mel spectrogram back to audio
audio_without_qari_voice = mel_spec_to_audio(output_mel_spec)

# Save the audio without Qari voice
output_audio_path = r&quot;E:\quran\Hindi\Hindi_Translation_Splitter\Output\Without_qari_output.wav&quot;
os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)
sf.write(output_audio_path, audio_without_qari_voice, 22050)
print(f&quot;Processed audio saved at: {output_audio_path}&quot;)`
</code></pre>
<p>My model is not saving after training, what can I do ?, Guide me please ,after completing  10 epoch this code has to save the model, but it doesn't save the model after taking 1 hour of training...........After that I thought to use google colab for this, but I have large dataset containing more than 20000 files , and the size of that dataset is 5gb . so, i can't upload this dataset on google colab `</p>
<pre><code>
I tried to solve this issue by chatgpt4o 
</code></pre>
","1","Question"
"78533567","","<p>In Yolov7, I use this code to test entire folder images:</p>
<pre><code>python test.py --save-txt --data data/coco.yaml --save-conf --conf 0.1 --weights yolov7_20240316best.pt --task test --name 0316conf01
</code></pre>
<p>Now I need to predict test.txt (include all image path) in yolov4.</p>
<p>I tried this command but did not work:</p>
<pre><code>darknet detector test data/obj.data cfg/yolo-obj.cfg backup/yolo-obj_best.weights -thresh 0.9 -dont_show data/test.txt result.txt
</code></pre>
","1","Question"
"78535919","","<p>I trained an LSTM model in tensorflow, and it works fine, but when I save the model and then try to load it from disk, it throws me a ValueError when loading. FYI saving and loading works for other models, here is the code to create the model:</p>
<pre><code>model1 = Sequential()
model1.add(int_vectorize_layer)
model1.add(Embedding(vocab_size, embedding_dim))
model1.add(SpatialDropout1D(drop_lstm))
model1.add(tf.keras.layers.Bidirectional(LSTM(units=32)))
model1.add(Dropout(drop_lstm))
model1.add(Dense(128, activation='relu'))
model1.add(Dense(64, activation='relu'))
model1.add(Dense(32, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])```
</code></pre>
<p>Here is the model's summary:
<a href=""https://i.sstatic.net/2XQCfjM6.png"" rel=""nofollow noreferrer"">model summary</a></p>
<p>saving and loading the model:</p>
<pre><code>#works fine
model1.save(&quot;model.keras&quot;)
#this line throws the error
loaded_model1 = tf.keras.models.load_model(&quot;model.keras&quot;)
</code></pre>
<p>the Error:</p>
<pre><code>ValueError: A total of 1 objects could not be loaded. Example error message for object &lt;LSTMCell name=lstm_cell, built=True&gt;:

Layer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['kernel', 'recurrent_kernel', 'bias']

List of objects that could not be loaded:
[&lt;LSTMCell name=lstm_cell, built=True&gt;]
</code></pre>
<p>Full Error with stack trace:</p>
<pre><code>ValueError                                Traceback (most recent call last)
Cell In[141], line 1
----&gt; 1 loaded_model1 = tf.keras.models.load_model(&quot;model.keras&quot;)

File ~/Desktop/projects/SPARK-STREAMING-PFA/Model/my-env/lib/python3.10/site-packages/keras/src/saving/saving_api.py:176, in load_model(filepath, custom_objects, compile, safe_mode)
    173         is_keras_zip = True
    175 if is_keras_zip:
--&gt; 176     return saving_lib.load_model(
    177         filepath,
    178         custom_objects=custom_objects,
    179         compile=compile,
    180         safe_mode=safe_mode,
    181     )
    182 if str(filepath).endswith((&quot;.h5&quot;, &quot;.hdf5&quot;)):
    183     return legacy_h5_format.load_model_from_hdf5(filepath)

File ~/Desktop/projects/SPARK-STREAMING-PFA/Model/my-env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:152, in load_model(filepath, custom_objects, compile, safe_mode)
    147     raise ValueError(
    148         &quot;Invalid filename: expected a `.keras` extension. &quot;
    149         f&quot;Received: filepath={filepath}&quot;
    150     )
    151 with open(filepath, &quot;rb&quot;) as f:
--&gt; 152     return _load_model_from_fileobj(
    153         f, custom_objects, compile, safe_mode
    154     )

File ~/Desktop/projects/SPARK-STREAMING-PFA/Model/my-env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:207, in _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode)
    204         asset_store.close()
    206     if failed_trackables:
--&gt; 207         _raise_loading_failure(error_msgs)
    208 return model

File ~/Desktop/projects/SPARK-STREAMING-PFA/Model/my-env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:295, in _raise_loading_failure(error_msgs, warn_only)
    293     warnings.warn(msg)
    294 else:
--&gt; 295     raise ValueError(msg)

ValueError: A total of 1 objects could not be loaded. Example error message for object &lt;LSTMCell name=lstm_cell, built=True&gt;:

Layer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['kernel', 'recurrent_kernel', 'bias']

List of objects that could not be loaded:
[&lt;LSTMCell name=lstm_cell, built=True&gt;]
</code></pre>
<p>I'm running the code on python-3.10.12 here are all the library versions:</p>
<pre><code>absl-py==2.1.0
asttokens==2.4.1
astunparse==1.6.3
certifi==2024.2.2
charset-normalizer==3.3.2
click==8.1.7
comm==0.2.2
contourpy==1.2.1
cycler==0.12.1
debugpy==1.8.1
decorator==5.1.1
exceptiongroup==1.2.1
executing==2.0.1
flatbuffers==24.3.25
fonttools==4.51.0
gast==0.5.4
gensim==4.3.2
google-pasta==0.2.0
grpcio==1.62.2
h5py==3.11.0
idna==3.7
ipykernel==6.29.4
ipython==8.23.0
jedi==0.19.1
joblib==1.4.0
jupyter_client==8.6.1
jupyter_core==5.7.2
keras==3.2.1
kiwisolver==1.4.5
libclang==18.1.1
Markdown==3.6
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.8.4
matplotlib-inline==0.1.7
mdurl==0.1.2
ml-dtypes==0.3.2
namex==0.0.8
nest-asyncio==1.6.0
nltk==3.8.1
numpy==1.26.4
opt-einsum==3.3.0
optree==0.11.0
packaging==24.0
pandas==2.2.2
parso==0.8.4
pexpect==4.9.0
pillow==10.3.0
platformdirs==4.2.0
prompt-toolkit==3.0.43
protobuf==4.25.3
psutil==5.9.8
ptyprocess==0.7.0
pure-eval==0.2.2
Pygments==2.17.2
pyparsing==3.1.2
python-dateutil==2.9.0.post0
pytz==2024.1
pyzmq==26.0.2
regex==2024.4.16
requests==2.31.0
rich==13.7.1
scikit-learn==1.4.2
scipy==1.13.0
seaborn==0.13.2
six==1.16.0
smart-open==7.0.4
stack-data==0.6.3
tensorboard==2.16.2
tensorboard-data-server==0.7.2
tensorflow==2.16.1
tensorflow-io-gcs-filesystem==0.36.0
termcolor==2.4.0
threadpoolctl==3.4.0
tornado==6.4
tqdm==4.66.2
traitlets==5.14.3
tweet-preprocessor==0.6.0
typing_extensions==4.11.0
tzdata==2024.1
urllib3==2.2.1
wcwidth==0.2.13
Werkzeug==3.0.2
wordcloud==1.9.3
wrapt==1.16.0
</code></pre>
<p>I've tried saving the model other ways, I've tried saving it in h5 format, but it gives me a deprecated warning and an error, I've tried saving the weights and reloading them to a model with the same architecture, but then the textVectorization layer needs to be adapted to the training data or loaded from another file, which is not an ideal situation, the optimal solution as I see it to make this code work so i can easily load the model from a single file. I think the bidirectionnal layer is at the root of the problem, since that's where the only two LSTM layers in the neural network exist. If you have any ideas on how to solve the problem I'd be willing to try it out. Any and all help is very much appreciated! Thank you!</p>
","0","Question"
"78536168","","<p>I am learning tensorflow.js and this is perhaps the first full program I am trying to write. However I feel something is wrong. My model is not able to predict right Weather and always predicting 'Haze' no matter how I train it. Following is the sample NodeJS program I am running to train and predict. Please help me figure out what is going wrong here:</p>
<pre><code>// Install the necessary packages:
// npm install @tensorflow/tfjs-node
const _ = require('lodash'),
    fs = require('fs'),
    tf = require('@tensorflow/tfjs-node');

var data = require('./data2.json');

// Sample data (replace with your actual data)
const features = [];
const labels = [];
for (var i = 0; i &lt; data.length; i++) {
    features.push([data[i].temp, data[i].wind, data[i].humidity, data[i].pressure, data[i].visibility, yearToDateTime(data[i].date, data[i].time)]);
    labels.push(data[i].weatherType);
}
var uniqLabels = _.uniq(labels);
(async() =&gt; {

    // Create a simple neural network model
    var model = null;
    if (fs.existsSync('./kolkata-weather-model/model.json')) {
        model = await tf.loadLayersModel('file://./kolkata-weather-model/model.json');
    }

    if (model == null) {
        model = tf.sequential();
        model.add(tf.layers.dense({ units: 1, activation: 'sigmoid', inputShape: [6] }));
        model.add(tf.layers.dense({ units: uniqLabels.length, activation: 'sigmoid' }));

        // Compile the model
        model.compile({ optimizer: 'sgd', loss: 'sparseCategoricalCrossentropy', metrics: ['accuracy'] });

        // Train the model
        const xs = tf.tensor2d(features);
        const ys = tf.tensor1d(_.map(labels, (o) =&gt; { return _.indexOf(uniqLabels, o); }));
        model.fit(xs, ys, { epochs: 100 }).then(() =&gt; {
            model.save('file://./kolkata-weather-model');
            // Example prediction for tomorrow's weather
            getTomorrowPrediction(model);
        });
    } else {
        getTomorrowPrediction(model);
    }
})();

function getTomorrowPrediction(model) {
    const tomorrowFeatures = [10, 2, 35, 1008, 30, yearToDateTime('20240101', '15:30')]; // Replace with actual values
    const prediction = model.predict(tf.tensor2d([tomorrowFeatures])).argMax(1).dataSync()[0];
    console.log('Weather prediction for tomorrow:', uniqLabels[prediction]);
}

function yearToDateTime(dt, time) {
    if (!_.isNil(dt) &amp;&amp; dt.length == 8 &amp;&amp; !_.isNil(time) &amp;&amp; time.length == 5) {
        var yr = dt.substring(0, 4);
        var mon = dt.substring(4, 6);
        var day = dt.substring(6);
        var firstDayOfYr = Date.parse(yr + '-01-01 00:00:00.000');
        var todayDate = Date.parse(yr + '-' + mon + '-' + day + ' ' + time + ':00.000');
        return todayDate - firstDayOfYr;
    }
    return -1;
}

function getWeatherTypeName(id) {
    if (id == 1) {
        return 'Sunny';
    } else if (id == 2) {
        return 'Cloudy';
    } else if (id == 3) {
        return 'Rainy';
    } else {
        return 'Unknown';
    }
}

function getWeatherType(weatherType) {
    weatherType = weatherType
        .replace('Cool', 1)
        .replace('Mild', 1)
        .replace('Clear', 1)
        .replace('Fog', 2)
        .replace('Haze', 1)
        .replace('Passing clouds', 2)
        .replace('Partly sunny', 1)
        .replace('Scattered clouds', 2)
        .replace('Drizzle Broken clouds.', 3)
        .replace('Broken clouds', 2)
        .replace('Light rain Partly cloudy.', 3)
        .replace('Rain Partly cloudy.', 3)
        .replace('Light rain Broken clouds.', 3)
        .replace('Partly cloudy', 2)
        .replace('Warm', 1)
        .replace('Thunderstorms Partly cloudy.', 2)
        .replace('Thunderstorms Passing clouds.', 2)
        .replace('Thunderstorms Broken clouds.', 2)
        .replace('Thunderstorms Partly sunny.', 2)
        .replace('Light rain Passing clouds.', 3)
        .replace('Light rain Scattered clouds.', 3)
        .replace('Thunderstorms Scattered clouds.', 2)
        .replace('Thunderstorms More clouds than sun.', 2)
        .replace('More clouds than sun', 2)
        .replace('Light rain More clouds than sun.', 3)
        .replace('Hot', 1)
        .replace('Strong thunderstorms Mostly cloudy.', 2)
        .replace('Thunderstorms Mostly cloudy.', 2)
        .replace('Light rain Mostly cloudy.', 3)
        .replace('Drizzle More clouds than sun.', 3)
        .replace('Drizzle Mostly cloudy.', 3)
        .replace('Rain Broken clouds.', 3)
        .replace('Rain Mostly cloudy.', 3)
        .replace('Mostly cloudy', 2)
        .replace('Rain More clouds than sun.', 3)
        .replace('Thundershowers Broken clouds.', 2)
        .replace('Sunny', 1)
        .replace('', '-1')
        .replace('Smoke', 2)
        .replace('Thundershowers Partly cloudy.', 3)
        .replace('Rain Cloudy.', 3)
        .replace('Heavy rain More clouds than sun.', 3)
        .replace('Heavy rain Mostly cloudy.', 3)
        .replace('Strong thunderstorms More clouds than sun.', 2)
        .replace('Extremely hot', 1)
        .replace('Strong thunderstorms Broken clouds.', 2)
        .replace('Rain Scattered clouds.', 3)
        .replace('Heavy rain Broken clouds.', 3)
        .replace('Light rain Partly sunny.', 3)
        .replace('Light fog', 1)
        .replace('Strong thunderstorms Partly cloudy.', 2)
        .replace('Rain showers More clouds than sun.', 3)
        .replace('Rain Partly sunny.', 3)
        .replace('Strong thunderstorms Cloudy.', 2)
        .replace('Thunderstorms Cloudy.', 2)
        .replace('Rain Passing clouds.', 3)
        .replace('Rain Overcast.', 3)
        .replace('Light rain Overcast.', 3)
        .replace('Heavy rain Cloudy.', 3)
        .replace('Light rain Cloudy.', 3)
        .replace('Hail Mostly cloudy.', 3)
        .replace('Thunderstorms Overcast.', 2)
        .replace('Overcast', 2)
        .replace('Drizzle Cloudy.', 3)
        .replace('Light rain Fog.', 3)
        .replace('Strong thunderstorms Scattered clouds.', 2)
        .replace('Cloudy', 2)
        .replace('Strong thunderstorms Partly sunny.', 2)
        .replace('Strong thunderstorms Passing clouds.', 2);
    return parseInt(weatherType, 10);
}
</code></pre>
<p>My data sample is like this which is used for training (data2.json):</p>
<pre><code>[   {
        &quot;date&quot;: &quot;20140101&quot;,
        &quot;time&quot;: &quot;00:20&quot;,
        &quot;temp&quot;: 14,
        &quot;weatherType&quot;: &quot;Cool&quot;,
        &quot;wind&quot;: null,
        &quot;humidity&quot;: 88,
        &quot;pressure&quot;: 1016,
        &quot;visibility&quot;: 0
    },
    {
        &quot;date&quot;: &quot;20200720&quot;,
        &quot;time&quot;: &quot;21:00&quot;,
        &quot;temp&quot;: 28,
        &quot;weatherType&quot;: &quot;Passing clouds&quot;,
        &quot;wind&quot;: null,
        &quot;humidity&quot;: 94,
        &quot;pressure&quot;: 1003,
        &quot;visibility&quot;: 21
    },
    {
        &quot;date&quot;: &quot;20150801&quot;,
        &quot;time&quot;: &quot;05:50&quot;,
        &quot;temp&quot;: 25,
        &quot;weatherType&quot;: &quot;Heavy rain More clouds than sun.&quot;,
        &quot;wind&quot;: 15,
        &quot;humidity&quot;: 100,
        &quot;pressure&quot;: 995,
        &quot;visibility&quot;: 5
   },
...
]
</code></pre>
<p>After Epoch 41 there is no change in acc and loss while training the model. And I also feel the prediction is remaining same, regardless of what input I provide. I am training the model with 10 years' weather data of Kolkata. I am certainly expecting better prediction than 'Haze' for any input. Can you please help me choose few things correctly so that the model is able to predict results more accurately? - How many dense layers to use? What should be the config (activation, units and shape) of the dense layers? Which optimizer and loss functions to use while compiling the model?</p>
","2","Question"
"78537136","","<p>I need to write SGD-Perceptron for digit recognition according to this guidelines:</p>
<ol>
<li>The selection is <a href=""https://en.wikipedia.org/wiki/MNIST_database"" rel=""nofollow noreferrer"">MNIST database</a> with 60000 training samples.</li>
<li>Get random vector and calculate <code>net</code> (weighted sum) and find <code>y_j = f(net_j)</code>, where <code>f(x)</code> is activation function. Also <code>w_0j</code> are offset weights (bias weights), <code>x0 = 1</code></li>
<li>Find <code>ε = 0.5 * sum((d_j-y_j) ** 2)</code>, where <code>d</code> is desired vector (zero vector with 1 across from desired digit, I think) created from selection (<code>y_train</code>). I also think <code>ε</code> is called loss function.</li>
<li>If <code>ε &lt; ε_threshold</code>, break the cycle.</li>
<li>Adjust the weights: <code>w_ij += -η * δ_j * x_i</code>, <code>δ_j = -(d_j-y_j) ∙ f'(net_j)</code>, where <code>η</code> is learning rate, <code>∙</code> operator is inner product, as I think.</li>
<li>Repeat steps 2-5 until the cycle breaks.</li>
</ol>
<p>The problem is perceptron do not learn whatever values of <code>η</code>, <code>ε_threshold</code> and <code>offset</code> (random initial weight offset) I set. I have similar results after every vector, for example:</p>
<p>Desired digit from y_train: <code>3</code> (index 3 in following arrays)<br />
Weighted fum for each j: <code>[-2.39 -2.21 -2.254 -2.49 -2.41 -2.16 -2.17 -2.37 -2.35  -2.32]</code><br />
Error vector (desired-y_pred): <code>[-0.083 -0.098 -0.095 0.924 -0.082 -0.102 -0.101 -0.085 -0.086 -0.088]</code></p>
<p>Which mean the weights point at all outputs equally probable, although error vector tries to adjust them correctly.</p>
<p>I use sigmoid (<code>σ(x)</code>) activation function with derivative <code>σ(x)*(1-σ(x))</code>.</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import random
import numpy as np
from keras.datasets import mnist
from scipy.misc import derivative

(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize data
X_train = X_train / 255.0
X_test = X_test / 255.0

# Constants from data
X_vector_len = 28*28
y_vector_len = 10

# Learning parameters
learning_rate = 0.05
offset = 0.03  # Random initial weight offset
e_threshold = 0.01


def activation_function(x):
    return 1 / (1 + np.exp(-x))


def acivation_derivative(x):
    return activation_function(x) * (1 - activation_function(x))


def train(X_train, y_train):
    # Weight initialization
    weights = np.random.rand(X_vector_len, y_vector_len) * offset * 2 - offset
    bias = np.random.rand(y_vector_len) * offset * 2 - offset

    epochs = 0
    while True:
        # Getting X and D vectors
        s = random.randrange(len(X_train))
        sample = np.append(X_train[s], [])
        desired = np.zeros(y_vector_len, dtype=np.float64)
        desired[y_train[s]] = 1

        net = np.dot(sample, weights) + bias  # Weighted sum
        y_pred = activation_function(net) 
        e_vec = desired - y_pred
        e = sum(e_vec ** 2) / 2  # Error ε for current vector

        if e &lt; e_threshold:
            break

        gradient = -e_vec.dot(acivation_derivative(net))
        for j in range(y_vector_len):
            for i in range(X_vector_len):
                weights[i, j] -= learning_rate * gradient * float(sample[i])
            bias[j] -= learning_rate * gradient
        epochs += 1
    return weights, bias, epochs


def test(X_test, y_test, weights, bias):
    correct_predictions = 0
    for j, text in enumerate(y_test):
        sample = np.append(X_train[j], [])
        net = np.dot(sample, weights) + bias
        print(f&quot;Test {j+1}: Desired {text}, prediction: {net.argmax()} ({net})&quot;)
        if text == net.argmax():
            correct_predictions += 1
    return correct_predictions / len(X_test)


if __name__ == &quot;__main__&quot;:
    weights, bias, epochs = train(X_train, y_train)
    print(f&quot;Model trained for {epochs} epochs&quot;)

    accuracy = test(X_test, y_test, weights, bias)
    print(f&quot;Accuracy for test selection: {accuracy * 100:.2f}%&quot;)
</code></pre>
<p>Searching for mistakes in formulas and reading some <a href=""https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/"" rel=""nofollow noreferrer"">implementations</a> and <a href=""https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation"" rel=""nofollow noreferrer"">math theory</a> about SGD didn't help me. I only got more questions because implementations are different:</p>
<ol>
<li><p>Why neural network does not learn despite the error vector contains right coefficients (positive for desired, negative for other digits)</p>
</li>
<li><p>The gradient <code>δ</code> is a number as it is an inner product, what is <code>δ_j</code>? If I should use <code>δ</code> as <code>δ_j</code>, doesn't it mix the weights? The value of <code>error_j = d_j - y_j</code> definitely must affect specific outputs. I tried to use both <code>δ</code> and <code>δ_j = -(d_j-y_j) * f'(net_j)</code> (regular product) with same results.</p>
</li>
<li><p>Sigmoid function has range of values = <code>(0, 1)</code>, leading to the values of error vector either <code>1-σ(x)</code> or <code>0-σ(x)</code>, in other words, <code>(-1, 1)</code>. Am I right the error vector is one of the coefficients for weights and it must be positive if the result is correct and vice versa, while other coefficients always non-negative? But how then other activation functions, e.g. <code>f(x) = x</code>, <code>f(x) = arctg(x)</code> with different range of values work?</p>
</li>
<li><p>I have used unit step function as activation function in the other algorithm, is it applicable for SGD?</p>
</li>
<li><p>Can I use numpy functions to adjust weights somehow? I need to multiply all <code>w_ij</code> values by <code>x_i</code> in rows.</p>
</li>
</ol>
","1","Question"
"78538485","","<pre class=""lang-none prettyprint-override""><code>!pip install skater

Collecting skater

  Downloading skater-1.1.2.tar.gz (96 kB)

     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.7/96.7 kB 1.0 MB/s eta 0:00:00

  Preparing metadata (setup.py) ... done

Requirement already satisfied: scikit-learn&gt;=0.18 in /usr/local/lib/python3.10/dist-packages (from skater) (1.2.2)

Collecting scikit-image==0.14 (from skater)

  Downloading scikit-image-0.14.0.tar.gz (27.0 MB)

     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.0/27.0 MB 17.4 MB/s eta 0:00:00

  Preparing metadata (setup.py) ... done

Requirement already satisfied: pandas&gt;=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skater) (2.0.3)

INFO: pip is looking at multiple versions of skater to determine which version is compatible with other requirements. This could take a while.

Collecting skater

  Downloading skater-1.1.0.tar.gz (52 kB)

     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 1.1 MB/s eta 0:00:00

  Preparing metadata (setup.py) ... done

  Downloading skater-1.0.4.tar.gz (41 kB)

     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 687.0 kB/s eta 0:00:00

  Preparing metadata (setup.py) ... done

  Downloading skater-1.0.2.tar.gz (37 kB)

  Preparing metadata (setup.py) ... done

  Downloading skater-1.0.1.tar.gz (36 kB)

  Preparing metadata (setup.py) ... done

ERROR: Cannot install skater==1.0.1, skater==1.0.2, skater==1.0.4, skater==1.1.0 and skater==1.1.2 because these package versions have conflicting dependencies.



The conflict is caused by:

    skater 1.1.2 depends on ds-lime&gt;=0.1.1.21

    skater 1.1.0 depends on ds-lime&gt;=0.1.1.21

    skater 1.0.4 depends on ds-lime&gt;=0.1.1.21

    skater 1.0.2 depends on ds-lime&gt;=0.1.1.21

    skater 1.0.1 depends on ds-lime&gt;=0.1.1.21



To fix this you could try to:

1. loosen the range of package versions you've specified

2. remove package versions to allow pip attempt to solve the dependency conflict



ERROR: ResolutionImpossible:for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
</code></pre>
<p>I tried to install skater package in python Google Colab, but I am getting the above two errors. I have visited the link mentioned but not able to understand how to solve this issue.</p>
","-1","Question"
"78540179","","<p>I have a issue regarding on how to modify a pretrained model to classify 3 classes instead of 1000. These are the 2 methods i came up with so far.. im not sure which one is the best.</p>
<pre><code>NASNetMobile_model = tf.keras.applications.NASNetMobile (
                                                input_shape=(224,224,3),
                                                include_top=False,
                                                pooling='avg',
                                                classes=3,
                                                weights='imagenet'
                                                )
NASNetMobile_model.trainable=False
NASNetMobile_model.summary()type here
</code></pre>
<p>In, Method 1, the NASNetMobile model is initialized with pre-trained ImageNet weights, excluding the top layer and using average pooling. The model is set to be non-trainable to prevent its weights from being updated during training. A new Sequential model is then constructed, which includes the pre-trained NASNetMobile model followed by two dense layers: one with 128 units and ReLU activation, and another with 3 units and softmax activation for the final classification. The Sequential model is compiled with the Adam optimizer and sparse categorical cross-entropy loss. Finally, the model is trained on the dataset for 20 epochs with a batch size of 4 and a validation split of 20%.</p>
<h1>Method 1</h1>
<pre><code>new_pretrained_model = tf.keras.Sequential()

new_pretrained_model.add(NASNetMobile_model)
new_pretrained_model.add(tf.keras.layers.Dense(128, activation='relu'))
new_pretrained_model.add(tf.keras.layers.Dense(3, activation='softmax'))

new_pretrained_model.layers[0].trainable = False
new_pretrained_model.summary() here


new_pretrained_model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
            )

new_pretrained_model.fit(
        Xtrain,
        Ytrain,
        epochs=20,
        batch_size=4,
        validation_split=0.2
        )
</code></pre>
<h1>Method 2</h1>
<p>In Method 2, the functional API is used to create a new model. The output of the pre-trained NASNetMobile model is taken as the input for a new dense layer with 128 units and ReLU activation, followed by a final dense layer with 3 units and softmax activation. This approach explicitly connects the input of the NASNetMobile model to the new output layers, forming a new model with the same input as the original NASNetMobile model but with additional dense layers for classification. The new model is then compiled with the Adam optimizer and sparse categorical cross-entropy loss and trained on the dataset for 20 epochs with a batch size of 4 and a validation split of 20%.</p>
<pre><code>NASNetMobile_model_out = NASNetMobile_model.output
x = tf.keras.layers.Dense(128, activation='relu')(NASNetMobile_model_out)
output = tf.keras.layers.Dense(3, activation='softmax')(x)
model_2 = tf.keras.Model(inputs = NASNetMobile_model.input, outputs=output)

model_2.summary()


model_2.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
            )

model_2.fit(
        Xtrain,
        Ytrain,
        epochs=20,
        batch_size=4,
        validation_split=0.2
        )
</code></pre>
","0","Question"
"78542429","","<p>I am currently trying to finetune <code>Wav2Vec2</code> model from: <a href=""https://huggingface.co/dima806/bird_sounds_classification"" rel=""nofollow noreferrer"">https://huggingface.co/dima806/bird_sounds_classification</a>. But my RAM utilisation is running over the free tier on Google Colab.</p>
<p>The following is my code:</p>
<pre><code>from transformers import TrainingArguments, Trainer

# Load model with ignore_mismatched_sizes=True
model = Wav2Vec2ForSequenceClassification.from_pretrained(
    &quot;dima806/bird_sounds_classification&quot;,
    num_labels=len(label2id),
    ignore_mismatched_sizes=True
)

# Set up training with gradient accumulation
batch_size = 1  # Reduce batch size to manage memory
accumulation_steps = 4  # Accumulate gradients over 4 steps

training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=accumulation_steps,  # Gradient accumulation
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=feature_extractor,
)

# Train the model
trainer.train()
</code></pre>
<p>What could be the reasons the RAM is going past 12.7GB? My dataset only contains 20 items. How can I address this issue?</p>
","1","Question"
"78543787","","<p>I have a problem while making a predictive model, so I'm leaving a question.</p>
<p>I'm trying to create a predictive model using machine learning methodologies such as random forest, xgboost, etc.</p>
<p>At this time, the y value is the differentiated monthly time series data, and the x value is the differentiated daily time series data.</p>
<p>For reference, t, which means time, is timed to the trading day of the U.S. stock market.</p>
<p>My model consists of the following format.</p>
<p>Predictive value = y_(t+21) - y_(t)
explanatory value = y(t) - y(t-1), y(t-1) - y(t-2) ... y(t-p) - y(t-p-1)
At this time, p is the last trading day of the month.</p>
<p>The problem here is that each month has a different number of trading days
For example, there are 23 trading days in January 1980, but there are 20 trading days in February 1981, and there is a possibility that fewer months exist for holidays.</p>
<p>In this case, when building a dataset of explanatory variables for predicting dependent variables, NaN values may be generated for some values in the column by row.</p>
<p>In this case, how should it be handled universally? Or is there a term or paper that refers to the issue of this?</p>
<p>y_(t+21) - y_(t) has two cases. One is to differentiated the end-of-month value and differentiated the average monthly value. For this reason, nothing has been touched yet.</p>
","-2","Question"
"78544969","","<p>I am currently building a big data pipeline for an MLOps project, the pipeline is intended for batch processing.</p>
<p>This is the current setup:</p>
<ul>
<li>I am storing my raw structured data in Hive.</li>
<li>Spark jobs ingest raw data and process it.</li>
<li>I am intending on using feast and Apache Cassandra as an offline store for storing computed and curated features resulting from my Spark jobs.</li>
</ul>
<p>I want to pass data efficiently from spark jobs to feast and Cassandra, I am not sure if an intermediary data persistence solution is needed for holding processed data before passing it to feast to be stored in the offline store, is it necessary in my case?</p>
","0","Question"
"78546693","","<p>I was measuring the RAM that is used by my script and I was surprised that it takes about 300Mb of RAM, while the tokenizer file itself is about 9MB. Why is that?</p>
<p>I tried:</p>
<pre><code>from transformers import AutoTokenizer
from memory_profiler import profile

@profile
def load_tokenizer():
    path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
    tokenizer = AutoTokenizer.from_pretrained(path)

    return tokenizer

load_tokenizer()
</code></pre>
<p>Output:</p>
<pre><code>Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     4    377.4 MiB    377.4 MiB           1   @profile
     5                                         def load_tokenizer():
     6    377.4 MiB      0.0 MiB           1       path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
     7    676.6 MiB    299.2 MiB           1       tokenizer = AutoTokenizer.from_pretrained(path)
     8                                             
     9                                         
    10    676.6 MiB      0.0 MiB           1       return tokenizer
</code></pre>
","0","Question"
"78547320","","<p>I've trained a YOLOV8 model to identify objects in an intersection (ie cars, roads etc).
It is working OK and I can get the output as an image with the objects of interested segmented.</p>
<p>However, what I need to do is to capture the raw geometries (polygons) so I can save them on a txt file later on.</p>
<p>I tried what Ive found in the documentation (<a href=""https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode</a>) however the returning object is not the same as the documentation says.</p>
<p>In fact, the result is a list of tensorflow numbers:</p>
<p><a href=""https://i.sstatic.net/Fy4lI1sV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fy4lI1sV.png"" alt=""enter image description here"" /></a></p>
<p>Here's my code:</p>
<pre><code>import argparse
import cv2
import numpy as np
from pathlib import Path
from ultralytics.yolo.engine.model import YOLO    
    
# Parse command line arguments
parser = argparse.ArgumentParser()
parser.add_argument('--source', type=str, required=True, help='Source image directory or file')
parser.add_argument('--output', type=str, default='output', help='Output directory')
args = parser.parse_args()

# Create output directory if it doesn't exist
Path(args.output).mkdir(parents=True, exist_ok=True)

# Model path
model_path = r'C:\\_Projects\\best_100img.pt'

# Load your model directly
model = YOLO(model_path)
model.fuse()

# Load image(s)
if Path(args.source).is_dir():
    image_paths = list(Path(args.source).rglob('*.tiff'))
else:
    image_paths = [args.source]

# Process each image
for image_path in image_paths:
    img = cv2.imread(str(image_path))
    if img is None:
        continue

    # Perform inference
    predictions = model.predict(image_path, save=True, save_txt=True)
    
print(&quot;Processing complete.&quot;)
</code></pre>
<p>Here's the problem: the return object (predictions variable) has no <strong>boxes, masks, keypoints</strong> and etc.</p>
<p>I guess my questions are:</p>
<ul>
<li>Why the result is so different from the documentation?</li>
<li>Is there a conversion step?</li>
</ul>
","0","Question"
"78550784","","<p>I am given binary data points of dimension 5,000. I am asked to perform machine learning predicting a binary vector of length 1k, where each position of the output is a class. The classes are <strong>not</strong> exclusive.</p>
<p>What I know about the class distribution:</p>
<ul>
<li>positions with small index are more common</li>
<li>a sample can belong to several classes</li>
<li>each sample fulfills only a handful of class requirements (i.e. the output is &quot;sparse&quot;)</li>
</ul>
<p>How can I keep track of the loss in my ML model? I have used multi-layer perceptrons (pytorch) and cross-entropy loss (CE loss), but I find it hard to interpret the results. I assume CE loss is used when you have several classes, but only one is chosen at a time (multi-class classification).
Moreover, my prediction leads to vectors with about half the bits set, whereas I expect between 20 and 50 bits set, not more.</p>
<pre><code># An &quot;example&quot; data point:
point = [1, 0, 0, 1, 0, 0, 1, 1, ...,1, 1, 0, 0, 1, 0, 1] # length 5000
label = [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...,0] # length 1000
# label has only 20-50 bits set
</code></pre>
","-1","Question"
"78550892","","<p>I am currently trying to create an azure Machine Learning workspace with the CLI. I used the code:</p>
<pre><code>az ml workspace create --name &quot;rg-dp100-labs&quot; --resource-group &quot;rg-dp100-labs&quot;
</code></pre>
<p>I got the following error.</p>
<pre><code>Code: ValidationError
Message: Missing dependent resources in workspace json
Target: workspace
Exception Details: (Invalid) Missing dependent resources in workspace json
Code: Invalid    
Message: Missing dependent resources in workspace json    
Target: workspace
</code></pre>
<p>Only the KeyVault and Storage accounts were created. Referring to this <a href=""https://learn.microsoft.com/en-us/answers/questions/1679456/how-can-i-properly-use-the-azure-cli-to-create-an"" rel=""nofollow noreferrer"">link on Microsoft Learn</a>, I found that others also have the same issue. Even a Google search gave me the same code as written above. lease, is anyone experiencing the same issue and has solved it? I think it is a recent problem.</p>
","2","Question"
"78551615","","<p>I am working on a project involving Step Functions with SageMaker. I have an existing Step Function that I need to integrate SageMaker into, and I tried adding steps such as processing, model training, registering the model, and batch transform job requests. I also added .sync at the end of each resource so it waits for one to complete before starting the next.</p>
<p>However, I encountered an issue with the Step Function's SageMaker processing job. The processing job runs but does not finish due to permission being denied to save a CSV file from my processed pandas dataframe.</p>
<pre class=""lang-py prettyprint-override""><code># dependencies imports
df = pd.read_csv(&quot;/opt/ml/processing/input/data/data.csv&quot;)
print(df.head())

# some processing on df

df.to_csv(&quot;/opt/ml/processing/output/result.csv&quot;, index=False)
</code></pre>
<p>Here are my state machine configurations for the processing request:
Please leave me a comment, if you have to see other parts of my configs</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;AppSpecification&quot;: {
    &quot;ContainerEntryPoint&quot;: [
      &quot;python3&quot;,
      &quot;/opt/ml/processing/input/code/processing.py&quot;
    ]
  },
  &quot;ProcessingInputs&quot;: [
    {
      &quot;InputName&quot;: &quot;Input-1&quot;,
      &quot;S3Input&quot;: {
        &quot;S3Uri&quot;: &quot;s3://my-dataset/data.csv&quot;,
        &quot;LocalPath&quot;: &quot;/opt/ml/processing/input/data&quot;
      }
    },
    {
      &quot;InputName&quot;: &quot;Input-2&quot;,
      &quot;S3Input&quot;: {
        &quot;S3Uri&quot;: &quot;s3://my-dataset/processing.py&quot;,
        &quot;LocalPath&quot;: &quot;/opt/ml/processing/input/code&quot;
      }
    }
  ],
  &quot;ProcessingOutputConfig&quot;: {
    &quot;Outputs&quot;: [
      {
        &quot;OutputName&quot;: &quot;Output-1&quot;,
        &quot;S3Output&quot;: {
          &quot;S3Uri&quot;: &quot;s3://my-dataset/data.csv&quot;,
          &quot;LocalPath&quot;: &quot;/opt/ml/processing/output/&quot;,
          &quot;S3UploadMode&quot;: &quot;EndOfJob&quot;
        }
      }
    ]
  }
}
</code></pre>
<p>The ProcessingInputs configurations are working as expected. I saw in the log that data.csv content is correctly printed in the log by <code>df.head()</code>. However, when it reaches the last line of code, I get the following error:
<code>PermissionError: Permission Denied '/opt/ml/processing/output/result.csv'</code>
I also tried saving it to other folders as I saw in some examples found online, such as saving to folders like <code>training, result</code>, and others, but no luck so far. It's giving me the same permission error. I used a Lambda function created just for this and made a request to the SageMaker processing job, and got exactly the same permission denied error.</p>
<p>I also tried saving into completely different folder out of /opt/ml/processing/, but /result.csv
But it gave me different error as SageMaker only allows us to save csv files under /opt/ml/processing/.... so I am not sure what to do with it.</p>
<p>Currently I am saving the result set manually using boto3 api and wait the processing job to pass the <code>StoppingCondition.MaxRuntimeInSeconds</code>time I set and eventually it stops and I use additinoal step to pick it up.
But I dislike the way I make a workaround to the problem and I really need to find a better way to resolve this.</p>
<p>Can someone tell me what I am missing?</p>
","0","Question"
"78554157","","<pre><code>url= &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris.data&quot;
names = ['sepal-length','sepal-width','petal-length','petal-width','petal-width','class'] 
dataset = pandas.read_csv(url, names = names)
</code></pre>
<blockquote>
<p>ValueError: Duplicate names are not allowed.</p>
</blockquote>
<p><a href=""https://i.sstatic.net/E9dDwNZP.png"" rel=""nofollow noreferrer"">I was expecting this output but instead there was an error</a></p>
","0","Question"
"78554214","","<p>Currently, I'm learning ML and started from K neighbours classification and I'm wondering how to deal with all the parameters (columns) given to me. I have only 1 dataset containing 10k rows and I'm splitting it like 80/20. I also have test data in another csv (without y).</p>
<p>But what is bothering me is that I can only get ~78% accuracy while learning and I was wondering, how I can improve my results. Looking at columns I have some questions about these columns in particular:</p>
<p><a href=""https://i.sstatic.net/fsG8tu6t.png"" rel=""nofollow noreferrer"">First dataset contains 2 different groups while test data contains equally distributed points</a></p>
<p><a href=""https://i.sstatic.net/lQwOlQd9.png"" rel=""nofollow noreferrer"">Reverse situation with the first picture</a></p>
<p><a href=""https://i.sstatic.net/AJRbPJW8.png"" rel=""nofollow noreferrer"">Same with another columns</a></p>
<p><a href=""https://i.sstatic.net/7gLac0eK.png"" rel=""nofollow noreferrer"">Something strange</a></p>
<p>Should I remove them, or should I try do something with them to use them in my training data?</p>
<p>Also at the moment, I did not understand why my model works better with metric='manhattan' instead of euclidean and how to choose optimal K with the test/train data. I read that you should use sqrt(N), where N is the number of test rows, but is that really the case?</p>
<p><a href=""https://file.io/wmoHa59Qm0mv"" rel=""nofollow noreferrer"">Train/Test data</a></p>
","1","Question"
"78554891","","<p>I am trying to fit multiple observations to a single Gaussian Process.</p>
<p>I try to fit the data of two observations (Y) like this:</p>
<pre><code>import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C


# Example data

# Input data X 
X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])

# Output data Y 
Y = np.array([[1.5, 2.5], [2.5, 3.5], [3.5, 4.5], [4.5, 5.5], [5.5, 6.5]])
kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

# Fitting the model
gp.fit(X, Y)

mean_prediction, cov_prediction = gp.predict(X, return_cov=True)
</code></pre>
<p>I am getting two array of mean_prediction and two cov_prediction matrices. But I want a single mean and covariance matrix of same dimension as the observations corresponding to single fitted GP. How can I implement that?</p>
","0","Question"
"78558728","","<p>I am using YOLOv5 to train a model to recognize cards in a card game. I started with the pre-trained model yolov5s.pt and my dataset consists of 138 images. However, the accuracy and mAP are very low during training, starting from 2.35e-05 and 2.27e-05 respectively, and after 80 epochs, they only reached 0.0169 and 0.0547.</p>
<p>I can't figure out what the problem is. Can someone help me?</p>
<p>Here's the picture of the training batch image and the output table.
<a href=""https://i.sstatic.net/eAXHJk0v.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eAXHJk0v.jpg"" alt=""training batch image"" /></a>
<a href=""https://i.sstatic.net/bmDEaBbU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmDEaBbU.png"" alt=""output table"" /></a></p>
<p>By the way I only want to recognize the discard not the handcard.</p>
<p>I've tried to change the batch size etc. But it didn't change a lot.</p>
","0","Question"
"78558863","","<p>Assuming I am building an XGBoost model in Python (xgboost version 2.0.3) (regression or classification here it is does not matter at all) to predict a target variable in a stock market time series analysis.</p>
<p>The target for example may be: next value in the time series or a binary variable set to 1 if the next value is higher than the previous one, 0 otherwise.</p>
<p>To train the model is it possible to use, for example, the MSE in the regression problem or the 'binary logistic' in the classification one.</p>
<p>After the training, it is possible to backtest a strategy based on the output of the model in the test set and compute the overall return.</p>
<p>My question is: using the xgboost scikit-learn interface, would it be possible to train the model on the performance metric used to backtest the strategy?</p>
<p>E.g.: to maximize the overall return in the training set following the strategy rules.</p>
<p>on the xgboost library website, it is shown how to use a custom loss function for training the model:</p>
<pre><code>def softprob_obj(labels: np.ndarray, predt: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:
    rows = labels.shape[0]
    classes = predt.shape[1]
    grad = np.zeros((rows, classes), dtype=float)
    hess = np.zeros((rows, classes), dtype=float)
    eps = 1e-6
    for r in range(predt.shape[0]):
        target = labels[r]
        p = softmax(predt[r, :])
        for c in range(predt.shape[1]):
            g = p[c] - 1.0 if c == target else p[c]
            h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)
            grad[r, c] = g
            hess[r, c] = h

    grad = grad.reshape((rows * classes, 1))
    hess = hess.reshape((rows * classes, 1))
    return grad, hess

clf = xgb.XGBClassifier(tree_method=&quot;hist&quot;, objective=softprob_obj)
</code></pre>
<p>The objective function requires the computation of the gradient and the hessian.</p>
<p>Assuming a function defined as below:</p>
<pre><code>def maximize_performance_metric(y_true: np.ndarray, y_pred: np.ndarray):
   # metrics computation (eg: overall return using y_pred
   overall_return = get_overall_return(y_pred, real_prices, ...) # overall_return is a float
   return grad, hess

</code></pre>
<p>Is it possible to compute the gradient and the hessian with respect to the overall return and so, training the model using this custom loss function?</p>
<p>How can the function <code>maximize_performance_metric()</code> have access to the variable that contains <code>real_prices</code> ( needed for the overall return computation )?</p>
","1","Question"
"78558974","","<p>Below is a section of Matlab code from a neural net I'm trying to write. It's my first attempt at anything related to machine learning. I'm following along with  Michael Nielson's book here: <a href=""http://neuralnetworksanddeeplearning.com/chap2.html"" rel=""nofollow noreferrer"">http://neuralnetworksanddeeplearning.com/chap2.html</a></p>
<p>I'm loading a set of 60000 28x28 grayscale images of handwritten digits with labels and am trying to train this neural net to identify them. There is also a test data set of 10000 images. The network has 784 input neurons (28^2), two hidden layers each with 16 neurons, and the output layer has 10 neurons. I'm initializing the weights and biases to a random value between -0.5 and 0.5.</p>
<p>I'm evaluating the cost function as C = 0.5*(a-y).^2. It seems mildly successful in that it started at C=1.35 and ended at C=0.46 before essentially flattening out (about 75 epochs). However, the error is still high enough it only guesses the correct digit 12% of the time, which is almost random chance. I've double and triple checked the math but can't find a mistake. I'm thinking there must be one I'm not seeing. The code below is everything within the main training loop, so any bugs should be in there. I'm not breaking the images into smaller batches, but am doing the entire 60k images at a time with each epoch. Since each image is only 28x28 pixels it's fast enough without breaking it apart. The input neurons a_0 are a 784x60000 array of doubles, with values between 0 and 1. I took the original images, where each pixel was a uint8, and converted to double then divided by 255 to get a_0. I'm numbering the layers in my code such that layer 0 is the input layer, layers 1 and 2 are the hidden layers, and layer 3 is the output layer.</p>
<pre><code>a_0 = training_images;
epoch = 0;
while epoch &lt; 5 || C(epoch - 1) - C(epoch) &gt; 0.001    
epoch = epoch + 1;

%Propagate forwards
z_1 = weights_1*a_0 + biases_1;
a_1 = sigmoid(z_1);
z_2 = weights_2*a_1 + biases_2;
a_2 = sigmoid(z_2);
z_3 = weights_3*a_2 + biases_3;
a_3 = sigmoid(z_3);

%Evaluate cost function
C(epoch) = 0.5*mean(sum((a_3-y).^2, 1));

%Propagate backwards
sigmoid_d1 = a_1 .* (1-a_1); %Sigmoid derivative
sigmoid_d2 = a_2 .* (1-a_2);
sigmoid_d3 = a_3 .* (1-a_3);
delta_3 = (a_3-y).*sigmoid_d3;
delta_2 = weights_3.'*delta_3 .* sigmoid_d2;
delta_1 = weights_2.'*delta_2 .* sigmoid_d1;

%Calculate gradient
for image_index = 1:num_images
    dC_dw3(:, :, image_index) = delta_3(:, image_index) * a_2(:, image_index).';
    dC_dw2(:, :, image_index) = delta_2(:, image_index) * a_1(:, image_index).';
    dC_dw1(:, :, image_index) = delta_1(:, image_index) * a_0(:, image_index).';
end

%Calculate adjustment
training_rate = 0.1;
adjust_biases_1 = -training_rate * mean(delta_1, 2);
adjust_biases_2 = -training_rate * mean(delta_2, 2);
adjust_biases_3 = -training_rate * mean(delta_3, 2);
adjust_weights_1 = -training_rate * mean(dC_dw1, 3);
adjust_weights_2 = -training_rate * mean(dC_dw2, 3);
adjust_weights_3 = -training_rate * mean(dC_dw3, 3);
biases_1 = biases_1 + adjust_biases_1;
biases_2 = biases_2 + adjust_biases_2;
biases_3 = biases_3 + adjust_biases_3;
weights_1 = weights_1 + adjust_weights_1;
weights_2 = weights_2 + adjust_weights_2;
weights_3 = weights_3 + adjust_weights_3;
</code></pre>
","-1","Question"
"78559061","","<p>I am working on a multi label classification problem and need some guidance on computing class weights using Scikit-Learn.</p>
<p><strong>Problem Context:</strong></p>
<p>I have a dataset with 9973 training samples.The labels are one-hot encoded, representing 13 different classes.The shape of my training labels is (9973, 13).</p>
<p>I want to use this code:</p>
<pre><code>import numpy as np
from sklearn.utils.class_weight import compute_class_weight

y_integers = np.argmax(y, axis=1)
class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)
d_class_weights = dict(enumerate(class_weights))
</code></pre>
<p>This does not work says, too may positional arguments. My training samples look like this:</p>
<pre><code>                          [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
                          [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
                          [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
</code></pre>
<p>How can i implement in multi class classification problem so that my dataset imbalance can be solved ?</p>
<p>Edit 1: It is working fine now, Do you think it works in multilabel because I read somewhere, that you must use sampling weight instead of class weight. How can i implement that ?</p>
","0","Question"
"78561885","","<p>I am designing a binary classifier random forest model using python and scikitlearn, in which I would like to retrieve the probability of my test set being one of the two labels. To my understanding, predict_proba(xtest) will give me the following result:</p>
<pre><code>Number Of Trees Voted For Classifier / Number Of Trees
</code></pre>
<p>I find this too imprecise, as certain tree nodes, may have separated my (non-deterministic) samples, into fairly precise leaves (100 class a , 0 class b), and imprecise leaves (5 class a, 3 class b). I would like an implementation of 'probability' that takes the total number of samples in my n-classifiers output leaves as a dominator and the total number of the overall chosen classifier in the output leaves as the numerator (even for tree's and their output leaves that chose the class most trees didn't).</p>
<p>For example (Simple):</p>
<p>2 Trees:</p>
<pre><code>Tree 1:  
   --- 5, 0 Class A (Chosen)  
10    
   --- 2, 3 Class B (Unchosen)  

Tree 2:  
   --- 3, 2 Class A (Chosen)  
10   
   --- 5, 0 Class B (Unchosen)
</code></pre>
<p><code>predict_proba</code> results:</p>
<pre><code>Number of Trees that chose Class A (2) / Number of Trees (2) = 1.0
</code></pre>
<p>Desired Results:</p>
<pre><code>Number of Class A Samples in Output Leaves (8) / Total Number Samples in Output Leaves (10) = 0.8
</code></pre>
<p>Does anyone have any knowledge on how to do this, or an implementation they are using?</p>
<p>An idea I had was to iterate through every tree, retrieving their probabilities, and averaging them. However, this would give higher bias to output leaves that have less samples (electoral college style).</p>
<p>How to directly access the number of samples and their classes of the output leaf of a decision tree for a specific sample (Or even just the leaves index, and go from there)? And in the case of a random forest, summing and averaging them?</p>
<p>If not switch platforms/libraries entirely? Or maybe just crank up the number of classifiers (not optimal)?</p>
<p>Some potentially helpful documentation?:</p>
<pre><code>dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples ?
</code></pre>
","1","Question"
"78563364","","<p>I get the following error in sagemaker code editor when trying to deploy meta/llama-3-8B-Instruct:</p>
<p>&quot;The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'.
The class this function is called from is 'LlamaTokenizer'.&quot;</p>
<p>I am using the Huggingface API to directly load a model from HF Hub to sagemaker code editor using an ml.g5xlarge instance type in AWS. There is no tokenizer necessary or exposed using the HF API.</p>
","0","Question"
"78564460","","<p>I am studying ML and was trying to make a reinforcement learning algorithm for a gymnasium environment. I already made a q-learning for a very basic and simple problem and I decided to use the same algorithm with a slightly more complex environment such as a the car-pole.</p>
<p>I believe the algorithm is working as expected as the AI is able to achieve pretty decent result and that the result are improving / worsening based on the number of episodes / learning rate / epsilon. However I noticed that if I run different tests after the AI has completed the training I obtain different result with each test: When I test, I remove the epsilon probability to explore, I therefore believe that the AI should take the best possible action each time and therefore should obtain the same result for every trial, however, this does not happen. Did I not understand correctly how the Q-Learning algorithm works or is it supposed to obtain slightly different result each time?</p>
<p>This is my code:</p>
<pre><code>import gymnasium as gym
import numpy as np
import random

# Hyperparamteters
alpha = 0.05
gamma = 0.90
epsilon = 1.0
epsilon_decay = 0.995
epsilon_min = 0.1
episodes = 10000
max_steps = 200

# Initialise environment
env = gym.make(&quot;CartPole-v1&quot;)
state_space = [20, 20, 50, 50] #cart_position, cart_velocity, pole_angle, pole_angular_velocity
q_table = np.zeros(state_space + [env.action_space.n])

def discretize_state(state):
    &quot;&quot;&quot;
    Discretize a space means to convert all continuous actions into discrete and finite actions. 
    Continuous action can be infinite or very large and will therefore be difficult to handle. 
    This function takes a state representing all the values for each dimension [cart position, cart velocity, pole angle, pole velocity]
    and returns a discretised tuple rounded to the closest integer. 
    &quot;&quot;&quot;

    # Normalization formula = (state - min) / (max - min). Returns a value between 0 - 1
    normalised_state = (state - env.observation_space.low) / (env.observation_space.high - env.observation_space.low)

    # Scales the normalised values into the number and size of bins, then it rounds each direction into their closest integer value.
    discretized = np.round(normalised_state * (np.array(state_space) - 1)).astype(int)

    return tuple(discretized)

# Q-learning loop algorithm
print(&quot;Training started:\n-----------------------------------\n&quot;)
for episode in range(episodes):
    state = discretize_state(env.reset()[0])
    total_reward = 0

    for step in range(max_steps):
        &quot;&quot;&quot;
        Decide wether to explore or exploit based on epsilon. with probability epsilon the 
        algorithm will explore by taking a random possible action. With probability 1 - epsilon
        the algorithm will take the best possible action based on the q-value of the previously
        explored actions. 
        As epsilon starts with a value of 1, the first action will always be random. 
        &quot;&quot;&quot;
        if random.uniform(0, 1) &lt; epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(q_table[state])

        next_state, reward, done, _, _ = env.step(action)
        next_state = discretize_state(next_state)
        total_reward += reward

        # Q-Learning algorithm:  Q(s, a) &lt;- Q(s, a) + alpha[R + gamma * max(Q(s1, a1)) - Q(s, a))]
        best_next_action = np.argmax(q_table[next_state])
        td_target = reward + gamma * q_table[next_state][best_next_action] # Temporal Difference Target -&gt; sum of total reward and the discounted q value of best action for next state
        td_error = td_target - q_table[state][action] # Temporal Difference Error -&gt; Difference between TDTarget and the current q-value
        q_table[state][action] = q_table[state][action] + alpha * td_error # Update current q-value based on the larning rate (alpha)

        state = next_state

        if done:
            break
        
    # Reduce epsilon by epsilon decay rate to gradually reduce exploration and favour learning on previous experiences
    epsilon = max(epsilon_min, epsilon * epsilon_decay)
    print(f&quot;Epsiode {episode + 1}: Total reward: {total_reward}&quot;)
print(&quot;Training finished.&quot;)

# Test trained agend
print(&quot;Testing:\n----------------------------------\n&quot;)
for episode in range(10):
    state = discretize_state(env.reset()[0])
    total_reward = 0

    for step in range(max_steps):
        action = np.argmax(q_table[state])
        next_state, reward, done, _, _ = env.step(action)
        state = discretize_state(next_state)
        total_reward += reward

        if done:
            print(f&quot;Episode {episode + 1} - Total reward: {total_reward}&quot;)
            break

env.close()
</code></pre>
<p>After training, an example of results batch is the following:</p>
<p><code>Episode 1 - Total reward: 38.0</code></p>
<p><code>Episode 2 - Total reward: 46.0</code></p>
<p><code>Episode 3 - Total reward: 48.0</code></p>
<p><code>Episode 4 - Total reward: 62.0</code></p>
<p><code>Episode 5 - Total reward: 48.0</code></p>
<p><code>Episode 6 - Total reward: 65.0</code></p>
<p><code>Episode 7 - Total reward: 44.0</code></p>
<p><code>Episode 8 - Total reward: 69.0</code></p>
<p><code>Episode 9 - Total reward: 59.0</code></p>
<p><code>Episode 10 - Total reward: 16.0</code></p>
","0","Question"
"78565125","","<p>I'm facing a ERROR: No matching distribution found for tensorflow-text Jupyter Notebook when i try to insatll it. In Google Colab when im loading tensorflow_hub, so when i install tensorflow-text then tensorflow_hub not working &amp; when install tensorflow_hub then tensorflow-text not working.</p>
<p><a href=""https://i.sstatic.net/KDww4SGy.png"" rel=""nofollow noreferrer"">Jupyter Notebook image</a> <a href=""https://i.sstatic.net/zFbe2p5n.png"" rel=""nofollow noreferrer"">Google Colab image 1</a> <a href=""https://i.sstatic.net/Ta7Y6vJj.png"" rel=""nofollow noreferrer"">Google Colab image 2</a></p>
<p>I tried installing tensorflow_text using pip install tensorflow_text in both Jupyter Notebook and Google Colab. I expected the library to be imported successfully in my code. Additionally, I experimented with installing specific versions of tensorflow_text but still encountered the same error.</p>
","-1","Question"
"78565463","","<p>I have read the article <a href=""https://medium.com/@tentotheminus9/elisa-analysis-in-python-deb8c6ed91db"" rel=""nofollow noreferrer"">ELISA Analysis in Python</a> on Medium.</p>
<p>The above article uses SciPy's <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"" rel=""nofollow noreferrer"">curve_fit</a> function to find an approximate curve based on the 4 parameter logistic regression (4PL) model as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
    return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]
</code></pre>
<p>I would like to do the same thing using the <a href=""https://hexdocs.pm/scholar/readme.html"" rel=""nofollow noreferrer"">Nx/Scholar</a> library in Elixir.</p>
<p>Is it possible? I would appreciate any hints you can give me.</p>
<hr />
<p>[UPDATE]</p>
<p>From a quick look at the Python <code>scipy.optimize</code> source code, it appears that <code>curve_fit</code> uses Fortran's MINPACK library internally.</p>
<p>As far as I know, there is no easy way to use MINPACK from Elixir.</p>
<p>Therefore, I conclude that it is difficult to do ELISA Analysis in Elixir at this time.</p>
<p>I welcome any additional information.</p>
","0","Question"
"78565774","","<p>After performing OneHotEncoding, my data frame gives values for its columns as True and False, not as 1 and 0, what I am supposed to have. My code looks like this, where the data frame is ds and the categorical column name is &quot;type&quot;:</p>
<pre><code>pd.get_dummies(ds,columns = [&quot;type&quot;])
</code></pre>
<p>I'm trying to get a numeric result.</p>
","-2","Question"
"78565978","","<p>I am working on a retail dataset in which invoice has 250000 duplicates value . How to handle the duplicates in the data I am still trying to figure out a way to resolve the query .I was thinking to resolve it with mean median or mode</p>
","-1","Question"
"78566413","","<p>I am creating an extension which will retrieve image from the web and then it will pass the picture to gemini-pro-vision, but I am unable to do it only with js. It is requiring node js or python, but extension is only built on JavaScript. How do I solve it?</p>
<p>I want to use gemini-pro-vision as an API. I want to send it to gemini and get the response.</p>
","1","Question"
"78572370","","<p>In the model below the accuracy reported at the end of the final validation stage is 0.46, but when reported during the manual testing the value is 0.53. What can account for this discrepancy?</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn
import torchvision.models as models
import pytorch_lightning as pl
from torchmetrics.classification import BinaryAccuracy
from pytorch_lightning.loggers import NeptuneLogger
from torch.utils.data import DataLoader, TensorDataset
from os import environ

class ResNet(pl.LightningModule):
    def __init__(self, n_classes=1, n_channels=3, lr=1e-3):
        super().__init__()
        self.save_hyperparameters()
        self.validation_accuracy = BinaryAccuracy()
        
        backbone = models.resnet18(pretrained=True)
        n_filters = backbone.fc.in_features
        if n_channels != 3:
            backbone.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        layers = list(backbone.children())[:-1]
        self.feature_extractor = nn.Sequential(*layers)
        self.classifier = nn.Linear(n_filters, n_classes)
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, x):
        features = self.feature_extractor(x)
        logits = self.classifier(features.squeeze())
        return logits

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.validation_accuracy.update(y_hat, y)
        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def on_validation_epoch_end(self):
        val_acc = self.validation_accuracy.compute()
        self.log('val_acc', val_acc, on_epoch=True, prog_bar=True, logger=True)
        self.validation_accuracy.reset()

    def predict_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        preds = torch.sigmoid(y_hat) &gt; 0.5
        return preds, y

def get_dataloader():
    # Create a simple synthetic dataset for demonstration purposes
    x = torch.randn(100, 3, 224, 224)
    y = torch.randint(0, 2, (100, 1)).float()
    dataset = TensorDataset(x, y)
    return DataLoader(dataset, batch_size=8)

# Set up model, data, and trainer
model = ResNet()
train_loader = get_dataloader()
val_loader = get_dataloader()

logger = NeptuneLogger(
        api_key=environ.get(&quot;NEPTUNE_API_TOKEN&quot;),
        project=&quot;richbai90/ResnetTest&quot;,
        tags=[&quot;MRE&quot;, &quot;resnet&quot;],
    )


trainer = pl.Trainer(max_epochs=3, logger=logger, log_every_n_steps=1)

# Train and validate the model
trainer.fit(model, train_loader, val_loader)

# Predict on validation data
val_loader = get_dataloader()
preds, targets = [], []
for batch in val_loader:
    batch_preds, batch_targets = model.predict_step(batch, 0)
    preds.extend(batch_preds)
    targets.extend(batch_targets)

# Calculate accuracy manually for comparison
preds = torch.stack(preds).view(-1)
targets = torch.stack(targets).view(-1)
manual_accuracy = (preds == targets).float().mean().item()
print(f&quot;Manual accuracy: {manual_accuracy:.4f}&quot;)
</code></pre>
","0","Question"
"78575816","","<p>I have been completely lost on trying to import this set_session for a project. I have tried using:</p>
<pre><code>from tensorflow.compat.v1.keras.backend import set_session
</code></pre>
<p>and</p>
<pre><code>from tensorflow.keras.backend import set_session`
</code></pre>
<p>(and numerous other variations)</p>
<p>Is all hope lost? I am using python 3.11.1, tensorflow 2.16.1 (which installs keras 3.3.3)</p>
<p><a href=""https://i.sstatic.net/mwhFHaDs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mwhFHaDs.png"" alt=""enter image description here"" /></a></p>
<p>I was hoping to make this program as easy to run as possible but it seems like the only way may be to install an old version of tensorflow (which you can't do with pip)</p>
<p>set_session should work, and I have gotten it to work on older versions of tensorflow I would just like it to be friendly to download my program without going through that struggle. I also have it use this:</p>
<pre><code>def get_session(gpu_fraction): #set GPU memory usage for TensorFlow
    config = tf.compat.v1.ConfigProto()
    config.gpu_options.per_process_gpu_memory_fraction = gpu_fraction
    return  tf.compat.v1.Session(config=config)
</code></pre>
","0","Question"
"78576306","","<p>I'm working on making forecasts using a model where variables were scaled by $  x_i = \frac{{x_i - \text{mean}(x_i)}}{{\text{sd}(x_i)}} $, and I've saved the mean and standard deviation. Now, for out-of-sample forecasts, let's say for the target variable $ ( x_i )$, based on the scaled model, how do I scale the forecasts back?</p>
<p>Should I use the in-sample $ \text{Mean}(x_i) $ and $ \text{sd}(x_i) $ to scale the out-of-sample forecasts back, so that:</p>
<p>$ \text{Re-scaled out-of-sample forecast} = \text{Scaled forecast} \times \text{sd}(x_i) + \text{mean}(x_i) $</p>
<p>What's the appropriate procedure here?</p>
<p>Python example:</p>
<pre><code>X = np.random.randn(100, 1) * 10 + 50  # Feature
y = 2 * X + 1 + np.random.randn(100, 1) * 5  # Target variable
X_train, X_test = X[:80], X[80:]
y_train, y_test = y[:80], y[80:]

scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
y_train_scaled = scaler_y.fit_transform(y_train)

model = LinearRegression()
model.fit(X_train_scaled, y_train_scaled)
</code></pre>
","1","Question"
"78576495","","<p>I am optimising 11 parameters for a Jansen &amp; Rit whole-brain model as follows:</p>
<p>search_space = [
Integer(1, 10, name=&quot;A&quot;), Integer(10, 40, name=&quot;B&quot;), Integer(90, 200, name=&quot;C&quot;), etc...
]</p>
<p>gbrt_minimize(find_bold_loss, search_space, n_calls=80, n_initial_points=32, initial_point_generator='sobol').</p>
<p>What I don't understand, is my loss function is highly complex and non-differentiable, as it first needs to run the model (which uses the Euler method and introduces Gaussian noise on each iteration). So, how does gbrt work if it is unable to calculate the gradient of the loss at each iteration? I'm new to skopt and GBRT, so some in-depth explanation would be appreciated!</p>
<p>N.B. I have run the model with gbrt and it works well (better than bayesian optimisation), so it must be doing something right...</p>
","0","Question"
"78577415","","<p>Helllo,</p>
<p>I have trouble using any kind of decision tree model from MLJ. I have tried 3 packages from MLJ, DecisionTree, Scikit, and now this BetaML. This only happens when I'm trying to train some kind of decision tree. I works fine with other MLJLinearModels and with XGBoost. I always get the same error. The error is coming from the following function:</p>
<pre><code> function machine_train_predict(df::DataFrame, df_train::DataFrame, model_name::String; args...)
        models = Dict(
        &quot;xgb_reg&quot;=&gt; [&quot;XGBoost&quot; =&gt; &quot;XGBoostRegressor&quot;],
        &quot;ridge_reg&quot;=&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;RidgeRegressor&quot;],
        &quot;lasso_reg&quot;=&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;LassoRegressor&quot;],
        &quot;rf_reg&quot; =&gt; [&quot;BetaML&quot; =&gt; &quot;RandomForestRegressor&quot;],
        &quot;lin_reg&quot; =&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;LinearRegressor&quot;],
        &quot;log_class&quot; =&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;LogisticClassifier&quot;],
        &quot;rf_class&quot; =&gt; [&quot;DecisionTree&quot; =&gt; &quot;RandomForestClassifier&quot;],
        &quot;xgb_class&quot; =&gt; [&quot;XGBoost&quot; =&gt; &quot;XGBoostClassifier&quot;]
        )

        y, X =  machine_input(df_train; rng=123)
        y = coerce(y, Continuous)

        mod = models[model_name][1]
        p = mod[1]
        m = mod[2]
        mname = model_name
        Model = @eval @load $(m) pkg=$(p) verbosity=0
        model = Model()

        # train machine and get parameters

        m1 = machine(model, X, y) |&gt; fit!

    #     prepare test set for machine predictions
        y, X =  machine_input(df)
        y = coerce(y, Continuous)
    #     predict
        yhat = MLJ.predict_mode(m1, X)
        return yhat
    end
</code></pre>
<p>And the error:</p>
<pre><code>Training. Dataset: global. Iteration N: 1ERROR: LoadError: MethodError: no method matching BetaML.Bmlj.RandomForestRegressor()
The applicable method may be too new: running in world age 33750, while current world is 33793.

Closest candidates are:
  BetaML.Bmlj.RandomForestRegressor(; n_trees, max_depth, min_gain, min_records, max_features, splitting_criterion, β, rng) (method too new to be called from this world context.)
   @ BetaML ~/.julia/packages/BetaML/8WVUG/src/Bmlj/Trees_mlj.jl:219
  BetaML.Bmlj.RandomForestRegressor(::Int64, ::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::Float64, ::Random.AbstractRNG) (method too new to be called from this world context.)
   @ BetaML ~/.julia/packages/BetaML/8WVUG/src/Bmlj/Trees_mlj.jl:193
  BetaML.Bmlj.RandomForestRegressor(::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any) (method too new to be called from this world context.)
   @ BetaML ~/.julia/packages/BetaML/8WVUG/src/Bmlj/Trees_mlj.jl:193

Stacktrace:
  [1] (::var&quot;#machine_train_predict#38&quot;{var&quot;#machine_train_predict#11#39&quot;})(df::DataFrame, df_train::DataFrame, model_name::String; args::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:250
  [2] (::var&quot;#machine_train_predict#38&quot;{var&quot;#machine_train_predict#11#39&quot;})(df::DataFrame, df_train::DataFrame, model_name::String)
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:230
  [3] (::var&quot;#train_rescore#36&quot;{var&quot;#train_rescore#10#37&quot;})(df::DataFrame, df_train::DataFrame, model_name::String; args::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:223
  [4] (::var&quot;#train_rescore#36&quot;{var&quot;#train_rescore#10#37&quot;})(df::DataFrame, df_train::DataFrame, model_name::String)
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:219
  [5] (::var&quot;#proto_train#32&quot;{var&quot;#proto_train#7#33&quot;})(df::DataFrame, df_t::DataFrame, model_name::String; nflds::Int64, args::Base.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nfolds,), Tuple{Int64}}})
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:211
  [6] (::var&quot;#evaluate_model#42&quot;{var&quot;#evaluate_model#13#43&quot;})(paths::String, output::String, dss::String, niter::Int64, model_name::String; nfolds::Int64, args::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:284
  [7] (::var&quot;#global_evaluate#40&quot;{var&quot;#global_evaluate#12#41&quot;})(paths::String, output::String, ds::Vector{String}, itern::Int64, model_name::String; nfolds::Int64, args::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:270
  [8] global_evaluate
    @ ~/exports/10fold_ml_model-nocache-by-iter.jl:267 [inlined]
  [9] main(args::Vector{String})
    @ Main ~/exports/10fold_ml_model-nocache-by-iter.jl:475
 [10] top-level scope
    @ ~/exports/10fold_ml_model-nocache-by-iter.jl:479 
</code></pre>
<p>The error is always related to world age and the corresponding MLJInterface of the model in use.</p>
<p>Please help. I have been trying to find a solution for days.</p>
<p>I'm trying to make predictions. The function in question corresponds to the training and prediction step of my script. I wasn't expecting to error because previous regressor models (linear, lasso, ridge, xgboost) under the MLJ framework worked fine.</p>
","1","Question"
"78579401","","<p>Here is the process of creating the datasets. I saved in both rds and csv format in case any internal data characteristics changes during the saving process:</p>
<pre><code># Save the data frame
saveRDS(train_proc_rds, &quot;train_proc_example1.rds&quot;)
fwrite(train_proc_rds, &quot;train_proc_example2.csv&quot;)


# reload data frame
train_proc_df1 &lt;- readRDS(&quot;train_proc_example1.rds&quot;)
train_proc_df2 &lt;- fread(&quot;train_proc_example2.csv&quot;)

</code></pre>
<p>Here are the links to the two sample data (the sample data are not large - only 23 kb, but may be particular that leads to the error):</p>
<pre>
https://drive.google.com/drive/folders/1ZUSQxp9uycL8nRJXBLHUMRADk2KM6eYG?usp=sharing
</pre>
<p>Here are my codes for running firth's model. There are always warnings of non-convergence and suggested me to increase iteration which does not really help. The bizarre thing is when I run the code it stuck forever but the activity monitor shows 99% CPU usage. Here are the detailed codes:</p>
<pre><code>library(caret)
library(logistf)
library(data.table)


# Define training control
train.control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
                              number = 3, repeats = 3,
                              savePredictions = TRUE,
                              classProbs = TRUE)

# Define the custom model function
firth_model &lt;- list(
  type = &quot;Classification&quot;,
  library = &quot;logistf&quot;,
  loop = NULL,
  parameters = data.frame(parameter = c(&quot;none&quot;), class = c(&quot;character&quot;), label = c(&quot;none&quot;)),
  grid = function(x, y, len = NULL, search = &quot;grid&quot;) {
    data.frame(none = &quot;none&quot;)
  },
  fit = function(x, y, wts, param, lev, last, classProbs, ...) {
    data &lt;- as.data.frame(x)
    data$group &lt;- y
    logistf(group ~ ., data = data, control = logistf.control(maxit = 100), ...)
  },
  predict = function(modelFit, newdata, submodels = NULL) {
    as.factor(ifelse(predict(modelFit, newdata, type = &quot;response&quot;) &gt; 0.5, &quot;AD&quot;, &quot;control&quot;))
  },
  prob = function(modelFit, newdata, submodels = NULL) {
    preds &lt;- predict(modelFit, newdata, type = &quot;response&quot;)
    data.frame(control = 1 - preds, AD = preds)
  }
)

</code></pre>
<p>when I ran model 2 with the second dataset it replicated my issue:</p>
<pre><code>
# Training the model
set.seed(123)
model1 &lt;- train(train_proc_df1[, .SD, .SDcols = !c(&quot;AD&quot;, &quot;group&quot;)],
                            train_proc_df1$group,
                            method = firth_model,
                            trControl = train.control)
print(model1)

</code></pre>
<p>here is the most recent error</p>
<pre><code>model2 &lt;- train(train_proc_df2[, .SD, .SDcols = !c(&quot;AD&quot;, &quot;group&quot;)],
+                 train_proc_df2$group,
+                             method = firth_model,
+                             trControl = train.control)
+ Fold1.Rep1: none=none 
Warning in logistf(group ~ ., data = data, control = logistf.control(maxit = 100),  :
  Nonconverged PL confidence limits: maximum number of iterations for variables: (Intercept), x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24 exceeded. Try to increase the number of iterations by passing 'logistpl.control(maxit=...)' to parameter plcontrol
- Fold1.Rep1: none=none 
+ Fold2.Rep1: none=none 
Warning in logistf(group ~ ., data = data, control = logistf.control(maxit = 100),  :
  Nonconverged PL confidence limits: maximum number of iterations for variables: (Intercept), x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24 exceeded. Try to increase the number of iterations by passing 'logistpl.control(maxit=...)' to parameter plcontrol
- Fold2.Rep1: none=none 
+ Fold3.Rep1: none=none 
Warning in logistf(group ~ ., data = data, control = logistf.control(maxit = 100),  :
  Nonconverged PL confidence limits: maximum number of iterations for variables: (Intercept), x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24 exceeded. Try to increase the number of iterations by passing 'logistpl.control(maxit=...)' to parameter plcontrol
- Fold3.Rep1: none=none 
+ Fold1.Rep2: none=none 
Warning in logistf(group ~ ., data = data, control = logistf.control(maxit = 100),  :
  logistf.fit: Maximum number of iterations for full model exceeded. Try to increase the number of iterations or alter step size by passing 'logistf.control(maxit=..., maxstep=...)' to parameter control

</code></pre>
<p>ignore the warning parts, the model seems to stuck on the 3rd fold forever (and it appears to be running because the activity monitor shows CPU usage of 99%). When I ran model 1, the same issue seemed to persist.</p>
<pre><code>

R.version
               _                           
platform       aarch64-apple-darwin20      
arch           aarch64                     
os             darwin20                    
system         aarch64, darwin20           
status                                     
major          4                           
minor          3.2                         
year           2023                        
month          10                          
day            31                          
svn rev        85441                       
language       R                           
version.string R version 4.3.2 (2023-10-31)
nickname       Eye Holes   
</code></pre>
<p>Here is my package version:</p>
<pre><code>&gt; packageVersion(&quot;caret&quot;)
[1] ‘6.0.94’
&gt; packageVersion(&quot;logistf&quot;)
[1] ‘1.26.0’
&gt; packageVersion(&quot;data.table&quot;)
[1] ‘1.14.10’
</code></pre>
","0","Question"
"78581041","","<p>I wanted to use llama-index locally with ollama and llama3:8b to index utf-8 json file. I dont have a gpu. I use uncharted to convert docs into json. Now If it is not possible to use llama-index locally without GPU I wanted to use hugging face inference API. But I am not certain if it is free.  Can anyone suggest a way?</p>
<p>This is my python code:</p>
<pre>

    from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
    from llama_index.llms.ollama import Ollama
    import json
    from llama_index.core import Settings
    
    
    # Convert the JSON document into LlamaIndex Document objects
    with open('data/UBER_2019.json', 'r',encoding='utf-8') as f:
        json_doc = json.load(f)
    documents = [Document(text=str(doc)) for doc in json_doc]
    
    # Initialize Ollama with the local LLM
    ollama_llm = Ollama(model=""llama3:8b"")
    Settings.llm = ollama_llm
    
    # Create the index using the local LLM
    index = VectorStoreIndex.from_documents(documents)#, llm=ollama_llm)

</pre>
<p>But i keep getting error that there is no OPENAI key. I wanted to use llama2 so that i dont require OPENAI key</p>
<p>Can anyone suggest what i am doing wrong? Also can i use huggingfaceinference API to do indexing of a local json file for free?</p>
","0","Question"
"78581619","","<p>I want to automate all of this:</p>
<ol>
<li>Select an object in an image</li>
<li>Crop my image on this object</li>
<li>Crop to 1:1 aspect ratio, leaving a slight gap around this object</li>
<li>Export my image in JPG format in 800x800px with my object in the center of image and with white background.</li>
</ol>
<p>I'm on win11 64bit</p>
<p>What I did :</p>
<ol>
<li>Installing Python and creating an environment</li>
<li>Installing<code>opencv-python-headless, pillow, numpy, Pytorch</code> for use with <code>CUDA 11.8</code></li>
<li>Clone the repository <code>segment-anything.git</code> and install it with PIP</li>
<li>Download <code>sam_vit_b_01ec64.pth</code></li>
</ol>
<p>Coding the py files like that :</p>
<pre><code>import os
import cv2
import numpy as np
from PIL import Image
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

def load_image(image_path):
    return cv2.imread(image_path)

def save_image(image, path):
    cv2.imwrite(path + '.jpg', image)

def select_object(image):
    sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;sam_vit_b_01ec64.pth&quot;)
    mask_generator = SamAutomaticMaskGenerator(sam)
    masks = mask_generator.generate(image)
    largest_mask = max(masks, key=lambda x: x['area'])
    return largest_mask['segmentation']

def crop_to_object(image, mask):
    x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
    padding = 5
    x = max(0, x - padding)
    y = max(0, y - padding)
    w = min(image.shape[1] - x, w + 2 * padding)
    h = min(image.shape[0] - y, h + 2 * padding)
    
    cropped_image = image[y:y+h, x:x+w]
    return cropped_image

def resize_to_square(image, size=800):
    h, w = image.shape[:2]
    scale = size / max(h, w)
    new_h, new_w = int(h * scale), int(w * scale)
    resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)

    new_image = np.ones((size, size, 3), dtype=np.uint8) * 255

    top = (size - new_h) // 2
    left = (size - new_w) // 2
    bottom = top + new_h
    right = left + new_w

    new_image[top:top+new_h, left:left+new_w] = resized_image

    return new_image

def process_image(image_path, output_path):

    image = load_image(image_path)
    mask = select_object(image)
    cropped_image = crop_to_object(image, mask)
    final_image = resize_to_square(cropped_image, 800)
    save_image(final_image, output_path + '.jpg')

def process_folder(input_folder, output_folder):

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for root, _, files in os.walk(input_folder):
        for filename in files:
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                input_path = os.path.join(root, filename)

                relative_path = os.path.relpath(input_path, input_folder)
                output_path = os.path.join(output_folder, relative_path)
                
                output_dir = os.path.dirname(output_path)
                if not os.path.exists(output_dir):
                    os.makedirs(output_dir)
                
                try:
                    process_image(input_path, output_path)
                    print(f&quot;Processed {input_path}&quot;)
                except Exception as e:
                    print(f&quot;Failed to process {input_path}: {e}&quot;)

if __name__ == &quot;__main__&quot;:
    input_folder = &quot;&quot;
    output_folder = &quot;&quot;
    process_folder(input_folder, output_folder)
</code></pre>
<p>What happen :</p>
<p>I import <a href=""https://i.sstatic.net/JqXqND2C.jpg"" rel=""nofollow noreferrer"">Base Image</a> I want <a href=""https://i.sstatic.net/LhO0eoQd.jpg"" rel=""nofollow noreferrer"">Expected result</a> and I obtain <a href=""https://i.sstatic.net/6EqKsgBM.jpg"" rel=""nofollow noreferrer"">Result</a></p>
<p>There is some different base-&gt;result that I had :</p>
<ul>
<li><a href=""https://i.sstatic.net/1oltPl3L.png"" rel=""nofollow noreferrer"">Base-white bg</a> -&gt; <a href=""https://i.sstatic.net/2fTLe5sM.jpg"" rel=""nofollow noreferrer"">Result</a></li>
<li><a href=""https://i.sstatic.net/zu6vAF5n.png"" rel=""nofollow noreferrer"">Base-nobg</a> -&gt; <a href=""https://i.sstatic.net/53nYe37H.png"" rel=""nofollow noreferrer"">Result</a></li>
</ul>
<p>May anyone help me to understand what I Missed?</p>
<p>Thanks in advance,</p>
<p>Cyril</p>
","0","Question"
"78582076","","<p>I'm new to machine learning and I have been learning gradient descent algorithm. I believe this code uses simultaneous update, even though it looks like sequential update. Since the values of partial derivative are calculated before updating either w or b, that is, from the original w and b, the algorithm thus applied on individual w,b is being applied from original values. Am I wrong?</p>
<pre><code>
dj_dw=((w*x[i]+b-y[i])*x[i])/m
dj_db=(w*x[i]+b-y[i])/m
w=w-a*dj_dw
b=b-a*dj_db
</code></pre>
<p>The language is python3.</p>
<p><code>x</code> and <code>y</code> are training set.</p>
<p><code>w</code> and <code>b</code> are parameters that the algorithm is being applied on.
I'm using gradient descent algorithm for linear regression.</p>
<p><code>dj_dw</code> is the partial differentiation of squared mean error cost function with respect to <code>w</code>. Same goes for <code>dj_db</code>.</p>
<p>Apologies for any errors, I'm new.</p>
<p>I tried cross checking using gemini and chatgpt and they said it was sequential, hence the confusion</p>
","0","Question"
"78583802","","<p>I want to count the total number of pixels for each segmented class, I only need the count for each general objects, like one class for every vehicle, one for every person and so on. For this reason, I'm using semantic segmentation instead of instance segmentation (which would consider each vehicle  or person instance separately).But the output of semantic segmentation in detectron2 does not have binary mask.</p>
<p>I know the output of instance segmentation is binary mask and can get the pixel count using the following code:</p>
<pre><code>masks = output['instances'].pred_masks 
results = torch.sum(torch.flatten(masks, start_dim=1),dim=1)
</code></pre>
<p>This gives the pixel count but considers each vehicle instance separately which I do not want
.
But the output of semantic segmentation is the field 'sem_seg' which contains predicted class probabilities for each general class and not binary mask, how can I go on into getting the pixel count for each class in semantic segmentation?</p>
","-1","Question"
"78584772","","<p>i have pre-defined model and stored in a blob storage. Now i want to register model and deploy it and enable end point within Azure ML. I used below code</p>
<pre><code>from azureml.core import Model
from azureml.core import Workspace

subscription_id = 'mysub
resource_group = 'my_resource_group'
workspace_name = 'my_ws_name'

ws = Workspace(subscription_id, resource_group, workspace_name)

model_path = 'my_model.joblib'

container = 'mycontainer'

model_name = 'my_model_v1'

model = Model.register(workspace=ws,
                       model_name=model_name,
                       model_path=model_path,
                       description=&quot;Test_Model&quot;,
                       tags={'area': &quot;emotion detection&quot;},
                       model_framework=Model.Framework.SCIKITLEARN,
                       model_framework_version='0.24.1',
                       resource_configuration=None,
                       container_registry=None,
                       properties=None,
                       sample_input_dataset=None,
                       sample_output_dataset=None,
                       datasets=None,
                       model_url=f'https://mystorage.blob.core.windows.net/{container}/{model_path}')
print(&quot;Ws object created&quot;)
</code></pre>
<p>but the code return below error</p>
<pre><code>TypeError: register() got an unexpected keyword argument 'container_registry'
</code></pre>
<p>Is any resolution to this or any alternative approach?</p>
","0","Question"
"78585509","","<p>I am trying to run the following code in my azure databricks workbook</p>
<pre><code>import pyspark.ml.feature
from pyspark.ml.feature import Tokenizer,StopWordsRemover
tokenizer = Tokenizer()
</code></pre>
<p>However I am facing this error:</p>
<pre><code>Py4JError: An error occurred while calling 
None.org.apache.spark.ml.feature.Tokenizer. Trace:
py4j.security.Py4JSecurityException: Constructor public 
org.apache.spark.ml.feature.Tokenizer(java.lang.String) is not 
whitelisted.
</code></pre>
<p>Similar errors are coming up for StopWordsRemover and some other functions from pyspark.ml.feature too</p>
<p>Is there a work around to avoid this error so that I can use the same code?</p>
","0","Question"
"78587301","","<p>After researching, I realized that <code>scale_pos_weight</code> is typically calculated as the ratio of the number of negative samples to the number of positive samples in the training data. My dataset has 840 negative samples and 2650 positive samples, so the ratio is 0.32. If my samples were the other way around, I believe <code>scale_pos_weight</code> would be a better approach.</p>
<p>Is it safe to assume that since the ratio is less than 1, it will still balance the classes correctly? Specificity is important in my study, but our primary goal focuses on recall, precision, and F1 score. Could this setting contribute to more false positives by impacting specificity the most?</p>
","1","Question"
"78587419","","<p>I'm exploring a dataset with the goal to find any interesting relationships (there are a bunch of variables of interest and I want to see which features or feature combinations predict them).</p>
<p>As a first approach I succesfully computed a multivariate (several target variables) regression with lasso.</p>
<pre><code>pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Lasso())])

search = GridSearchCV(pipeline,
                      {'model__alpha':np.arange(0.1,10,0.1)},
                      cv = 5, scoring=&quot;neg_mean_squared_error&quot;,verbose=3
                      )
search.fit(X_train,y_train)
search.best_params_
coefficients = search.best_estimator_.named_steps['model'].coef_
importance = np.abs(coefficients)
</code></pre>
<p>Now I want to see the importance of the predictors INCLUDING their feature names, cause <code>importance</code> is just a bunch of numbers.</p>
<p>I thought about creating an array with the column names of the features and targets &amp; print the name + the coefficient but my problem is that I'm not entirely be sure how to ensure correspondence (that the correct names are displayed with the correct coefficients).
Can anyone help me out?</p>
<p>Here some additional info:</p>
<ul>
<li>number of predictors: 26</li>
<li>number of targets: 30</li>
<li>shape of <code>importance</code>: (30, 26)</li>
</ul>
<p>I'm also grateful about any other advice on which importance metrics to use or any suggestions regarding possible analyses.</p>
","0","Question"
"78592454","","<p>From my understanding of <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html</a> and gradient boosting decision trees in general, I assume that for N parameters, the regressor picks a bunch of splits along each parameter, works out how applying this split partitions the data, then decides which split to pick which minimises the loss the most (for a particular quantile).</p>
<p>My question is, how do you decide what parameter values to split at, if your parameters are real? I was expecting to find some sort of parameter to determine how many &quot;equally spaced&quot; splits to make, but I only see one that determines the number of data values you'd need either side of a split to make it valid. Does that mean it somehow works this out backwards?</p>
<p>Thanks</p>
","0","Question"
"78594171","","<p>I understand the zig-zag nature of the cost function when applying gradient descent, but what bothers me is that the cost started out at a low 300 only to increase to 1600 in the end.</p>
<p>The cost function would oscillate between 300 and 4000 to end up at 1600. I thought I should get a number that is 300 or lower. I have tried changing the learning rate and all it does is still take me to 1600. I should get a cost around 300, not one that grows it.</p>
<p>Data:</p>
<pre><code>square_feet = [1661.0, 871.0, 1108.0, 1453.0, 1506.0, 1100.0, 1266.0, 1514.0, 948.0, 1878.0, 1522.0, 931.0, 1475.0, 1177.0, 1844.0, 1469.0, 2155.0, 967.0, 1092.0]
prices = [1350.0, 489.0, 589.0, 539.0, 775.0, 575.0, 749.0, 795.0, 644.9, 590.0, 575.0, 699.0, 999.0, 775.0, 599.0, 599.0, 895.0, 550.0, 849.0]
</code></pre>
<p>Both of those lists are Pandas series in the original code, but have converted them to lists here for clarity.</p>
<p>Main:</p>
<pre><code># Add starting weight and bias
w_init = 5e-1 # Increase in price for every 1 square feet
b_init = 200 # Starting price for the cheapest houses

# Iterations and learning rate for the gradient descent algorithm
iterations = 10000
alpha = 1.0e-6 # Delicate and causes a divergence if it's set too large

w_final, b_final, J_hist, p_hist = gradient_descent(
    square_feet, prices, w_init, b_init, alpha, iterations)

print(f'w: {w_final}, b: {b_final}, Costs: {J_hist}, Weight and Bias: {p_hist}')
</code></pre>
<p>Functions:</p>
<pre><code># Cost Function to determine cumulative error between real and predicted values
def cost_function(x, y, w, b):

    # 1) Number of training examples
    m = x.size
    cost = 0

    # 2) Index the training examples and account for cost per instance
    for i in range(m):
        y_hat = w * x[i] + b      
        cost += (y_hat-y[i])**2
        cost /= 2 * m

    # 3) Return total cost
    return cost

# Compute the gradient, i.e., the scalar that improves accuracy
def gradient_function(x, y, w, b):

    m = x.size

    # Partial derivatives of the cost function with respect to weight and bias
    dj_dw = 0
    dj_db = 0

    for i in range(m):
        y_hat = w * x[i] + b
        dj_dw_i = (y_hat - y[i]) * x[i]
        dj_db_i = (y_hat - y[i])
        dj_db += dj_db_i
        dj_dw += dj_dw_i

    dj_dw /= m
    dj_db /= m

    return dj_dw, dj_db

def gradient_descent(x, y, w_init, b_init, learning_rate, 
    num_iters):
    
    # Used for graphing
    J_history = []
    p_history = []
    b = b_init
    w = w_init

    for i in range(num_iters):
        dj_dw, dj_db = gradient_function(x, y, w, b) # Gradient

        # Update weight, bias
        w -= learning_rate * dj_dw
        b -= learning_rate * dj_db
        
        # Prevents resource exhaustion; unnecessary to store similar costs
        # Past 100000 iterations
        if i &lt; 100000:
            J_history.append(cost_function(x, y, w, b))
            p_history.append([w,b])
    
    return w, b, J_history, p_history
</code></pre>
<p>I'm stumped over this issue.</p>
","2","Question"
"78594832","","<p>I'm facing issues while running an OCR process using Nougat with two different errors for two different users. The errors are related to importing <code>cached_property</code> and an unexpected keyword argument <code>cache_position</code>. Below are the full error messages for both cases:</p>
<p><strong>Error 1: ImportError for <code>cached_property</code></strong></p>
<pre><code>ImportError: cannot import name 'cached_property' from 'nougat.utils' (/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/utils/__init__.py)
OCRing with base model failed on /lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf... trying small model
Traceback (most recent call last):
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/bin/nougat&quot;, line 5, in &lt;module&gt;
    from predict import main
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/predict.py&quot;, line 18, in &lt;module&gt;
    from nougat import NougatModel
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/__init__.py&quot;, line 1, in &lt;module&gt;
    from nougat.app import Nougat
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/app.py&quot;, line 5, in &lt;module&gt;
    from nougat.asgi import serve
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/asgi.py&quot;, line 6, in &lt;module&gt;
    from nougat.context.request import Request
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/context/__init__.py&quot;, line 1, in &lt;module&gt;
    from nougat.context.request import Request
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/context/request.py&quot;, line 8, in &lt;module&gt;
    from nougat.utils import cached_property, File
ImportError: cannot import name 'cached_property' from 'nougat.utils' (/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/utils/__init__.py)
OCRing with small model failed on /lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf
The following files failed OCRing and need manual review:
/lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/putnam_and_beyond.pdf
/lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf
Time taken: 8.497612476348877 seconds, 0.1416268746058146 minutes, 0.00236044791009691 hours
</code></pre>
<p><strong>Error 2: TypeError for <code>cache_position</code> in BARTDecoder</strong></p>
<pre><code>TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
OCRing with base model failed on /lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf... trying small model
/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined]
  0%|                                                 | 0/288 [00:24&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/bin/nougat&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
             ^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/predict.py&quot;, line 167, in main
    model_output = model.inference(
                   ^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/nougat/model.py&quot;, line 592, in inference
    decoder_output = self.decoder.model.generate(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/transformers/generation/utils.py&quot;, line 1758, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/transformers/generation/utils.py&quot;, line 2394, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
OCRing with small model failed on /lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf
The following files failed OCRing and need manual review:
/lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf
Time taken: 65.70022702217102 seconds, 1.0950037837028503 minutes, 0.018250063061714172 hours
</code></pre>
<p><strong>Environment:</strong></p>
<ul>
<li>Python 3.12</li>
<li>Nougat</li>
<li>Torch</li>
<li>Transformers</li>
</ul>
<p><strong>Questions:</strong></p>
<ol>
<li><p><strong>ImportError</strong>: How can I resolve the <code>ImportError: cannot import name 'cached_property' from 'nougat.utils'</code>? What could be causing this issue, and is there an alternative approach to import <code>cached_property</code>?</p>
</li>
<li><p><strong>TypeError</strong>: How can I address the <code>TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'</code>? Is this due to a version mismatch between <code>transformers</code> and <code>torch</code>? How can I ensure compatibility?</p>
</li>
</ol>
<p>Any guidance on these issues would be greatly appreciated! Thank you in advance!</p>
","0","Question"
"78596157","","<p>In my code, I put the labels as the first feature in the first timestep, and the LSTM is unable to learn that the answer is in the first timestemp, almost like it is blind to it.</p>
<p>I ran this test because in my real data, my LSTM thinks some variables in the last timestep have no importance in the prediction, yet a gradient tree on the same data finds this pattern.</p>
<p>What's up with my LSTMs? They seems so poor.</p>
<pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional

test_length = 0.15

test_length = 0.15

#features = np.random.rand(1000000, 20, 20)
features = np.zeros((100000, 40, 40))
labels = np.random.rand(100000)

features[:, 0, 0] = labels.copy()

features_train = features[0:int(len(features)*(1-test_length))]
labels_train = labels[0:int(len(labels)*(1-test_length))]
features_test = features[int(len(features)*(1-test_length)):]
labels_test = labels[int(len(labels)*(1-test_length)):]

model = Sequential([
    LSTM(100, return_sequences=False),
    Dropout(0.2),
    Dense(50, activation='relu'),
    Dropout(0.2),
    Dense(30, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(
    features_train, labels_train,
    epochs=1,
    batch_size=40,
    validation_data=(features_test, labels_test),
    verbose=1
)
</code></pre>
","1","Question"
"78596363","","<p>I am trying to use the Teachable Machines object recognizer from a script in PyCharm, but I keep getting an error. The purpose of this code is to take in an image, and then print what the object is with a confidence score, based on the model. I have downloaded the model and label files and have put them in the same folder as the script. I believe I have downloaded all of the necessary packages, Tensorflow, NumPy, Pillow, Keras. Not sure if it matters, but I am on windows.</p>
<p>This is the code, I copied it directly from Teachable Machine:</p>
<pre><code>from keras.models import load_model  # TensorFlow is required for Keras to work
from PIL import Image, ImageOps  # Install pillow instead of PIL
import numpy as np

# Disable scientific notation for clarity
np.set_printoptions(suppress=True)

# Load the model
model = load_model(&quot;keras_Model.h5&quot;, compile=False)

# Load the labels
class_names = open(&quot;labels.txt&quot;, &quot;r&quot;).readlines()

# Create the array of the right shape to feed into the keras model
# The 'length' or number of images you can put into the array is
# determined by the first position in the shape tuple, in this case 1
data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)

# Replace this with the path to your image
image = Image.open(&quot;&lt;IMAGE_PATH&gt;&quot;).convert(&quot;RGB&quot;)

# resizing the image to be at least 224x224 and then cropping from the center
size = (224, 224)
image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)

# turn the image into a numpy array
image_array = np.asarray(image)

# Normalize the image
normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1

# Load the image into the array
data[0] = normalized_image_array

# Predicts the model
prediction = model.predict(data)
index = np.argmax(prediction)
class_name = class_names[index]
confidence_score = prediction[0][index]

# Print prediction and confidence score
print(&quot;Class:&quot;, class_name[2:], end=&quot;&quot;)
print(&quot;Confidence Score:&quot;, confidence_score)
</code></pre>
<p>I have replaced the &quot;IMAGE_PATH&quot; with the path to an image. I get the following as an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\ops\operation.py&quot;, line 208, in from_config
    return cls(**config)
           ^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\layers\convolutional\depthwise_conv2d.py&quot;, line 118, in __init__
    super().__init__(
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\layers\convolutional\base_depthwise_conv.py&quot;, line 106, in __init__
    super().__init__(
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\layers\layer.py&quot;, line 264, in __init__
    raise ValueError(
ValueError: Unrecognized keyword arguments passed to DepthwiseConv2D: {'groups': 1}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\olive\Downloads\pythonProject2\TESTER.py&quot;, line 9, in &lt;module&gt;
    model = load_model(&quot;keras_model.h5&quot;, compile=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\saving\saving_api.py&quot;, line 183, in load_model
    return legacy_h5_format.load_model_from_hdf5(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\legacy_h5_format.py&quot;, line 133, in load_model_from_hdf5
    model = saving_utils.model_from_config(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\saving_utils.py&quot;, line 85, in model_from_config
    return serialization.deserialize_keras_object(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\serialization.py&quot;, line 495, in deserialize_keras_object
    deserialized_obj = cls.from_config(
                       ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\models\sequential.py&quot;, line 333, in from_config
    layer = saving_utils.model_from_config(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\saving_utils.py&quot;, line 85, in model_from_config
    return serialization.deserialize_keras_object(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\serialization.py&quot;, line 495, in deserialize_keras_object
    deserialized_obj = cls.from_config(
                       ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\models\sequential.py&quot;, line 333, in from_config
    layer = saving_utils.model_from_config(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\saving_utils.py&quot;, line 85, in model_from_config
    return serialization.deserialize_keras_object(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\serialization.py&quot;, line 495, in deserialize_keras_object
    deserialized_obj = cls.from_config(
                       ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\models\model.py&quot;, line 517, in from_config
    return functional_from_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\models\functional.py&quot;, line 517, in functional_from_config
    process_layer(layer_data)
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\models\functional.py&quot;, line 497, in process_layer
    layer = saving_utils.model_from_config(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\saving_utils.py&quot;, line 85, in model_from_config
    return serialization.deserialize_keras_object(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\legacy\saving\serialization.py&quot;, line 504, in deserialize_keras_object
    deserialized_obj = cls.from_config(cls_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\olive\Downloads\pythonProject2\.venv\Lib\site-packages\keras\src\ops\operation.py&quot;, line 210, in from_config
    raise TypeError(
TypeError: Error when deserializing class 'DepthwiseConv2D' using config={'name': 'expanded_conv_depthwise', 'trainable': True, 'dtype': 'float32', 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': False, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'depthwise_regularizer': None, 'depthwise_constraint': None}.

Exception encountered: Unrecognized keyword arguments passed to DepthwiseConv2D: {'groups': 1}
</code></pre>
","2","Question"
"78597010","","<p>I have a list of edges with weights (node1 node2 weight) as an input for an undirected graph.</p>
<pre><code>A B 10
A C 9
A D 2
B C 1
</code></pre>
<p>I also have a list of colors as input like below:</p>
<pre><code>Red: 2
Blue: 2
</code></pre>
<p>Let's say we define distance of 2 nodes is the shortest path between nodes A and B is denoted by dist(A,B). Notation c is the list of unique colors. In the example, c=[red, blue]
Sum of distance of pair of nodes within the same group is denoted like this: I<sub>red</sub>,
I<sub>blue</sub>, etc.<br>
Sum of distance of pair of nodes between one group to another is denoted like this: E<sub>red,blue</sub> <br>
After assigning the colors to nodes, the solution is evaluated by the following equation:<br><br>
<a href=""https://i.sstatic.net/xVkS6bqi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xVkS6bqi.png"" alt="""" /></a></p>
<p>If we pick B, C to be Red in the above example and A, D to be blue, the score would be:<br>
I<sub>red</sub>=1<br>
I<sub>red</sub>=2<br>
E<sub>red,blue</sub>=dist(A,B)+dist(A,C)+dist(D,B)+dist(D,C)=10+9+12+11=42<br>
Score=4/5(1+2)+1/5(42)=2.4+8.4=10.8<br></p>
<p>I'm looking for an algorithm that assigns colors to nodes in a way, that the calculated score is minimal. I believe finding global optimum is not possible with polynomial complexity, but the algorithm should find a good approximation at least.</p>
","2","Question"
"78597865","","<p>I have a complaint during the model train that may be caused by my code syntax. I want to create a mushroom image classification model that will produce 2 outputs to appear on mobile apps (tflite deploy to mobile apps). <strong>I created 2 classes for the output where there are types of mushrooms, namely Edible and Non-Edible Mushroom and also the name of the mushroom type which has 20 names of mushroom type names</strong>. In the code below, I used mobilnetV2 transfer learning :</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input
from tensorflow.keras.models import Model

# Load the MobileNetV2 model with pre-trained weights
IMG_SHAPE = (224, 224, 3)
base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)

# Freeze the base model
base_model.trainable = False

# Add custom layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)

# Output layer for classifying mushroom type (20 classes)
mushroom_class = Dense(20, activation='softmax', name='mushroom_class')(x)

# Output layer for classifying edibility (2 classes)
edibility_output = Dense(2, activation='softmax', name='edibility_output')(x)
# edibility_output = Dense(2, activation='softmax', name='edibility_output')(x)

# Create the full model with two outputs
model = Model(inputs=base_model.input, outputs=[edibility_output, mushroom_class])

# Summary of the model
model.summary()
</code></pre>
<pre><code>from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy
# Compile the model
model.compile(optimizer=Adam(),
              loss={'edibility_output': 'binary_crossentropy', 'mushroom_class': 'categorical_crossentropy'},
              metrics={'mushroom_class': 'accuracy', 'edibility_output': 'accuracy'})
</code></pre>
<p>I also created a custom generator to classify the types of mushrooms and the names of the types of mushrooms.</p>
<pre><code>def custom_generator(generator):
    while True:
        x, y = generator.next()
        # Split labels into two parts: 1-class edible and 20-class mushroom types
        edible_labels = y[:, :2]  # Assuming the first column is edible labels
        mushroom_labels = y[:, 0:20]  # The remaining columns are mushroom type labels
        yield x, {'edibility_output': edible_labels, 'mushroom_class': mushroom_labels}

train_generator_custom = custom_generator(train_generator)
validation_generator_custom = custom_generator(validation_generator)
</code></pre>
<p>For the folder directory place, more or less like the image below:
<a href=""https://i.sstatic.net/QsGcr66n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QsGcr66n.png"" alt=""Edible Mushroom Folder"" /></a>
<a href=""https://i.sstatic.net/fNOu4G6t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fNOu4G6t.png"" alt=""Non-Edible Mushroom Folder"" /></a></p>
<p>I also declared ImageDataGenerator :</p>
<pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Path to your training and validation directories
train_dir = 'mushroom3/MO_95/mushroom_dataset/train'
validation_dir = 'mushroom3/MO_95/mushroom_dataset/test'

train_ediblemushroom_dir = os.path.join(train_dir, 'Edible')
train_inediblemushroom_dir = os.path.join(train_dir, 'Non-Edible')

validation_ediblemushroom_dir = os.path.join(validation_dir, 'Edible')
validation_inediblemushroom_dir = os.path.join(validation_dir, 'Non-Edible')

# Define ImageDataGenerator for training data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# validation_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen = ImageDataGenerator(rescale=1./255)

# Generate batches of augmented data from the directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=20,
    class_mode='categorical'  # Because you have multiple classes
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(224, 224),
    batch_size=20,
    class_mode='categorical'  # Because you have multiple classes
)
</code></pre>
<p>This is the code syntax for train model :</p>
<pre><code>history = model.fit(
    train_generator_custom,
    steps_per_epoch=len(train_generator),
    epochs=200,
    validation_data=validation_generator_custom,
    validation_steps=len(validation_generator)
)
</code></pre>
<p><strong>When I want to train the model, the following error message appears :</strong></p>
<pre><code>InvalidArgumentError: Graph execution error:
........
........
Node: 'categorical_crossentropy/softmax_cross_entropy_with_logits'
logits and labels must be broadcastable: logits_size=[20,20] labels_size=[20,2]
     [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_8124]
</code></pre>
<p>Can anyone help with the error message in my script code?</p>
<p>I want the mobile apps to produce output for mushroom types and names of mushroom types with tflite after I create tflite.
<a href=""https://i.sstatic.net/Z4tA1vMm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Z4tA1vMm.jpg"" alt=""No output for 2 classes, namely mushroom types and names of mushroom types."" /></a></p>
","0","Question"
"78598596","","<p>I want to train a game that requires user login. Currently, I hard-code the login to use the training account. Each player can only be logged in once, therefore using the --num-envs=x parameter for training will still result in only one environment actually training. Is there a way to access the ID of the current environment, so that I can use different logins for each separate environment? I want to be able to say</p>
<pre><code>playerName = $&quot;player{envId};
</code></pre>
<p>Is there a way to do that?</p>
<p>I looked into the Academy and Communicator implementations and did not find anything useful.</p>
","0","Question"
"78599045","","<p>I'm trying to run a Python script from a PHP script using proc_open. The Python script works fine when run directly from the command line, but raises a UnicodeEncodeError when run from PHP. I suspect the issue might be related to how the output is captured and handled in PHP, but I'm not sure how to fix it.</p>
<p>I'm working on a project where I need to invoke a Python script from a PHP script. The Python script processes some data and prints the results. Here is the PHP code I'm using:</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php

function my_shell_exec($cmd, &amp;$stdout=null, &amp;$stderr=null) {
    $proc = proc_open($cmd,[
        1 =&gt; ['pipe','w'],
        2 =&gt; ['pipe','w'],
    ],$pipes);
    $stdout = stream_get_contents($pipes[1]);
    fclose($pipes[1]);
    $stderr = stream_get_contents($pipes[2]);
    fclose($pipes[2]);
    return proc_close($proc);
}

$output = my_shell_exec('.\\.venv\\Scripts\\activate &amp;&amp; .\\.venv\\Scripts\\python.exe infer.py', $stdout, $stderr);
var_dump($output);
var_dump($stdout);
var_dump($stderr);
exit();
</code></pre>
<p>Output from PHP:</p>
<pre class=""lang-none prettyprint-override""><code>int(1)
string(183) &quot;Init model
Model already exists in keras-model/model-transventricular-v3.keras, skipping model init.
Infer image
(1, 256, 256, 1)
&quot; keras-model\model-transventricular-v3.keras &quot;
&quot;
string(2416) &quot;2024-06-09 22:57:14.361024: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
...
Traceback (most recent call last):
  File &quot;D:\path\to\infer.py&quot;, line 220, in &lt;module&gt;
    infer_image(&quot;input.dat&quot;)
  File &quot;D:\path\to\infer.py&quot;, line 201, in infer_image
    prediction = model.predict(images)
  File &quot;C:\path\to\python\lib\encodings\cp1252.py&quot;, line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 19-38: character maps to &lt;undefined&gt;
</code></pre>
<p>Direct Command Line Execution (Works Fine):</p>
<pre><code>(.venv) D:\path\to\project&gt;.\.venv\Scripts\activate &amp;&amp; .\.venv\Scripts\python.exe infer.py
2024-06-09 22:56:04.593392: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on.
...
Init model
Model already exists in keras-model/model-transventricular-v3.keras, skipping model init.
Infer image
(1, 256, 256, 1)
I0000 00:00:1717948579.475089   18516 service.cc:153]   StreamExecutor device (0): Host, Default Version
2024-06-09 22:56:19.530820: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1717948580.392302   18516 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step

</code></pre>
<p>Python Script (infer.py):</p>
<pre><code>import os
from keras.models import load_model  # type: ignore
# from keras.optimizers import Adam # type: ignore
from dotenv import load_dotenv
import cv2
import numpy as np
import gdown

if __name__ == '__main__':
    if not os.path.exists('input.dat'):
        print(&quot;input.dat not exists&quot;)
        exit(0)

    print(&quot;Init model&quot;)
    init_model()

    print(&quot;Infer image&quot;)
    infer_image(&quot;input.dat&quot;)

def infer_image(image_filepath):
    # Preprocess image
    images = preproces(image_filepath)
    print(images.shape)

    path = os.path.join('keras-model', 'model-transventricular-v3.keras')
    print('&quot;', path, '&quot;')

    # load model
    model = load_model(path)

    # Predict image
    prediction = model.predict(images)
    predictions = np.argmax(prediction, axis=1)

    # further processing
</code></pre>
","0","Question"
"78599128","","<p>I have a document with three attributes: tags, location, and text.</p>
<p>Currently, I am indexing all of them using LangChain/pgvector/embeddings.</p>
<p>I have satisfactory results, but I want to know if there is a better way since I want to find one or more documents with a specific tag and location, but the text can vary drastically while still meaning the same thing. I thought about using embeddings/vector databases for this reason.</p>
<p>Would it also be a case of using RAG (Retrieval-Augmented Generation) to &quot;teach&quot; the LLM about some common abbreviations that it doesn't know?</p>
<pre><code>import pandas as pd

from langchain_core.documents import Document
from langchain_postgres import PGVector
from langchain_postgres.vectorstores import PGVector
from langchain_openai.embeddings import OpenAIEmbeddings

connection = &quot;postgresql+psycopg://langchain:langchain@localhost:5432/langchain&quot;
embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)
collection_name = &quot;notas_v0&quot;

vectorstore = PGVector(
    embeddings=embeddings,
    collection_name=collection_name,
    connection=connection,
    use_jsonb=True,
)


# START INDEX

# df = pd.read_csv(&quot;notes.csv&quot;)
# df = df.dropna()  # .head(10000)
# df[&quot;tags&quot;] = df[&quot;tags&quot;].apply(
#     lambda x: [tag.strip() for tag in x.split(&quot;,&quot;) if tag.strip()]
# )


# long_texts = df[&quot;Texto Longo&quot;].tolist()
# wc = df[&quot;Centro Trabalho Responsável&quot;].tolist()
# notes = df[&quot;Nota&quot;].tolist()
# tags = df[&quot;tags&quot;].tolist()

# documents = list(
#     map(
#         lambda x: Document(
#             page_content=x[0], metadata={&quot;wc&quot;: x[1], &quot;note&quot;: x[2], &quot;tags&quot;: x[3]}
#         ),
#         zip(long_texts, wc, notes, tags),
#     )
# )

# print(
#     [
#         vectorstore.add_documents(documents=documents[i : i + 100])
#         for i in range(0, len(documents), 100)
#     ]
# )
# print(&quot;Done.&quot;)

### END INDEX

### BEGIN QUERY

result = vectorstore.similarity_search_with_relevance_scores(
    &quot;EVTD202301222707&quot;,
    filter={&quot;note&quot;: {&quot;$in&quot;: [&quot;15310116&quot;]}, &quot;tags&quot;: {&quot;$in&quot;: [&quot;abcd&quot;, &quot;xyz&quot;]}},
    k=10, # Limit of results
)

### END QUERY
</code></pre>
","1","Question"
"78603383","","<p>I'm working on some interactive development in an Azure Machine Learning notebook and I'd like to save some data directly from a <code>pandas DataFrame</code> to a <code>csv</code> file in my default connected blob storage account. I'm currently loading some data the following way:</p>
<pre><code>import pandas as pd

uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df = pd.read_csv(uri)
</code></pre>
<p>I have no problem loading this data, but after some basic transformations I'd like to save this data to my storage account. Most, if not all solutions I have found suggest saving this file to a local directory and then uploading this saved file to my storage account. The best solution I have found on this is the following, which uses <code>tmpfile</code> so I don't have to go and delete any 'local' files afterwards:</p>
<pre><code>from azureml.core import Workspace
import tempfile

ws = Workspace.from_config()
datastore = ws.datastores.get(&quot;exampleblobstore&quot;)

with tempfile.TemporaryDirectory() as tmpdir:
    tmpath = f&quot;{tmpdir}/example_file.csv&quot;
    df.to_csv(tmpath)
    datastore.upload_files([tmpath], target_path=&quot;path/to/target.csv&quot;, overwrite=True)
</code></pre>
<p>This is a reasonable solution, but I'm wondering if there is any way I can directly write to my storage account without the need to save the file first. Ideally I'd like to do something as simple as:</p>
<pre><code>target_uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df.to_csv(target_uri)
</code></pre>
<p>After some reading I thought the class <code>AzureMachineLearningFileSystem</code> may allow me to read and write data to my datastore, in a similar way to how I might when developing on a local machine, however, it appears this class will not let me write data, only inspect the 'file system' and read data from it.</p>
","1","Question"
"78604530","","<p>I'm trying to perform a prediction with the Adaboost algorithm using the <code>predict.boosting()</code> function of the <code>adabag</code> package, but an error occurs:</p>
<blockquote>
<p>&quot;Error in predict.boosting(adaboost, train01_new, newmfinal = 9) :
newmfinal must be 1&lt;newmfinal&lt;mfinal&quot;</p>
</blockquote>
<p>Here is the script:</p>
<pre><code>install.packages(&quot;adabag&quot;)
library(&quot;adabag&quot;)
adaboost &lt;- boosting.cv(factor_new ~ RFS +LI+SDI+LDI+DR+DBT+FCT+FII+DITP+ADCG+ADDG+ROA+ROI+ROS+ROE,data = train01_new, boos = TRUE, mfinal = 10, v=5, par=TRUE, control = rpart.control(cp=.001))
predict_adaboost_cv_train &lt;- predict.boosting(adaboost, train01_new, newmfinal = 9)
</code></pre>
<p>I used newmfinal=9 as the error suggests but the error is still here.</p>
","1","Question"
"78605622","","<p>I'm working on a classification task using the XGBoost classifier model. My dataset contains categorical variables, my target classes ('Dropout', 'Enrolled', 'Graduate').</p>
<pre><code>from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=200,
    max_depth=6,  
    learning_rate=0.1,  
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric='mlogloss'  
)

xgb.fit(X_train, y_train)
</code></pre>
<p>I get the following error</p>
<pre><code>ValueError: Invalid classes inferred from unique values of `y`. Expected: [0 1 2], 
got ['Dropout' 'Enrolled' 'Graduate']
</code></pre>
<p>After that, I'm using label encoder techniques; it works fine. But I need ['Dropout' 'Enrolled' 'Graduate'] this categorical in the production section. How I can change this  [0 1 2] to ['Dropout' 'Enrolled' 'Graduate'] after the train XGBClassifier.</p>
","0","Question"
"78606543","","<p>I'm working on fine-tuning LayoutLMv3 using the CORD-v2 dataset. I'm struggling with the data preprocessing part, specifically on how to correctly extract the total amount (TTC) from the images. The examples I've found online seem to use the older CORD dataset, which has a different format. The new CORD-v2 dataset only includes images and ground truth labels.</p>
<p>How to approach this?</p>
<p>I've tried examples from YouTube and Hugging Face but haven't had any success.</p>
","0","Question"
"78610497","","<p>I would like to know how to increase the accuracy score and lower the loss in a multilabel classification problem.</p>
<p>If you look at the sklearn reference, there is a mention of multilabel in Multiclass and multioutput algorithms and I am testing it now.
(<a href=""https://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/multiclass.html</a>)</p>
<p>The sample data had 10 features using make_multilabel_classification in sklearn.datasets, and a dataset was created by modifying n_classes.</p>
<p>When there are two classes in multilabel, it seems that the accuracy and loss are somewhat satisfactory.</p>
<pre><code>from numpy import mean
from numpy import std
from sklearn.datasets import make_multilabel_classification
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, hamming_loss

# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=2, random_state=1)

# summarize dataset shape
print(X.shape, y.shape)
# summarize first few examples
for i in range(10):
 print(X[i], y[i])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
print(scaler.mean_)
print(scaler.var_)

x_train_std = scaler.transform(X_train)
x_test_std = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_std, y_train)

pred = knn.predict(x_test_std)

print(accuracy_score(y_test, pred))
print(hamming_loss(y_test, pred))
</code></pre>
<p>accuracy_score: 0.8345, hamming_loss: 0.08875</p>
<p>However, as the number of classes exceeds 3, the accuracy score gradually decreases and the loss increases.</p>
<pre><code># define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=3, random_state=1)
</code></pre>
<p>n_classes= 3 --&gt; accuracy_score: 0.772, hamming_loss: 0.116</p>
<p>n_classes= 4 --&gt; accuracy_score: 0.4875, hamming_loss: 0.194125</p>
<p>This is also similary when using the RandomForestClassifier algorithm and MLPClassifier algorithm, as shown in Reference, or when using ClassifierChain(estimator=SVC) to use an algorithm that does not support Multilabel classification.</p>
<p>Which hyperparameters should I adjust to improve accuracy?</p>
","-1","Question"
"78610947","","<p>I trained a bert based model that I had been working on for quite sometime now. After the training, I got a few files in the model directory - pytorch_model.bin, training_args.bin, merges.txt, vocab.json. Now I want to test the model by providing an input to the model and examine it's output. But i'm unable to understand how am i supposed to do so.</p>
<p>I tried looking on the internet and was suggested to use Gradio.</p>
","-1","Question"
"78615860","","<p>In a Pytorch gradient descent algorithm, the function</p>
<pre><code>def TShentropy(wf):
    unique_elements, counts = wf.unique(return_counts=True)
    entrsum = 0
    for x in counts:
        p = x/len_a #Calculates probability of x
        entrsum-= p*torch.log2(p) #Shannon's Entropy Formula        
    return entrsum
</code></pre>
<p>uses the method <code>torch.unique()</code> which is breaking the gradient flow. Whenever I switch it to a continuous probability calculation such as <code>torch.softmax()</code> the program runs. However, the formula needs to use a discrete probability mass distribution, which does not work with softmax.</p>
<p>I have tried using <code>torch.nn.functional.one_hot</code> and <code>torch.bincount</code>, both of which gave the same error:</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Is this doomed to fail? Should I try to interpolate the probability function somehow?</p>
","2","Question"
"78616686","","<pre><code>import os
import gym
from gym import spaces
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from tqdm import tqdm
from TTset_main import load_TTset

# Load the dataset
filted_tset_path = &quot;training_path.csv&quot;
filted_vset_path = &quot;validation_path.csv&quot;
TTset_training, TTset_vali, training_data_length = load_TTset(filted_tset_path, filted_vset_path)

# Custom Dataset Class
class VideoDataset(Dataset):
    def __init__(self, data_list):
        self.data_list = data_list
        self.length = sum(len(video[0]) for video in data_list)  # Total number of frames

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        video_idx = 0
        frame_idx = idx
        # Find the corresponding video and frame
        for video, labels in self.data_list:
            if frame_idx &lt; len(video):
                frame = video[frame_idx]
                label = labels[frame_idx]
                return frame, label
            frame_idx -= len(video)
        raise IndexError(&quot;Index out of range&quot;)

# Create DataLoader
batch_size = 1  # One frame at a time
video_dataset = VideoDataset(TTset_training)
data_loader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)

# CNN Model
class CNNModel(nn.Module):
    def __init__(self, num_classes):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64*30*115*115, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool3d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool3d(x, 2)
        x = x.view(-1, 64*30*115*115)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNModel(num_classes=20).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()

# Training with progress bar
total_epochs = 10
for epoch in range(total_epochs):
    training_acu_count = 0  # Reset the accuracy count for each epoch
    for i, (x, y) in tqdm(enumerate(data_loader)):
        model.train()
        x = x.to(device).unsqueeze(0)  # Add batch dimension
        y = y.to(device)
        pred = model(x)
        t_loss = loss_function(pred, y.unsqueeze(0))  # Match dimensions for loss calculation
        t_loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        pred_convert = torch.argmax(pred, 1)
        training_acu_count += (pred_convert == y).sum().item()

    print(f&quot;Epoch {epoch+1}/{total_epochs}, Training Accuracy: {training_acu_count/len(video_dataset):.4f}&quot;)

# Save the model
torch.save(model.state_dict(), &quot;video_classification_model.pth&quot;)

</code></pre>
<p>I'm using the above code to train a RL model that classifies videos. The TTset_training is a dataloader object that contains inputs and targets. Input dimensions are 129x15x3x60x230x230, and target dimensions are 129x15. The video frames are tensors of dimensions 3x60x230x230.
I'm running this code on a 4070Ti, and I'm getting the following error:</p>
<pre><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.43 GiB. GPU
</code></pre>
<p>I've tried multiple tweaks, changing batch size, using a loop to input data individually, but no luck. I've searched online for solutions but all of them recommend changing the size of the frames, which is not possible as I would lose a lot of data.</p>
","1","Question"
"78617059","","<p>I decided to create a computer vision model that detect an object. It takes a real-time image from camera and show rectangles of detected objects. But I dunno how to visualize OpenCV imshow() function.</p>
<p>My model use Yolov8 and I tried to convert it to tflite, but Tensorflow has been updated, so for now it's impossible. So one way to deploy model on mobile app is to send results on the screen. But I don't know Kotlin and for me it's too hard</p>
","-2","Question"
"78619753","","<p>It appears that model.train expects a path to a data.yml file, and that file needs to have a path to the training and validation sets. s3 references do not seem to be actual paths, I see people use data generators to use S3s for training. Does anyone have a tip on how to do that with YOLOv8?</p>
","-2","Question"
"78620402","","<p>When trying to implement the yolo algortithm, one of the steps is to resize each image to (448, 448). But even with the transform applied the Dataloader throw an exception about difference of size in the dataset.</p>
<p>The exact error message: &quot;RuntimeError: each element in list of batch should be of equal size&quot;</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import datasets
from torchvision.transforms import v2, ToTensor

from torch.utils.data import DataLoader

validation_data = datasets.voc.VOCDetection(
    root='.DATA/',
    download=False,
    image_set=&quot;val&quot;,
    transform=v2.Compose([ v2.Resize(size=(448, 448)), ToTensor() ])
)

batch_size = 64

validation_dataloader = DataLoader(validation_data, batch_size=batch_size)

for X, y in validation_dataloader:
    print(f&quot;Shape of X [N, C, H, W]: {X.shape}&quot;)
    break
</code></pre>
","0","Question"
"78622030","","<p>I'm trying to predict using the Essentia discogs_track_embeddings-effnet-bs64 model with ML.NET. I've tried both using tensorflow and onnx, but when I try to predict anything I'm stuck with</p>
<pre><code>Exception thrown: 'System.InvalidOperationException' in Microsoft.ML.Data.dll
An unhandled exception of type 'System.InvalidOperationException' occurred in Microsoft.ML.Data.dll
Splitter/consolidator worker encountered exception while consuming source data
</code></pre>
<p>Currently, I'm using onnx so the rest of this will be stack traces and code from that attempt.</p>
<p>Full Call Stack:</p>
<pre><code>   at Microsoft.ML.Data.DataViewUtils.Splitter.Batch.SetAll(OutPipe[] pipes)
   at Microsoft.ML.Data.DataViewUtils.Splitter.Cursor.MoveNextCore()
   at Microsoft.ML.Data.RootCursorBase.MoveNext()
   at Microsoft.ML.Data.ColumnCursorExtensions.&lt;GetColumnArrayDirect&gt;d__4`1.MoveNext()
   at System.Collections.Generic.List`1..ctor(IEnumerable`1 collection)
   at System.Linq.Enumerable.ToList[TSource](IEnumerable`1 source)
   at Program.&lt;Main&gt;$(String[] args) in Program.cs:line 122
</code></pre>
<p>on the line: <code>var embeddingColumn = transformedData.GetColumn&lt;float[]&gt;(&quot;embeddings&quot;).ToList();</code></p>
<p>onnx loading and predicting code:</p>
<pre><code>Console.WriteLine($&quot;[+] Loading Model&quot;);
var mlContext = new MLContext();

// load melspectrogram data into pipeline
var modelPath = &quot;discogs_track_embeddings-effnet-bs64-1.onnx&quot;;
var pipeline = mlContext.Transforms.ApplyOnnxModel(
    modelFile: modelPath,
    fallbackToCpu: true
);
IDataView mockData = mlContext.Data.LoadFromEnumerable(new List&lt;ModelInput&gt;() { new ModelInput() });
var model = pipeline.Fit(mockData);

var schema = model.Transform(mockData).Schema;
Console.WriteLine(&quot;[*] Model Schema:&quot;);
foreach (var column in schema)
{
    Console.WriteLine($&quot;Column Name: {column.Name}, Column Type: {column.Type}&quot;);
}


List&lt;ModelOutput&gt; allPredictions = new List&lt;ModelOutput&gt;();

foreach (var segment in melSpectrogram)
{
    var seg = MelSpectrogramGenerator.ConvertToFloat(segment);
    
    var data = new ModelInput
    {
        Melspectrogram = seg
    };
    IDataView dataView = mlContext.Data.LoadFromEnumerable(new [] { data });
    var transformedData = model.Transform(dataView);

    // Retrieve the embeddings
    var embeddingColumn = transformedData.GetColumn&lt;float[]&gt;(&quot;embeddings&quot;).ToList();
    foreach (var value in embeddingColumn.First())
    {
        Console.Write($&quot;{value} &quot;);
    }
    //allPredictions.Add(scoredData.);
    Console.WriteLine(&quot;Wheee&quot;);
}


public class ModelInput
{
    [VectorType(64, 128, 96)]
    [ColumnName(&quot;melspectrogram&quot;)]
    public float[,,] Melspectrogram { get; set; }
    public ModelInput()
    {
        Melspectrogram = new float[64, 128, 96];
    }
}

// Define output schema
public class ModelOutput
{
    [VectorType(64, 512)]
    [ColumnName(&quot;embeddings&quot;)]
    public float[,] Embeddings { get; set; }
    public ModelOutput()
    {
        Embeddings = new float[64, 512];
    }
}
</code></pre>
<p>Currently on Microsoft.ML 3.0.1, Microsoft.ML.OnnxRuntime.Managed 1.18.0</p>
<p>I've checked and my data has no NaN in it, and none of my variables are Null. I'm super lost and unsure how to fix this or even move forward with troubleshooting.</p>
","0","Question"
"78623717","","<p>I'm experimenting with attributed node embeddings and the structural embeddings, but the <a href=""https://github.com/benedekrozemberczki/karateclub"" rel=""nofollow noreferrer"">karateclub</a> implementations give back matrices with strange numbers of columns.</p>
<p>MUSAE gives 128 'features' instead of the requested 32.  GLEE gives 33 when I requested 32.  Am I missing something?</p>
<pre class=""lang-py prettyprint-override""><code>import random
import numpy as np
import networkx as nx
from scipy.sparse import coo_matrix

from karateclub.node_embedding.attributed import MUSAE
from karateclub.node_embedding.neighbourhood import GLEE

g = nx.newman_watts_strogatz_graph(50, 10, 0.2)

X = {i: random.sample(range(150),50) for i in range(50)}

row = np.array([k for k, v in X.items() for val in v])
col = np.array([val for k, v in X.items() for val in v])
data = np.ones(50*50)
shape = (50, 150)

X = coo_matrix((data, (row, col)), shape=shape)

model = MUSAE(dimensions=32)
model.fit(g, X)
emb = model.get_embedding()
print(emb.shape)

model = GLEE(dimensions=32)
model.fit(g)
emb = model.get_embedding()
print(emb.shape)
</code></pre>
<p>Output:</p>
<pre><code>(50, 128)
(50, 33)
</code></pre>
","1","Question"
"78625475","","<p>I'm (trying to) learn AI/ML for speech synthesis and trying to undestand how HiFi-GAN is used by Vits.</p>
<p>From my understanding, <a href=""https://github.com/jaywalnut310/vits"" rel=""nofollow noreferrer"">Vits</a> will convert text input into mel spectograms which is then converted to audio waves by <a href=""https://github.com/jik876/hifi-gan"" rel=""nofollow noreferrer"">HiFi-GAN</a>.</p>
<p>What confuses me is why the input sent from Vits to HiFi-GAN is not a mel spectogram.</p>
<p>For example, when I test <em><strong>other</strong></em> models and add the code below to the forward method from HiFi-GAN:</p>
<pre><code>class Generator(torch.nn.Module):
  ...
  def forward(self, x):
    plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
    ...
  ...
</code></pre>
<p>it saves the correct image which looks like a mel spectogram image, however, when I do the same with vits, the saved image is a plain green image which of course is not a representation of a mel spectogram.</p>
<p>But the resulting audio file is of course a valida audio file.</p>
<p>So could anyone explain that to me?</p>
<p>I'm evaluating a few neural tts models and what I wanted to do is save the mel spectogram created by the models to compare them later and also run them through different vocoders to compare them as well.</p>
<p>I noticed that the HiFi-GAN code in the vits repo is slightly different from the original repo but I can't undertand why.</p>
<p>Is there any way I can convert the input param <code>x</code> to the mel spectogram representation without first converting it to audio and then convert the audio to mel?</p>
","0","Question"
"78625589","","<p>I'm learning clustering and had some problems trying to find databases with data
labeled to work with, this was a limitation for me because I found very interesting unlabeled data sets. I have read about various unsupervised clustering techniques and would like to implement hierarchical clustering.</p>
<p>I loaded my data into a pandas DataFrame, standardized the data and applied hierarchical clustering. Then I visualized the dendrogram but I'm not sure how to interpret the results or if I'm using the right parameters.</p>
","-1","Question"
"78626157","","<p>I have trained a <code>detectron2</code> model with one class. Now I want to set everything that is not inside of a bbox to the color white and keep the rest as it is. There can be multiple bboxes on one picture and they can overlay.</p>
<p>I read the <a href=""https://detectron2.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">docs of detectron2</a> as well as <a href=""https://docs.opencv.org/4.x/"" rel=""nofollow noreferrer"">cv2</a> but I was not able to find a solution to my problem.</p>
<p>This is my code for the predictions:</p>
<pre><code>from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
import cv2

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/tmp'))
cfg.MODEL.WEIGHTS = 'tmp'

predictor = DefaultPredictor(cfg)

img = cv2.imread('tmp')

out = predictor(img)
</code></pre>
","0","Question"
"78626417","","<p>I am doing Stepwise selection for feature selection using statsmodels.api as sm  and while running the codes I am getting this error
<code>ValueError: list.remove(x): x not in list</code></p>
<p>for the below piece of code</p>
<pre class=""lang-py prettyprint-override""><code>def stepwise_selection(x, y,
                       initial_list=['discount', 'sla','product_procurement_sla', 'order_payment_type',
       'online_order_perc', 'TV_ads','Sponsorship_ads', 'Content_marketing_ads', 'Online_marketing_ads',
       'NPS', 'Stock_Index', 'Special_sales', 'Payday', 'heat_deg_days', 'cool_deg_days', 
       'total_rain_mm', 'total_snow_cm','snow_on_grnd_cm', 'MA4_listed_price',
       'MA2_discount_offer'],
                       threshold_in=0.01,threshold_out = 0.05, verbose=True):
    
    included = list(initial_list)
    while True:
        changed=False
        ###forward step
        excluded = list(set(x.columns)-set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(x[included+[new_column]]))).fit()
            new_pval[new_column] = model.pvalues[new_column]
        best_pval = new_pval.min()
        if best_pval &lt; threshold_in:
            best_feature = new_pval.argmin()
            included.append(best_feature)
            changed=True
            if verbose:
                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))
                
                
        ###backward step
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(x[included ]))).fit()
        ###use all coefs except intercept
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max() ###null if pvalues is empty
        if worst_pval &gt; threshold_out:
            changed=True
            worst_feature = pvalues.argmax()
            included.remove(worst_feature)
            if verbose:
                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))
        if not changed:
            break
    return included
</code></pre>
<pre><code>import statsmodels.api as sm  

final_features = stepwise_selection(x, y)

print(&quot;\n&quot;,&quot;final_selected_features:&quot;,final_features)
</code></pre>
<p>at line</p>
<pre><code> included.remove(worst_feature)
</code></pre>
<p>I tried using del function, but the expected error is different</p>
","1","Question"
"78626998","","<p>I am using one_hot function from keras to convert my words into numbers. But for some reason it is producing the same number for different words. In the code below you can see 48 is used for &quot;amazing&quot; but 48 is also used for &quot;too&quot;. Why is that?</p>
<pre><code>from tensorflow.keras.preprocessing.text import one_hot

reviews = ['nice food',
        'amazing restaurant',
        'too good',
        'just loved it!',
        'will go again',
        'horrible food',
        'never go there',
        'poor service',
        'poor quality',
        'needs improvement']

# convert to ont hot vector 
encoded_reviews = [one_hot(d, vocab_size) for d in reviews]
</code></pre>
<p>When I print encoded_reviews it shows:</p>
<pre><code>[[13, 12],
 [48, 44],
 [48, 19],
 [38, 28, 46],
 [13, 29, 19],
 [46, 12],
 [19, 29, 4],
 [18, 38],
 [18, 35],
 [42, 7]]
</code></pre>
","0","Question"
"78630853","","<p>Have been learning how to train neural networks a problem was encountered.Here in this code the confusion is how to understand when to do multiplication using '*' and when 'using np.dot()'</p>
<pre><code>def loss_gradients(forward_info: Dict[str, ndarray], 
                   weights: Dict[str, ndarray]) -&gt; Dict[str, ndarray]:
    '''
    Compute the partial derivatives of the loss with respect to each of the parameters in the neural network.
    '''    
    dLdP = -(forward_info['y'] - forward_info['P'])
    
    dPdM2 = np.ones_like(forward_info['M2'])

    dLdM2 = dLdP * dPdM2
  
    dPdB2 = np.ones_like(weights['B2'])

    dLdB2 = (dLdP * dPdB2).sum(axis=0)
    
    dM2dW2 = np.transpose(forward_info['O1'], (1, 0))
    
    dLdW2 = np.dot(dM2dW2, dLdP)

    dM2dO1 = np.transpose(weights['W2'], (1, 0)) 

    dLdO1 = np.dot(dLdM2, dM2dO1)
    
    dO1dN1 = sigmoid(forward_info['N1']) * (1- sigmoid(forward_info['N1']))
    
    dLdN1 = dLdO1 * dO1dN1
    
    dN1dB1 = np.ones_like(weights['B1'])
    
    dN1dM1 = np.ones_like(forward_info['M1'])
    
    dLdB1 = (dLdN1 * dN1dB1).sum(axis=0)
    
    dLdM1 = dLdN1 * dN1dM1
    
    dM1dW1 = np.transpose(forward_info['X'], (1, 0)) 

    dLdW1 = np.dot(dM1dW1, dLdM1)

    loss_gradients: Dict[str, ndarray] = {}
    loss_gradients['W2'] = dLdW2
    loss_gradients['B2'] = dLdB2.sum(axis=0)
    loss_gradients['W1'] = dLdW1
    loss_gradients['B1'] = dLdB1.sum(axis=0)
    
    return loss_gradients
</code></pre>
<p>The thought was that knowing when to use which is essential since the outputs differ</p>
","0","Question"
"78631415","","<p>I am working on reading an Arabic book using python (the pdf is selectable it does not require any OCR (optical character recognition to extract text from images)) so I have used multiple libraries pdfplumber, pdfminer.six and flitz (PyMuPdf)) this is one of the code I have used:</p>
<pre><code>import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
    # Remove NULL bytes and control characters
    cleaned_text = re.sub(r'[\x00-\x1F\x7F]', '', text)
    return cleaned_text

def reshape_and_bidi_text(text):
    # Reshape Arabic text and apply bidi algorithm
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    return bidi_text

def extract_text_from_pdf(pdf_path):
    text = &quot;&quot;
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + &quot;\n&quot;
    return text

def save_text_to_file(text, output_path):
    with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
        text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
    # Extract text from the PDF using pdfplumber
    extracted_text = extract_text_from_pdf(pdf_path)
    
    # Clean the extracted text
    cleaned_text = clean_text(extracted_text)
    
    # Reshape and apply bidi algorithm to the text
    reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)
    
    # Save the cleaned and reshaped text to a text file
    save_text_to_file(reshaped_bidi_text, output_path)
    print(f&quot;Text from {pdf_path} has been saved to {output_path}&quot;)

# Example usage
pdf_path = r'C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf'
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)
</code></pre>
<p>So when using these libraries always I obtain the following output with wrong encoding and I don't know what to use to fix is?</p>
<p>Note: attached above <a href=""https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530"" rel=""nofollow noreferrer"">https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530</a> is the arabic book I try to read</p>
","0","Question"
"78636029","","<p>I have a <code>HistGradientBoostingClassifier</code> model and I want to plot one or more of its decision trees, nevertheless I can't manage to find a native function to do it, I can access the Tree predictor objects and thus it's nodes, but in order to plot it into the <code>sklearn.tree.plot_tree</code> function it needs to be a <code>DecisionTree</code> type object</p>
<p>I tried this:</p>
<pre><code>from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])
</code></pre>
<p>getting this error:</p>
<blockquote>
<p>InvalidParameterError: The 'decision_tree' parameter of plot_tree must
be an instance of 'sklearn.tree._classes.DecisionTreeClassifier' or an
instance of 'sklearn.tree._classes.DecisionTreeRegressor'. Got
&lt;sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
object at 0x7f676ebf0310&gt; instead.</p>
</blockquote>
<p>Note: <code>RF_90</code> is the <code>HistGradientBoostingClassifier</code> fitted model</p>
","1","Question"
"78636296","","<p>I trained a machine learning model, and a few of the same rows (around 100 out of 1800) in my test set are giving wrong predictions at every run (10 times) with different seeds. Is there anything I should do about it e.g. throw it in the train set in exchange for some data from the train set to test set or leave it as it is?</p>
<p>Ran the model 10 times with a different seed. Got some rows getting wrong predictions consistently.</p>
","-2","Question"
"78639577","","<p>I'm trying to implement this paper:
<a href=""https://arxiv.org/pdf/2212.07677"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2212.07677</a></p>
<p>(Here's their code):
<a href=""https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd"" rel=""nofollow noreferrer"">https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd</a></p>
<p>I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually.</p>
<p>As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?</p>
","-1","Question"
"78640262","","<p>So I have a machine learning model that has been trained and there is no overfit or underfit, this model is perfect and can detect objects correctly. However, after that I try what if I train this model again repeatedly and what happens is that overfit occurs, does training a model repeatedly on a model that is already perfect can affect the model? So what was originally normal becomes overfit.</p>
<p>I hope to get an explanation about my case</p>
","-1","Question"
"78643538","","<p>I'm upgrading my stable diffusion from 2-1 to stable-diffusion-3-medium-diffusers</p>
<p>Here is my code which is working for version 2-1</p>
<pre><code># source venv/bin/activate

from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-2-1&quot;)
pipe = pipe.to(&quot;mps&quot;)

pipe.enable_attention_slicing()

print(&quot;Starting Process&quot;)

steps = 200
query = &quot;Stormy Weather in Monte Carlo&quot;

image = pipe(query, num_inference_steps=steps).images[0]

image.save(&quot;oneOffImage.jpg&quot;)

print(&quot;Successfully Created Image as oneOffImage.jpg&quot;)
</code></pre>
<p>I upgraded <code>diffusers</code>, signed up on hugging face for access to the gated repo, created and added the HF_TOKEN to my .env, and ran this code</p>
<pre><code># source venv/bin/activate

from diffusers import StableDiffusion3Pipeline
from dotenv import load_dotenv
import os

load_dotenv()

print(&quot;Starting Process&quot;)

pipe = StableDiffusion3Pipeline.from_pretrained(&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;)
pipe = pipe.to(&quot;mps&quot;)

# pipe.set_progress_bar_config(disable=True)
pipe.enable_attention_slicing()

print(&quot;Starting Process&quot;)

steps = 200
query = &quot;Stormy Weather in Monte Carlo&quot;

image = pipe(query, num_inference_steps=steps).images[0]

image.save(&quot;oneOffImage.jpg&quot;)

print(&quot;Successfully Created Image as oneOffImage.jpg&quot;)
</code></pre>
<p>I was able to download the model, also I logged the token and confirmed it's in the env vars, I tried adding torch and setting <code>, torch_dtype=torch.float16)</code> but that did nothing plus I think thats for cuda, I also tried adding an auth tag but that did nothing, I upgraded my transformers but I don't even thing that did anything. I'm running out of ideas.</p>
<p>Here is the current error</p>
<pre><code>(venv) mikeland@mikes-mac-mini WeatherWindow % python3 oneOffGenStableDiffusion.py 
/Users/mikeland/WeatherWindow/venv/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
  deprecate(&quot;Transformer2DModelOutput&quot;, &quot;1.0.0&quot;, deprecation_message)
Starting Process
Loading pipeline components...:   0%|                                                                                  | 0/9 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/Users/mikeland/WeatherWindow/oneOffGenStableDiffusion.py&quot;, line 15, in &lt;module&gt;
    pipe = StableDiffusion3Pipeline.from_pretrained(&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;)
  File &quot;/Users/mikeland/WeatherWindow/venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py&quot;, line 114, in _inner_fn
    return fn(*args, **kwargs)
  File &quot;/Users/mikeland/WeatherWindow/venv/lib/python3.9/site-packages/diffusers/pipelines/pipeline_utils.py&quot;, line 881, in from_pretrained
    loaded_sub_model = load_sub_model(
  File &quot;/Users/mikeland/WeatherWindow/venv/lib/python3.9/site-packages/diffusers/pipelines/pipeline_loading_utils.py&quot;, line 703, in load_sub_model
    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)
  File &quot;/Users/mikeland/WeatherWindow/venv/lib/python3.9/site-packages/transformers/modeling_utils.py&quot;, line 3122, in from_pretrained
    raise ImportError(
ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`
</code></pre>
<p>I'm just looking for ANY example I can get to work on Mac at this point.</p>
","0","Question"
"78643575","","<p>I'm working in a notebook in Azure Machine Learning Studio and I'm using the following code block to instantiate a job using the <a href=""https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml?view=azure-python#azure-ai-ml-command"" rel=""nofollow noreferrer"">command function</a>.</p>
<pre><code>from azure.ai.ml import command, Input, Output
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes

subscription_id = &quot;&lt;subscription_id&gt;&quot;
resource_group = &quot;&lt;resource_group&gt;&quot;
workspace = &quot;&lt;workspace&gt;&quot;
storage_account = &quot;&lt;storage_account&gt;&quot;
input_path = &quot;&lt;input_path&gt;&quot;
output_path = &quot;&lt;output_path&gt;&quot;

input_dict = {
    &quot;input_data_object&quot;: Input(
        type=AssetTypes.URI_FILE, 
        path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{input_path}&quot;
    )
}

output_dict = {
    &quot;output_folder_object&quot;: Output(
        type=AssetTypes.URI_FOLDER,
        path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{output_path}&quot;,
    )
}

job = command(
    code=&quot;./src&quot;, 
    command=&quot;python 01_read_write_data.py -v --input_data=${{inputs.input_data_object}} --output_folder=${{outputs.output_folder_object}}&quot;,
    inputs=input_dict,
    outputs=output_dict,
    environment=&quot;&lt;asset_env&gt;&quot;,
    compute=&quot;&lt;compute_cluster&gt;&quot;,
)

returned_job = ml_client.create_or_update(job)
</code></pre>
<p>This runs successfully but with each run, if the code stored within the <code>./src</code> directory changes then a new copy is uploaded to the default blob storage account. I don't mind this, but with each run, the code is uploaded to a new container at the root of my blob storage account. Therefore my default storage account is getting cluttered with containers. I've read the docs for instantiating a <code>command</code> object using the <code>command()</code> function, but I see no parameter available to control where my <code>./src</code> code gets uploaded. Is there any way to control this?</p>
","1","Question"
"78645720","","<p>I have a dataset of the handwritten words - &quot;TRUE&quot;, &quot;FALSE&quot; and &quot;NONE&quot;.I want my model to recognize the words and put them in the correct class. I created a simple CNN model copying the TinyVGG architecture. However, the loss and accuracy values are the same for all epochs. I tried changing the batch size, epochs and learning rate and it's not working. What changes should I make to the model?</p>
<pre class=""lang-py prettyprint-override""><code>class CUSTOMDATASETV2(nn.Module):
  
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
        super().__init__()
        self.block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape, 
                      out_channels=hidden_units, 
                      kernel_size=3, # how big is the square that's going over the image?
                      stride=1, # default
                      padding=1),# options = &quot;valid&quot; (no padding) or &quot;same&quot; (output has same shape as input) or int for specific number 
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, 
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,
                         stride=2) # default stride value is same as kernel_size
        )
        self.block_2 = nn.Sequential(
            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            # Where did this in_features shape come from? 
            # It's because each layer of our network compresses and changes the shape of our inputs data.
            nn.Linear(in_features=hidden_units*16*16, 
                      out_features=output_shape)
        )
    
    def forward(self, x: torch.Tensor):
        x = self.block_1(x)
        # print(x.shape)
        x = self.block_2(x)
        # print(x.shape)
        x = self.classifier(x)
        # print(x.shape)
        return x

torch.manual_seed(42)
model_2 = CUSTOMDATASETV2(input_shape=3, 
    hidden_units=10, 
    output_shape=len(class_names)).to(device)
model_2

</code></pre>
<pre class=""lang-py prettyprint-override""><code>
from tqdm.auto import tqdm
torch.manual_seed(42)

# Measure time
from timeit import default_timer as timer
train_time_start_model_2 = timer()

# Train and test model 
epochs = 10
for epoch in tqdm(range(epochs)):
    print(f&quot;Epoch: {epoch}\n---------&quot;)
    train_step(data_loader=train_dataloader, 
        model=model_2, 
        loss_fn=loss_fn,
        optimizer=optimizer,
        accuracy_fn=accuracy_fn,
        device=device
    )
    test_step(data_loader=test_dataloader,
        model=model_2,
        loss_fn=loss_fn,
        accuracy_fn=accuracy_fn,
        device=device
    )

train_time_end_model_2 = timer()
total_train_time_model_2 = print_train_time(start=train_time_start_model_2,
                                           end=train_time_end_model_2,
                                           device=device)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def train_step(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               accuracy_fn,
               device: torch.device = device):
    train_loss, train_acc = 0, 0
    model.to(device)
    for batch, (X, y) in enumerate(data_loader):
        # Send data to GPU
        X, y = X.to(device), y.to(device)

        # 1. Forward pass
        y_pred = model(X)
        y_pred_prob = torch.softmax(y_pred, dim=1).argmax(dim=1)

        # 2. Calculate loss
        loss = loss_fn(y_pred, y)
        train_loss += loss
        train_acc += accuracy_fn(y_true=y,
                                 y_pred=y_pred_prob) # Go from logits -&gt; pred labels

        # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()

    # Calculate loss and accuracy per epoch and print out what's happening
    train_loss /= len(data_loader)
    train_acc /= len(data_loader)
    print(f&quot;Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%&quot;)

def test_step(data_loader: torch.utils.data.DataLoader,
              model: torch.nn.Module,
              loss_fn: torch.nn.Module,
              accuracy_fn,
              device: torch.device = device):
    test_loss, test_acc = 0, 0
    model.to(device)
    model.eval() # put model in eval mode
    # Turn on inference context manager
    with torch.inference_mode():
        for X, y in data_loader:
            # Send data to GPU
            X, y = X.to(device), y.to(device)

            # 1. Forward pass
            test_pred = model(X)
            test_pred_prob = torch.softmax(test_pred, dim=1).argmax(dim=1)

            # 2. Calculate loss and accuracy
            test_loss += loss_fn(test_pred, y)
            test_acc += accuracy_fn(y_true=y,
                y_pred=test_pred_prob # Go from logits -&gt; pred labels
            )

        # Adjust metrics and print out
        test_loss /= len(data_loader)
        test_acc /= len(data_loader)
        print(f&quot;Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\n&quot;)
</code></pre>
","0","Question"
"78649554","","<p>I have a very simple dataset of employee age and years of experience as features and income as label. The ask is to predict the income level using various regressors and I am using 4: Decision Trees (DT), Random Forest (RF), K-Nearest Neighbor (KNN) and Linear Regression (LR). The following table gives the income level as predicted by each of the 4 regressors.</p>
<p>My questions are:</p>
<ol>
<li>Why is DT predicting such tight income levels, (almost equal to the actual income of the data set) while other regressors seem to be working fine? Is it because the data is too small for the DT to train / the number of features too low (only 2 features) or is it something else?</li>
<li>How come DT gives absurd results but RF does not?</li>
<li>The R^2 and MSE for DT is less than and more than that for LR, respectively, but still, DT seems to be producing more tighter range. How?</li>
</ol>
<p><a href=""https://i.sstatic.net/OlZpnzk1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OlZpnzk1.png"" alt=""Prediction Results"" /></a></p>
<p><a href=""https://i.sstatic.net/WDnuLmwX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WDnuLmwX.png"" alt=""Model Evaluation Results"" /></a></p>
<p>Not including any code here as this is more of a contextual question. Can provide code if needed.</p>
","-1","Question"
"78649611","","<p>I trained a YOLO-V8 instance segmentation model to segment an object with class label 0. I used the CLI to instantiate the trained model and predict on the test data.</p>
<pre><code>!yolo task=segment mode=predict model='/weights/best.pt' conf=0.25 source='/test/images' imgsz=1024 save=True save_txt=True save_conf=True
</code></pre>
<p>After prediction, the label files gets stored in .txt format. These label files contain the class index followed by the polygonal coordinates and finally the confidence score of the bounding box predictions. But, bounding box coordinates, that is, x-center, y-center, width, height are not included in the label file. I would also like to include these bounding box coordinates to each of the labels file since I would like to use these bounding box coordinates later for post-processing. A sample label file content looks like this:</p>
<pre><code>0 0.21582 0.0898438 0.214844 0.0908203 0.213867 0.0908203 0.210938 0.09375 0.210938 0.0947266 0.203125 0.102539 0.203125 0.103516 0.201172 0.105469 0.200195 0.105469 0.199219 0.106445 0.199219 0.113281 0.200195 0.114258 0.200195 0.115234 0.203125 0.115234 0.204102 0.116211 0.223633 0.116211 0.224609 0.117188 0.227539 0.117188 0.228516 0.118164 0.230469 0.118164 0.231445 0.119141 0.234375 0.119141 0.235352 0.120117 0.248047 0.120117 0.249023 0.121094 0.251953 0.121094 0.25293 0.12207 0.254883 0.0927734 0.260742 0.0917969 0.256836 0.0917969 0.255859 0.0908203 0.233398 0.0908203 0.232422 0.0898438 0.910849
</code></pre>
<p>I am not saving the predictions to any 'result' variable here and I am running the predictions only in the CLI.</p>
","0","Question"
"78649700","","<p>I have made a Custom Keras model for Machine Translation. After training and saving the model as tf_model.keras , during loading I am getting an error. I have added get_config() function and also added @keras.saving.register_keras_serializable() at the top of the class.</p>
<p>Code for the Model :</p>
<pre><code>@keras.saving.register_keras_serializable()
class BidirectionalEncoderandDecoderWithAttention(keras.Model):
  def __init__(
      self,
      input_vocabulary_size = 5000,
      output_vocabulary_size=5000,
      embedding_size = 256,
      units_lstm =512,
      **kwargs):
    super().__init__(**kwargs)
    self.input_vocabulary_size = input_vocabulary_size
    self.output_vocabulary_size = output_vocabulary_size
    self.embedding_size = embedding_size
    self.units_lstm = units_lstm
    self.encoder_embedding = Embedding(self.input_vocabulary_size,self.embedding_size,mask_zero = True)
    self.encoder = Bidirectional(LSTM(self.units_lstm//2,return_sequences = True,return_state = True))
    self.decoder_embedding = Embedding(self.output_vocabulary_size,self.embedding_size,mask_zero = True)
    self.decoder = LSTM(self.units_lstm,return_sequences = True)
    self.attention = Attention()
    self.output_layer = Dense(self.output_vocabulary_size,activation = 'softmax')
  def call(self,inputs):
      encoder_inputs,decoder_inputs = inputs
      encoder_embeddings = self.encoder_embedding(encoder_inputs)
      decoder_embeddings = self.decoder_embedding(decoder_inputs)

      encoder_op,*encoder_state = self.encoder(encoder_embeddings)
      encoder_state = [
            tf.concat(encoder_state[0::2], axis=-1),
            tf.concat(encoder_state[1::2], axis=-1),
        ]
      decoder_op = self.decoder(decoder_embeddings,initial_state = encoder_state)
      attention_output = self.attention([decoder_op,encoder_op])
      output = self.output_layer(attention_output)
      return output
  def get_config(self):
        config = super().get_config()
        config.update ({
            'input_vocabulary_size':self.input_vocabulary_size,
            'output_vocabulary_size':self.output_vocabulary_size,
            'embedding_size':self.embedding_size,
            'units_lstm':self.units_lstm,
            'encoder_embedding':self.encoder_embedding,
            'encoder':self.encoder,
            'decoder_embedding' : self.decoder_embedding,
            'decoder': self.decoder,
            'attention':self.attention,
            'output_layer':self.output_layer
            })
        return config

</code></pre>
<p>Code for compilation and fitting :</p>
<pre><code>def adapts_compile_fit(model,
                       train_data,
                       valid_data,
                       n_epochs = 40,
                       n_patience = 20,
                       init_lr = 0.001,
                       lr_decay_rate = 0.1):

  # early_stopping = keras.callbacks.EarlyStopping(monitor=&quot;val_accuracy&quot;,patience = n_patience,restore_best_weights = True)
  n_decay_steps = n_epochs * len(list(train_ds))
  scheduled_lr = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=init_lr,
        decay_steps=n_decay_steps,
        decay_rate=lr_decay_rate)
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer = keras.optimizers.RMSprop(learning_rate = scheduled_lr),
                metrics = ['accuracy'])
  return model.fit(train_ds,
                   epochs = n_epochs,
                   validation_data = valid_ds
                   ,verbose=1)

</code></pre>
<p>I have saved it using <code>model.save(&quot;/content/tf.keras&quot;)</code> and loading the model as <code>my_model_2 = keras.models.load_model(&quot;/content/tf.keras&quot;,custom_objects={'BidirectionalEncoderandDecoderWithAttention':BidirectionalEncoderandDecoderWithAttention})</code> and getting following error :</p>
<blockquote>
<p>TypeError: Unable to revive model from config. When overriding the <code>get_config()</code> method, make sure that the returned config contains all items used as arguments in the  constructor to &lt;class '<strong>main</strong>.BidirectionalEncoderandDecoderWithAttention'&gt;, which is the default behavior. You can override this default behavior by defining a <code>from_config(cls, config)</code> class method to specify how to create an instance of BidirectionalEncoderandDecoderWithAttention from its config.</p>
<p>Received config={'input_vocabulary_size': 14617, 'output_vocabulary_size': 29604, 'embedding_size': 256, 'units_lstm': 512, 'encoder_embedding': {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_6', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': [None, None], 'input_dim': 14617, 'output_dim': 256, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': True, 'input_length': None}, 'registered_name': None, 'build_config': {'input_shape': [None, None]}}, 'encoder': {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': True, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None, 'shared_object_id': 138063236840848}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'Orthogonal', 'config': {'gain':...</p>
<p>Error encountered during deserialization: ('Keyword argument not understood:', 'encoder_embedding')</p>
</blockquote>
<p>I have added all the parameters in config file but the error is still there.</p>
","0","Question"
"78657727","","<p>I have a medical project that want extract a special part (Conjuctiva Puplpebral)
<img src=""https://i.sstatic.net/9QhRzjbK.png"" alt=""my just wanted region"" /></p>
<p>to extract of eye image without understanding of its coordinates automatically not manually also the coordinate of this wanted region changing because I capture from many patients and I think must find the shape of it. my goal is determining anemia from non anemia by counting red pixel in conjuctiva pulpubral. I use masking method(k means) to do that, but I wish could do a way that first extract conjuctiva pulpebral directly, then use k means to mask the image and find because my result will be more accurate. When I use k mean from image segmentation, I find another and overlapping red pixel that ruin my accuracy.</p>
<p><img src=""https://i.sstatic.net/339DLElD.png"" alt=""k_means"" />. I also hear about machine learning to use but but after doing machine learning to find near region in images of my patients, then I will need to extract conjuctival pulpabral. So I need to codes to extract only and only conjuctival pulpubral.</p>
<p>I try k_means and kernel but add another unwanted red pixel. I hear about <em>instance segmentation</em> and <em>MASK RCNN</em>. you assume I have my just wanted region as see its image above as data for CNN so how use it for my project.</p>
<pre><code>import cv2
import numpy as np

# Read the image
image = cv2.imread('c:/users/stk/desktop/d.png')

# Convert the image to HSV
hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# Define the lower and upper bounds for the red color
lower_red = np.array([0, 120, 70])
upper_red = np.array([10, 255, 255])

# Create a mask for the red color
mask1 = cv2.inRange(hsv, lower_red, upper_red)

# Define the lower and upper bounds for the red color
lower_red = np.array([170, 120, 70])
upper_red = np.array([180, 255, 255])

# Create a mask for the red color
mask2 = cv2.inRange(hsv, lower_red, upper_red)

# Combine the two masks
mask = mask1 + mask2

# Create a kernel for morphological operations
kernal = np.ones((5, 5), np.uint8)

# Perform morphological operations
mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernal)
mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernal)

# Apply the mask to the original image
result = cv2.bitwise_and(image, image, mask = mask)

# Save the result
cv2.imwrite('extracted_red_object.png', result)

# Display the result
cv2.imshow('EXTRACTED RED OBJECT', result)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>
","0","Question"
"78658886","","<p>The following code simulates a simple TFX pipeline ingesting a CSV file and converting it to TFRecord.</p>
<p>You can also see the corresponding notebook: <a href=""https://colab.research.google.com/drive/1GEytZjnNZZ7r_f9QQ9FbauohKNLGSooC?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1GEytZjnNZZ7r_f9QQ9FbauohKNLGSooC?usp=sharing</a></p>
<pre class=""lang-py prettyprint-override""><code>output_config = example_gen_pb2.Output(split_config=
    example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)
    ])
)

example_gen = CsvExampleGen(
    input_base='data',
    output_config=output_config
)

pipeline_root = 'artifacts'

pipeline = Pipeline(
    pipeline_name='testing pipeline',
    pipeline_root=pipeline_root,
    components=[example_gen],
    enable_cache=True,
    metadata_connection_config=metadata.sqlite_metadata_connection_config(
        os.path.join('artifacts', 'metadata.sqlite')
    )
)

LocalDagRunner().run(pipeline)
</code></pre>
<p>I have manually verified that the TFRecord have been properly generated. However, the pipeline's outputs dictionaries is empty.</p>
<pre class=""lang-py prettyprint-override""><code>print(pipeline.outputs)
# output: {}
print(example_gen.outputs['examples'].get())
# output: []
</code></pre>
<p>This problem persists in both a .ipynb notebook and .py file.</p>
<p>Interestingly enough, <code>InteractiveContext</code> does not have this problem.</p>
<p>What is causing this?</p>
","2","Question"
"78663805","","<p>I am fairly new to mlflow. I stumbled across some strange behavior. When I am running a simple Keras model fit with MLFlow Autolog like this:</p>
<pre><code>mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;keras_log&quot;)
mlflow.tensorflow.autolog()

# Define the parameters.
num_epochs = 10
batch_size = 256

# Train the model.
history = model.fit(X_train,
                      y_train,
                      epochs=num_epochs,
                      batch_size=batch_size,
                      validation_data=(X_test, y_test))
mlflow.end_run()
</code></pre>
<p>This produces the expected behavior. I can see the model in artifacts and metrics.</p>
<p><a href=""https://i.sstatic.net/oTlcS9WA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTlcS9WA.png"" alt=""enter image description here"" /></a></p>
<p>However, when I autolog along with custom figure containing the training accuracy and loss , the artifacts contain only the custom images. As if autolog didn't work at all.</p>
<pre><code>mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;keras_log&quot;)
mlflow.tensorflow.autolog()

# Define the parameters.
num_epochs = 10
batch_size = 256
    
# Train the model.
history = model.fit(X_train,
                    y_train,
                    epochs=num_epochs,
                    batch_size=batch_size,
                    validation_data=(X_test, y_test))

##_________ Problematic Part
fig, ax = plt.subplots(1,2,figsize=(10,4))
ax[0].plot(history.history['accuracy'], label='Accuracy' )
ax[0].plot(history.history['val_accuracy'], label='Val Accuracy' )
ax[0].set_title('Accuracy')
ax[0].legend(loc='best')
ax[1].plot(history.history['loss'], label='Loss' )
ax[1].plot(history.history['val_loss'], label='Val Loss' )
ax[1].set_title('Loss')
ax[1].legend(loc='best')
mlflow.log_figure(fig,'training_history.png')
# _________

mlflow.end_run()
</code></pre>
<p>The model artifacts aren't there. The metrics also aren't recorded.</p>
<p>Am I missing something straightforward?</p>
<p><a href=""https://i.sstatic.net/2oRZZfM6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2oRZZfM6.png"" alt=""enter image description here"" /></a>
Please help.</p>
","1","Question"
"78664794","","<p>I am training a Machine Learning model for STS task using the Sentence Transformers library.
When I was testing it, I noticed that my model generated different results for the same number of epochs depending on the number of epochs I provided to the SentenceTransformerTrainingArguments function.
As an example, I tested the model using 2 and 4 epochs, capturing the loss on the development/validation set and the Pearson correlation coefficient. Here are the results:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>EpochTotal</th>
<th>EpochParcial</th>
<th>EvalLoss</th>
<th>Pearson</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>1</td>
<td>2.7101428508758545</td>
<td>0.8982</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2.676791191101074</td>
<td>0.9186</td>
</tr>
<tr>
<td>3</td>
<td>4</td>
<td>1</td>
<td>2.734797716140747</td>
<td>0.8934</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>2</td>
<td>2.613370180130005</td>
<td>0.9252</td>
</tr>
<tr>
<td>5</td>
<td>4</td>
<td>4</td>
<td>2.509316921234131</td>
<td>0.9404</td>
</tr>
</tbody>
</table></div>
<p>In the table, the EpochTotal column represents the number of epochs my model was trained for, and the EpochPartial column refers to the epoch in which the results were collected.
In the example, the results for Id's 1 and 3 should be the same, since I ran them for a single season (partial epoch). The same goes for Id's 2 and 4.
To make sure that the values ​​were not generated randomly due to the model initialization, I tested with the same parameters twice for each epoch, and found that they were not. Checking the bib, I found that it used seed = 42.
Here is the code I am using:</p>
<pre><code>import logging
import sys
import traceback
import json
from datetime import datetime
from datasets import load_dataset, Dataset
from sentence_transformers import SentenceTransformer, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.similarity_functions import SimilarityFunction
from sentence_transformers.trainer import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments
from transformers import EarlyStoppingCallback
from sklearn.metrics.pairwise import paired_cosine_distances
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr, spearmanr

model_name = &quot;neuralmind/bert-large-portuguese-cased&quot;
output_dir = &quot;OutputModel&quot;
dataset    = &quot;assin2&quot;

train_key = 'train'
valll_key = 'validation'
testt_key = 'test'

sent1_key = 'premise'
sent2_key = 'hypothesis'
label_key = 'relatedness_score'

logging.basicConfig(format=&quot;%(asctime)s - %(message)s&quot;, datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;, level=logging.INFO)

num_epochs = 4 # 2 e 4
train_batch_size = 8

model = SentenceTransformer(model_name)

train_loss = losses.CoSENTLoss(model)

# ----------------------------------------------------------------------------------
# Carregando dataset

dataset = load_dataset(dataset, trust_remote_code=True)

# ----------------------------------------------------------------------------------
# Normalizando os datatasets 0 e 1 (na teoria)

scMin = 1.0
scMax = 5.0

train_norm = []
for sc in dataset[train_key][label_key]:
    normalized = float((sc-scMin)/(scMax-scMin))
    train_norm.append(normalized)

testt_norm = []
for sc in dataset[testt_key][label_key]:
    normalized = float((sc-scMin)/(scMax-scMin))
    testt_norm.append(normalized)

valll_norm = []
for sc in dataset[valll_key][label_key]:
    normalized = float((sc-scMin)/(scMax-scMin))
    valll_norm.append(normalized)

# ----------------------------------------------------------------------------------------------
# Removendo todas as colunas desnecessárias

dataset = dataset.remove_columns(list(set(dataset[train_key].features.keys())-set([sent1_key, sent2_key])))

# ----------------------------------------------------------------------------------
# Adicionando as colunas normalizadas

label_key = 'score'
dataset[train_key] = dataset[train_key].add_column(label_key, train_norm)
dataset[testt_key] = dataset[testt_key].add_column(label_key, testt_norm)
dataset[valll_key] = dataset[valll_key].add_column(label_key, valll_norm)

logging.info(dataset)

# ----------------------------------------------------------------------------------

# 4. Define an evaluator for use during training. This is useful to keep track of alongside the evaluation loss.
dev_evaluator = EmbeddingSimilarityEvaluator(
    sentences1      = dataset[valll_key][sent1_key],
    sentences2      = dataset[valll_key][sent2_key],
    scores          = dataset[valll_key][label_key],
    main_similarity = SimilarityFunction.COSINE,
    name            = &quot;sts-dev&quot;,
)

# 5. Define the training arguments
args = SentenceTransformerTrainingArguments(
    output_dir                  = output_dir,
    num_train_epochs            = num_epochs,
    per_device_train_batch_size = train_batch_size,
    per_device_eval_batch_size  = train_batch_size,
    learning_rate               = 1e-5,
    fp16                        = True,
    bf16                        = False,
    evaluation_strategy         = &quot;epoch&quot;,
    logging_steps               = 100,
    run_name                    = &quot;sts&quot;,
    save_strategy               = &quot;no&quot;,
)

# 6. Create the trainer &amp; start training
trainer = SentenceTransformerTrainer(
    model         = model,
    args          = args,
    train_dataset = dataset[train_key],
    eval_dataset  = dataset[valll_key],
    loss          = train_loss,
    evaluator     = dev_evaluator,
)

trainer.train()
</code></pre>
<p>Does it make sense for the model to achieve different results in the same epoch when the total number of epochs is different?
Is there a parameter that depends on the total number of epochs and could be generating this difference? Does it make sense for the results to be different?</p>
","0","Question"
"78665213","","<p>I wanted to predict stock price in the next 60 days but after I finished writing the code, it predicted backward instead. Can anyone advise me? How can I do it? I revised my code but it's not working.</p>
<p>My code is</p>
<pre><code>import math
from mplfinance.original_flavor import candlestick_ohlc
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import mean_squared_error


Tencent2.head()
#Define the features and target
##features = ['Open', 'Vol.']
##target = 'Price'


#Plot line chart
Tencent2['Date'] = pd.to_datetime(Tencent2['Date'])
plt.figure(figsize=(15,6))
plt.plot(Tencent2['Date'], Tencent2['Close'], marker='.')
plt.title('Close by Month', fontsize=15)
plt.xlabel('Date', fontsize=13)
plt.ylabel('Close', fontsize=13)
plt.xticks(rotation=45)
plt.show()


#Plot Candle Chart
matplotlib_date = mdates.date2num(Tencent2['Date'])
ohlc = np.vstack((matplotlib_date,Tencent2['Open'],Tencent2['High'],Tencent2['Low'],Tencent2['Close'])).T
plt.figure(figsize=(15,6))
ax = plt.subplot()
candlestick_ohlc(ax,ohlc,width=0.8,colorup='g',colordown='r')
ax.xaxis_date()
plt.title('Movement of Close Price')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()


#Resetting the index
Tencent2.set_index('Date', inplace=True)


#Create a new dataframe with only the 'Close' column
data = Tencent2.filter(['Close'])


#Convert the dataframe to a numpy array
dataset = data.values
#Get the number of rows to train the model on
training_data = math.ceil( len(dataset) * 0.7 )


#Scale the data
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(dataset)


#Create the training data set 
#Create the scaled training data set
train_data = scaled_data[0:training_data, :]
#Split the data into x_train and y_train data sets
x_train = []
y_train = []
#We create a loop
for i in range(60, len(train_data)):
  x_train.append(train_data[i-60:i, 0]) 
  y_train.append(train_data[i, 0]) 
  if i &lt;= 60:
    print(x_train)
    print(y_train)
    print()


#Convert the x_train and y_train to numpy arrays
x_train, y_train = np.array(x_train), np.array(y_train)


#Reshape the data
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape


#Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))


#Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')


#Train the model
model.fit(x_train, y_train, batch_size=1, epochs=1)


#Create the testing data set
#Create a new array containing scaled values from index 1738 to 2247
test_data = scaled_data[training_data - 60:]
#Create the data set X_test and y_test
X_test = []
y_test = dataset[training_data:, :]
for i in range(60, len(test_data)):
  X_test.append(test_data[i-60:i, 0])


#Convert the data to a numpy array
X_test = np.array(X_test)


#Reshape the data
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))


#Get the model's predicted price values for the X_test data set
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)


#Evaluate model (get the root mean quared error (RMSE))
rmse = np.sqrt( np.mean( predictions - y_test )**2 )


#Plot the data
train = data[:training_data]
valid = data[training_data:]
valid['Predictions'] = predictions
#Visualize the data
plt.figure(figsize=(16,8))
plt.title('Model')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price', fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Validation', 'Predictions'], loc='lower right')
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/k9f6K5b8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/k9f6K5b8.png"" alt=""enter image description here"" /></a></p>
<p>I want to predict the price 60 days into the future, but the price is predicted in the past. How can I predict the price after 2024-06?
Thank you for your help.</p>
","0","Question"
"78666757","","<p>i was trying to run this command-  onnx-tf convert -i yolov8n.onnx -o yolov8_tf
then i encountered this error-
2024-06-25 15:20:44.150147: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable <code>TF_ENABLE_ONEDNN_OPTS=0</code>.
2024-06-25 15:20:50.272876: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable <code>TF_ENABLE_ONEDNN_OPTS=0</code>.
Traceback (most recent call last):
File &quot;&quot;, line 198, in <em>run_module_as_main
File &quot;&quot;, line 88, in <em>run_code
File &quot;C:\Users\aasth\AppData\Roaming\Python\Python312\Scripts\onnx-tf.exe_<em>main</em></em>.py&quot;, line 4, in 
File &quot;C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf_<em>init</em></em>.py&quot;, line 1, in 
from . import backend
File &quot;C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf\backend.py&quot;, line 29, in 
from onnx_tf.common.handler_helper import get_all_backend_handlers
File &quot;C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf\common\handler_helper.py&quot;, line 3, in 
from onnx_tf.handlers.backend import *  # noqa
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf\handlers\backend\hardmax.py&quot;, line 3, in 
import tensorflow_addons as tfa
ModuleNotFoundError: No module named 'tensorflow_addons'</p>
<p>i tried installing the module using -  pip install tensorflow-addons
then i got this error- Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)
ERROR: No matching distribution found for tensorflow-addons
i tried installing with.whl files also but i got the same error.
tensorflow version is 2.16
python verson is 3.12</p>
","-1","Question"
"78666961","","<p>I am working with the meta-feature extractor package: <a href=""https://github.com/ealcobaca/pymfe"" rel=""nofollow noreferrer"">pymfe</a> for complexity analysis.
On a small dataset, this is not a problem, for example.</p>
<pre><code>pip install -U pymfe

from sklearn.datasets import make_classification
from sklearn.datasets import load_iris
from pymfe.mfe import MFE

data = load_iris()
X= data.data
y = data.target

extractor = MFE(features=[ &quot;t1&quot;], groups=[&quot;complexity&quot;],
                  summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
extractor.extract()
(['t1'], [0.12])
</code></pre>
<p>My dataset is large <code>(32690, 80)</code> and this computation gets killed for exessive memory usage. I work on <code>Ubuntu 24.04</code> having <code>32GB</code> RAM.</p>
<p>To reproduce scenario:</p>
<pre class=""lang-py prettyprint-override""><code># Generate the dataset
X, y = make_classification(n_samples=20_000,n_features=80,
    n_informative=60, n_classes=5, random_state=42)

extractor = MFE(features=[ &quot;t1&quot;], groups=[&quot;complexity&quot;],
                  summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
extractor.extract()
Killed
</code></pre>
<p><strong>Question:</strong></p>
<p>How do I split this task to compute on small partitions of the dataset, and combine final results (averaging)?</p>
","2","Question"
"78666998","","<p>I am trying to finetune a DeepLabV3Plus model from the keras_cv library to use on a custom dataset, but upon attempting to export to SavedModel format, I get this error:</p>
<pre><code>File &quot;C:\Users\u\.pyenv\pyenv-win\versions\3.10.2\lib\site-packages\keras\src\utils\traceback_utils.py&quot;, line 731, in error_handler  *
    return fn(*args, **kwargs)

TypeError: Exception encountered when calling UpSampling2D.call().

unsupported operand type(s) for *: 'NoneType' and 'int'

Arguments received by UpSampling2D.call():
  • inputs=tf.Tensor(shape=(None, None, None, 256), dtype=float32)
</code></pre>
<p>This is confusing, as I have specified the input shape for my model when calling DeepLabV3Plus.from_preset as [224,224,3], but the model summary shows None shapes for all layers (see <a href=""https://i.imgur.com/2CEPeom.png"" rel=""nofollow noreferrer"">here</a> for model.summary() output). However, from notebooks I have seen, this is the intended behavior even when you specify input shape.</p>
<p>As for the training script, this is the code I used:</p>
<pre><code>model = keras_cv.models.DeepLabV3Plus.from_preset(
    &quot;mobilenet_v3_large_imagenet&quot;,
    num_classes=NUM_CLASSES,
    input_shape=[224,224,3],
    load_weights=True
)

layers_to_train = 1
def disable_training(x): x.trainable = False
[disable_training(layer) for layer in model.layers[:-layers_to_train]]
model.summary()


model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss=[keras.losses.CategoricalFocalCrossentropy(from_logits=False, alpha=class_weights, gamma=3)],
    metrics=[keras.metrics.OneHotMeanIoU(num_classes=NUM_CLASSES), 'accuracy'])
 
callback_cyclic = CyclicLR(base_lr = LEARNING_RATE, max_lr = MAX_LEARNING_RATE, step_size=STEP_SIZE, mode = CYCLIC_MODE)

history = model.fit(train_dataset, epochs=NUM_EPOCHS, batch_size=NUM_BATCH, validation_data=val_dataset, callbacks=[callback_cyclic])


model.export(savepath_dir+&quot;model.tf&quot;)
</code></pre>
","0","Question"
"78667039","","<p>I am trying to build an API using flask which will extract the text from a url given and generate valid tags for that text. Say for example, the text is for a recipe of chicken curry, valid tags can be recipe, Indian Cuisine, food etc.</p>
<p>I have tried nltk library, TF-IDF vectorizer etc, but all of them are analysing maximum frequency of the words, not generating new words.
Does any one have any solution for this problem.
I have also tried using gtp2 module, but the output is not what I am expecting</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load pre-trained GPT-2 model and tokenizer
model_name = 'gpt2-medium'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def fetch_text_from_url(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise HTTPError for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find all paragraphs and concatenate their text
        paragraphs = soup.find_all('p')
        text = ' '.join([para.get_text() for para in paragraphs])
        
        return text
    except requests.exceptions.RequestException as e:
        print(f&quot;Error fetching content from {url}: {str(e)}&quot;)
        return None
    except Exception as e:
        print(f&quot;Error parsing content from {url}: {str(e)}&quot;)
        return None

def generate_tags(text, max_length=20, num_return_sequences=3):
    try:
        # Create a prompt to generate tags
        prompt = &quot;Tags for this text: &quot;

        # Tokenize the input text and prompt
        input_ids = tokenizer.encode(prompt + text, return_tensors='pt')

        # Generate tags using GPT-2 model
        output = model.generate(
            input_ids,
            max_length=max_length + len(input_ids[0]),
            num_return_sequences=num_return_sequences,
            num_beams=5,
            no_repeat_ngram_size=2,
            early_stopping=True
        )

        # Decode generated tags
        tags = []
        for seq in output:
            decoded_seq = tokenizer.decode(seq, skip_special_tokens=True).strip()
            # Extract tags from the generated text
            tags.extend([t.strip() for t in decoded_seq.split() if t.startswith('#')])

        return tags
    
    except Exception as e:
        print(f&quot;Error generating tags: {str(e)}&quot;)
        return None

@app.route('/generate_tags', methods=['POST'])
def generate_tags_api():
    data = request.get_json()
    url = data.get('url')
    if not url:
        return jsonify({'error': 'URL is required'}), 400

    try:
        text = fetch_text_from_url(url)
        if not text:
            return jsonify({'error': 'Failed to fetch content from URL'}), 500
        
        tags = generate_tags(text)
        if tags:
            return jsonify({'tags': tags})
        else:
            return jsonify({'error': 'Failed to generate tags from URL'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == &quot;__main__&quot;:
    app.run(port=8000, debug=True)
</code></pre>
","0","Question"
"78668432","","<p>I need to run another library's algo on each row of a large df but am having trouble converting my code to polars expressions for better performance. Here are a couple sample DFs:</p>
<pre><code>df_products = pl.DataFrame({
    'SKU':['apple','banana','carrot','date'],
    'DESCRIPTION': [
        &quot;Wire Rope&quot;,
        &quot;Connector&quot;,
        &quot;Tap&quot;,
        &quot;Zebra&quot;
    ],
    'CATL3': [
        &quot;Fittings&quot;,
        &quot;Tube&quot;,
        &quot;Tools&quot;,
        &quot;Animal&quot;
    ],
    'YELLOW_CAT': [
        &quot;Rope Accessories&quot;,
        &quot;Tube Fittings&quot;,
        &quot;Forming Taps&quot;,
        &quot;Striped&quot;
    ],
    'INDEX': [0, 5, 25, 90],
    'EMBEDDINGS': [
        [1, 2, 3],
        [4, 5, 6],
        [7, 8, 9],
        [10,11,12]
    ],
})


df_items_sm_ex = pl.DataFrame({
    'PRODUCT_INFO':['apple','banana','carrot'],
    'SEARCH_SIMILARITY_SCORE': [
        [1., 0.87, 0.54, 0.33],
        [1., 0.83, 0.77, 0.55],
        [1., 0.92, 0.84, 0.65]
    ],
    'SEARCH_POSITION': [
        [0, 5, 25, 90],
        [1, 2, 151, 373],
        [3, 5, 95, 1500]
    ],
    'SKU':['apple','banana','carrot'],
    'YELLOW_CAT': [
        &quot;Rope Accessories&quot;,
        &quot;Tube Fittings&quot;,
        &quot;Forming Taps&quot;
    ],
    'CATL3': [
        &quot;Fittings&quot;,
        &quot;Tube&quot;,
        &quot;Tools&quot;
    ],
    'EMBEDDINGS': [
        [1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]
    ],
})
</code></pre>
<p>and now the code</p>
<p>Per each row I have 3 main operations: Generate the base new dataframe, preprocess / clean / run predictions on the dataframe, write the dataframe to a few SQL tables. I have noticed step 1 and 2 easily take the longest time to execute:</p>
<pre><code>df_items_sm_ex.select(
        pl.struct(df_items_sm_ex.columns)
        .map_elements(lambda row: build_complements(
            row, df_items, rfc, rfc_comp, engine, current_datetime
            )))


def build_complements(row, df_products, ml, ml_comp, engine, current_datetime):
    try:
        #step 1 - generate the base new dataframe
        output_df = build_candidate_dataframe(row, df_products)
        #step 2 - preprocess / clean / run predictions on the dataframe
        output_df = process_candidate_output(df_products, output_df, ml, ml_comp)
        #step 3 write dataframes to SQL
        write_validate_complements(output_df, row, current_datetime, engine)
    except Exception as e:
        print(f'exception: {repr(e)}')

def build_candidate_dataframe(row, df_products):
    df_len = len(row['SEARCH_SIMILARITY_SCORE'])
    schema = {'QUERY': str,
              'SIMILARITY_SCORE': pl.Float32, 
              'POSITION': pl.Int64,
              'QUERY_SKU': str, 
              'QUERY_LEAF': str,
              'QUERY_CAT': str,
              'QUERY_EMBEDDINGS': pl.List(pl.Float32)
            }
    output_df = pl.DataFrame({'QUERY': [row['PRODUCT_INFO']] * df_len,
                    'SIMILARITY_SCORE': row['SEARCH_SIMILARITY_SCORE'], 
                    'POSITION': row['SEARCH_POSITION'],
                    'QUERY_SKU': [row['SKU']] * df_len, 
                    'QUERY_LEAF': [row['YELLOW_CAT']] * df_len,
                    'QUERY_CAT': [row['CATL3']] * df_len,
                    'QUERY_EMBEDDINGS': [row['EMBEDDINGS']] * df_len
                    }, schema=schema).sort(&quot;SIMILARITY_SCORE&quot;, descending=True)
                
    output_df = output_df.join(df_products[['SKU', 'EMBEDDINGS', 'INDEX', 'DESCRIPTION', 'CATL3', 'YELLOW_CAT']], left_on=['POSITION'], right_on=['INDEX'], how='left')
    output_df = output_df.rename({&quot;DESCRIPTION&quot;: &quot;SIMILAR_PRODUCT_INFO&quot;, &quot;CATL3&quot;: &quot;SIMILAR_PRODUCT_CAT&quot;, &quot;YELLOW_CAT&quot;: &quot;SIMILAR_PRODUCT_LEAF&quot;})
    return output_df

def process_candidate_output(df_products, output_df, ml, ml_comp):
    combined_embeddings = (output_df.to_pandas()['QUERY_EMBEDDINGS'] + output_df.to_pandas()['EMBEDDINGS']) / 2
    output_df = output_df.with_columns(pl.Series(name='COMBINED_EMBEDDINGS', values=combined_embeddings))
    output_df = output_df[['QUERY', 'QUERY_SKU', 'QUERY_CAT', 'QUERY_LEAF', 'SIMILAR_PRODUCT_INFO', 'SIMILAR_PRODUCT_CAT', 'SIMILAR_PRODUCT_LEAF', 'SIMILARITY_SCORE', 'COMBINED_EMBEDDINGS', 'SKU', 'POSITION']]
    output_df = output_df.filter(
        pl.col('SKU') != output_df['QUERY_SKU'][0]
    )
    #ML predictions
    output_df = predict_complements(output_df, ml)
    output_df = output_df.filter(
        pl.col('COMPLEMENTARY_PREDICTIONS') == 1
    )
    #Other ML predictions
    output_df = predict_required_accessories(output_df, ml_comp)
    output_df = output_df.sort(by='LABEL_PROBABILITY', descending=True)
    return output_df
</code></pre>
","0","Question"
"78668638","","<pre><code>import keras
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()
...
</code></pre>
<p>I currently started with machine learning, but I'm unable to load the MNIST data set due to an error thats says:</p>
<blockquote>
<p>Exception: URL fetch failure on <a href=""https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz"" rel=""nofollow noreferrer"">https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz</a>: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)</p>
</blockquote>
<p>I ensured that everything was up to date (macOS version 14.5, pip version 24.1, latest tensorflow and keras libraries and up-to-date safari and vscode versions). I have tried reinstalling keras several times, made sure to not download it from pip3 cache with the latest pip 24.1 version. This error keeps persisting and I haven't found a fix yet. The only part I've figured out is that it is an error resulting from the ...load_data() function.</p>
<p>How to resolve this issue?</p>
","0","Question"
"78669554","","<p>I am currently working on a deep learning project 'leaf disease segmentation'. I have trained a model over 50 epochs and obtained the following accuracy and loss metrics:
Train Loss: 19.4736, Train Accuracy: 0.9395
Validation Loss: 19.6197, Validation Accuracy: 0.9100
Test Loss: 19.6148, Test Accuracy: 0.9123</p>
<p>However, when I plot the predicted masks, they appear inaccurate. Is there any issue with my plotting code.</p>
<pre><code>def plot_predictions(model, images, masks, num_samples=5):
    predictions = model.predict(images[:num_samples])
    for i in range(num_samples):
        plt.figure(figsize=(15, 5))
        plt.subplot(1, 3, 1)
        plt.title('Real Image')
        plt.imshow(images[i])
        plt.subplot(1, 3, 2)
        plt.title('Ground Truth Mask')
        plt.imshow(masks[i], cmap='gray')  # Assuming masks are already binary
        plt.subplot(1, 3, 3)
        plt.title('Predicted Mask')
        plt.imshow(predictions[i][:, :, 0], cmap='gray')  # Convert predicted mask to binary
        plt.show()

plot_predictions(model, test_images.numpy(), test_masks_L, num_samples=5)
</code></pre>
<p><a href=""https://i.sstatic.net/65klcuaB.png"" rel=""nofollow noreferrer"">Original Image-Mask-Predicted Mask</a></p>
<p>Please review my code and help identify any errors that might be causing this issue?</p>
","-1","Question"
"78671295","","<p>I have fitted a ElasticNetCV in Python with three splits:</p>
<pre><code>import numpy as np
from sklearn.linear_model import LinearRegression

#Sample data:
num_samples = 100  # Number of samples
num_features = 1000  # Number of features 
X = np.random.rand(num_samples, num_features)
Y = np.random.rand(num_samples)

#Model
l1_ratios = np.arange(0.1, 1.1, 0.1)
tscv=TimeSeriesSplit(max_train_size=None, n_splits=3)
regr = ElasticNetCV(cv=tscv.split(X), random_state=42,l1_ratio=l1_ratios)
regr.fit(X,Y)
</code></pre>
<p>Now I want to get the whole grid of combinations of hyperparameters with the corresponding MSE as a Data Frame, I tried the following. However, the problem is that the resulting data frame shows the MSE for a combination of hyperparameters which are not shown as the minimum in the ElasticNetCV object which can be obtained by <code>regr.alpha_</code> and <code>regr.l1_ratio_</code>
:</p>
<pre><code>mse_path =  regr.mse_path_
alpha_path = regr.alphas_

# Reshape mse_path to have l1_ratios, n_alphas, cross_validation_step as separate columns
mse_values = mse_path.flatten()
alpha_values = alpha_path.flatten()
l1_values=np.tile(l1_ratios ,int(alpha_values.shape[0]/l1_ratios.shape[0]))
repeated_l1_ratios = np.repeat(l1_ratios, 100)

# mse has dimensions (11, 100, 3)
array_3d = mse

# Flatten the 3D array into a 2D array
# Each sub-array of shape (100, 3) becomes a row in the new 2D array
array_2d = array_3d.reshape(-1, 3)

# Create a DataFrame from the 2D array
df = pd.DataFrame(array_2d, columns=['MSE Split1', 'MSE Split2', 'MSE Split3'])

df['alpha_values'] = alpha_values
df['l1_values'] = repeated_l1_ratios
</code></pre>
<p>The following then results in a hyperparameter combination that is not the true one. So when combining the MSEs and the hyperparameter values, something is wrong:</p>
<pre><code># Calculate the minimum MSE for each row across the three splits
df['Min MSE'] = df[['MSE Split1', 'MSE Split2', 'MSE Split3']].min(axis=1)

# Identify the row with the overall minimum MSE
min_mse_row_index = df['Min MSE'].idxmin()

# Retrieve the row with the minimum MSE
min_mse_row = df.loc[min_mse_row_index]

print(&quot;Row with the minimum MSE across all splits:&quot;)
print(min_mse_row)
</code></pre>
","1","Question"
"78672059","","<p>I have a code with which I iterate over the hyperparameters of both the model itself and the entire pipeline</p>
<pre><code>preprocessor = ColumnTransformer(
            [
                ('OneHotEncoder', OneHotEncoder(drop='if_binary', sparse_output=False), binary_cols),
                ('CatBoostEncoder', CatBoostEncoder(random_state=RANDOM_STATE), non_binary_cat_cols),
                ('StandardScaler', StandardScaler(), num_cols)
            ],
            verbose_feature_names_out=False,
            remainder='drop'
        )
    
        pipe_final = ImbPipeline([
            ('preprocessor', preprocessor),
            ('target_imbalance', ADASYN()),
            ('selection', PCA()),
            ('models', CatBoostClassifier(random_state=RANDOM_STATE))
        ])

        # Гиперпараметры для CatBoostClassifier
        param_grid = {
            'models__iterations': [1000, 2000, 3000],
            'models__class_weights': ['Balanced', None],
            'target_imbalance': [ADASYN(random_state=RANDOM_STATE), SMOTETomek(random_state=RANDOM_STATE),
                                 SMOTE(random_state=RANDOM_STATE, k_neighbors=10), 'passthrough'],
            'preprocessor__StandardScaler': [StandardScaler(), RobustScaler(), MinMaxScaler(),
                                             PowerTransformer(), QuantileTransformer(),
                                             Normalizer(),PolynomialFeatures(degree=2, include_bias=False), 'passthrough'],
            'selection': [PCA(random_state=RANDOM_STATE, n_components=&quot;mle&quot;, svd_solver=&quot;full&quot;),
                          SelectKBest(mutual_info_classif, k=40),
                          SelectKBest(f_classif, k=40),
                          SelectKBest(chi2, k=40),
                          SelectPercentile(mutual_info_classif, percentile=10),
                          SelectPercentile(f_classif, percentile=10),
                          SelectFromModel(CatBoostClassifier(random_state=RANDOM_STATE)),
                          SelectFromModel(LogisticRegression(random_state=RANDOM_STATE)),
                          SelectFromModel(RandomForestClassifier(random_state=RANDOM_STATE)),
                          'passthrough'],
        }

        gs = GridSearchCV(
            pipe_final, 
            param_grid, 
            cv=5, 
            scoring='roc_auc', 
            n_jobs=-1
        )

        # Запускаем поиск гиперпараметров
        gs.fit(X, y_enc)
</code></pre>
<p>It takes a very long time to complete and I want to speed it up. For this I want to use OptunaSearchCV. Do I understand correctly that using OptunaSearchCV I can enumerate the hyperparameters of the model itself, but not the entire pipeline, because Is there no Distribution with which I can set SelectKBest(f_classif, k=40), RobustScaler(), etc.?</p>
<p>Sorry if my wording is not accurate somewhere, I use Google translator, because... I am not a native speaker</p>
","0","Question"
"78677115","","<p>Trying to implement a vectorized version of an algorithm (from computational geometry) using Jax. I have made the minimum working example using a LinkedList to particularly express my query (I am using a DCEL otherwise).</p>
<p>The idea is that this vectorized algorithm will be checking certain criteria over a DCEL. I have substituted this “criteria checking procedure” with a simple summation algorithm for the sake simplicity.</p>
<pre><code>
import jax
from jax import vmap
import jax.numpy as jnp

class Node: 
  
    # Constructor to initialize the node object 
    def __init__(self, data): 
        self.data = data 
        self.next = None

class LinkedList: 
  
    def __init__(self): 
        self.head = None
  
    def push(self, new_data): 
        new_node = Node(new_data) 
        new_node.next = self.head 
        self.head = new_node 

    def printList(self): 
        temp = self.head 
        while(temp): 
            print (temp.data,end=&quot; &quot;) 
            temp = temp.next

def summate(list) :
    prev = None
    current = list.head
    sum = 0
    while(current is not None): 
        sum += current.data
        next = current.next
        current = next
    return sum

list1 = LinkedList() 
list1.push(20) 
list1.push(4) 
list1.push(15) 
list1.push(85) 

list2 = LinkedList() 
list2.push(19)
list2.push(13)
list2.push(2)
list2.push(13)

#list(map(summate, ([list1, list2])))

vmap(summate)(jnp.array([list1, list2]))

</code></pre>
<p>I get the following error.</p>
<p><code> TypeError: Value '&lt;__main__.LinkedList object at 0x1193799d0&gt;' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.</code></p>
<p>The objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion. I have implemented what I want in basic Python, but I want to do it in Jax as there is a larger probabilistic function which I will be using this subprocedure for (it’s a Markov Chain).</p>
<p>It might be the case that I am completely unable to work over such data structures over Jax as the error suggests that only numeric types are supported. Can I use <code>pytrees</code> in some way to mitigate this constraint?</p>
<p>It will be tempting to suggest I use a simple list from jnp, but I am using Linkedlist just as an example of a simple(st) data structure. As mentioned earlier, am actually working over a DCEL.</p>
<p>PS : the Linkedlist code was taken from GeeksForGeeks, as I wanted to come up with a minimum working example quickly.</p>
","1","Question"
"78677544","","<p>I have an object detection code in Android, I want to convert it to Flutter to use it for IOS as well</p>
<pre><code>public static Classifier create(
            AssetManager assetManager,
            String modelFilename,
            String[] labels,
            int inputSize,
            int imageMean,
            float imageStd,
            String inputName,
            String outputName) {
        final TensorFlowImageClassifier c = new TensorFlowImageClassifier();
        c.inputName = inputName;
        c.outputName = outputName;

        // Read the label names into memory.
        Collections.addAll(c.labels, labels);

        c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);

        // The shape of the output is [N, NUM_CLASSES], where N is the batch size.
        final Operation operation = c.inferenceInterface.graphOperation(outputName);
        final int numClasses = (int) operation.output(0).shape().size(1);

        // Ideally, inputSize could have been retrieved from the shape of the input operation.  Alas,
        // the placeholder node for input in the graphdef typically used does not specify a shape, so it
        // must be passed in as a parameter.
        c.inputSize = inputSize;
        c.imageMean = imageMean;
        c.imageStd = imageStd;

        // Pre-allocate buffers.
        c.outputNames = new String[]{outputName};
        c.intValues = new int[inputSize * inputSize];
        c.floatValues = new float[inputSize * inputSize * 3];
        c.outputs = new float[numClasses];

        return c;
    }
</code></pre>
<p>Called:</p>
<pre><code>create(
                                getAssets(),
                                &quot;file:///android_asset/xxx&quot;,
                                getResources().getStringArray(R.array.yyy),
                                INPUT_SIZE,
                                128,
                                128,
                                &quot;input&quot;,
                                &quot;InceptionV3/Predictions/Reshape_1&quot;)
</code></pre>
<p>org.tensorflow.contrib.android.TensorFlowInferenceInterface</p>
<p>This class is not found in flutter, what should I replace it with?</p>
","0","Question"
"78677993","","<p>I would like to construct a surrogate model of a physics simulation. Thus I am able to generate the data by myself. The data itself is very big, so it makes sense to generate a few data samples (e.g. a batch of 124) and then use it for training immediately instead of generating million of samples as I could not save such data set. In this way I could generate a batch, save it temporarily/send it to my neural network that is waiting for data and then after training on it discard it.</p>
<p>I usually implement my models with pytorch and I am also familiar with the torch.utils.data.DataLoader that can fetch batches into the RAM but it needs the directory structure and file ID's, i.e. a whole dataset needs to be saved somewhere.</p>
","1","Question"
"78678402","","<p>I'm having a bit of coding trouble if anyone can help! I'm trying to get the Datetime types from an oml.Dataframe df. I have tried this code:</p>
<pre><code> df = oml.sync(query=QUERY)
 df_datetime = df.select_types(include=['oml.Datetime'])
</code></pre>
<p>But I get an error that no columns are selected. Am I using this function incorrectly?
I found a workaround using</p>
<pre><code> df = oml.sync(query=QUERY)
 df_datetime = []
 for col, dtype in df.dtypes.items():
    if dtype.__name__ == 'Datetime':
       df_datetime.append(col)
</code></pre>
<p>and this does return Datetime objects, so I know they exist. I would much prefer using the <code>select_types</code> method if I can, if anyone can explain to me what I'm doing wrong.</p>
","0","Question"
"78679016","","<p>I'm working in Azure Machine Learning Studio to create components that I will run together in a pipeline. In this basic example, I have a single <code>python</code> script and a single <code>yml</code> file that make up my component, along with a notebook I am using to define, instantiate and run a pipeline. See an overview of the folder structure I have below for this component.</p>
<pre><code>📦component
 ┣ 📜notebook.ipynb
 ┣ 📜component_script.py
 ┗ 📜component_def.yml
</code></pre>
<p>Inside my notebook I can then load the component and register it to the workspace using the code below (note that here I have already instantiated my <code>ml_client</code> object).</p>
<pre><code># importing the Component Package
from azure.ai.ml import load_component

# Loading the component from the yml file
component = load_component(&quot;component_def.yml&quot;)

# Now we register the component to the workspace
component = ml_client.create_or_update(component)
</code></pre>
<p>I can then pass this component into a pipeline successfully. My question is, now that I have registered my component, I should no longer need to instantiate my component object using <code>component = load_component(&quot;component_def.yml&quot;)</code> which requires access to the <code>yml</code> file. I should instead be able to instantiate my component object from the registered component. How can I do this?</p>
","0","Question"
"78680846","","<p>I have the following output but can't figure out how to evaluate because there is no <code>F1 score</code> or <code>confusion matrix</code>.</p>
<pre class=""lang-py prettyprint-override""><code>Average Recall (AR) @[ IoU=0.50:0.95 | area= small |maxDets=100] = -1.000

Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250

Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410

20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:

21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:

22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:
</code></pre>
<p>I trained for 400 epochs, and this is just a small part of the output. I can't see the mAP either.</p>
<p>I have this line to eval</p>
<pre><code>!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0
</code></pre>
<p>Is there a way to obtain detailed evaluation metrics such as <code>F1 score</code>, <code>confusion matrix</code>, and <code>mAP</code>?</p>
","-1","Question"
"78684235","","<p>If I have a table stored on my hdb process at port 5012.</p>
<p>I have installed PyKX and successfully imported it to the python prompt in terminal.</p>
<p>I then connect to my <code>host=‘localhost’, port=5012 </code> and run a simple query to return my data from hdb <code>q(‘{select name,price,volume,vwap from tab where date&gt;2024.01.01}’)</code></p>
<p>How do I then use this data in one of the python machine learning algorithms. How do you go about converting the table data into usable python data points to then feed into your selected model? Do you have to extract each column of data and save as a variable of a certain type like running an <code>exec </code> statement in q process?</p>
","0","Question"
"78686328","","<p>I am working with a synthetic dataset generated using <code>make_classification</code> from <code>sklearn.datasets</code> with 5 classes. I have trained a <code>RandomForestClassifier</code> on this data and am evaluating its performance using two different methods. However, I am observing differences in the sensitivity (recall) values between these two methods.</p>
<p>Here is the code I am using:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, precision_recall_fscore_support
import numpy as np
import pandas as pd

# Generate a synthetic dataset with 5 classes
X, y = make_classification(n_samples=1000, n_classes=5, n_informative=10, n_clusters_per_class=1, random_state=42)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Method 1: classification_report
print(&quot;Classification Report&quot;)
print(classification_report(y_test, y_pred))

# Method 2: Loop with precision_recall_fscore_support
res = []

for l in range(5):
    prec, recall, _, _ = precision_recall_fscore_support(np.array(y_test) == l,
                                                         np.array(y_pred) == l,
                                                         pos_label=True, average=None)
    res.append([l, recall[0], recall[1]])

df = pd.DataFrame(res, columns=['class', 'sensitivity', 'specificity'])
print(&quot;\nSensitivity and Specificity&quot;)
print(df)
</code></pre>
<h3>Outputs:</h3>
<pre><code>Classification Report
              precision    recall  f1-score   support

           0       0.76      0.71      0.74        35
           1       0.72      0.93      0.81        30
           2       0.72      0.81      0.76        32
           3       0.85      0.86      0.86        59
           4       0.88      0.64      0.74        44

    accuracy                           0.79       200
   macro avg       0.78      0.79      0.78       200
weighted avg       0.80      0.79      0.79       200


Sensitivity and Specificity
   class  sensitivity  specificity
0      0     0.951515     0.714286
1      1     0.935294     0.933333
2      2     0.940476     0.812500
3      3     0.936170     0.864407
4      4     0.974359     0.636364
</code></pre>
<h3>Question:</h3>
<p>Why do the sensitivity (recall) values differ between the <code>classification_report</code> and the loop using <code>precision_recall_fscore_support</code>? Specifically, why is there a discrepancy between the recall values reported by <code>classification_report</code> and the sensitivity values calculated in the loop method? <em>If possible can u show it with a simple example (solved manually)</em></p>
<h3>What did you try and what were you expecting?</h3>
<p>I used two methods to evaluate the performance of my <code>RandomForestClassifier</code>. First, I used <code>classification_report</code> to get precision, recall, and F1-score for each class. Then, I calculated sensitivity and specificity for each class using a loop with <code>precision_recall_fscore_support</code>.</p>
<p>I expected the sensitivity values calculated in the loop method to match the recall values from the <code>classification_report</code>, as sensitivity and recall are often considered synonymous in classification tasks. However, I observed discrepancies between the two sets of values.</p>
<h3>What actually resulted?</h3>
<p>The recall values from the <code>classification_report</code> are different from the sensitivity values calculated in the loop method. The <code>classification_report</code> provides recall values for each class in a multi-class context, while the loop method treats each class as a binary classification problem, leading to different sensitivity and specificity values.</p>
","0","Question"
"78687394","","<p>I am trying to understand a code, I am confused about the test cells. When i am printing the shape of the output it is hidden_output.shape =(num_test, 20, 4, 4), test_hidden_block_stride(hidden_output).shape) == (num_test, 20, 10, 10) and Gen_output.shape=(num_test, 1,28,28) for Mnist dataset. I am trying to understand how the sizes are being calculated here. Any help will be greatly appreciated!</p>
<pre><code>class Generator(nn.Module):
    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):
        super(Generator, self).__init__()
        self.z_dim = z_dim
        # Build the neural network
        self.gen = nn.Sequential(
            self.make_gen_block(z_dim, hidden_dim * 4),
            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),
            self.make_gen_block(hidden_dim * 2, hidden_dim),
            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),
        )
def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0 ,final_layer=False):
        # Build the neural block
        layers = []
        layers.append(nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding, output_padding=padding))
        if not final_layer:
            layers.append(nn.BatchNorm2d(output_channels))
            layers.append(nn.ReLU(True))
        else:
            layers.append(nn.Tanh())
        
        return nn.Sequential(*layers)
# Testing
gen = Generator()
num_test = 100
# Test the hidden block
test_hidden_noise = get_noise(num_test, gen.z_dim)
test_hidden_block = gen.make_gen_block(10, 20, kernel_size=4, stride=1)
test_uns_noise = gen.unsqueeze_noise(test_hidden_noise)
hidden_output = test_hidden_block(test_uns_noise)
# Check that it works with other strides
test_hidden_block_stride = gen.make_gen_block(20, 20, kernel_size=4, stride=2)
test_final_noise = get_noise(num_test, gen.z_dim) * 20
test_final_block = gen.make_gen_block(10, 20, final_layer=True)
test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)
final_output = test_final_block(test_final_uns_noise)
# Test the whole thing:
test_gen_noise = get_noise(num_test, gen.z_dim)
test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)
gen_output = gen(test_uns_gen_noise)
</code></pre>
<p>I am trying to calculate the sizes from the formula by hand. I am just seeing different kernel sizes and strides and padding. not sure which values to use.</p>
","0","Question"
"78687946","","<p>I am working on a project which involves object detection and tracking. For object detection I am using <code>yolov8</code> and for tracking, I am using <code>SORT</code> tracker. After running the below code, my GPU usage is always under 10% and CPU usage is always more than 40%. I have installed <code>cuda</code>, <code>cudnn</code> and have installed <code>torch</code> using <code>cuda</code>. I have also compiled <code>opencv</code> with <code>cuda</code> support. I am using <code>RTX 4060 ti</code> but looks like it's not being used.</p>
<p>Is there a way to further optimize the below code so that all of the work is handled by GPU and not CPU?</p>
<pre><code>from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO('yolov8s.pt').to(device)

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()  
    if not ret:
        print(&quot;**No frame received**&quot;)
        continue

    results = model(frame)
    dets_to_sort = np.empty((0, 6))
    for result in results:
        for obj in result.boxes:
            bbox = obj.xyxy[0].cpu().numpy().astype(int)
            x1, y1, x2, y2 = bbox

            conf = obj.conf.item()
            class_id = int(obj.cls.item())
            dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))
    
    tracked_dets = sort_tracker.update(dets_to_sort)
    for det in tracked_dets:
        x1, y1, x2, y2 = [int(i) for i in det[:4]]
        track_id = int(det[8]) if det[8] is not None else 0
        class_id = int(det[4])
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
        cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

    frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
    cv2.imshow(&quot;Frame&quot;, frame)
    key = cv2.waitKey(1)
    if key == ord(&quot;q&quot;):
        break
    if key == ord(&quot;p&quot;):
        cv2.waitKey(-1)
    
cap.release()  
cv2.destroyAllWindows()
</code></pre>
","1","Question"
"78688976","","<p>I am working on a project aimed at recognizing whether a photo of any individual exists in the university's records. The proposed method involves storing the embeddings of each student's photo, along with their details, in a vector database. When a photo needs to be compared, the system will generate the embedding value for that photo and then compare this value against the database. If the value falls within a specific threshold, it will indicate that the individual exists in the record.</p>
<p>I am seeking expert advice on whether this approach is feasible. If there are any concerns with this method, I would appreciate recommendations for the best solution.</p>
","-1","Question"
"78691574","","<p>I have a YoloV8 data file format that is an annotation of data (images) done manually.
What is the most effective and straightforward way of generating a model and therefore yielding the weights file? is it using <code>darknet</code> through the command:</p>
<pre><code>darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights
</code></pre>
<p>then something like the following to generate the associate model:</p>
<pre><code>python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5
</code></pre>
<p>Or using <code>Roboflow</code> through:</p>
<pre><code>version.deploy(model_type=&quot;yolov8&quot;, model_path=f”{HOME}/runs/detect/train/”)
</code></pre>
<p>Seems to me darknet is more difficult to install.</p>
","1","Question"
"78691616","","<p>I have the following model that I tuned with my own dataset trained with DataParallel:</p>
<pre><code>model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Sequential(nn.Linear(768, 512),nn.ReLU(),nn.BatchNorm1d(512),nn.Dropout(p=0.2),nn.Linear(512, 141))
checkpoint = torch.load('vit_b_16v3.pth')
checkpoint = {k.partition('module.')[2]: v for k, v in checkpoint.items()}
# Load parameters
model.load_state_dict(checkpoint)
</code></pre>
<p>However, I have no idea on how to get the penultimate layer output of such a vision transformer. I tried <a href=""https://www.kaggle.com/code/mohammaddehghan/pytorch-extracting-intermediate-layer-outputs"" rel=""nofollow noreferrer"">This tutorial</a> but it is not working. I only want to input an image and have a 512-D vector describing it. With Tensorflow it is a piece of cake to do it but in Pytorch I am struggling.</p>
<p>My last layers are as follows:</p>
<pre><code>(norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (fc_norm): Identity()
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Sequential(
    (0): Linear(in_features=768, out_features=512, bias=True)
    (1): ReLU()
    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=512, out_features=141, bias=True)
  )
)
</code></pre>
","0","Question"
"78694076","","<p><a href=""https://developer.apple.com/documentation/coreml"" rel=""nofollow noreferrer"">https://developer.apple.com/documentation/coreml</a> mentions macOS 10.13+:</p>
<p><a href=""https://i.sstatic.net/wdJGZSY8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wdJGZSY8.png"" alt=""enter image description here"" /></a></p>
<p>How can one run a Core ML model on macOS 10.12?</p>
<hr />
<p>Example of a Core ML model created on Ubuntu 20.04 (tested with Python 3.10 and torch 2.3.1) with <a href=""https://github.com/huggingface/exporters"" rel=""nofollow noreferrer"">Hugging Face's Exporters lib</a>:</p>
<pre><code>git clone https://github.com/huggingface/exporters.git
cd exporters
pip install -e .
python -m exporters.coreml --model=distilbert-base-uncased exported/ --quantize=float32 
</code></pre>
","-3","Question"
"78696449","","<p>I have built a DQ network ,which analyses the states of asteroids and the rocket gives out the best possible action to be performed (left, right ,shoot , idle).But the agent keeps spamming bullets ?
can anyone help me in solving this problem ?</p>
<p>I thought the agent would explore all 4 actions but it keeps spamming bullets and harldy moves left or right ,stays almost at the centre and keeps spamming bullets .</p>
","-1","Question"
"78698316","","<p>I am trying to train <a href=""https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/"" rel=""nofollow noreferrer"">PyTorch's DeepLabV3</a> on <a href=""https://cocodataset.org/#home"" rel=""nofollow noreferrer"">COCO 2017 dataset</a> for semantic segmetation but I am unsure on how to deal with the different resolution images. I know that DeepLab's architecture can process them without any problems, but I can't stack them in batches due to their resolution. What is the best practice to handle this issue? Do I resize them to a fixed size? Do I take a random crop of fixed size? I know that there are a lot of solutions for this issue, but I don't really know what's the best practice in the context of semantic segmentation training.</p>
<p>Thanks!</p>
","1","Question"
"78698399","","<p>I coded the perceptron algorithm form scratch and compared the weights I obtained after training with the weights I obtained after training the sklearn perceptron model. I believe even the sklearn model initializes the weights and biases as a zero vector and I chose the learning rate <code>eta0=1</code> to match my perceptron code. (Note: the bias in my code is the last term in the vector <code>w_b</code>)</p>
<p>My Code:</p>
<pre><code>def perceptron(X_train, y_train):
    #initialize weights as 0
    w = np.zeros(len(X_train.columns))
    b = 0
    w_b = np.append(w, b)
    while True:
        misclassifications = 0  
        for X , Y in zip(X_train.values, y_train.values):
            X_i = np.append(X, 1)
            if Y*(np.dot(X_i,w_b)) &lt;= 0:
                w_b = w_b + Y*X_i
                misclassifications += 1
        if misclassifications == 0:
            break
    return w_b

w_b = perceptron(X_train, y_train)
</code></pre>
<p>Result: <code>[-3.   6.7 -1. ]</code></p>
<p>sklearn code:</p>
<pre><code>perceptron = Perceptron(max_iter=1000, eta0=1,random_state=42) 
perceptron.fit(X_train, y_train)

print(&quot;weights are&quot;,perceptron.coef_)
print(&quot;bias is&quot;,perceptron.intercept_)
</code></pre>
<p>Result: <code>weights are [[-4.7 10.1]] bias is [-2.]</code></p>
<p>I expected the weights to be same but they aren't. Any clue on why?</p>
","0","Question"
"78700237","","<p>I am trying to calculate the cosine similarity between two columns in a dataframe. the code snippet for it is as follows :</p>
<pre><code>def cal_cosine_similarity(row):
    vec1 = np.array(row['sup_vec'])
    vec2 = np.array(row['vector'])
    return cosine_similarity([vec1], [vec2])[0][0]
cross_join_df['cos_sim'] = cross_join_df.apply(cal_cosine_similarity,axis = 1)
</code></pre>
<p>This works fine most of the times but sometimes I am getting an error like :</p>
<pre><code>Traceback (most recent call last):
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py&quot;, line 2898, in get_loc
   return self._engine.get_loc(casted_key)
 File &quot;pandas/_libs/index.pyx&quot;, line 70, in pandas._libs.index.IndexEngine.get_loc
 File &quot;pandas/_libs/index.pyx&quot;, line 101, in pandas._libs.index.IndexEngine.get_loc
 File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item
 File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'cos_sim'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/generic.py&quot;, line 3576, in _set_item
   loc = self._info_axis.get_loc(key)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py&quot;, line 2900, in get_loc
   raise KeyError(key) from err
KeyError: 'cos_sim'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File &quot;/opt/prism/src/main.py&quot;, line 79, in &lt;module&gt;
   res = job.run()
 File &quot;/opt/prism/src/jobs/v2/SparkJob.py&quot;, line 45, in run
   self.start()
 File &quot;/opt/prism/src/jobs/v2/SparkJob.py&quot;, line 71, in start
   raise e
 File &quot;/opt/prism/src/jobs/v2/SparkJob.py&quot;, line 68, in start
   self.execute(self.input_data, 1)
 File &quot;/opt/prism/src/jobs/v2/DprmMappingInference.py&quot;, line 289, in execute
   cross_join_df['cos_sim'] = cross_join_df.apply(cal_cosine_similarity,axis = 1)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/frame.py&quot;, line 3044, in __setitem__
   self._set_item(key, value)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/frame.py&quot;, line 3121, in _set_item
   NDFrame._set_item(self, key, value)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/generic.py&quot;, line 3579, in _set_item
   self._mgr.insert(len(self._info_axis), key, value)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/managers.py&quot;, line 1198, in insert
   block = make_block(values=value, ndim=self.ndim, placement=slice(loc, loc + 1))
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/blocks.py&quot;, line 2744, in make_block
   return klass(values, ndim=ndim, placement=placement)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/blocks.py&quot;, line 2400, in __init__
   super().__init__(values, ndim=ndim, placement=placement)
 File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/blocks.py&quot;, line 130, in __init__
   raise ValueError(
ValueError: Wrong number of items passed 9, placement implies 1
</code></pre>
<p>I am not able to locate this error. Is this error due to some functionality of cosine similarity function?</p>
","0","Question"
"78703313","","<p>I am trying to run <a href=""https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb</a> locally, on an M2 MacBook with Sonoma 14.5. However, I keep running into the following error at step 11:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[75], line 1
----&gt; 1 masks = mask_generator.generate(image)

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)
    112 @functools.wraps(func)
    113 def decorate_context(*args, **kwargs):
    114     with ctx_factory():
--&gt; 115         return func(*args, **kwargs)

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:163, in SamAutomaticMaskGenerator.generate(self, image)
    138 &quot;&quot;&quot;
    139 Generates masks for the given image.
    140 
   (...)
    159          the mask, given in XYWH format.
    160 &quot;&quot;&quot;
    162 # Generate masks
--&gt; 163 mask_data = self._generate_masks(image)
    165 # Filter small disconnected regions and holes in masks
    166 if self.min_mask_region_area &gt; 0:

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:206, in SamAutomaticMaskGenerator._generate_masks(self, image)
    204 data = MaskData()
    205 for crop_box, layer_idx in zip(crop_boxes, layer_idxs):
--&gt; 206     crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
    207     data.cat(crop_data)
    209 # Remove duplicate masks between crops

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:236, in SamAutomaticMaskGenerator._process_crop(self, image, crop_box, crop_layer_idx, orig_size)
    234 cropped_im = image[y0:y1, x0:x1, :]
    235 cropped_im_size = cropped_im.shape[:2]
--&gt; 236 self.predictor.set_image(cropped_im)
    238 # Get points for this crop
    239 points_scale = np.array(cropped_im_size)[None, ::-1]

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/predictor.py:57, in SamPredictor.set_image(self, image, image_format)
     55 # Transform the image to the form expected by the model
     56 input_image = self.transform.apply_image(image)
---&gt; 57 input_image_torch = torch.as_tensor(input_image, device=self.device)
     58 input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
     60 self.set_torch_image(input_image_torch, image.shape[:2])

RuntimeError: Could not infer dtype of numpy.uint8
</code></pre>
<p>I am using a conda environment with Python 3.9.19, and also tested with Python 3.11. Based on online comments I suspected this to be an issue with numpy versions, but having tried multiple versions I cannot find the correct combination. I am currently trying with the following:</p>
<pre><code>numpy==1.24.4
torch==1.9.0
torchvision==0.10.0
opencv-python==4.10.0.84
</code></pre>
<p>Running the same notebook on Google Colab works fine, and the versions indicated there are:</p>
<pre><code>import numpy as np
import torch
import cv2

print(np.__version__)
print(torch.__version__)
print(cv2.__version__)

1.25.2
2.3.0+cu121
4.8.0
</code></pre>
<p>This is using Python 3.10.12. These versions are not available on Mac, so I am stuck.</p>
<p>How can I find out why numpy.uint8 is not being recognized, and how can I fix this error? Most online comments point to upgrading numpy, but I have tried several numpy versions without luck. Any help is appreciated.</p>
","0","Question"
"78704234","","<p>I'm writing a custom loss function in PyTorch for multiclass semantic segmentation. One part of this function is thresholding select channels from the tensor, which are indicated with tracker_index.</p>
<p>The last part of the function that is a part of the computational graph is the channel_tensor, and if I comment out the line where torch.where is applied, everything runs smoothly. I've tried setting 1 and 0 to float32 tensors and ensured that they are on the same device as the channel_tensor, which leads me to believe that eighter thresholding is not differentiable, so cannot be a part of the loss function, or torch.where will always detach the tensor from the computational graph. Please advise.</p>
<pre><code>channel_tensor =torch.select(
     segmentation_output,
     dim=-3,
     index=tracker_index
)
channels[tracker_index]= torch.where(channel_tensor &gt; self.threshold, torch.tensor(1, device=channel_tensor.device, dtype=torch.float32), torch.tensor(0, device=channel_tensor.device, dtype=torch.float32))
</code></pre>
","1","Question"
"78704542","","<p>I use the code below to export a Bert-based PyTorch model to CoreML.</p>
<p>Since I used</p>
<pre><code>dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)
</code></pre>
<p>the CoreML model only works with that input when tested on macOS. How can I make the CoreML model work for any input (i.e., any text)?</p>
<hr />
<p>Export script:</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;Core ML Export
pip install transformers torch coremltools nltk
&quot;&quot;&quot;
import os
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
import torch.nn as nn
import nltk
import coremltools as ct

nltk.download('punkt')

# Load the model and tokenizer
model_path = os.path.join('model')
model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

# Modify the model's forward method to return a tuple
class ModifiedModel(nn.Module):
    def __init__(self, model):
        super(ModifiedModel, self).__init__()
        self.model = model
        self.device = model.device  # Add the device attribute

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        return outputs.logits


modified_model = ModifiedModel(model)

# Export to Core ML
def convert_to_coreml(model, tokenizer):
    # Define a dummy input for tracing
    dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)
    dummy_input = {k: v.to(model.device) for k, v in dummy_input.items()}

    # Trace the model with the dummy input
    traced_model = torch.jit.trace(model, (
    dummy_input['input_ids'], dummy_input['attention_mask'], dummy_input.get('token_type_ids')))

    # Convert to Core ML
    inputs = [
        ct.TensorType(name=&quot;input_ids&quot;, shape=dummy_input['input_ids'].shape),
        ct.TensorType(name=&quot;attention_mask&quot;, shape=dummy_input['attention_mask'].shape)
    ]
    if 'token_type_ids' in dummy_input:
        inputs.append(ct.TensorType(name=&quot;token_type_ids&quot;, shape=dummy_input['token_type_ids'].shape))

    mlmodel = ct.convert(traced_model, inputs=inputs)

    # Save the Core ML model
    mlmodel.save(&quot;model.mlmodel&quot;)
    print(&quot;Model exported to Core ML successfully&quot;)

convert_to_coreml(modified_model, tokenizer)
</code></pre>
<p>To use the exported model:</p>
<pre><code>import os
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
import torch.nn as nn
import nltk
import coremltools as ct
from coremltools.models import MLModel
import numpy as np
from transformers import AutoTokenizer
import nltk

nltk.download('punkt')

# Load the Core ML model
model = MLModel('model.mlmodel')

# Load the tokenizer
model_path = 'model'
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

def prepare_input(text, tokenizer):
    tokens = nltk.tokenize.word_tokenize(text)
    tokenized_inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=&quot;np&quot;)
    input_ids = tokenized_inputs['input_ids'].astype(np.int32)
    attention_mask = tokenized_inputs['attention_mask'].astype(np.int32)
    
    input_data = {
        'input_ids': input_ids,
        'attention_mask': attention_mask
    }
    
    if 'token_type_ids' in tokenized_inputs:
        input_data['token_type_ids'] = tokenized_inputs['token_type_ids'].astype(np.int32)
    
    return input_data, tokens

def predict(text):
    # Prepare the input
    input_data, tokens = prepare_input(text, tokenizer)
    
    # Make the prediction
    prediction = model.predict(input_data)
    
    # Extract the predicted labels
    logits = prediction['output']  # Adjust this key according to your model's output
    predicted_label = np.argmax(logits, axis=-1)[0]
    
    # Display the results
    for word, label in zip(tokens, predicted_label):
        print(f&quot;{word}: {model.model_description.outputDescriptions[0].dictionaryType.int64KeyType.stringDictionary[label]}&quot;)

# Test the model with a sentence
predict(&quot;A French fan&quot;)
</code></pre>
<p>The script only works with the example 'A French Fan'. When I tried another example <code>predict(&quot;A footbal fan is standing in the stadium.&quot;)</code>, it triggers an error:</p>
<pre><code>NSLocalizedDescription = &quot;MultiArray shape (1 x 12) does not match the shape (1 x 5) specified in the model description&quot;;
</code></pre>
<hr />
<p>Environments:</p>
<ul>
<li>Export script: Tested Python 3.10 and torch 2.3.1 on Ubuntu 20.04 (does <a href=""https://stackoverflow.com/q/78687580/395857"">not</a> work on Windows 10).</li>
<li>Prediction script: must be run on macOS 10.13+, as CoreML model <a href=""https://stackoverflow.com/q/78694076/395857"">only</a> supports predictions on macOS 10.13+.</li>
</ul>
","0","Question"
"78704861","","<p>I'm trying to understand what values are being multiplied when I have higher dimensional tensors:</p>
<pre><code>inp = torch.rand(1,2,3) # B,C,W
linear = nn.Linear(3,4)
out = linear(inp)
print(out.shape)
&gt;&gt;&gt; torch.Size([1, 2, 4])

inp = torch.rand(1,2,3,4) # B,C,W,H
linear = nn.Linear(4,5)
out = linear(inp)
print(out.shape)
&gt;&gt;&gt; torch.Size([1, 2, 3, 5])
</code></pre>
<p>It seems like only the last dimension is being changed, but when I try to manually multiply the linear weights (<code>linear.weight.data</code>) with each <code>inp</code>'s last dimension, I can't get to the correct answer (seems like all the values are changing and only the last dimension's size is being modify somehow).</p>
","1","Question"
"78710808","","<p>I'm trying to get the data in the table labelled &quot;Key rates&quot;. However, I can only extract the Names as they have a unique style (  ).</p>
<p>I want data from the table and arranged into a table by using Pandas.
Please help</p>
<p>Code:</p>
<pre><code>URL = &quot;https://www.centralbank.go.ke/&quot;
page = requests.get(url)
soup = BeautifulSoup(page.content, 'html.parser')

job_elems = soup.find_all('td', class_=&quot;tg-4eph&quot;)             
for job_elem in job_elems:                                                      
    title_elem = job_elem.find('small')                                             
    if title_elem:                                                              
        print(title_elem.text.strip())
</code></pre>
","0","Question"
"78713048","","<p>My code is as follows:</p>
<pre><code>from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
import pandas as pd
RANDOM_STATE = 55 ## You will pass it to every sklearn call so we ensure reproducibility
n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval
This will replace the columns with the one-hot encoded ones and keep the columns outside 'columns' argument as it is.
df = pd.read_csv(&quot;doc/heart.csv&quot;)
cat_variables = ['Sex',
'ChestPainType',
'RestingECG',
'ExerciseAngina',
'ST_Slope'
]
df = pd.get_dummies(data = df,
prefix = cat_variables,
columns = cat_variables)
var = [x for x in df.columns if x not in 'HeartDisease'] ## Removing our target variable
X_train, X_test, y_train, y_test = train_test_split(df[var], df['HeartDisease'], train_size = 0.8, random_state = RANDOM_STATE)
print(X_train.shape)
X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]
import xgboost
print(xgboost.__version__)    # 2.1.0
xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)
xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)],early_stopping_rounds = 10)
</code></pre>
<p>The detailed error message is as follows:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\my_document\11_Python\exercise\main.py&quot;, line 153, in &lt;module&gt;
    xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)],early_stopping_rounds = 10)
  File &quot;C:\Users\samc\AppData\Local\Programs\Python\Python312\Lib\site-packages\xgboost\core.py&quot;, line 726, in inner_f
    return func(**kwargs)
           ^^^^^^^^^^^^^^
TypeError: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'
</code></pre>
<p>How can I resolve it?</p>
","1","Question"
"78713551","","<p>I load a huggingface-transformers float32 model, cast  it to float16, and save it. How can I load it as float16?</p>
<p>Example:</p>
<pre><code># pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f&quot;Parameter: {name}, Data type: {param.dtype}&quot;)

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True)
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)
</code></pre>
<p>In this example, <code>model2</code> loads as a <code>float32</code> model (as shown by <code>print_model_layer_dtype(model2)</code>), even though <code>model2</code> was saved as float16 (as <a href=""https://i.sstatic.net/Um45egKE.png"" rel=""nofollow noreferrer"">shown in <code>config.json</code></a>). What is the proper way to load it as float16?</p>
<p>Tested with <code>transformers==4.36.2</code> and Python 3.11.7 on Windows 10.</p>
","-1","Question"
"78717924","","<p>I wrote the following code to learn the score in the machine learning methods. but I get the following error. what would be the reason??</p>
<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [6396, 1599]
</code></pre>
<pre><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('Armenian Market Car Prices.csv')

df['Car Name'] = df['Car Name'].astype('category').cat.codes

df = df.join(pd.get_dummies(df.FuelType, dtype=int))
df = df.drop('FuelType', axis=1)

df['Region'] = df['Region'].astype('category').cat.codes

df['Price'] = df.pop('Price')

X = df.drop('Price', axis=1)
y = df['Price']

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()

model.fit(X_train, y_train)
</code></pre>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[358], line 1
----&gt; 1 model.fit(X_train, y_train)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1473, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-&gt; 1473     return fit_method(estimator, *args, **kwargs)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_base.py:609, in LinearRegression.fit(self, X, y, sample_weight)
    605 n_jobs_ = self.n_jobs
    607 accept_sparse = False if self.positive else [&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;]
--&gt; 609 X, y = self._validate_data(
    610     X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True
    611 )
    613 has_sw = sample_weight is not None
    614 if has_sw:

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:650, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
    648         y = check_array(y, input_name=&quot;y&quot;, **check_y_params)
    649     else:
--&gt; 650         X, y = check_X_y(X, y, **check_params)
    651     out = X, y
    653 if not no_val_X and check_params.get(&quot;ensure_2d&quot;, True):

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:1291, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1273 X = check_array(
   1274     X,
   1275     accept_sparse=accept_sparse,
   (...)
   1286     input_name=&quot;X&quot;,
   1287 )
   1289 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
-&gt; 1291 check_consistent_length(X, y)
   1293 return X, y

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:460, in check_consistent_length(*arrays)
    458 uniques = np.unique(lengths)
    459 if len(uniques) &gt; 1:
--&gt; 460     raise ValueError(
    461         &quot;Found input variables with inconsistent numbers of samples: %r&quot;
    462         % [int(l) for l in lengths]
    463     )

ValueError: Found input variables with inconsistent numbers of samples: [6396, 1599]
</code></pre>
<p>I tried everything but nothing works or I don't know how to solve the problem.</p>
<p>Jupyternaut:</p>
<blockquote>
<p>The error message you provided indicates that there is a problem with the input data. Specifically, it appears that there are two different versions of the input data, one with 6396 samples and another with 1599 samples. This can cause issues when trying to fit a model or perform other operations on the data.</p>
<p>To resolve this issue, you may need to check your code and ensure that you are using the correct version of the input data for each operation. You may also want to try cleaning up the input data by removing any duplicates or inconsistencies.</p>
</blockquote>
","0","Question"
"78719068","","<p>How to visualize samples that are used to test models that have been created using a confusion matrix? For example as below.
<a href=""https://i.sstatic.net/ED13sAEZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ED13sAEZ.png"" alt=""enter image description here"" /></a></p>
<p>you can visit GitHub, the whole thing is similar, just the architecture and dataset are different</p>
<p><a href=""https://github.com/cendekialnazalia/CaisimPestDetection/blob/main/Percobaan%20E%20-%20CNN%20add%20Models%20Xception.ipynb"" rel=""nofollow noreferrer"">https://github.com/cendekialnazalia/CaisimPestDetection/blob/main/Percobaan%20E%20-%20CNN%20add%20Models%20Xception.ipynb</a></p>
<p>this my code</p>
<p>Train model</p>
<pre><code>epochs = 10

mc = ModelCheckpoint('sequential', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)  
early_stopping = EarlyStopping(monitor='val_loss', patience=2)
history=model.fit(x=train_gen, epochs=epochs, validation_data=valid_gen)
</code></pre>
<pre><code>def print_info( test_gen, preds, print_code, save_dir, subject ):
    class_dict=test_gen.class_indices
    labels= test_gen.labels
    file_names= test_gen.filenames 
    error_list=[]
    true_class=[]
    pred_class=[]
    prob_list=[]
    new_dict={}
    error_indices=[]
    y_pred=[]
    for key,value in class_dict.items():
        new_dict[value]=key             # dictionary {integer of class number: string of class name}
    # store new_dict as a text fine in the save_dir
    classes=list(new_dict.values())     # list of string of class names
    dict_as_text=str(new_dict)
    dict_name= subject + '-' +str(len(classes)) +'.txt'  
    dict_path=os.path.join(save_dir,dict_name)    
    with open(dict_path, 'w') as x_file:
        x_file.write(dict_as_text)    
    errors=0      
    for i, p in enumerate(preds):
        pred_index=np.argmax(p)        
        true_index=labels[i]  # labels are integer values
        if pred_index != true_index: # a misclassification has occurred
            error_list.append(file_names[i])
            true_class.append(new_dict[true_index])
            pred_class.append(new_dict[pred_index])
            prob_list.append(p[pred_index])
            error_indices.append(true_index)            
            errors=errors + 1
        y_pred.append(pred_index)    
    if print_code !=0:
        if errors&gt;0:
            if print_code&gt;errors:
                r=errors
            else:
                r=print_code           
            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')
            print_in_color(msg, (0,255,0),(55,65,80))
            for i in range(r):                
                split1=os.path.split(error_list[i])                
                split2=os.path.split(split1[0])                
                fname=split2[1] + '/' + split1[1]
                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])
                print_in_color(msg, (255,255,255), (55,65,60))
                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               
        else:
            msg='With accuracy of 100 % there are no errors to print'
            print_in_color(msg, (0,255,0),(55,65,80))
    if errors&gt;0:
        plot_bar=[]
        plot_class=[]
        for  key, value in new_dict.items():        
            count=error_indices.count(key) 
            if count!=0:
                plot_bar.append(count) # list containg how many times a class c had an error
                plot_class.append(value)   # stores the class 
        fig=plt.figure()
        fig.set_figheight(len(plot_class)/3)
        fig.set_figwidth(10)
        plt.style.use('fivethirtyeight')
        for i in range(0, len(plot_class)):
            c=plot_class[i]
            x=plot_bar[i]
            plt.barh(c, x, )
            plt.title( ' Errors by Class on Test Set')
    y_true= np.array(labels)        
    y_pred=np.array(y_pred)
    if len(classes)&lt;= 30:
        # create a confusion matrix 
        cm = confusion_matrix(y_true, y_pred )        
        length=len(classes)
        if length&lt;8:
            fig_width=8
            fig_height=8
        else:
            fig_width= int(length * .5)
            fig_height= int(length * .5)
    
        plt.figure(figsize=(fig_width, fig_height))
        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       
        plt.xticks(np.arange(length)+.5, classes, rotation= 90)
        plt.yticks(np.arange(length)+.5, classes, rotation=0)
        plt.xlabel(&quot;Predicted&quot;)
        plt.ylabel(&quot;Actual&quot;)
        plt.title(&quot;Confusion Matrix&quot;)
        plt.show()
    clr = classification_report(y_true, y_pred, target_names=classes)
    print(&quot;Classification Report:\n----------------------\n&quot;, clr)
</code></pre>
<p>Confution matrix</p>
<pre><code>print_code=0
preds=model.predict(test_gen) 
print_info( test_gen, preds, print_code, save_dir, subject ) 
</code></pre>
<p>output
<a href=""https://i.sstatic.net/ZaNyPHmS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZaNyPHmS.png"" alt=""enter image description here"" /></a></p>
<p>I want to not only display the table output and recall, precision and f1 score values, but also display a visualization of each image predicted by CM, perhaps for example like the image above or it could be better.</p>
<p>for example in the &quot;Daun Sehat&quot;  table there is one data sample that is predicted to be &quot;Karat Merah&quot; but I don't know which image sample of &quot;Daun Sehat&quot;  is detected as &quot;Karat Merah&quot; if there is no visualization of that information</p>
<p>After the last line of code &quot;make predictions on test set and generate confusion matrix and classification report&quot; , I added code like the one below to find out the prediction for each test data</p>
<pre><code>test_gen.class_indices
</code></pre>
<pre><code>print(preds,preds.shape)
</code></pre>
<pre><code>result_index = np.argmax(preds[])
print(result_index)
</code></pre>
<pre><code>for i in range(len(preds)):
  if(np.argmax(preds[i]) == 0):
      print(&quot;Bercak Daun&quot;)
  elif(np.argmax(preds[i]) == 1):
      print(&quot;Daun Sehat&quot;)
  elif(np.argmax(preds[i]) == 2):
      print(&quot;Karat Merah&quot;)
  else:
      print(&quot;Lainya&quot;)
</code></pre>
<p>output</p>
<pre><code>Bercak Daun
Daun Sehat
.
.
.
up to 177
Daun Sehat
</code></pre>
<p>instead of only displaying the Prediction with the string I also want to display it along with the image as well. Maybe there is a better and more efficient code for my problem?</p>
","1","Question"
"78719585","","<p>I've literally been pulling my hair out for the last 2 days trying to fix this. I have made a file <code>test.py</code> to test my model by predicting a single sample:</p>
<pre><code>import os
import keras
import numpy as np
import tensorflow as tf
from main_copy import path_to_fft

model = keras.models.load_model(os.path.join(os.getcwd(), &quot;model.keras&quot;))

model.summary()
model.summary(expand_nested=True)

first_sample = path_to_fft(os.path.join(os.getcwd(), 'Sounds', 'Thomas', 'thomas_original.wav'))
second_sample = path_to_fft(os.path.join(os.getcwd(), 'Sounds', 'Thomas', 'thomas_original.wav'))

print(&quot;First: &quot;, first_sample.shape)
print(&quot;Second: &quot;, second_sample.shape)

prediction = model.predict([first_sample, second_sample])
print(prediction)
</code></pre>
<p>The <code>path_to_fft</code> function decodes a .wav file and applies a Fast Fourier Transform, then returns the first half of the positive frequencies of that transform, transforming the 5 second 16kHz audio from shape (80000, 1) to (40000, 1), which is the correct size for my model. However, when I try to predict the distance between two samples using <code>model.predict</code>, I get this error message:</p>
<pre><code>Model: &quot;functional&quot;
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ InputA (InputLayer)           │ (None, 40000, 1)          │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ InputB (InputLayer)           │ (None, 40000, 1)          │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ SiameseBranch (Functional)    │ (None, 32)                │      14,003,824 │ InputA[0][0], InputB[0][0] │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ lambda (Lambda)               │ (None, 1)                 │               0 │ SiameseBranch[0][0],       │
│                               │                           │                 │ SiameseBranch[1][0]        │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘

 Input 0 of layer &quot;SiameseBranch&quot; is incompatible with the layer: expected shape=(None, 40000, 1), found shape=(32, 1)

 Arguments received by Functional.call():
   • inputs=('tf.Tensor(shape=(32, 1), dtype=float32)', 'tf.Tensor(shape=(32, 1), dtype=float32)')
   • training=False
   • mask=('None', 'None')
</code></pre>
<p>When I scroll up, I see that three (?????) arguments are generated by Tensorflow?:</p>
<pre><code>Level 1:tensorflow:Creating new FuncGraph for Python function &lt;function StructuredFunctionWrapper.__init__.&lt;locals&gt;.trace_tf_function.&lt;locals&gt;.wrapped_fn at 0x0000027A795AE480&gt; (key: FunctionContext(context=EagerContext(parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=None, xla_context_id=0), scope_type=&lt;ScopeType.VARIABLE_CREATION: 2&gt;), Input Parameters:
  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(32,), dtype=tf.int64, name=None)
  args_1 (POSITIONAL_ONLY): TensorSpec(shape=(40000, 1), dtype=tf.float32, name=None)
  args_2 (POSITIONAL_ONLY): TensorSpec(shape=(40000, 1), dtype=tf.float32, name=None)
Output Type:
  None
Captures:
  None)
</code></pre>
<p>I have absolutely zero idea what to do or make of these error logs. This is my first time using models that have been saved as files and it's an absolute nightmare.</p>
<p>For reference, here is link to the GitHub repository (containing the <code>path_to_fft</code> function and code that was used to fit the model and the model itself): <a href=""https://github.com/brainage04/WestpacHackathon"" rel=""nofollow noreferrer"">https://github.com/brainage04/WestpacHackathon</a></p>
<p>Here is the complete error log from running the <code>test.py</code> function, from start to finish: <a href=""https://pastebin.com/iVZ7dUWn"" rel=""nofollow noreferrer"">https://pastebin.com/iVZ7dUWn</a></p>
","1","Question"
"78719712","","<p>I was running the yolov3 on custom dataset on my local laptop after doing everything when I am running this final command</p>
<pre><code>./darknet detector train DATASET/voc.data cfg/yolov3-voc.cfg darknet53.conv.74
</code></pre>
<p>And I am using windows power shell I am getting an error which is</p>
<pre><code>./darknet : The term './darknet' is not recognized as the name of a cmdlet, function, script file, or operable
program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:1
+ ./darknet detector train DATASET/voc.data cfg/yolov3-voc.cfg darknet5 ...
+ ~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (./darknet:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException
</code></pre>
<p>So how can I resolve this error</p>
<p>I have tried changing the system enviroment variable path and also I have tried running some command but it Still throwing an same error So what should I try</p>
","0","Question"
"78721703","","<p>I'm using Keras to make a prediction model. It takes in two time series and outputs a number between 0 and 1. Currently, I am getting very low accuracy as the model is only considered &quot;correct&quot; if it gets the exact number. For example, the correct number is 0.34, it would be considered incorrect if it predicted 0.35. I want to be able to consider all numbers within a range to be correct, for example: within 0.05 of the true value. Another option may be to round, but I have the problem of it outputting 6 decimal places.</p>
<ol>
<li>How can I consider all numbers within a range to be &quot;correct&quot; for the accuracy?</li>
<li>How can I round the output of the CNN?</li>
</ol>
<p>Here is my CNN code:</p>
<pre class=""lang-py prettyprint-override""><code>def networkModel():
    model = tf.keras.Sequential([

tf.keras.layers.Conv2D(filters = 16, kernel_size=(2, 2), activation='relu',padding='same'),
tf.keras.layers.Conv2D(filters = 9, kernel_size=(2, 2), activation='relu',padding='same'),
tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dense(1, activation='sigmoid')

])

    model.compile(optimizer='adam',
            loss = tf.keras.losses.BinaryCrossentropy(),
            metrics=['accuracy'])

    return model
</code></pre>
","1","Question"
"78722250","","<p>I was trying to build a linear regression model to predict the price of houses to begin with machine learning but come accross negative values of score when using cross validation in this code:</p>
<pre><code>from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
x = df.drop(['MedHouseVal'], axis=1)
y = df['MedHouseVal']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
model = LinearRegression()
model.fit(x_train, y_train)
model.score(x_test, y_test)
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, x, y, cv=100)
plt.plot(scores)
</code></pre>
<p>i noticed that as i increased cv, the average decreased score. Therefore, i decided to plot it and realized that the score takes on negative values at some points but how can true predictions/sample size be negative is it calculated with (TP + TN - FP - FN)/sample size?</p>
<p><a href=""https://i.sstatic.net/GAsYojQE.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","-2","Question"
"78722413","","<p>I want to use Keras' Grad-CAM with my own CNN model. I have followed this <a href=""https://keras.io/examples/vision/grad_cam/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/grad_cam/</a> where the function make_gradcam_heatmap is also from. I am new to CNNs so I might be missing something obvious, but why does it not recognise when I am running my model?</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import os
import tensorflow as tf
import keras
from tensorflow.keras.models import load_model
import cv2
from tensorflow.keras.models import Model

os.environ[&quot;KERAS_BACKEND&quot;] = &quot;tensorflow&quot;

from IPython.display import Image, display
import matplotlib as mpl
import matplotlib.pyplot as plt

img_path = '/Users/.../image_1.npy'

model = load_model('/Users/.../particle_classifier_model.h5')

model_builder = keras.applications.xception.Xception
preprocess_input = keras.applications.xception.preprocess_input
decode_predictions = keras.applications.xception.decode_predictions


image = np.load(img_path)
img_size = image.shape # should be an array of shape (240, 146)

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # First, we create a model that maps the input image to the activations
    # of the last conv layer as well as the output predictions
    grad_model = keras.models.Model(
        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last conv layer
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by &quot;how important this channel is&quot; with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # For visualization purpose, we will also normalize the heatmap between 0 &amp; 1
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()


last_conv_layer_name = &quot;max_pooling2d_4&quot;

image = image.reshape(1, 240, 146, 1)
preds = model.predict(image)

model.layers[-1].activation = None

heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name)
plt.matshow(heatmap)
plt.show()
</code></pre>
<p>My data are numpy arrays of dimension (240, 146) which the CNN takes as input.</p>
","3","Question"
"78724837","","<p>I am working on a project to analyze text messages to extract specific data from them. Regular expressions in Python don't work well because the text formats are constantly changing and there is no consistency. Therefore, I decided to use a language model to process these texts and return the result if the text contains what I am interested in.</p>
<p>While developing the program, I ran the model with the following command:</p>
<p><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</code></p>
<p>After this, I sent a request in my code as follows:</p>
<pre class=""lang-py prettyprint-override""><code>url = 'http://localhost:11434/api/generate'
data = {  
    &quot;model&quot;: &quot;llama3&quot;,
    &quot;prompt&quot;: f&quot;{input_text}&quot;,
}
</code></pre>
<p>This worked (although the response took a bit long to generate).</p>
<p>Now, when I try to configure my <code>docker-compose.yml</code> file to start the language model container, it looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>ollama:
  container_name: ollama
  image: ollama/ollama
  volumes:
    - ollama:/root/.ollama
  ports:
    - &quot;11434:11434&quot;

volumes:
  ollama:
</code></pre>
<p>But I just get a 404 error, meaning the endpoint is not found. I don't understand what I am doing wrong. Can someone help with this?</p>
<p>Additionally, does anyone know if the language model supports multithreading? My script sends text very quickly, and I am not sure whether to limit the rate of sending requests to the language model or if it can handle multithreading and asynchronous requests.</p>
","0","Question"
"78725972","","<p>I am trying to train a model based on a modified MNSIT dataset so it classifies random images with label 10. I am constantly getting a Typeerror.</p>
<pre><code>transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

dataset1 = datasets.MNIST(root='./data', train=True, transform = transform)
dataset2 = datasets.MNIST(root='./data', train=False, transform=transform)
num_new_images = 7000
noisy_images = torch.randn(num_new_images, 1, 28, 28)
mean = 0.1307
std = 0.3081
random_images = (noisy_images-mean)/std
noisy_labels = torch.full((num_new_images,),10, dtype=torch.long)
new_dataset = torch.utils.data.TensorDataset(noisy_images, noisy_labels)
combined_dataset = torch.utils.data.ConcatDataset([dataset1, new_dataset])
len(combined_dataset)   
num_val_images = 1000
noisy_images = torch.randn(num_val_images, 1, 28, 28)
random_val_images = (noisy_images-mean)/std
noisy_val_labels = torch.full((num_val_images,),10, dtype=torch.long)
new_val_dataset = torch.utils.data.TensorDataset(random_val_images, noisy_val_labels)
combined_val_dataset = torch.utils.data.ConcatDataset([dataset2, new_val_dataset])
batch_size = 128
train_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(combined_val_dataset, batch_size=batch_size, shuffle=False)
</code></pre>
<p>Error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In[12], line 74
     72 # Train the neural network
     73 for epoch in range(num_epochs):
---&gt; 74     for images, labels in train_loader:
     75         outputs = model(images)
     76         loss = criterion(outputs, labels)

File ~\PycharmProjects\tensorflow_start\venv\Lib\site-packages\torch\utils\data\dataloader.py:633, in _BaseDataLoaderIter.__next__(self)
    630 if self._sampler_iter is None:
    631     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    632     self._reset()  # type: ignore[call-arg]
--&gt; 633 data = self._next_data()
    634 self._num_yielded += 1
    635 if self._dataset_kind == _DatasetKind.Iterable and \
    636         self._IterableDataset_len_called is not None and \
    637         self._num_yielded &gt; self._IterableDataset_len_called:
</code></pre>
<p>I have already tried to change the datatypes of the labels, but it didn't work</p>
","0","Question"
"78726907","","<p>I'm creating a yoga AI trainer using ml5 and p5 on React.</p>
<p>I created a component where it takes an individual pose as a prop from a local JSON file. The component also loads a model which I added in the public folder. The goal of this component is to detect a certain yoga pose and the component dynamically returns the pose name that is detected from the webcam.</p>
<p>I tested two webcam pages. let's call it page 1 and page 2.
Page 1 works. the URL is /practice. page 1 leads to webcam 1. webcam 1 works.</p>
<p>page 2 the url is /practice/poseId. page 2 leads to a different webcam component, webcam 2 has the exact same code as webcam 1 except it takes in a prop and that prop is the specific pose that matches the id.</p>
<p>On page two, I get this error</p>
<pre><code>Error loading model: SyntaxError: Unexpected token '&lt;', &quot;&lt;!DOCTYPE &quot;... is not valid JSON
</code></pre>
<p>It is pointing to this code</p>
<pre><code>        const modelInfo = {
          model: &quot;model/model.json&quot;,
          metadata: &quot;model/model_meta.json&quot;,
          weights: &quot;model/model.weights.bin&quot;,
        };

        fetch(modelInfo.model)
          .then((response) =&gt; {
            if (!response.ok) {
              throw new Error(`HTTP error! status: ${response.status}`);
            }
            return response.json();
          })
          .then((data) =&gt; {
            console.log(&quot;Model JSON:&quot;, data);
            brain.load(modelInfo, brainLoaded);
          })
          .catch((error) =&gt; {
            console.error(&quot;Error loading model:&quot;, error);
          });
</code></pre>
<p>I don't understand why my component works on the /practice URL, but when I add poseId (/practice/:poseID), it shows that error even though it's the same code.</p>
<p>The error is on the URL that ends in /practice/:poseId e.g. /practice/1.</p>
<p>Error example (you don't see the pose label at the bottom):
<a href=""https://i.sstatic.net/xVLG6nDi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xVLG6nDi.png"" alt=""error"" /></a></p>
<p>Example (it works if the page URL is /practice)
<a href=""https://i.sstatic.net/6H0SVAWB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6H0SVAWB.png"" alt=""working"" /></a></p>
<p>This is my repo: <a href=""https://github.com/laura-nguyen/yoga-ai/tree/feature/page-pose-cam"" rel=""nofollow noreferrer"">https://github.com/laura-nguyen/yoga-ai/tree/feature/page-pose-cam</a></p>
","0","Question"
"78728307","","<p>I have a list of sentences, and a list of their ideal embeddings on a 25-dimensional vector. I am trying to use a neural network to generate new encodings, but I am struggling. While the model runs fine, its output makes no sense, and it doesn't even accurately replicate training data!</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Tokenization
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentence_list)
sequences = tokenizer.texts_to_sequences(sentence_list)


# Assuming your vectors are 25-dimensional
input_dim = 25

# Define encoder
input_vec = Input(shape=(max_sequence_length,))
encoded = Dense(25, activation='tanh')(input_vec)   # Example reduction to 16 dimensions
encoder = Model(input_vec, encoded)

# Define decoder
decoded = Dense(input_dim, activation='sigmoid')(encoded)
autoencoder = Model(input_vec, decoded)

# Compile model
autoencoder.compile(optimizer=Adam(), loss='mse')

# Train the model
autoencoder.fit(padded_sequences, combined_vectors_clean,
                epochs=10,
                batch_size=32,
                shuffle=True, validation_split= 0.2)
</code></pre>
<p>As far as I can tell, there's nothing wrong with my input and my labels, so what am I missing?</p>
","-1","Question"
"78728869","","<p>I have trained a yolov9 model on custom dataset for instance segmentation, now I want to get segmentation area after segmentation.</p>
<p>An output like the given below image but for each and every object segmented in the image.</p>
<p><a href=""https://i.sstatic.net/jyJKp1VF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jyJKp1VF.png"" alt=""enter image description here"" /></a></p>
<pre><code>from pathlib import Path
import numpy as np
import torch
import cv2


model = torch.hub.load('.', 'custom', path='yolov9-inst/runs/train-seg/gelan-c-seg15/weights/best.pt', source='local') 
# Image
img = 'WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg'
# Inference
res = model(img)
</code></pre>
<p>But I am getting this error while finding res only.</p>
<pre><code>YOLO 🚀 v0.1-104-g5b1ea9a Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (NVIDIA RTX A5000, 24248MiB)

Fusing layers... 
gelan-c-seg-custom summary: 414 layers, 27364441 parameters, 0 gradients, 144.2 GFLOPs
WARNING ⚠️ YOLO SegmentationModel is not yet AutoShape compatible. You will not be able to run inference with this model.
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[84], line 6
      4 img = 'WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg'
      5 # Inference
----&gt; 6 results = model(img)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-&gt; 1518     return self._call_impl(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File /workspace/yolov9-inst/./models/common.py:868, in DetectMultiBackend.forward(self, im, augment, visualize)
    866 def forward(self, im, augment=False, visualize=False):
    867     # YOLO MultiBackend inference
--&gt; 868     b, ch, h, w = im.shape  # batch, channel, height, width
    869     if self.fp16 and im.dtype != torch.float16:
    870         im = im.half()  # to FP16

AttributeError: 'str' object has no attribute 'shape'
</code></pre>
<p>Please can anyone help me to resolve this issue</p>
","-1","Question"
"78729235","","<p>I have a special binary classification use case, where, depending on the model's decisions, the next data it is evaluated on changes.</p>
<pre><code>An example:
[[x0,y0], [x1,y1], [x2,y2]...]

If the model predicts 1 for x0, then the next point it is evaluated on is [x1,y1].
If the model predicts 0 for x0, then the next point it is evaluated on is [x2,y2].
</code></pre>
<p>At first I thought I would just train the model on all the points, and that it would just be good in the final scenario where evaluation depends on previous prediction, but it isn't.</p>
<p>I developed a function that computes the evaluation function with the interdependent fashion I explained, and see that it doesn't improve when the loss function on the entire set of points improves.
It doesn't improve even though I train and validate on the same set of data.</p>
<p>So I would like to modify the loss function such that it uses the model to select the subset of training points that it should use for computing the loss, depending on the current state of the model (at each boosting step).</p>
<p>I believe this should be possible in theory. My questions are:</p>
<p>Do you also think it is possible?
If it is possible, how can I pass the model to the custom loss?</p>
<p>Thank you in advance!</p>
","0","Question"
"78731177","","<p><strong>System Information:</strong></p>
<ul>
<li>Model Builder Version17.15.0.2337001</li>
<li>Visual Studio Version 2022</li>
</ul>
<p>I created a machine learning model (Value Prediction), everything's fine
<img src=""https://github.com/dotnet/machinelearning-modelbuilder/assets/88751712/3eae4d97-e4ad-4d2c-b375-1bed09d684f6"" alt=""image"" /><br />
By this image, you see that model working normal
But when I use the code by the model builder:</p>
<pre class=""lang-cs prettyprint-override""><code>using ConsoleApp2;

//Load sample data
var sampleData = new MLModel1.ModelInput()
{
    Device = 1F,
    Temperature = 20F,
    Weather = 1F,
    Time = 2F,
};

//Load model and predict output
var result = MLModel1.Predict(sampleData);
Console.WriteLine(result.Predict);
</code></pre>
<p>The output alway is 0 or the predict alway is 0:
<img src=""https://github.com/dotnet/machinelearning-modelbuilder/assets/88751712/65b6cbc2-29a4-4dc4-a3b0-5f74aa4fb9d5"" alt=""image"" /></p>
","0","Question"
"78731508","","<p>I'm trying to train a keras model with 2 inputs: an image part that's a <code>tf.data.Dataset</code> and a nor mal part represented by a <code>pd.DataFrame</code></p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.optimizers import Adam
opt = Adam(learning_rate=1e-3, decay=1e-3 / 200)

model.compile(loss=&quot;mean_absolute_percentage_error&quot;, optimizer=opt)

model.fit(
    x=[df.loc[:, df.columns != 'target'], ds.batch(8)], y=df[&quot;target&quot;],
    epochs=200)
</code></pre>
<p>I was trying to fit the model but I get ValueError</p>
<pre><code>ValueError: Unrecognized data type: x=[...][401059 rows x 52 columns]
, &lt;_BatchDataset element_spec=(TensorSpec(shape=(None, 32, 256, 256, 3), 
dtype=tf.float32, name=None), 
TensorSpec(shape=(None, 32, 256, 256, 3), dtype=tf.float32, name=None))&gt;] (of type &lt;class 'list'&gt;)
</code></pre>
","0","Question"
"78735084","","<p>I was training various ANN when I encountered this error in this part. I don't know if any function has been depreciated or not.</p>
<pre><code>def analyze_results(rainfall_data, test_rainfall_data, name, flag=False):
    optimized_params = rainfall_data.loc[rainfall_data.RMSE.argmin]
    future_steps = optimized_params.future_steps
    forecast_values = optimized_params[-1*int(future_steps):]
    y_true = test_rainfall_data.iloc[:int(future_steps)]
    forecast_values.index = y_true.index
    
    print('=== Best parameters of ' + name + ' ===\n')
    if (name == 'FNN' or name == 'LSTM'):
        model = create_NN(optimized_params.look_back, 
                          optimized_params.hidden_nodes, 
                          optimized_params.output_nodes)
        print('Input nodes(p): ' + str(optimized_params.look_back))
        print('Hidden nodes: ' + str(optimized_params.hidden_nodes))
        print('Output nodes: ' + str(optimized_params.output_nodes))
    elif (name == 'TLNN'):
        model = create_NN(len(optimized_params.look_back_lags), 
                          optimized_params.hidden_nodes, 
                          optimized_params.output_nodes)
        s = ''
        for i in optimized_params.look_back_lags:
            s = s+' '+str(i)
        print('Look back lags: ' + s)
        print('Hidden nodes: ' + str(optimized_params.hidden_nodes))
        print('Output nodes: ' + str(optimized_params.output_nodes))
    elif (name == 'SANN'):
        model = create_NN(optimized_params.seasonal_period, 
                          optimized_params.hidden_nodes, 
                          optimized_params.seasonal_period)
        print('Input nodes(s): ' + str(optimized_params.seasonal_period))
        print('Hidden nodes: ' + str(optimized_params.hidden_nodes))
        print('Output nodes: ' + str(optimized_params.seasonal_period))
        
    print('Number of epochs: ' + str(optimized_params.epochs))
    print('Batch size: ' + str(optimized_params.batch_size))
    print('Number of future steps forecasted: ' + str(optimized_params.future_steps))
    print('Mean Squared Error(MSE): ' + str(optimized_params.MSE))
    print('Mean Absolute Error(MAE): ' + str(optimized_params.MAE))
    print('Root Mean Squared Error(RMSE): ' + str(optimized_params.RMSE))
    print('\n\n')

</code></pre>
<p>The error was shown on this line
<code>optimized_params = rainfall_data.loc[(rainfall_data.RMSE.argmin)]</code></p>
<p>The error was shown as</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_12964\1785142711.py in ?()
      9 
     10 # look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps
     11 parameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]
     12 
---&gt; 13 RMSE_info = compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)

~\AppData\Local\Temp\ipykernel_12964\2478982653.py in ?(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)
      1 def compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps):
      2 
      3     information_FNN_df = get_accuracies_FNN(rainfall_data, test_rainfall_data, parameters_FNN, scaler)
----&gt; 4     optimized_params_FNN = analyze_results(information_FNN_df, test_rainfall_data, 'FNN')
      5 
      6     information_TLNN_df = get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters_TLNN, scaler)
      7     optimized_params_TLNN = analyze_results(information_TLNN_df, test_rainfall_data, 'TLNN')

~\AppData\Local\Temp\ipykernel_12964\4019196368.py in ?(rainfall_data, test_rainfall_data, name, flag)
      1 def analyze_results(rainfall_data, test_rainfall_data, name, flag=False):
----&gt; 2     optimized_params = rainfall_data.loc[(rainfall_data.RMSE.argmin)]
      3     future_steps = optimized_params.future_steps
      4     forecast_values = optimized_params[-1*int(future_steps):]
      5     y_true = test_rainfall_data.iloc[:int(future_steps)]

~\anaconda3\Lib\site-packages\pandas\core\indexing.py in ?(self, key)
   1185         else:
   1186             # we by definition only have the 0th axis
   1187             axis = self.axis or 0
   1188 
-&gt; 1189             maybe_callable = com.apply_if_callable(key, self.obj)
   1190             maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable)
   1191             return self._getitem_axis(maybe_callable, axis=axis)

~\anaconda3\Lib\site-packages\pandas\core\common.py in ?(maybe_callable, obj, **kwargs)
    380     obj : NDFrame
    381     **kwargs
    382     &quot;&quot;&quot;
    383     if callable(maybe_callable):
--&gt; 384         return maybe_callable(obj, **kwargs)
    385 
    386     return maybe_callable

~\anaconda3\Lib\site-packages\pandas\core\base.py in ?(self, axis, skipna, *args, **kwargs)
    765     def argmin(
    766         self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs
    767     ) -&gt; int:
    768         delegate = self._values
--&gt; 769         nv.validate_minmax_axis(axis)
    770         skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)
    771 
    772         if isinstance(delegate, ExtensionArray):

~\anaconda3\Lib\site-packages\pandas\compat\numpy\function.py in ?(axis, ndim)
    395     ValueError
    396     &quot;&quot;&quot;
    397     if axis is None:
    398         return
--&gt; 399     if axis &gt;= ndim or (axis &lt; 0 and ndim + axis &lt; 0):
    400         raise ValueError(f&quot;`axis` must be fewer than the number of dimensions ({ndim})&quot;)

~\anaconda3\Lib\site-packages\pandas\core\generic.py in ?(self)
   1575     @final
   1576     def __nonzero__(self) -&gt; NoReturn:
-&gt; 1577         raise ValueError(
   1578             f&quot;The truth value of a {type(self).__name__} is ambiguous. &quot;
   1579             &quot;Use a.empty, a.bool(), a.item(), a.any() or a.all().&quot;
   1580         )

ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().



</code></pre>
","0","Question"
"78736775","","<p>I'm using Azure Machine Learning Studio and I have an <code>sklearn mlflow</code> model stored in my default datastore (blob storage) which I have then registered as a model asset. How can I load this model inside an interactive notebook to perform some quick model inferencing and testing before deploying this as a batch endpoint.</p>
<p>I have seen a post linked <a href=""https://learn.microsoft.com/en-us/answers/questions/1578278/using-registered-model-to-make-predictions-without"" rel=""nofollow noreferrer"">here</a> that suggests downloading the model artefacts locally but I shouldn't need to do this. I should be able to load the model directly from the datastore or the registered asset without the need to duplicate the model in multiple places. I have tried the following without success.</p>
<p><strong>Reading from Registered Model Asset</strong></p>
<pre><code>import mlflow
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Model

ml_client = MLClient(DefaultAzureCredential(), &quot;&lt;subscription_id&gt;&quot;, &quot;&lt;resource_group&gt;&quot;, &quot;&lt;workspace_id&gt;&quot;)

model = ml_client.models.get(&quot;&lt;model_name&gt;&quot;, version=&quot;1&quot;)
loaded_model = mlflow.sklearn.load_model(model.id)

&gt;&gt;&gt; OSError: No such file or directory: ...
</code></pre>
<p><strong>Reading from Datastore</strong></p>
<pre><code>import mlflow

model_path = &quot;&lt;datastore_uri_to_model_folder&gt;&quot;
loaded_model = mlflow.sklearn.load_model(model_path)

&gt;&gt;&gt; DeserializationError: Cannot deserialize content-type: text/html
</code></pre>
","0","Question"
"78736804","","<p>I have a problem with removing watermarks from a collection of images. The watermarks appear near each other but are different (see below). One watermark is a red square and has white text inside of it. The other is a semi-transparent grey sentence. The purpose is to process the images for machine learning purposes.</p>
<p>Problem solving attempts:</p>
<p>Because the watermarks vary in their location across the image dataset, I attempted the following:</p>
<ol>
<li>Made a copy of the image and converted it to the HSV colorspace</li>
<li>Selected a range of lower and upper values for the region of interest (the values were selected after segmenting the image and constructing a histogram for each channel)</li>
<li>Built a mask using the cv2.inRange() function</li>
<li>Used the mask to in-paint the watermark in the original image</li>
</ol>
<p>For the red square, the first three steps worked perfectly. But the third step only sort-of worked. The watermark is significantly less visible than before, but still very apparent. For the text, I was unable to get a good enough mask in step 3 - it simply is too close in color/pixel intensity to the surrounding area and the text itself.</p>
<p>This is looking more like a machine learning problem, which is fine, but I wanted to exhaust other options beforehand. Any thoughts on how to resolve this issue using machine learning or algorithmic methods?</p>
","-2","Question"
"78739816","","<p>I am working on document layout analysis and have been exploring CNNs and transformer-based networks for this task. Typically, images are passed as 3-channel RGB inputs to these networks. However, my data source is in PDF format, from which I can extract the exact position and character information directly.</p>
<p>I am concerned that converting this PDF data into images for analysis will result in the loss of valuable positional and character information. My idea is to modify the input dimensions of the CNN from the standard 3 RGB channels to a higher-dimensional input that includes this additional positional and character information.</p>
<p>I understand how CNNs work and highly suspect that this approach might not work, but I would appreciate any feedback or suggestions from the community. Has anyone experimented with augmenting input channels in this way, or does anyone have insights into integrating positional and character data directly into CNNs?</p>
","-3","Question"
"78740880","","<p>I was trying to interpret my binary classification model using the SHAP value for each feature. I am wondering:
Does the positive SHAP value mean the feature contributed more to predicting the &quot;1&quot; class and the negative SHAP value mean the feature contributed more to predicting the &quot;0&quot; class?
and what if I use absolute SHAP value difference to describe the feature contribution changes is this idea legit?</p>
","0","Question"
"78751670","","<p>I am trying to take first and second derivatives of functions in JAX however, my ways of doing that give me the wrong number or zeros. I have an array with two columns for each variable and two rows for each input</p>
<pre><code>import jax.numpy as jnp
import jax

rng = rng = jax.random.PRNGKey(1234)
array = jax.random.normal(rng, (2,2))
</code></pre>
<p>Two test functions</p>
<pre><code>def F1(arr):
    return 1/arr

def F2(arr):
    return jnp.array([arr[0]**2 + arr[1]**3])
</code></pre>
<p>and two methods of taking first and second derivatives, one using <code>jax.grad()</code></p>
<pre><code>def dF_m1(arr, F):
    return jax.grad(lambda arr: F(arr)[0])(arr)

def ddF_m1(arr, F, dF):
    return jax.grad(lambda arr: dF(arr, F)[0])(arr)
</code></pre>
<p>and another using <code>jax.jacobian()</code></p>
<pre><code>def dF_m2(arr, F):
    jac = jax.jacobian(lambda arr: F(arr))(arr)
    return jnp.diag(jac)

def ddF_m2(arr, F, dF):
    hess = jax.jacobian(lambda arr: dF(arr, F))(arr)
    return jnp.diag(hess)
</code></pre>
<p>Computing the first and second derivative (and error) of each function using both methods gives the following</p>
<pre><code>exact_dF1  = (-1/array**2)
exact_ddF1 = (2/array**3)

print(&quot;Function 1 using all grad()&quot;)
dF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)
ddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)
print(dF1_m1  - exact_dF1,&quot;\n&quot;)
print(ddF1_m1 - exact_ddF1,&quot;\n&quot;)

print(&quot;Function 1 using all jacobian()&quot;)
dF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)
ddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)
print(dF1_m2  - exact_dF1,&quot;\n&quot;)
print(ddF1_m2 - exact_ddF1,&quot;\n&quot;)
</code></pre>
<p>Output</p>
<pre><code>Function 1 using all grad()
[[ 0.         48.43877   ]
 [ 0.          0.62903005]] 

[[  0.        674.248    ]
 [  0.          0.9977852]] 

Function 1 using all jacobian()
[[0. 0.]
 [0. 0.]] 

[[0. 0.]
 [0. 0.]] 
</code></pre>
<p>and</p>
<pre><code>exact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))
exact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))

print(&quot;Function 2 using all grad()&quot;)
dF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)
ddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)
print(dF2_m1  - exact_dF2,&quot;\n&quot;)
print(ddF2_m1 - exact_ddF2,&quot;\n&quot;)

print(&quot;Function 2 using all jacobian()&quot;)
dF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)
ddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)
print(dF2_m2  - exact_dF2,&quot;\n&quot;)
print(ddF2_m2 - exact_ddF2,&quot;\n&quot;)
</code></pre>
<p>Output</p>
<pre><code>Function 2 using all grad()
[[0. 0.]
 [0. 0.]] 

[[0.         0.86209416]
 [0.         7.5651155 ]] 

Function 2 using all jacobian()
[[ 0.         -0.10149619]
 [ 0.         -6.925739  ]] 

[[0.        2.8620942]
 [0.        9.565115 ]] 
</code></pre>
<p>I would prefer only to use <code>jax.grad()</code> for something like <code>F1</code> but it seems right now that only <code>jax.jacobian</code> is working. The whole reason for this is that I need to calculate higher-order derivatives of a neural network with respect to its inputs. Thank you for any help.</p>
","1","Question"
"78751682","","<p>I'm hoping to use CLIP to get a single embedding for rows of multimodal (image and text) data.</p>
<p>Say I have the following model:</p>
<pre><code>from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import torchvision.transforms as transforms

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

def convert_image_data_to_tensor(image_data):
    return torch.tensor(image_data)

dataset = df[['image_data', 'text_data']].to_dict('records')

embeddings = []
for data in dataset:
    image_tensor = convert_image_data_to_tensor(data['image_data'])
    text = data['text_data']

    inputs = processor(text=text, images=image_tensor, return_tensors=True)
    with torch.no_grad():
        output = model(**inputs)
</code></pre>
<p>I want to get the embeddings calculated in <code>output</code>. I know that <code>output</code> has the addtributes <code>text_embeddings</code> and <code>image_embeddings</code>, but I'm not sure how they interact later on. If I want to get a single embedding for each record, should I just be concatenating these attributes together? Is there another attribute that combines the two in some other way?</p>
<p>These are the attributes stored in output:</p>
<pre><code>print(dir(output))

['__annotations__', '__class__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'image_embeds', 'items', 'keys', 'logits_per_image', 'logits_per_text', 'loss', 'move_to_end', 'pop', 'popitem', 'setdefault', 'text_embeds', 'text_model_output', 'to_tuple', 'update', 'values', 'vision_model_output']
</code></pre>
<p>Also, is there a way to specify the size of the embedding that CLIP outputs? Similar to how you can specify the embedding size in BERT configs?</p>
<p>Thanks in advance for any help here. Feel free to correct me if I'm misunderstanding anything critical here.</p>
","0","Question"
"78752483","","<p>I'm working with a large dataset (~10 million rows and 50 columns) in pandas and experiencing significant performance issues during data manipulation and analysis. The operations include filtering, merging, and aggregating the data, and they are currently taking too long to execute.</p>
<p>I've read about several optimization techniques, but I'm unsure which ones are most effective and applicable to my case. Here are a few specifics about my workflow:</p>
<p>I primarily use pandas for data cleaning, transformation, and analysis.
My operations include multiple groupby and apply functions.
I'm running the analysis on a machine with 16GB RAM.</p>
<p>Could the community share best practices for optimizing pandas performance on large datasets?</p>
<p>1.Memory management techniques.
2.Efficient ways to perform groupby and apply.
3.Alternatives to pandas for handling large datasets.
4.Any tips for parallel processing or utilizing multiple cores effectively.</p>
<p>I primarily use pandas for data cleaning, transformation, and analysis.
My operations include multiple groupby and apply functions.
I'm running the analysis on a machine with 16GB RAM.</p>
","2","Question"
"78752899","","<p>I have the following code using <code>AdamW</code> optimizer from Pytorch:</p>
<pre><code>optimizer = AdamW(params=self.model.parameters(), lr=0.00005)
</code></pre>
<p>I tried to log in using wandb as follows:</p>
<pre><code>lrs = {f'lr_group_{i}': param_group['lr']
       for i, param_group in enumerate(self.optimizer.param_groups)}
wandb.log({&quot;train_loss&quot;: avg_train_loss, &quot;val_loss&quot;: val_loss, **lrs})
</code></pre>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html"" rel=""nofollow noreferrer"">Note that</a> default value for <code>weight_decay</code> parameter is <code>0.01</code> for <code>AdamW</code>.</p>
<p>When I checked wandb dashboard, it showed the same LR for AdamW even after 200 epochs and it was not decaying at all. I tried this in several runs.</p>
<p><a href=""https://i.sstatic.net/IdppF0Wk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IdppF0Wk.png"" alt=""enter image description here"" /></a></p>
<p>Why the LR decay is not happening?</p>
<p>Also, it was showing LR for only one param group. Why is it so? It seems that I miss something basic here. Can someone point out?</p>
","0","Question"
"78753201","","<p>I am trying to find the chemical state of a particle with deep learning. As inputs I have the position of the particle according to time in X_train with shape (num_train,sequence_length). (My sequence length is 100), the ouput is the frame of transition (between 1 and 100) contained in Y_train of shape (num_train,1).</p>
<p>Here is an example of a sequence  (<a href=""https://i.sstatic.net/Ddmhjc24.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/Ddmhjc24.jpg</a>) with the transition at frame 84.</p>
<p>All of the data is generated with a very specific algorithm, however the algorithm doesn't generate very complex data, and I think it's easy to find the transition by myself, but I want this deep learning model to work.</p>
<p>Here is the LSTM Code :</p>
<pre><code># Filtering

# Définir le modèle LSTM
model = Sequential([
    LSTM(64, input_shape=(sequence_length, 1), return_sequences=False),
    Dense(64, activation='relu'),
    Dense(1)
])

# Compiler le modèle
model.compile(optimizer='adam', loss='mse')  # Mean Squared Error pour une régression

# Afficher le résumé du modèle
model.summary()

# Entraîner le modèle
model.fit(X_train, Y_train, epochs=40, batch_size=32, validation_data=(X_test, Y_test))

# Exemple de prédiction sur une nouvelle donnée
prediction = model.predict(X_test)
print(prediction)
</code></pre>
<p>The results :</p>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 64)                16896     
                                                                 
 dense (Dense)               (None, 64)                4160      
                                                                 
 dense_1 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 21121 (82.50 KB)
Trainable params: 21121 (82.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - loss: 1043.6710 - val_loss: 840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - loss: 840.9444 - val_loss: 839.9596
Epoch 3/10
631/631 [==============================] - 32s 50ms/step - loss: 841.6289 - val_loss: 840.7188
Epoch 4/10
631/631 [==============================] - 30s 48ms/step - loss: 840.9946 - val_loss: 840.6344
Epoch 5/10
631/631 [==============================] - 33s 52ms/step - loss: 841.8745 - val_loss: 839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/step - loss: 841.6499 - val_loss: 839.8434
Epoch 7/10
631/631 [==============================] - 31s 49ms/step - loss: 841.2045 - val_loss: 840.0717
Epoch 8/10
631/631 [==============================] - 30s 48ms/step - loss: 842.0576 - val_loss: 840.2137
Epoch 9/10
631/631 [==============================] - 33s 52ms/step - loss: 842.7056 - val_loss: 840.5657
Epoch 10/10
631/631 [==============================] - 30s 48ms/step - loss: 841.5714 - val_loss: 839.8404
70/70 [==============================] - 2s 16ms/step
[[52.569366]
 [52.569286]
 [52.569378]
 ...
 [52.569344]
 [52.569313]
 [52.56937 ]]
</code></pre>
<p>As you can see, the output is the same whatever the input is when I test the trained model. The val_loss doesn't improve with the number of epochs. And this is the problem, I don't understand what happens.</p>
<p>I triple checked my data, X_train is normalized, and I tried to add some drop out, and other layers on my model but nothing changes.</p>
<p>Maybe there is no possibility to do this with LSTM, but I think the data is very simple. I really want to try to find a way to use deep learning to find this.</p>
","0","Question"
"78757193","","<p>I encountered an error like this:</p>
<p>The output of MHSA (multi-head self-attention) is as follows:</p>
<pre><code>torch.Size([20, 197, 768])
</code></pre>
<ul>
<li>20 for batch size</li>
<li>197 for sequence length (previously 196, after adding the class token it became 197)</li>
<li>768 for embedding dimension</li>
</ul>
<p>I want to reshape it to fit the format below in order to feed it to a convolutional layer:</p>
<pre><code>torch.Size([batch_size, channel, width, height])
</code></pre>
<p>I've attempted to achieve this by adding a new dimension using the following approach:</p>
<pre><code>torch.unsqueeze(1)
torch.transpose(1, 3)
</code></pre>
<p>This successfully allows feeding to the convolutional layer. However, I'm unsure if this approach is correct, so please correct me if it's not.</p>
<p>Currently, I'm trying a different approach:</p>
<pre><code>new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)
</code></pre>
<p>This resulted in an error stating that the shape is invalid for an input of size (some_number). This is because the sequence length (197) doesn't square perfectly, yielding a decimal number, and the view function expects an integer input, the square operation yields 16 after converting to an integer, but batch_size * 768 * 16 * 16 does not equal batch_size * 197 * 768, leading to the error</p>
<p>Is my analysis correct? And how can I resolve this issue? and is there any better approach?</p>
","-1","Question"
"78758312","","<p>I am working on a RAG application using DSPy and ChromaDB for pdf files.</p>
<p>At first I fetched the text from the pdf and add it to the Chromadb as chunks. Also added the embeddings of the chunks. And tried to Retrieve the chunk that is related to the query using DSPy. But its getting an error</p>
<p>storing data and embeddings</p>
<pre><code>def store_document_in_chromadb(text):
    chunks = chunk_document(text)
    ids = [f'chunk_{i}' for i in range(len(chunks))]
    embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

    collection.add(ids=ids, documents=chunks, embeddings=embeddings)
</code></pre>
<p>And I try to retrieve the relevant chunks like this,</p>
<pre><code>retriever_model = ChromadbRM(&quot;contracts_collection&quot;, 'db/', k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
    &quot;&quot;&quot;Answer the question based on the context given.&quot;&quot;&quot;
    context = dspy.InputField(desc=&quot;may contain relevant context&quot;)
    question = dspy.InputField()
    answer = dspy.OutputField(desc=&quot;often between 5 to 10 words&quot;)

class RAG(dspy.Module): 
    def __init__(self, num_passages=2):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=num_passages)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
    
    def forward(self, question):
        context = self.retrieve(question).passages
        prediction = self.generate_answer(context=context, question=question)
        return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
  module = RAG()
  response = module(&quot;What is the Total Spend&quot;)
  print(response)
</code></pre>
<p>When I a running this, getting this error</p>
<p><code>InvalidDimensionException: Embedding dimension 384 does not match collection dimensionality 768</code></p>
<p>but when I remove the embedding from the ChromaDB, its retrieving the relevant chunks correctly.</p>
<p>Why is this not getting while using embeddings?</p>
","1","Question"
"78759790","","<p>I have a number of datasets, which I create from a dictionary like so:</p>
<pre><code>info = DatasetInfo(
        description=&quot;my happy lil dataset&quot;,
        version=&quot;0.0.1&quot;,
        homepage=&quot;https://www.myhomepage.co.uk&quot;
    )
train_dataset = Dataset.from_dict(prepare_data(data[&quot;train&quot;]), info=info)
test_dataset = Dataset.from_dict(prepare_data(data[&quot;test&quot;]), info=info)
validation_dataset = Dataset.from_dict(prepare_data(data[&quot;validation&quot;]),info=info)
</code></pre>
<p>I then combine these into a DatasetDict.</p>
<pre><code># Create a DatasetDict
dataset = DatasetDict(
    {&quot;train&quot;: train_dataset, &quot;test&quot;: test_dataset, &quot;validation&quot;: validation_dataset}
)
</code></pre>
<p>So far, so good. If I access <code>dataset['train'].info.description</code> I see the expected result of &quot;My happy lil dataset&quot;.</p>
<p>So I push to the hub, like so:</p>
<pre><code>dataset.push_to_hub(f&quot;{organization}/{repo_name}&quot;, commit_message=&quot;Some commit message&quot;)
</code></pre>
<p>And this succeeds too.</p>
<p>However, when I come to pull the dataset back down from the hub, and access the information associated with it, rather than getting the description of my dataset, I just get an empty string; like so:</p>
<pre><code>pulled_data = full = load_dataset(&quot;f{organization}/{repo_name}&quot;, use_auth_token = True)

# I expect the following to print out &quot;my happy lil dataset&quot;
print(pulled_data[&quot;train&quot;].info.description)
# However, instead it returns ''
</code></pre>
<p>Am I loading my data in from the hub incorrectly? Am I pushing only my dataset and not the info somehow?
I feel like I’m missing something obvious, but I’m really not sure.</p>
","0","Question"
"78761234","","<pre><code>NotImplementedError                       Traceback (most recent call last)
Cell In[19], line 1
----&gt; 1 mlmodel = ct.convert(model, convert_to=&quot;mlmodel&quot;, source=&quot;tensorflow&quot;)

File ~/anaconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:551, in convert(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)
    539 exact_target = _determine_target(convert_to, minimum_deployment_target)
    540 _validate_conversion_arguments(
    541     model,
    542     exact_source,
   (...)
    549     minimum_deployment_target,
    550 )
--&gt; 551 need_fp16_cast_pass = _need_fp16_cast_pass(compute_precision, exact_target)
    553 if pass_pipeline is None:
    554     pass_pipeline = PassPipeline()

File ~/anaconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:624, in _need_fp16_cast_pass(compute_precision, convert_to)
    620 def _need_fp16_cast_pass(
    621     compute_precision: Optional[Union[precision, FP16ComputePrecision]], convert_to: Text
    622 ) -&gt; bool:
    623     if convert_to not in (&quot;mlprogram&quot;, &quot;neuralnetwork&quot;, &quot;milinternal&quot;, &quot;milpython&quot;):
--&gt; 624         raise NotImplementedError(f&quot;Backend converter {convert_to} not implemented&quot;)
    626     if compute_precision is None:
    627         return convert_to != &quot;neuralnetwork&quot;

NotImplementedError: Backend converter mlmodel not implemented
</code></pre>
<p>No idea what this error means. I am trying to convert my CNN model, which I built using tensorflow, to a coreml model, but I keep getting the error above.</p>
<pre><code>mlmodel = ct.convert(model, convert_to=&quot;mlmodel&quot;, source=&quot;tensorflow&quot;)
</code></pre>
<p>This is done after importing the model from a saved .h5 file.</p>
","0","Question"
"78761783","","<p>Error message as in the title. It doesn't make sense to me, per my code below:</p>
<pre><code>clf = xgboost.XGBClassifier(verbosity=1)
print (clf.__class__, clf.verbosity) 
# prints &lt;class 'xgboost.sklearn.XGBClassifier'&gt; 1
clf.fit(X=train_data_iter[features].fillna(0), y=train_data_iter['y']) # the error is raised here
</code></pre>
<p>The value is clearly 1, but it somehow gets -1? I don't get it.</p>
","0","Question"
"78762914","","<p>I am trying to use the <code>bm3d</code> package for image denoising in Google Colab. I was able to use it without any issues a few days ago. However, now I am facing an import error even though the package installs successfully.</p>
<ol>
<li><p>Install the <code>bm3d</code> package:
<code>!pip install bm3d</code></p>
</li>
<li><p>Try to import the <code>bm3d</code> package:
<code>import bm3d</code></p>
</li>
</ol>
<p>Observed Error:</p>
<pre><code>ImportError: cannot import name '_sub_module_deprecation' from 
'scipy._lib.deprecation' (/usr/local/lib/python3.10/dist-packages/scipy/_lib/deprecation.py)
</code></pre>
<p>How to resolve the issue and import the package in colab?</p>
","0","Question"
"78763327","","<p>I want to do LLaVA inference in ollama, so I need to convert it in gguf file format.
My model has the file format safetensors.(trained with lora)
It seems that ollama supports only llama, but not llava as shown here,
<a href=""https://github.com/ollama/ollama/blob/main/docs/import.md"" rel=""nofollow noreferrer"">https://github.com/ollama/ollama/blob/main/docs/import.md</a></p>
<p>I followed the instruction of llama.cpp, and used the code convert_lora_to_gguf.py here,
<a href=""https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py"" rel=""nofollow noreferrer"">https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py</a></p>
<p>But I get an error like,</p>
<pre><code>ERROR:lora-to-gguf:Model LlavaLlamaForCausalLM is not supported
</code></pre>
<p>If I write llama model in config.json of model file and run following code, then I got another error.</p>
<pre><code>model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;Model successfully exported to {model_instance.fname_out}&quot;)
</code></pre>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
    model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module 'gguf' has no attribute 'GGUFType'
</code></pre>
<p>It seems that all codes and gguf package don't support llava, but llama only. I have to convert my own trained model into gguf. I cannot use gguf llava model from hugging face for inference.</p>
<p>Is there a way to convert it?</p>
","1","Question"
"78765868","","<p>I have a use case where I will record a speech in live stream, and I want to have the text transcription of the audio, and then the translation of this transcription, all in real-time.</p>
<p>Do I need to use the Speech to Text API of Google and then send the generated text to the Translation API or can I do it in one row ?</p>
","0","Question"
"78767192","","<p>I am using KNNImputer to impute np.nan values in several pd.DataFrame. I checked that all the datatypes of each one of the dataframes are numeric. However, KNNImputer drops some columns in some dataframes:</p>
<pre><code>&gt;&gt;&gt;input_df.shape   
(816, 216) 

&gt;&gt;&gt; input_df.dtypes.value_count()
float64    216
dtype: int64

&gt;&gt;output_df.shape 
(816, 27)
</code></pre>
<p>I used the following KNNImputer configuration</p>
<pre><code>imputer = KNNImputer(n_neighbors=1, 
                     weights=&quot;uniform&quot;,
                     add_indicator=False)

output_df = imputer.fit_transform(input_df)
</code></pre>
<p>I would like to know why it is happening since each one of the dataframes have np.nan values. By the way, the parameter n_neighbors=1 should not have any impact in the outcome since I am replacing missing values with the values of the closest neighbor.</p>
","0","Question"
"78769858","","<p>My yTrue are basically like [.2,.8] but never [1,0] or [0,1]
sum(yTrue)=1 always</p>
<p>I tried CategoricalCrossentropy But a TypeError occured-</p>
<blockquote>
<p>TypeError: Expected float32, but got &lt;keras.src.losses.losses.CategoricalCrossentropy object at 0x7752c1300580&gt; of type 'CategoricalCrossentropy'.</p>
</blockquote>
","-2","Question"
"78773489","","<p>I have three trained binary classification models, they are trained with <strong>sigmoid</strong> activation at output layer.</p>
<ol>
<li>First model returning probability scalar from 0 to 1 to check whether image is number <strong>ZERO</strong> or not.</li>
<li>Second model returning probability scalar from 0 to 1 to check whether image is number <strong>ONE</strong> or not.</li>
<li>Third model returning probability scalar from 0 to 1 to check whether image is number <strong>TWO</strong> or not.</li>
</ol>
<p><a href=""https://i.sstatic.net/1GxF5p3L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1GxF5p3L.png"" alt=""enter image description here"" /></a></p>
<p>I know I can just trained them with <strong>softmax</strong> with constructing model with three neurons at output layer. But suppose I have a situation where it's really take a long time to train their weights due to complex model, all I have just their individual model of binary classification. Or, I want extract their hidden representation features at hidden layer for example <code>model_0</code> (binary classification to check whether image is number zero or not).</p>
<p>So, how to concat/combine/merge them into single model?
My code currently stuck at this point:</p>
<pre><code>model_0 = init_binary_classification_model((28,28))
model_0.load_weights('trained_weight_of_binary_classification_to_check_whether_image_is_zero.h5')

model_1 = init_binary_classification_model((28,28))
model_1.load_weights('trained_weight_of_binary_classification_to_check_whether_image_is_one.h5')

model_2 = init_binary_classification_model((28,28))
model_2.load_weights('trained_weight_of_binary_classification_to_check_whether_image_is_two.h5')
</code></pre>
<p>Where:</p>
<pre><code>def init_binary_classification_model(input_shape=(28,28)):
  input_layer = Input(shape=input_shape)
  tensor = Flatten()(input_layer)
  tensor = Dense(16, activation='relu')(tensor)
  tensor = Dense(8, activation='relu')(tensor)
  output_layer = Dense(1, activation='sigmoid')(tensor)

  return Model(inputs=input_layer, outputs=output_layer)
</code></pre>
<p>I expect the multi-classification model has same input shape <code>(28,28)</code> and different output shape <code>(3)</code> and I don't need retrain the model (if possible).</p>
<p>Full code is available at <a href=""https://colab.research.google.com/drive/1y1mvAzebIFU_cuEQo8Q60L1I6uT8i2Ce?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1y1mvAzebIFU_cuEQo8Q60L1I6uT8i2Ce?usp=sharing</a></p>
","-2","Question"
"78773942","","<p>Here is the full error:</p>
<pre><code>`---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[33], line 2
      1 gnb = GaussianNB()
----&gt; 2 cv = cross_val_score(gnb,X_train,y_train,cv=5, error_score = 'raise')
      3 print(cv)
      4 print(cv.mean())

File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    512 # To ensure multimetric format is not supported
    513 scorer = check_scoring(estimator, scoring=scoring)
--&gt; 515 cv_results = cross_validate(
    516     estimator=estimator,
    517     X=X,
    518     y=y,
    519     groups=groups,
    520     scoring={&quot;score&quot;: scorer},
    521     cv=cv,
    522     n_jobs=n_jobs,
    523     verbose=verbose,
    524     fit_params=fit_params,
    525     pre_dispatch=pre_dispatch,
    526     error_score=error_score,
    527 )
    528 return cv_results[&quot;test_score&quot;]

File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--&gt; 266 results = parallel(
    267     delayed(_fit_and_score)(
    268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63, in Parallel.__call__(self, iterable)
     58 config = get_config()
     59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
---&gt; 63 return super().__call__(iterable_with_config)

File /opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1918, in Parallel.__call__(self, iterable)
   1916     output = self._get_sequential_output(iterable)
   1917     next(output)
-&gt; 1918     return output if self.return_generator else list(output)
   1920 # Let's create an ID that uniquely identifies the current call. If the
   1921 # call is interrupted early and that the same instance is immediately
   1922 # re-used, this id will be used to prevent workers that were
   1923 # concurrently finalizing a task from the previous call to run the
   1924 # callback.
   1925 with self._lock:

File /opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1847, in Parallel._get_sequential_output(self, iterable)
   1845 self.n_dispatched_batches += 1
   1846 self.n_dispatched_tasks += 1
-&gt; 1847 res = func(*args, **kwargs)
   1848 self.n_completed_tasks += 1
   1849 self.print_progress()

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123, in _FuncWrapper.__call__(self, *args, **kwargs)
    121     config = {}
    122 with config_context(**config):
--&gt; 123     return self.function(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686, in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)
    684         estimator.fit(X_train, **fit_params)
    685     else:
--&gt; 686         estimator.fit(X_train, y_train, **fit_params)
    688 except Exception:
    689     # Note fit time as time until error
    690     fit_time = time.time() - start_time

File /opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:267, in GaussianNB.fit(self, X, y, sample_weight)
    265 self._validate_params()
    266 y = self._validate_data(y=y)
--&gt; 267 return self._partial_fit(
    268     X, y, np.unique(y), _refit=True, sample_weight=sample_weight
    269 )

File /opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:427, in GaussianNB._partial_fit(self, X, y, classes, _refit, sample_weight)
    424 if _refit:
    425     self.classes_ = None
--&gt; 427 first_call = _check_partial_fit_first_call(self, classes)
    428 X, y = self._validate_data(X, y, reset=first_call)
    429 if sample_weight is not None:

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:420, in _check_partial_fit_first_call(clf, classes)
    413             raise ValueError(
    414                 &quot;`classes=%r` is not the same as on last call &quot;
    415                 &quot;to partial_fit, was: %r&quot; % (classes, clf.classes_)
    416             )
    418     else:
    419         # This is the first call to partial_fit
--&gt; 420         clf.classes_ = unique_labels(classes)
    421         return True
    423 # classes is None and clf.classes_ has already previously been set:
    424 # nothing to do

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:107, in unique_labels(*ys)
    105 _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
    106 if not _unique_labels:
--&gt; 107     raise ValueError(&quot;Unknown label type: %s&quot; % repr(ys))
    109 if is_array_api:
    110     # array_api does not allow for mixed dtypes
    111     unique_ys = xp.concat([_unique_labels(y) for y in ys])

ValueError: Unknown label type: (array([0.0, 1.0], dtype=object),)`
</code></pre>
<p>I am trying to implement cross-validation in the Spaceship Titanic Project on Kaggle, you can find my full notebook <a href=""https://www.kaggle.com/code/nicklausmathias/space-titanic-prediction"" rel=""nofollow noreferrer"">here</a>. I have not been able to get the cross-validation function to run. I have been fiddling with it for hours, and I have not been able to find the solution.</p>
<p>I followed <a href=""https://www.kaggle.com/code/kenjee/titanic-project-example/comments#2924956"" rel=""nofollow noreferrer"">Ken Jee's Titanic Project Example</a> to complete my own <a href=""https://www.kaggle.com/code/nicklausmathias/titanic-prediction"" rel=""nofollow noreferrer"">Titanic Prediction submission</a>. In this code, the cross-validation function can successfully run, and I tried to re-purpose this to the Spaceship Titanic project.</p>
<p>Please let me know if you need any other details. I am quite new to Machine Learning, so I thank you in advance for your patience.</p>
<p>I've tried getting rid of null values, verified that the data types for my training and test lists are the same, and I've tried mapping categorical variables to numerical variables.</p>
","0","Question"
"78776364","","<p>I have had implemented Yolo 3D model in my workspace, and it is working fine. I am getting an image as an output. So I would like to know where I can get the labels for my 3D bounding boxes. And I would even like know to the the format in which labels are stored is it.</p>
<p>x,y,x,y... format or x,y,z,x,y,z... format.</p>
<p>(Edit)
I have implemented the Yolo 3D model:
<a href=""https://github.com/ruhyadi/YOLO3D"" rel=""nofollow noreferrer"">https://github.com/ruhyadi/YOLO3D</a></p>
<p>I downloaded the pretrained weights, and I have run the inference using:</p>
<pre><code>python inference.py \
    --weights yolov5s.pt \
    --source eval/image_2 \
    --reg_weights weights/resnet18.pkl \
    --model_select resnet18 \
    --output_path runs/ \
    --show_result --save_result
</code></pre>
<p>Now in the runs folder, I am just getting a PNG image as an output. My output is:</p>
<p><a href=""https://i.sstatic.net/2f4PtDaM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2f4PtDaM.png"" alt=""enter image description here"" /></a></p>
<p>Now I would like to even get the points for the 3D bounding boxes in this image, and I would even like to know the format in which those points are stored are they in x,y,x,y.. format or in x,y,z,x,y,z.. format?</p>
","1","Question"
"78778576","","<p>I am using the Walker2D environment from <code>pybullet_envs</code> and I am trying to get the 6 joint angles in order to use them for an actual bipedal robot controlled by an Arduino.</p>
<p>I tried but didn't manage to find documentation such as the one of <code>gym</code> where the indices for the 6 joint angles are specified and their value are measured by rad. I found someone that indicated that inside the <code>pybullet_envs</code> GitHub documentation there is commented that the joint positions are the even elements of the observation space. I think that this true starting from the index 8 of the array for each column for the Walker2D env. My problem here is that I don't know the units of these values and I am not sure that they are correct. Is there an easier way to access the actual values (rad or deg) through the environment with the Physics Client Id and the Robot Id?</p>
","0","Question"
"78780530","","<p>we are evaluating apache flink for deploying streaming ml applications.</p>
<p>How is dependency management handled in apache flink and especially the execution environment?</p>
<p>Imagine python tasks with different dependencies should be submitted to the flink cluster.</p>
<p>We only see that flink Task Manager can handle dependency management with python virtual environments. When we have different dependencies for every task should we deploy a new task manager for every task?</p>
<p>Coming from a container setup, we could deploy every task inside a separate docker image.</p>
<p>How is this usually handled when using apache flink? We do not see that flink is great at handling a huge number of tasks that need their specific dependencies, but would like to make use of the streaming processor.</p>
","0","Question"
"78781313","","<p>I am trying to make an LSTM model that will detect anomalies in timeseries data. It takes 5 inputs and produces 1 boolean output (True/False if anomaly is detected). The anomaly pattern will usually be between 3 - 4 timesteps in a row. Unlike most LSTM examples where they are forecasting to predict future data, or classifying a whole sequence of data, I am trying to have a True/False detection flag output at every timestep (True at the last timestep point in the patter if it is detected).</p>
<p>Unfortunately it seems like CrossEntropyLoss doesn't allow for anything more than 1D output tensors, and in this case it will be 2D [num sequences, length of sequence with boolean data]</p>
<p>Here is some example code of what I am trying to produce:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define LSTM classifier model
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Input - 100 examples containing 5 data points per timestep (where there are 10 timesteps)
X_train = np.random.rand(100, 10, 5)
# Output - 100 examples containing 1 True/False output per timestep to match the input
y_train = np.random.choice(a=[True, False], size=(100, 10))  # Binary labels (True or False)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.bool)

# Define model parameters
input_size = X_train.shape[2] # 5 inputs per timestep
hidden_size = 4 # Pattern we are trying to detect is usually 4 timesteps long
num_layers = 1
output_size = 1 # True/False

# Instantiate the model
model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 10
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

# Test the model
X_test = np.random.rand(10, 10, 5) # Generate some test data - same dimensions as input
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
with torch.no_grad():
    predictions = model(X_test_tensor)
    predicted_outputs = torch.argmax(predictions, dim=1)
    print(&quot;Predicted Outputs:&quot;, predicted_outputs)
</code></pre>
<p>Do I need to re-shape the output (or make the number of outputs from the LSTM equal to the sequence length), or perhaps use a different loss function, or a model other than LSTM?</p>
","0","Question"
"78784723","","<p>I want to create a recursice model to solve the most simple sequence that I know, Arithmetic progression. With having <code>a</code> as the base and <code>d</code> as the step size, the sequence would be as follows:</p>
<p><code>a, a+d, a+2d, a+3d, a+4d, ...</code></p>
<p>To solve this, denoting hidden state as <code>h</code>, the model has to learn a simple 2*2 matrix. This is actually setting <code>h1 = t0</code>.</p>
<p><a href=""https://i.sstatic.net/Qsy1qmgn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qsy1qmgn.png"" alt=""enter image description here"" /></a></p>
<p>To put it in other words, you can see it like this too:</p>
<p><a href=""https://i.sstatic.net/U7fmayED.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/U7fmayED.png"" alt=""enter image description here"" /></a></p>
<p>So this model with a 2*2 fully connected layer should be able to learn this matrix:</p>
<pre><code>class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(2, 2, bias=False)

    def forward(self, x):
        x = self.fc1(x)
        return x
</code></pre>
<p>But to my surprise is does not converge! There should be something wrong with my setup. If you help me find it I will appreciate it. I suspect the problem should be in my training loop.</p>
<p>P.S. I intentionally set batch size to 1 right now. I want to work with padding the input data later. The model should learn without batches anyway.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np

class CustomDataset(Dataset):
    def __init__(self, size):
        self.size = size

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        a0 = (np.random.rand() - 0.5) * 200
        d = (np.random.rand() - 0.5) * 40
        length = np.random.randint(2, MAX_Length_sequence + 1)

        sequence = np.arange(length) * d + a0
        next_number = sequence[-1] + d

        return length, torch.tensor(sequence, dtype=torch.float32), torch.tensor(next_number, dtype=torch.float32)

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(2, 2, bias=False)

    def forward(self, x):
        x = self.fc1(x)
        return x

# Hyperparameters
EPOCHS = 10
BATCH_SIZE = 1
LEARNING_RATE = 0.001
DATASET_SIZE = 10000
criterion = nn.MSELoss()

# Model
model = Model()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

</code></pre>
<p>My traning loop:</p>
<pre><code>for epoch in range(EPOCHS):
    dataset = CustomDataset(DATASET_SIZE)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)
    model.train()
    total_loss = 0

    for length, sequence, next_number in dataloader:
        optimizer.zero_grad()
        loss = 0
        h = torch.zeros(BATCH_SIZE)

        for i in range(length):
            x = torch.cat([h, sequence[0, i].unsqueeze(0)])
            y = sequence[0, i + 1] if i != length - 1 else next_number[0]

            output = model(x)
            h, y_hat = output[0].unsqueeze(0), output[1]

            loss += criterion(y_hat, y)

        loss.backward()
        optimizer.step()
        total_loss += loss.item() 
        
    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')
</code></pre>
","0","Question"
"78786001","","<p>I want mask my images with json formats as data for training model of u net.
I use this below code to mask them:</p>
<pre><code>
import json
import numpy as np
import cv2
import os

# Path to the folder containing JSON files
json_folder = os.path.expanduser('~/Desktop/jeson')  # Folder with JSON files
# Path to the folder for saving mask images
mask_folder = os.path.expanduser('~/Desktop/masks')  # Folder to save masks

# Ensure the folder for saving masks exists
os.makedirs(mask_folder, exist_ok=True)

# List all JSON files in the folder
for filename in os.listdir(json_folder):
    if filename.endswith('.json'):
        json_file = os.path.join(json_folder, filename)

        # Load data from the JSON file
        with open(json_file) as f:
            data = json.load(f)

        # Create an empty mask
        mask = np.zeros((data['imageHeight'], data['imageWidth']), dtype=np.uint8)

        # Add regions to the mask
        for shape in data['shapes']:
            points = np.array(shape['points'], dtype=np.int32)
            if len(points) &gt; 0:
                # Fill the area defined by the points with white color
                cv2.fillPoly(mask, [points], 49)

        # Save the mask as a PNG image using OpenCV
        mask_filename = os.path.splitext(filename)[0] + '_mask.png'
        cv2.imwrite(os.path.join(mask_folder, mask_filename), mask)

print(&quot;Conversion completed, and mask images have been saved in the 'masks' folder!&quot;)


</code></pre>
<p>but there is an interesting problem. it mask just a few of them good in start but others masked with just a thin line. when I again try to use this code it masked all them with a thin line.</p>
<p>for instance image before process in labelme tool:
<a href=""https://i.sstatic.net/5uk5NvHO.jpg"" rel=""nofollow noreferrer"">my images before process them in lamelme</a></p>
<p>for instance masked jeson data:
<a href=""https://i.sstatic.net/M3ZSNspB.png"" rel=""nofollow noreferrer"">jeson masked</a></p>
<p>how I can fix this problem?</p>
","0","Question"
"78786800","","<p>I'm trying to install tf-models-official with <code>!pip install tf-models-official</code> and when it started to collecting kaggle&gt;=1.3.9, it returned error below :</p>
<pre><code>Collecting kaggle&gt;=1.3.9 (from tf-models-official)
  Using cached kaggle-1.6.15.tar.gz (9.1 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [35 lines of output]
      Traceback (most recent call last):
        File &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 353, in &lt;module&gt;
          main()
        File &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 152, in prepare_metadata_for_build_wheel
          whl_basename = backend.build_wheel(metadata_directory, config_settings)
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/build.py&quot;, line 58, in build_wheel
          return os.path.basename(next(builder.build(directory=wheel_directory, versions=['standard'])))
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py&quot;, line 155, in build
          artifact = version_api[version](directory, **build_data)
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/wheel.py&quot;, line 475, in build_standard
          for included_file in self.recurse_included_files():
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py&quot;, line 176, in recurse_included_files
          yield from self.recurse_selected_project_files()
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py&quot;, line 180, in recurse_selected_project_files
          if self.config.only_include:
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/config.py&quot;, line 806, in only_include
          only_include = only_include_config.get('only-include', self.default_only_include()) or self.packages
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/wheel.py&quot;, line 260, in default_only_include
          return self.default_file_selection_options.only_include
        File &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/functools.py&quot;, line 981, in __get__
          val = self.func(instance)
        File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/wheel.py&quot;, line 248, in default_file_selection_options
          raise ValueError(message)
      ValueError: Unable to determine which files to ship inside the wheel using the following heuristics: https://hatch.pypa.io/latest/plugins/builder/wheel/#default-file-selection
      
      The most likely cause of this is that there is no directory that matches the name of your project (kaggle).
      
      At least one file selection option must be defined in the `tool.hatch.build.targets.wheel` table, see: https://hatch.pypa.io/latest/config/build/
      
      As an example, if you intend to ship a directory named `foo` that resides within a `src` directory located at the root of your project, you can define the following:
      
      [tool.hatch.build.targets.wheel]
      packages = [&quot;src/foo&quot;]
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
</code></pre>
<p>I was able to install 2 weeks back, now on the new jupyter notebook kernel suddenly not able to install. I've tried to reinstall on the old kernel, the same error happens too. Anyone know how to solve this?</p>
","1","Question"
"78787112","","<p>I am trying to train a CNN model on an image dataset, but I am stuck  getting decimal values for TruePositives, TrueNegatives, FalsePositives, and FalseNegatives. How can that be possible?</p>
<pre><code>ERROR sample
Epoch 1/3
36/36 ━━━━━━━━━━━━━━━━━━━━ 69s 2s/step - false_negatives: 30.1351 - false_positives: 35.3784 - loss: 2.1995 - true_negatives: 389.0540 - true_positives: 437.6487

</code></pre>
<p>There are some (tp+tn+fp+tn) that are not equal to the total number of samples.</p>
<p><strong>Complete code</strong></p>
<pre><code>
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.layers import Dense,Flatten,InputLayer,Conv2D,MaxPooling2D,Concatenate,Input,BatchNormalization
from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import EarlyStopping
</code></pre>
<pre><code>datagen=ImageDataGenerator(rescale=1.0/255.0)
train_gen=datagen.flow_from_directory('train',class_mode='binary',
                                      target_size=(224,224),batch_size=32,shuffle=True)

</code></pre>
<pre><code>Output:

Found 1146 images belonging to 2 classes.
</code></pre>
<pre><code>tp = tf.keras.metrics.TruePositives()
tn = tf.keras.metrics.TrueNegatives()
fp = tf.keras.metrics.FalsePositives()
fn = tf.keras.metrics.FalseNegatives()
tp.update_state([0.4, .9, .7, .8], [1.0, 0.0, 1.0, 1.0])
tp.result()
</code></pre>
<pre><code>output
&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;
</code></pre>
<pre><code>model_input=Input(shape=(224,224,3))

x=Conv2D(filters=32, kernel_size=(3,3),activation='relu',padding='valid')(model_input)
x=MaxPooling2D(pool_size=(2,2),strides=2)(x)
x=Conv2D(filters=64, kernel_size=(3,3),activation='relu',padding='valid')(x)
x=MaxPooling2D(pool_size=(2,2),strides=2)(x)
x=BatchNormalization()(x)
x=Conv2D(filters=64, kernel_size=(3,3),activation='relu',padding='valid')(x)
x=MaxPooling2D(pool_size=(2,2),strides=2)(x)
x=BatchNormalization()(x)
x=Flatten()(x)
x=Dense(units=1000,activation='relu')(x)
output=Dense(units=1,activation='sigmoid')(x)
model=Model(inputs=model_input,outputs=output)
</code></pre>
<pre><code>model.compile(optimizer=Adam(),loss=BinaryCrossentropy(),metrics=[tp,fp,fn,tn])
early_stopping = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)


history=model.fit(x=train_gen,epochs=3,callbacks=[early_stopping])
</code></pre>
<p><strong>DECIMAL VALUES ERROR</strong></p>
<pre><code>Epoch 1/3
36/36 ━━━━━━━━━━━━━━━━━━━━ 69s 2s/step - false_negatives: 30.1351 - false_positives: 35.3784 - loss: 2.1995 - true_negatives: 389.0540 - true_positives: 437.6487
Epoch 2/3
36/36 ━━━━━━━━━━━━━━━━━━━━ 61s 2s/step - false_negatives: 7.8378 - false_positives: 13.5135 - loss: 0.1692 - true_negatives: 283.1081 - true_positives: 300.4054
Epoch 3/3
36/36 ━━━━━━━━━━━━━━━━━━━━ 65s 2s/step - false_negatives: 2.3243 - false_positives: 3.0811 - loss: 0.0546 - true_negatives: 289.8108 - true_positives: 308.3513

</code></pre>
","-1","Question"
"78790401","","<p>So I have a video of bacterial cells moving around which I converted to frames. Now I want to find the instantaneous velocity of each bacterial cells. For this I am interested in finding by how much a bacterial cell moved but I dont know how to tell my programme to accurately identify that this specific bacteria moved. For example, lets say I have only two images. For each image, I have the COM coordinate of each bacteria. Now how do I relate this data. How do I ask my programme to accurately identify that this much was the change in COM for this specific bacteria. I have attached the two images for reference.</p>
<p><a href=""https://i.sstatic.net/Tj9oYiJj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tj9oYiJj.jpg"" alt=""sample image 1"" /></a>
<a href=""https://i.sstatic.net/MYNwsGpB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MYNwsGpB.jpg"" alt=""sample image 2"" /></a></p>
<p>A method I thought was to give each contour a unique id and associate the features of that contour with that unique id. For example its major axis and minor axis length. Then this way I can relate the contour's initial and final COM. But this idea assumes that all the bacterial cells are unique and that my code to accurately and precisely identifies contours of each bacterial cells which it does not. I have attached my code for finding contours of each bacterial cells too just in case you are interested. Can someone suggest some better ideas? Thankyou so much.</p>
<pre><code>import cv2 as cv
import numpy as np
from numpy.typing import NDArray
import math

def gaussian_filter_multiscale_retinex(image: NDArray, sigmas: list[float], weights: list[float]) -&gt; NDArray:
    img32 = image.astype('float32') / 255

    img32_log = cv.log(img32 + 1)

    msr = np.zeros(image.shape, np.float32)
    for sigma, weight in zip(sigmas, weights):

        blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
        blur_log = cv.log(blur + 1)
        ssr = cv.subtract(img32_log, blur_log)
        ssr = cv.multiply(ssr, weight)

        msr = cv.add(msr, ssr)

    msr = cv.divide(msr, sum(weights))

    msr = cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)
    return msr
def calculate_ellipse_area(ellipse):

    (cx, cy), (a, b), angle = ellipse
    semi_major_axis = a / 2
    semi_minor_axis = b / 2
    area = math.pi * semi_major_axis * semi_minor_axis
    return area,angle

def process_image(img, size_threshold):
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rtnx = gaussian_filter_multiscale_retinex(gray, sigmas=[15, 55, 185], weights=[10, 5, 1])
    thresholded = cv.adaptiveThreshold(rtnx, 255, adaptiveMethod=cv.ADAPTIVE_THRESH_GAUSSIAN_C,
                                        thresholdType=cv.THRESH_BINARY, blockSize=7, C=-7)
    nb_components, output, stats, _ = cv.connectedComponentsWithStats(thresholded, connectivity=8)
    sizes = stats[1:, -1]
    new_img = np.zeros_like(thresholded)
    for i in range(0, nb_components - 1):
        if sizes[i] &gt;= size_threshold:
            new_img[output == i + 1] = 255
    connected_components = cv.connectedComponentsWithStats(new_img)
    (numLabels, labels, stats, centroids) = connected_components

    result_image = np.ones_like(img) * 255
    for i in range(1, numLabels):
        componentMask = (labels == i).astype('uint8')
        contours, _ = cv.findContours(componentMask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)
        if len(contours) &gt; 0:
            cnt = contours[0]
            if len(cnt) &gt;= 5:
                ellipse = cv.fitEllipse(cnt)
                area, angle = calculate_ellipse_area(ellipse)
                if area &lt; 250:
                    cv.ellipse(result_image, ellipse, (0, 0, 0), 1)  # Draw black contours on white background
    return result_image
img1path = &quot;/Users/yahya2/Desktop/1.png&quot;
img = cv.imread(img1path)
size_threshold = 16
result_image = process_image(img, size_threshold)

cv.imshow('Contours', result_image)
cv.waitKey(0)
cv.destroyAllWindows()
</code></pre>
","1","Question"
"78793796","","<p><strong>Context</strong>: When preprocessing a data set using sklearn, you use <code>fit_transform</code> on the <strong>training</strong> set and <code>transform</code> on the <strong>test</strong> set, to avoid data leakage. Using leave one out (LOO) encoding, you need the target variable value to calculate the encoded value of a feature value. When using the LOO encoder in a pipeline, you can apply it to the training set using the <code>fit_transform</code> function, which accepts the features (<code>X</code>) and the target values (<code>y</code>).</p>
<p>How do I calculate the LOO encodings for the test set with the same pipeline, knowing that <code>transform</code> does not accept the target variable values as an argument? I'm quite confused about this. The <code>transform</code> function indeed transforms the columns but without considering the value of the target, since it doesn't have that information.</p>
","1","Question"
"78796342","","<p>I am trying to predict stock prices. Here is my code:</p>
<pre><code>import pandas as pd
import yfinance as web
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Dropout
from tensorflow.python.keras.layers.recurrent import LSTM

company = 'TSLA'

start='2012-01-01'
end='2024-03-01'

data = web.download(company, start=start, end=end)

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1,1))

prediction_days = 60

x_train = []
y_train = []

for x in range(prediction_days, len(scaled_data)):
    x_train.append(scaled_data[x-prediction_days:x, 0])
    y_train.append(scaled_data[x, 0])

model = Sequential()

model.add(LSTM(units = 50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))
</code></pre>
<p>I excepted it to input out nothing, however I got this error:
but it says as an error,</p>
<pre><code>Traceback (most recent call last):
File
&quot;c:\Users\User1\OneDrive\Documents\Desktop\python\projects\machine\stock_price_predictor.py&quot;, 
line 32, in &lt;module&gt;
model.add(LSTM(units = 50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
                                                               ^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'shape'
</code></pre>
<p>How to resolve this? I tried converting it into <code>np.array</code> but nothing worked. This was my attempt:</p>
<pre><code>import pandas as pd
import yfinance as web
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Dropout
from tensorflow.python.keras.layers.recurrent import LSTM

company = 'TSLA'

start='2012-01-01'
end='2024-03-01'

data = web.download(company, start=start, end=end)

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data=.fit_transform(data['Close'].values.reshape(-1,1))

prediction_days = 60

x_train = np.array([])
y_train = np.array([])

for x in range(prediction_days, len(scaled_data)):
x_train = np.append(x_train, scaled_data[x-prediction_days:x, 0])
y_train = np.append(y_train, scaled_data[x, 0])

model = Sequential()

model.add(LSTM(units = 50, return_sequences=True, input_shape= 
(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))
</code></pre>
<p>But instead I got this error</p>
<pre><code>Traceback (most recent call last):
 File 
&quot;c:\Users\User1\OneDrive\Documents\Desktop\python\projects\machine 
 learning\stock_price_predictor.py&quot;, line 32, in &lt;module&gt;
 model.add(LSTM(units = 50, return_sequences=True, input_shape= 
(x_train.shape[1], 1)))                                                              
~~~~~~~~~~~~~^^^
IndexError: tuple index out of range
</code></pre>
","0","Question"
"78796671","","<p>Trying to tune the min_samples_leaf in a GradientBoostedClassifer(). I'm seeing expected results with the bias/variance tradeoff. However, just to test the boundaries, I made the min_samples_leaf &gt; n_samples in the training dataset, expecting an error or something, but I am still getting results similar to when the model is tuned:</p>
<pre><code>df = df_a # number of samples = 347
df=df.sample(frac=1) 
train_proportion = 0.8 
n = len(df)
t = int(train_proportion * n)

# separate training and test sets
y = df['detected']
X = df.loc[:, ~df.columns.isin(['detected'])]

# samples in training set
train_x = X.iloc[:t,:].reset_index().iloc[:,1:]
# samples in test set
test_x = X.iloc[t:,:].reset_index().iloc[:,1:]
#targets in train set
train_y = pd.Series(y[:t].reset_index().iloc[:,1:].iloc[:,0])
#targets in test set
test_y = pd.Series(y[t:].reset_index().iloc[:,1:].iloc[:,0])

clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 10, random_state= 0, min_samples_leaf=500)
clf.fit(train_x,train_y)
print(clf.score(train_x,train_y))
print(clf.score(test_x,test_y))

output:
0.924187725631769
0.9142857142857143
</code></pre>
<p>Why would this be the case? I expected there would be an error or no splits would be made. There seems to be nothing in the documentation on what would happen if min_samples_leaf &gt; n_samples. The only requirement for int's is the range [1,inf]. There's no additional notes on this, either.</p>
<p>I was thinking that maybe it would reset the min_samples_leaf to some usable value, but the subtrees all have no depth and no splits are made: <a href=""https://i.sstatic.net/TMSDXmBJ.png"" rel=""nofollow noreferrer"">subtree</a></p>
","0","Question"
"78796974","","<p>I train a linear model to predict house price, and then I compare the Shapley values calculation manually vs the values returned by the <code>SHAP</code> library and they are slightly different.</p>
<p>My understanding is that for linear models the Shapley value is given by:</p>
<pre><code>coeff * features for obs - coeffs * mean(features in training set)
</code></pre>
<p>Or as stated in the SHAP documentation: <code>coef[i] * (x[i] - X.mean(0)[i])</code>, where i is one feature.</p>
<p>The question is, why does SHAP return different values from the manual calculation?</p>
<p>Here is the code:</p>
<pre><code>import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import shap

X, y = fetch_california_housing(return_X_y=True, as_frame=True)
X = X.drop(columns = [&quot;Latitude&quot;, &quot;Longitude&quot;, &quot;AveBedrms&quot;])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0,
)

scaler = MinMaxScaler().set_output(transform=&quot;pandas&quot;).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

linreg = LinearRegression().fit(X_train, y_train)
coeffs = pd.Series(linreg.coef_, index=linreg.feature_names_in_)

X_test.reset_index(inplace=True, drop=True)
obs = 6188

# manual shapley calculation
effect = coeffs * X_test.loc[obs]
effect - coeffs * X_train.mean()
</code></pre>
<p>Which returns:</p>
<pre><code>MedInc        0.123210
HouseAge     -0.459784
AveRooms     -0.128162
Population    0.032673
AveOccup     -0.001993
dtype: float64
</code></pre>
<p>And the SHAP library returns something slightly different:</p>
<pre><code>explainer = shap.LinearExplainer(linreg, X_train)
shap_values = explainer(X_test)
shap_values[obs]
</code></pre>
<p>Here the result:</p>
<pre><code>.values =
array([ 0.12039244, -0.47172515, -0.12767778,  0.03473923, -0.00251017])

.base_values =
2.0809714707337523

.data =
array([0.25094137, 0.01960784, 0.06056066, 0.07912217, 0.00437137])
</code></pre>
<p>It is set to ignore interactions:</p>
<pre><code>explainer.feature_perturbation
</code></pre>
<p>returning</p>
<pre><code>'interventional'
</code></pre>
","1","Question"
"78800789","","<pre><code>layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
layer_1_b = tf.layers.batch_normalization(layer_1)
layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1_b, w_2), b_2))
layer_2_b = tf.layers.batch_normalization(layer_2)
layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2_b, w_3), b_3))
layer_3_b = tf.layers.batch_normalization(layer_3)
y = tf.nn.relu(tf.add(tf.matmul(layer_3, w_4), b_4))
g_q_action = tf.argmax(y, axis=1)

# compute loss
g_target_q_t = tf.placeholder(tf.float32, None, name=&quot;target_value&quot;)
g_action = tf.placeholder(tf.int32, None, name='g_action')
action_one_hot = tf.one_hot(g_action, n_output, 1.0, 0.0, name='action_one_hot')
q_acted = tf.reduce_sum(y * action_one_hot, reduction_indices=1, name='q_acted')

g_loss = tf.reduce_mean(tf.square(g_target_q_t - q_acted), name='g_loss')
optim = tf.train.RMSPropOptimizer(learning_rate=0.001, momentum=0.95, epsilon=0.01).minimize(g_loss)
</code></pre>
<p>Error statement:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\T\PycharmProjects\Project1\.venv\main_test.py&quot;, line 139, in &lt;module&gt;
    layer_1 = tf.compat.v1.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
                                          ^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\ops\weak_tensor_ops.py&quot;, line 142, in wrapper
    return op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;, line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 1037, in _create_c_op
    raise ValueError(e.message)
</code></pre>
<p>To change 1.x to 2.x Tensorflow.</p>
","0","Question"
"78802177","","<p>I am using the <a href=""https://github.com/NickSwardh/YoloDotNet"" rel=""nofollow noreferrer"">YoloDotNet</a> NuGet package to test the performance of YOLO models. I'm doing this testing for my degree thesis. However, I have encountered an issue where the GPU performance is significantly worse than the CPU performance.</p>
<blockquote>
<p>The problem is that the performance for the first 50/60 inferences are really good(like 20ms) and then they start to get worst until the time gets stable around 70/75ms per image. I don't get why the performance gets worst in that way.</p>
</blockquote>
<p>Environment:</p>
<ul>
<li>YoloDotNet version: v2.0</li>
<li>CPU: AMD ryzen 7 7800X3D</li>
<li>GPU: 4070 super</li>
<li>CUDA/cuDNN version: cuda 11.8 and cudnn 8.9.7</li>
<li>.NET version: 8</li>
</ul>
<p>Steps to reproduce:</p>
<pre><code>var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
    var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

    using var image = SKImage.FromEncodedData(file);
    sw.Restart();
    var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
    sw.Stop();
    image.Draw(results);

    image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
        SKEncodedImageFormat.Jpeg);
    times.Add(sw.Elapsed.TotalMilliseconds);
    Console.WriteLine($&quot;Time taken for image {i}: {sw.Elapsed.TotalMilliseconds:F2} ms&quot;);
</code></pre>
<p>This is the way I'm taking the time measure for the detections.</p>
<p>To load the model i use this setup in the GPU case</p>
<pre><code>yolo = new Yolo(new YoloOptions
{
    OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
    ModelType = ModelType.ObjectDetection,  // Model type
    Cuda = true,                           // Use CPU or CUDA for GPU accelerated inference. Default = true
    GpuId = 0,                               // Select Gpu by id. Default = 0
    PrimeGpu = true,                       // Pre-allocate GPU before first. Default = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;Using GPU for version {yolo_version}{version}&quot;);
</code></pre>
<p>Performance Metrics using yolov8:</p>
<pre><code>CPU Inference Time: 
Total time taken for version m: 25693 ms

Average time per image for version m: 51.25 ms

GPU Inference Time: 
Total time taken for version m: 34459.73 ms

Average time per image for version m: 69.74 ms
</code></pre>
<p>I would like to post graphs about the times but I do not have enough reputation</p>
<p>The issue presents its self for different sizes of the model. I have printed only the size m for ease of visualization.</p>
<p>Expected behavior is that the inference using the GPU should be faster than inference using the CPU.
But the performance is not improving using the GPU.</p>
","0","Question"
"78802566","","<p>I am doing SHELF PRODUCT RECOGNITION wherein the webApp (built using flask) analyzes shelf images using pretrained model of Azure Vision AI. I have using an Azure VM instance for this.</p>
<p>I need to detect Objects as well as the <strong>empty areas</strong> i.e. the <strong>gaps</strong> between these objects.</p>
<p>The following code of <code>app.py</code> marks the detected objects and the gaps between them:</p>
<pre><code>import os
from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes
from msrest.authentication import CognitiveServicesCredentials
import cv2
import numpy as np
import matplotlib.pyplot as plt

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'static/uploads/'

# Initialize Azure Computer Vision client
endpoint = os.getenv('AZURE_COMPUTER_VISION_ENDPOINT')
key = os.getenv('AZURE_COMPUTER_VISION_KEY')
computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(key))

def preprocess_image(image_path):
    &quot;&quot;&quot;
    Preprocess the image to detect edges.
    &quot;&quot;&quot;
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blurred, 50, 150)
    return edges

def analyze_image(filepath):
    &quot;&quot;&quot;
    Analyze the uploaded image using Azure Computer Vision and detect objects,
    and empty areas.
    &quot;&quot;&quot;
    with open(filepath, &quot;rb&quot;) as image_contents:
        results = computervision_client.analyze_image_in_stream(image_contents, visual_features=[VisualFeatureTypes.objects])

    image = cv2.imread(filepath)
    height, width, _ = image.shape

    empty_areas = []
    bounding_boxes = []

    # Analyze detected objects
    confidence_threshold = 0.5
    shelves = {}
    num_shelves = 5
    for obj in results.objects:
        if obj.confidence &gt; confidence_threshold:
            left = int(obj.rectangle.x)
            top = int(obj.rectangle.y)
            right = left + int(obj.rectangle.w)
            bottom = top + int(obj.rectangle.h)
            bounding_boxes.append((left, top, right, bottom))
            row_key = (top // (height // num_shelves))
            if row_key not in shelves:
                shelves[row_key] = []
            shelves[row_key].append((left, top, right, bottom))

    # Detect empty areas between objects
    gap_threshold = 50
    for row_key, objects in shelves.items():
        objects.sort(key=lambda x: x[0])
        for i in range(len(objects) - 1):
            _, _, right1, _ = objects[i]
            left2, _, _, _ = objects[i + 1]
            gap_width = left2 - right1
            if gap_width &gt; gap_threshold:
                empty_areas.append((right1, row_key * (height // num_shelves), left2, (row_key + 1) * (height // num_shelves)))

    # Create an output image with bounding boxes
    fig, ax = plt.subplots()
    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw bounding boxes for objects
    for (left, top, right, bottom) in bounding_boxes:
        rect = plt.Rectangle((left, top), right - left, bottom - top, edgecolor='g', facecolor='none')
        ax.add_patch(rect)

    # Draw bounding boxes for empty areas
    for (left, top, right, bottom) in empty_areas:
        rect = plt.Rectangle((left, top), right - left, bottom - top, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

    # Save the output image
    result_filepath = os.path.join(app.config['UPLOAD_FOLDER'], 'result_' + os.path.basename(filepath))
    plt.savefig(result_filepath)
    plt.close()

    return {
        'bounding_boxes': bounding_boxes,
        'empty_areas': empty_areas,
        'image_url': result_filepath
    }

@app.route('/', methods=['GET', 'POST'])
def index():
    &quot;&quot;&quot;
    Handle the upload of the image and display analysis results.
    &quot;&quot;&quot;
    if request.method == 'POST':
        if 'file' not in request.files:
            return redirect(request.url)
        file = request.files['file']
        if file.filename == '':
            return redirect(request.url)
        if file:
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
            file.save(filepath)
            results = analyze_image(filepath)
            return render_template('result.html', results=results)

    return render_template('index.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
</code></pre>
<p>The other files are <code>index.html</code> and <code>result.html</code>.</p>
<p>These files, combined with the updated app.py, will now handle object detection and gap detection. However, I am not able to correctly identify and mark all the objects and then detect the gaps between them.</p>
<p>The image being used for analysis has been attached. Following is the link for reference:
<a href=""https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/shelf-analyze"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/shelf-analyze</a></p>
<p>What could be the additional changes that I can make in the <code>analyze_image</code> function to correctly identify the gaps between the objects?</p>
<p><a href=""https://i.sstatic.net/e8S8sDPv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e8S8sDPv.jpg"" alt=""Shelf Image Used for Analysis:"" /></a></p>
","0","Question"
"78804107","","<p>I'm having problems with my gradient descent function.</p>
<p>The scatter plot of my diagram shows a negative correlation but the line of best fit gotten from my gradient descent function shows a positive correlation.</p>
<p>Here is the formula I applied for my gradient descent function.</p>
<p><a href=""https://i.sstatic.net/f4vlMZ6t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f4vlMZ6t.png"" alt="""" /></a>
Here is my code.</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from ISLP import load_data

# Load the Boston dataset
ds = load_data(&quot;Boston&quot;)
# Percentage of lower status people vs House Prices
x_train, y_train = ds['lstat'], ds['medv']

# Compute cost function
def compute_cost(x, y, w, b):
    m = len(y)
    total_cost = 0
    for i in range(m):
        prediction = w * x[i] + b
        cost = (prediction - y[i]) ** 2
        total_cost += cost
    return total_cost / (2 * m)

def compute_gradient(x, y, w, b):
    m = len(y)
    w_gradient = 0
    b_gradient = 0
    for i in range(m):
        prediction = w * x[i] + b
        w_gradient += (prediction - y[i]) * x[i]
        b_gradient += (prediction - y[i])

    w_gradient /= m
    b_gradient /= m
    return w_gradient, b_gradient

# Gradient descent function
def gradient_descent(x, y, w, b, alpha, num_iterations):
    m = len(y)
    for i in range(num_iterations):
        w_gradient, b_gradient = compute_gradient(x, y, w, b)
        w -= alpha * w_gradient
        b -= alpha * b_gradient
    return w, b

# Parameters
alpha = 0.0001
num_iterations = 2000
initial_w = 0
initial_b = 0

# Run gradient descent
w, b = gradient_descent(x_train, y_train, initial_w, initial_b, alpha, num_iterations)

# Print the found parameters
print('w found by gradient descent:', w)
print('b found by gradient descent:', b)

# Plot the data and the linear fit
plt.scatter(x_train, y_train, marker='x', c='r', label='Data Points')
plt.plot(x_train, w * x_train + b, '-', label='Linear Fit')
plt.title(&quot;Percentage of lower status people vs House Prices&quot;)
plt.ylabel(&quot;House Prices in thousands&quot;)
plt.xlabel(&quot;Percentage of lower status people&quot;)
plt.legend()
plt.show()
</code></pre>
<p>I looked on geeksforgeeks and this is how they went about theirs.</p>
<pre><code># Importing Libraries
import numpy as np
import matplotlib.pyplot as plt

def mean_squared_error(y_true, y_predicted):
    
    # Calculating the loss or cost
    cost = np.sum((y_true-y_predicted)**2) / len(y_true)
    return cost

# Gradient Descent Function
# Here iterations, learning_rate, stopping_threshold
# are hyperparameters that can be tuned
def gradient_descent(x, y, iterations = 1000, learning_rate = 0.0001, 
                    stopping_threshold = 1e-6):
    
    # Initializing weight, bias, learning rate and iterations
    current_weight = 0.1
    current_bias = 0.01
    iterations = iterations
    learning_rate = learning_rate
    n = float(len(x))
    
    costs = []
    weights = []
    previous_cost = None
    
    # Estimation of optimal parameters 
    for i in range(iterations):
        
        # Making predictions
        y_predicted = (current_weight * x) + current_bias
        
        # Calculating the current cost
        current_cost = mean_squared_error(y, y_predicted)

        # If the change in cost is less than or equal to 
        # stopping_threshold we stop the gradient descent
        if previous_cost and abs(previous_cost-current_cost)&lt;=stopping_threshold:
            break
        
        previous_cost = current_cost

        costs.append(current_cost)
        weights.append(current_weight)
        
        # Calculating the gradients
        weight_derivative = -(2/n) * sum(x * (y-y_predicted))
        bias_derivative = -(2/n) * sum(y-y_predicted)
        
        # Updating weights and bias
        current_weight = current_weight - (learning_rate * weight_derivative)
        current_bias = current_bias - (learning_rate * bias_derivative)
                
        # Printing the parameters for each 1000th iteration
        print(f&quot;Iteration {i+1}: Cost {current_cost}, Weight \
        {current_weight}, Bias {current_bias}&quot;)
    
    
    # Visualizing the weights and cost at for all iterations
    plt.figure(figsize = (8,6))
    plt.plot(weights, costs)
    plt.scatter(weights, costs, marker='o', color='red')
    plt.title(&quot;Cost vs Weights&quot;)
    plt.ylabel(&quot;Cost&quot;)
    plt.xlabel(&quot;Weight&quot;)
    plt.show()
    
    return current_weight, current_bias


def main():
    
    # Data
    X = np.array([32.50234527, 53.42680403, 61.53035803, 47.47563963, 59.81320787,
        55.14218841, 52.21179669, 39.29956669, 48.10504169, 52.55001444,
        45.41973014, 54.35163488, 44.1640495 , 58.16847072, 56.72720806,
        48.95588857, 44.68719623, 60.29732685, 45.61864377, 38.81681754])
    Y = np.array([31.70700585, 68.77759598, 62.5623823 , 71.54663223, 87.23092513,
        78.21151827, 79.64197305, 59.17148932, 75.3312423 , 71.30087989,
        55.16567715, 82.47884676, 62.00892325, 75.39287043, 81.43619216,
        60.72360244, 82.89250373, 97.37989686, 48.84715332, 56.87721319])

    # Estimating weight and bias using gradient descent
    estimated_weight, estimated_bias = gradient_descent(X, Y, iterations=2000)
    print(f&quot;Estimated Weight: {estimated_weight}\nEstimated Bias: {estimated_bias}&quot;)

    # Making predictions using estimated parameters
    Y_pred = estimated_weight*X + estimated_bias

    # Plotting the regression line
    plt.figure(figsize = (8,6))
    plt.scatter(X, Y, marker='o', color='red')
    plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='blue',markerfacecolor='red',
            markersize=10,linestyle='dashed')
    plt.xlabel(&quot;X&quot;)
    plt.ylabel(&quot;Y&quot;)
    plt.show()

    
if __name__==&quot;__main__&quot;:
    main()
</code></pre>
<p>Looks like their approach for the gradient descent function is completely different.</p>
<p>The line of best fit shouldn't show a positive correlation.</p>
<p><a href=""https://i.sstatic.net/9nsb7htK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9nsb7htK.png"" alt="""" /></a></p>
<p>I'm confused.</p>
","1","Question"
"78804130","","<p>I've been at my wits end trying to solve this issue with Conv1D in Keras. Basically, I have a vector bathyZ that is 100 x 1. I'd like to do some convolution on it before merging it with 2 scalar inputs Tperiod and AMP_WK to predict another 100 x 1 vector. (bathyZ has spatial variability I'm looking to pick up). This is my first time using tensorflow datasets and parsing/deserializing. I consistently run into shape errors that I can't seem to understand though.</p>
<p>Mainly, it seems like the first dense layer isn't receiving the correct dimensions in its inputs, and I can't figure out why, since the input dimensions for everything seem to make sense to me. Why is it expecting 6402 and getting (None, 165)? I'm debugging on a single record, hence the small batch size, if that was a concern. It should work regardless though, right? My understanding is that my input should be (None,100,1) to allow for different batch sizes for a 100 x 1 sequence. I've tried reshaping a few ways, but none of them have seemed to work, so maybe I'm missing something more fundamental</p>
<p>Error message:</p>
<pre><code>ValueError: Exception encountered when calling Functional.call().

Input 0 of layer &quot;dense_110&quot; is incompatible with the layer: expected axis -1 of input shape to have value 6402, but received input with shape (None, 165)

Arguments received by Functional.call():
  • inputs={'bathyZ': 'tf.Tensor(shape=(None, 100, 1), dtype=float32)', 'AMP_WK': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'Tperiod': 'tf.Tensor(shape=(None, 1), dtype=float32)'}
  • training=True
  • mask={'bathyZ': 'None', 'AMP_WK': 'None', 'Tperiod': 'None'}
</code></pre>
<p>Code:</p>
<pre><code>feature_description = {
        'bathyZ': tf.io.FixedLenFeature([], tf.string),
        'bathyZ_shape': tf.io.FixedLenFeature([3], tf.int64),
        'AMP_WK': tf.io.FixedLenFeature([], tf.float32),
        'Tperiod': tf.io.FixedLenFeature([], tf.float32),
        'skew': tf.io.FixedLenFeature([], tf.string),
        'skew_shape': tf.io.FixedLenFeature([3], tf.int64),
    }
    
    def _parse_function(proto):
        # Parse
        parsed_features = tf.io.parse_single_example(proto, feature_description)
    
        # Decode/reshape the serialized tensors
        bathyZ = parsed_features['bathyZ']
        bathyZ = tf.io.parse_tensor(bathyZ, out_type=tf.float32)
        bathyZ = tf.reshape(bathyZ, [100, 1])
    
        skew = parsed_features['skew']
        skew = tf.io.parse_tensor(skew, out_type=tf.float32)
        skew = tf.reshape(skew, [100, 1])
    
        # Get other inputs, reshape
        AMP_WK = parsed_features['AMP_WK']
        Tperiod = parsed_features['Tperiod']
    
        AMP_WK = tf.reshape(AMP_WK, [1])
        Tperiod = tf.reshape(Tperiod, [1])
        
        # Create tuple
        inputs = {'bathyZ': bathyZ, 'AMP_WK': AMP_WK, 'Tperiod': Tperiod}
        outputs = {'skew': skew}
        
        return inputs, outputs
    
    # Create a TFRecordDataset and map the parsing function
    tfrecord_path = 'ML_0004.tfrecord'
    dataset = tf.data.TFRecordDataset(tfrecord_path)
    dataset = dataset.map(_parse_function)
        
        
    # Model
    
    
    def create_model():
        # Tensor input branch (shape: 100 timesteps, 1 feature)
        bathyZ = Input(shape=(100, 1), name='bathyZ')
        x = layers.Conv1D(32, 3, activation='relu', padding='same')(bathyZ)
        x = layers.Conv1D(64, 3, activation='relu', padding='same')(x)
        x = layers.Flatten()(x)
    
        # Scalar inputs
        AMP_WK = Input(shape=(1,), name='AMP_WK')
        Tperiod = Input(shape=(1,), name='Tperiod')
        
        # Combine all branches
        combined = layers.concatenate([x, AMP_WK, Tperiod])
    
        # Fully connected layer
        z = layers.Dense(64, activation='relu')(combined)
        z = layers.Dense(128, activation='relu')(z)
    
        # Output layer (tensor output, same shape as input tensor)
        skew = layers.Dense(100, activation='linear', name='skew')(z)
    
        # Create the model
        model = models.Model(inputs=[bathyZ, AMP_WK, Tperiod], outputs=skew)
        return model
    
    # Example usage:
    model = create_model()
    model.compile(optimizer='adam', loss='mse')
    model.summary()
    dataset = dataset.batch(1)  
    model.fit(dataset)
</code></pre>
<p>I tried reshaping various input tensors in the parsing and preprocessing steps to different versions of [1,100], [1,100,1] and so on, and tracking how the shape evolves. But I always run into some sort of dimension error</p>
","1","Question"
"78804884","","<p>I'm trying to create a decision tree to predict whether a given loan applicant would default or repay their debt.</p>
<p>I'm using the following dataset</p>
<pre><code>library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)

loans &lt;- read_csv('https://assets.datacamp.com/production/repositories/718/datasets/7805fceacfb205470c0e8800d4ffc37c6944b30c/loans.csv')
</code></pre>
<p>Since the response variable <code>default</code> is encoded as <code>dbl</code>, I convert it to <code>chr</code> first and then <code>fct</code> type variable to use it in my classification model.</p>
<pre><code>loans &lt;- loans %&gt;% mutate(default = factor(as.character(default), levels = c(0, 1), labels = c('repaid', 'defaulted')))
</code></pre>
<p>Now, I start building the recursive partitioning (<code>rpart()</code>) object, <code>loans_model</code>: The response variable is <code>default</code> and the explanatory variables are <code>loan_amount + credit_score + debt_to_income</code>.</p>
<pre><code>loans_model &lt;- rpart(default ~ loan_amount + credit_score + debt_to_income, data = loans, method = 'class')
</code></pre>
<p>When I make predictions with this model, all the predicted values get the same value, <code>repaid</code>.</p>
<pre><code>loans$pred_default &lt;- predict(loans_model, newdata = loans, type = &quot;class&quot;)

unique(unique(loans$pred_default)

</code></pre>
<p>Output:</p>
<pre><code>[1] repaid
Levels: repaid defaulted
</code></pre>
<p>Also when I try to visualize the decision tree, I get only one node (the root).</p>
<pre><code>rpart.plot(loan_model)
</code></pre>
<p><a href=""https://i.sstatic.net/fz1vLAb6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fz1vLAb6.png"" alt=""loan_model_plot"" /></a></p>
<p>Why does the model I built not make appropriate predictions?</p>
","0","Question"
"78807931","","<p>I fitted a simple binary classification model with only 3 trees and wanted to check if feature importance results are simmillar to the <a href=""https://catboost.ai/en/docs/concepts/fstr#regular-feature-importance"" rel=""nofollow noreferrer"">formula</a> in Catboost documentation (PredictionValuesChange).</p>
<p>After training the model, I followed steps in <a href=""https://github.com/catboost/tutorials/blob/master/model_analysis/model_export_as_json_tutorial.ipynb"" rel=""nofollow noreferrer"">CatBoost JSON model tutorial</a> and got the following tree structure:</p>
<pre><code>{
  &quot;leaf_values&quot;: [
    -0.13915912880676032,
    0.1097787155963716
  ],
  &quot;leaf_weights&quot;: [
    2143.0251545906067,
    2252.974784851074
  ],
  &quot;splits&quot;: [
    {
      &quot;border&quot;: 3.5,
      &quot;float_feature_index&quot;: 13,
      &quot;split_index&quot;: 0,
      &quot;split_type&quot;: &quot;FloatFeature&quot;
    }
  ]
} 
</code></pre>
<p>Each tree in the model is only depth = 1 and there is only one tree (index = 1) with the feature of interest. I decided to manually calculate the feature importance according to formula above and compare the results with <code>.get_feature_importance</code> method. The results were very different:</p>
<ul>
<li>Feature Importance: 28.2947825</li>
<li>Manual Calculation: 68.06248029261762</li>
</ul>
<p>Below is the code used for feature importance calculations:</p>
<pre class=""lang-py prettyprint-override""><code>tree_indx = 1
v_1 = model['oblivious_trees'][tree_indx]['leaf_values'][0]
v_2 = model['oblivious_trees'][tree_indx]['leaf_values'][1]

c_1 = model['oblivious_trees'][tree_indx]['leaf_weights'][0]
c_2 = model['oblivious_trees'][tree_indx]['leaf_weights'][1]

avr  = (v_1*c_1 + v_2*c_2)/(c_1+c_2)

fi  = ((v_1 - avr)**2)*c_1 + ((v_2 - avr)**2)*c_2
print(fi)
</code></pre>
<p><strong>Am I making a mistake?</strong></p>
","0","Question"
"78815544","","<p>We're using <a href=""https://github.com/ml-explore/mlx"" rel=""nofollow noreferrer"">MLX</a> to fine tune a model fetched from hugging face.</p>
<pre><code>from transformers import AutoModel
model = AutoModel.from_pretrained('deepseek-ai/deepseek-coder-6.7b-instruct')
</code></pre>
<p>We fine tuned the model with command like <code>python -m mlx_lm.lora --config lora_config.yaml</code> and the config file looks like:</p>
<pre><code># The path to the local model directory or Hugging Face repo.
model: &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;
# Save/load path for the trained adapter weights.
adapter_path: &quot;adapters&quot;
</code></pre>
<p>When the adapter files generated after fine tuning, we evaluated the model by scripts like</p>
<pre><code>from mlx_lm.utils import *
model,tokenizer = load(path_or_hf_repo =&quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;,
                      adapter_path = &quot;adapters&quot; # path to new trained adaptor
                      )
text = &quot;Tell sth about New York&quot;
response = generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100)
</code></pre>
<p>and it works as expected.</p>
<p>However, after we saved the model and evaluated with mlx_lm.generate, the model worked poor. (the behavior is completely different from invoking the model with <code>generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100)</code>.</p>
<pre><code>mlx_lm.fuse  --model &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot; --adapter-path &quot;adapters&quot; --save-path new_model
mlx_lm.generate --model new_model --prompt &quot;Tell sth about New York&quot; --adapter-path &quot;adapters&quot; --temp 0.01
</code></pre>
","1","Question"
"78816668","","<p>Lets suppose that I have a point of interest in my matrix that is NxN. The point is located in the position ij. So, given the index ij, is there a simple way to get the line elements passing trough ij and to the origin (that is located in the middle of the matrix) ?</p>
<p>I am using torch and I though that using torch.diag would be a first start but acttualy this function does not pass in the middle of the matrix.</p>
<pre><code>def directionalK(kx,ky, indices):
    '''Function that provides the K values at a given direction dictated by the indices'''
    kx_grid,ky_grid = torch.meshgrid(kx,kx, indexing='ij')
    k_grid = torch.sqrt(kx_grid**2 + ky_grid**2) 
    k_grid[...,:len(k_grid)//2] *=-1 
    y,x = indices
    diag = x - len(k_grid)//2
</code></pre>
","1","Question"
"78816835","","<p>I have a dataset of images, each containing a 1 to 5letter word. I want to use deep learning to classify the characters that make up the word in each image. The labels for these images are formatted as follows:
<code>totalcharacter_indexoffirstchar_indexofsecondchar_.._indexoflastchar</code>
I'm trying to load these images into TensorFlow pipelines to reduce complexity due to memory constraints. Below is my code for loading and processing images and labels from directory:</p>
<pre><code>def process_img(file_path):
    label = get_label(file_path)
    image = tf.io.read_file(file_path)
    image = tf.image.decode_png(image, channels=1) 
    image = tf.image.convert_image_dtype(image, tf.float32) 
    target_shape = [695, 1204]
    image = tf.image.resize_with_crop_or_pad(image, target_shape[0], target_shape[1])
    
    # Encode the label
    encoded_label = tf.py_function(func=encode_label, inp=[label], Tout=tf.float32)
    encoded_label.set_shape([5, len(urdu_alphabets)])
    
    return image, encoded_label
input_dir = '/kaggle/input/dataset/Data/*'
images_ds = tf.data.Dataset.list_files(input_dir, shuffle=True)

train_count = int(tf.math.round(len(images_ds) * 0.8))
train_ds = images_ds.take(train_count)
test_ds = images_ds.skip(train_count)
train_ds = train_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.batch(32)
train_ds = train_ds.cache()
test_ds = test_ds.cache()
train_ds = train_ds.shuffle(len(train_ds))
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
print(train_ds)
print(test_ds)
</code></pre>
<p>The train_ds looks like this:
<code>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 695, 1204, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 39), dtype=tf.float32, name=None))&gt;</code></p>
<p>Now, I want to apply simple augmentations on the images such as rotation, shear, erosion, and dilation. I initially used the following function:</p>
<pre><code>def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)
    image = tf.keras.preprocessing.image.random_rotation(image, rg=15, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest', cval=0.0, interpolation_order=1)
    image = tf.image.random_zoom(image, [0.85, 0.85])
    image = tf.image.random_shear(image, 0.3)
    image = tf.image.random_shift(image, 0.1, 0.1)
    return image, label

train_augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_augmented_ds = train_augmented_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
</code></pre>
<p>However, many of these functions in tf.image are deprecated. How can I apply these augmentations on images in a TensorFlow pipeline in an efficient way?</p>
<p>Note: I can perform these augmentations by loading images without TensorFlow pipelines using NumPy arrays, but my dataset is very large (1.1 million images), so I need an efficient way to do this.</p>
","0","Question"
"78820748","","<p>I was trying to run YOLOv8 using visual code studio. Installed ultralytics and ran <code>yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'</code> on the vs code terminal.<br />
However the output i received was</p>
<pre><code>2 persons, 1 bicycle, 5 cars, 10 motorcycles, 73 boats, 3 stop signs, 1 dog, 10 horses, 10 cows, 32 bears, 1 giraffe, 63 umbrellas, 6 handbags, 9 frisbees, 15 snowboards, 5 surfboards, 12 knifes, 5 beds, 37 dining tables
</code></pre>
<p>which are clearly not part of this picture.
<img src=""https://i.sstatic.net/f3ZtJT6t.jpg"" alt="""" /></p>
<p>When i first installed ultralytics and tried running torch there was a missing dependency error. <code>fbgemm.ddl</code> was missing. Later when i installed vs_BuildTools this issue was resolved. Then i proceeded to run the code in a virtual environment where a program using torch ran without any errors. Then i proceeded to type this code snippet and encountered this problem. I also tried running using command prompt and jupyter notebook but the same issue persists.</p>
<p>I also did check if the versions are compatible, which they are. I haven't installed cuda yet is it because of that or are there any other issues which i am not aware of?</p>
","5","Question"
"78820971","","<p>I try to use Pyannotes models offline.</p>
<p>I was loading and applying models like this:</p>
<pre><code>from pyannote.audio import Pipeline

access_token = 'xxxxxxxxxxx'

model = Pipeline.from_pretrained(
         &quot;pyannote/speaker-diarization-3.1&quot;,
         use_auth_token=access_token)

path_in = 'blabla/1-137-A-32.wav'

num_speakers = 1

model(path_in,
   num_speakers=num_speakers).labels()
</code></pre>
<p>That works fine.</p>
<p>But now I followed the instructions for offline use: <a href=""https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/applying_a_pipeline.ipynb"" rel=""nofollow noreferrer"">https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/applying_a_pipeline.ipynb</a></p>
<p>My directory structure is as follows:</p>
<p>src-<br />
     |-pyannote_offline_config.yaml<br />
     |-pyannote_pytorch_model.bin</p>
<p>---- YAML ----</p>
<pre><code>version: 3.1.0

pipeline:
  name: pyannote.audio.pipelines.SpeakerDiarization
  params:
    clustering: AgglomerativeClustering
    embedding: pyannote/wespeaker-voxceleb-resnet34-LM
    embedding_batch_size: 32
    embedding_exclude_overlap: true
    segmentation: src/pyannote_pytorch_model.bin
    segmentation_batch_size: 32

params:
  clustering:
    method: centroid
    min_cluster_size: 12
    threshold: 0.7045654963945799
  segmentation:
    min_duration_off: 0.0
</code></pre>
<p>---- Loading Model ----</p>
<pre><code>path_yaml = 'src/pyannote_offline_config.yaml'

model = Pipeline.from_pretrained(path_yaml)

path_in = 'blabla/1-137-A-32.wav'

num_speakers = 1

model(path_in,
         num_speakers=num_speakers).labels()
</code></pre>
<p>But that results in: &quot;A pipeline must be instantiated with <code>pipeline.instantiate(parameters)</code> before it can be applied.&quot;</p>
<p>OK, next try:</p>
<p>---- Loading Model ----</p>
<pre><code>path_yaml = 'src/pyannote_offline_config.yaml'

model = Pipeline.from_pretrained(path_yaml)

params = {'clustering':
    {'method': 'centroid',
    'min_cluster_size': 12,
    'threshold': 0.7045654963945799},
  'segmentation':
    {'min_duration_off': 0.0}}


pipeline = model.instantiate(params)

path_in = 'blabla/1-137-A-32.wav'

num_speakers = 1

pipeline(path_in,
         num_speakers=num_speakers).labels()
</code></pre>
<p>But that results in: &quot;A pipeline must be instantiated with <code>pipeline.instantiate(parameters)</code> before it can be applied.&quot;</p>
<p>I don't understand the problem.</p>
<p>It works if I do it like that:
---- Loading Model ----</p>
<pre><code>path_yaml = 'src/pyannote_offline_config.yaml'

model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;, path_yaml)

path_in = 'blabla/1-137-A-32.wav'

num_speakers = 1

model(path_in,
         num_speakers=num_speakers).labels()
</code></pre>
<p>But after an upload to gitlab the test pipline gives me:&quot;Could not download 'pyannote/speaker-diarization-3.1' pipeline.
It might be because the pipeline is private or gated so make
sure to authenticate. Visit <a href=""https://hf.co/settings/tokens"" rel=""nofollow noreferrer"">https://hf.co/settings/tokens</a> to
create your access token and retry with:
Pipeline.from_pretrained('pyannote/speaker-diarization-3.1',
...                          use_auth_token=YOUR_AUTH_TOKEN)&quot;</p>
<p>So it seems that something is on my local computer that is not downloaded with the pip install. E.g. if I load it without the yaml and only with <code>model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;)</code>, it also works.</p>
","0","Question"
"78821038","","<p>I was following a tutorial in Youtube, when wanted to load Llama3 8B:</p>
<pre><code>model_name = &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hugging_face_key)
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hugging_face_key)
</code></pre>
<p>Got:</p>
<pre><code>Your session has failed because all available RAM has been used
</code></pre>
<p>Tried: <code>model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hugging_face_key, low_cpu_mem_usage=True)</code>
But again the same error</p>
","0","Question"
"78823685","","<p>I'm trying to use a weighted loss function to handle class imbalance in my data. My problem is a multi-class and multi-output problem. For example (my data has five output/target columns (output_1, output_2, output_3) and I have three classes (class_0, class_1, and class_2) in each target column. I am currently using pytorch's cross entropy loss function <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</a> and I see that it has a weight parameter but my understanding is that this the same weight would be applied uniformly to each output/target, but I want to apply separate weights for each class in each output/target.</p>
<p>For concreteness, I could have data that looks like this</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
<th>OUTPUT_1</th>
<th>OUTPUT_2</th>
<th>OUTPUT_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.65</td>
<td>3.56</td>
<td>0.94</td>
<td>9.23</td>
<td>6.43</td>
<td>0</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>7.43</td>
<td>3.95</td>
<td>1.24</td>
<td>7.22</td>
<td>2.66</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>9.31</td>
<td>2.42</td>
<td>2.91</td>
<td>2.64</td>
<td>6.28</td>
<td>2</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>8.19</td>
<td>5.12</td>
<td>1.32</td>
<td>3.12</td>
<td>8.41</td>
<td>0</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>9.35</td>
<td>1.92</td>
<td>3.12</td>
<td>4.13</td>
<td>3.14</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>8.43</td>
<td>9.72</td>
<td>7.23</td>
<td>8.29</td>
<td>9.18</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>4.32</td>
<td>2.12</td>
<td>3.84</td>
<td>9.42</td>
<td>8.19</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3.92</td>
<td>3.91</td>
<td>2.90</td>
<td>8.19</td>
<td>8.41</td>
<td>2</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>7.89</td>
<td>1.92</td>
<td>4.12</td>
<td>8.19</td>
<td>7.28</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>5.21</td>
<td>2.42</td>
<td>3.10</td>
<td>0.31</td>
<td>1.31</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>whereby,</p>
<pre><code>     the proportion in output 1 is : 0 = 0.6, 1 = 0.1, 2 = 0.3
     the proportion in output 2 is : 0 = 0.4, 1 = 0.3, 2 = 0.3
     the proportion in output 3 is : 0 = 0.4, 1 = 0.2, 2 = 0.4
</code></pre>
<p>I want to apply the class weight based on the distribution of classes in each output column such that it renormalizes (or rebalances? not sure what the terminology to use here is) class 1 to 0.15 and class 0 and class 2 to 0.425 each (so for output_1 the weights would be [0.425/0.6, 0.15/0.1, 0.425/0.3], for output 2 it'll be [0.425/0.4, 0.15/0.3, 0.425/0.3] etc). Rather, what I understand the weight parameter in pytorch's crossentropy loss function is currently doing, is it'll apply a single class weight to each output column. I'm wondering if i'm missing something and there's a way to do this using pytorch's crossentropyloss function?</p>
","0","Question"
"78826248","","<p>I am following a neuralnine tutorial on how to do image classification with neural networks.
I am using an Imac.
Below is the code:</p>
<pre><code>import cv2 as cv
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models
#preparing data
(training_images, training_labels), (testing_images, testing_labels) = datasets.cifar10.load_data()
training_images, testing_images = training_images / 255, testing_images / 255

class_names = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']


for i in range(16):
    plt.subplot(4,4,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(training_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[training_labels[i][0]])

plt.show()

training_images = training_images[:5000] #save time
training_labels = training_labels[:5000]
testing_images = testing_images[:4000]
testing_labels = testing_labels[:4000]

model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(training_images, training_labels, epochs=10, validation_data=(testing_images, testing_labels))
</code></pre>
<p>I am geting the following message in my terminal:</p>
<pre><code>ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/2g/.../T/org.python.python.savedState
2024-08-02 16:29:19.788 Python[48146:6869495] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.
</code></pre>
<p>completely clueless as to what to do</p>
","1","Question"
"78827482","","<p>My implementation for the AutoModel AutoTokenizer classes are fairly simple:</p>
<pre><code>from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

    def bert(self, model_name, text):
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        return embeddings
    
    def create_chunks(self, text, chunk_size):
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
</code></pre>
<p>But I can't get this warning to go away:</p>
<pre><code>A parameter name that contains 'beta' will be renamed internally to 'bias'. 
Please use a different name to suppress this warning.
A parameter name that contains 'gamma' will be renamed internally to 'weight'. 
Please use a different name to suppress this warning.
</code></pre>
<p>There is no reference to the word beta or gamma anywhere in my repo.</p>
<p>Updating the package, suppressing the warnings with <code>import warnings</code></p>
","1","Question"
"78827762","","<p>I have a very large .txt file (several gigabytes) that I need to split into training and test sets for a machine learning project. The usual methods of reading the entire file into memory and then splitting it are not feasible due to memory constraints. I am looking for a way to efficiently split the file without overloading memory.</p>
<p>I attempted to use scikit-learn for the split, but it loads the entire file into memory, which causes performance issues and is not suitable for my large dataset.</p>
","0","Question"
"78828998","","<p>I am getting error:</p>
<pre><code>32 vqc.fit(X_train, X_test)
33 
34 # Evaluate the classifier

15 frames
/usr/local/lib/python3.10/dist-packages/qiskit_machine_learning/neural_networks/neural_network.py in _validate_input(self, input_data)
132 
    133         if shape[-1] != self._num_inputs:
--&gt; 134             raise QiskitMachineLearningError(
    135                 f&quot;Input data has incorrect shape, last dimension &quot;
    136                 f&quot;is not equal to the number of inputs: &quot;

QiskitMachineLearningError: 'Input data has incorrect shape, last dimension is not equal to the number of inputs: 0, but got: 2.'
</code></pre>
<pre><code>from qiskit import QuantumCircuit, transpile, assemble
from qiskit_aer import Aer
from qiskit_machine_learning.algorithms import VQC
from qiskit_algorithms.optimizers import COBYLA
from qiskit.circuit.library import TwoLocal
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

# Sample data for demonstration purposes
# Replace this with your actual data
scaled_data = np.random.rand(150, 2)  # Replace with your scaled data
y = np.random.randint(0, 3, size=150)  # Replace with your labels

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)

# Define the number of qubits based on the feature dimension
num_qubits = X_train.shape[1]

# Define the quantum feature map and ansatz with correct parameters
feature_map = TwoLocal(num_qubits=num_qubits, entanglement='linear') #, rotation_blocks=['ry', 'rz'], entanglement_gate='cz')
ansatz = TwoLocal(num_qubits=num_qubits, entanglement='linear')#, rotation_blocks=['ry', 'rz'], entanglement_gate='cz')

# Define the optimizer
optimizer = COBYLA()

# Initialize the VQC classifier with unique parameter names
vqc = VQC(feature_map=feature_map, ansatz=ansatz, optimizer=optimizer)

# Train the quantum classifier
vqc.fit(X_train, X_test)
</code></pre>
","-1","Question"
"78831225","","<p>how can I make this model generate 512x512 px images or bigger? Now it generates 64x64px images.I tried changing some values in the model but it didn't work. And how these convolutional layers work especially Conv2D and Conv2DTranspose? I don't understand how the image is resized in those layers.</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt

cd /content/drive/MyDrive

dataset = keras.preprocessing.image_dataset_from_directory(
    directory = 'Humans', label_mode = None, image_size = (64,64), batch_size = 32,
    shuffle = True
).map(lambda x: x/255.0)

discriminator = keras.models.Sequential(
    [
        keras.Input(shape = (64,64,3)),
        layers.Conv2D(64, kernel_size = 4, strides = 2, padding = 'same'),
        layers.LeakyReLU(0.2),
        layers.Conv2D(128, kernel_size = 4, strides = 2, padding = 'same'),
        layers.LeakyReLU(0.2),
        layers.Conv2D(128, kernel_size = 4, strides = 2, padding = 'same'),
        layers.LeakyReLU(0.2),
        layers.Flatten(),
        layers.Dropout(0.2),
        layers.Dense(1,activation = 'sigmoid')
    ]
)

latent_dim = 128
generator = keras.models.Sequential(
    [
        layers.Input(shape = (latent_dim,)),
        layers.Dense(8*8*128),
        layers.Reshape((8,8,128)),
        layers.Conv2DTranspose(128, kernel_size = 4, strides = 2, padding = 'same'),
        layers.LeakyReLU(0.2),
        layers.Conv2DTranspose(256, kernel_size = 4, strides = 2, padding = 'same'),
        layers.LeakyReLU(0.2),
        layers.Conv2DTranspose(512, kernel_size = 4, strides = 2, padding = 'same'),
        layers.LeakyReLU(0.2),
        layers.Conv2D(3, kernel_size = 5,padding = 'same',activation = 'sigmoid')
    ]
)

opt_gen = keras.optimizers.Adam(1e-4)
opt_disc = keras.optimizers.Adam(1e-4)
loss_fn = keras.losses.BinaryCrossentropy()

for epoch in range(500):
  for idx, real in enumerate(tqdm(dataset)):
    batch_size = real.shape[0]
    random_latent_vectors = tf.random.normal(shape = (batch_size,latent_dim))
    fake = generator(random_latent_vectors)

    if idx % 50 == 0:
      img = keras.preprocessing.image.array_to_img(fake[0])
      img.save(f'gen_images/generated_img{epoch}_{idx}_.png')

    with tf.GradientTape() as disc_tape:
      loss_disc_real = loss_fn(tf.ones((batch_size,1)), discriminator(real))
      loss_disc_fake = loss_fn(tf.zeros(batch_size,1), discriminator(fake))
      loss_disc = (loss_disc_real+loss_disc_fake)/2

    grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)

    opt_disc.apply_gradients(
        zip(grads, discriminator.trainable_weights)
    )

    with tf.GradientTape() as gen_tape:
      fake = generator(random_latent_vectors)
      output = discriminator(fake)
      loss_gen = loss_fn(tf.ones(batch_size,1),output)

    grads = gen_tape.gradient(loss_gen, generator.trainable_weights)
    opt_gen.apply_gradients(
        zip(grads, generator.trainable_weights)
    )
</code></pre>
<p>I tried changing the image size and some values in the conv layers but it didn't work.</p>
","1","Question"
"78832537","","<p>I am trying to split my data into testing and training sets using package caret. I have 77 rows with complete data in each column. The function 'createDataPartition' results in 4 rows for training data and 73 rows for testing data, which doesn't seem right. Any help would be appreciated. Here is my code:</p>
<pre><code>&gt; library(caret)
&gt; # Split data into train and test
&gt; set.seed(123)
&gt; data.full &lt;- data.full %&gt;% select(fasting_status, a1c, glu, uc_ratio)
&gt; training.samples &lt;- data.full %&gt;% 
+   createDataPartition(p = 0.8, list = FALSE)
Warning messages:
1: In createDataPartition(., p = 0.8, list = FALSE) :
  Some classes have no records (  ) and these will be ignored
2: In createDataPartition(., p = 0.8, list = FALSE) :
  Some classes have a single record (  ) and these will be selected for the sample
&gt; train.data  &lt;- data.full[training.samples, ]
&gt; test.data &lt;- data.full[-training.samples, ] 
</code></pre>
<p>Here are my reproducible data:</p>
<pre><code>&gt; dput(data.full)
structure(list(fasting_status = structure(c(1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L), levels = c(&quot;1&quot;, &quot;2&quot;), class = &quot;factor&quot;), 
    a1c = c(4.3, 4.5, 4.4, 2.9, 4.3, 4.4, 4.2, 4.5, 4.2, 4.2, 
    4.5, 4.5, 4.8, 4.5, 5.2, 4.9, 4.6, 4.2, 4.4, 4.9, 4.6, 4.5, 
    4.4, 4.8, 4.5, 4.1, 3.8, 3.1, 4.3, 4.6, 4.7, 4.9, 4.6, 4.4, 
    3.1, 4.6, 4.4, 4.2, 4.4, 5.2, 4.4, 5.1, 4.6, 4.7, 5.2, 4.7, 
    4.7, 4.6, 4.4, 4.4, 4.2, 4.5, 4.6, 4.4, 3.2, 4.8, 5.2, 5.2, 
    4.6, 4.9, 5.6, 4.6, 4.9, 4.5, 5.1, 4.6, 4.9, 4.6, 4.3, 4.6, 
    4.6, 4.3, 4.6, 4.3, 4.6, 6.5, 4.8), glu = c(88.5, 98, 117.5, 
    53, 108.5, 106, 105, 101, 91, 99.5, 128.5, 113, 114, 121.5, 
    121, 131.5, 160.5, 96, 110, 140, 119.5, 115.3, 112, 143.5, 
    116.5, 116.5, 111, 139.5, 123.5, 131, 113, 137, 114, 98.5, 
    124.5, 123.5, 111.5, 111, 127, 123, 137.5, 119, 107, 130.5, 
    142.5, 115, 133.5, 119, 148.3, 125.5, 138.5, 106.5, 153.5, 
    126.5, 179, 145, 143, 124.5, 134, 146.5, 127.5, 124.5, 123, 
    129, 145.3, 125.5, 146.5, 153.5, 115.5, 128, 110.5, 131, 
    139.5, 124, 154, 94, 76.3), uc_ratio = c(30.65603924, 15.32801962, 
    60.59075991, 7.39973361, 57.84661317, 27.46781116, 16.0944206, 
    6.131207848, 94.61568474, 19.50838861, 7.803355443, 19.41549152, 
    7.464079119, 19.67095851, 29.50643777, 62.94706724, 80.472103, 
    25.75107296, 73.57449418, 39.01677721, 41.13018598, 10.62933697, 
    7.803355443, 30.04291845, 32.75355771, 49.52129416, 5.969860273, 
    22.72153497, 7.153075823, 75.61823012, 23.50296342, 53.64806867, 
    11.19611891, 38.25340549, 88.36152487, 51.50214592, 9.196811772, 
    41.98544505, 6.35828962, 9.196811772, 94.87237407, 12.87553648, 
    6.035407725, 7.39973361, 10.72961373, 11.70503316, 9.035464197, 
    16.34988759, 11.68917269, 35.11509949, 61.85306741, 11.36076748, 
    12.2624157, 7.153075823, 14.30615165, 10.40447392, 3.901677721, 
    52.11526671, 21.45922747, 30.49469166, 81.06819266, 1.950838861, 
    34.33476395, 8.0472103, 24.94635193, 9.754194304, 64.3776824, 
    9.196811772, 11.92179304, 34.87124464, 74.39198856, 124.4635193, 
    13.79521766, 5.722460658, 66.76204101, 69.9757432, 19.50838861
    )), row.names = c(NA, -77L), class = &quot;data.frame&quot;)
</code></pre>
","0","Question"
"78833727","","<p>I want to save my trained model and load it in a different file.</p>
<p>I've tried using
<code>model.save(my_model.keras)</code>
but when I load it in a different file or when I load it after my runtime gets disconnected (Colab notebook), it does not work. Can anyone suggest a different method or explain what I am doing wrong?</p>
","-1","Question"
"78833739","","<p>I have a data_set that stores the assistant's answers to my questions/tasks</p>
<pre><code>data_set = {  'whats new':'passive Is nothing special...',  '':'', }
</code></pre>
<p>passive - a stub in a simple dialog with a bot.</p>
<p>can an assistant perform 2 functions at once? for example, the functions of opening the browser and the game.</p>
<p>that is, I want it to look something like this:</p>
<pre><code>data_set = {  'whats new':'game browser Is nothing special...',  '':'', }

def game():
    pass
def browser():
    pass
</code></pre>
<p>I expect my assistant to perform 2 functions at once in one phrase</p>
","0","Question"
"78835380","","<p>I am experimenting with TensorFlow Federated, simulating a training process with the FedAvg algorithm.</p>
<pre class=""lang-py prettyprint-override""><code>def model_fn():
  # Wrap a Keras model for use with TensorFlow Federated
  keras_model = get_uncompiled_model()

  # For the federated procedure, the model must be uncompiled
  return tff.learning.models.functional_model_from_keras(
        keras_model,
        loss_fn=tf.keras.losses.BinaryCrossentropy(),
        input_spec=(
              tf.TensorSpec(shape=[None, X_train.shape[1]], dtype=tf.float32),
              tf.TensorSpec(shape=[None], dtype=tf.int32)
        ),
        metrics_constructor=collections.OrderedDict(
              accuracy=tf.keras.metrics.BinaryAccuracy,
              precision=tf.keras.metrics.Precision,
              recall=tf.keras.metrics.Recall,
              false_positives=tf.keras.metrics.FalsePositives,
              false_negatives=tf.keras.metrics.FalseNegatives,
              true_positives=tf.keras.metrics.TruePositives,
              true_negatives=tf.keras.metrics.TrueNegatives
            )
  )

trainer = tff.learning.algorithms.build_weighted_fed_avg(
                      model_fn= model_fn(),
                      client_optimizer_fn=client_optimizer,
                      server_optimizer_fn=server_optimizer
                    )
</code></pre>
<p>I want to use custom weights to aggregate the clients' updates instead of using their number of samples. I know that <code>tff.learning.algorithms.build_weighted_fed_avg()</code> has a parameter called <code>client_weighting,</code> but the only value accepted is from the class <code>tff.learning.ClientWeighting</code>, which is an enum.</p>
<p>So, the only way to do that seems to be to write a custom WeightedAggregator. I've tried following <a href=""https://github.com/google-parfait/tensorflow-federated/blob/main/docs/tutorials/custom_aggregators.ipynb"" rel=""nofollow noreferrer"">this tutorial</a> that explains how to write an unweighted aggregator, but I cannot make it work transforming it into a weighted one.</p>
<p>This is what I've tried to do:</p>
<pre class=""lang-py prettyprint-override""><code>@tff.tensorflow.computation
def custom_weighted_aggregate(values, weights):
    # Normalize client weights
    total_weight = tf.reduce_sum(weights)
    normalized_weights = weights / total_weight

    # Compute weighted sum of client updates
    weighted_sum = tf.nest.map_structure(
        lambda v: tf.reduce_sum(normalized_weights * v, axis=0),
        values
    )

    return weighted_sum

class CustomWeightedAggregator(tff.aggregators.WeightedAggregationFactory):
    def __init__(self):
        pass

    def create(self, value_type, weight_type):
        @tff.federated_computation
        def initialize():
            return tff.federated_value(0.0, tff.SERVER)

        @tff.federated_computation(
            initialize.type_signature.result,
            tff.FederatedType(value_type, tff.CLIENTS),
            tff.FederatedType(weight_type, tff.CLIENTS)
        )
        def next(state, value, weight):
            aggregate_value = tff.federated_map(custom_weighted_aggregate, (value, weight))
            return tff.templates.MeasuredProcessOutput(
                state, aggregate_value, tff.federated_value((), tff.SERVER)
            )

        return tff.templates.AggregationProcess(initialize, next)

    @property
    def is_weighted(self):
        return True
</code></pre>
<p>But I get the following error:</p>
<p><em><strong>AggregationPlacementError</strong>: The &quot;result&quot; attribute of return type of <code>next_fn</code> must be placed at SERVER, but found {&lt;float32[7],float32,float32[1],float32&gt;}@CLIENTS.</em></p>
","1","Question"
"78835539","","<p>I am using Isolation Forest to identify anomalies in a very large data frame. The data is noisy, so I have conducted many filtering operations to smooth out the noise so that the true anomalies present in the data stand out. I then used .diff() on this data set to create a straight line that spikes when an anomaly occurs. Isolation Forest is then used to identify these anomalies.</p>
<p>My issue is that Isolation Forest is identifying the anomaly at the earliest point it can detect an anomaly from occurring, but I need it to detect it at the peak difference.</p>
<pre><code>df[&quot;Ref Wt. Denoised&quot;] = denoise(df[&quot;Ref Wt.&quot;].values, level=2)
df[&quot;Ref Wt. Savgol&quot;] = apply_savgol_filter(df[&quot;Ref Wt. Denoised&quot;], window_length=101, polyorder=3)
df[&quot;Ref Wt. Smoothed&quot;] = df[&quot;Ref Wt. Savgol&quot;].rolling(window=indexer).mean()
df[&quot;Ref Wt. Diff&quot;] = df[&quot;Ref Wt. Smoothed&quot;].diff(periods=300).fillna(0)

df[&quot;WOB Anomaly&quot;] = detect_wob.predict(df[&quot;Ref Wt. Diff&quot;].values.reshape(-1, 1))

df[&quot;WOB Zero Event&quot;] = df[&quot;WOB Anomaly&quot;] == -1
</code></pre>
<p>I have played around using .shift() to fix it, but this manual change works for some values but not all. I really want to avoid changing the window size that I use to smooth the data over because this severely affects accuracy.</p>
<p><a href=""https://i.sstatic.net/1LNaEk3L.png"" rel=""nofollow noreferrer"">Image of Issue and Fix I'm Looking For</a></p>
","0","Question"
"78836514","","<p>I'm trying to set weights on one of the layers within my model to no avail.</p>
<p>I've followed the solutions to similar problems online and none of them are seeming to work for me. The variable 'w' (as denoted in the code below) is of structure [numpy array, numpy array]. The first is of size(3, 3, 3, 64) and the second is of shape (64,). I want to achieve a similar functionality to that of the 'weights' kwarg in tf 2.X, but can't seem to get it working. Here's my code:</p>
<pre><code>encoder = Sequential()
encoder.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', use_bias=False, input_shape=(SIZE, SIZE, 3)))
w = model.layers[0].get_weights()
encoder.layers[0].set_weights([w])
encoder.add(layers.MaxPooling2D((2, 2), padding='same'))
encoder.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', weights=model.layers[2].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding='same'))
encoder.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same', weights=model.layers[4].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding='same'))
encoder.summary()
</code></pre>
<p>And the error:</p>
<pre><code>ERROR: ValueError: You called `set_weights(weights)` on layer 'conv2d_7' with 
a weight list of length 2, but the layer was expecting 1 weights.
</code></pre>
","1","Question"
"78837133","","<p>When my friend was working on an <strong>Age and Gender Detection model</strong> using <strong>Convolutional Neural Network</strong>, he faced this ValueError. He has given the 'metrics' argument as a list during model defining, but it said it should have as many entries as the model have. My model has two outputs 'age' and 'gender'.</p>
<p>He was defining his CNN model and when he tried to fit the model on training data it showed an error saying</p>
<blockquote>
<p>ValueError: For a model with multiple outputs, when providing the <code>metrics</code> argument as a list, it should have as many entries as the model has outputs. Received:
metrics=['accuracy']
of length 1 whereas the model has 2 outputs.</p>
</blockquote>
<p>This is my full error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError                                Traceback (most recent call last)
Cell In[47], line 1
----&gt; 1 History=Model.fit(X_train,Y_train_2,batch_size=64,validation_data=(X_test,Y_test_2),epochs=10,callbacks=callback_list)

File c:\Users\mebub_9a7jdi8\Desktop\Age_Gender_Detection_Model\.env\Lib\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File c:\Users\mebub_9a7jdi8\Desktop\Age_Gender_Detection_Model\.env\Lib\site-packages\keras\src\trainers\compile_utils.py:250, in CompileMetrics._build_metrics_set(self, metrics, num_outputs, output_names, y_true, y_pred, argument_name)
    248 if isinstance(metrics, (list, tuple)):
    249     if len(metrics) != len(y_pred):
--&gt; 250         raise ValueError(
    251             &quot;For a model with multiple outputs, &quot;
    252             f&quot;when providing the `{argument_name}` argument as a &quot;
    253             &quot;list, it should have as many entries as the model has &quot;
    254             f&quot;outputs. Received:\n{argument_name}={metrics}\nof &quot;
    255             f&quot;length {len(metrics)} whereas the model has &quot;
    256             f&quot;{len(y_pred)} outputs.&quot;
    257         )
    258     for idx, (mls, yt, yp) in enumerate(
...
    261         if not isinstance(mls, list):

ValueError: For a model with multiple outputs, when providing the `metrics` argument as a list, it should have as many entries as the model has outputs. Received:
metrics=['accuracy']
of length 1 whereas the model has 2 outputs. 
</code></pre>
<p>This is my code for defining model:</p>
<pre class=""lang-py prettyprint-override""><code>def model(input_shape):
    inputs=Input((input_shape))
    conv_1=Convolution(inputs,32)
    maxp_1=MaxPooling2D(pool_size=(2,2))(conv_1)
    conv_2=Convolution(maxp_1,64)
    maxp_2=MaxPooling2D(pool_size=(2,2))(conv_2)
    conv_3=Convolution(maxp_2,128)
    maxp_3=MaxPooling2D(pool_size=(2,2))(conv_3)
    conv_4=Convolution(maxp_3,256)
    maxp_4=MaxPooling2D(pool_size=(2,2))(conv_4)
    flatten= Flatten()(maxp_4)
    dense_1=Dense(64,activation='relu')(flatten)
    dense_2=Dense(64,activation='relu')(flatten)
    drop_1=Dropout(0.2)(dense_1)
    drop_2=Dropout(0.2)(dense_2)
    output_1=Dense(1,activation='sigmoid',name='sex_out')(drop_1)
    output_2=Dense(1,activation='relu',name='age_out')(drop_2)
    model=Model(inputs=[inputs],outputs=[output_1,output_2])
    model.compile(loss=[&quot;binary_crossentropy&quot;,&quot;mae&quot;],optimizer=&quot;Adam&quot;,metrics=[&quot;accuracy&quot;])
    return model
</code></pre>
","0","Question"
"78839246","","<p>When testing an example code from the <strong>TensorFlow</strong> website using <strong>Jupyter Notebook</strong>, which is available <a href=""https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy"" rel=""nofollow noreferrer"">here</a>, I encountered an error. You can find my SO question about that error <a href=""https://stackoverflow.com/q/78836989/5029509"">here</a>.</p>
<p>As a result, I decided to write equivalent implementations for the same functionality using <strong>PyTorch</strong> with <strong>Opacus</strong> and <strong>PySyft</strong>. However, I unfortunately encountered another error.</p>
<p>Below is the code for implementing the same functionality of the example code from the TensorFlow website, but using PyTorch with Opacus and PySyft, along with the error message.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

# Define a simple model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.fc1 = nn.Linear(32*26*26, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(-1, 32*26*26)
        x = self.fc1(x)
        return torch.log_softmax(x, dim=1)

# Data loaders
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Initialize model, optimizer, and loss function
model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.NLLLoss()

# Initialize PrivacyEngine
privacy_engine = PrivacyEngine(
    model,
    batch_size=64,
    sample_size=len(train_loader.dataset),
    epochs=1,
    max_grad_norm=1.0,
)

privacy_engine.attach(optimizer)

# Training loop
model.train()
for epoch in range(1):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

# Print privacy statistics
epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[1], line 32
     29 criterion = nn.NLLLoss()
     31 # Initialize PrivacyEngine
---&gt; 32 privacy_engine = PrivacyEngine(
     33     model,
     34     batch_size=64,
     35     sample_size=len(train_loader.dataset),
     36     epochs=1,
     37     max_grad_norm=1.0,
     38 )
     40 privacy_engine.attach(optimizer)
     42 # Training loop

TypeError: PrivacyEngine.__init__() got an unexpected keyword argument 'batch_size'
</code></pre>
","1","Question"
"78839421","","<p>I want to create <code>predict</code> and <code>predict_proba</code> methods in my DecisionTreeClassifier implementation, but it gives the error</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\Nijat\project.py&quot;, line 136, in &lt;module&gt;
    print(model.predict(X))
          ^^^^^^^^^^^^^^^^
  File &quot;c:\Users\Nijat\project.py&quot;, line 128, in predict
    return [1 if p[0] &gt; 0.5 else 0 for p in self.predict_proba(X)]
                                            ^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\Nijat\project.py&quot;, line 121, in predict_proba
    class1_proba = self.bypass_tree(self.tree, sample)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\Nijat\project.py&quot;, line 105, in bypass_tree
    while node['type'] == 'node':
          ~~~~^^^^^^^^
KeyError: 'type'
</code></pre>
<p>Here's my code:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd

class MyTreeClf:
    def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.max_leafs = max_leafs
        self.tree = None
        self.leafs_cnt = 0

    def node_entropy(self, probs):
        return -np.sum([p * np.log2(p) for p in probs if p &gt; 0])

    def node_ig(self, x_col, y, split_value):
        left_mask = x_col &lt;= split_value
        right_mask = x_col &gt; split_value

        if len(x_col[left_mask]) == 0 or len(x_col[right_mask]) == 0:
            return 0

        left_counts = np.bincount(y[left_mask])
        right_counts = np.bincount(y[right_mask])

        left_probs = left_counts / len(y[left_mask]) if len(y[left_mask]) &gt; 0 else np.zeros_like(left_counts)
        right_probs = right_counts / len(y[right_mask]) if len(y[right_mask]) &gt; 0 else np.zeros_like(right_counts)

        entropy_after = (len(y[left_mask]) / len(y) * self.node_entropy(left_probs) +
                         len(y[right_mask]) / len(y) * self.node_entropy(right_probs))
        entropy_before = self.node_entropy(np.bincount(y) / len(y))

        return entropy_before - entropy_after

    def get_best_split(self, X: pd.DataFrame, y: pd.Series):
        best_col, best_split_value, best_ig = None, None, -np.inf

        for col in X.columns:
            sorted_unique_values = np.sort(X[col].unique())

            for i in range(1, len(sorted_unique_values)):
                split_value = (sorted_unique_values[i - 1] + sorted_unique_values[i]) / 2

                ig = self.node_ig(X[col], y, split_value)

                if ig &gt; best_ig:
                    best_ig = ig
                    best_col = col
                    best_split_value = split_value

        return best_col, best_split_value

    def fit(self, X: pd.DataFrame, y: pd.Series, depth=1, node=None):
        if self.max_leafs &lt; 2:
            self.leafs_cnt = 2
            return

        if node is None:
            node = {}
            self.tree = node

        best_col, best_split_value = self.get_best_split(X, y)

        node['type'] = None
        node['feature'] = best_col
        node['threshold'] = best_split_value

        if len(y.unique()) == 1:
            self.leafs_cnt += 1
            node['type'] = 'leaf'
            node['class_counts'] = {y.unique()[0]: len(y)}
            return

        if len(y) == 1:
            self.leafs_cnt += 1
            node['type'] = 'leaf'
            node['class_counts'] = {y.values[0]: 1}
            return

        if depth &gt;= self.max_depth or len(y) &lt; self.min_samples_split or (self.leafs_cnt + 2 &gt; self.max_leafs):
            self.leafs_cnt += 1
            node['type'] = 'leaf'
            node['class_counts'] = y.value_counts().to_dict()
            return

        if best_col is None:
            node['type'] = 'leaf'
            node['class_counts'] = y.value_counts().to_dict()
            self.leafs_cnt += 1
            return

        node['type'] = 'node'
        node['feature'] = best_col
        node['threshold'] = best_split_value

        left_mask = X[best_col] &lt;= best_split_value
        right_mask = X[best_col] &gt; best_split_value

        node['left'] = {}
        node['right'] = {}

        self.fit(X[left_mask], y[left_mask], depth + 1, node['left'])
        self.fit(X[right_mask], y[right_mask], depth + 1, node['right'])

    def bypass_tree(self, node, sample):
        while node['type'] == 'node':
            feature_value = sample[node['feature']]
            if feature_value &lt;= node['threshold']:
                node = node['left']
            else:
                node = node['right']
    
        total_count = sum(node['class_counts'].values())
        class_1_count = node['class_counts'].get(1, 0)
        class1_proba = class_1_count / total_count if total_count &gt; 0 else 0

        return class1_proba

    def predict_proba(self, X: pd.DataFrame):
        proba = []
        for _, sample in X.iterrows():
            class1_proba = self.bypass_tree(self.tree, sample)

            proba.append(class1_proba)
    
        return np.array(proba)

    def predict(self, X: pd.DataFrame):
        return [1 if p[0] &gt; 0.5 else 0 for p in self.predict_proba(X)]

df = pd.read_csv('c:\\Users\\Nijat\\Downloads\\banknote+authentication.zip', header=None)
df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'target']
X, y = df.iloc[:, :4], df['target']

model = MyTreeClf(max_depth=3, min_samples_split=2, max_leafs=1)
model.fit(X, y)
print(model.predict(X))
</code></pre>
<p>The <code>predict</code> and <code>predict_proba</code> methods take a matrix of features in the form of a pandas dataframe.
For each row from the dataframe:</p>
<ul>
<li>Traverse the nodes of the tree up to one of the leaves.</li>
<li>Write the probability for the first class.</li>
<li>Return a list of predictions - as many as there are rows in the dataframe:
<code>predict_proba</code> - returns probabilities (for the first class).
<code>predict</code> - translates probabilities into binary classes by threshold &gt; 0.5</li>
</ul>
<p>Validation:</p>
<p>Input data: two sets of parameters for the decision tree</p>
<p>Output: returned predictions (their sum) of probabilities and labels</p>
<p>I exactly don't know about datasets that used for checking the code but I think that it's &quot;Banknote authentication&quot;</p>
<p>Sample input:</p>
<pre><code>{&quot;max_depth&quot;: 3, &quot;min_samples_split&quot;: 2, &quot;max_leafs&quot;: 1}
</code></pre>
<p>Sample output:</p>
<pre><code>11.2443438914
</code></pre>
","0","Question"
"78841275","","<p>I'm learning to use Google AI Studio and when generating the snippet I came across these terms:</p>
<pre class=""lang-js prettyprint-override""><code>const generationConfig = {
  temperature: 1,
  topP: 0.95,
  topK: 64,
  maxOutputTokens: 8192,
  responseMimeType: &quot;text/plain&quot;,
};
</code></pre>
<p>I'm struggling to understand what those terms mean. What are <code>topP</code>, <code>topK</code>, and <code>maxOutputTokens</code>. I want to understand these in order to use them properly.</p>
","0","Question"
"78841736","","<p>I have no experience in ML and am a beginner in web application programming. I currently have a flask app that uses audiocraft and a classificatoin model. Currently i have these stored locally within the application folder. I have built an image of this app and it turned out to be 7GB.</p>
<p>Is there a way to have these models/ framework stored somewhere else and only referenced when I need them?</p>
<p>Also, when i run the container on docker, it takes around 10minuts to generate a 8s audio from audiocraft. What would you recommend I do to fasten the process?</p>
<p>music_generation\routes.py (snippet)</p>
<pre><code>def load_model():
    model = MusicGen.get_pretrained('facebook/musicgen-small')
    return model

def generate_music_tensors(description, duration: int):
    model = load_model()
    model.set_generation_params(
        use_sampling=True,
        top_k=250,
        duration=duration
    )
    output = model.generate(
        descriptions=[description],
        progress=True,
        return_tokens=True
    )
    return output[0]

@music_generation_bp.route('/', methods=['POST'])
def generate_music():
    data = request.json

    description = data.get('description')
    duration = data.get('duration', 8)  # Default to 8 seconds if not provided
    print(&quot;Description:&quot;, description)
    print(&quot;Duration:&quot;, duration)

    if not description:
        return jsonify({'error': 'Description is required'}), 400
    # Generate unique key for the user
    user_id = str(uuid.uuid4())  # or use a user ID from your authentication system
    audio_key_prefix = f&quot;generated_music_{user_id}_{description}&quot;

    # Generate music tensors
    music_tensors = generate_music_tensors(description, duration)
    print(&quot;Music Tensors: &quot;, music_tensors)
...

</code></pre>
<p>image_classification\routes.py  (snippet)</p>
<pre><code># Load the pre-trained model
model_path = os.path.join(MODELS_DIR, 'multi_output_model.h5')
model = tf.keras.models.load_model(model_path)

@image_classification_bp.route('/', methods=['POST'])
@limiter.limit(&quot;1/minute&quot;)
def classify_image():
     if 'file' not in request.files:
         return jsonify({&quot;error&quot;: &quot;No file part in the request&quot;}), 400

     file = request.files['file']

     if file.filename == '':
         return jsonify({&quot;error&quot;: &quot;No selected file&quot;}), 400

    #file = os.path.join(TEST_IMG_DIR, 'blue-dress2.png')

    if file:
        # Read the image file
        img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_UNCHANGED)
        img = cv2.resize(img, (IMAGE_DIMS[1], IMAGE_DIMS[0]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = preprocess_input(img)
        img = np.expand_dims(img, axis=0)

        # Perform prediction
        predictions = model.predict(img)

</code></pre>
","1","Question"
"78842184","","<p>I have 20k small label images and each image has the word &quot;Back&quot; or &quot;Front&quot;.</p>
<p>Image resolution is all (200px, 25px)</p>
<p><a href=""https://i.sstatic.net/oJMDTLlA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oJMDTLlA.png"" alt=""enter image description here"" /></a></p>
<p>I can classify these images 100% accuracy with tesseract_OCR.</p>
<pre><code>        txt = pytesseract.image_to_string(img, lang='eng')
        if &quot;Front&quot; in txt:
            return &quot;Front&quot;
        if &quot;Back&quot; in txt:
            return &quot;Back&quot;
</code></pre>
<p>problem is, it is too slow(1 hour for 20k images) and need to install OCR packages.</p>
<p>I know even 3 layer Simple CNN also works well for it, but I think this problem seems to be solvable with simple algorithm without difficult techniques.</p>
<p>Can you recommend a new approach for me?</p>
<p>thank you.</p>
","1","Question"
"78842299","","<p>I have a segmented image as shown below<a href=""https://i.sstatic.net/WTS7PFwX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WTS7PFwX.png"" alt=""enter image description here"" /></a></p>
<p>There is a very thin line of different colors on the edges of the vehicle (Red patch where the person is driving). I would like to get rid of this thin line by assigning different label ids (either red or black) based on the neighboring pixels. I know how to extract pixels based on the required color or even ids. But in this case, the color or ID is not fixed, it could be different colors or IDs in a different image. I am not able to come up with a way to extract just these pixels. Can someone help me with extracting pixels that belong to the thin line?</p>
","0","Question"
"78843004","","<ol>
<li>Why am I encountering the ModuleNotFoundError for the datachain.lib module?</li>
<li>Are there any additional steps I need to take to properly use the datachain package in my project?</li>
</ol>
<p>I'm working on a Python project and I encountered the following error when trying to import a module:</p>
<pre><code>import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain
</code></pre>
<p>Error Message:</p>
<pre><code>ModuleNotFoundError: No module named 'datachain.lib'; 'datachain' is not a package
</code></pre>
<p>Details:</p>
<ul>
<li>I've installed datachain using pip: <code>pip install datachain</code>.</li>
<li>The installed version of datachain is 0.2.18 as shown by running <code>pip list</code>.</li>
<li>I have verified that the package is correctly installed and located in my Python environment.</li>
</ul>
","-1","Question"
"78843331","","<p>Code for getting the data:</p>
<pre><code>import pandas as pd
import torch

dataset = pd.read_csv('/kaggle/input/fish-bear/population_data.csv')
years = torch.tensor(dataset['year'], dtype = torch.float64)
fish_pop = torch.tensor(dataset['fish_hundreds'], dtype = torch.float64)
bears_pop = torch.tensor(dataset['bears_hundreds'], dtype = torch.float64)
pop = torch.cat((fish_pop.reshape((51, 1)), bears_pop.reshape((51, 1))), 1)
</code></pre>
<p>Ordinary Differential Equation Solver</p>
<pre><code>from typing import List, Callable, Sequence, NamedTuple, Union

class _Tableau(NamedTuple):

    c: List[float]
    b: List[float]
    a: List[List[float]]


rk4_tableau = _Tableau(c=[0.0, 0.5, 0.5, 1.0],
                       b=[1 / 6., 1 / 3., 1 / 3., 1 / 6.],
                       a=[[0.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.0],
                          [0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]])


def explicit_rk(tableau: _Tableau, fcn: Callable[..., torch.Tensor],
                y0: torch.Tensor, t: torch.Tensor,
                params: Sequence[torch.Tensor]):
    c = tableau.c
    a = tableau.a
    b = tableau.b
    s = len(c)
    nt = len(t)

    # set up the results list
    yt_lst: List[torch.Tensor] = []
    yt_lst.append(y0)
    y = yt_lst[-1]
    for i in range(nt - 1):
        t0 = t[i]
        t1 = t[i + 1]
        h = t1 - t0
        ks: List[torch.Tensor] = []
        ksum: Union[float, torch.Tensor] = 0.0
        for j in range(s):
            if j == 0:
                k = fcn(y, t0, params)
            else:
                ak: Union[float, torch.Tensor] = 0.0
                aj = a[j]
                for m in range(j):
                    ak = aj[m] * ks[m] + ak
                k = fcn(h * ak + y, t0 + c[j] * h, params)
            ks.append(k)
            ksum = ksum + b[j] * k
        y = h * ksum + y
        yt_lst.append(y)
    yt = torch.stack(yt_lst, dim=0)
    return yt

def rk4_ivp(fcn: Callable[..., torch.Tensor], y0: torch.Tensor, t: torch.Tensor,
            params: Sequence[torch.Tensor], **kwargs):
    return explicit_rk(rk4_tableau, fcn, y0, t, params)
</code></pre>
<p>Minimization Code:</p>
<pre><code>import torch

def lotka_volterra(y, t, params):
    y1, y2 = y
    a, b, c, d = params

    return torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2])

def loss_function(params):

    y0 = torch.tensor([fish_pop[0], bears_pop[0]], dtype = torch.float64)

    t = torch.linspace(years[0], years[-1], len(years), dtype = torch.float64)

    output = rk4_ivp(lotka_volterra, y0, t, params)

    loss = torch.sum((output - pop)**2)
    loss.requires_grad = True
    return loss

def minimize(loss_function, initial_parameters: torch.Tensor):
    list_params = []
    params = initial_parameters
    params.requires_grad = True
    optimizer = torch.optim.SGD([params], lr=0.5)

    for i in range(5):
        optimizer.zero_grad()
        loss: torch.Tensor = loss_function(params)
        loss.backward()
        optimizer.step()
        list_params.append(params.detach().clone())

    return params, list_params

starting_point = torch.nn.Parameter(torch.tensor([1.1, .4, .1, .4], dtype = torch.float64))
minimized_params, list_of_params = minimize(loss_function, starting_point)

loss_function(minimized_params), minimized_params
</code></pre>
<p>At the end of iteration the parameters do not get optimised and return as it is.</p>
<p>Result:</p>
<pre><code>(tensor(118.6865, dtype=torch.float64, requires_grad=True),
 Parameter containing:
 tensor([1.1000, 0.4000, 0.1000, 0.4000], dtype=torch.float64,
        requires_grad=True))
</code></pre>
<p>Kaggle Notebook Link: <a href=""https://www.kaggle.com/code/rakshitsingh421/parameter-estimation/edit"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/rakshitsingh421/parameter-estimation/edit</a></p>
<p>I tried to change requires_grad attributes but it didn't worked.</p>
","0","Question"
"78844279","","<p>I am trying to train diffusers/UNet2DConditionModel from scratch. Currently I have error on unet forwarding: mat1 and mat2 shapes cannot be multiplied (288x512 and 1280x512). I noticed that mat1 first dimension (288) can vary depending on dataset batch.</p>
<p>How do I fix matrices shapes error? Do I need to pad mat1 with zeros to make its shape as same as mat2: 1280x512, or I have invalid model init parameters set. I will be thankful for any help.</p>
<p>Here is my training code</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms as T
from transformers import CLIPModel, CLIPProcessor, CLIPTextModel, AutoTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler
import pandas as pd
from PIL import Image
import io
from tqdm.auto import tqdm

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


# Dataset class, returns images and corresponding textual captions
class TextImgDataset(Dataset):
    def __init__(self, fp: str):
        self.df = pd.read_parquet(fp)
        self.transform = T.Compose([
            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
            T.Resize((64, 64)),
            T.ToTensor(),
        ])

    def __len__(self) -&gt; int:
        return self.df.shape[0]

    def __getitem__(self, idx) -&gt; (torch.Tensor, str):
        row = self.df.iloc[idx]
        img_bytes = io.BytesIO(row['image']['bytes'])
        image = Image.open(img_bytes)
        image_tensor = self.transform(image)
        caption = row['text']

        return image_tensor, caption


# Initialize models
clip_model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;).to(device)
clip_processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

text_encoder = CLIPTextModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

vae = AutoencoderKL.from_single_file(
    &quot;https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors&quot;).to(
    device)

unet = UNet2DConditionModel(
    in_channels=4,
    out_channels=4,
    layers_per_block=2,
    sample_size=64,
    block_out_channels=(128, 256, 512, 512),
    down_block_types=(&quot;DownBlock2D&quot;, &quot;DownBlock2D&quot;, &quot;DownBlock2D&quot;, &quot;AttnDownBlock2D&quot;),
    up_block_types=(&quot;AttnUpBlock2D&quot;, &quot;UpBlock2D&quot;, &quot;UpBlock2D&quot;, &quot;UpBlock2D&quot;),
).to(device)

noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=&quot;linear&quot;)

# DataLoader
# I use dataset from here https://huggingface.co/datasets/pranked03/flowers-blip-captions
dataset = TextImgDataset(fp='~/dataset.parquet')
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Optimizers
optimizer_vae = torch.optim.Adam(vae.parameters(), lr=1e-4)
optimizer_unet = torch.optim.Adam(unet.parameters(), lr=1e-4)

# Training Loop
num_epochs = 10

unet.train()

for epoch in range(num_epochs):
    for batch in tqdm(dataloader):
        images, captions = batch
        images = images.to(device)
        latents = vae.encode(images).latent_dist.sample()
        latents = latents * vae.config.scaling_factor

        noise = torch.randn_like(latents)
        bsz = latents.shape[0]
        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()

        inputs = tokenizer(captions, padding=True, return_tensors=&quot;pt&quot;, truncation=True)
        outputs = text_encoder(**inputs).last_hidden_state.to(device)

        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
        #noisy_latents shape: [16, 4, 8, 8]
        #timesteps shape: torch.Size([16])
        #encoder_hidden_states shape: torch.Size([16, 18, 512])
        pred = unet(sample=noisy_latents, timestep=timesteps, encoder_hidden_states=outputs, return_dict=False) # getting the error here

        # .... rest of the code (backpropagation and sampling)

</code></pre>
","1","Question"
"78844901","","<p>In the gradient descent algorithm, I update the B and M values ​​according to their derivatives and then multiply them with the Learning rate value, but when I use the same value for L, such as 0.0001, it does not work correctly. Decreasing or increasing the L value does not work. As a workaround, I had to set different L values ​​for both b and m values. Is this normal or is there an error?</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import time
import random

# Veri seti
veri_seti = &quot;study_score_decreasing.csv&quot; # study_score_decreasing.csv # study_score_increasing.csv 
data = pd.read_csv(veri_seti)

# Gradient Descent Fonksiyonu
def gradient_descent(m_next, b_next, points, L):
    m_gradient = 0
    b_gradient = 0
    n = len(points)
    
    for i in range(n):
        x = points.iloc[i].study_time
        y = points.iloc[i].score
        
        m_gradient += -(2/n) * x * (y - (m_next * x + b_next))
        b_gradient += -(2/n) * (y - (m_next * x + b_next))
    
    m = m_next - m_gradient * 0.0001 #(L = 0.0001)
    b = b_next - b_gradient * 0.1    #(L = 0.1)
    
    return m, b

# Grafik Gösterim Fonksiyonu
def show_graph(m, b):
    plt.scatter(data.study_time, data.score, color=&quot;red&quot;)
    x_range = range(int(data.study_time.min()), int(data.study_time.max()) + 1)
    plt.plot(x_range, [m * x + b for x in x_range], color=&quot;blue&quot;)
    plt.xlabel('Study Time')
    plt.ylabel('Score')
    plt.title('Study Time vs Score')
    plt.show()
    time.sleep(0.001)
    print(&quot;=&gt;  F(X):&quot;, round(m, 1), &quot;X +&quot;, round(b, 3))

# Ana Fonksiyon
def main(m, b, L, epochs):
    print(&quot;=&gt;  F(X):&quot;, m, &quot;X&quot;, b)
    
    for i in range(epochs):
        m, b = gradient_descent(m, b, data, L)
        show_graph(m, b)
        
# Başlangıç değerleri
main(random.uniform(-1, 110), random.uniform(-10, 10), 0.1, 250)
</code></pre>
<p>I updated the L values ​​one by one and got a logical result, but with a common L value, why does the solution seem illogical?</p>
","-1","Question"
"78844904","","<p>I have a code to classify an image.</p>
<pre><code>training1 = xlsread('Data Train');

% mentions the location of the training data matrix in the excel file
training = [training1(:,1) training1(:,2) training1(:,3) training1(:,4) training1(:,5) training1(:,6) training1(:,7) training1(:,8) training1(:,9) training1(:,10) training1(:,11) training1(:,12) training1(:,13) training1(:,14) training1(:,15) training1(:,16) training1(:,17) training1(:,18) training1(:,19) training1(:,20) training1(:,21) training1(:,22) training1(:,23) training1(:,24)];

% mentions input data variables
Z=[MeanR MeanG MeanB MeanH MeanS MeanV VarRed VarGreen VarBlue VarH VarS VarV RangeR RangeG RangeB RangeH RangeS RangeV sdR sdG sdB sdH sdS sdV];

%perform knn classification
result = knnsearch(training,Z);

if (result&gt;=1 &amp;&amp; result&lt;=20)
    set(handles.EditBox,'string','Raw');
elseif (result&gt;=21 &amp;&amp; result&lt;=40)
    set(handles.EditBox,'string','Undercook');
elseif (result&gt;=41 &amp;&amp; result&lt;=60)
    set(handles.EditBox,'string','Cook');
elseif (result&gt;=61 &amp;&amp; result&lt;=80)
    set(handles.EditBox,'string','Rotten');
end
</code></pre>
<p>Does the <code>knnsearch</code> syntax only default k with a value of 1?</p>
<p>How can I get the k value in <code>knnsearch</code>to be 5?
when I try to change it to</p>
<pre><code>k = 5;
result = knnsearch(training,Z,'K',k); 
</code></pre>
<p>the system does not display the classification result.</p>
","-1","Question"
"78846949","","<p>i cant load my model it keep saying error<br />
ValueError: Layer &quot;dense_2&quot; expects 1 input(s), but it received 2 input tensors. Inputs received: [&lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2896&gt;, &lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2897&gt;]</p>
<p>here's my code</p>
<pre><code>image_generator = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)
</code></pre>
<pre><code>train_dataset = image_generator.flow_from_directory(
    directory=path_to_dataset,
    target_size=(224, 224),
    batch_size=32,
    subset='training'
)

validation_dataset = image_generator.flow_from_directory(
    directory=path_to_dataset,
    target_size=(224, 224),
    batch_size=32,
    subset='validation'
)
</code></pre>
<pre><code># Menentukan jumlah kelas (num_classes) berdasarkan jumlah subfolder dalam dataset
num_classes = len(train_dataset.class_indices)
</code></pre>
<pre><code>from tensorflow.keras.applications.mobilenet import MobileNet

# Load the MobileNet model
pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
                                                      include_top=False,
                                                      weights='imagenet')


pre_trained_model.summary()

# Print dataset information for debugging
print(f&quot;Training dataset shape: {train_dataset.image_shape}&quot;)
print(f&quot;Validation dataset shape: {validation_dataset.image_shape}&quot;)
</code></pre>
<pre><code>pre_trained_model.trainable = False

# Menambahkan layer kustom di atas model pre-trained
model = tf.keras.Sequential([
    pre_trained_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax') 
])
</code></pre>
<pre><code># Compile model
#from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
</code></pre>
<pre><code> batch=40
 history = model.fit(train_dataset,
               validation_data=validation_dataset,
               epochs=20,
               steps_per_epoch = train_dataset.samples//batch,
               validation_steps = validation_dataset.samples//batch,
               verbose = 1
           )
</code></pre>
<pre><code># Load the model
model_save_path = '/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5'

# Load the model, ensuring it's compiled if needed
loaded_model = tf.keras.models.load_model(model_save_path) 

# Now you can modify the loaded model if necessary
# For example, if you want to extract a sub-model:
input_layer_index = 0  # Replace with the actual index
dense_2_index = 3  # Replace with the actual index
loaded_model = tf.keras.models.Model(inputs=loaded_model.layers[input_layer_index].input, 
                                     outputs=loaded_model.layers[dense_2_index].output)

# Check the configuration of the loaded model
for i, layer in enumerate(loaded_model.layers):
    print(f&quot;Layer {i}: {layer.name} - Input shape: {layer.input_shape} - Output Shape: {layer.output_shape}&quot;)

print(&quot;Revised model loaded successfully.&quot;)
</code></pre>
<p>i try to load model and i expect it loaded to test</p>
","-1","Question"
"78848428","","<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-37-9a9cd8046c6f&gt; in &lt;cell line: 18&gt;()
     16               loss=tf.keras.losses.CategoricalCrossentropy(),
     17               metrics=['accuracy'])
---&gt; 18 history1=model1.fit(train_data,epochs=5,validation_data=valid_data)

1 frames
/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in categorical_crossentropy(target, output, from_logits, axis)
    552         )
    553     if len(target.shape) != len(output.shape):
--&gt; 554         raise ValueError(
    555             &quot;Arguments `target` and `output` must have the same rank &quot;
    556             &quot;(ndim). Received: &quot;

ValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 19)
</code></pre>
<p>What is the problem here?</p>
<p>When I train my model with SpareCategoricalCrossentropy, then it's just working fine, but is giving a very bad accuracy and in the valid data it's zero.</p>
<p>I tried to use SpareCategorical, yet the accuracy was very low. My dataset is from kaggle, so I just downloaded it gave it an image size</p>
<pre><code>train_dir=&quot;Dataset/train/&quot;
test_dir=&quot;Dataset/test/&quot;
train_data= Images(train_dir,image_size=(150,150),batch_size=32)
valid_data= Images(test_dir,image_size=(150,150),batch_size=32)
</code></pre>
<p>And I recalled this. Now can anyone tell me what should I do ??</p>
","-2","Question"
"78850531","","<p>I am working on a binary classification problem that will classify variable stars between Heartbeat stars and ECL stars from their light curve. When I run my code, the recall  increases to 1 as the precision suddenly drops to 0.5. <a href=""https://i.sstatic.net/cwFw0o6g.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cwFw0o6g.png"" alt=""enter image description here"" /></a></p>
<p>But when I analyze the epochs, this doesn't seem to be happening:</p>
<p><a href=""https://i.sstatic.net/MB99xGsp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MB99xGsp.png"" alt=""enter image description here"" /></a></p>
<p>Here's the relevant sections of my code:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Convert columns to numeric, coercing errors to NaN
df['HJD-2450000'] = pd.to_numeric(df['HJD-2450000'], errors='coerce')
df['mag'] = pd.to_numeric(df['mag'], errors='coerce')

# Handle any NaN values if they exist
df.dropna(inplace=True)

# Create binary labels
df['Type'] = df['ID'].apply(lambda x: 0 if x.startswith('HB') else 1)

# Feature scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['HJD-2450000', 'mag']])

# Add polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(scaled_features)

# Update DataFrame with new features
df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names_out())
df_poly['Type'] = df['Type']

# Separate majority and minority classes
X = df_poly.drop('Type', axis=1)
y = df_poly['Type']

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=29130)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=29130)

# Define model inputs
inputs = tf.keras.layers.Input(shape=(X_train.shape[1],), dtype=tf.float32, name='features')

# Model architecture with regularization
x = tf.keras.layers.Dense(64, activation='relu')(inputs)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
x = tf.keras.layers.Dropout(0.3)(x)
dense_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)

def create_model(my_inputs, my_outputs, my_learning_rate):
    model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),
                  loss='binary_crossentropy',
                  metrics=[tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.BinaryAccuracy(name='BA')])
    return model

learning_rate = 0.001  # Use Adam optimizer with a lower learning rate
my_model = create_model(inputs, dense_output, learning_rate)

# Train the model with early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

def train_model(model, features, labels, epochs, batch_size):
    history = model.fit(x=features, y=labels, batch_size=batch_size,
                        epochs=epochs, validation_split=0.2, shuffle=True, callbacks=[early_stopping])
    epochs = history.epoch
    hist = pd.DataFrame(history.history)
    ba = hist[&quot;BA&quot;]
    return epochs, ba

epochs, ba = train_model(my_model, X_train, y_train, epochs=50, batch_size=64)

# Plotting the accuracy curve
def plot_the_loss_curve(epochs, ba):
    plt.figure()
    plt.xlabel(&quot;Epoch&quot;)
    plt.ylabel(&quot;Binary Accuracy&quot;)
    plt.plot(epochs, ba, label=&quot;Binary Accuracy&quot;)
    plt.legend()
    plt.ylim([ba.min()*0.94, ba.max()*1.05])
    plt.show()

plot_the_loss_curve(epochs, ba)

# Evaluate the model and plot precision-recall curve
def plot_precision_recall_curve(model, features, labels):
    # Predict probabilities
    y_scores = model.predict(features).ravel()

    # Calculate precision and recall at different thresholds
    precision, recall, thresholds = precision_recall_curve(labels, y_scores)

    # Plot the precision-recall curve
    display = PrecisionRecallDisplay(precision=precision, recall=recall)
    display.plot()
    plt.title(&quot;Precision-Recall Curve&quot;)
    plt.show()

plot_precision_recall_curve(my_model, X_test, y_test)

# Evaluate the model
evaluation = my_model.evaluate(x=X_test, y=y_test, batch_size=64)
print(f&quot;Model AUC: {evaluation[1]}&quot;)

</code></pre>
<p>I am using the time value and the magnitude (luminosity) value as my features to observe the light curve as the variable star changes over time
Is there something I'm missing? What I think is happening is that since this is an imbalanced class set (ECL stars make up 90% of the set) the model is just classifying everything as an ECL star. How do I prevent this from happening? Or specifically, is there something wrong with my features in particular? Do I have to take a cross product of time and magnitude to capture their relation to the computer?</p>
<p>I have tried dropping come of the ECL stars' value, but the precession keeps dropping to 0.5</p>
","0","Question"
"78851057","","<p>I am trying to develop a NetLogo extension to communicate with different LLMS (online, offline).  The LLM calls return a JSON formatted string.  I'd like to parse the JSON and convert it into nested TABLE objects, accessing the NetLogo Table extension.</p>
<p>Is there a way for one extension to access and use the classes in another extension?</p>
","2","Question"
"78851708","","<p>I am creating a traditional neural network for PA behavioral modeling in Python using TensorFlow. The model takes in input <em>I</em> and <em>Q</em> values and predicts the amplifier output. So that's a .csv file with two columns. One of my goals is to prune (or optimize in any way) the model I built. The original model works just fine. However, I am facing problems while pruning the created, trained, and tested model.</p>
<p>Below is the original model:</p>
<pre><code>import os

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import keras
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model, load_model, save_model, clone_model, Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.metrics import Accuracy
import tensorflow_model_optimization as tfmot
import time

# Load your data
data_input = pd.read_csv('Input_TimeAligned.csv',header = None)
data_input.columns = ['i','q']
data_input.describe()
data_input.head()

data_input_arr = data_input.to_numpy()
print('Input data as an array:',data_input_arr)
print(len(data_input_arr))
print()

data_output = pd.read_csv('Output_TimeAligned.csv',header = None)
data_output.columns = ['i','q']
data_output.describe()
data_output.head()

data_output_arr = data_output.to_numpy()
print('Output data as an array:',data_output_arr)
print(len(data_output_arr))
print()

data_input_tr = data_input_arr[0:122879,:]
data_output_tr = data_output_arr[0:122879,:]

data_input_test = data_input_arr[122880:491519,:]
data_output_test = data_output_arr[122880:491519,:]

X_train = data_input_tr
y_train = data_output_tr

X_test = data_input_test
y_test = data_output_test

# Define the model architecture.
start = time.time()
model = keras.Sequential([
  keras.layers.InputLayer(input_shape = (2,)),
  keras.layers.Dense(units = 128, activation = 'tanh', name = 'layer_1'),
  keras.layers.Dense(units = 256, activation = 'tanh', name = 'layer_2'),
  keras.layers.Dense(units = 512, activation = 'tanh', name = 'layer_3'),
  keras.layers.Dense(units = 256, activation = 'tanh', name = 'layer_4'),
  keras.layers.Dense(units = 128, activation = 'tanh', name = 'layer_5'),
  keras.layers.Dense(units = 2, activation = 'tanh', name = 'output_layer'),
])

model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['Accuracy'])
end = time.time()
print('Time taken to compile the model is:', end - start)
print()

print(model.summary())
print()

start = time.time()
model.fit(X_train, y_train, epochs = 3, batch_size = 32)    
end = time.time()
print('Time taken to train the model is:', end - start)
print()

start = time.time()
y_hat = model.predict(X_test)
end = time.time()
print('Time taken to test the model is:', end - start)
print('The predicted output is:', y_hat)
print()

start = time.time()
model.evaluate(X_test, y_test)
end = time.time()
print('Time taken to evaluate the model is:', end - start)
print()
</code></pre>
<p>I tried pruning as shown below:</p>
<pre><code>prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude

# Define model for pruning.
pruning_params = {
      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,
                                                               final_sparsity=0.50,
                                                               begin_step=0,
                                                               end_step=1000)
}

model_for_pruning = prune_low_magnitude(model, **pruning_params)

# `prune_low_magnitude` requires a recompile.
model_for_pruning.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model_for_pruning.summary()

start = time.time()
model.compile(
    loss=&quot;mse&quot;,
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
)
end = time.time()
print('Time taken to compile the pruned model is:', end - start)
print()

print(model.summary())
print()

start = time.time()
model.fit(
    X_train, 
    y_train, 
    epochs=3, 
    batch_size=32, 
    # callbacks= pruning_callback, 
    # verbose=1
)
end = time.time()
print('Time taken to train the pruned model is:', end - start)

start = time.time()
y_hat = model.predict(X_test)
end = time.time()
print('Time taken to test the pruned model is:', end - start)
print('The predicted output is:', y_hat)
print()

start = time.time()
model.evaluate(X_test, y_test)
end = time.time()
print('Time taken to evaluate the pruned model is:', end - start)
print()
</code></pre>
<p>This is the error I am receiving:</p>
<pre><code>ValueError: `prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Sequential.
</code></pre>
<p>What am I doing wrong here? I am not that proficient in programming. I'd appreciate any help!</p>
<p>Also, am I better off doing building my model in PyTorch? Are the optimization tools there any better? Thanks again.</p>
","1","Question"
"78852192","","<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map=&quot;cuda:3&quot;,
)
</code></pre>
<p>There are many GPUs on the server, but I can only use two of them. How should I configure device_map (or other parameters) so that the model runs on both GPUs?</p>
","2","Question"
"78853571","","<p>I made very simple python script which loads <strong>torchvision ResNet50</strong> model and tries to export to onnx file in two ways (<strong>torch.onnx.export</strong> and <strong>torch.onnx.dynamo_export</strong>)</p>
<pre><code>import torch
import torch.onnx

import torchvision

torch_model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2( weights='DEFAULT')
torch_model.eval()
torch_input = torch.randn(1, 3, 32, 32)

is_dynamo_export = False

if (is_dynamo_export):
    onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)
    onnx_program.save(&quot;onnx_dynamo_export_ResNET50.onnx&quot;)        
else:
    torch.onnx.export(torch_model,               # model being run
                      torch_input,                         # model input (or a tuple for multiple inputs)
                      &quot;onnx_export_ResNET50.onnx&quot;,   # where to save the model (can be a file or file-like object)
                      export_params=True,        # store the trained parameter weights inside the model file
                      opset_version=10,          # the ONNX version to export the model to
                      do_constant_folding=True,  # whether to execute constant folding for optimization
                      input_names = ['input'],   # the model's input names
                      output_names = ['output'], # the model's output names
                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes
                                    'output' : {0 : 'batch_size'}})  
</code></pre>
<p>The errors were appeared:</p>
<pre><code>File &quot;C:\tools\Python311\Lib\site-packages\torch\onnx\_internal\exporter.py&quot;, line 1439, in dynamo_export
  raise OnnxExporterError(

torch.onnx.OnnxExporterError: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues
</code></pre>
<pre><code>torch.onnx.errors.SymbolicValueError: Unsupported: ONNX export of Pad in opset 9. The sizes of the padding must be constant. Please try opset version 11. [Caused by the value '535 defined in (%535 : int[] = prim::ListConstruct(%405, %534, %405, %533, %405, %532), scope: torchvision.models.detection.faster_rcnn.FasterRCNN::
</code></pre>
<p>Both methods works good with extremally simple models such as</p>
<pre><code>class MyModel(nn.Module):

    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
</code></pre>
","0","Question"
"78854257","","<p>I'm trying to use MMPose to find the keypoints in 3d space of individuals in a video. The code I'm using (which has worked before in 2d) is:</p>
<pre><code>from mmpose.apis import MMPoseInferencer
from pathlib import Path
import os

data_folder = Path(&quot;x/videos&quot;)
for filename in os.listdir(data_folder):
    if not filename.endswith('.mp4'):
        continue
    img_path = os.path.join(data_folder, filename)

    inferencer = MMPoseInferencer(pose3d=&quot;human3d&quot;)

    result_generator = inferencer(img_path, out_dir='output')
</code></pre>
<p>Whenever I try and access the results_generator (like <code>results = [result for result in result_generator]</code>), I get a segfault, which I guess is because there's nothing in result_generator. I would also like for there to be visualisations and data in the output folder, which is empty. Is there anything obvious I'm doing wrong?</p>
","-1","Question"
"78855135","","<p>I am working in Python 3.10, using a sentence-transformers model to encode/embed a list of text strings. I want to use sentence-transformer's <code>encode_multi_process</code> method to exploit my GPU. This is a very specific function that takes in a string, or a list of strings, and produces a numeric vector (or list of vectors). The function distributes the work among system CPUs and GPUs.</p>
<p>I also want to parallelize my custom chunking function <code>create_chunks</code>, which splits a raw text string into chunks that are small enough to fit into the model's constraints. So, for any given text input, it has to go through <code>create_chunks</code> before going through <code>encode_multi_process</code>. I'm pretty sure that using multiple CPU cores to parallelize this step is the way to go.</p>
<p>Right now I am considering using <code>multiprocessing</code> to apply <code>create_chunks</code> to my dataset, and then <code>encode_multi_process</code>, but this seems inefficient: the chunks that come out of <code>create_chunks</code> have to wait until the whole dataset is finished before moving on to <code>encode_multi_process</code>. Are there more efficient Python alternatives? I have to build my solution around <code>encode_multi_process</code>, which is the main difficulty.</p>
<p>I wish I could use Dask, but the language model is too big to fit into a Dask task graph.</p>
","1","Question"
"78857269","","<p>I'm trying to add metrics to this SummaryWriter, but it's not working.</p>
<p>I'm using the add_hparams() function of SummaryWriter, the details of which can be found here: <a href=""https://pytorch.org/docs/stable/tensorboard.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/tensorboard.html</a>.</p>
<p>I'm doing it this way:</p>
<pre class=""lang-py prettyprint-override""><code>writer = SummaryWriter(f'runs/lstm_experiment_final')

for e in tqdm(range(num_epochs)):
    tr_loss, tr_f1, tr_precision, tr_recall = training_loop(model, train_dataloader, loss_function, optimizer, e, writer)
    val_loss, val_f1, val_precision, val_recall = validation_loop(model, test_dataloader, loss_function, e, writer)

metric_dict = {'Loss/train': tr_loss, 'Loss/valid': val_loss,
                         'F1/train': tr_f1, 'F1/valid': val_f1,
                         'Precision/train': tr_precision, 'Precision/valid': val_precision,
                         'Recall/train': tr_recall, 'Recall/valid': val_recall}
writer.add_hparams(best_params, metric_dict, global_step=num_epochs-1)
writer.close()    
</code></pre>
<p>This is what's happening.</p>
<p><a href=""https://i.sstatic.net/lG0wv009.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>In other words, hyperparameters are indeed being logged on TensorBoard, but the metric values are not.</p>
<p>I hope someone has already seen my problem and knows how to fix this.
Thanks in advance.</p>
","0","Question"
"78858321","","<p>I have been trying to convert a dictionary into a dataframe but everytime i keep getting ValueError: All arrays must be of the same length. i Have checkde the length of each array and confirmed them to be the same but i am still getting the same error</p>
<pre><code>def metrics_from_pipes(pipes_dict):
     for name, pipeline in pipes_dict.items():
        
        pipeline.fit(X_train, y_train)
        y_pred_val = pipeline.predict(X_val)
        y_pred_train = pipeline.predict(X_train)


train_metrics = {
            'model':list(pipes_dict.keys()),
            'MAE':train_mae,
            'MAPE':train_mape,
            'RMSE':train_rmse,
            'RSquared':train_rsquared
        }
        
        train_metrics_data = pd.DataFrame(train_metrics)
        val_metrics = {
            'model':list(pipes_dict.keys()),
            'MAE':val_mae,
            'MAPE':val_mape,
            'RMSE':val_rmse,
            'RSquared':val_rsquared            
        }
        
        val_metrics_data = pd.DataFrame(val_metrics,)

        #Merging metrics from train and test set
        train_val_metrics = train_metrics_data.merge(val_metrics_data,
                                               on = 'Model',
                                               how = 'left',
                                               suffixes = ('_train', '_val'))
        
        # sorting columns 
        train_val_metrics = train_val_metrics.reindex(columns = ['Model',
                                                               'MAE_train',
                                                                'MAPE_train',
                                                                'RMSE_train',
                                                                'RSquared_train',
                                                                'MAE_val',
                                                                'MAPE_val',
                                                                'RMSE_val',
                                                                'RSquared_val'])
        
    
    
    return train_val_metrics.set_index('Model').transpose()

# get the metrics table
metrics_table = metrics_from_pipes(pipelines)
</code></pre>
<p>running this code gives this error</p>
<pre><code>ValueError                                Traceback (most recent call last)
Cell In[45], line 82
     80     return train_val_metrics.set_index('Model').transpose()
     81 # get the metrics table
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
     83 #print('Table 1: Base Models Metrics')
     84 #metrics_table.style.background_gradient(cmap = Blues)
     85 metrics_table

Cell In[45], line 50, in metrics_from_pipes(pipes_dict)
     41 # aggregate the performance metric lists into seperate dataframes
     42 train_metrics = {
     43     'model':list(pipes_dict.keys()),
     44     'MAE':train_mae,
   (...)
     47     'RSquared':train_rsquared
     48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
     51 val_metrics = {
     52     'model':list(pipes_dict.keys()),
     53     'MAE':val_mae,
   (...)
     56     'RSquared':val_rsquared            
     57 }
     59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: All arrays must be of the same length
</code></pre>
<p>when i checked for the result of the dictionary for both train_metrics and val metrics, i got this</p>
<pre><code>({'model': ['Linear Regression',
   'Random Forest Regressor',
   'Gradient Boost Regression',
   'Extra Tree Regressor'],
  'MAE': [829.1023412412194,
   288.33455697065233,
   712.9637267872279,
   0.0010629575741748962],
  'MAPE': [1.0302372135902111,
   0.20937541440883897,
   0.538244903316323,
   6.306697580961048e-07],
  'RMSE': [1120.5542708017374,
   416.48933196590013,
   1012.399201767692,
   0.05804079289490426],
  'RSquared': [0.5598288286601083,
   0.9391916010838417,
   0.6406981997919169,
   0.9999999988190745]},
 {'model': ['Linear Regression',
   'Random Forest Regressor',
   'Gradient Boost Regression',
   'Extra Tree Regressor'],
  'MAE': [855.9254413559535,
   802.5902302175274,
   772.3140648475379,
   839.9018341377154],
  'MAPE': [1.0395487579496652,
   0.5607987708065988,
   0.5438627253681279,
   0.5852285872937784],
  'RMSE': [1148.6549900167981,
   1158.8411708570625,
   1109.6145558003204,
   1223.23337689915],
  'RSquared': [0.5876710102285392,
   0.5803255834810521,
   0.6152231339508221,
   0.5323905190373128]})
</code></pre>
","-1","Question"
"78859343","","<p>I'm trying to confirm that my GPT-2 model is being trained from scratch, rather than using any pre-existing pre-trained weights. Here's my approach:</p>
<ol>
<li><strong>Load the pre-trained GPT-2 XL model</strong>: I load a pre-trained GPT-2 XL model using <code>AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)</code> and calculate the total L2 norm of the weights for this model.</li>
<li><strong>Initialize a new GPT-2 model from scratch</strong>: I then initialize a new GPT-2 model from scratch with a custom configuration using <code>GPT2Config</code>.</li>
<li><strong>Compare L2 norms</strong>: I calculate the L2 norm of the weights for both the pre-trained model and the freshly initialized model. My assumption is that the L2 norm of the scratch model should be much smaller compared to the pre-trained model if the scratch model is truly initialized from random weights.</li>
</ol>
<p>Here's the code snippet:</p>
<pre><code>import torch
from transformers import GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM

# Step 1: Load the pre-trained GPT-2 XL model
pretrained_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)

# Step 2: Calculate the L2 norm of the weights for the pre-trained model
pretrained_weight_norm = 0.0
for param in pretrained_model.parameters():
    pretrained_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2 norm of pre-trained model weights: {pretrained_weight_norm:.2f}&quot;)

# Step 3: Initialize a new GPT-2 model from scratch with custom configuration
config = GPT2Config(
    vocab_size=52000,  # Ensure this matches the tokenizer's vocabulary size
    n_ctx=1024,  # Context window size (number of tokens the model can see at once)
    bos_token_id=0,  # Begin-of-sequence token
    eos_token_id=1,  # End-of-sequence token
)
model = GPT2LMHeadModel(config)

# Step 4: Calculate the L2 norm of the weights for the freshly initialized model
scratch_weight_norm = 0.0
for param in model.parameters():
    scratch_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2 norm of model initialized from scratch: {scratch_weight_norm:.2f}&quot;)
</code></pre>
<p>Is this method a valid way to confirm that the model is being trained from scratch? Are there any potential issues or better ways to verify that the model has no pre-existing learned weights?</p>
<p>Looks right</p>
<pre><code>~/beyond-scale-language-data-diversity$ /opt/conda/envs/beyond_scale_div_coeff/bin/python /home/ubuntu/beyond-scale-language-data-diversity/playground/test_gpt2_pt_vs_reinit_scratch.py
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 689/689 [00:00&lt;00:00, 8.05MB/s]
model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 6.43G/6.43G [00:29&lt;00:00, 221MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00&lt;00:00, 1.03MB/s]
Total L2 norm of pre-trained model weights: 24542.74
Total L2 norm of model initialized from scratch: 1637.31
(beyond_scale_div_coeff)                                                        
</code></pre>
<p>cross: <a href=""https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905</a></p>
<p>ref: <a href=""https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18"" rel=""nofollow noreferrer"">https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18</a></p>
","0","Question"
"78860154","","<p>I'm using the YOLOv8 command-line interface (CLI) to run object detection on an image, but I'm getting unexpected results. The model seems to be detecting a bunch of random objects that aren't present in the image.</p>
<p>Here’s the command I'm using:
<code>yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'</code></p>
<p>When I run this, it produces a result similar to the one shown in the image below:</p>
<p><a href=""https://i.sstatic.net/pYe7pYfg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pYe7pYfg.jpg"" alt=""enter image description here"" /></a></p>
","0","Question"
"78860233","","<p>I'm trying to implement <a href=""https://arxiv.org/pdf/2005.13044"" rel=""nofollow noreferrer"">this paper</a> but I'm not following something in it.</p>
<p>It wants me to use ResNet50 to extract features from an image but tells me the extracted features will be of dimension [w, h, f]. Everything I'm seeing with ResNet50, though, is giving me back a tensor of [f] (as in, it turns my whole image into features and not my pixels into features)</p>
<p>Am I reading this wrong or do I just not understand what I'm supposed to be doing with ResNet50?</p>
<p>Relevant quotes from paper:
&quot;We obtain an intermediate visual feature representation Fc of size f. We use the ResNet50 [26] as our backbone convolutional architecture.&quot;</p>
<p>&quot;In a first step, the three-dimensional feature Fc is reshaped into a two-dimensional feature by keeping its width, i.e. obtaining a feature shape (f × h, w).&quot;</p>
","-1","Question"
"78861705","","<p>I was training a CNN using ImageDataGenerator and encountered this problem where after the Second Epoch an Attribute error is raised.</p>
<p>The model is as follows</p>
<h1>Model</h1>
<pre><code>import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop

def create_model():
  '''Creates a CNN with 4 convolutional layers'''
  model = tf.keras.models.Sequential([
      tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
      tf.keras.layers.MaxPooling2D(2, 2),
      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(512, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])

  model.compile(loss='binary_crossentropy',
                optimizer=RMSprop(learning_rate=1e-4),
                metrics=['accuracy'])
  
  return model


from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=20,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary',
        shuffle= False)


EPOCHS = 20

model = create_model()

history = model.fit(
      train_generator,
      steps_per_epoch=100,  # 2000 images = batch_size * steps
      epochs=EPOCHS,
      validation_data=validation_generator,
      validation_steps=50,  # 1000 images = batch_size * steps
      verbose=2)
</code></pre>
<h1>Output</h1>
<pre><code>AttributeError                            Traceback (most recent call last)
Cell In[15], line 8
      5 model = create_model()
      7 # Train the model
----&gt; 8 history = model.fit(
      9       train_generator,
     10       steps_per_epoch=100,  # 2000 images = batch_size * steps
     11       epochs=EPOCHS,
     12       validation_data=validation_generator,
     13       validation_steps=50,  # 1000 images = batch_size * steps
     14       verbose=2)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py:354, in TensorFlowTrainer.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)
    333         self._eval_epoch_iterator = TFEpochIterator(
    334             x=val_x,
    335             y=val_y,
...
    355     }
    356     epoch_logs.update(val_logs)
    358 callbacks.on_epoch_end(epoch, epoch_logs)

AttributeError: 'NoneType' object has no attribute 'items'
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
</code></pre>
<p>I have tried the following debugging steps:</p>
<ol>
<li>upgrading Tensorflow and Keras</li>
<li>trying a simpler Neural Network to see if that has the same issue but it worked fine.</li>
<li>Instead of passing validation_generator to the model.fit(), doing it manually using numpy but that also didn't work out since for it the accuracy and error on training data was coming 0, for the even epochs only.</li>
</ol>
<p>Have also checked the validation data it is properly loaded.</p>
<p>Python Version: 3.11.9
Tensorflow Version: 2.17.0
Keras Version: 3.4.1</p>
","0","Question"
"78862449","","<p>I have set up a very basic environment in gymnasium, consisting on an nxn matrix, populated by 0s and 1s. The neural network now should output a vector, pointing to one specific entry of the matrix.</p>
<p>For now, this should simply be an entry with 0, which will then changed to 1: essentially the ai populates the matrix with 1s via reinforcement learning. Later, I want it to find approate positions to place shapes into the matrix (a bit like a Tetris game only the blocks are not falling).</p>
<p>Anyways, I have up to now used DQN models, which seem not to be really suitable here. Could anyone please point me to a better approach for this input/output setup?</p>
","-1","Question"
"78862511","","<p>I am using the following code to summarize my text in Python. The code is being run in Jupyter Notebook. I have already install sumy using pip command.</p>
<pre><code>pip install sumy nltk
python -m nltk.downloader punkt
</code></pre>
<pre><code>from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from io import StringIO

# Define the text to be summarized
text = &quot;&quot;&quot;
Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and respond to human language in a way that is both valuable and meaningful.
NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand. When the text has been provided, a computer can take many different approaches to process it. The algorithm can be a rule-based or a machine learning-based approach.
&quot;&quot;&quot;

# Use StringIO to simulate a file-like object
text_io = StringIO(text)

# Parse the text
parser = PlaintextParser.from_file(text_io, Tokenizer(&quot;english&quot;))

# Initialize the LSA summarizer
summarizer = LsaSummarizer()

# Generate the summary (you can adjust the number of sentences)
summary = summarizer(parser.document, sentences_count=2)

# Print the summary
for sentence in summary:
    print(sentence) 
</code></pre>
<p>When I run the program I get the following error:</p>
<pre><code>ame)
    662 def find_class(self, module, name):
    663     # Forbid every function
--&gt; 664     raise pickle.UnpicklingError(f&quot;global '{module}.{name}' is forbidden&quot;)

UnpicklingError: global 'copy_reg._reconstructor' is forbidden 
</code></pre>
<p>Any ideas!</p>
","1","Question"
"78863529","","<pre><code>import torch
from PIL import Image
import numpy as np
from effdet import get_efficientdet_config, EfficientDet


config = get_efficientdet_config('tf_efficientdet_d0')
model = EfficientDet(config, pretrained_backbone=True)
model.eval()
</code></pre>
<p>when I run this I am getting the error</p>
<pre><code>Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
</code></pre>
<p>I researched some bit and got to know that this is because of timm builder but didn't find any solutions. How to fix this?</p>
<p>I wanted to load the efficientdet weights but it resulted in an unexpected keys error</p>
","0","Question"
"78863903","","<p>I am competing in a Kaggle competiton (<a href=""https://www.kaggle.com/competitions/playground-series-s4e8"" rel=""nofollow noreferrer"">https://www.kaggle.com/competitions/playground-series-s4e8</a>) where we have to predict whether a mushroom is poisonous or not based on the data provided.
The issue I am facing is that my models perform well inside the training and validation sets just fine (around 98-99% accuracy) but they fall apart when I actually submit the final predictions for the competition.</p>
<p>The best accuracy I got until now using the Random forest model was 52% and the rest of my submissions had substantially worse performances. Since the models are performing well inside the notebooks and data with labels,
I assumed that the issue is with the way I am handling data in general because I did not implement techniques like feature engineering and I am not sure if the way I converted categorical data to numeric data works fine or not.</p>
<p>And as mentioned before, I am using the Random Forest Model and/or XGBoost model and these two models are quite well known to be a lot less prone to overfitting than other models.</p>
<p>I also ran multiple iterations of multiple models to find the models with the best parameters (as evident from the code below) so that makes the problem of overfitting less likely.</p>
<p>Here is the code for my data handling:</p>
<pre><code>def dataType(array_like):
    return array_like.dtype.name
types = []
for i in range(df_shape[1]):
    category = dataType(df_train[df_train.columns.values[i]])
    if category == 'object':
        types.append(1)
    else:
        types.append(0)
print(len(types))
print(types)
for t in range(len(types)):
    if types[t] == 1:
        column = df_train.columns.values[t]
        encoder = LabelEncoder()
        df_train[column] = encoder.fit_transform(df_train[column])
print(df_train.head())
features_to_scale = ['cap-diameter', 'stem-height', 'stem-width']

scaler = StandardScaler()
df_train[features_to_scale] = scaler.fit_transform(df_train[features_to_scale])
# df_train = df_train.replace('NaN', 0)
X = df_train[['id', 'cap-diameter', 'cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-height', 'stem-width', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season']].to_numpy()
Y = df_train['class']
knn_imputer = KNNImputer(n_neighbors=3)
imputed_X = knn_imputer.fit_transform(X)
# X[np.isnan(X)] = 0
# Y[np.isnan(Y)] = 1
X_train, X_test, y_train, y_test = train_test_split(
    imputed_X, Y, test_size=0.2, random_state=42)

print(np.isnan(X).sum())
print(np.isnan(imputed_X).sum())
</code></pre>
<p>My code for XGBoost implementation and finding the model with the best hyperparameters:</p>
<pre><code> def objective(trial):
          param = {
         'max_depth': trial.suggest_int('max_depth', 2, 10),
         'learning_rate': trial.suggest_float('learning_rate', 0.01, 10.0, log=True),
         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
         'subsample': trial.suggest_float('subsample', 0.5, 1.0),
         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
         'gamma': trial.suggest_float('gamma', 0, 5),
         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
         'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
         'reg_lambda': trial.suggest_float('reg_lambda', 1, 5)
     }

     # Initialize the model with the suggested parameters
     model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', **param)

     # Train the model
     model.fit(X_train, y_train)

     # Predict on the test set
     y_pred = model.predict(X_test)


     accuracy = accuracy_score(y_test, y_pred)

     return accuracy
 study = optuna.create_study(direction='maximize')

 study.optimize(objective, n_trials=75)

 best_params = study.best_params

 best_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', **best_params)

 best_model.fit(X_train, y_train)

 y_pred = best_model.predict(X_test)

 accuracy = accuracy_score(y_test, y_pred)
 print(accuracy)
</code></pre>
<p>My code for Random Forest implementation and finding the model with the best hyperparameters:</p>
<pre><code>params = (
    (2, 10, 42),
    (5, 20, 0),
    (10, 30, 123),
    (15, 40, 1),
    (20, 50, 7),
    (2, 15, 99),
    (5, 25, 56),
    (10, 35, 78),
    (15, 45, 100),
    (20, 60, 202),
    (3, 12, 8),
    (6, 18, 15),
    (12, 28, 30),
    (18, 38, 60),
    (25, 50, 90),
    (8, 20, 45),
    (14, 32, 67)
)

model = RandomForestClassifier(max_depth=35, min_samples_split=10, random_state=78)
model.fit(X_train, y_train)
predictions_train = model.predict(X_train)
predictions_test = model.predict(X_test)
accuracy_train = accuracy_score(y_train, predictions_train)
accuracy_test = accuracy_score(y_test, predictions_test)
print(accuracy_train)
print(accuracy_test)
random_forest_model = model

for min_samples_split, max_depth, random_state in params:
    model = RandomForestClassifier(min_samples_split=min_samples_split, max_depth=max_depth, random_state=random_state)
    model.fit(X_train, y_train)
    predictions_train = model.predict(X_train)
    predictions_test = model.predict(X_test)
    random_forest_models.append(model)
    accuracy_train = accuracy_score(y_train, predictions_train)
    accuracy_test = accuracy_score(y_test, predictions_test)
    random_forest_accuracies_train.append(accuracy_train)
    random_forest_accuracies_test.append(accuracy_test)
    print(&quot;Model trained&quot;)

print(random_forest_accuracies_train)
print(random_forest_accuracies_test)
</code></pre>
<p>I then save the model with the highest accuracy (with the least overfitting) and save it in my kaggle notebook.</p>
","-1","Question"
"78865593","","<p>I want to create an AI model that generates text. Specifically, BDD Gherkin cucumber scenarios and step definitions based of input of a user story.</p>
<p><a href=""https://i.sstatic.net/jtnrgVUF.png"" rel=""nofollow noreferrer"">User story with BDD Gherkin cucumber example</a></p>
<p>For example.</p>
<p>User story (Input): I want to add products to my shopping basket on an e-commerce website to make a purchase.</p>
<p>Output: Automatically create the test case scenarios and step definitions
Test case scenarios:</p>
<ul>
<li>Scenario 1: Validate that user can add one item to cart</li>
<li>Scenario 2: Validate that user can remove one item from cart</li>
</ul>
<p>Test case scenario 1:</p>
<ul>
<li>Given the user launch and login e-commerce application with  and </li>
<li>Then the user navigates to items page.</li>
<li>And the user selects and click on a .</li>
<li>And the user clicks the “Add to cart” button.</li>
<li>Then the user should navigate to the Shopping Cart page.</li>
<li>And user should validate  in Shopping Cart Page has been successfully added.</li>
</ul>
<p>Test case scenario 2:</p>
<ul>
<li>Given the user launch and login e-commerce application with  and </li>
<li>Then the user should navigate to the Shopping Cart page.</li>
<li>And the user finds  in shopping cart and click “Remove from cart” button.</li>
<li>Then user should validate that  in shopping has been successfully removed.</li>
</ul>
<p>I have created a sample dataset that contains user stories mapped to scenarios and step definitions.</p>
<p><a href=""https://i.sstatic.net/yrxKDwo0.png"" rel=""nofollow noreferrer"">Dataset</a></p>
<p>From my current understand, the logic is that: I want to train a model based of the dataset of existing user stories and scenarios. After the model has been trained, I want to input a user story, and the model should come up with a suitable scenario with step definitions.</p>
<p>I am new to machine learning and has only done some form of supervised learning, regression. From some research, I would need to use some NLP techniques to process the dataset. From then on, I am pretty lost. I've seen some people talking about using ChatGPT to train of the dataset or something.</p>
<p>What would be a good way to do this project.</p>
<p>Essentially, I want to find out how to train a model using text so that the model can receive text and output text.</p>
","-2","Question"
"78866202","","<p>I'm developing an algorithm using Yolo-nas , I prepared the dataset with labelImg . I'm using Python 3.10.11 to do this algorithm together with super-gradient supervision. The problem is the following: The algorithm loads the data but when plotting the image it shows that it cannot find the image in the directory, I carried out some tests with other algorithms and it can find the path to the directory. I suspect it's the super-gradient version (3.7.1)</p>
<p>The error starts when I have to plot my training data</p>
<pre><code>FileNotFoundError :dataset\\images\\train\\img1.png was not found. 
Please make sure that the dataset was downloaded and that the path is correct
</code></pre>
<p>note: the images in the dataset were pdfs and I converted them to png to be able to use them in the labelImg and identify the object classes</p>
<ul>
<li>I tried changing the directory</li>
<li>remade the dataset</li>
<li>I checked if another algorithm can search for the images and it does.</li>
</ul>
<pre><code>import torch
torch.__version__

from tqdm.notebook import tqdm
from super_gradients.training import dataloaders
from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val
from super_gradients.training import models
from super_gradients.training.losses import PPYoloELoss
from super_gradients.training.metrics import DetectionMetrics_050
from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback

dataset_params = {
    'data_dir': &quot;nf/dataset&quot;, 
    'train_images_dir': &quot;dataset/images/train&quot;,
    'train_labels_dir': &quot;dataset/labels/train&quot;,
    'val_images_dir': &quot;dataset/images/val&quot;,
    'val_labels_dir': &quot;dataset/labels/val&quot;,
    'classes': ['cabecalho', 'assinatura', 'rodape']
}

MODEL_ARCH = 'yolo_nas_l'
DEVICE = 'cuda' if torch.cuda.is_available() else &quot;cpu&quot;
BATCH_SIZE = 10 
MAX_EPOCHS = 12
CHECKPOINT_DIR = '\checkpoint'
EXPERIMENT_NAME = &quot;nf&quot;


dados_treino = coco_detection_yolo_format_train(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['train_images_dir'],
        'labels_dir': dataset_params['train_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size': BATCH_SIZE,
        'num_workers': 1
    }
)

val_dados = coco_detection_yolo_format_val(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['val_images_dir'],
        'labels_dir': dataset_params['val_labels_dir'],  
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size': BATCH_SIZE,
        'num_workers': 1
    }
)

dados_treino.dataset.transforms

dados_treino.dataset.plot()
</code></pre>
","0","Question"
"78867452","","<p>I  trained yolov8 nano to detect fish embryos swimming in a petri dish. There is only ever 1 embryo in a dish at any given time, so this is a fairly simple task and the model performs well (mAP50=0.994). The ultimate goal of my project is to have a piece of software that takes a video as input, and have it output metrics (x,y coords at each frame, swim distance, swim velocity, etc.) ONLY for the frames in the video where the embryo is swimming. For example, a video might be 200 frames, and approx 40 first frames, the embryo is not yet swimming, and then 140 frames of swimming, and then 20 frames of no swimming (fish has stopped). So, for this video I would want to have a function that extracts only the 140 relevant frames from a csv file containing info for all the frames in the video.</p>
<p>The main issue with using a hard-coded algorithm to do this is that the data is noisy, making the end of an embryo's swim pattern hard to detect. For example, a min velocity-per-frame figure (given that an embryo can swim a min of 1 pixel) is usually around 10mm/s. However, random variability in the model's predictions shift the bounding box's center by a few pixels even when the fish is still, so the noise is around 10-20mm/s. For this reason, I applied simple exponential smoothing to the velocity column to try and reduce noise:</p>
<pre><code>def simple_exponential_smoothing(data, alpha):
    &quot;&quot;&quot;
    Apply simple exponential smoothing to the data.

    Parameters:
    data (array-like): The input time series data.
    alpha (float): The smoothing factor (0 &lt; alpha &lt;= 1).

    Returns:
    np.ndarray: The smoothed time series data.
    &quot;&quot;&quot;
    result = [data[0]]  # First value is same as series
    for n in range(1, len(data)):
        result.append(alpha * data[n] + (1 - alpha) * result[n-1])
    return np.array(result)

</code></pre>
<p>My initial approach is to use a csv file (containing one video's predictions, one for each frame), and run a &quot;detector&quot; function on it. I attempted to use the following function to extract start and end frames so that I can trim the data into only the relevant frames for further calculations:</p>
<pre><code>def find_start_end_rows(df, velocity_column, filtered_velocity_column, frame_rate):
        &quot;&quot;&quot;
        Find the start and end row indices based on a more refined approach.

        Parameters:
        df (pd.DataFrame): The dataframe to analyze.
        velocity_column (str): The name of the velocity column to search.
        filtered_velocity_column (str): The name of the filtered velocity column.

        Returns:
        tuple: A tuple containing the start row index and the end row index.
        &quot;&quot;&quot;
        start_row = None
        end_row = None
        velocity_threshold = 20  # Minimum velocity to start swim
        filtered_velocity_threshold = 10  # Minimum filtered velocity to consider movement
        consistent_low_velocity_frames = 5  # Number of consecutive low-velocity frames to detect the end

        # Find the start row
        for i in range(len(df)):
            if df.loc[i, velocity_column] &gt;= velocity_threshold:
                start_row = i - 1
                break

        # If start_row is still None, it means no value &gt;= 20 was found
        if start_row is None:
            return (-1,-1)  # -1 indicates the function failed

        # Find the end row by checking for consistent low velocities after the start row
        low_velocity_count = 0
        for i in range(start_row + 2, len(df)):
            if df.loc[i, filtered_velocity_column] &lt; filtered_velocity_threshold:
                low_velocity_count += 1
                if low_velocity_count &gt;= consistent_low_velocity_frames:
                    end_row = i - consistent_low_velocity_frames
                    break
            else:
                low_velocity_count = 0

        # If end_row is still None, it means no consistent low-velocity frames were found
        if end_row is None:
            end_row = len(df) - 1

        return start_row, end_row
</code></pre>
<p>As we can see in the graph below, however, this function does not perform very well. The graph demonstrates the error in start frame prediction and end frame prediction using this function (comparing the function's outputs to the true start/end frames in those videos). It is crucial to the project that the variability we see in predicting start/end frames is at most 2-3 frames.</p>
<p><a href=""https://i.sstatic.net/CULWOBor.png"" rel=""nofollow noreferrer"">Plot showing find_start_end_frames error</a></p>
<p>What approach might be best to detecting start/end frames within a video? It would be great to solve this algorithmically instead of having to train a whole other ML model for this task, but I am open to any solutions people might think will work.</p>
","0","Question"
"78868439","","<p>I am trying to train a YoloV5 model with my custom data on my own computer, but I keep getting this error:</p>
<pre><code>train: weights=yolov5s.pt, cfg=models/yolov5s.yaml, data=data.yaml, hyp=data\hyps\hyp.scratch-low.yaml, epochs=300, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False
    github: skipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5
    YOLOv5  2024-7-15 Python-3.12.2 torch-2.3.1+cpu CPU
    hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
    TensorBoard: Start with 'tensorboard --logdir runs\train', view at http://localhost:6006/
    Traceback (most recent call last):
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;, line 986, in &lt;module&gt;
        main(opt)
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;, line 688, in main
        train(opt.hyp, opt, device, callbacks)
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;, line 180, in train
        loggers = Loggers(
                  ^^^^^^^^
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\utils\loggers\__init__.py&quot;, line 121, in __init__
        self.tb = SummaryWriter(str(s))
                  ^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;, line 249, in __init__
        self._get_file_writer()
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;, line 281, in _get_file_writer
        self.file_writer = FileWriter(
                           ^^^^^^^^^^^
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;, line 75, in __init__
        self.event_writer = EventFileWriter(**
                            ^^^^^^^^^^^^^^^^
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorboard\summary\writer\event_file_writer.py&quot;, line 72, in __init__
        tf.io.gfile.makedirs(logdir)
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\lib\io\file_io.py&quot;, line 513, in recursive_create_dir_v2
        _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))
    tensorflow.python.framework.errors_impl.FailedPreconditionError: runs\train\exp10 is not a directory
</code></pre>
<p>I have tried training my model from a pre-trained one (as the yolov5 docs recommend), like this:
<code>python train.py --img 640 --batch 32 --epochs 300 --data data.yaml --weights yolov5s.pt</code>
or from scratch, like this:
<code>python train.py --img 640 --batch 32 --epochs 300 --data data.yaml --cfg models/yolov5s.yaml</code></p>
<p>I have also seen other issues, like issue #12008 on GitHub and this issue from Stack Overflow <a href=""https://stackoverflow.com/questions/76934905/tensorflow-python-framework-errors-impl-failedpreconditionerror-runs-train-exp3"">tensorflow.python.framework.errors_impl.FailedPreconditionError: runs\train\exp3 is not a directory</a>, but havent found any solution</p>
","-1","Question"
"78869863","","<p>I have a simple ML classification problem. I have 8 folder each one represent class so I have first load these images from folders and assign labels and then save it as csv file (code in below)</p>
<pre><code>def load_images_from_folder(root_folder):
 image_paths = []
 images = []
 labels = []
    for label in os.listdir(root_folder):
        label_path = os.path.join(root_folder, label)
        if os.path.isdir(label_path):
            for filename in os.listdir(label_path):
                img_path = os.path.join(label_path, filename)
                if os.path.isfile(img_path) and (filename.endswith(&quot;.jpg&quot;):
                img = Image.open(img_path)
                img = img.resize((128, 128))
                img_array = np.array(img)
                image_paths.append(img_path)
                images.append(img_array)
                labels.append(label)
 return image_paths, images, labels
if __name__ == &quot;__main__&quot;:
root_folder_path = &quot;./Datasets_1&quot;
image_paths, images, labels = load_images_from_folder(root_folder_path)
</code></pre>
<p>I then convert images and labels to DataFrame and load it</p>
<pre><code>data = {&quot;Images&quot;: image_paths, &quot;Labels&quot;: labels}
df = pd.DataFrame(data)
df.to_csv(&quot;original_data.csv&quot;, index=False)
csv_file = &quot;original_data.csv&quot;
df = pd.read_csv(csv_file)
</code></pre>
<p>I'm also add a new column 'Encoded_Labels' to the DataFrame with the encoded labels and convert 'Encoded_Labels' column to integers</p>
<pre><code>df['Encoded_Labels'] = encoded_labels
df['Encoded_Labels'] = df['Encoded_Labels'].astype(int)
</code></pre>
<p>Finally I have split the dataset into training and testing sets and preprocess images for training</p>
<pre><code>train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
def load_and_preprocess_images(file_paths, target_size=(128, 128)):
    images = []
    for file_path in file_paths:
        img = Image.open(file_path)
        img = img.resize(target_size)
        img_array = np.array(img) / 255.0  # Normalize pixel values
        images.append(img_array)
    return np.array(images)

X_train = load_and_preprocess_images(train_df['Images'].values)
y_train = train_df['Encoded_Labels'].values
X_test = load_and_preprocess_images(test_df['Images'].values)
y_test = test_df['Encoded_Labels'].values**your text**
</code></pre>
<p>And the output shape of X_train is</p>
<pre><code>(20624, 128, 128, 3)
</code></pre>
<p>For this point I have no problem and I can use it with DL models with no problem but when try to use ML models such as KNN, SVM, DT, etc. For examples codes in below</p>
<pre><code>from sklearn.svm import SVC
svc = SVC(kernel='linear',gamma='auto')
svc.fit(X_train, y_train)`
</code></pre>
<p>or</p>
<pre><code>knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
print(&quot;Accuracy of KNN Classifier : %.2f&quot; % (accuracy*100))
</code></pre>
<p>I got this error</p>
<pre><code>ValueError: Found array with dim 4. SVC expected &lt;= 2.
</code></pre>
<p>How to fix this error?</p>
","3","Question"
"78870012","","<p>Can anyone help me understand the implementation of <code>nn.Bilinear</code>
As per the documentation, this function implements y = x<sub>1</sub><sup>T</sup> * A * x<sub>2</sub>
taking <code>x1 = (100,20)</code> , <code>x2 = (100,30')</code> , assuming <code>output_features = 50</code>. The  matrix <code>A</code> has dimensions of <code>[50,20,30]</code>.
I am finding it difficult how these matrices are multiplied to get the <code>output = [100,50]</code></p>
<p>Based on the size of x<sub>1</sub>,x<sub>2</sub> and <code>A</code> matrix, the multiplication seems incompatible as per  y = x<sub>1</sub><sup>T</sup> * A * x<sub>2</sub> . What am I missing here?</p>
","0","Question"
"78872766","","<p>I'm trying to construct a simple neural network which gets a 2D Matrix (16x3) and outputs a single value, here is how I am trying to construct that network;</p>
<pre><code>def GenerateModel():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.InputLayer((16,3)))
    model.add(tf.keras.layers.Dense(16*16, input_shape=(16,3)))
    model.add(tf.keras.layers.Dense(4*4))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(1))

    return model
</code></pre>
<p>Results of model.summary() is as follows;</p>
<pre><code>_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_11 (Dense)            (None, 16, 256)           1024      
                                                                 
 dense_12 (Dense)            (None, 16, 16)            4112      
                                                                 
 flatten_1 (Flatten)         (None, 256)               0         
                                                                 
 dense_13 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 5,393
Trainable params: 5,393
Non-trainable params: 0
</code></pre>
<p>So when I try to create a model using GenerateModel and pass it the following matrix I get an error.</p>
<pre><code>[[15, 81, -25.169450961198],
 [53, 108, -36.4360540112101],
 [73, 84, -40.6695085658194],
 [0, 69, -20.7084454809044],
 [35, 97, -31.3954623571617],
 [117, 102, -44.2065629437328],
 [48, 68, -35.7085340456925],
 [17, 59, -23.1078535464318],
 [64, 24, -24.2111029108488],
 [101, 2, -25.97821118996],
 [57, 35, -23.7173409519286],
 [117, 101, -44.1413580763786],
 [88, 10, -25.4001185503816],
 [11, 14, -22.0778253042297],
 [46, 50, -24.7021623105999],
 [0, 0, -1]]
</code></pre>
<p>The error is;</p>
<pre><code>ValueError: Layer &quot;sequential_5&quot; expects 1 input(s), but it received 48 input tensors. Inputs received: [&lt;tf.Tensor: shape=(), dtype=int32, numpy=15&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=81&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-25.169450961198&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=53&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=108&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-36.4360540112101&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=73&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=84&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-40.6695085658194&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=69&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-20.7084454809044&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=35&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=97&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-31.3954623571617&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=117&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=102&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-44.2065629437328&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=48&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=68&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-35.7085340456925&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=17&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=59&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-23.1078535464318&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=64&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=24&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-24.2111029108488&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=101&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-25.97821118996&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=57&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=35&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-23.7173409519286&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=117&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=101&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-44.1413580763786&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=88&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=10&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-25.4001185503816&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=11&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=14&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-22.0778253042297&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=46&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=50&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=-24.7021623105999&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=-1&gt;]
</code></pre>
<p>How can I fix this?</p>
<p>I also tried to generate my network as follows;</p>
<pre><code>def GenerateModel():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(16*16, input_shape=(16,3)))
    model.add(tf.keras.layers.Dense(4*4))
    model.add(tf.keras.layers.Dense(1))

    return model
</code></pre>
<p>and also this;</p>
<pre><code>def GenerateModel():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.InputLayer((16,3)))
    model.add(tf.keras.layers.Dense(16*16))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(4*4))
    model.add(tf.keras.layers.Dense(1))

    return model
</code></pre>
<p>I tried to add a Flatten layer after different layers, did not solve my problem.</p>
<p>How can this be solved?</p>
","1","Question"
"78873685","","<p><img src=""https://i.sstatic.net/KnUmU7FG.png"" alt=""Customer Data"" /></p>
<p>I have encoded Gender column by OneHotEncoder. I want to apply log transformation to only Female[0] column but it is applying log to all the columns — why?</p>
<p>My code:</p>
<pre><code>import pandas as p
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer
import numpy as n

customer=p.read_csv('/content/Customers.csv')
customer.drop(['CustomerID','Profession','Family Size','Work Experience'],axis=1,inplace=True)
column=ColumnTransformer(
    [
        ('ohe_gender',OneHotEncoder(sparse=False,dtype=n.int32),[0])
    ],remainder='passthrough'
)
function=ColumnTransformer(
    [
        ('function',FunctionTransformer(n.log1p),[0])
    ],remainder='passthrough'
)
s=column.fit_transform(customer)
function.fit_transform(s)
</code></pre>
<p>Output:</p>
<pre><code>    array([[0.00000000e+00, 6.93147181e-01, 1.90000000e+01, 1.50000000e+04, 3.90000000e+01],
           [0.00000000e+00, 6.93147181e-01, 2.10000000e+01, 3.50000000e+04, 8.10000000e+01],
           [6.93147181e-01, 0.00000000e+00, 2.00000000e+01, 8.60000000e+04, 6.00000000e+00],
           ...,
           [0.00000000e+00, 6.93147181e-01, 8.70000000e+01, 9.09610000e+04, 1.40000000e+01],
           [0.00000000e+00, 6.93147181e-01, 7.70000000e+01, 1.82109000e+05, 4.00000000e+00],
           [0.00000000e+00, 6.93147181e-01, 9.00000000e+01, 1.10610000e+05, 5.20000000e+01]]
</code></pre>
<p>After encoding (OHE) before FunctionTransformer the output was</p>
<pre><code>array([[     0,      1,     19,  15000,     39],
       [     0,      1,     21,  35000,     81],
       [     1,      0,     20,  86000,      6],
       ...,
       [     0,      1,     87,  90961,     14],
       [     0,      1,     77, 182109,      4],
       [     0,      1,     90, 110610,     52]])
</code></pre>
<p>I do want to apply log transformation in the 0th index of the above array but as you can see in first output it is applying on all the values although I have specified [0] in column transformer, why? I expect the output with log of only [0] index.</p>
","1","Question"
"78875648","","<p>I'm fitting transfer learning models on Google Colab. However, I've encountered a warning message with the code</p>
<pre><code>Epoch 1/30
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: 
UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in 
its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. 
Do not pass these arguments to `fit()`, as they will be ignored.
  self._warn_if_super_not_called()
</code></pre>
<p>and after the first epoch, I'm receiving the following error:</p>
<pre><code>---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-23-962a870d4412&gt; in &lt;cell line: 16&gt;()
     14 # fit the model
     15 # Run the cell. It will take some time to execute
---&gt; 16 training_history = model_efficientnet.fit(
     17   training_set,
     18   validation_data=validate_set,
</code></pre>
<p>I've successfully fit six other transfer learning models without any issues, and their accuracies were satisfactory.</p>
<p>How to resolve this issue?</p>
<p>I would like to receive training accuracy and validation accuracy</p>
","1","Question"
"78881480","","<p>I am  trying to create a regression model to predict the closing price of bitcoin using the folloςing dataset: <a href=""https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv</a></p>
<p>It has over 600k records with 15 features (some that I created).</p>
<p>I have tried to train it on google colab and on my laptop multiple times. I even left it overnight but it is taking way too long.</p>
<p>Is there some way to speed it up?</p>
<p>Laptop Specs:</p>
<pre><code>CPU: Ryzen 7 5800H  
GPU: RTX 3050  
RAM: 16 gb
</code></pre>
<p>This is the training code:</p>
<pre><code>models = {
    'Linear Regression': {
        'model': LinearRegression(),
        'params': {}
    },
    'Ridge Regression': {
        'model': Ridge(random_state=42),
        'params': {'alpha': [0.01, 0.1, 1, 5, 10, 50, 100]}
    },
    'Lasso Regression': {
        'model': Lasso(random_state=42),
        'params': {'alpha': [0.001, 0.01, 0.1, 1, 10]}
    },
    'Decision Tree': {
        'model': DecisionTreeRegressor(random_state=42),
        'params': {'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}
    },
    'Random Forest': {
        'model': RandomForestRegressor(random_state=42),
        'params': {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}
    },
    'Support Vector Regression': {
        'model': SVR(),
        'params': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'epsilon': [0.01, 0.1, 1]}
    }
}

results = {}

for model_name, model_data in models.items():
    print(f&quot;Tuning {model_name}&quot;)
    grid_search = GridSearchCV(model_data['model'], model_data['params'], cv=5, scoring='neg_mean_squared_error', verbose=1)
    grid_search.fit(X_train, y_train)

    # Get the best model
    best_model = grid_search.best_estimator_

    # Predictions
    y_pred = best_model.predict(X_test)

    # Performance Metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results[model_name] = {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'R2': r2,
        'Best Model': best_model,
        'Best Params': grid_search.best_params_
    }
</code></pre>
","-2","Question"
"78884108","","<p>I am migrating from R to PySpark. I have a process that creates a regression tree that is currently built using R's <code>rpart</code> algorithm.</p>
<p>While configuring this in PySpark, I am unable to see an option to specify a custom
custom impurity function. I have a skewed dataset, and instead of using mean and variance/ standard deviation in the formula as criterion for impurity of a node, I want to use a metric more suited for my skewed data.
How can I define a custom impurity function in PySpark?</p>
<p>I've looked at the documentation for <a href=""https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-regression"" rel=""nofollow noreferrer"">Decision Tree Regression</a> and <a href=""https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html#pyspark.ml.regression.DecisionTreeRegressor.impurity"" rel=""nofollow noreferrer"">documentation for the <code>impurity</code> parameter</a>  only mentions support for <code>variance</code></p>
<blockquote>
<p>impurity = Param(parent='undefined', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance')</p>
</blockquote>
<p>Is there any workaround to define a custom impurity function?</p>
","1","Question"
"78885044","","<p>I have a Jupyter Notebook. I installed ntlk using the following line of code:</p>
<pre><code>!pip install nltk
</code></pre>
<p>I get the following output:</p>
<pre><code>Requirement already satisfied: nltk in ./env/lib/python3.12/site-packages (3.9)
Requirement already satisfied: click in ./env/lib/python3.12/site-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in ./env/lib/python3.12/site-packages (from nltk) (1.4.0)
Requirement already satisfied: regex&gt;=2021.8.3 in ./env/lib/python3.12/site-packages (from nltk) (2024.7.24)
Requirement already satisfied: tqdm in ./env/lib/python3.12/site-packages (from nltk)
</code></pre>
<p>Now when I import nltk using</p>
<pre><code>import nltk  
</code></pre>
<p>I get the following error:</p>
<pre><code>LookupError                               Traceback (most recent call last)
File ~/Desktop/machine-learning/env/lib/python3.12/site-packages/nltk/corpus/util.py:84, in LazyCorpusLoader.__load(self)
     83 try:
---&gt; 84     root = nltk.data.find(f&quot;{self.subdir}/{zip_name}&quot;)
     85 except LookupError:

File ~/Desktop/machine-learning/env/lib/python3.12/site-packages/nltk/data.py:579, in find(resource_name, paths)
    578 resource_not_found = f&quot;\n{sep}\n{msg}\n{sep}\n&quot;
--&gt; 579 raise LookupError(resource_not_found)

LookupError: 
**********************************************************************
  Resource wordnet not found.
  Please use the NLTK Downloader to obtain the resource:
</code></pre>
<p>What am I missing?</p>
","0","Question"
"78885395","","<p>I was assigned to implement a machine learning model based on a paper I've read.
This paper implements a multi-task learning model for attribute classification (labelled images are the model input, by labelled I mean attributes annotations, 40 for each image).</p>
<p>It is a multi-task learning model because there is a shared dense layer right after the model input layer and 40 attribute branches, each one of them with their own loss function (binary cross-entropy for all of them) and own sigmoid activation function (on the last layer, to predict if each one of the 40 attributes are or not at the image).
After a lot of hard work it finally started returning probabilities on all sigmoid function on all branches, but the probabilities only of val_accuracy were wrong: The val_loss and the loss (training loss) were getting smaller and smaller, and the acc (training accuracy) was also in a normal range of probabilities values, except for the val_accuracy which were always the same value or it complement.</p>
<p>For example (5 epochs just for example):
The Accuracy for one attribute prediction at one of the 40 branches:</p>
<pre><code>5_o_Clock_Shadow_Accuracy
0   0.823665
1   0.891178
2   0.891178
3   0.891178
</code></pre>
<p>The loss for the same attribute:</p>
<pre><code>    5_o_Clock_Shadow_loss
0   0.921046
1   0.701494
2   0.913597
3   0.765397
4   0.894950
</code></pre>
<p>The val_loss:</p>
<pre><code>val_5_o_Clock_Shadow_loss
0   730232.750000
1   300412.500000
2   376215.843750
3   0.747685
4   1.607191
</code></pre>
<p>And finally the val_Accuracy:</p>
<pre><code>val_5_o_Clock_Shadow_Accuracy
0   0.882382
1   0.117618
2   0.882382
3   0.882382
4   0.882382
</code></pre>
<p>My model:</p>
<pre><code>def subnet(shared_layers_output, i):
    
    att_branch = Dense(512, name='dense_'+str(i)+'_1')(shared_layers_output)
    att_branch = ReLU()(att_branch)
    att_branch = BatchNormalization()(att_branch)
    att_branch = Dropout(0.5)(att_branch)

    att_branch = Dense(512, name='dense_'+str(i)+'_2')(att_branch)
    att_branch = ReLU()(att_branch)
    att_branch = BatchNormalization()(att_branch)
    att_branch = Dropout(0.5)(att_branch)

    branch_output = Dense(1, name=att_list[i], activation='sigmoid')(att_branch)

    return branch_output

def multi_task_model():

    #Input
    input_layer = Input(shape=(512,), name='input_layer')
    
    #Camada compartilhada (1 única)
    shared_x = Dense(512, name='shared_dense_layer')(input_layer)
    shared_x = ReLU()(shared_x)
    shared_x = BatchNormalization()(shared_x)
    shared_x = Dropout(0.5)(shared_x)

    branch_outputs = list()
    for i in range(40):
        branch_outputs.append(subnet(shared_x, i))

    model = Model(input_layer, branch_outputs, name='model')

    return model

</code></pre>
<pre><code>Train and test input shape: (n_samples, 512)
Train and test labels input shape: (40, n_samples)
Learning rate: 1e-03
</code></pre>
<p>5_o_Clock_Shadow loss, val_loss, acc and val_acc over 5 epochs</p>
<pre><code>    loss    val_loss    acc val_acc
0   0.422385    1.949578    0.864272    0.8873
1   0.354094    151.987991  0.888797    0.1127
2   0.354356    58.867992   0.888797    0.1127
3   0.352891    94.257980   0.888797    0.1127
4   0.353390    10.997763   0.888797    0.1127
</code></pre>
","0","Question"
"78885552","","<p>I am defining a pipeline using Jenkins. I am working on a text summarizer project and using jenkins for CICD. After triggering the pipeline,in CD stage I am getting error as below:</p>
<pre><code>ssh -o StrictHostKeyChecking=no -l ubuntu 3.226.221.21 'cd /home/ubuntu/ &amp;&amp; wget https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml &amp;&amp; export IMAGE_NAME=${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/textsum:latest &amp;&amp; aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com &amp;&amp; docker compose up -d '
Shell Script
1.6 sec
+ ssh -o StrictHostKeyChecking=no -l ubuntu 3.226.221.21 cd /home/ubuntu/ &amp;&amp; wget https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml &amp;&amp; export IMAGE_NAME=****:latest &amp;&amp; aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ****.dkr.ecr.us-east-1.amazonaws.com &amp;&amp; docker compose up -d 
--2024-08-18 19:19:08--  https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 95 [text/plain]
Saving to: ‘docker-compose.yml.10’
     0K                                                       100% 3.77M=0s
2024-08-18 19:19:08 (3.77 MB/s) - ‘docker-compose.yml.10’ saved [95/95]
WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credential-stores
Login Succeeded
yaml: line 229: mapping values are not allowed in this context
script returned exit code 15
Jenkins 2.462.1
</code></pre>
<p>I don't have any clue for solving this issue:
I am sharing my github repo containing jenkins file.
https://github.com/mishraatharva/textsummarization</p>
<p>How to resolve this?</p>
","0","Question"
"78890391","","<p>I have an table and I want pass the features = &quot;train_1, train_2, train_3, train_4&quot; and target_result = result_cor.</p>
<p>I want know when the values are = &quot;1 or 2&quot; in my predicion:</p>
<p><a href=""https://i.sstatic.net/nSaJ5NDP.png"" rel=""nofollow noreferrer"">follow my data</a></p>
<p>follow my code:</p>
<pre><code>from enum import auto
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report 
from sklearn import svm
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import seaborn as sns

sheet_id = '1CfnVwuqysTYNPKLVhgjJ44Af8VDcdN1l'
dados = pd.read_excel(f'https://docs.google.com/spreadsheets/export?id={sheet_id}&amp;format=xlsx')
dados.head()

# Dados para realizar o aprendizado e ver quanto vai prever corretamente
x = dados[['train_1','train_2','train_3','train_4']]

#  Gabarito ou resultados ja corretos
y = dados[['result_cor']]

#  Efetuo a separação dos treinos de x e y e testes de x e y
treino_x, teste_x, treino_y, teste_y = train_test_split(x,y,test_size=0.33)

# Tipo de modelo
modelo = DecisionTreeClassifier()

#  Efetuo o treinamento
modelo.fit(x,np.ravel(y,order=&quot;c&quot;))

# Predicion new valor 
model_predict = [0,1,0,1]
treino_x[:1] = model_predict
model_predict = treino_x[:1]

result_cor = [1]
treino_y[:1] = result_cor 
result_cor = treino_y[:1]

# predicion the new model
previsoes = modelo.predict(model_predict)

#  Check acuracy 
accuracy = accuracy_score(result_cor,previsoes) * 100

print(f'A acuracia é: {round(accuracy,2)}')

</code></pre>
<p>but result is always 100.0 % or 0.0. I need to know the percent of times my result_cor appears in model_predict of my model trained model</p>
<p>please help</p>
","-1","Question"
"78891901","","<p>I'm working on a Deep Learning project where I'm adding, on a pre-trained Wide ResNet a couple of linear layers after that, and another technique that I'm using.</p>
<p>My question is if I construct the Network where I add the layers and the techniques that I'm using to the broad resnet as a ModuleList, then I load the pre-trained weights on the entire model with the additions. Does that raise an error in the loading procedure?</p>
<p>Or does it knows that it should only load the weights partially (i.e. only on the reset).</p>
<p>example code:</p>
<pre class=""lang-py prettyprint-override""><code>model = get_model(backbone) # Load ResNet
model = ConstructNewModel(model) # Add layers
load_weights(model, pretrained_path) # load ResNet Weights
model = model.to(&quot;cuda&quot;) # Add to GPU
</code></pre>
<p>or should it look more like this?</p>
<pre class=""lang-py prettyprint-override""><code>model = get_model(backbone) # Load ResNet
load_weights(model, pretrained_path) # load ResNet Weights
model = ConstructNewModel(model) # Add layers
model = model.to(&quot;cuda&quot;) # Add to GPU
</code></pre>
<p>In other words, how does loading the weights in PyTorch work using torch.load?</p>
","0","Question"
"78893376","","<p>I am stumped on an issue with Python/Sci-Kit Learn/Pipelines. I am receiving an error that the shape of the data as it passes through the pipeline is not what is expected.</p>
<p>Specific error:</p>
<pre><code>blocks[0,:] has incompatible row dimensions. Got blocks[0,6].shape[0] == 4, expected 794.
</code></pre>
<p>What I am sending into TFIDF</p>
<pre><code>X['Subject'].head()
5          FW: Customer PO 345 \\ HAC 73054 and 7345
8                            Insured return request, o# 35693
10                             Issue with a new Feature - QAR
13                                       FW: ABC / TSS PO catchup
15    WTM request - 1deaSe sales orders for CDE PO TSSe9-1r9
</code></pre>
<p>The relevant code is below:</p>
<pre><code>#FeatureSelector selects a list of columns from a data
#frame and returns them to a pipeline step for processing
class FeatureSelector(BaseEstimator, TransformerMixin):
    def __init__(self, keys, description = &quot;&quot;):
        self.keys = keys
        self.description = description
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X, y=None):
        print(&quot;Keys out&quot;, self.keys)
        return X[self.keys]

# Custom transformer to help with debugging. 
class Debugger(BaseEstimator, TransformerMixin):

    def __init__(self, stepName = &quot;&quot;):
        self.stepName = stepName

    def transform(self, data): 
        print(&quot;Step Name&quot;, self.stepName)
        print(&quot;Contents of data&quot;, data.columns)
        print(&quot;Shape of Pre-processed Data:&quot;, data.shape)
        #print(pd.DataFrame(data).head())
        return data

    def fit(self, data, y=None, **fit_params):
        # No need to fit anything, because this is not an actual  transformation. 
        return self


y = df['Assigned To']
X = df

# Construct Pipelines
ohe_pipe = Pipeline([
    (&quot;feature_selector&quot;, FeatureSelector(df.select_dtypes(exclude=['int64','object']).columns.to_list())),
    ('feature selector debugger', Debugger()),
    (&quot;ohe&quot;, OneHotEncoder()),
    ('ohe debugger', Debugger())],
    verbose = True)
text_pipe = Pipeline([
    (&quot;feature_selector&quot;, FeatureSelector(df.select_dtypes(include='object').columns.to_list())),
    ('feature selector debugger', Debugger()),
    (&quot;tfidf_vectorizer&quot;, TfidfVectorizer()),
    ('tfidf debugger', Debugger())], 
    verbose = True)

knn_pipe = Pipeline([
    (&quot;feature_union&quot;, FeatureUnion([
        (&quot;ohe_pipe&quot;, ohe_pipe),
        (&quot;text_pipe&quot;, text_pipe)
        ])),
    (&quot;classifier&quot;, KNeighborsClassifier())
    ])

knn_grid = GridSearchCV(
                     estimator=knn_pipe,
                     param_grid = {'classifier__n_neighbors': [x for x in range(5, 20, 1) if x % 2 != 0],
                                   'classifier__weights': ['distance'],
                                   'classifier__leaf_size': range(20,40,1),
                                   'feature_union__text_pipe__tfidf_vectorizer__min_df': [.01, .02, .03, .04, .05],
                                   'feature_union__text_pipe__tfidf_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)], 
                                   'feature_union__text_pipe__tfidf_vectorizer__stop_words': ['english'],
                                   'feature_union__text_pipe__tfidf_vectorizer__lowercase': [True],
                                   'feature_union__ohe_pipe__ohe__sparse_output': [False]
                                   },
                     scoring = {'accuracy': make_scorer(accuracy_score)
                                # ,
                                # 'f1': make_scorer(f1_score),
                                # 'precision': make_scorer(precision_score),
                                # 'recall': make_scorer(recall_score),
                                # 'roc_auc': make_scorer(roc_auc_score)
                                },
                     cv = 3, 
                     refit = 'accuracy', 
                     error_score='raise')
knn_grid.fit(X, y)
</code></pre>
<p>Following my debug trace, you can see, in bold below, that the TFIDF step is returning a shape of (1,1) rather than (529, 1). If I run all these steps (including TFIDF) outside of a pipeline, TFIDF returns (529,1). I would like to use the pipeline for the Grid Search capabilities.</p>
<pre><code>Keys out ['Issue Classification', 'Application', 'Case Submitter']
[Pipeline] .. (step 1 of 4) Processing feature_selector, total=   0.0s
Step Name 
Shape of Pre-processed Data: (529, 3)
[Pipeline]  (step 2 of 4) Processing feature selector debugger, total=   0.0s
[Pipeline] ............... (step 3 of 4) Processing ohe, total=   0.0s
Step Name 
Shape of Pre-processed Data: (529, 66)
[Pipeline] ...... (step 4 of 4) Processing ohe debugger, total=   0.0s
Keys out ['Subject']
[Pipeline] .. (step 1 of 4) Processing feature_selector, total=   0.0s
Step Name 
Shape of Pre-processed Data: (529, 1)
[Pipeline]  (step 2 of 4) Processing feature selector debugger, total=   0.0s
[Pipeline] .. (step 3 of 4) Processing tfidf_vectorizer, total=   0.0s
Step Name 
**Shape of Pre-processed Data: (1, 1)
[Pipeline] .... (step 4 of 4) Processing tfidf debugger, total=   0.0s**
</code></pre>
","2","Question"
"78896800","","<p>I am running the python script in google collab. When I do this I am getting an error with the config but I am unsure on how to fix this</p>
<p>The code snippet:</p>
<pre><code>import hydra
from pathlib import Path  # Import Path for file path handling
import sys  # Import the sys module

@hydra.main(config_path=&quot;cfgs&quot;, config_name=&quot;config.yaml&quot;)
def main(cfg):
   print(cfg) # Print the parsed configuration
   from train import Workspace as W
   root_dir = Path.cwd()

   workspace = W(cfg)

   snapshot = root_dir / 'snapshot.pt'

   if snapshot.exists():
      print(f'resuming: {snapshot}')
      workspace.load_snapshot()

   workspace.train()



if __name__ == '__main__':
     main() 
</code></pre>
<p>The output I get is this:</p>
<pre><code>usage: colab_kernel_launcher.py [--help] [--hydra-help] [-- 
version] [--cfg {job,hydra,all}]
                               [--resolve] [--package PACKAGE] [-- 
run] [--multirun]
                            [--shell-completion] [--config-path 
CONFIG_PATH]
                            [--config-name CONFIG_NAME] [--config- 
dir CONFIG_DIR]
                            [--info 
[{all,config,defaults,defaults-tree,plugins,searchpath}]]
                            [overrides ...]
colab_kernel_launcher.py: error: unrecognized arguments: -f
 An exception has occurred, use %tb to see the full traceback.
    SystemExit: 2
</code></pre>
","-1","Question"
"78896943","","<p>Using scikit-learn I'm building machine learning models on a training set, and then evaluating them on a test set. On the train set I perform data imputation and scaling with the <code>ColumnTransformer</code>, then build a logistic regression model using Kfold CV, and the final model is used to predict the values on the test set. The final model is also using its results from <code>ColumnTransformer</code> to impute the missing values on the test set. For example min-max scalar would be taking the min and max values from the train set and would use those values when scaling the test set. How can I see these scaling values that are derived from the the train set and then used to predict on the test set? I can't find anything on the scikit-learn documentation about it. Here is the code I'm using:</p>
<pre><code>from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

def preprocessClassifierLR(categorical_vars, numeric_vars):###categorical_vars and numeric_vars are lists defining the column names of the categorical and numeric variables present in X


    categorical_pipeline = Pipeline(steps=[('mode', SimpleImputer(missing_values=np.nan, strategy=&quot;most_frequent&quot;)),
                                           (&quot;one_hot_encode&quot;, OneHotEncoder(handle_unknown='ignore'))])

    numeric_pipeline = Pipeline(steps=[('numeric', SimpleImputer(strategy=&quot;median&quot;)),
                                       (&quot;scaling&quot;, MinMaxScaler())])

    col_transform = ColumnTransformer(transformers=[(&quot;cats&quot;, categorical_pipeline, categorical_vars),
                                                    (&quot;nums&quot;, numeric_pipeline, numeric_vars)])

    lr = SGDClassifier(loss='log_loss', penalty='elasticnet')
    model_pipeline = Pipeline(steps=[('preprocess', col_transform),
                                     ('classifier', lr)])


    random_grid_lr = {'classifier__alpha': [1e-1, 0.2, 0.5],
                      'classifier__l1_ratio': [1e-3, 0.5]}

    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=47)

    param_search = GridSearchCV(model_pipeline, random_grid_lr, scoring='roc_auc', cv=kfold, refit=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

param_search = preprocessClassifierLR(categorical_vars, numeric_vars)
train_mod = param_search.fit(X_train, y_train)
print(&quot;Mod AUC:&quot;, train_mod.best_score_)

test_preds = train_mod.predict_proba(X_)[:,1]
</code></pre>
<p>I can't provide the real data, but X is a dataframe with the independent variables and y is the binary outcome variable. train_mod is a pipeline which contains the columntransformer and SGDclassifier steps. I can easily get similar parameter information from the classifier such as the optimal lambda and alpha values by running: <code>train_mod.best_params_</code>, but I cannot figure out the stats used for the column transformer such as 1) the modes used for the simple imputer for the categorical features, 2) the median values used for the simple imputer for the numeric features, and 3) the min and max values used for the scaling of the numeric features. How to access this information?</p>
<p>I assumed that <code>train_mod.best_estimator_['preprocess'].transformers_</code> would contain this information, in a similar way to how <code>train_mod.best_params_</code> gives me the alpha and lambda values derived from the model training that are then applied to the test set.</p>
","2","Question"
"78898892","","<p>Simple code to reproduce follows</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow.keras as keras

inp = keras.Input((3, 3))
layer = keras.layers.Dense(1)
tar = layer(inp)

tar2 = layer(inp, training=False)
model2 = keras.Model(inp, tar2)
model2.compile(loss='mse', optimizer=keras.optimizers.Adam(0.01))

# fit
a = tf.random.normal((1, 3, 3))
b = tf.random.normal((1, 3, 1))

model2.fit(a, b)
</code></pre>
<p>If you check <code>model2.trainable_variables</code> before/after the training, you can easily check that parameters of the <code>model2</code> have been changed which means it has been trained.</p>
<p>What should I do in order not to update the <strong>certain layer in certain time</strong> during training?</p>
<p>The reason why I need is, my scheme is to reuse some layers I made before and I don't want those layers to be trained <strong>again</strong> while I still update other layers in the model. Like follows:</p>
<pre class=""lang-py prettyprint-override""><code>inp1 = keras.Input((2, 3))
inp2 = keras.Input((4, 3))

layer1 = keras.layers.Dense(1)
intermediate_output1 = layer1(inp1)
intermediate_output2 = layer1(inp2, training=False) # I don't want to train this layer again for inp2.

added = tf.concat([intermediate_output1, intermediate_output2], axis=1)
layer2 = keras.layers.Dense(2)
final_output = layer2(added)
final_output.shape # [None, 6, 4]
</code></pre>
","0","Question"
"78900691","","<p>I am using <a href=""https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/"" rel=""nofollow noreferrer"">this link</a> to learn langhain and pinecone. Just copy pasted the code as bellow:</p>
<p>I am using default <code>PINECONE_API_KEY</code> and using bellow code I am able to create index in pinecone, facing problem while saving embedding in index itself.</p>
<p>My Code:</p>
<pre><code>import getpass
import os
import time

from pinecone import Pinecone

os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')
pinecone_api_key = os.environ.get(&quot;PINECONE_API_KEY&quot;)

pc = Pinecone(api_key=pinecone_api_key)


index_name = &quot;langchain-pinecone-learning&quot;  # change if desired

existing_indexes = [index_info[&quot;name&quot;] for index_info in pc.list_indexes()]

if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=3072,
        metric=&quot;cosine&quot;,
        spec=ServerlessSpec(cloud=&quot;aws&quot;, region=&quot;us-east-1&quot;),
    )
    while not pc.describe_index(index_name).status[&quot;ready&quot;]:
        time.sleep(1)

index = pc.Index(index_name)
index


embeddings=(
    OllamaEmbeddings(model=&quot;gemma:2b&quot;)  ##by default it ues llama2
)
</code></pre>
<p><strong>Document is exactly similar to Document in article itself</strong></p>
<pre><code>from langchain_pinecone import PineconeVectorStore
vector_store = PineconeVectorStore(index=index, embedding=embeddings)
vector_store
</code></pre>
<p>Not there are 2 methods to save data in vector_db:
1.vector_store.from_documents(documents,embeddings)
2.vector_store.add_documents(documents=documents, ids=uuids)</p>
<p>In both cases I am getting different error:</p>
<p><strong>For <code>vector_store.from_documents(documents,embeddings)</code>:</strong></p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[28], line 73
     68 uuids = [str(uuid4()) for _ in range(len(documents))]
     70 # vector_store.add_documents(documents=documents, ids=uuids)
---&gt; 73 vector_store.from_documents(documents,embeddings)

File u:\GENERATIVE_AI\genv\lib\site-packages\langchain_core\vectorstores\base.py:833, in VectorStore.from_documents(cls, documents, embedding, **kwargs)
    831 texts = [d.page_content for d in documents]
    832 metadatas = [d.metadata for d in documents]
--&gt; 833 return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)

File u:\GENERATIVE_AI\genv\lib\site-packages\langchain_pinecone\vectorstores.py:453, in PineconeVectorStore.from_texts(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, async_req, id_prefix, **kwargs)
    407 @classmethod
    408 def from_texts(
    409     cls,
   (...)
    424     **kwargs: Any,
    425 ) -&gt; PineconeVectorStore:
    426     &quot;&quot;&quot;Construct Pinecone wrapper from raw documents.
    427 
    428     This is a user friendly interface that:
   (...)
    451             )
    452     &quot;&quot;&quot;
...
    403         f&quot;Did you mean one of the following indexes: {', '.join(index_names)}&quot;
    404     )
    405 return index

ValueError: Index 'None' not found in your Pinecone project. Did you mean one of the following indexes: langchain-pinecone-learning
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
</code></pre>
<p><strong>For second Case i.e <code>vector_store.add_documents(documents=documents, ids=uuids)</code>:</strong></p>
<pre><code>---------------------------------------------------------------------------
PineconeApiException                      Traceback (most recent call last)
Cell In[18], line 70
     55 documents = [
     56     document_1,
     57     document_2,
   (...)
     65     document_10,
     66 ]
     68 uuids = [str(uuid4()) for _ in range(len(documents))]
---&gt; 70 vector_store.add_documents(documents=documents, ids=uuids)
     73 # vector_store.from_documents(documents,embeddings)

File u:\GENERATIVE_AI\genv\lib\site-packages\langchain_core\vectorstores\base.py:282, in VectorStore.add_documents(self, documents, **kwargs)
    280     texts = [doc.page_content for doc in documents]
    281     metadatas = [doc.metadata for doc in documents]
--&gt; 282     return self.add_texts(texts, metadatas, **kwargs)
    283 raise NotImplementedError(
    284     f&quot;`add_documents` and `add_texts` has not been implemented &quot;
    285     f&quot;for {self.__class__.__name__} &quot;
    286 )

File u:\GENERATIVE_AI\genv\lib\site-packages\langchain_pinecone\vectorstores.py:175, in PineconeVectorStore.add_texts(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)
    164 if async_req:
    165     # Runs the pinecone upsert asynchronously.
...
PineconeApiException: (400)
Reason: Bad Request
HTTP response headers: HTTPHeaderDict({'Date': 'Thu, 22 Aug 2024 08:58:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '104', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1019', 'x-pinecone-request-id': '7419458757206642138', 'x-envoy-upstream-service-time': '41', 'server': 'envoy'})
HTTP response body: {&quot;code&quot;:3,&quot;message&quot;:&quot;Vector dimension 2048 does not match the dimension of the index 3072&quot;,&quot;details&quot;:[]}
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
</code></pre>
","0","Question"
"78902994","","<p>I'm trying to train a model previously working fine until recently. The fit function throws the error below:</p>
<pre><code>    &lt;ipython-input-20-01755a6ded38&gt; in &lt;cell line: 1&gt;()
----&gt; 1 model.fit(
      2     dataset,
      3     epochs=100,
      4     verbose=1,
      5     batch_size=8)

1 frames /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
    120             # To get the full stack trace, call:
    121             # `keras.config.disable_traceback_filtering()`
--&gt; 122             raise e.with_traceback(filtered_tb) from None
    123         finally:
    124             del filtered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py in squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1)
    105 def squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1=True):
    106     &quot;&quot;&quot;Squeeze/expand last dim if ranks differ from expected by exactly 1.&quot;&quot;&quot;
--&gt; 107     x1_rank = len(x1.shape)
    108     x2_rank = len(x2.shape)
    109     if x1_rank == x2_rank:

ValueError: Cannot take the length of shape with unknown rank.
</code></pre>
<p>I've tried passing just the x and y generated by the generator into fit and it works fine, so it's not a shape problem.
Here's a recreation of the error. The model is just a simple Sequential model:</p>
<pre><code>model = keras.models.Sequential(
    [keras.layers.Dense(10, activation='relu'),
     keras.layers.Dense(1, activation='sigmoid')]
)
model.compile(loss='binary_crossentropy', metrics=['accuracy'])
</code></pre>
<p>The dataset is generated from tf.data.Dataset.from_generator() as below:</p>
<pre><code>#Random dataset
a = tf.convert_to_tensor(np.random.randint(0,100, size=[10,10]))
    
#Data generator class
class DataGenerator:
   def __init__(self, data, ratio=3):
      self._ratio = ratio
      self._data = data
    
   def __call__(self):
      shape = tf.shape(self._data).numpy()
      x = tf.convert_to_tensor(np.random.randint(1000,100000, size=[shape[0] * self._ratio, shape[1]]))
      x = tf.concat([self._data, x], axis = 0)
      y = tf.convert_to_tensor(np.random.random(shape[0]*(1 + self._ratio)))
    
      yield x, y


data_gen = DataGenerator(a, 3)
dataset = tf.data.Dataset.from_generator(
            data_gen,
            output_signature=(
                tf.TensorSpec(shape=(None,10), dtype=tf.int32),
                tf.TensorSpec(shape=(None), dtype=tf.float32)))
</code></pre>
<p>Model.fit() produced the error above:</p>
<pre><code>model.fit(
    dataset,
    epochs=100,
    verbose=1,
    batch_size=8)
</code></pre>
<p>Here's a recreation of the error in Colab: <a href=""https://colab.research.google.com/drive/1f7I2St2U3LxaWZZSTxT2xWN14gCZMBSE?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1f7I2St2U3LxaWZZSTxT2xWN14gCZMBSE?usp=sharing</a></p>
","1","Question"
"78903793","","<p>I'm trying to recreate the <code>cv2.warpAffine()</code> function, taking a tensor input and output rather than a Numpy array. However, gradients calculated from the output tensor produce a Non-None gradient tensor of all 0s. I've spent all night trying to see where I'm going wrong. If someone can spot my mistake, I will be so happy to hear what it is. I'm happy to provide any further information.</p>
<pre><code>def tensor_warp_affine(tensor, M, output_shape, border_value=0.0):
    height, width = output_shape
    channels = tensor.shape[2]
    device = tensor.device
    dtype = tensor.dtype
   
   
    # Convert M to tensor and compute inverse
    M_tensor = torch.tensor(M, dtype=torch.float32, device=device)
    M_padded = torch.cat([M_tensor, torch.tensor([[0, 0, 1]], dtype=torch.float32, device=device)], dim=0)
    M_inv = torch.inverse(M_padded)[:2]
    # Create coordinate grids
    y_coords, x_coords = torch.meshgrid(torch.arange(height, device=device), torch.arange(width, device=device), indexing='ij')
    coords = torch.stack([x_coords.flatten(), y_coords.flatten(), torch.ones_like(x_coords.flatten())], dim=0).float()
    # Transform coordinates
    src_coords = M_inv @ coords
    src_x, src_y = src_coords[0], src_coords[1]
    # Compute interpolation weights
    x0 = src_x.long()
    y0 = src_y.long()
    x1 = x0 + 1
    y1 = y0 + 1
    x0 = torch.clamp(x0, 0, tensor.shape[1] - 1)
    x1 = torch.clamp(x1, 0, tensor.shape[1] - 1)
    y0 = torch.clamp(y0, 0, tensor.shape[0] - 1)
    y1 = torch.clamp(y1, 0, tensor.shape[0] - 1)
    dx = src_x - x0.float()
    dy = src_y - y0.float()
    # Perform bilinear interpolation
    wa = (1 - dx) * (1 - dy)
    wb = dx * (1 - dy)
    wc = (1 - dx) * dy
    wd = dx * dy
    img_flat = tensor.reshape(-1, channels)
   
    pixel = (wa[:, None] * img_flat[y0 * tensor.shape[1] + x0] +
             wb[:, None] * img_flat[y0 * tensor.shape[1] + x1] +
             wc[:, None] * img_flat[y1 * tensor.shape[1] + x0] +
             wd[:, None] * img_flat[y1 * tensor.shape[1] + x1])
   
    result = pixel.reshape(height, width, channels)
    return result
</code></pre>
<p>I'm doing this so I can pass the resulting tensor into an encoder model that generates latent vectors. I'm trying to optimise added perturbations to an input image using Stochastic gradient descent. My suspicion is that some of the transformation points are out of bounds, and being rounded to zero, is this non-differentiable?</p>
<p>I've tried printing out out-of-bounds values, but nothing ever flags up.
I've tried using pytorche affine_grid but the results seem to be incorrect in my implementation.
input tensor:</p>
<pre><code>tensor([[[0.0000, 0.0078, 0.0078],
     [0.0039, 0.0118, 0.0118],
     [0.0000, 0.0078, 0.0078],
     ...,
     [0.0000, 0.0157, 0.0196],
     [0.0000, 0.0157, 0.0196],
     [0.0000, 0.0157, 0.0196]],

    [[0.0000, 0.0078, 0.0078],
     [0.0039, 0.0118, 0.0118],
     [0.0000, 0.0078, 0.0078],
     ...,
     [0.0000, 0.0157, 0.0196],
     [0.0000, 0.0157, 0.0196],
     [0.0000, 0.0157, 0.0196]],

    [[0.0000, 0.0078, 0.0078],
     [0.0000, 0.0078, 0.0078],
     [0.0000, 0.0078, 0.0078],
     ...,
     [0.0000, 0.0157, 0.0196],
     [0.0000, 0.0157, 0.0196],
     [0.0000, 0.0157, 0.0196]],

    ...,

    [[0.0275, 0.1059, 0.4392],
     [0.0196, 0.0980, 0.4314],
     [0.0157, 0.0941, 0.4275],
     ...,
     [0.3373, 0.5216, 0.8392],
     [0.3333, 0.5176, 0.8353],
     [0.3059, 0.4902, 0.8078]],

    [[0.0275, 0.1059, 0.4392],
     [0.0196, 0.0980, 0.4314],
     [0.0196, 0.0980, 0.4314],
     ...,
     [0.3373, 0.5216, 0.8392],
     [0.3216, 0.5059, 0.8235],
     [0.3059, 0.4902, 0.8078]],

    [[0.0275, 0.1059, 0.4392],
     [0.0196, 0.0980, 0.4314],
     [0.0196, 0.0980, 0.4314],
     ...,
     [0.3176, 0.5020, 0.8196],
     [0.3216, 0.5059, 0.8235],
     [0.3176, 0.5020, 0.8196]]], grad_fn=&lt;SqueezeBackward1&gt;)
</code></pre>
<p>The desired output tensor should look something like this:</p>
<pre><code>tensor([[[8.5652e-03, 4.3859e-02, 9.7611e-02],
     [7.6292e-03, 4.6845e-02, 8.6061e-02],
     [0.0000e+00, 3.5453e-02, 7.4669e-02],
     ...,
     [3.9216e-03, 1.1765e-02, 5.0980e-02],
     [2.5561e-03, 1.7278e-02, 3.6886e-02],
     [0.0000e+00, 7.8431e-03, 4.7059e-02]],

    [[3.9216e-03, 3.9216e-02, 9.4118e-02],
     [7.7203e-04, 3.9988e-02, 7.9203e-02],
     [0.0000e+00, 3.7218e-02, 6.9977e-02],
     ...,
     [1.6439e-03, 8.9082e-03, 4.8124e-02],
     [2.9788e-03, 1.0822e-02, 5.0038e-02],
     [3.9216e-03, 1.1765e-02, 5.0980e-02]],

    [[0.0000e+00, 3.9216e-02, 6.6035e-02],
     [0.0000e+00, 3.1373e-02, 6.6667e-02],
     [0.0000e+00, 3.5294e-02, 3.5833e-02],
     ...,
     [0.0000e+00, 3.9216e-03, 4.3137e-02],
     [3.9216e-03, 1.0668e-02, 4.9884e-02],
     [4.4216e-04, 5.1212e-03, 4.4337e-02]],

    ...,

    [[1.4902e-01, 2.5882e-01, 3.7647e-01],
     [1.1612e-01, 2.2642e-01, 3.4406e-01],
     [9.3184e-02, 1.7629e-01, 2.6298e-01],
     ...,
     [8.1880e-02, 1.7951e-01, 3.2182e-01],
     [1.0127e-01, 1.7469e-01, 3.4608e-01],
     [2.0947e-02, 7.9770e-02, 2.1282e-01]],

    [[1.4533e-01, 2.6514e-01, 3.8574e-01],
     [1.2677e-01, 2.4847e-01, 3.7004e-01],
     [9.8382e-02, 1.9250e-01, 2.9153e-01],
     ...,
     [8.0654e-02, 1.5194e-01, 3.3930e-01],
     [1.8802e-01, 2.5493e-01, 4.4500e-01],
     [1.8050e-02, 6.2331e-02, 2.3506e-01]],

    [[1.4967e-01, 2.6551e-01, 3.8581e-01],
     [1.1913e-01, 2.3896e-01, 3.6051e-01],
     [9.7532e-02, 1.9963e-01, 2.9095e-01],
     ...,
     [5.5842e-02, 1.2251e-01, 3.1291e-01],
     [1.6402e-01, 2.2266e-01, 4.1669e-01],
     [8.3060e-02, 1.3796e-01, 3.3012e-01]]], grad_fn=&lt;ViewBackward0&gt;)
</code></pre>
<p>gradient calculation:</p>
<pre><code>tensor([[[[0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.],
      ...,
      [0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.]],

     [[0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.],
      ...,
      [0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.]],

     [[0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.],
      ...,
      [0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.]],

     ...,

     [[0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.],
      ...,
      [0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.]],

     [[0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.],
      ...,
      [0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.]],

     [[0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.],
      ...,
      [0., 0., 0.],
      [0., 0., 0.],
      [0., 0., 0.]]]])
</code></pre>
","0","Question"
"78903794","","<p>I have trained Random forest model using June dataset to predict status_value of an employee and  using 0.3 split for test_size. I am including code snippets as the code it self works well without any errors</p>
<pre class=""lang-py prettyprint-override""><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
rf = RandomForestClassifier()
rf.fit(X_resampled,y_resampled)
y_pred = rf.predict(X_test)
</code></pre>
<p>when I use trained RF model to predict the status_value without splitting July dataset it still only predicts for 30% of the employees and not the full dataset.</p>
<pre class=""lang-py prettyprint-override""><code>ml_cols = ['age','months tenure','salary_deviation','review_rating','term reason enc','color_rating','satisfaction score','ytd commission','Sales Activities','DurationOfSalesCall','MileageOfSalesCall','Referrals','Admits']
ml_df_pred =merged_df_pred[ml_cols]
X_pred = ml_df_pred[ml_cols]
y_fcst = merged_df_pred['status_value']
y_pred = rf.predict(X_pred)
</code></pre>
<p>How do I go about ensuring that it doesn't split July data and predicts for all 180 employees listed in July dataset?</p>
","-1","Question"
"78905671","","<p>I am implementing a simple MLP using sagemaker particularly an aws ml.g4.xlarge machine and I noticed that ram memory keeps increasing over epoch. I am using pytorch.lightning for ML model architecture. I am using a dataloader with 64 workers.</p>
<p><a href=""https://i.sstatic.net/5weE7oHO.png"" rel=""nofollow noreferrer"">ML Flow - System Metrics</a></p>
<p>What happens is that the model when reaches 70% of training crashes the machine saying it memory max limit is reached. As you can see between batches the memory is increasing - my understanding is that between batches each batch is used and then removed out of memory so that only thing that changes between batches is the weight updates. So the memory usage should not increase over steps.</p>
<p>I tried removing logs, nothing changed. Though that might be the number of workers, added just one worker and the memory usage kept increasing over time. However with num_workers=0 it stabilizes and does not increase over time - but the training takes significant amount of time</p>
<p><a href=""https://i.sstatic.net/O9CwGDv1.png"" rel=""nofollow noreferrer"">ML Flow - System Metrics - num_workers=0</a></p>
<p>How can I fix this issue?</p>
","-2","Question"
"78907434","","<p>In the gradient descent page on the intro to <a href=""https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent"" rel=""nofollow noreferrer"">machine-learning course</a> provided by google provided are features and corresponding labels, MSE loss function, initial datasets, and results. I am having difficulty verifying the results that they have and I am wondering if anyone can assist in confirming whether or not I am making a mistake or they are.</p>
<p>I have the following:</p>
<pre><code>import pandas as pd
import numpy as np

data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
initial_data_df = pd.DataFrame(data,columns=['pounds','mpg'])

number_of_iterations = 6
weight = 0 # initialize weights
bias = 0 # initialize weights
weight_slope = 0
bias_slope = 0
final_results_df = pd.DataFrame()
learning_rate = 0.01

for i in range(number_of_iterations):
    loss = calculate_loss(initial_data_df,weight,bias)
    final_results_df = update_results(final_results_df,weight,bias,loss)
    weight_slope = find_weight_slope(initial_data_df,weight,bias)
    bias_slope = find_bias_slope(initial_data_df,weight,bias)
    weight = new_weight_update(weight,learning_rate,weight_slope)
    bias = new_bias_update(bias,learning_rate,bias_slope)
print(final_results_df)

def calculate_loss(df,weight,bias):
    loss_summation = []
    for i in range(0,len(df)):
        loss_summation.append((df['mpg'][i]-((weight*df['pounds'][i])+bias))**2)
    return (sum(loss_summation)//len(df))

def update_results(df,weight,bias,loss):
    if df.empty:
        df = pd.DataFrame([[weight,bias,loss]],columns=['weight','bias','loss'])
    else:
        df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
    return df

def find_weight_slope(df,weight,bias):
    weight_update_summation = []
    for i in range(0,len(df)):
        wx_plus_b = (weight*df['pounds'][i])+bias
        wx_plus_b_minus_y = wx_plus_b-df['mpg'][i]
        weight_update_summation.append(2*(wx_plus_b_minus_y*df['pounds'][i]))
    return sum(weight_update_summation)//len(df)

def find_bias_slope(df,weight,bias):
    bias_update_summation = []
    for i in range(0,len(df)):
        wx_plus_b = (weight*df['pounds'][i])+bias
        wx_plus_b_minus_y = wx_plus_b-df['mpg'][i]
        bias_update_summation.append(2*wx_plus_b_minus_y)
    total_sum = sum(bias_update_summation)
    return total_sum//len(df)

def new_weight_update(old_weight,lr,slope):
    return old_weight-1*lr*slope

def new_bias_update(old_bias,lr,slope):
    return old_bias-1*lr*slope
</code></pre>
<p>Which yields:</p>
<pre><code>iter weight  bias   loss
0    0.00    0.00   303.0
0    1.20    0.35   170.0
0    2.06    0.60   102.0
0    2.67    0.79   67.0
0    3.10    0.93   50.0
0    3.41    1.04   41.0
</code></pre>
<p>This differs from the provided solution provided on the website:</p>
<pre><code>Iteration   Weight  Bias    Loss (MSE)
1           0       0       303.71
2           1.2     0.34    170.67
3           2.75    0.59    67.3
4           3.17    0.72    50.63
5           3.47    0.82    42.1
6           3.68    0.9     37.74
</code></pre>
","-1","Question"
"78907557","","<p>I wish to apply a custom-training with YoloV10n on a dataset I have (I've been using YoloV8 until now), but I'm not sure wether to use/import YOLO or YOLOv10.</p>
<p>I've initially tried to use YOLOv10, and was able to complete successfully custom training and then inference. However I wasn't able to export the model to tflite due to inconsistency assertions. I then switched back to YOLO (starting from a pre-trained yolov10.pt model) and was able to export it for tflite. Moreover, the official documentation of Ultralytics do guide for using YOLO (and not YOLOv10) for YoloV10 training. On the other side RoboFlow tutorial does guide to use YOLOv10... 🤔</p>
<p>Shall I use <code>from ultralytics import YOLOv10</code> or <code>from ultralytics import YOLO</code>? Does it matter? Does same answer hold for both training, inference and export (tflite, etc.)?</p>
","-1","Question"
"78907608","","<p>We have a MessageGraph for an LLMCompiler implementation and as expected we pass a users's question when running invoke as a list of HumanMessage objects (which are mapped to a default &quot;messages&quot; key and passed to prompt templates), this works fine for simple use cases but now we need to pass an additional piece of information/context at invoke time (not at graph building time), we did something similar with a react agent and it was very easy passing a dictionary similar to <code>invoke({&quot;messages&quot;:input, &quot;context&quot;:context})</code>. But for MessageGraph this did not work, it looks as the list of messages passed when you run <code>invoke(messages)</code> is automatically mapped in the prompt to the &quot;messages&quot; key and no other inputs can be added, I tried passing a dicionary <code>invoke({&quot;messages&quot;:messages, &quot;context&quot;:context})</code> and it did not work, failed with error:</p>
<blockquote>
<p>Message dict must contain 'role' and 'content' keys, got
{&quot;messages&quot;:messages,&quot;context&quot;:context}</p>
</blockquote>
","0","Question"
"78911068","","<p>I'm trying to implement the Resnet in torch. But I found the output of the forward pass varies greatly between train and eval mode. Since the train and eval mode doesn't affect anything besides batch norm and dropout, I don't know if the results make sense.</p>
<p>Below is my test code:</p>
<pre><code>import torch
from torch import nn
from torchvision import models

class resnet_lstm(torch.nn.Module):
    def __init__(self):
        super(resnet_lstm, self).__init__()
        resnet = models.resnet50(pretrained=True)
        self.share = torch.nn.Sequential()
        self.share.add_module(&quot;conv1&quot;, resnet.conv1)
        self.share.add_module(&quot;bn1&quot;, resnet.bn1)  # Use BatchNorm3d
        self.share.add_module(&quot;relu&quot;, resnet.relu)
        self.share.add_module(&quot;maxpool&quot;, resnet.maxpool)
        self.share.add_module(&quot;layer1&quot;, resnet.layer1)
        self.share.add_module(&quot;layer2&quot;, resnet.layer2)
        self.share.add_module(&quot;layer3&quot;, resnet.layer3)
        self.share.add_module(&quot;layer4&quot;, resnet.layer4)
        self.share.add_module(&quot;avgpool&quot;, resnet.avgpool)
        self.fc = nn.Sequential(nn.Linear(2048, 512),
                                nn.ReLU(),
                                nn.Linear(512, 7))

    def forward(self, x):
        x = x.view(-1, 3, 224, 224)
        x = self.share(x)
        return x
    
model = resnet_lstm()

input_ = torch.randn(1, 3, 224, 224)
model.train()
print(&quot;train mode output&quot;, model(input_))
model.eval()
print(&quot;eval mode output&quot;, model(input_))

</code></pre>
<p>Terminal output:</p>
<pre class=""lang-none prettyprint-override""><code>train mode output tensor([[[[0.3603]],

         [[0.5518]],

         [[0.4599]],

         ...,

         [[0.3381]],

         [[0.4445]],

         [[0.3481]]]], grad_fn=&lt;MeanBackward1&gt;)
eval mode output tensor([[[[0.1582]],

         [[0.1822]],

         [[0.0000]],

         ...,

         [[0.0567]],

         [[0.0054]],

         [[0.3605]]]], grad_fn=&lt;MeanBackward1&gt;)
</code></pre>
<p>As you can see, the output of the two modes are very different from each other. Would this damage the performance?</p>
","0","Question"
"78911175","","<p>I'm trying to train a language model for machine translation between a low-resource language and Portuguese using Tensorflow. unfortunately, I'm getting the following error:</p>
<pre><code>PS C:\Users\myuser\PycharmProjects\teste&gt; python .\tensorflow_model.py                   
2024-08-23 21:29:50.839647: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File &quot;.\tensorflow_model.py&quot;, line 52, in &lt;module&gt;
    dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 831, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;, line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;, line 45, in __init__
    batch_dim.assert_is_compatible_with(
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\framework\tensor_shape.py&quot;, line 300, in assert_is_compatible_with
    raise ValueError(&quot;Dimensions %s and %s are not compatible&quot; %
ValueError: Dimensions 21 and 22 are not compatible
</code></pre>
<p>How can I overcome this error?</p>
<pre><code>import tensorflow as tf
import numpy as np
import re
import os

# Clean data
def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    sentence = re.sub(r&quot;([?.!,¿])&quot;, r&quot; \1 &quot;, sentence)
    sentence = re.sub(r'[&quot; &quot;]+', &quot; &quot;, sentence)
    sentence = re.sub(r&quot;[^a-zA-Z?.!,¿]+&quot;, &quot; &quot;, sentence)
    sentence = sentence.strip()
    sentence = '&lt;start&gt; ' + sentence + ' &lt;end&gt;'
    return sentence

#Function to load data
def load_data(file_path_src, file_path_tgt):
    src_sentences = open(file_path_src, 'r', encoding='utf-8').read().strip().split('\n')
    tgt_sentences = open(file_path_tgt, 'r', encoding='utf-8').read().strip().split('\n')

    src_sentences = [preprocess_sentence(sentence) for sentence in src_sentences]
    tgt_sentences = [preprocess_sentence(sentence) for sentence in tgt_sentences]

    return src_sentences, tgt_sentences

#load data
src_sentences, tgt_sentences = load_data('src_language.txt', 'portuguese.txt')

#Tokenization
src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
tgt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')

src_tokenizer.fit_on_texts(src_sentences)
tgt_tokenizer.fit_on_texts(tgt_sentences)

src_tensor = src_tokenizer.texts_to_sequences(src_sentences)
tgt_tensor = tgt_tokenizer.texts_to_sequences(tgt_sentences)

src_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_tensor, padding='post')
tgt_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_tensor, padding='post')

BUFFER_SIZE = len(src_tensor)

#Creating the Dataset
dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE) 
</code></pre>
","0","Question"
"78911757","","<p>I am trying to create a project using already provided template &quot;Model building, training, and deployment&quot; in AWS Sagemaker studio. However, I am encountering the following error: <a href=""https://i.sstatic.net/e8MZgiMv.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/e8MZgiMv.png</a></p>
<p>Following steps I did:</p>
<ol>
<li><p>Create a domain in SageMaker.
<a href=""https://i.sstatic.net/ibo2tqj8.png"" rel=""nofollow noreferrer"">domain</a></p>
</li>
<li><p>Added additional policies (AWSCodeCommitFullAccess, AmazonS3FullAccess, AWSCloudFormationFullAccess) to the IAM role attached with the domain.
<a href=""https://i.sstatic.net/cWimiNeg.png"" rel=""nofollow noreferrer"">iam</a></p>
</li>
</ol>
<p>I still get that above error. Any clues as to what I am missing?</p>
<p>PS:</p>
<p>The quota of AWSCodeCommit is following:<a href=""https://i.sstatic.net/8O5rt7TK.png"" rel=""nofollow noreferrer"">codecommitquota</a></p>
<p>Testing Code Commit Repo creation: <a href=""https://i.sstatic.net/fiz4hg6t.png"" rel=""nofollow noreferrer"">codecommitrepo</a></p>
","0","Question"
"78911839","","<p>I train a model with keras tensorflow in python.
Also as you can see in the below code, I used seed parameter, but every time I run same code with same data, I face with different accuracy percentage.</p>
<p>My code:</p>
<pre><code>#Seed
tf.random.set_seed(42)
np.random.seed(42)
set_random_seed(42)
random.seed(42)

data = ('data.csv')

data = pd.get_dummies(data, columns=['cp', 'restecg'], drop_first=True)

X = data.drop('num', axis=1)
y = data['num']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def create_model(optimizer='adam', init='glorot_uniform', neurons=[16, 8], dropout_rate=0.3):
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1],)))
    model.add(Dense(neurons[0], activation='relu', kernel_initializer=init))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))
    model.add(Dense(neurons[1], activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

param_grid = {
    'optimizer': ['adam', 'rmsprop'],
    'model__neurons': [[16, 8]],
    'model__init': ['glorot_uniform', 'normal'],
    'model__dropout_rate': [0.3],
    'epochs': [50], 
    'batch_size': [10],
}

model = KerasClassifier(model=create_model, verbose=0)

kfold = KFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, n_jobs=-1)
grid_search_result = grid_search.fit(X_train, y_train)

print(f&quot;Best Parameters: {grid_search_result.best_params_}&quot;)
print(f&quot;Best Accuracy: {grid_search_result.best_score_}&quot;)

best_model = grid_search_result.best_estimator_

keras_model = best_model.model
keras_model.trainable = False 

y_pred_prob = best_model.predict(X_test).flatten()
y_pred = np.where(y_pred_prob &gt; 0.5, 1, 0)

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f'Accuracy (manual calculation): {accuracy:.2f}')
print(f'ROC AUC: {roc_auc:.2f}')
</code></pre>
<p>I need to get same accuracy every time.
How should i solve this problem?</p>
","0","Question"
"78912616","","<p><a href=""https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html</a></p>
<p>I have 2 questions in this example:</p>
<p>Q1: In theory, it should be the more data, the less regulation. But &quot;C=50.0 / train_samples&quot; would lead to the more data with stronger regulation. Is it the deficit of the code, or my misundertanding?
<code>clf = LogisticRegression(C=50.0 / train_samples, penalty=&quot;l1&quot;, solver=&quot;saga&quot;, tol=0.1)</code></p>
<p>Q2: Why it is possible to draw the final number image with only the coef matrix, what's the ideas behind it?</p>
","-1","Question"
"78916012","","<p>I am trying to build model to predict posts likes, the model takes text and content type which is one hot encoded column.</p>
<p>I have made a TensorFlow dataset but when trying to fit the model I got this error:</p>
<pre><code>Layer &quot;functional_13&quot; expects 2 input(s), but it received 1 input tensors. 
Inputs received: [&lt;tf.Tensor 'data:0' shape=(None, 1000) dtype=int64&gt;]
</code></pre>
<p>Here is some snippets of my code:</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,
                                             content,
                                             df['likes_rate']))

dataset= dataset.cache()
dataset= dataset.shuffle(160000)
dataset= dataset.batch(16)
dataset= dataset.prefetch(8)
</code></pre>
<p>This is my model</p>
<pre><code>from tensorflow.keras.layers import Input, Embedding, Concatenate,LSTM,Bidirectional
text_input= Input(shape=(1000,))
content_input=Input(shape=(3,))

text_embeddings = tf.keras.layers.Embedding(Max_Features+1, 32)(text_input)  # Adjust embedding dim
lstm= Bidirectional(LSTM(32,activation='tanh'))(text_embeddings)
# Concatenate text embeddings and content features
combined_features = tf.keras.layers.Concatenate()([lstm, content_input])

# Hidden layers (adjust number/activation functions)
x = tf.keras.layers.Dense(256, activation='relu')(combined_features)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
# Output layer for likes prediction
output = tf.keras.layers.Dense(1, activation='linear')(x)
</code></pre>
<p>I want to make an embedding layer to for the text then pass it to LSTM then combine the output of the LSTM and the contnent to Dense layers.</p>
<p>When trying to fit the model I am getting the problem above.</p>
<pre><code>model = tf.keras.models.Model(inputs=[text_input, content_input], outputs=output)
model.compile(loss='mse',optimizer='Adam')
model.fit(dataset,epochs=10)
</code></pre>
<p>If I iterate over the dataset. The code works correctly. But <code>.fit</code> every time make a random weights so the model make no progress.</p>
<pre><code>for text_batch, content_batch, y_batch in dataset:
    # Train model on the current batch
    model.fit(x=[text_batch, content_batch], y=y_batch)
</code></pre>
","0","Question"
"78917847","","<p>I've created DataLoader objects for my training and validation datasets, but when I try to pass them to the trainer.train() method, I get the following error:</p>
<p>Log summary:</p>
<pre><code>TypeError: 'DataLoader' object is not subscriptable
</code></pre>
<p>Full log trace:</p>
<pre><code>[2024-08-27 07:35:44] WARNING - sg_trainer.py - Train dataset size % batch_size != 0 and drop_last=False, this might result in smaller last batch.
The console stream is now moved to /content/drive/MyDrive/.../checkpoints/yolo_nas_version_m/console_Aug27_07_35_44.txt
An error occurred during training: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py&quot;, line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
TypeError: 'DataLoader' object is not subscriptable

Traceback:
Traceback (most recent call last):
  File &quot;&lt;ipython-input-17-a2a5c064ba0b&gt;&quot;, line 5, in &lt;cell line: 4&gt;
    trainer.train(
  File &quot;/usr/local/lib/python3.10/dist-packages/super_gradients/training/sg_trainer/sg_trainer.py&quot;, line 1323, in train
    first_batch = next(iter(self.train_loader))
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py&quot;, line 630, in __next__
    data = self._next_data()
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py&quot;, line 1344, in _next_data
    return self._process_data(data)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py&quot;, line 1370, in _process_data
    data.reraise()
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/_utils.py&quot;, line 706, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py&quot;, line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
TypeError: 'DataLoader' object is not subscriptable
</code></pre>
<p>My code to create dataloader:</p>
<pre><code>from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val
from torch.utils.data import ConcatDataset, DataLoader

# List of dataset folders containing COCO datasets
yolo_folders = [f'{LOCATION}/dataset1', f'{LOCATION}/dataset2', f'{LOCATION}/dataset3', f'{LOCATION}/dataset4']

# Load YOLO TRAIN datasets from each folder
train_datasets = []
for folder in yolo_folders:
    dataset = coco_detection_yolo_format_train(
      dataset_params={
        'data_dir': folder,
        'images_dir': f'{folder}/train/images',
        'labels_dir': f'{folder}/train/labels',
        'classes': dataset_params['classes'],
        'input_dim': (640, 640)
      },
      dataloader_params={
          'batch_size': BATCH_SIZE,
          'num_workers': 2
      }
    )
    train_datasets.append(dataset)

# Combine the training datasets
combined_train_dataset = ConcatDataset(train_datasets)

# Create a DataLoader for the combined training dataset
train_dataloader = DataLoader(combined_train_dataset, batch_size=16, shuffle=True, num_workers=4)
</code></pre>
<p>My code to create model and call Trainer.train()</p>
<pre><code>import torch
from super_gradients.training import models
from super_gradients.training.losses import PPYoloELoss
from super_gradients.training.metrics import DetectionMetrics_050
from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback
from super_gradients.training import Trainer

model = models.get(MODEL_ARCH, num_classes=len(dataset_params['classes']), pretrained_weights=&quot;coco&quot;).to(DEVICE)

train_params = {
    # ENABLING SILENT MODE
    'silent_mode': False,
    &quot;average_best_models&quot;:True,
    &quot;warmup_mode&quot;: &quot;linear_epoch_step&quot;,
    &quot;warmup_initial_lr&quot;: 1e-6,
    &quot;lr_warmup_epochs&quot;: 3,
    &quot;initial_lr&quot;: 5e-4,
    &quot;lr_mode&quot;: &quot;cosine&quot;,
    &quot;cosine_final_lr_ratio&quot;: 0.1,
    &quot;optimizer&quot;: &quot;Adam&quot;,
    &quot;optimizer_params&quot;: {&quot;weight_decay&quot;: 0.0001},
    &quot;zero_weight_decay_on_bias_and_bn&quot;: True,
    &quot;ema&quot;: True,
    &quot;ema_params&quot;: {&quot;decay&quot;: 0.9, &quot;decay_type&quot;: &quot;threshold&quot;},
    &quot;max_epochs&quot;: 20,
    &quot;mixed_precision&quot;: False, #Set to True if using GPU to speed up training
    &quot;loss&quot;: PPYoloELoss(
        use_static_assigner=False,
        num_classes=len(dataset_params['classes']),
        reg_max=16
    ),
    &quot;valid_metrics_list&quot;: [
        DetectionMetrics_050(
            score_thres=0.1,
            top_k_predictions=300,
            # NOTE: num_classes needs to be defined here
            num_cls=len(dataset_params['classes']),
            normalize_targets=True,
            post_prediction_callback=PPYoloEPostPredictionCallback(
                score_threshold=0.01,
                nms_top_k=1000,
                max_predictions=300,
                nms_threshold=0.7
            )
        )
    ],
    &quot;metric_to_watch&quot;: 'mAP@0.50'
}

trainer = Trainer(experiment_name='yolo_nas_version_m', ckpt_root_dir=CHECKPOINT_DIR)

trainer.train(
    model=model,
    training_params=train_params,
    train_loader=train_dataloader,
    valid_loader=val_dataloader
)
</code></pre>
<p>If I do as below it works, but just using a single dataset directly:</p>
<pre><code>trainer.train(
    model=model,
    training_params=train_params,
    train_loader=train_datasets[0],
    valid_loader=val_datasets[0]
)
</code></pre>
<p>Please give me some advice.</p>
<p>P.S: I know a easy fix would be instead of having multiple dataset folders I could merge it in 1 and use trainer.train() with a single dataset instead of a combined dataloader.
But the solution is growing and I need to split those datasets in case I want to test with few of them or additional datasets.</p>
","0","Question"
"78925914","","<p>I am trying to create real-time inference pipeline in azure ml , following all straight forward steps, but I am unable to generate the web service input . only web service output is generated.</p>
<p>Further, if I manually add web service input or add data manually, in the next steps to follow i.e. deployment to create endpoint, I am unable to get populated data schema in 'consume' section of endpoint because of which I am not able to run my prediction using input data. (If I had the data schema for input , I would create a notebook and run input there to get my prediction).</p>
<p><a href=""https://i.sstatic.net/Qw7wmOnZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qw7wmOnZ.png"" alt=""pipeline"" /></a><a href=""https://i.sstatic.net/psEFiPfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/psEFiPfg.png"" alt=""consume section in endpoint"" /></a></p>
<p>I have used all sorts of computes, including container instance, akscompute, checked my dataset thoroughly.
Desperate for any solution, I am so worked up because I have tried every possible solution .</p>
","-1","Question"
"78932302","","<p>I have a dataset with personal characteristics of customers who purchase from a fictional company. Initially, I don't have any target variable, only their characteristics. My goal is to find a pattern, which may not necessarily be the most frequent characteristic in each column. Is it possible to do this with RandomForest, for example? Or should I use another technique?</p>
<p>The dataset has a structure similar to the following. The columns are all in <code>object</code> format and there are some <code>NaN</code> values represented as <code>'Blank'</code>:</p>
<pre><code>Date            Name     Salary      Position            Age
'05/10/2023'   'Daniel'  '10,000'    'IT'                32
'05/12/2024'   'John'    '9,000'     'Blank'             27
'03/01/2023'   'Niel'    'Blank'     'Data Scients'      21
'03/01/2023'   'Isa'     '10,000'    'Engineer'          51
'05/10/2023'   'Ana'     '11,000'    'Data Scients'      52
'05/12/2024'   'Ian'     '9,500'     'Doctor'            48
'03/01/2023'   'Fred'    'Blank'     'IT'                21
'03/01/2023'   'Carol'   '15,000'    'Blank'             30
</code></pre>
<p>I'm thinking of something that returns an output, for example, stating the characteristics that form the most standard profile, such as:</p>
<pre><code>The most standard profile is: Salary x, Position y, and Age z.
</code></pre>
<p>I thought about using clustering, but I don't believe it is the best method (the output for the salary, for example, was a simple average). I believe the best approach would be to create a profile that may not necessarily exist and is based on studying the pattern of each variable (Salary, Position, and Age).</p>
<pre><code># Encode categorical variables
df['Position'] = pd.Categorical(df['Position']).codes

# Perform clustering
kmeans = KMeans(n_clusters=1, random_state=42)
kmeans.fit(df[['Salary', 'Position', 'Age']])

# Get the centroid of the cluster
centroid = kmeans.cluster_centers_[0]
</code></pre>
<p>Is there a better way to do this? NLP or RandomForest is an option?</p>
","-1","Question"
"78937759","","<p>I have following torch dataset (I have replaced actual code to read data from files with random number generation to make it minimal reproducible):</p>
<pre><code>from torch.utils.data import Dataset
import torch 

class TempDataset(Dataset):
    def __init__(self, window_size=200):
        
        self.window = window_size

        self.x = torch.randn(4340, 10, dtype=torch.float32) # None
        self.y = torch.randn(4340, 3, dtype=torch.float32) 

        self.len = len(self.x) - self.window + 1 # = 4340 - 200 + 1 = 4141 
                                                # Hence, last window start index = 4140 
                                                # And last window will range from 4140 to 4339, i.e. total 200 elements

    def __len__(self):
        return self.len

    def __getitem__(self, index):

        # AFAIU, below if-condition should NEVER evaluate to True as last index with which
        # __getitem__ is called should be self.len - 1
        if index == self.len: 
            print('self.__len__(): ', self.__len__())
            print('Tried to access eleemnt @ index: ', index)
            
        return self.x[index: index + self.window], self.y[index + self.window - 1]

ds = TempDataset(window_size=200)
print('len: ', len(ds))
counter = 0 # no record is read yet
for x, y in ds:
    counter += 1 # above line read one more record from the dataset
print('counter: ', counter)
</code></pre>
<p>It prints:</p>
<pre><code>len:  4141
self.__len__():  4141
Tried to access eleemnt @ index:  4141
counter:  4141
</code></pre>
<p>As far as I understand, <code>__getitem__()</code> is called with <code>index</code> ranging from <code>0</code> to <code>__len__()-1</code>. If thats correct, then why it tried to call <code>__getitem__()</code> with index 4141, when the length of the data itself is 4141?</p>
<p>One more thing I noticed is that despite getting called with <code>index = 4141</code>, it does not seem to return any elements, which is why <code>counter</code> stays at 4141</p>
<p>What my eyes (or brain) are missing here?</p>
<p>PS: Though it wont have any effect, just to confirm, I also tried to wrap <code>DataSet</code> with torch <code>DataLoader</code> and it still behaves the same.</p>
","1","Question"
"78938961","","<p>I am trying to classify either an image of 25x25 px stacked together as 50x25 px is the same(1) or different(0). I am using keras to create the NN layers. The Keras sequential layers are shown below:</p>
<pre><code>layers.Input((2*imsize,imsize,3)),            # shape of input with 3 channels
layers.Reshape((2,imsize,imsize,3)),          # Turn the input into two 25x25 images
layers.LayerNormalization(axis=[-1,-2,-3]),   # Normalize images
layers.Flatten(),                             # Flatten array
layers.Dense(16,activation='relu'),           # 16 outputs hidden layer
layers.Dense(2,activation='softmax')    
</code></pre>
<p>Then I compiled the layers using adam optimiser with loss and accuracy like so:</p>
<pre><code>ml.compile(optimizer='adam',
       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
       metrics=['accuracy'])
</code></pre>
<p>After that, I trained the model using <code>epoch=20</code> and <code>batch_size=100</code>. I got these as a result after plotting it based on epoch.</p>
<p><strong>Results</strong></p>
<p><a href=""https://i.sstatic.net/22zUKmM6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/22zUKmM6.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/VCLflBYt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VCLflBYt.png"" alt=""enter image description here"" /></a></p>
<p><em>Current Evaluation:</em></p>
<p>My current observation is that</p>
<ol>
<li>the model is overfitting as it performs normally only on the training set?</li>
<li>the model is learning the wrong things as the loss is increasing instead of decreasing on the validation set</li>
</ol>
<p><strong>My question is</strong>: Is my evaluation of the model correct? How should I understand this results in order to improve it?</p>
<p><strong>Update:</strong>
Sample Dataset of same(1):</p>
<p><a href=""https://i.sstatic.net/l2VI1S9F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l2VI1S9F.png"" alt=""enter image description here"" /></a></p>
<p>Sample Dataset of different(0):</p>
<p><a href=""https://i.sstatic.net/nuDs9bXP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nuDs9bXP.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"78939808","","<p>I do not understand the parameter <code>nrounds</code>. I am working in R and using the package xgboost to create an 'extreme' gradient boost regression model.</p>
<p>There is a parameter called <code>nrounds</code> and my understanding of it was that it sets the number of trees in your gradient boosted model (also referred to as boosting iterations). However when I set <code>nrounds = 0</code>, I do not get that all predictions are equal to the base_score (initial guess in a gradient boosted model).</p>
<p>Why is that?</p>
<p>Note:
It is not that I want an xgboost model with 0 trees, but I tried it to sanity check my understanding of <code>nrounds</code>.</p>
<p>I have a data.frame that I transform to an xgb.DMatrix and then I train the model:</p>
<pre><code>params &lt;- list(objective = &quot;reg:squarederror&quot;, eval_metric = &quot;rmse&quot;, base_score = 0.5)

bst_model &lt;- xgb.train(params = params, data = dtrain, nrounds = 0, verbose = 1)
</code></pre>
<p>Using predict I get:</p>
<pre><code>pred &lt;- predict(bst_model, newdata = dtrain)
print(pred[1:10])
</code></pre>
<pre><code>[1] 1052663  874001 1498940 1991579 2316396 1086949  874001 1432935 1086949 2351261
</code></pre>
<p>Where I expected:</p>
<pre><code>[1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
</code></pre>
","1","Question"
"78940523","","<p>Is it possible to parallelize a (natively) single batch model?</p>
<p>Usually parallelization is done via the torch.bmm (batched matrix multiplication) in stead of the torch.matmul and fixing one dimension specifically for the batches. However this is not available for example for the torch.tensordot function.</p>
<p>So if one has such a model, is it possible to compute each gradient of the batch in parallel? Ideally, the parallelization should work with both training and inference.</p>
<p>A code example:</p>
<pre><code>import torch
import torch.nn as nn

class LinearMultidimModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearMultidimModel, self).__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, hidden_dim, output_dim))
        self.bias = nn.Parameter(torch.randn(output_dim))

    def forward(self, x):
        # Using torch.tensordot to perform the linear transformation
        out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
        return out

# Example usage
input_dim = 3
hidden_dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# Dummy input
x = torch.randn(input_dim, hidden_dim)# But what if I want to put in a batch, torch.randn(batch_size, input_dim, hidden_dim)?
output = model(x)
print(output)
</code></pre>
<p>Keep in mind that if there is no hidden_dim, it natively does the parallelization, one can entirely remove hidden_dim and get a result with</p>
<p><code>x = torch.randn(5, input_dim)</code>.</p>
<p>I've tried using Einsum, but that works for a fixed amount of hidden dimensions...</p>
","0","Question"
"78942595","","<p>I am reading Sutton&amp;Barto's &quot;Reinforcement Learning: An Introduction&quot;. Trying to test <strong>Gradient-bandit agent</strong> (chapter 2.7). But performance is extremely low. I've tried:</p>
<ul>
<li>using baseline = average reward, not using baseline;</li>
<li>alpha = 0.1, 0.2, 0.3, 0.4;</li>
<li>initial preferences H = 0, 1, 10, 100.</li>
</ul>
<p>Nothing helps.</p>
<p>This is my Python-code for an agent's <strong>life step = action selection + params update</strong> <em>(self = agent)</em>:</p>
<pre><code># for probabilities calculation:
pref_exps = np.exp(self.params[&quot;preferences&quot;])
pref_exps_sum = sum(pref_exps)

# choosing a bandit:
choice_dice = np.random.uniform() * pref_exps_sum
accum_pref_exp = 0
for i, pref_exp in enumerate(pref_exps):
    accum_pref_exp += pref_exp
    if accum_pref_exp &gt;= choice_dice:
        self.chosen_bandit_i = i
        break

# self.reward is filled here:
self.perform_bandit(self.chosen_bandit_i)

# updating baseline:
self.params[&quot;lifetime&quot;] += 1
self.params[&quot;average_reward&quot;] += 1 / self.params[&quot;lifetime&quot;] * (self.reward - self.params[&quot;average_reward&quot;])

# updating preferences:
for i, pref_exp in enumerate(pref_exps):
    probability = pref_exp / pref_exps_sum
    if i == self.chosen_bandit_i:
        self.params[&quot;preferences&quot;][i] += self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * (1 - probability)
    else:
        self.params[&quot;preferences&quot;][i] -= self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * probability
</code></pre>
<p>This code lead to extremely poor performance (100 agents, each accessing its own 10 1-armed-bandits, testing over 2000 steps), which we can see from the lower plot:
<a href=""https://i.sstatic.net/pzHXaHYf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pzHXaHYf.png"" alt=""100 agents testing over 1000 steps"" /></a></p>
<p>I've seen <a href=""https://stats.stackexchange.com/questions/324973/gradient-bandit-algorithm-baseline"">this</a> post and it seems that its code is effectively equal to mine after fixing the error, which was the reason of that post. But unlike my code, that post's code works properly when rectified!</p>
<p>I can't figure out where I've made a mistake. Can you help me to properly use the full power of <strong>Gradient-bandit agent</strong>?</p>
","-1","Question"
"78946669","","<p>I'm using Google colab, and have a problem importing <code>KerasRegressor</code>.</p>
<p>I run:</p>
<pre><code>from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
</code></pre>
<p>and get the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'tensorflow.keras.wrappers'
</code></pre>
<p>In some forums, I found people solving the issue with the &quot;scikeras&quot;, but I am having problems with this on colab.</p>
<p>How to resolve this?</p>
","0","Question"
"78946820","","<p>I have a single blog post with a title and description, and I want to compare its uniqueness against multiple blog entries in a CSV file. The CSV contains several blogs, each with a title and meta description.</p>
<p>I am currently using TF-IDF vectorization and cosine similarity to compare the single blog with all the entries in the CSV file. However, this approach only matches based on the exact words and not the context.</p>
","-1","Question"
"78949615","","<p>I'm trying to understand the dimensions of the output of the Generator of my GAN. The dims of the result after each layer is as follows:</p>
<pre><code>Start: torch.Size([128, 74, 1, 1])  
After block1: torch.Size([128, 256, 3, 3])  
After block2: torch.Size([128, 128, 6, 6])  
After block3: torch.Size([128, 64, 13, 13])  
After block4: torch.Size([128, 1, 28, 28])
</code></pre>
<p>The Generator code is below. Here z_dim is 74 but was initially 64. It was appended with 10 class labels as shown below.</p>
<pre><code>fake_noise = get_noise(cur_batch_size, z_dim, device=device) 
noise_and_labels = combine_vectors(fake_noise, one_hot_labels)
fake = gen(noise_and_labels)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>class Generator(nn.Module):
    '''
    Generator Class
    Values:
        z_dim: the dimension of the noise vector, a scalar
        im_chan: the number of channels of the output image, a scalar
              (MNIST is black-and-white, so 1 channel is your default)
        hidden_dim: the inner dimension, a scalar
    '''
    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):
        super(Generator, self).__init__()
        self.z_dim = z_dim
        # Build the neural network
        self.block1 = self.make_gen_block(z_dim, hidden_dim * 4)
        self.block2 = self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1)
        self.block3 = self.make_gen_block(hidden_dim * 2, hidden_dim)
        self.block4 = self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True)

    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=1, final_layer=False):
        '''
        Function to return a sequence of operations corresponding to a generator block of DCGAN;
        a transposed convolution, a batchnorm (except in the final layer), and an activation.
        Parameters:
            input_channels: how many channels the input feature representation has
            output_channels: how many channels the output feature representation should have
            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)
            stride: the stride of the convolution
            final_layer: a boolean, true if it is the final layer and false otherwise
                      (affects activation and batchnorm)
        '''
        if not final_layer:
            return nn.Sequential(
                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
                nn.BatchNorm2d(output_channels),
                nn.LeakyReLU(0.2, inplace=True),
            )
        else:
            return nn.Sequential(
                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
                nn.Tanh(),
            )

    def forward(self, noise):
        '''
        Function for completing a forward pass of the generator: Given a noise tensor,
        returns generated images.
        Parameters:
            noise: a noise tensor with dimensions (n_samples, input_dim)
        '''
        x = noise.view(len(noise), self.z_dim, 1, 1)
        print(f'Gen: {x.shape}')
        x = self.block1(x)
        print(f'After block1: {x.shape}')
        x = self.block2(x)
        print(f'After block2: {x.shape}')
        x = self.block3(x)
        print(f'After block3: {x.shape}')
        x = self.block4(x)
        print(f'After block4: {x.shape}')
        return x

def get_noise(n_samples, z_dim, device='cpu'):
    '''
    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)
    creates a tensor of that shape filled with random numbers from the normal distribution.
    Parameters:
      n_samples: the number of samples to generate, a scalar
      z_dim: the dimension of the noise vector, a scalar
      device: the device type
    '''
    return torch.randn(n_samples, z_dim, device=device)
</code></pre>
<p>According to the formula <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#:%7E:text=output.%20Default%3A%20True-,Shape%3A,-Input%3A"" rel=""nofollow noreferrer"">here</a>, the result after the first block will be <code>(1 + 2x0 -1x(3-1) -1)/2 +1 = 0</code> but it shows 3x3. What am I doing wrong here?</p>
","0","Question"
"78950394","","<p>I would like two torch.nn.Module classes to share part of their architecture and weights, as in the example below:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn

class SharedBlock(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()

        self.block = nn.Sequential(
            # Define some block architecture here...
        )

    def forward(self, x):
        return self.block(x)

class MyNestedModule(nn.Module):
    def __init__(self, shared_block: nn.Module, *args, **kwargs):
        super().__init__()

        self.linear = nn.Linear(...)
        self.shared_block = shared_block

    def forward(self, x):
        return self.shared_block(self.linear(x))

class MyModule(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()

        
        # SHOULD THIS BE:
        shared_block = SharedBlock(*args, **kwargs)
        # OR:
        self.shared_block = SharedBlock(*args, **kwargs)  # Note: self.
        # ...AND WHAT IS THE DIFFERENCE, IF ANY?


        self.nested1 = MyNestedModule(shared_block, *args, **kwargs)
        self.nested2 = MyNestedModule(shared_block, *args, **kwargs)

    def forward(self, x):
        x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
        y_1 = self.nested1(x_1)
        y_2 = self.nested2(y_2)
        return y_1, y_2
</code></pre>
<p>I would like to know whether <code>shared_block</code> should be an object parameter of <code>MyModule</code>. I assume it does not, since it is set as an object parameter in both the <code>MyNestedModule</code> class objects so it should be registered in torch grad but if I did create it as an object parameter in <code>MyModule</code> what would happen?</p>
","0","Question"
"78953273","","<p>I'm trying to import a model from my local but everytime I get the same error from gcp logs. The framework is scikit-learn
<code>AttributeError: Can't get attribute 'preprocess_text' on &lt;module 'model_server' from '/usr/app/model_server.py'&gt; </code>
The code snippet with this problem is</p>
<pre><code>complaints_clf_pipeline = Pipeline(
    [
        (&quot;preprocess&quot;, text.TfidfVectorizer(preprocessor=utils.preprocess_text, ngram_range=(1, 2))),
        (&quot;clf&quot;, naive_bayes.MultinomialNB(alpha=0.3)),
    ]
)
</code></pre>
<p>this</p>
<pre><code>preprocess_text 
</code></pre>
<p>comes from the cell above, but I keep receiving this issue with model_server which is not present on my code.</p>
<p>Can someone help?</p>
<p>I tried to refactor the code but got the same error, tried to undo this pipeline structure but then I got another error while trying to consult the model by API.</p>
","1","Question"
"78958361","","<p>I am using RStudio and tidymodels in an R markdown document. I would like to incorporate some models from scikit-learn. Getting data from the R code chunks to the Python code chunk works well, but when I train and test a model using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

log_reg_pipe = Pipeline([
  ('Logistic Regression', LogisticRegression())
])

log_reg_pipe.fit(X_train, y_train).score(X_val, y_val)
</code></pre>
<p>I get the error</p>
<pre><code>DataConversionWarning: A column-vector y was passed when a 1d array was expected. 
Please change the shape of y to (n_samples, ), for example using ravel().
</code></pre>
<p>I can solve it by training the data using <code>y_train['clinical_course'].to_numpy()</code>, but I would ideally like this to be done directly in the pipeline. Is this possible?</p>
<p>Note that the code above is just a simple example to show my problem. In this case <code>X_train</code> has four columns and <code>y_train</code> has one.</p>
<p>As described above I tried to use <code>.to_numpy()</code>, but I would like a solution that does all the transformations within the pipeline.</p>
","1","Question"
"78959131","","<p>I'm encountering an issue when using the VLLM library in Python. Specifically, when I create a VLLM model object inside a function, I run into memory problems and cannot clear the GPU memory effectively, even after deleting objects and using <code>torch.cuda.empty_cache()</code>.</p>
<p>The problem occurs when I try to instantiate a <code>LLM</code> object inside a function, but it does not happen if I instantiate the object in the parent process or global scope. This suggests that VLLM has issues with creating and managing objects in functions, which leads to memory retention and GPU exhaustion.</p>
<p>Here’s a simplified version of the code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, sampling_params, path_2_eval_dataset):
    # Instantiate LLM in a function
    llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

    # Run some VLLM inference or evaluation here (simplified)
    result = llm.generate([path_2_eval_dataset], sampling_params)

    # Clean up after inference
    del llm
    gc.collect()
    torch.cuda.empty_cache()

# After this, GPU memory is not cleared properly and causes OOM errors
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()
</code></pre>
<p>but</p>
<pre class=""lang-py prettyprint-override""><code>llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)
</code></pre>
<p>Works.</p>
<p>Even after explicitly deleting the LLM object and clearing the cache, the GPU memory is not properly freed, leading to out-of-memory (OOM) errors when trying to load or run another model in the same script.</p>
<p>Things I've Tried:</p>
<ul>
<li>Deleting the LLM object with del.</li>
<li>Running gc.collect() to trigger Python's garbage collection.</li>
<li>Using torch.cuda.empty_cache() to clear CUDA memory.</li>
<li>Ensuring no VLLM objects are instantiated in the parent process.</li>
</ul>
<p>None of these seem to fix the issue when the LLM object is created within a function.</p>
<p>Questions:</p>
<ul>
<li>Has anyone encountered similar memory issues when creating VLLM objects inside functions?</li>
<li>Is there a recommended way to manage or clear VLLM objects in a function to prevent GPU memory retention?</li>
<li>Are there specific VLLM handling techniques that differ from standard Hugging Face or PyTorch models in this context?</li>
</ul>
","1","Question"
"78959447","","<p><strong>Problem:</strong> In one of my programs, I need to calculate a matrix multiplication <code>A @ B</code> where both are of size N by N for considerably large N. I'm conjecturing that approximating this product by using <code>band_matrix(A, width) @ B</code> could just suffice the needs, where <code>band_matrix(A, width)</code> denotes a band matrix part of <code>A</code> with width <code>width</code>. For example, <code>width = 0</code> gives the diagonal matrix with diagonal elements taken from <code>A</code> and <code>width = 1</code> gives the tridiagonal matrix taken in a similar manner.</p>
<p><strong>My try:</strong> I'm trying to extract the tridiagonal matrix, for instance, in the following way:</p>
<pre class=""lang-py prettyprint-override""><code># Step 1: Extract the main diagonal
main_diag = torch.diagonal(A, dim1=-2, dim2=-1)  # Shape: [d1, d2, N]

# Step 2: Extract the upper diagonal (offset=1)
upper_diag = torch.diagonal(A, offset=1, dim1=-2, dim2=-1)  # Shape: [d1, d2, N-1]

# Step 3: Extract the lower diagonal (offset=-1)
lower_diag = torch.diagonal(A, offset=-1, dim1=-2, dim2=-1)  # Shape: [d1, d2, N-1]

# Step 4: Reconstruct the tridiagonal matrix
# Main diagonal
tridiag = torch.diag_embed(main_diag)  # Shape: [d1, d2, N, N]

# Upper diagonal (shift the values to create the first upper diagonal)
tridiag += torch.diag_embed(upper_diag, offset=1)

# Lower diagonal (shift the values to create the first lower diagonal)
tridiag += torch.diag_embed(lower_diag, offset=-1)
</code></pre>
<p>but I'm not sure if <code>tridiag @ B</code> would be much more efficient than the original <code>A @ B</code> or just the same complexity since Torch may not know the specific structure to <code>tridiag</code>. In theory, computation with a tridiagonal matrix should be <code>N</code> times faster.</p>
<hr />
<p>Any help with understanding PyTorch's behaviour in this type of scenario or implementing some alternative GPU optimized approaches would be greatly appreciated.</p>
","0","Question"
"78961133","","<p>I'm learning about perceptrons in class and how to use backpropagation to train the model. I'm currently having trouble with my implementation because it's only providing me a 50% accuracy rate with the data I'm given, while the majority of others in my class is getting a 90% rate. Is there something I'm overlooking in my implementation? This is what I have so far from the sources I've looked at.</p>
<pre class=""lang-py prettyprint-override""><code>class Perceptron():
    def __init__(self, num_features):
        self.num_features = num_features
        self.weights = np.random.rand(num_features) * 0.1 # This makes an array filled w/ zeros with the shape of num_features
        self.bias = 0.0
        
    def forward(self, x):
        linear = np.dot(x, self.weights) + self.bias
        print(linear)
        predictions = np.where(linear &gt; 0, 1, 0)
        return predictions
        
    def backward(self, x, y, predictions):
        errors = y - predictions
        self.weights += self.learning_rate * np.dot(x.T, errors)
        self.bias += self.learning_rate * np.sum(errors)
        return errors
        
    def train(self, x, y, epochs, learning_rate = 0.01):
        self.learning_rate = learning_rate
        for e in range(epochs):
            for i in range(y.shape[0]):
                x_i, y_i = x[i], y[i]
                prediction = self.forward(x_i)
                self.backward(x_i, y_i, prediction)
                
    def evaluate(self, x, y):
        predictions = self.forward(x)
        accuracy = np.mean(predictions == y)
        return accuracy
</code></pre>
<p>So far, I've tried different learning rates and asking others in my class, which to be quite honest, hasn't really changed the outcome of my implementation. I'm expecting a ~90% accuracy rate, but I'm only getting a 50% accuracy rate.</p>
<p>Here is some sample data:</p>
<pre><code>0.77    -1.14   0
-0.33   1.44    0
0.91    -3.07   0
-0.37   -1.91   0
-1.84   -1.13   0
-1.50   0.34    0
-0.63   -1.53   0
-1.08   -1.23   0
0.39    -1.99   0
-1.26   -2.90   0
-5.27   -0.78   0
-0.49   -2.74   0
1.48    -3.74   0
-1.64   -1.96   0
0.45    0.36    0
-1.48   -1.17   0
-2.94   -4.47   0
-2.19   -1.48   0
0.02    -0.02   0
-2.24   -2.12   0
-3.17   -3.69   0
-4.09   1.03    0
-2.41   -2.31   0
-3.45   -0.61   0
-3.96   -2.00   0
-2.95   -1.16   0
-2.42   -3.35   0
-1.74   -1.10   0
-1.61   -1.28   0
-2.59   -2.21   0
-2.64   -2.20   0
-2.84   -4.12   0
-1.45   -2.26   0
-3.98   -1.05   0
-2.97   -1.63   0
-0.68   -1.52   0
-0.10   -3.43   0
-1.14   -2.66   0
-2.92   -2.51   0
-2.14   -1.62   0
-3.33   -0.44   0
-1.05   -3.85   0
0.38    0.95    0
-0.05   -1.95   0
-3.20   -0.22   0
-2.26   0.01    0
-1.41   -0.33   0
-1.20   -0.71   0
-1.69   0.80    0
-1.52   -1.14   0
3.88    0.65    1
0.73    2.97    1
0.83    3.94    1
1.59    1.25    1
3.92    3.48    1
3.87    2.91    1
1.14    3.91    1
1.73    2.80    1
2.95    1.84    1
2.61    2.92    1
2.38    0.90    1
2.30    3.33    1
1.31    1.85    1
1.56    3.85    1
2.67    2.41    1
1.23    2.54    1
1.33    2.03    1
1.36    2.68    1
2.58    1.79    1
2.40    0.91    1
0.51    2.44    1
2.17    2.64    1
4.38    2.94    1
1.09    3.12    1
0.68    1.54    1
1.93    3.71    1
1.26    1.17    1
1.90    1.34    1
3.13    0.92    1
0.85    1.56    1
1.50    3.93    1
2.95    2.09    1
0.77    2.84    1
1.00    0.46    1
3.19    2.32    1
2.92    2.32    1
2.86    1.35    1
0.97    2.68    1
1.20    1.31    1
1.54    2.02    1
1.65    0.63    1
1.36    -0.22   1
2.63    0.40    1
0.90    2.05    1
1.26    3.54    1
0.71    2.27    1
1.96    0.83    1
2.52    1.83    1
2.77    2.82    1
4.16    3.34    1
</code></pre>
<p>Before using the perceptron model, this code is first randomized and then split into 2 parts: 2/3 of the original data into training and the other 1/3 into testing. After that, z-score standardization is performed the first 2 features of the training and testing datasets.</p>
<p>This is how I'm using the class:</p>
<pre class=""lang-py prettyprint-override""><code>perceptron = Perceptron(num_features = 2)
perceptron.train(combined_x_train[:, :2], combined_x_train[:, 2], epochs = 5, learning_rate=0.1)
accuracy = perceptron.evaluate(x_train, y_train)
print(f'Final Accuracy: {accuracy * 100:.2f}%')
</code></pre>
","-1","Question"
"78963755","","<p>I’m trying to understand why PyTorch’s <code>nn.Linear(in_features, out_features)</code> layer has its weight matrix with the shape <code>(out_features, in_features)</code> instead of <code>(in_features, out_features)</code>.</p>
<p>From a basic matrix multiplication perspective, it seems like having the shape <code>(in_features, out_features)</code> would eliminate the need for transposing the weight matrix during multiplication. For example, with an input tensor <code>x</code> of shape <code>(batch_size, in_features)</code>, the multiplication with a weight matrix of shape <code>(in_features, out_features)</code> would result directly in an output of shape <code>(batch_size, out_features)</code>, without requiring the transpose operation.</p>
<p>However, PyTorch defines the weight matrix as <code>(out_features, in_features)</code>, meaning it gets transposed during the forward pass. What is the benefit of this design? How does it align with the broader principles of linear algebra and neural network implementations? Are there any efficiency or consistency considerations behind this choice that make it preferable?</p>
","1","Question"
"78969286","","<p>I'm working on expanding the background of an image using OpenCV in Python. My current approach involves replicating the border and applying a Gaussian blur to the extended areas to blend them into the original image. The goal is to make the background extension look more natural, especially for images with consistent textures.</p>
<p>Here's the code I'm currently using:</p>
<pre><code>import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
    img = cv2.imread(image_path)
    original_h, original_w = img.shape[:2]

    expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)
    
    blurred_img = expanded_img.copy()

    if top &gt; 0:
        blurred_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)
    
    if bottom &gt; 0:
        blurred_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)
    
    if left &gt; 0:
        blurred_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)
    
    if right &gt; 0:
        blurred_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

    cv2.namedWindow(&quot;Smart Blended Expanded Image&quot;, cv2.WINDOW_NORMAL)
    cv2.namedWindow(&quot;Orig Image&quot;, cv2.WINDOW_NORMAL)
    cv2.imwrite('expanded_smart_blended_image.jpg', blurred_img)
    cv2.imshow('Smart Blended Expanded Image', blurred_img)
    cv2.imshow(&quot;Orig Image&quot;, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


expand_image_with_smart_blend('test_img.jpg', top=100, bottom=100, left=100, right=100)
</code></pre>
<p><strong>What I've tried:</strong>
cv2.BORDER_REPLICATE: I use this to replicate the edges of the original image into the newly expanded areas.
Gaussian blur: Applied to the expanded regions to soften the transitions between the original image and the new areas.</p>
<p><strong>The issue:</strong>
The results are somewhat acceptable, but the transitions still don't look as natural as I'd like. In particular:</p>
<p>Over-blurring in some areas makes the background look unrealistic.
The replication of the edges doesn't always work well for images with more complex textures.</p>
<p><a href=""https://i.sstatic.net/EPnoD7ZP.jpg"" rel=""nofollow noreferrer"">Original Image</a>    <a href=""https://i.sstatic.net/bmLJSpqU.jpg"" rel=""nofollow noreferrer"">Result Image</a></p>
<p><strong>Question:</strong>
Is there a more sophisticated approach to expanding the background of an image in OpenCV or other libraries that would give more natural, seamless results? I'm open to approaches involving advanced image processing techniques or machine learning. Any approach with Diffusion Models would also work.</p>
","0","Question"
"78969962","","<p>I probably missed something, but here is the same workflow with Pytorch and Tensorflow Keras.</p>
<p>The results are here:</p>
<p>The PyTorch version</p>
<p><a href=""https://i.sstatic.net/V4n1bSth.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/V4n1bSth.png"" alt=""With pytorch"" /></a></p>
<p>The Keras version:</p>
<p><a href=""https://i.sstatic.net/19lwOpX3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/19lwOpX3.png"" alt=""with tensorflow keras"" /></a></p>
<p>Hard to explain the whole process but this is what I do with PyTorch:</p>
<p>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</p>
<pre class=""lang-py prettyprint-override""><code>model = models.segmentation.deeplabv3_resnet50(weights=&quot;DEFAULT&quot;).to(device)

# Mettre le modèle en mode évaluation
model.eval()

# Transformations similaires à celles utilisées dans Keras
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((img_size, img_size)),
])

def t_read_image(imagepath: Image.Image):
    image = transform(imagepath)
    # make the image to be from -1 to 1
    print(image.min(), image.max())
    image = image * 2 - 1
    image = image.to(device)
    return image

def t_infer(model, image):
    with torch.no_grad():
        output = model(image.unsqueeze(0))

    output = output['out']
    output = np.squeeze(output.cpu().numpy())
    
    output = output[1:]
    output = np.argmax(output, axis=0)
    return output

def t_decode_segmentation_masks(mask, colormap, n_classes):
    r = np.zeros_like(mask).astype(np.uint8)
    g = np.zeros_like(mask).astype(np.uint8)
    b = np.zeros_like(mask).astype(np.uint8)
    for l in range(0, n_classes):
        idx = mask == l
        r[idx] = colormap[l, 0]
        g[idx] = colormap[l, 1]
        b[idx] = colormap[l, 2]
    rgb = np.stack([r, g, b], axis=2)
    return rgb

def t_get_overlay(image, colored_mask):
    image = image.cpu().numpy()
    image = (image - image.min()) / (image.max() - image.min()) * 255
    image = image.astype(np.uint8)
    image = np.transpose(image, (1, 2, 0))
    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)
    return overlay

def t_segmentation(input_image: Image.Image):
    image_tensor = t_read_image(input_image)
    prediction_mask = t_infer(image=image_tensor, model=model)
    prediction_colormap = t_decode_segmentation_masks(prediction_mask, colormap, 20)
    overlay = t_get_overlay(image_tensor, prediction_colormap)
    return (overlay, prediction_colormap)


img = Image.open('./image.jpg')
img = np.array(img)
overlay, segs = t_segmentation(img)
plt.imshow(overlay)
plt.show()
</code></pre>
<p>And the same thing with Keras</p>
<pre class=""lang-py prettyprint-override""><code>model = from_pretrained_keras(&quot;keras-io/deeplabv3p-resnet50&quot;)

def read_image(image):
    image = tf.convert_to_tensor(image)
    image.set_shape([None, None, 3])
    image = tf.image.resize(images=image, size=[img_size, img_size])
    image = image / 127.5 - 1
    return image

def infer(model, image_tensor):
    predictions = model.predict(np.expand_dims((image_tensor), axis=0))
    predictions = np.squeeze(predictions)
    predictions = np.argmax(predictions, axis=2)
    return predictions

def decode_segmentation_masks(mask, colormap, n_classes):
    r = np.zeros_like(mask).astype(np.uint8)
    g = np.zeros_like(mask).astype(np.uint8)
    b = np.zeros_like(mask).astype(np.uint8)
    for l in range(0, n_classes):
        idx = mask == l
        r[idx] = colormap[l, 0]
        g[idx] = colormap[l, 1]
        b[idx] = colormap[l, 2]
    rgb = np.stack([r, g, b], axis=2)
    return rgb

def get_overlay(image, colored_mask):
    image = tf.keras.preprocessing.image.array_to_img(image)
    image = np.array(image).astype(np.uint8)
    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)
    return overlay

def segmentation(input_image):
    image_tensor = read_image(input_image)
    prediction_mask = infer(image_tensor=image_tensor, model=model)
    prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)
    overlay = get_overlay(image_tensor, prediction_colormap)
    return (overlay, prediction_colormap)

img = Image.open('./image.jpg')
img = np.array(img)
overlay, segs = segmentation(img)
plt.imshow(overlay)
plt.show()
</code></pre>
<p>As you can see, this is the same model, and globally the same code. But the result is very different. What I would like is to get the same result than with Keras, using Pytorch.</p>
<p>I share the notebook here: <a href=""https://colab.research.google.com/drive/1mgWSRs4Z7lqag8vxBnq5vi65BohXNdMd?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1mgWSRs4Z7lqag8vxBnq5vi65BohXNdMd?usp=sharing</a></p>
<p>Thanks a lot.</p>
<p>EDIT, using normalization from imagenet, the result is better but not the excpected one:</p>
<p><a href=""https://i.sstatic.net/xFcNgpri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFcNgpri.png"" alt=""Pytorch With imagenet norm"" /></a></p>
<p>It seems that the model is not exactly the same. Keras uses DeepLabV3Plus while PyTorch is using DeepLabV3.</p>
<p>So, is there a difference ? The model proposes 20 classes for both implementations.</p>
","0","Question"
"78974474","","<p>I want to make a classifier for text, which is further use to suggest the most similar text for a one given.</p>
<p>The flow of the app is the following:</p>
<ul>
<li>extract the main 10 topics from the text, using a <code>llm</code> (it can choose from a 150 words pool)</li>
<li>I make the word vector a binary vector, basically working in a 150 dimensional space, where each text have a coordinate like this <code>[1, 0, 1, ..., 0]</code></li>
<li>then I find the closest neighbour (I want to extend to 3-5, but for simplicity, let's assume it is only one) using <code>cosine</code> distance</li>
<li>I receive the closest text</li>
</ul>
<p>The problem is that the texts are pretty different, and the <code>llm</code> gives the topics pretty well, but the suggested texts are not exactly what I was expecting. I tried to order the topics based on importance and make the vector non-binary (<code>[10, 0, 0, 9, ..., 1]</code>), but that didn't seem to help a lot.</p>
<p>I was wondering wheter this approach is not good for my problem, or if I should use other parameters or anything else for grouping my texts.</p>
","-1","Question"
"78975293","","<p>I have a model that adds numbers between -10 to positive 10. The task is simply to predict the sum of those numbers. However the train accuracy quickly reaches 100%.<br />
I am not sure if the model is just quickly training, or if there is something wrong and it is not properly learning. Can anyone give some insight why this happens here?</p>
<p>Here is my code and minimal working example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

# Create the dataset
data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
  x = np.random.randint(-10, 10)
  y = np.random.randint(-10,10)
  bothNumber = [x, y]  # Inputs
  data.append(bothNumber)
  labels.append(x+y)  # Targets

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

# Model Factory
def createModel():
  class myModel(nn.Module):
    def __init__(self):
      super().__init__()

      self.input = nn.Linear(2,8)
      self.fc1 = nn.Linear(8,8)
      self.output = nn.Linear(8,1)

    def forward(self,x):
      x = F.relu( self.input(x) )
      x = F.relu( self.fc1(x) )
      return self.output(x)

  net = myModel()
  lossfun = nn.MSELoss()
  optimizer = torch.optim.SGD(net.parameters(),lr=.001)

  return net,lossfun,optimizer


def trainModel():

  numepochs = 100
  net,lossfun,optimizer = createModel()
  losses   = torch.zeros(numepochs)
  trainacc = []
  testacc = []

  for epochi in range(numepochs):
    batchLoss = []

    for X,y in train_loader:
      X = X.float()
      y = y.float()
      yHat = net(X)

      loss = lossfun(yHat,y)
      batchLoss.append(loss.item())

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    losses[epochi] = np.mean(batchLoss)

    with torch.no_grad():
      train_predictions = []
      train_labels = []
      for x_train, y_train in train_loader:
        x_train = x_train.float()
        y_train = y_train.float()
        train_pred = net(x_train)
        train_predictions.append(train_pred)
        train_labels.append(y_train)

      train_predictions = torch.cat(train_predictions)
      train_labels = torch.cat(train_labels)

      train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
      trainacc.append(train_acc.item())


    X,y = next(iter(test_data))
    X = X.float()  # Convert X to float for test data
    y = y.float()  # Convert y to float for test data
    with torch.no_grad():
      yHat = net(X)

    testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

  return trainacc,testacc,losses,net


trainAcc, testAcc, losses , net = trainModel()
</code></pre>
","2","Question"
"78976058","","<p>I'm trying to create a ML data asset and I'm following this link: <a href=""https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html"" rel=""nofollow noreferrer"">https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html</a>
to get it done but I am constantly getting this error
<a href=""https://i.sstatic.net/I8InVrWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I8InVrWk.png"" alt=""enter image description here"" /></a></p>
<p>I know my files are not the exact ones in the examples but even when I use those files I'm getting the same error.So I'm not sure what the problem is since this my first time using ML in Azure.</p>
","-1","Question"
"78976316","","<p>I have tried to run this model from the link:<br />
<a href=""https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/notebook"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/notebook</a></p>
<p>When I did it on colab with similar dataset (but smaller, 2100 img to 6 classes), it works well. But when I added this code to predict the input image:</p>
<pre><code>from google.colab import files
from PIL import Image


def process_uploaded_image(image_path, target_size=(224, 224)):
    img = Image.open(image_path)
    img = img.resize(target_size)  
    img_array = np.array(img) 

    if img_array.shape[-1] == 4:  
        img_array = img_array[..., :3]

    img_array = img_array / 255.0 
    img_array = np.expand_dims(img_array, axis=0)  
    img_array = mobilenetv2.preprocess_input(img_array) 

    return img_array

uploaded = files.upload()

for fn in uploaded.keys():  
    processed_image = process_uploaded_image(fn, target_size=IMAGE_SIZE)    
    preds = model.predict(processed_image)
    pred_class = np.argmax(preds, axis=1)
  
    plt.imshow(Image.open(fn))  # Display the uploaded image
    plt.title(f'Predicted class: {categories[pred_class[0]]}')
    plt.axis('off')
    plt.show()
    print(f'File {fn} is predicted as: {categories[pred_class[0]]}')
</code></pre>
<p>The result is a wrong prediction. For example, the model always predict my input as a &quot;trash&quot; class. It will change to another class when I stop the runtime, but it still in a wrong prediction too.<br />
I also added this code to check the prediction probabilities:</p>
<pre><code>preds = model.predict(processed_image)
pred_probs = preds[0]  # Get the prediction probabilities for the first (and only) batch
print(&quot;Prediction probabilities:&quot;, pred_probs)
pred_class = np.argmax(pred_probs)
print(&quot;Predicted class:&quot;, categories[pred_class])
</code></pre>
<p>the output:</p>
<pre><code>**1/1** ━━━━━━━━━━━━━━━━━━━━ **0s** 24ms/step Prediction probabilities: \[0.31027108 0.12315894 0.47848797 0.00863316 0.07789086 0.00155797\]   
Predicted class: metal  
</code></pre>
<p>Why is this happening, and how can my model predict the result correctly?</p>
","0","Question"
"78981288","","<p>I'm working on a <strong>multilabel classification</strong> problem using the <strong>ClassifierChain</strong> approach with <strong>RandomForestClassifier</strong> as the base estimator. I've encountered an issue where my input matrix X contains np.nan values. When using RandomForestClassifier alone, it handles NaN values without any problem, as it natively supports missing values via its internal tree splitting mechanism.</p>
<p>This is confusing to me because the base estimator (RandomForestClassifier) does handle NaN values correctly. I don't understand why ClassifierChain, which is just a wrapper, raises this error when the underlying classifier doesn't have an issue with NaNs.</p>
<p>When I train a simple RandomClassifier it does handle np.nan:</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
import numpy as np

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
y_single_label = [0, 0, 1, 1]

tree = RandomForestClassifier(random_state=0)
tree.fit(X, y_single_label)
X_test = np.array([np.nan]).reshape(-1, 1)
tree.predict(X_test)
</code></pre>
<p>Even when I use MultiOutputClassifier instead of ClassifierChain (which doesn't model dependencies between labels), the training proceeds without any errors, even with NaNs in the input - as expected.</p>
<pre><code>import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain , MultiOutputClassifier

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)

# Two label columns for multilabel classification
y = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])

# Base classifier
base_clf = RandomForestClassifier()

# MultiOutputClassifier (Binary Relevance) with the base classifier
clf_BR = MultiOutputClassifier(base_clf)

# Fitting the model
clf_BR.fit(X, y)
</code></pre>
<p>However, when I switch to the ClassifierChain approach:</p>
<pre><code># Classifier chain with the base classifier
clf_chain = ClassifierChain(base_clf)

# Fitting the model
clf_chain.fit(X, y)
</code></pre>
<p>I get the following error during hyperparameter tuning:</p>
<blockquote>
<p>Trial 0 failed with parameters: {'n_estimators': 30, 'max_depth': 16, 'max_samples': 0.4497444900238575, 'max_features': 550, 'order_type': 'random'} because of the following error: ValueError('Input X contains NaN.\nClassifierChain does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See <a href=""https://scikit-learn.org/stable/modules/impute.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/impute.html</a> You can find a list of all estimators that handle NaN values at the following page: <a href=""https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values%27"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values'</a>)</p>
</blockquote>
<p>Since it's important for us to keep the missing values as they are and not impute or drop them, I'm wondering if there's a way to make ClassifierChain work with missing values. Is there any workaround or something I'm missing here?</p>
<p>Here are my environment details:</p>
<ul>
<li>Python version: 3.12.5 (packaged by conda-forge)</li>
<li>scikit-learn version: 1.5.1</li>
</ul>
","2","Question"
"78985137","","<p>I have a model that I was reading from huggingface using the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)
</code></pre>
<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>
<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>
<p>So simply something like</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)

model.device_map = &quot;auto&quot;
</code></pre>
","2","Question"
"78991472","","<p>I'm looking at a some food waste data where I have a fair bit of data including the Ingredients for what was in the food. I'm trying to do some ML on the data, and I'm having some trouble getting it ready.</p>
<p><a href=""https://i.sstatic.net/TMVWR8nJ.png"" rel=""nofollow noreferrer"">example</a></p>
<p>I am able to separate all of individual ingredients into there own column, but I am struggling marking a 1 when that ingredient is in the list/column of Ingredients.</p>
<p><a href=""https://i.sstatic.net/Lhn3jVUd.png"" rel=""nofollow noreferrer"">example2</a></p>
<p>I have been trying something along the lines of going row by row and seeing if the column name is in the Ingredients column then changing that to a 1. I haven't got very close at all, but this is the current disaster I have been trying. Also have experimented with get_dummies and a few others.</p>
<p>Is there an easier way of doing this?</p>
<pre><code>def xs_os(df, Ingredients_Column):
    df2 = df.drop(&quot;Ingredients&quot;, axis = 0) 
    for z in df:
        for x in list(df2.columns.values):
            if x in str(Ingredients_Column):
                df.at[z, df[x]] = 1
xs_os(df, df['Ingredients'])
df.head()
</code></pre>
","0","Question"
"78992679","","<p>I am using the MobileNetV2 model as my base model, but I added 5 new trainable layers and I'm re-training it on two new categories: monkeypox positive and monkeypox negative. The monkeypox negative image folder includes different skin infections that look similar to monkeypox but are infections like chickenpox. I have a dataset with colored images, but I converted those to gray scale(code below).</p>
<p>When I tried to run the model, I got the issue of</p>
<pre><code>Input 0 of layer &quot;Conv1&quot; is incompatible with the layer: expected axis -1 of input shape to 
have value 3, but received input with shape (None, 200, 200, 1)
</code></pre>
<p>I know I have to trick the model into thinking the value of 1 is 3, but I'm not sure how. I am using imagenet for the weights.</p>
<pre><code>from tensorflow.keras.preprocessing import image as IMAGE
import numpy as np
import cv2
import os
os.environ[&quot;CUDA DEVICE ORDER&quot;]=&quot;PCI BUS ID&quot;
os.environ[&quot;CUDA VISIBLE DEVICES&quot;]=&quot;0&quot;

from tensorflow.keras import Model
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
</code></pre>
<pre><code>import tensorflow as tf
from PIL import Image
from google.colab.patches import cv2_imshow
</code></pre>
<pre><code>!pip3 install tensorflowjs
</code></pre>
<pre><code>import tensorflowjs as tfjs
</code></pre>
<pre><code>train = ImageDataGenerator(rescale = 1/255)
test = ImageDataGenerator(rescale = 1/255)
val = ImageDataGenerator(rescale = 1/255)
</code></pre>
<pre><code>training_set = train.flow_from_directory(&quot;/content/drive/MyDrive/mpox_data/Train&quot;,
                                         target_size=(200,200),
                                         batch_size = 15,
                                         class_mode = &quot;categorical&quot;,
                                         color_mode='rgb'
                                        )

testing_set = test.flow_from_directory(&quot;/content/drive/MyDrive/mpox_data/Test&quot;,
                                          target_size=(200,200),
                                          batch_size = 15,
                                          class_mode = &quot;categorical&quot;,
                                       color_mode='rgb'
                                        )
validation_set = val.flow_from_directory(&quot;/content/drive/MyDrive/mpox_data/Val&quot;,
                                         target_size=(200,200),
                                         batch_size=15,
                                         class_mode = &quot;categorical&quot;,
                                         color_mode='rgb'
                                         )
</code></pre>
<pre><code>x, y = training_set[0]
a, b = testing_set[0]
d, c = validation_set[0]
</code></pre>
<pre><code>base_model = MobileNetV2(weights = &quot;imagenet&quot;, include_top = False)

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(42, activation=&quot;relu&quot;)(x)
x = Dense(64, activation = &quot;relu&quot;)(x)
x = Dense(32, activation=&quot;relu&quot;)(x)
preds = Dense(2, activation = &quot;sigmoid&quot;)(x)

model = Model(inputs = base_model.input, outputs = preds)
</code></pre>
<pre><code>epochs = 5
optimizer = Adam(learning_rate = 0.0003)
model.compile(loss = &quot;binary_crossentropy&quot;, optimizer = optimizer, metrics = [&quot;Accuracy&quot;])
model.fit(training_set, validation_data = validation_set, epochs = epochs, shuffle=True)
</code></pre>
<p>I tried to use grayscale images with the model, expecting to have it run successfully, but it gave me the error mentioned above.</p>
<p>Why does my program fail to work and how do I fix it, allowing the model to continue training itself on the grayscale images?</p>
","2","Question"
"78995310","","<p>I am building a Minimum Spanning Trees model, and it succeeded. I generated a plot and wanted to identify which alternative data points are connected for each data point. Is there a way to do that?</p>
<p>The modeling code is as below.</p>
<pre><code>data(iris)
mst.mod &lt;- ape::mst(dist(iris))
plot(mst.mod)
</code></pre>
<p>The tree is visualized. It looks a bit messy but I want to identify, for example, which instances are connected with instance 1 and so on. Visually, it can be seen that instance has an edge with instances 28 and 40. But is there a R code to find them all for each data point?
<a href=""https://i.sstatic.net/zIZfp65n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zIZfp65n.png"" alt=""enter image description here"" /></a></p>
","1","Question"
"78995759","","<p>I am unable to create calibration plots for my survival analysis project. (oesophageal cancer dataset)</p>
<p>I have finalised my model (AORSF) after tuning:</p>
<pre><code>aorsf_fit &lt;- last_fit(
  final_aorsf_wf,
  split = initial_split(final_main, prop = 0.75),
  metrics = survival_metrics,
  eval_time = time_points_complete,
)
</code></pre>
<p>I then collect my prediction set, using the following code:</p>
<pre><code>&gt; predictions &lt;- collect_predictions(aorsf_fit)
&gt; predictions  
# A tibble: 687 × 6
   .pred            .pred_time id                .row   surv .config             
   &lt;list&gt;                &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;Surv&gt; &lt;chr&gt;               
 1 &lt;tibble [8 × 3]&gt;      107.  train/test split     3  57.8+ Preprocessor1_Model1
 2 &lt;tibble [8 × 3]&gt;       96.1 train/test split     7  96.8+ Preprocessor1_Model1
 3 &lt;tibble [8 × 3]&gt;       94.2 train/test split    11 130.0  Preprocessor1_Model1
 4 &lt;tibble [8 × 3]&gt;       11.4 train/test split    15   9.0  Preprocessor1_Model1
 5 &lt;tibble [8 × 3]&gt;      102.  train/test split    16  69.1+ Preprocessor1_Model1
 6 &lt;tibble [8 × 3]&gt;       37.8 train/test split    17  33.0  Preprocessor1_Model1
 7 &lt;tibble [8 × 3]&gt;      103.  train/test split    18 142.8  Preprocessor1_Model1
 8 &lt;tibble [8 × 3]&gt;       23.4 train/test split    20  13.5  Preprocessor1_Model1
 9 &lt;tibble [8 × 3]&gt;       89.7 train/test split    21 146.5  Preprocessor1_Model1
10 &lt;tibble [8 × 3]&gt;      107.  train/test split    23  60.1+ Preprocessor1_Model1
# ℹ 677 more rows
</code></pre>
<p>I am even able to get time dependent ROC curves (thanks to the roc_curve_survival() function):</p>
<pre><code>predictions |&gt; 
  roc_curve_survival(truth = surv, .pred)|&gt; 
  filter(.eval_time == 60) |&gt; 
  ggplot(aes(1 - specificity, sensitivity)) +
  geom_line() +
  theme_minimal()
</code></pre>
<p>However, I am unable to use the '.pred' list to create a calibration plot. I have tried using the code suggested within the link: <a href=""https://www.tidymodels.org/learn/models/calibration/#other-model-types"" rel=""nofollow noreferrer"">An introduction to calibration with tidymodels</a></p>
<p>I want to be able to build the calibration plot by using the predicted survival data within the 'predictions' object. The '.pred' variable within the predictions object contains the following 3 rows:
.eval_time
.pred_survival
.weight_censored</p>
<p>I am sure this, if extracted somehow, can be used to create the calibration plots as within the link and can then be used to show how well the model functions in contrast to observed survival! (I have observed survival for my dataset as well)</p>
<p>I have tried the following only, with no results:</p>
<pre><code>predictions |&gt; 
+ ggplot(aes(.pred_survival))
</code></pre>
","0","Question"
"78996028","","<p>I was playing around with AWS Sagemaker, trained a model with some labeled data, deployed it to endpoint and set up Lambda for serving predictions.</p>
<p>All good, but I want to re-train my model regularly, using, say, 1-week historical data.</p>
<p>But my historical data is unlabeled which means it cannot be used for training. How do I label it?</p>
<p>I initially thought that I can just use my model's predictions to label new (unlabeled) data but I have read that this not a good idea because it would simply assure my model about its accuracy, even though it might be far from being accurate.</p>
<p>So where do I get the labels for my historical data?</p>
<p>And if historical data cannot be labeled by model, then does it mean that it is supposed to be labeled manually? In that case, what's the point of training and serving a model then?</p>
<p>As an example, let's take a fraudulent transaction detection. Ok, there is some initial data, labeled manually by someone who knows exactly if the transaction was fraudulent or not, and thus having 100% accuracy.
Is it then supposed to be periodically and manually updated with additional 100%-accurate events?</p>
","-1","Question"
"78996950","","<p>I'm trying to set up and test the OCR model from the doctr library provided by Mindee
(<code>https://github.com/mindee/doctr</code>)
within a VSCode. I installed the package in <code>run.ipynb</code> using the following command:</p>
<pre><code>!pip install -qe doctr/.
</code></pre>
<p>I then tried to run the sample OCR code provided in the repository:</p>
<pre><code>from doctr.io import DocumentFile
from doctr.models import ocr_predictor
</code></pre>
<p>However, I encountered the following error:</p>
<pre><code>    &quot;name&quot;: &quot;ImportError&quot;,
    &quot;message&quot;: &quot;DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.&quot;,
    &quot;stack&quot;: &quot;---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[3], line 2
      1 from doctr.io import DocumentFile
----&gt; 2 from doctr.models import ocr_predictor
      4 model = ocr_predictor(pretrained=True)
      5 # PDF

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\__init__.py:1
----&gt; 1 from . import io, models, datasets, contrib, transforms, utils
      2 from .file_utils import is_tf_available, is_torch_available
      3 from .version import __version__  # noqa: F401

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\models\\__init__.py:1
----&gt; 1 from .classification import *
      2 from .detection import *
      3 from .recognition import *

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\models\\classification\\__init__.py:1
----&gt; 1 from .mobilenet import *
      2 from .resnet import *
      3 from .vgg import *

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\models\\classification\\mobilenet\\__init__.py:4
      1 from doctr.file_utils import is_tf_available, is_torch_available
      3 if is_tf_available():
----&gt; 4     from .tensorflow import *
      5 elif is_torch_available():
      6     from .pytorch import *

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\models\\classification\\mobilenet\\tensorflow.py:15
     12 from tensorflow.keras import layers
     13 from tensorflow.keras.models import Sequential
---&gt; 15 from ....datasets import VOCABS
     16 from ...utils import conv_sequence, load_pretrained_params
     18 __all__ = [
     19     \&quot;MobileNetV3\&quot;,
     20     \&quot;mobilenet_v3_small\&quot;,
   (...)
     25     \&quot;mobilenet_v3_small_page_orientation\&quot;,
     26 ]

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\__init__.py:3
      1 from doctr.file_utils import is_tf_available
----&gt; 3 from .generator import *
      4 from .cord import *
      5 from .detection import *

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\generator\\__init__.py:4
      1 from doctr.file_utils import is_tf_available, is_torch_available
      3 if is_tf_available():
----&gt; 4     from .tensorflow import *
      5 elif is_torch_available():
      6     from .pytorch import *  # type: ignore[assignment]

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\generator\\tensorflow.py:8
      1 # Copyright (C) 2021-2024, Mindee.
      2 
      3 # This program is licensed under the Apache License 2.0.
      4 # See LICENSE or go to &lt;https://opensource.org/licenses/Apache-2.0&gt; for full license details.
      6 import tensorflow as tf
----&gt; 8 from .base import _CharacterGenerator, _WordGenerator
     10 __all__ = [\&quot;CharacterGenerator\&quot;, \&quot;WordGenerator\&quot;]
     13 class CharacterGenerator(_CharacterGenerator):

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\generator\\base.py:14
     11 from doctr.io.image import tensor_from_pil
     12 from doctr.utils.fonts import get_font
---&gt; 14 from ..datasets import AbstractDataset
     17 def synthesize_text_img(
     18     text: str,
     19     font_size: int = 32,
   (...)
     22     text_color: Optional[Tuple[int, int, int]] = None,
     23 ) -&gt; Image.Image:
     24     \&quot;\&quot;\&quot;Generate a synthetic text image
     25 
     26     Args:
   (...)
     36         PIL image of the text
     37     \&quot;\&quot;\&quot;

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\datasets\\__init__.py:4
      1 from doctr.file_utils import is_tf_available, is_torch_available
      3 if is_tf_available():
----&gt; 4     from .tensorflow import *
      5 elif is_torch_available():
      6     from .pytorch import *  # type: ignore[assignment]

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\datasets\\tensorflow.py:15
     11 import tensorflow as tf
     13 from doctr.io import read_img_as_tensor, tensor_from_numpy
---&gt; 15 from .base import _AbstractDataset, _VisionDataset
     17 __all__ = [\&quot;AbstractDataset\&quot;, \&quot;VisionDataset\&quot;]
     20 class AbstractDataset(_AbstractDataset):

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\datasets\\datasets\\base.py:16
     13 from doctr.io.image import get_img_shape
     14 from doctr.utils.data import download_from_url
---&gt; 16 from ...models.utils import _copy_tensor
     18 __all__ = [\&quot;_AbstractDataset\&quot;, \&quot;_VisionDataset\&quot;]
     21 class _AbstractDataset:

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\models\\utils\\__init__.py:4
      1 from doctr.file_utils import is_tf_available, is_torch_available
      3 if is_tf_available():
----&gt; 4     from .tensorflow import *
      5 elif is_torch_available():
      6     from .pytorch import *  # type: ignore[assignment]

File c:\\Users\\vishw\\Desktop\\test\\doctr\\doctr\\models\\utils\\tensorflow.py:12
      9 from zipfile import ZipFile
     11 import tensorflow as tf
---&gt; 12 import tf2onnx
     13 from tensorflow.keras import Model, layers
     15 from doctr.utils.data import download_from_url

File c:\\Python312\\Lib\\site-packages\\tf2onnx\\__init__.py:8
      3 \&quot;\&quot;\&quot;tf2onnx package.\&quot;\&quot;\&quot;
      5 __all__ = [\&quot;utils\&quot;, \&quot;graph_matcher\&quot;, \&quot;graph\&quot;, \&quot;graph_builder\&quot;,
      6            \&quot;tfonnx\&quot;, \&quot;shape_inference\&quot;, \&quot;schemas\&quot;, \&quot;tf_utils\&quot;, \&quot;tf_loader\&quot;, \&quot;convert\&quot;]
----&gt; 8 import onnx
      9 from .version import git_version, version as __version__
     10 from . import verbose_logging as logging

File c:\\Python312\\Lib\\site-packages\\onnx\\__init__.py:77
     73 from typing import IO, Literal, Union
     76 from onnx import serialization
---&gt; 77 from onnx.onnx_cpp2py_export import ONNX_ML
     78 from onnx.external_data_helper import (
     79     load_external_data_for_model,
     80     write_external_data_tensors,
     81     convert_model_to_external_data,
     82 )
     83 from onnx.onnx_pb import (
     84     AttributeProto,
     85     EXPERIMENTAL,
   (...)
    111     Version,
    112 )

ImportError: DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.&quot;
</code></pre>
<p>Details about my environment:</p>
<ul>
<li>I'm working within a Jupyter notebook (run.ipynb).</li>
<li>I'm testing the docTR model in my local Windows environment.</li>
<li>My project structure looks like this</li>
</ul>
<p><a href=""https://i.sstatic.net/M6lnohFp.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/M6lnohFp.png"" alt=""This is my project directory structure (Im testing the model in run.ipynb)"" /></a></p>
<p>Im trying to run the docTR model given in the github repo link which i've posted and i want someone to help me fix the DLL issue related to ONNX so that i can run docTR models.</p>
<p>also I want to understand:</p>
<ul>
<li>Whether this error is due to ONNX or the OCR model setup.</li>
<li>How to resolve this DLL loading issue to successfully test docTR models in my VSCode Jupyter notebook.</li>
</ul>
","6","Question"
"79000230","","<p>I have a pytorch <code>Dataset</code> subclass and I create a pytorch <code>DataLoader</code> out of it. It works when I return two tensors from DataSet's <code>__getitem__()</code> method. I tried to create minimal (but not working, more on this later) code as below:</p>
<pre><code>import torch
from torch.utils.data import Dataset
import random

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class DummyDataset(Dataset):
    def __init__(self, num_samples=3908, window=10): # same default values as in the original code
        self.window = window
        # Create dummy data
        self.x = torch.randn(num_samples, 10, dtype=torch.float32, device='cpu')  
        self.y = torch.randn(num_samples, 3, dtype=torch.float32, device='cpu')
        self.t = {i: random.choice([True, False]) for i in range(num_samples)}

    def __len__(self):
        return len(self.x) - self.window + 1

    def __getitem__(self, i):
        return self.x[i: i + self.window], self.y[i + self.window - 1] #, self.t[i]

ds = DummyDataset()
dl = torch.utils.data.DataLoader(ds, batch_size=10, shuffle=False, generator=torch.Generator(device='cuda'), num_workers=4, prefetch_factor=16)

for data in dl:
    x = data[0]
    y = data[1]
    # t = data[2]
    print(f&quot;x: {x.shape}, y: {y.shape}&quot;) # , t: {t}
    break   
</code></pre>
<p>Above code gives following error:</p>
<pre><code>RuntimeError: Expected a 'cpu' device type for generator but found 'cuda'
</code></pre>
<p>on line <code>for data in dl:</code>.</p>
<p>But my original code is exactly like above: dataset contains tensors created on <code>cpu</code> and dataloader's generator's device set to <code>cuda</code> and it works (I mean above minimal code does not work, but same lines in my original code does indeed work!).</p>
<p>When I try to return a boolean value from it by un-commenting <code>, self.t[i]</code> from <code>__get_item__()</code> method, it gives me following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/my_project/src/train.py&quot;, line 66, in &lt;module&gt;
    trainer.train_validate()
  File &quot;/my_project/src/trainer_cpu.py&quot;, line 146, in train_validate
    self.train()
  File &quot;/my_project/src/trainer_cpu.py&quot;, line 296, in train
    for train_data in tqdm(self.train_dataloader, desc=&quot;&gt;&gt; train&quot;, mininterval=5):
  File &quot;/usr/local/lib/python3.9/site-packages/tqdm/std.py&quot;, line 1181, in __iter__
    for obj in iterable:
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py&quot;, line 630, in __next__
    data = self._next_data()
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py&quot;, line 1344, in _next_data
    return self._process_data(data)
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py&quot;, line 1370, in _process_data
    data.reraise()
  File &quot;/usr/local/lib/python3.9/site-packages/torch/_utils.py&quot;, line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py&quot;, line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 55, in fetch
    return self.collate_fn(data)
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py&quot;, line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py&quot;, line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py&quot;, line 174, in &lt;listcomp&gt;
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py&quot;, line 146, in collate
    return collate_fn_map[collate_type](batch, collate_fn_map=collate_fn_map)
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py&quot;, line 235, in collate_int_fn
    return torch.tensor(batch)
  File &quot;/usr/local/lib/python3.9/site-packages/torch/utils/_device.py&quot;, line 79, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/torch/cuda/__init__.py&quot;, line 300, in _lazy_init
    raise RuntimeError(
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
</code></pre>
<p>Why is it so? Why it does not allow me to return extra boolean value from <code>__get_item__</code>?</p>
<p><strong>PS:</strong></p>
<p>Above is main question. However, I noticed some weird observations: above code (with or without <code>, self.t[i]</code> commented) starts working if I replace <code>DalaLoader</code>'s generator's device from <code>cuda</code> to <code>cpu</code> ! That is, if I replace <code>generator=torch.Generator(device='cuda')</code> with <code>generator=torch.Generator(device='cpu')</code>, it outputs:</p>
<pre><code>x: torch.Size([10, 10, 10]), y: torch.Size([10, 3])
</code></pre>
<p>And if I do the same in my original code, it gives me following error:</p>
<pre><code>RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'
</code></pre>
<p>on line <code>for data in dl:</code>.</p>
<p><strong>Update</strong></p>
<p>It started working as soon as I changed type of <code>self.t</code> from python <code>dict</code> to torch tensor of type bool and moved it to cpu:</p>
<pre><code>self.t = torch.tensor([random.choice([True, False]) for _ in range(num_samples)], dtype=torch.bool).to('cpu')
</code></pre>
<p>Please explain why.</p>
","0","Question"
"79005084","","<p>I am building MLOPs pipelines for a machine learning model. How do I access the evaluation metrics of my model in the SageMake Studio UI after registering the model?</p>
<p>Here's my sample evaluation.json that I am saving in S3</p>
<pre><code>{
    &quot;metric_groups&quot;: [
        {
            &quot;name&quot;: &quot;regression_metrics&quot;,
            &quot;metric_data&quot;: [
                {
                    &quot;name&quot;: &quot;mse&quot;,
                    &quot;value&quot;: 6107087691.96
                },
                {
                    &quot;name&quot;: &quot;mae&quot;,
                    &quot;value&quot;: 46717.104
                },
                {
                    &quot;name&quot;: &quot;rmse&quot;,
                    &quot;value&quot;: 78147.85
                },
                {
                    &quot;name&quot;: &quot;r2&quot;,
                    &quot;value&quot;: 0.90
            ]
        }
    ]
}
</code></pre>
<p>And here's my register step:</p>
<pre><code>import logging
from sagemaker.workflow.functions import Join
from sagemaker.model_metrics import MetricsSource, ModelMetrics
from sagemaker.workflow.step_collections import RegisterModel


def create_register_step(
        role,
        sagemaker_session,
        model_package_group_name,
        model_approval_status,
        training_step,
        evaluation_step
):
    
    logging.basicConfig(level=logging.INFO)
    logging.info(f'Creating the register step')

    # log evaluation_report
    logging.info(f'Evaluation Report: {evaluation_step}')

    evaluation_s3_uri = evaluation_step.properties.ProcessingOutputConfig.Outputs['evaluation'].S3Output.S3Uri

    
    model_metrics = ModelMetrics(
        model_statistics=MetricsSource(
            s3_uri=Join(
                on=&quot;/&quot;,
                values=[
                    evaluation_s3_uri,
                    &quot;evaluation.json&quot;
                ]
            ),
            content_type=&quot;application/json&quot;
        )
    )


    # Create the RegisterModel step
    register_step = RegisterModel(
        name='ModelRegisterStep',
        estimator=training_step.estimator,
        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
        content_types=[&quot;text/csv&quot;],
        response_types=[&quot;text/csv&quot;],
        inference_instances=[&quot;ml.m5.large&quot;, &quot;ml.m5.xlarge&quot;],
        transform_instances=[&quot;ml.m5.large&quot;],
        model_package_group_name=model_package_group_name,
        approval_status=model_approval_status,
        model_metrics=model_metrics
    )

    return register_step

</code></pre>
<p>My pipeline executes successfully but I cannot see the evaluation metrics
<a href=""https://i.sstatic.net/KonykZGy.png"" rel=""nofollow noreferrer"">Image attached</a></p>
<p>I have also tried manually adding evaluation report from S3 to the model version but it doesn't work</p>
","0","Question"
"79009687","","<p>I wrote the following keras model</p>
<pre><code>input_A = Input(shape=[5], name=&quot;wide_input&quot;)
hidden_layer_1 = Dense(10, activation=&quot;relu&quot;, name='h_wide_layer')(input_A)
input_B = Input(shape=[6], name=&quot;deep_input&quot;)
hidden_layer_2 = Dense(30, activation=&quot;relu&quot;, name='h_deep_layer_1')(input_B)
hidden_layer_3 = Dense(30, activation=&quot;relu&quot;, name='h_deep_layer_2')(hidden_layer_2)
concat = Concatenate()([hidden_layer_1, hidden_layer_3])
output = Dense(1, name=&quot;output&quot;)(concat)
complex_model_2_1 = keras.Model(inputs=[input_A, input_B], outputs=[output])

#Compile the second complexe model
complex_model_2_1.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;sgd&quot;)

#Train
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test_full[:, :5], X_test_full[:, 2:]
X_new_A, X_new_B = X_new[:, :5], X_new[:, 2:]
print(f&quot;Shape of X_train_A: {X_train_A.shape}&quot;)
print(f&quot;Shape of X_train_B: {X_train_B.shape}&quot;)
history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
                                y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))
</code></pre>
<p><em>X_train</em> has <strong>8 features</strong> so <em>X_train_A</em> and <em>X_train_B</em> have actually <strong>5 and 6 features</strong> respectively. But, I don't understand why I get the bellow <em>incompatible shape error for hidden_layer_2</em>:</p>
<pre><code>ValueError Traceback (most recent call last)
&lt;ipython-input-42-86b1419058f7&gt; in &lt;cell line: 11&gt;()
      9 print(f&quot;Shape of X_train_A: {X_train_A.shape}&quot;)
     10 print(f&quot;Shape of X_train_B: {X_train_B.shape}&quot;)
---&gt; 11 history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
     12                                 y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))
     13 

1 frames
/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)
    225                     None,
    226                 }:
--&gt; 227                     raise ValueError(
    228                         f'Input {input_index} of layer &quot;{layer_name}&quot; is '
    229                         f&quot;incompatible with the layer: expected axis {axis} &quot;

ValueError: Exception encountered when calling Functional.call().

Input 0 of layer &quot;h_deep_layer_1&quot; is incompatible with the layer: expected axis -1 of input shape to have value 6, but received input with shape (None, 5)

Arguments received by Functional.call():
  • inputs={'wide_input': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'deep_input': 'tf.Tensor(shape=(None, 6), dtype=float32)'}
  • training=True
  • mask={'wide_input': 'None', 'deep_input': 'None'}
</code></pre>
<p>How to fix it?</p>
<p>PS: Gemini in Google colab fails to explain the issue and proposes to me X_train_B = X_train[:, 5:] which is incorrect (having a shape of (_, 3)</p>
","1","Question"
"79009698","","<p>I'm building a restaurant recommender for my city using a Kaggle dataset and RandomForestRegressor.</p>
<p>I built the model, and now want the model to recommend a good restaurant when it is given 4 parameters: location, approx cost, type of restaurant, and number of votes. However, it is returning a value error:</p>
<pre><code>X has 8 features, but RandomForestRegressor is expecting 2924 features as input.
</code></pre>
<p>This is what I'm trying to run:</p>
<pre><code>import joblib
import numpy as np
from sklearn.preprocessing import StandardScaler

model = joblib.load('my_model.pkl')
scaler = joblib.load('scaler.pkl')

def preprocess_input(location, type_, cost, votes):
    one_hot_location = [1 if loc == location else 0 for loc in ['Whitefield', 'Koramangala', 'Indiranagar']]
    one_hot_type = [1 if t == type_ else 0 for t in ['Casual Dining', 'Quick Bites', 'Cafe']]
    
    scaled_features = scaler.transform([[cost, votes]])
    
    return np.array(one_hot_location + one_hot_type + list(scaled_features[0])).reshape(1, -1)

input_data = preprocess_input('Whitefield', 'Casual Dining', 1000, 500)

prediction = model.predict(input_data)

print(f&quot;Predicted restaurant: {prediction}&quot;)
</code></pre>
<p>The shapes of the train data:</p>
<p><code>X_train.shape = (41373, 2924)</code></p>
<p><code>y_train.shape = (41373,)</code></p>
<p><a href=""https://i.sstatic.net/JfqTlo52.png"" rel=""nofollow noreferrer"">This is how my dataset looks like</a></p>
","-1","Question"
"79010018","","<pre><code>from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm import tqdm

def train_one_epoch(model, dataloader, optimizer):
    model.train()
    loss_list = []
    for batch in tqdm(dataloader):
        batch_data = {
            'input_ids': batch['input_ids'],
            'attention_mask': batch['attention_mask'],
            'labels': batch['labels']
        }
        loss = model(**batch_data).loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        loss_list.append(loss.detach().item())
    avg_loss = sum(loss_list) / len(loss_list)
    print('avg loss in epoch:', avg_loss)

def evaluate(model, dataloader):
    model.eval()
    all_labels = []
    all_predictions = []
    for batch in dataloader:
        with torch.no_grad():
            batch_data = {
                'input_ids': batch['input_ids'],
                'attention_mask': batch['attention_mask']
            }
            logits = model(**batch_data).logits
            predictions = torch.argmax(logits, dim=-1)
            labels = batch['labels']
            all_labels.extend(labels)
            all_predictions.extend(predictions)
    accuracy = compute_accuracy(all_predictions, all_labels)
    print(&quot;Accuracy&quot;, accuracy)
    return accuracy

def compute_accuracy(predictions, labels):
    correct = 0
    for pred, label in zip(predictions, labels):
        if pred == label:
            correct += 1
    return correct / len(labels)

def my_collate_fn(batched_samples):
    texts = [example['text'] for example in batched_samples]
    labels = [example['label'] for example in batched_samples]
    text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors='pt')
    labels = torch.LongTensor(labels)
    return {
        'input_ids': text_encoding['input_ids'].cuda(),
        'attention_mask': text_encoding['attention_mask'].cuda(),
        'labels': labels.cuda()
    }

torch.manual_seed(64)
batch_size = 16
learning_rate = 5e-5
num_epochs = 10
model_name = &quot;roberta-base&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
    datasets['train'],
    batch_size=8,
    shuffle=True,
    collate_fn=my_collate_fn,
    num_workers=0
)

validation_dataloader = DataLoader(
    datasets['validation'],
    batch_size=8,
    shuffle=False,
    collate_fn=my_collate_fn,
    num_workers=0
)

best_acc = 0.0
for epoch in range(1, num_epochs + 1):
    train_one_epoch(model, train_dataloader, optimizer)
    valid_acc = evaluate(model, validation_dataloader)

</code></pre>
<pre><code>100%|██████████| 865/865 [01:27&lt;00:00,  9.89it/s]


avg loss in epoch: 0.6746856869559068


Accuracy 0.4908256880733945


100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]


avg loss in epoch: 0.6922555248516833


Accuracy 0.4908256880733945


100%|██████████| 865/865 [01:27&lt;00:00,  9.89it/s]


avg loss in epoch: 0.6976809655310791


Accuracy 0.5091743119266054
</code></pre>
<p>Changing learning rate also does not work.</p>
","0","Question"
"79016443","","<p>I am following <a href=""https://heartbeat.comet.ml/using-machine-learning-for-language-detection-517fa6e68f22"" rel=""nofollow noreferrer"">this</a> tutorial for language detection using machine learning. In the dataset I am using, however, there are multiple variables as features. I tried, in the place of <code>X = data[&quot;Text&quot;]</code>, <code>X = df[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]</code>,(message, fingers, and tail are the three feature variables I am using) but it throws a KeyError;</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\usr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pandas\core\indexes\base.py&quot;, line 3805, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;index.pyx&quot;, line 167, in pandas._libs.index.IndexEngine.get_loc
  File &quot;index.pyx&quot;, line 196, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\\_libs\\hashtable_class_helper.pxi&quot;, line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File &quot;pandas\\_libs\\hashtable_class_helper.pxi&quot;, line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ('message', 'fingers', 'tail')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;c:\Users\usr\Downloads\thecode.py&quot;, line 13, in &lt;module&gt;
    X = df[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]
        ~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\usr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pandas\core\frame.py&quot;, line 4102, in __getitem__
    indexer = self.columns.get_loc(key)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\usr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pandas\core\indexes\base.py&quot;, line 3812, in get_loc
    raise KeyError(key) from err
KeyError: ('message', 'fingers', 'tail')
</code></pre>
<p>How should I implement code so as to use all features without throwing errors?</p>
","-1","Question"
"79016929","","<p>I am trying to build a model to predict &quot;species&quot; based on data with features &quot;message&quot;, &quot;tail&quot;, and &quot;finger&quot;, and label &quot;species&quot;(see the first few rows of data.csv below):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>message</th>
<th>fingers</th>
<th>tail</th>
<th>species</th>
</tr>
</thead>
<tbody>
<tr>
<td>pluvia arbor aquos</td>
<td>4</td>
<td>no</td>
<td>Aquari</td>
</tr>
<tr>
<td>cosmix xeno nebuz odbitaz</td>
<td>5</td>
<td>yes</td>
<td>Zorblax</td>
</tr>
<tr>
<td>solarix glixx novum galaxum quasar</td>
<td>5</td>
<td>yes</td>
<td>Zorblax</td>
</tr>
<tr>
<td>arbor insectus pesros ekos dootix nimbus</td>
<td>2</td>
<td>yes</td>
<td>Florian</td>
</tr>
</tbody>
</table></div>
<p>My code is:</p>
<pre><code>import warnings
warnings.simplefilter(&quot;ignore&quot;)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv(&quot;data.csv&quot;)
X = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
X = [str (item) for item in X]
y = df[&quot;species&quot;]

le = LabelEncoder()
y = le.fit_transform(y)

cv = CountVectorizer()
X = cv.fit_transform(X).toarray()

model = MultinomialNB()
model.fit(X, y)

test_data = pd.read_csv('test.csv')
test_data_array = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
test_data_array = [str (item) for item in test_data_array]
test_data_array = cv.fit_transform(test_data_array).toarray()

y_prediction = model.predict(test_data_array)
y_prediction = le.inverse_transform(y_prediction)

print(y_prediction)
</code></pre>
<p>I followed <a href=""https://heartbeat.comet.ml/using-machine-learning-for-language-detection-517fa6e68f22"" rel=""nofollow noreferrer"">this</a> tutorial for the same.</p>
<p>The problem is, when I tried running it, it just outputs the species column of the original training data word-for-word apart from a few differences (there are 493 results while the test data consisted of 299 entries, and the training data consisted of 500 entries). It doesn't actually predict anything for the test data. I don't understand why the code won't work. Could someone help out?</p>
","-1","Question"
"79021064","","<p>I'm working on a project using the SBERT pre-trained models (specifically <a href=""https://huggingface.co/nreimers/MiniLM-L6-H384-uncased"" rel=""nofollow noreferrer"">MiniLM</a>) for a text classification project with 995 classifications. I am following the steps laid out <a href=""https://www.sbert.net/docs/sentence_transformer/training_overview.html#why-finetune"" rel=""nofollow noreferrer"">here</a> for the most part and everything seems to run.</p>
<p>My issue occurs when actually training the model. No matter what values I set in the training arguments the training always seems to end early and never completes all the batches. For example, I set <code>num_train_epochs=1</code> but it only gets up to 0.49 epochs. If <code>num_train_epochs=4</code>, it always ends at 3.49 epochs.</p>
<p>Here is my code:</p>
<pre><code>from datasets import load_dataset
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
    SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
    &quot;nreimers/MiniLM-L6-H384-uncased&quot;,
    model_card_data=SentenceTransformerModelCardData(
        language=&quot;en&quot;,
        license=&quot;apache-2.0&quot;,
        model_name=&quot;all-MiniLM-L6-v2&quot;,
    )
)

loss = BatchAllTripletLoss(model)
# Loss overview: https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# This particular loss method: https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss


# training args: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
    # Required parameter:
    output_dir=&quot;finetune/model20240924&quot;,
    # Optional training parameters:
    num_train_epochs=1,
    max_steps = -1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.GROUP_BY_LABEL,  # 
    # Optional tracking/debugging parameters:
    eval_strategy=&quot;no&quot;,
    eval_steps=100,
    save_strategy=&quot;epoch&quot;,
   # save_steps=100,
    save_total_limit=2,
    logging_steps=100,
    run_name=&quot;miniLm-triplet&quot;,  # Will be used in W&amp;B if `wandb` is installed
)

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=trainDataset,
    eval_dataset=devDataset,
    loss=loss,
    #evaluator=dev_evaluator,
)
trainer.train()
</code></pre>
<p>Note that I am not using an evaluator because we are creating the model and testing it after the fact with a dedicated test set of values. My dataset is structured as:</p>
<pre><code>Dataset({
    features: ['Title', 'Body', 'label'],
    num_rows: 23961
})
</code></pre>
<p>with the <code>dev</code> dataset being the same structure, only with fewer rows. This gives the following output:</p>
<pre><code> [1473/2996 57:06 &lt; 59:07, 0.43 it/s, Epoch 0/1]
Step    Training Loss
100     1.265600
200     0.702700
300     0.633900
400     0.505200
500     0.481900
600     0.306800
700     0.535600
800     0.369800
900     0.265400
1000    0.345300
1100    0.516700
1200    0.372600
1300    0.392300
1400    0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={'train_runtime': 3427.9198, 'train_samples_per_second': 6.99, 'train_steps_per_second': 0.874, 'total_flos': 0.0, 'train_loss': 0.5003972503496366, 'epoch': 0.4916555407209613})
</code></pre>
<p>As much as I adjust the values I cannot get it to complete all of the batches. How to resolve this issue?</p>
","0","Question"
"79024541","","<p>I've been trying to do transfer learning in EfficientNet in a binary classification task.</p>
<p>My directory structure looks like this:</p>
<pre><code>training
├── label0
└── label1

validation
├── label0
└── label1
</code></pre>
<p>and I use this to create the image datagen:</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,           
    width_shift_range=0.0,       
    height_shift_range=0.0,
    shear_range=0.0,            
    zoom_range=0.0,              
    horizontal_flip=True,
    fill_mode='nearest'
)
no augmentations are applied
validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'training',
    target_size=(240, 240),     
    batch_size=128,               
    class_mode='binary',          # Binary classification
    shuffle=True
)

validation_generator = validation_datagen.flow_from_directory(
    'validation', 
    target_size=(240, 240),
    batch_size=128,
    class_mode='binary',
    shuffle=True
)
</code></pre>
<p>With an output of :</p>
<pre class=""lang-bash prettyprint-override""><code>Found 19747 images belonging to 2 classes.
Found 4938 images belonging to 2 classes.
</code></pre>
<p>This is the code for building the model:</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.applications import EfficientNetB1
from tensorflow.keras.models import Model
from tensorflow.keras import layers

NUM_CLASSES = 2
IMG_SIZE = 240
size = (IMG_SIZE, IMG_SIZE)

def build_model():
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    
    model = EfficientNetB1(include_top=False, input_tensor=inputs, weights=&quot;imagenet&quot;)

    model.trainable = False
    
    x = layers.GlobalAveragePooling2D(name=&quot;avg_pool&quot;)(model.output)
    x = layers.BatchNormalization()(x)
    # x = layers.Dense(128, activation=&quot;relu&quot;)(x)
    # top_dropout_rate = 0.2
    # x = layers.Dropout(top_dropout_rate, name=&quot;dropout&quot;)(x)
    # outputs = layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;pred&quot;)(x)
    outputs = layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;pred&quot;)(x)
    
    model = tf.keras.Model(inputs, outputs, name=&quot;EfficientNet&quot;)
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    model.compile(
        optimizer=optimizer,
        loss=&quot;binary_crossentropy&quot;,
        metrics=[&quot;accuracy&quot;]
    )
    
    return model
</code></pre>
<p>you can see that I'm trying <strong>lots</strong> of things just to make something work but to no avail. I set <code>model.trainable</code> to False since I am following <a href=""https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#transfer-learning-from-pretrained-weights"" rel=""nofollow noreferrer"">this</a> implementation on transfer learning. I still haven't finished the first step. I was planning to move on to the unfreezing once I reached 60-70% but I can't even make it to 52%.</p>
<p>Here is how I start the training:</p>
<pre class=""lang-py prettyprint-override""><code>model = build_model()
epochs = 50
hist = model.fit(train_generator, 
                 epochs=epochs, 
                 steps_per_epoch=len(train_generator), 
                 validation_data=validation_generator,
                 validation_steps=len(validation_generator))
</code></pre>
<p>and the accuracy and val_accuracy are always almost 50% at the first ten epochs, which is no more than a random guessing.</p>
<p>I tried lowering or increasing the learning rate (1e-1 to 1e-6), the batch size (32 - 256), adding dropouts (0.1 - 0.5), adding relu layers (32 - 128), and making sure the images are in the right class. What can I do to make the model <strong>really</strong> learn?</p>
","-1","Question"
"79026315","","<p>I have initially written two functions to provide me with a model and function calling directly from OpenAI. Here is the complete code:</p>
<pre><code>from langchain_openai import ChatOpenAI

def get_open_ai(temperature=0, model='gpt-4'):

    llm = ChatOpenAI(
    model=model,
    temperature = temperature,
)
    return llm

def get_open_ai_json(temperature=0, model='gpt-4'):
    llm = ChatOpenAI(
    model=model,
    temperature = temperature,
    model_kwargs={&quot;response_format&quot;: {&quot;type&quot;: &quot;json_object&quot;}},
)
    return llm
</code></pre>
<p>The problem is I now need to use LiteLLM as a proxy, which I am forwarding to a local port, for example <code>localhost:3005</code>. I know I should have an option somewhere to put an <code>openai_base = &quot;localhost:3005&quot;</code> or something similar, to make my code hit the LiteLLM gateway instead of OpenAI directly, but this does not work in my above code. I also looked at the <a href=""https://docs.litellm.ai/docs/proxy/user_keys"" rel=""nofollow noreferrer"">LiteLLM documentation</a> which gives an example with the OpenAI library not with the Langchain wraper. Could anyone please tell me what I must change in my code to make it work with my own LiteLLM server ?</p>
","0","Question"
"79027142","","<p>This multihead self attention code causes the training loss and validation loss to become NaN, but when I remove this part, everything goes back to normal. I know that when the training loss and validation loss become NaN, it means there's an exploding gradient there. However, I don’t know what's wrong with my code that's causing the gradient to explode. When I compare it with the official PyTorch code, it looks similar. When I use nn.MultiheadSelfAttention, the gradient doesn’t explode, but when I use my own code, the gradient starts exploding. There is no error message displayed. Does anyone know what's wrong with my code below?</p>
<pre class=""lang-py prettyprint-override""><code>class MultiHeadAttention(nn.Module):
  def __init__(self, in_dim, num_heads=8, dropout=0):
    super().__init__()
    self.num_heads = num_heads
    self.head_dim = in_dim // num_heads
    self.conv_q = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.conv_k = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.conv_v = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.att_drop = nn.Dropout(dropout)
    self.proj = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.proj_drop = nn.Dropout(dropout)

  def forward(self, x):

    b, _, h, w = x.shape
    
    q = self.conv_q(x)
    k = self.conv_k(x)
    v = self.conv_v(x)

    q = rearrange(q, &quot;b (nh hd) h w -&gt; b nh (h w) hd&quot;, nh=self.num_heads)
    k = rearrange(k, &quot;b (nh hd) h w -&gt; b nh (h w) hd&quot;, nh=self.num_heads)
    v = rearrange(v, &quot;b (nh hd) h w -&gt; b nh (h w) hd&quot;, nh=self.num_heads)

    att_score = q @ k.transpose(2, 3) ** (self.head_dim ** -0.5)
    att_score = F.softmax(att_score, dim=-1)
    att_score = self.att_drop(att_score)

    x = att_score @ v

    x = rearrange(x, 'b nh (h w) hd -&gt; b (nh hd) h w', h=h, w=w)

    x = self.proj(x)
    x = self.proj_drop(x)

    return x
</code></pre>
","0","Question"
"79028217","","<p>The coefficients on uncorrelated variables denote the degree to which the unique information in them influences the final variable. But what do the coefficients on correlated variables mean - who is how much of a “tug-of-war”? (no math, please)</p>
","-2","Question"
"79029841","","<p>I was building a project in Python, but since it was taking too much resources and lack concurrency I shifted to rust. Now I am confused on how to properly migrate it, most of the code has been migrated but I cannot import the ml model that was exported as .pkl file.</p>
","-1","Question"
"79030256","","<p>I'm working with a dataset containing details about used cars, and I've encountered several missing values in the Fuel_Type column. The possible values include 'Gasoline', 'E85 Flex Fuel', 'Hybrid', 'Diesel', and others. Currently, my data has over 4,000 electric vehicles, fewer than 50 gasoline vehicles, and some hybrids with missing Fuel_Type entries. Additionally, some entries contain non-standard values like '–' and 'not supported'. Accurately filling these missing values is crucial for my analysis, as they significantly impact the results.</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Sample DataFrame
data = {
    'Car': ['Toyota', 'Honda', 'Tesla', None, 'Ford'],
    'Fuel_Type': ['Gasoline', 'E85 Flex Fuel', np.nan, 'Hybrid', None],
    'Transmission': ['Automatic', None, 'Automatic', 'Manual', 'Manual']
}

df = pd.DataFrame(data)

# Initial imputation attempt
imputer = SimpleImputer(strategy='most_frequent')
df['Fuel_Type'] = imputer.fit_transform(df[['Fuel_Type']])
print(df)
</code></pre>
","-1","Question"
"79031959","","<p>I want to use some of the models available through huggingface. I am having the hardest time even getting started. Can anyone help me identify and solve this problem?</p>
<p>I am using Kubuntu 24.04.</p>
<hr />
<p>First, I create and activate a virtual environment within which to install transformers.</p>
<pre><code>python3 -m venv .env
source .env/bin/activate
</code></pre>
<p>This is successful, as now my terminal in Visual Code Studio has the prefix '<code>(.env)</code>'.</p>
<p>Next, I install the latest transformers from github:</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>The output is successful. I then test its success with the recommended method on hugginface.co:</p>
<pre><code>python3 -c &quot;from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))&quot;
</code></pre>
<p>The output looks right to me:</p>
<pre><code>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
[{'label': 'POSITIVE', 'score': 0.9998656511306763}]
</code></pre>
<p>From there, I try to run the following code:</p>
<pre><code>from transformers import pipeline
</code></pre>
<p>but every time I get the following output:</p>
<pre><code>/bin/python3 /path-to/main.py
Traceback (most recent call last):
  File &quot;/path-to/main.py&quot;, line 5, in &lt;module&gt;
    from transformers import pipeline
ModuleNotFoundError: No module named 'transformers'
</code></pre>
","1","Question"
"79032297","","<p>I am trying to predict values from a lot of different devices (1000s, testing on a few hundred for performance reasons) on 15 min resolution. All of the devices log: name and value. I have tried different time ranges for training but so far no luck.</p>
<p>A few examples of the issue:
A device logs 40 all the time -&gt; Arima predicts range of 20-80 starting with 1st prediction (30d training)
A device logs 60-64 -&gt; Arima predicts 65-120 spikes which then flatten at 102 (1d training). I am struggling to understand how Arima plus even arrived at values magnitudes higher than all values in training dataset.</p>
<p>This is pseudocode of the arima plus model creation and forecasting:
I am using a summary table as a source with data at 15 min resolution
training:</p>
<pre><code>CREATE OR REPLACE MODEL forecasting.model
  OPTIONS(
    MODEL_TYPE='ARIMA_PLUS'
    ,TIME_SERIES_TIMESTAMP_COL='timestamp'
    ,TIME_SERIES_DATA_COL='value'
    ,TIME_SERIES_ID_COL='id'
    -- ,auto_arima = TRUE
    ,clean_spikes_and_dips = FALSE
    ,adjust_step_changes = FALSE
    -- ,data_frequency = 'AUTO_FREQUENCY'
    -- ,auto_arima_max_order = 2 -- default is 5
   -- ,max_time_series_length = 96 -- 1 d?
  ) AS
      (
      SELECT timestamp
        ,id
        ,SUM(value) as value

      FROM `forecasting.summary`
      WHERE CAST(TIMESTAMP_TRUNC(timestamp, DAY) AS DATE) &lt; _CUTOFF_DATE
        AND CAST(TIMESTAMP_TRUNC(timestamp, DAY) AS DATE) &gt;= DATE_ADD(CURRENT_DATE(), INTERVAL _PERIOD DAY)
      GROUP BY 1,2

      );
</code></pre>
<p>While I do use SUM(value), pretty sure its sum of only 1 row. Left from other attempts.</p>
<pre><code>SELECT 
    id
    ,forecast_timestamp as timestamp
    ,forecast_value as value
    ,standard_error 
    ,confidence_level 
    ,prediction_interval_lower_bound 
    ,prediction_interval_upper_bound
  FROM 
    ML.FORECAST(MODEL `forecasting.forecast`
                  ,STRUCT(672 as horizon -- 192 is 2d; 672 is 7d 
                          ,0.95 as confidence_level
                  )
    )
    ;
</code></pre>
<p>One approach which worked for me in the past is to predict each 15m as its own time series. The problem is, it would lead to tens or hundreds thousands of timeseries in this case. While ArimaPlus claims to be able to handle millions, it's training performance is already slow if I do 30d and do not limit max pdq.</p>
<p>I had a similar issue on forecasting of data exhibiting a daily and weekly seasonality. In the current case with device data, there is likely daily and annual seasonality related to climate and weather conditions.</p>
<p>How can I make Arima work? If not, what approach would you recommend?</p>
","1","Question"
"79033026","","<p>I have begun experimenting with training machine learning models and encountered some confusion around the concepts of epochs and steps in the training process. While researching online, I came across a <a href=""https://stackoverflow.com/a/78579075/23282059"">formula</a> (𝜎 = (𝜀 × 𝜂) ÷ 𝛽) relating epochs, steps, and batch size. Applying this formula to my own dataset yielded a fractional number of steps, which raised questions for me about how steps are typically handled in practice. I'm unsure if fractional steps are rounded down or how exactly this translates to the actual training process. My lack of hands-on experience with implementing training loops has made it challenging to intuitively grasp how these concepts map to real-world model training scenarios.</p>
<p>To better understand the relationship between epochs, steps, and batch size, I tried applying the formula I found (𝜎 = (𝜀 × 𝜂) ÷ 𝛽) to a dataset (this is only an theorical example dataset):</p>
<pre><code>total_samples = 10000  # Total number of samples in my dataset
batch_size = 32        # Batch size I plan to use
epochs = 10            # Number of epochs I want to train for

steps_per_epoch = total_samples / batch_size
total_steps = (epochs * total_samples) / batch_size

print(f&quot;Steps per epoch: {steps_per_epoch}&quot;)
print(f&quot;Total steps: {total_steps}&quot;)
</code></pre>
<p>This produced the following output:</p>
<pre><code>Steps per epoch: 312.5
Total steps: 3125.0
</code></pre>
<p>The fractional result for steps per epoch (312.5) left me uncertain about how this would be implemented in a real training loop. Specifically:</p>
<ol>
<li>Are fractional steps typically rounded down in practice?</li>
<li>If rounding occurs, does this mean some data samples might be skipped in each epoch?</li>
<li>How do common machine learning frameworks handle this situation?</li>
</ol>
<p>I haven't actually implemented a training loop yet, so I'm not sure how these fractional steps would be handled in code. My main difficulty is bridging the gap between the theoretical calculation and its practical application in model training.</p>
","0","Question"
"79033786","","<p>I'm trying to install causalml on my Windows machine using Python 3.12 with the command <code>pip install causalml</code>.
But the installation fails when trying to build a wheel for causalml. Here's a snippet of the error:</p>
<pre><code>note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for causalml
Failed to build causalml
ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (causalml)
</code></pre>
<p>I've attached on pastebin the error:
<a href=""https://pastebin.com/dehRfgrk"" rel=""nofollow noreferrer"">https://pastebin.com/dehRfgrk</a></p>
<p>But they key error messages are:</p>
<pre><code>'use_tracing': is not a member of '_PyCFrame' in causalml/inference/tree/_tree/_tree.cpp.
The command &quot;cl.exe&quot; fails with exit code 2.
Warning regarding deprecated NumPy API and usage of Python 2.7 options (bdist_wheel.universal is deprecated).
Setuptools warnings about missing packages in the packages configuration (causalml.inference.tree, etc.)
</code></pre>
<p>I'm using:
Python version: 3.12.<br>
OS: Windows 10.<br>
Compiler: Microsoft Visual Studio 2022 Build Tools.<br>
Environment: Anaconda 3.<br>
NumPy version: 1.26.4.</p>
<p>I tried updating visual studio build tools and installed the latest version and ensured C++ build tools were included.</p>
","0","Question"
"79035928","","<p>I'm trying to extract the product price for different sentences using <a href=""https://rubixml.com/"" rel=""nofollow noreferrer"">https://rubixml.com/</a>, but it always returns 260, the first label I give to it</p>
<pre><code>&lt;?php
include_once '../vendor/autoload.php';

use Rubix\ML\Datasets\Labeled;
use Rubix\ML\Datasets\Unlabeled;
use Rubix\ML\Classifiers\KNearestNeighbors;
use Rubix\ML\Transformers\WordCountVectorizer;
use Rubix\ML\Transformers\TfIdfTransformer;
use Rubix\ML\Pipeline;
use Rubix\ML\Extractors\CSV;

$samples= ['The price is 260','The cost is 500','This shirt costs 300','The value of this item is 450','Sold for 150 dolars'];
$labels = ['260',             '500',            '300',                 '450',                           '150'];

$dataset = new Labeled($samples, $labels);

// genrate the model
$pipeline = new Pipeline([
    new WordCountVectorizer(100),
    new TfIdfTransformer(),
], new KNearestNeighbors(3));

// training with dataset
$pipeline-&gt;train($dataset);

// analize new frace
$new = Unlabeled::build([
    ['Price: 1200'],
]);

// Predict
$predictions = $pipeline-&gt;predict($new);
var_dump($predictions);
</code></pre>
<p>I have changed the values for the KNearestNeighbors, provide larger inputs for train dataset, change the Vectorizer. But nothing changes.</p>
","1","Question"
"79036243","","<p>I have a data table with a response 'y' and some predictors, 'X1' and 'X2' among them. I can create two one-factor models with pROC:</p>
<pre><code>roc1 &lt;- roc(data$y, data$X1)
roc2 &lt;- roc(data$y, data$X2)
</code></pre>
<p>But I'm trying to calculate ROC AUC for two-factor model:</p>
<pre><code>t1 = data$X1
t2 = data$X2
t12 = cbind(t1, t2)
roc12 &lt;- roc(data$y, t12)
</code></pre>
<p>and get an error message:</p>
<pre><code>Response and predictor must be vectors of the same length.
</code></pre>
<p>Is there a way to make multifactor models in pROC?</p>
","0","Question"
"79038884","","<p>suppose we have the following model:
<a href=""https://i.sstatic.net/zoY2tA5n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zoY2tA5n.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>how can we build such a model and export it as PMML file?</li>
<li>is PMML capable to encode such model structure?</li>
<li>what are the necessary component in PMML to generate N output nodes in this model?  I understand using &lt;OutputField ... feature=&quot;predictedValue&quot;&gt; for each output node is not giving the expected result.</li>
</ol>
","0","Question"
"79039003","","<p>I’m working on a deep learning model using Keras where I’m trying to combine three inputs: images, masks, and CSV data. The goal of my model is to predict the presence and type of brain hemorrhage from medical images (CT scans) and CSV data. I have structured the model using an Attention U-Net for image segmentation and a dense layer for CSV data processing.</p>
<p>The input data consists of:</p>
<ol>
<li><p>Images: Grayscale CT scans with shape (256, 256, 1).</p>
</li>
<li><p>Masks: Binary masks corresponding to the segmentation with shape (256, 256, 1).</p>
</li>
<li><p>CSV Data: Numerical data for various hemorrhage types with shape (6,).</p>
</li>
</ol>
<p>I’m encountering the following error when trying to train the model using <code>model.fit()</code>:</p>
<p><code>ValueError: Invalid input shape for input Tensor(&quot;functional_1/Cast:0&quot;, shape=(None, 6), dtype=float32). Expected shape (None, 256, 256, 1), but input has incompatible shape (None, 6)</code></p>
<p>It seems that the CSV data, which has a shape of (None, 6), is being fed into the model where the image data (with shape (None, 256, 256, 1)) is expected. I’ve double-checked the input shapes and used train_test_split to split the data, but the issue persists.</p>
<p>Here’s a simplified version of my code:</p>
<pre class=""lang-py prettyprint-override""><code># Model definition
inputs, outputs = attention_unet()  # UNet model for images

csv_input = layers.Input(shape=(6,), name='csv_input')  # CSV data input

mask_input = layers.Input(shape=(256, 256, 1), name='mask_input')  # Mask input

# CSV data processing
csv_x = layers.Dense(64, activation='relu')(csv_input)csv_x = layers.Dense(32, activation='relu')(csv_x)

# Combine CSV with U-Net output
flatten_outputs = layers.Flatten()(outputs)combined = layers.Concatenate()([flatten_outputs, csv_x])

# Final output
final_output = layers.Dense(1, activation='sigmoid', name='final_output')(combined)

# Model compilation
model = models.Model(inputs=[inputs, csv_input, mask_input], outputs=[outputs, final_output])model.compile(optimizer=Adam(), loss={'final_output': 'binary_crossentropy', 'outputs': 'categorical_crossentropy'}, metrics=['accuracy', dice_coef, jaccard_index])

# Data shapes
print(f&quot;Shape of X_train: {X_train.shape}&quot;)  # Shape of images: (batch_size, 256, 256, 1)print(f&quot;Shape of csv_train: {csv_train.shape}&quot;)  # Shape of CSV: (batch_size, 6)print(f&quot;Shape of y_train: {y_train.shape}&quot;)  # Shape of masks: (batch_size, 256, 256, 1)

# Model training
model.fit({'input_layer': X_train, 'csv_input': csv_train, 'mask_input': y_train},{'final_output': csv_train, 'outputs': y_train},validation_data=({'input_layer': X_val, 'csv_input': csv_val, 'mask_input': y_val},{'final_output': csv_val, 'outputs': y_val}),epochs=50,batch_size=32,callbacks=callbacks)
</code></pre>
<p>Steps I’ve Tried:</p>
<ul>
<li>I’ve printed the shapes of the training data, and they all seem correct:
<ul>
<li>Images: (batch_size, 256, 256, 1)</li>
<li>CSV: (batch_size, 6)</li>
<li>Masks: (batch_size, 256, 256, 1)</li>
</ul>
</li>
<li>I’ve checked the order of data input in model.fit() to ensure the data is being passed to the correct layers.</li>
</ul>
<p>How can I resolve this shape mismatch issue and correctly combine the image, mask, and CSV data in the model?</p>
","0","Question"
"79039465","","<p>I'm working with K-Fold Cross-Validation in a Grid Search setup for hyperparameter tuning. I have a few questions about how the model is trained and evaluated:</p>
<ol>
<li><p>When I use <code>GridSearchCV</code>, the model is evaluated across multiple folds (let's say 10). For each hyperparameter combination, the model is trained on ( K-1 ) folds and validated on the remaining fold. When I obtain the <code>best_grid</code> model after the grid search, which specific training data (i.e., which folds) was this model trained on?</p>
</li>
<li><p>When I call <code>best_grid.predict(X_test)</code>, on which dataset is this model making predictions? Has it been trained on the entire dataset after the grid search, or is it still based on the folds used during cross-validation?</p>
</li>
<li><p>If the <code>best_grid</code> model has not been trained on the entire dataset yet, do I need to explicitly fit it to the full dataset again before making predictions?</p>
</li>
<li><p>I want to get the R² train score, but I'm confused about the score I receive when using the following Code:</p>
<pre class=""lang-py prettyprint-override""><code>param_grid = {f'regressor__regressor__{param}': values for param, values in model_info['params'].items()}
     grid_search = GridSearchCV(full_pipeline, param_grid, cv=stratified_kf.split(X, y_binned), scoring=&quot;r2&quot;, n_jobs=4, return_train_score=True)

     grid_search.fit(X,y)

     if grid_search.best_score_ &gt; best_score:
         best_score = grid_search.best_score_
         best_model = model_name
         best_grid = grid_search


 mean_train_score = best_grid.cv_results_['mean_train_score'][best_grid.best_index_] #
 print(mean_train_score) # THIS THING HERE
</code></pre>
</li>
</ol>
","-1","Question"
"79047845","","<pre><code>ValueError: Sequential model 'sequential_2' has already been configured to use 
input shape (None, 224, 224, 3). You cannot build it with input_shape [None, 224, 224, 3]
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def load_model(model_path):
  &quot;&quot;&quot;
  Loads a saved model from a specified path.
  &quot;&quot;&quot;

  tf.keras.config.enable_unsafe_deserialization()
  print(f&quot;Loading saved model from: {model_path}&quot;)
  model = tf.keras.models.load_model(model_path)
  return model

loaded_1000_image_model = load_model('/content/drive/MyDrive/Dog Vision/models/20241002-16491727887796-1000-images-mobilenetv2-Adam.h5')
</code></pre>
<p>I was expecting that it would load the model without any errors but for some reason it's giving a Value Error even thought I am not trying to rebuild or give any INPUT SHAPE again.</p>
","1","Question"
"79055573","","<p>I am trying to implement and train an SVM multi-class classifier from scratch using python and numpy in jupyter notebooks.</p>
<p>I have been using the CS231n course as my base of knowledge, especially this page: <a href=""https://cs231n.github.io/optimization-1/"" rel=""nofollow noreferrer"">https://cs231n.github.io/optimization-1/</a> which discusses gradient descent.  I have implemented a class, SVM, that I believe is on the right track.</p>
<p>Here is the basic profile for that class:</p>
<pre><code>class SVM:
  def __init__(self):
    self.weights = np.random.randn(len(labels), X_train.shape[1]) * 0.1
    self.history = []

  def predict(self, X):
    '''
    returns class predictions in np array of size
    n x num_classes, where n is the number of examples in X
    '''

    #matrix multiplication to apply weights to X
    bounds = self.weights @ X.T

    #return the predictions
    return np.array(bounds).T

  def loss(self, scores, y, delta=1):
    '''computes the loss'''
    #calculate and return the loss for a prediction and corresponding truth label
    #hinge loss in this case
    total_loss = 0

    #compute loss for each example...
    for i in range(len(scores)):
      #extract values for this example
      scores_of_x = scores[i]
      label = y[i]
      correct_score = scores_of_x[label]
      incorrect_scores = np.concatenate((scores_of_x[:label], scores_of_x[label+1:]))

      #use the scores for example x to compute the loss at x
      wj_xi = correct_score           #these should be a vector of INCORRECT scores
      wyi_xi = incorrect_scores       #this should be a vector of the CORRECT score
      wy_xi = wj_xi - wyi_xi + delta  #core of the hinge loss formula
      losses = np.maximum(0, wy_xi)   #lower bound the losses at 0
      loss = np.sum(losses)           #sum the losses

      #add to the total loss
      total_loss += loss

    #return the loss
    avg_loss = total_loss / len(scores)
    return avg_loss

  def gradient(self, scores, X, y, delta=1):
    '''computes the gradient'''
    #calculate the loss and the gradient of the loss function
    #gradient of hinge loss function
    gradient = np.zeros(self.weights.shape)

    #calculate the gradient in each example in x
    for i in range(len(X)):
      #extract values for this example
      scores_of_x = scores[i]
      label = y[i]
      x = X[i]
      correct_score = scores_of_x[label]
      incorrect_scores = np.concatenate((scores_of_x[:label], scores_of_x[label+1:]))

      #
      ##
      ### start by computing the gradient of the weights of the correct classifier
      ##
      #
      wj_xi = correct_score           #these should be a vector of INCORRECT scores
      wyi_xi = incorrect_scores       #this should be a vector of the CORRECT score
      wy_xi = wj_xi - wyi_xi + delta  #core of the hinge loss formula
      losses = np.maximum(0, wy_xi)   #lower bound the losses at 0

      #get number of nonzero losses, and scale data vector by them to get the loss
      num_contributing_classifiers = np.count_nonzero(losses)
      #print(f&quot;Num loss contributors: {num_contributing_classifiers}&quot;)
      g = -1 * x * num_contributing_classifiers   #NOTE the -, very important here, doesn't apply to other scores

      #add the gradient of the correct classifier to the gradient
      gradient[label] += g  #because arrays are 0-indexed, but the labels are 1-indexed
      # print(f&quot;correct label: {label}&quot;)
      #print(f&quot;gradient:\n{gradient}&quot;)
      #
      ##
      ### then, compute the gradient of the weights for each incorrect classifier
      ##
      #
      for j in range(len(scores_of_x)):

        #skip the correct score, since we already did it
        if j == label:
          continue
        wj_xi = scores_of_x[j]          #should be a vector containing the score of the CURRENT classifier
        wyi_xi = correct_score          #should be a vector containing the score of the CORRECT classifier
        wy_xi = wj_xi - wyi_xi + delta  #core of the hinge loss formula
        loss = np.maximum(0, wy_xi)   #lower bound the loss at 0

        #get whether this classifier contributed to the loss, and scale the data vector by that to get the gradient
        contributed_to_loss = 0
        if loss &gt; 0:
          contributed_to_loss = 1

        g = x * contributed_to_loss        #either times 1 or times 0

        #add the gradient of the incorrect classifier to the gradient
        gradient[j] += g


    #divide the gradient by number of examples to get the average gradient
    return gradient / len(X)

  def fit(self, X, y, epochs = 1000, batch_size = 256, lr=1e-2, verbose=True):
    #gradient descent loop
    for epoch in range(epochs):
      self.history.append({'epoch': epoch})

      #create a batch of samples to calculate the gradient
      #NOTE: this significantly boosts the speed of training
      indices = np.random.choice(len(X), batch_size, replace=False)
      X_batch = X.iloc[indices]
      y_batch = y.iloc[indices]
      
      X_batch = X_batch.to_numpy()
      y_batch = y_batch.to_numpy()

      #evaluate class scores on training set
      predictions = self.predict(X_batch)
      predicted_classes = np.argmax(predictions, axis=1)

      #compute the loss: average hinge loss
      loss = self.loss(predictions, y_batch)
      self.history[-1]['loss'] = loss

      #compute accuracy on the test set, for an intuitive metric
      accuracy = np.mean(predicted_classes == y_batch)
      self.history[-1]['accuracy'] = accuracy
      
      #print progress
      if epoch%50 == 0 and verbose:
        print(f&quot;Epoch: {epoch} | Loss: {loss} | Accuracy: {accuracy} | LR: {lr} \n&quot;)


      #compute the gradient on the scores assigned by the classifier
      gradient = self.gradient(predictions, X_batch, y_batch)
      
      #backpropagate the gradient to the weights + bias
      step = gradient * lr

      #perform a parameter update, in the negative??? direction of the gradient
      self.weights += step
</code></pre>
<p>That is my implementation.  The fit() method is the one that trains the weights on the data passed in.  I am at a stage where loss tends to decrease from one iteration to the next.</p>
<p>But, the problem is, accuracy drops down to zero even as loss decreases.</p>
<p>I know that they are not directly related, but shouldn't my accuracy generally trend upwards as loss goes down?  This makes me think I have done something wrong in the loss() and gradient() methods.  But, I can't seem to find where I went wrong.  Also, sometimes, my loss will increase from one epoch to the next.  This could be an impact of my batched evaluation of the gradient, but I am not certain.</p>
<p>Here is a link to my Jupyter notebook, which should let you run my code in its current state:</p>
<p><a href=""https://colab.research.google.com/drive/12z4DevKDicmT4iE6AlMGrRiN6He8R9_4#scrollTo=uBTUQlscWksP"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/12z4DevKDicmT4iE6AlMGrRiN6He8R9_4#scrollTo=uBTUQlscWksP</a>
And here is a link to the data set I am using: <a href=""https://www.kaggle.com/datasets/taweilo/fish-species-sampling-weight-and-height-data/code"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/taweilo/fish-species-sampling-weight-and-height-data/code</a></p>
","-1","Question"
"79057233","","<p>I possess a compilation of the different states of arm within the game &quot;rock paper scissors&quot;. My intention is to program these categories in a similar manner as this.</p>
<pre><code>[1, 0, 0] - rock 
[0, 1, 0] - paper 
[0, 0, 1] - scissors
</code></pre>
<p>is there a convenient automatic way?</p>
<p>I employed the Embedding layer, but I'm uncertain if it's appropriate.</p>
","-1","Question"
"79057484","","<p>I tried to run:</p>
<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf
import numpy as np

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers import TextVectorization
from sklearn.model_selection import train_test_split
from tensorflow import keras 

from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
from sklearn.feature_extraction.text import CountVectorizer

dataf=pd.read_csv('D:/datafile.csv')
data=pd.read_csv(&quot;D:/dataset1c2f4b7/dataset/train.csv&quot;,encoding='latin-1')
l=[]
for a in dataf['text']:
    l.append(a)
m=[]
for a in dataf['target']:
    m.append(a)

X_train, X_test, y_train, y_test = train_test_split(l, m, test_size=0.2, random_state=42)

vectorizer = CountVectorizer()
vectorizer.fit(X_train)
X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)
X_train=np.array(X_train)
X_test=np.array(X_test)
y_train=np.array(y_train)
y_test=np.array(y_test)
print(X_train)
model = keras.models.Sequential() 
model.add(keras.layers.Embedding(10000, 128)) 
model.add(keras.layers.SimpleRNN(64, return_sequences=True)) 
model.add(keras.layers.SimpleRNN(64)) 
model.add(keras.layers.Dense(128, activation=&quot;relu&quot;)) 
model.add(keras.layers.Dropout(0.4)) 
model.add(keras.layers.Dense(1, activation=&quot;sigmoid&quot;)) 
model.summary() 




model.compile(&quot;rmsprop&quot;, 
              &quot;binary_crossentropy&quot;, 
              metrics=[&quot;accuracy&quot;])
model.fit(X_train, y_train,epochs=5,verbose=False,validation_data=(X_test, y_test),batch_size=10)
model.save('gfgModel.h5')  
tf.saved_model.save(model, 'one_step 05')
</code></pre>
<p>This shows</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix)
</code></pre>
<p>I am trying to create a text classifier.</p>
<p>I was just expecting the model to be trained as everything is in array form.</p>
","0","Question"
"79057824","","<p>I am trying to predict a classification using logistic regression, based on training data, for a series of points in testing data.</p>
<p>I am getting an output, without errors, but the results, I am told, are wrong (the incorrect output would be considered an error, but there are no errors when running the code).</p>
<p>The training data is a collection of 4 different categories in 375 points, there are 3 variates per point, so plotting on a 3D graph. I ran a counting program on the set and found that over 50% of the points are in classification 2. My initial results were all testing points came up as class 2. I have tried sorting the training data into several different sets: random selection of 125 entries (which is the size of the testing data), and finding the min number of all the categories, and creating a training set using the same number of points from each category.</p>
<p>No sorting = all class 2</p>
<p>random sorting = all class 2</p>
<p>equal class number sorting = gives me an answer that has points classified in all 4 categories, but when I plug them into the online final test form, I have an accuracy score of 26%, which is the same as random chance. So, I am not processing data correctly, and I am not sure where. I am hoping someone with more experience with regression classification can point me in the right direction.</p>
<p>Do I need to reformat (transform) the train_X, train_y, and test_X arrays before I call LogisticRegression? If so, how? Maybe I am just feeding it malformed data?</p>
<pre><code># forming tables to push through logistic regression
train_X = []
train_y = []
for i in range(len(train_table)):
    train_X.append(
        [train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
    )
    train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
    test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# Trying with and without normalize
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
    sort_results(test_table, predict, prob),
    columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)
</code></pre>
","-1","Question"
"79062004","","<p>I have a machining part (.STL) and want to recognize (and extract) it's machining features. Some of the features are simple but some are more complex that's why I think a machine learning approach would be fitting because I can't describe the feature mathematically.</p>
<p>There is <a href=""https://github.com/zibozzb/FeatureNet"" rel=""nofollow noreferrer"">FeatureNet</a> which basically does this job expect that it can't recognize multiple features and the code doesn't work as expected.</p>
<p>I also know of <a href=""https://github.com/whjdark/AAGNet"" rel=""nofollow noreferrer"">AAGNet</a> which does what I want but it uses .STEP as input but I have a mesh (or point cloud if I convert it).</p>
<p>Since there are a lot more point clouds repositories I thought that I could maybe use them to solve my problem. Is something like FPFH the right direction or am I on the wrong path?</p>
<p>If I would use a machine learning approach I can easily create a labeled dataset.</p>
","-1","Question"
"79063665","","<p>I'm wondering if there is a non-linear regression routine with scikit-learn which allows for incremental learning, ie though the partial_fit call. I see that SGDRegressor and PassiveAggressiveRegressor both allow partial_fit, but are linear, and my data is decidedly non-linear so fits are nowhere close.</p>
","0","Question"
"79065062","","<p>I ran the below code to export the ML Model in <strong>Azure Databricks</strong> based mlflow but I seem to be getting this error</p>
<blockquote>
<p>MLflow host or token is not configured correctly</p>
</blockquote>
<p>I'm unable to figure out what the issue is. The URL for the workspace is correct along with the PAT Token.</p>
<p>The export_import tools is very buggy. It expects mlfow library but what comes with Databricks ML Runtime is mlflow-skinny.</p>
<pre><code>import mlflow
import os
from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient

# Set the Databricks MLflow tracking URI with the workspace URL
mlflow.set_tracking_uri(&quot;https://adb-xxxyyymmmnnnyyy.1.azuredatabricks.net/&quot;)

# Set both tokens for compatibility
os.environ[&quot;DATABRICKS_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;
os.environ[&quot;MLFLOW_TRACKING_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;

# Initialize the MLflow client (no need to pass tracking URI as it's set globally)
mlflow_client = MlflowClient()

# Initialize the ModelExporter with the MLflow client
exporter = ModelExporter(mlflow_client)

# Export the model
exporter.export_model(
    model_name=&quot;Signature_Test&quot;,
    output_dir=&quot;/tmp/mlflow_export/model&quot;,
    stages=None,  # Use &quot;None&quot; to export all stages, or specify &quot;Staging&quot; or &quot;Production&quot;
    export_metadata_tags=True
)
</code></pre>
","-1","Question"
"79071235","","<p>I'm trying to run a Stable Diffusion model using the code provided in the DDIM Inversion tutorial. However, when the input's batch size is set to a value greater than 1 (e.g., 32), I encounter the following error:</p>
<pre><code>RuntimeError: The size of tensor a (131072) must match the size of tensor 
b (4096) at non-singleton dimension 1.
</code></pre>
<p>It appears that 131072 is maybe derived from 32 x 4096, indicating a mismatch in tensor dimensions. The specific line where the error occurs is:</p>
<pre><code>noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
</code></pre>
<p>Here’s the relevant portion of my code for the inversion process:</p>
<pre><code>## Inversion (https://github.com/huggingface/diffusion-models-class/blob/main/unit4/01_ddim_inversion.ipynb)
    def invert_process(self, guidance_scale, input, denoise_kwargs):

        pred_images = []
        pred_latents = []
        
        decode_kwargs = {'vae': self.vae}

        # Reversed timesteps &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
        timesteps = reversed(self.scheduler.timesteps)
        num_inference_steps = len(self.scheduler.timesteps)

        with torch.no_grad():
            for i in tqdm(range(0, num_inference_steps)):

                t = timesteps[i]
                self.cur_t = t.item()
                
                # For text condition on stable diffusion
                if 'encoder_hidden_states' in denoise_kwargs.keys():
                    bs = denoise_kwargs['encoder_hidden_states'].shape[0]
                    input = torch.cat([input] * bs)

                # Predict the noise residual
                noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
                noise_pred = noisy_residual

                # For text condition on stable diffusion
                if noisy_residual.shape[0] == 2:
                    # perform guidance
                    noise_pred_text, noise_pred_uncond = noisy_residual.chunk(2)
                    noisy_residual = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                    input, _ = input.chunk(2)

                current_t = max(0, self.cur_t - (1000//num_inference_steps)) #t
                next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
                alpha_t = self.scheduler.alphas_cumprod[current_t].to(self.device)
                alpha_t_next = self.scheduler.alphas_cumprod[next_t].to(self.device)

                latents = input

                # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)
                # Add noise to latents


                latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred
                
                input = latents
                
                pred_latents.append(latents)
                pred_images.append(decode_latent(latents, **decode_kwargs))
                
        return pred_images, pred_latents

</code></pre>
<p>What could be causing the tensor size mismatch when the batch size is greater than 1? How can I resolve this issue while maintaining a batch size greater than 1 in the model?</p>
<p>I attempted to change the size of t to be a tensor with shape (batch size,).</p>
<p>Additionally, I confirmed that the model works correctly when the batch size is 1.</p>
","0","Question"
"79073506","","<p>I'm trying to train a mobileNetV3Large with a simple PyTorch Scheduler.
This is the portion of the code responsible for training:</p>
<pre><code>bench_val_loss = 1000
bench_acc = 0.0
epochs = 15
optimizer = optim.Adam(embeddingNet.parameters(), lr=1e-3) 
loss_optimizer = torch.optim.Adam(loss_fn.parameters(), lr=1e-3)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, threshold=0.02)

for epoch in range(1, epochs + 1):

    print(f'current lr: {scheduler.get_last_lr()}')
    loss=train(embeddingNet, loss_fn, device, train_dataloader, optimizer, loss_optimizer, epoch)
    val_loss, accuracy =test(train_dataset, val_dataset, embeddingNet, accuracy_calculator, loss_fn, epoch, val_dataloader)
    #val_loss = simpleTest(train_dataset, val_dataset, embeddingNet, accuracy_calculator, loss_fn, epoch, val_dataloader)

    
    torch.save(embeddingNet.state_dict(), 'my/path/mobileNetV3L_ArcFaceLAST.pth')

    if accuracy &gt;= bench_acc:
      bench_val_loss = val_loss
      torch.save(embeddingNet.state_dict(), 'my/path/mobileNetV3L_ArcFaceBEST.pth')

    scheduler.step(accuracy)

    writer.add_scalars('Training vs. Validation Loss',
                       {'Training': loss, 'Validation': val_loss},
                       global_step=epoch+1)
</code></pre>
<p>And here you'll find the first 7 training logs</p>
<pre><code>Test set accuracy (Precision@1) = 0.17834772304046048
current lr: [0.001]
Epoch 3: Loss = 39.68284225463867
Epoch 3: valLoss = 39.9765007019043
100%|██████████| 962/962 [01:43&lt;00:00,  9.28it/s]
100%|██████████| 370/370 [00:41&lt;00:00,  8.92it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.31242593533096324
current lr: [0.001]
Epoch 4: Loss = 39.4412841796875
Epoch 4: valLoss = 39.67761562450512
100%|██████████| 962/962 [01:45&lt;00:00,  9.11it/s]
100%|██████████| 370/370 [00:41&lt;00:00,  8.86it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.3633824276282377
current lr: [0.001]
Epoch 5: Loss = 39.09823989868164
Epoch 5: valLoss = 39.54649614901156
100%|██████████| 962/962 [01:42&lt;00:00,  9.37it/s]
100%|██████████| 370/370 [00:41&lt;00:00,  8.87it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.44244117149145085
current lr: [0.001]
Epoch 6: Loss = 38.70449447631836
Epoch 6: valLoss = 39.1865906792718
100%|██████████| 962/962 [01:45&lt;00:00,  9.15it/s]
100%|██████████| 370/370 [00:39&lt;00:00,  9.25it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.5167597765363129
current lr: [0.0001]
</code></pre>
<p>I can't figure out why the scheduler decided to reduce the learning rate even thought the accuracy was increasing more quickly than the threshold.</p>
<p>Where is the error?</p>
","0","Question"
"79074519","","<p>I am trying to implement pspnet in tensorflow.
It requires a pooling module which takes in the input, along with several kernel sizes:</p>
<ul>
<li><p>does average pooling on the input with the each kernel <code>AveragePooling2D</code></p>
</li>
<li><p>does a 1x1 convolution, after which <code>UpSampling2D</code> is used</p>
</li>
</ul>
<p>Finally all the different conv-deconv outputs are concatenated together and fed forward</p>
<pre><code>def pyramid_pooling_module(x, pool_sizes):
    pool_outputs = []
    for pool_size in pool_sizes:
        pooled= layers.AveragePooling2D(pool_size)(x)
        pooled= layers.Conv2D(512, (1,1), padding='same')(pooled)
        pooled= layers.UpSampling2D(size=pool_size, interpolation='bilinear')(pooled)
        print(pool_size)
        pool_outputs.append(pooled)
    return layers.Concatenate()(pool_outputs)
</code></pre>
<p>The inputs would be of the dimension 68, 120
Because of which the kernels used (1x1, 2x2, 3x3, 6x6) would have rounding errors from the average pooling layers for 3x3, 6x6</p>
<p>such that the final pooled outputs for those layers are (66, 120)</p>
<p>I'm not sure how to go about fixing this, should resize my inputs to a size which is exactly divisible by 6x6? is there another way?</p>
","0","Question"
"79075311","","<p>I am trying to create a training pipeline to train a custom yolov9 model with user inputted labeled images.</p>
<p>I am having an issue where if I make my data.yaml file use relative paths, I get the error:</p>
<pre><code>RuntimeError: Dataset 'OIT_model/customOIT/customdatasetyolo/data.yaml' error
Dataset 'OIT_model/customOIT/customdatasetyolo/data.yaml' images not found , missing path 'C:\GitHub\Anomaly_detection_combine\OIT_model\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\val'
</code></pre>
<p>What is even more odd, is that the path the error mentions,</p>
<pre><code>'C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val'
</code></pre>
<p>is not a path that exists or is being requested anywhere. The actual path is</p>
<pre><code>'C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val'
</code></pre>
<p>for some reason it is repeating the first part of the path 3 times.</p>
<p>This is the data.yaml file:</p>
<pre><code>path: OIT_model/customOIT/customdatasetyolo
train: OIT_model/customOIT/customdatasetyolo/train
val: OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: ['5']
</code></pre>
<p>and this is the code that is starting training:</p>
<pre><code>def train_custom_dataset_yolo(data_path, epochs=100, imgsz=64, verbose=True):
    model = YOLO(&quot;OIT_model/yolov9c.pt&quot;)
    # Specify the save directory for training runs
    save_dir = 'OIT_model/customOIT/yolocustomtrainoutput'
    if os.path.exists(save_dir):
        for file in os.listdir(save_dir):
            file_path = os.path.join(save_dir, file)
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
    os.makedirs(save_dir, exist_ok=True)
    model.train(data=data_path, epochs=epochs, imgsz=imgsz, verbose=verbose, save_dir=save_dir)
    return
train_custom_dataset_yolo('OIT_model/customOIT/customdatasetyolo/data.yaml', epochs=1,imgsz=64, verbose=True)
</code></pre>
<p>Very strangely however, when I replace the relative paths with absolute paths, like so:</p>
<pre><code>path: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo
train: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/train
val: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: ['5']
</code></pre>
<p>training works without issue. Using absolute pathing is not an option for me, as this application needs to be reproductible on others machines.</p>
","0","Question"
"79076968","","<p>I have a nan R2 score problem with grid search.</p>
<pre><code>FDODB=pd.read_excel('Final Training Set for LOGO.xlsx')
array = FDODB.values
X = array[:,2:126]
Y = array[:,1]
Compd = array[:,0]

scaler = StandardScaler()
X = scaler.fit_transform(X)

params = {
    'max_depth': [5,7,9],
    'learning_rate': [0.03,0.05,0.07],
    'n_estimators': [200,300,400],
    'min_child_weight': [5,7,9],
    'subsample': [0.3, 0.5, 0.7],
    'base_score': [0.4, 0.5, 0.6]
}

xgb_reg = XGBRegressor(tree_method='hist', device='cuda')
logo = LeaveOneGroupOut()

grid_search = GridSearchCV(
    estimator=xgb_reg,
    param_grid=params,
    scoring='r2',
    error_score=&quot;raise&quot;,
    cv=logo,
    verbose=2,
    n_jobs=-1,
    return_train_score=True
)

grid_search.fit(X, Y, groups=Compd)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print('Best hyperparameters:', best_params)
print('Best R2 score:', best_score)
</code></pre>
<p>Output:</p>
<pre><code>c:\\Anaconda\\envs\\machinelearning\\Lib\\site-packages\\sklearn\\model_selection_search.py:1102: UserWarning: One or more of the test scores are non-finite: \[nan\]
warnings.warn(
Best hyperparameters: {'base_score': 0.5, 'learning_rate': 0.03, 'max_depth': 8, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.3}
Best R2 score: nan
</code></pre>
<p>When I used 10-fold CV for grid search, I had any problem to get R2 scores.</p>
<p>But whenever I tried LOO or LOGO CV, all R2 scores are just nan. I checked if my training set had nan values, but it was fine.</p>
<p>What is the possible cause of this and how to resolve it?</p>
","1","Question"
"79084313","","<p>I am trying to use a custom boid flocking environment with gymnasium and stable baselines. I have a custom policy and training loop.<br />
My action and observation spaces are as follows:</p>
<pre><code>min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32)
max_action = np.array([5, 5] * len(self.agents), dtype=np.float32)
</code></pre>
<pre><code>min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32)
max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32)
</code></pre>
<p>Training Code:</p>
<pre><code>import numpy as np
import torch as th
from Parameters import *
from stable_baselines3 import PPO
from main import FlockingEnv, CustomMultiAgentPolicy
from Callbacks import TQDMProgressCallback, LossCallback
import os
from stable_baselines3.common.vec_env import DummyVecEnv


if os.path.exists(Results[&quot;Rewards&quot;]):
    os.remove(Results[&quot;Rewards&quot;])
    print(f&quot;File {Results['Rewards']} has been deleted.&quot;)

if os.path.exists(&quot;training_rewards.json&quot;):
    os.remove(&quot;training_rewards.json&quot;)
    print(f&quot;File training_rewards has been deleted.&quot;)    

def seed_everything(seed):
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    th.manual_seed(seed)
    th.cuda.manual_seed(seed)
    th.backends.cudnn.deterministic = True
    env.seed(seed)
    env.action_space.seed(seed)


loss_callback = LossCallback()
env = DummyVecEnv([lambda: FlockingEnv()])

seed_everything(SimulationVariables[&quot;Seed&quot;])

# # Model Training
model = PPO(CustomMultiAgentPolicy, env, tensorboard_log=&quot;./ppo_Agents_tensorboard/&quot;, verbose=1)
model.set_random_seed(SimulationVariables[&quot;ModelSeed&quot;])
progress_callback = TQDMProgressCallback(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;])
# Train the model
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])
</code></pre>
<p>Error:</p>
<pre><code>Using cuda device
Traceback (most recent call last):
File &quot;D:\Thesis_\FlockingFinal\MultiAgentFlocking\Training.py&quot;, line 45, in &lt;module&gt;
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])      
File &quot;C:\Python312\Lib\site-packages\stable_baselines3\ppo\ppo.py&quot;, line 315, in learn
return super().learn(
            ^^^^^^^^^^^^^^
File &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py&quot;, line 287, in learn
total_timesteps, callback = self._setup_learn(
                                 ^^^^^^^^^^^^^^^^^^
File &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\base_class.py&quot;, line 423, in _setup_learn
self._last_obs = self.env.reset()  # type: ignore[assignment]
                      ^^^^^^^^^^^^^^^^
File &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py&quot;, line 77, in reset
obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: too many values to unpack (expected 2)
</code></pre>
<p>I used similar seeding function with gym as well but no error, I thought it might be causing error but here even when I don't use it the error doesn't go away.</p>
","0","Question"
"79084869","","<p>I'm trying to make a classification network for IDing pictures from the cifar10 dataset.
When I try to use the summary() function, I keep getting this error.</p>
<pre class=""lang-py prettyprint-override""><code>ValueError                                Traceback (most recent call last)
Cell In[267], line 4
      1 #base_model.summary()
      2 #top_model.summary()
      3 #print(base_model.output_shape)
----&gt; 4 model2.summary()

File c:\Users\noahc\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File c:\Users\noahc\anaconda3\Lib\site-packages\optree\ops.py:747, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    745 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    746 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 747 return treespec.unflatten(map(func, *flat_args))

ValueError: Undefined shapes are not supported.
</code></pre>
<p>Here's the code...</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from keras.applications import VGG16

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

top_model = tf.keras.Sequential([
    layers.Flatten(input_shape=base_model.output_shape[1:]),
    layers.Dense(10, activation='softmax')
])

for layer in base_model.layers[:10]:
    layer.trainable = False

model2 = tf.keras.models.Sequential([
    base_model,
    top_model
])

model2.summary() # Error Occurs Here
</code></pre>
<p>I've done a summary for the base and top model and it works fine. But when I test model2 the error occurs. Have no idea why. Not sure what it means by 'undefined' shapes. Not sure what else to try. The summary worked when I only took the first 11 or 15 layers of the vgg16. I've heard it might be a problem with the python version itself, but idk...</p>
","0","Question"
"79086293","","<p>I'm learning about binary classification with Gaussian Processes and I am comparing GPy with scikit-learn on a toy 1D problem inspired by <a href=""https://krasserm.github.io/2020/11/04/gaussian-processes-classification"" rel=""nofollow noreferrer"">Martin Krasser's blog post</a>. Both implementations (GPy and scikit-learn) seem to use a similar setup with a RBF kernel. After optimizing the kernel hyperparameters, the lenghtscales are similar, but the variances differ by a lot. The GPy kernel variance appears to be too small.</p>
<p><strong>How can I modify my GPy implentation and obtain a result similar to those with scikit-learn?</strong> I suspect it has to do with the internal implentation of each algorithm, but I can't tell what's causing this drastic difference. I explain further below why I believe my GPy implementation needs to be fixed.</p>
<p>Implementation details: Python 3.9 with GPy 1.13.2 and scikit-learn 1.5.1. Reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# Part 1: toy dataset creation
##############################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2  # latent function
t = bernoulli.rvs(sigmoid(a))    # Bernoulli training data (0s and 1s)

#####################################
# Part 2: scikit-learn implementation
#####################################

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import ConstantKernel, RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
        * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
    kernel=rbf,
    optimizer='fmin_l_bfgs_b',
    n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# Part 3: GPy implementation
############################

import GPy

kern = GPy.kern.RBF(
    input_dim=1,
    variance=1.,
    lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
    X=X,Y=t, kernel=kern, 
    inference_method=GPy.inference.latent_function_inference.laplace.Laplace(),    
    likelihood=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
    num_restarts=10, optimizer='lbfgs',
    verbose=True, robust=True)

print(m.kern)
#  rbf.         |               value  |  constraints  |  priors
#  variance     |  0.8067562453940487  |  0.001,10.0   |        
#  lengthscale  |  0.8365668826459536  |  0.001,10.0   |
</code></pre>
<p>The lenghtscale values are roughly similar (0.858 vs 0.836), but the variance values are very different (1.5**2 = 2.25 for scikit-learn and only 0.806 for GPy).</p>
<p>The reason why I believe my GPy implementation needs adjustments is that the true latent function (see &quot;a&quot; in part 1 of code above) does not closely match the predicted one even with +/- 2 standard deviation bounds. On the other hand, the scikit-learn implementation matches it reasonably well (it it possible to retrieve the latent function mean and std. deviation with scikit-learn <a href=""https://stackoverflow.com/a/71847135"">as shown here</a>).</p>
<p><a href=""https://i.sstatic.net/6533EQvB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6533EQvB.png"" alt="""" /></a>
<sup><i> Left: predicted probabilities are similar with both models (makes sense since they share similar lenghtscale values). Right: the predicted latent function of GPy does not fit the true latent function as well as the scikit-learn model does. </i></sup></p>
<p>What I've tried so far, without significant change in the results:</p>
<ul>
<li>Input feature (X) normalization</li>
<li>Use GPy.inference.latent_function_inference.expectation_propagation.EP() as the GPy inference method instead of the Laplace one</li>
<li>Add a WhiteKernel component to the scikit-learn implementation, as suggested <a href=""https://stackoverflow.com/a/64693040"">here</a></li>
</ul>
","0","Question"
"79088393","","<p>I'm a little puzzled where (and if) EOS tokens are being added when using Huggignface's trainer classes to train a T5 (LongT5 actually) model.</p>
<p>The data set contains pairs of text like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>from</th>
<th>to</th>
</tr>
</thead>
<tbody>
<tr>
<td>some text</td>
<td>some corresponding text</td>
</tr>
<tr>
<td>some other text</td>
<td>some other corresponding text</td>
</tr>
</tbody>
</table></div>
<p>The tokenizer has been custom trained:</p>
<pre><code>tokenizer = SentencePieceUnigramTokenizer()
tokenizer.train_from_iterator(iterator=iterator, vocab_size=32_128, show_progress=True, unk_token=&quot;&lt;unk&gt;&quot;)
</code></pre>
<p>and is loaded like this:</p>
<pre><code>tokenizer = T5TokenizerFast(tokenizer_file=&quot;data-rb-25000/tokenizer.json&quot;,  
                            padding=True, bos_token=&quot;&lt;s&gt;&quot;, 
                            eos_token=&quot;&lt;/s&gt;&quot;,unk_token=&quot;&lt;unk&gt;&quot;, 
                            pad_token=&quot;&lt;pad&gt;&quot;)
</code></pre>
<p>Before training, the data set is tokenized and examples that have a too high token count are filtered out, like so:</p>
<pre><code>MAX_SEQUENCE_LENGTH = 16_384 / 2

def preprocess_function(examples):
    inputs = tokenizer(
        examples['from'],
        truncation=False,  # Don't truncate yet
        padding=False,     # Don't pad yet
        return_length=True,
    )
    labels = tokenizer(
        examples['to'],
        truncation=False,
        padding=False,
        return_length=True,
    )

    inputs[&quot;input_length&quot;] = inputs[&quot;length&quot;]
    inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
    inputs[&quot;label_length&quot;] = labels[&quot;length&quot;]

    inputs.pop(&quot;length&quot;, None)

    return inputs

tokenized_data = dataset.map(preprocess_function, batched=True, remove_columns=dataset[&quot;train&quot;].column_names)

def filter_function(example):
    return example['input_length'] &lt;= MAX_SEQUENCE_LENGTH and example['label_length'] &lt;= MAX_SEQUENCE_LENGTH

filtered_data = tokenized_data.filter(filter_function)
</code></pre>
<p>Training is done like this:</p>
<pre><code>from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=&quot;google/long-t5-tglobal-base&quot;)

from transformers import AutoModelForSeq2SeqLM, AutoConfig

config = AutoConfig.from_pretrained(
    &quot;google/long-t5-tglobal-base&quot;,
    vocab_size=len(tokenizer),
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    decoder_start_token_id=tokenizer.pad_token_id,
)

model = AutoModelForSeq2SeqLM.from_config(config)

from transformers import GenerationConfig

generation_config = GenerationConfig.from_model_config(model.config)
generation_config._from_model_config = False
generation_config.max_new_tokens = 16_384

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;rb-25000-model&quot;,
    eval_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    logging_steps=1,
    predict_with_generate=True,
    load_best_model_at_end=True,
    bf16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=filtered_data[&quot;train&quot;],
    eval_dataset=filtered_data[&quot;test&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    generation_config=generation_config,
)

trainer.train()
</code></pre>
<p>I know that the tokenizer doesn't add the EOS token:</p>
<pre><code>inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=&quot;pt&quot;)
labels = inputs[&quot;input_ids&quot;]

print(labels)
print(tokenizer.convert_tokens_to_ids(['&lt;s&gt;'])[0])
print(tokenizer.convert_tokens_to_ids(['&lt;pad&gt;'])[0])
print(tokenizer.convert_tokens_to_ids(['&lt;unk&gt;'])[0])
print(tokenizer.convert_tokens_to_ids(['&lt;/s&gt;'])[0])

print(tokenizer.convert_ids_to_tokens([1]))
</code></pre>
<p>Output:</p>
<pre><code>tensor([[1, 10356, 1, 5056],
        [1, 10356, 16002, 16002]])
16000
16002
0
16001
['▁']
</code></pre>
<p>(I don't really understand what's that strange token with index 1.</p>
<p>Anyway, I was wondering if the Trainer class or the DataCollator actually adds the EOS. I did not find any examples online of how and where to add EOS.</p>
<p>I suspect it's not there, because after training the model it doesn't stop generating until it reaches max_new_tokens (set to pretty high).</p>
<p>What's the best practice here? Where should I add EOS? Is there anything else about this code that should be checked or that looks weird for more experienced eyes?</p>
","1","Question"
"79090407","","<p>I’m using scikit-learn’s SVC for multiclass classification of the iris dataset and one class has almost all its data points as support vectors. Is this expected, or could there be an issue with my implementation or parameter setup?</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

# Read the Excel file
df = pd.read_excel(&quot;Classification iris.xlsx&quot;, index_col=&quot;instance_id&quot;)

# Split the data into training and test data
train_data = pd.concat([
    df.loc[1:35],
    df.loc[51:85],
    df.loc[101:135]
])

test_data = pd.concat([
    df.loc[36:50],
    df.loc[86:100],
    df.loc[136:150]
])

# Seperate the features and target variables
train_features = train_data.drop(columns=['class'])
train_target = train_data['class']
test_features = test_data.drop(columns=['class'])
test_target = test_data['class']

# Build the Support Vector Machine with One-vs-Rest strategy
svm = OneVsRestClassifier(SVC(kernel='linear', C=1e5))
svm.fit(train_features, train_target)

# Make predictions
targets_train_pred = svm.predict(train_features)
targets_test_pred = svm.predict(test_features)

# Calculate total errors
total_training_error =  sum(targets_train_pred != train_target) / len(train_data)
total_testing_error = sum(targets_test_pred != test_target) / len(test_data)

# Print the total training and testing errors
print(f&quot;Q2.2.2 Calculation using Standard SVM Model:\ntotal training error: {total_training_error:.10f}, total testing error: {total_testing_error:.10f}&quot;)

# Initialize lists to store results
classes = ['setosa', 'versicolor', 'virginica']
linear_separable = []

for i, class_name in enumerate(classes):
    # Get the binary classifier for the current class
    classifier = svm.estimators_[i]
    
    # Calculate the errors for each class
    train_class_error = sum((targets_train_pred == class_name) != (train_target == class_name)) / len(train_data)
    test_class_error = sum((targets_test_pred == class_name) != (test_target == class_name)) / len(test_data)
    
    # Get the weight vector w and bias b
    w = classifier.coef_[0]
    b = classifier.intercept_[0]
    support_vector_indices = classifier.support_

    # Print the results for each class in the proper format
    print(f&quot;\nclass {class_name}:&quot;)
    print(f&quot;training error: {train_class_error:.10f}, testing error: {test_class_error:.10f}&quot;)
    print(f&quot;w: [{', '.join([f'{x:.10f}' for x in w])}]&quot;)
    print(f&quot;b: {b:.10f}&quot;)
    print(f&quot;support vector indices: {support_vector_indices.tolist()}&quot;)
    
    # Check if the class is linearly separable
    if train_class_error == 0:
        linear_separable.append(class_name)

print(f&quot;\nLinear separable classes: {', '.join(linear_separable)}&quot;)
</code></pre>
<p>This is the output:</p>
<pre><code>Q2.2.2 Calculation using Standard SVM Model:
total training error: 0.0571428571, total testing error: 0.0888888889

class setosa:
training error: 0.0000000000, testing error: 0.0000000000
w: [0.0097327108, 0.5377790363, -0.8273513712, -0.3820427629]
b: 0.7734548984
support vector indices: [42, 23, 24]

class versicolor:
training error: 0.0000000000, testing error: 0.0000000000
w: [1.8485997715, -4.5023738999, -1.1043393026, 0.3212849949]
b: 5.6773042297
support vector indices: [1, 2, 8, 9, 13, 25, 27, 34, 71, 72, 73, 77, 78, 80, 81, 86, 88, 89, 90, 91, 92, 93, 96, 99, 100, 102, 103, 104, 35, 36, 37, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 52, 55, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69]

class virginica:
training error: 0.0000000000, testing error: 0.0000000000
w: [-3.6465034341, -5.1763639697, 7.4285254512, 11.0024158268]
b: -17.5703922240
support vector indices: [55, 57, 62, 68, 96, 97, 99, 103]

Linear separable classes: setosa, versicolor, virginica
</code></pre>
<p>Since there are only 35 training data points of Versicolor, I did not expect them all to be support vectors based on my theoretical understanding of SVMs. Also having now training error for the individual classes seems a bit odd to me.</p>
","0","Question"
"79091245","","<p>I'm experiencing a connection issue with H2O when running it on Windows Subsystem for Linux (WSL) after a recent Windows update. Here's a detailed breakdown of the problem and the troubleshooting steps I've taken so far:</p>
<p><strong>System Information:</strong></p>
<ul>
<li>Operating System: Windows with WSL (Linux kernel version
5.15.153.1-microsoft-standard-WSL2)</li>
<li>WSL Distribution: Ubuntu 24.04</li>
<li>Java Version: Java 11.0.24</li>
<li>Python Version: Tested with Python 3.7, 3.9, 3.10, 3.11, and 3.12</li>
<li>Anaconda Version: Anaconda3-2024.06-1-Linux-x86_64</li>
<li>H2O Versions: 3.42.0.2 up to 3.46.0.5</li>
</ul>
<p><strong>Problem Description:</strong>
When I start H2O on WSL using h2o.init() in Python, it attempts to form a cloud of size 2, including the node at 10.255.255.254:54321, but fails with a &quot;Connection refused&quot; error. This issue does not occur on a dedicated PC with Ubuntu OS.</p>
<p><strong>Steps Taken:</strong></p>
<ol>
<li><p>Force Local Connection: Used h2o.init(ip=&quot;127.0.0.1&quot;, port=54321,
force_connect=True) to ensure H2O operates only locally. No success.</p>
</li>
<li><p>Network Configuration: Verified that no firewall settings are
blocking necessary ports. Temporarily disabled the firewall, but the
issue persisted.</p>
</li>
<li><p>WSL Configuration: Ensured WSL is up to date using wsl --update.
Restarted WSL and the machine, but the problem remains.</p>
</li>
<li><p>H2O Configuration: Checked for configurations that might instruct
H2O to connect to other nodes. Set H2O to use only the local IP
127.0.0.1.</p>
</li>
<li><p>Logs Analysis: Reviewed various H2O logs, all indicating the same
connection error with the node 10.255.255.254:54321.</p>
</li>
<li><p>Connection Test: Tested the connection using python3 -m http.server
54321 and was able to access the server from outside WSL without
issues.</p>
</li>
<li><p>Comparison with Ubuntu Laptop: On a laptop running native Ubuntu,
H2O functions correctly, suggesting the problem is specific to WSL.</p>
</li>
</ol>
<p><strong>Additional Information:</strong>
Until May 2024, H2O worked perfectly on my system. The only changes made since it last worked were keeping Windows and WSL updated. The issue was detected around 20/09/2024.</p>
<p>What might be causing this problem?</p>
","-1","Question"
"79096421","","<p>I am developing a Python script to test an algorithm. I have a dataset that I need to split into 80% for training and 20% for testing. However, I want to save the test set for further analysis, ensuring no overlap with previous test sets.</p>
<p>Although my code works well overall, I encountered one issue: the test dataset sometimes contains records that were already selected in previous test runs due to the random selection process.</p>
<p>In the end of the process all 100% of the records should be tested at one of the runs</p>
<p>To clarify with an example:</p>
<ul>
<li>On the first run, my dataset <code>{0,1,2,3,4,5,6,7,8,9}</code> is split into a training set <code>{0,1,2,4,5,7,8,9}</code> and a test set <code>{3,6}</code>.</li>
<li>On the second run, the training set is <code>{0,1,2,3,4,5,7,9}</code> and the test set is <code>{6,8}</code>.</li>
</ul>
<p>As you can see, the record <code>{6}</code> was selected twice for testing, which I want to avoid.</p>
<p>How can I modify the code to ensure that the 20% test set is chosen randomly each time but excludes any records that were previously selected?</p>
<p>Here is the current code:</p>
<pre><code>df = pd.read_csv(&quot;CustomersInfo.csv&quot;)
y = df['CustomerRank']
X = df.drop('CustomerRank', axis=1, errors='ignore')


#-------------------------------------------------------------------
#This is the part that need to be fixed
for RandStat in [11, 22, 33, 44, 55]:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RandStat)
#-------------------------------------------------------------------



    clf = XGBClassifier(random_state=RandStat)
    clf.fit(X_train, y_train)
    fnStoreAnalyse(y_train)
</code></pre>
","-1","Question"
"79103126","","<p>I'm working on comparing the impact of pruning on a CoreML model.</p>
<p>While I can easily measure changes in the file size (in kB), I'm struggling to estimate the changes in the number of model parameters, as CoreML does not offer a direct method like PyTorch's <code>model.parameters()</code>. How can I estimate or calculate the number of parameters in a pruned CoreML model?</p>
","0","Question"
"79104305","","<p>I am fine-tuning the Llama 3.1 model in Google Colab Pro using an A100 GPU with a custom dataset (using LoRA techniques) via the Unsloth library. Below is the LoRA code I am using:</p>
<pre><code>max_seq_length = 2048
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                    &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    lora_alpha=16,
    lora_dropout=0,  # Supports any, but = 0 is optimized
    bias=&quot;none&quot;,     # Supports any, but = &quot;none&quot; is optimized
     
    use_gradient_checkpointing=&quot;unsloth&quot;,  # True or &quot;unsloth&quot; for very long context
    random_state=3407,
    use_rslora=False,  # We support rank stabilized LoRA
    loftq_config=None,  # And LoftQ
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,  # Can make training 5x faster for short sequences.
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps=60,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim=&quot;adamw_8bit&quot;,
        weight_decay=0.01,
        lr_scheduler_type=&quot;linear&quot;,
        seed=3407,
        output_dir=&quot;outputs&quot;,
    ),
)
</code></pre>
<p>When loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but I am setting it to 2048 in this example since it consumes more compute and VRAM. Additionally, the dtype parameter automatically detects if your GPU supports the BF16 format for more stability during training (this feature is restricted to Ampere and more recent GPUs).</p>
<p>My Questions:</p>
<ul>
<li>If I set max_seq_length to 2048 while training, what will be my model's context length after training, 128k or 2048?</li>
<li>After training the model, can we utilize a context length of 128k, or will it still be limited to 2048?</li>
</ul>
","0","Question"
"79105371","","<p>The task is as follows: the site needs to add functionality for removing the background from the image of a car disk.</p>
<p>I decided to use the rembg library as a basis: <a href=""https://github.com/danielgatis/rembg"" rel=""nofollow noreferrer"">https://github.com/danielgatis/rembg</a></p>
<p>This library in turn works on the basis of u2net: <a href=""https://github.com/xuebinqin/U-2-Net"" rel=""nofollow noreferrer"">https://github.com/xuebinqin/U-2-Net</a></p>
<p>However, the standard u2net model removes all the background from the outside, leaving the space inside the disk untouched - between the spokes, holes, etc.</p>
<p>After googling a bit, I came to the conclusion that I can further train the u2net model for my specific needs.</p>
<p>The algorithm of actions is as follows:</p>
<ol>
<li>further train the model</li>
<li>load it into rembg</li>
<li>use a custom model for cropping</li>
</ol>
<p>I managed to train the standard u2net model, it cuts out black and white masks perfectly as I need.</p>
<p>However, when converting the model from .pth to .onnx format (which is required for working in rembg), it starts to work poorly.
The masks are blurry and soapy. I tried to convert the standard untrained u2net model
and use it in rembg - the result is the same, the masks are blurry, background cropping does not work.</p>
<p>Therefore, the conclusion is that the training was successful.
The problem is in the conversion.</p>
<p>So here are examples of the mask of my trained model.</p>
<p><a href=""https://i.sstatic.net/2fOLBDWM.jpg"" rel=""nofollow noreferrer"">Original image</a></p>
<p><a href=""https://i.sstatic.net/Tp2lwzrJ.png"" rel=""nofollow noreferrer"">The mask that my trained model generates</a></p>
<p><a href=""https://i.sstatic.net/wiN4P67Y.png"" rel=""nofollow noreferrer"">The mask that my trained model generates after converting to .onnx format</a></p>
<p>To generate a mask in u2net I use:</p>
<pre><code>python3 u2net_test.py
</code></pre>
<p>To generate a mask in rembg I use the command:</p>
<pre><code>rembg i -om -m u2net_custom -x '{&quot;model_path&quot;: &quot;~/.u2net/u2net_custom.onnx&quot;}' 55.jpg 55.png
</code></pre>
<p>I tried to convert the finished model. Here is the conversion code:</p>
<pre><code>import torch
import torch.onnx
from model.u2net import U2NET

def load_model(model_path, model_class):
    checkpoint = torch.load(model_path, map_location='cpu')
    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
        model = model_class()
        model.load_state_dict(checkpoint['state_dict'])
    else:
        model = model_class()
    model.eval()
    return model

def convert_to_onnx(model, output_path):
    dummy_input = torch.randn(1, 3, 320, 320)
    torch.onnx.export(model, dummy_input, output_path, opset_version=12,
                          dynamic_axes={'input': {0: 'batch_size', 2: 'height', 3: 'width'},
                                        'output': {0: 'batch_size', 2: 'height', 3: 'width'}})
    print(f&quot;success {output_path}&quot;)

if __name__ == &quot;__main__&quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&quot;conversion PyTorch to ONNX&quot;)
    parser.add_argument('--model-path', type=str, required=True, help='path to .pth file')
    parser.add_argument('--output-path', type=str, required=True, help='save ONNX file')

    args = parser.parse_args()
    model = load_model(args.model_path, U2NET)
    convert_to_onnx(model, args.output_path)
</code></pre>
<p>and tried to save the model during the training process:</p>
<pre><code>        if ite_num % save_frq == 0:
            timestamp = int(time.time())
            filePath = model_dir + model_name+&quot;_%d_%d.&quot; % (ite_num, timestamp)

            torch.save(net.state_dict(), filePath + 'pth')

            dummy_input = torch.randn(1, 3, 320, 320)
            net.eval()
            torch.onnx.export(net, dummy_input, filePath + 'onnx', opset_version=12)

            running_loss = 0.0
            running_tar_loss = 0.0
            net.train()  # resume train
            ite_num4val = 0
</code></pre>
<p>I tried changing settings, changing library versions, changing opset_version and everything else that chatGpt suggests.
The result is always the same.
After conversion the model stops working.</p>
<p>What mistakes did I make?</p>
","2","Question"
"79106002","","<p>Suppose I create a sequential input LSTM in Tensorflow along the lines of:</p>
<pre><code>def Sequential_Input_LSTM(df, input_sequence):
    df_np = df.to_numpy()
    X = []
    y = []
    
    for i in range(len(df_np) - input_sequence):
        row = [a for a in df_np[i:i + input_sequence]]
        X.append(row)
        label = df_np[i + input_sequence]
        y.append(label)
        
    return np.array(X), np.array(y)

X, y = Sequential_Input_LSTM(df_data , 10) # pandas DataFrame df_data contains our data
</code></pre>
<p>In this example, I slice my data in <code>X</code> (input vector) and <code>y</code> (labels) in such a way that e.g. the first 10 values (sequence length) serve as X and the 11th value is the first y. Then, the window of 10 values is moved one step to the right (one timestep further) and we take again 10 values for <code>X</code> and the value after this second row as the next <code>y</code>, and so on.</p>
<p>Then suppose I take a part of <code>X</code> as my <code>X_test</code>, and use a LSTM <code>model</code> to make a time-series prediction, like <code>predictions = model.predict(X_test)</code>.</p>
<p>When I actually tried this, and plotted the results from <code>predict(X_test)</code>, it looks like the <code>y</code> array and the predictions results are synchronized without further adjustments. I expected that I would have to shift the prediction array manually 10 timesteps to the right when plotting it together with the labels, since I cannot explain where the first 10 timestamps of prediction come from.</p>
<p>Where do the predictions for the first 10 timesteps of <code>X_test</code> come from, seeing as the model has not received 10 input sequence values yet? Does Tensorflow use the last timesteps in <code>X_test</code> to create the predictions of the first 10 values, or are the predictions at the beginning just pure guesses?</p>
","1","Question"
"79107349","","<p>I am using the auto-mpg dataset . I am giving the link of the dataset below:</p>
<p><a href=""https://www.kaggle.com/datasets/uciml/autompg-dataset"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/uciml/autompg-dataset</a></p>
<p>I am giving the code below:</p>
<pre><code>df = pd.read_csv('data/auto-mpg.csv')

df.head()
df = df.drop('car name', axis=1)
X = df

X.head()
y = df['mpg']

y.head()
SEED = 1
# Split the data into 70% train and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)

# Instantiate a DecisionTreeRegressor dt
dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)

# Compute the array containing the 10-folds CV MSEs
MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv = 10,
                                    scoring = 'neg_mean_squared_error', n_jobs = 1)

RMSE_CV = (MSE_CV_scores.mean())**(1/2)
</code></pre>
<p>#Error
ValueError:
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.</p>
<p>Below are more details about the failures:</p>
<pre><code>--------------------------------------------------------------------------------
10 fits failed with the following error:

          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: could not convert string to float: '?'
</code></pre>
","-3","Question"
"79109880","","<p>I need to split a dataset into train/test with the following constraints:</p>
<ol>
<li>The approximate train/test ration split is honored (for example 70/30)</li>
<li>Each row in the dataset has a key. Those keys are not unique, meaning I could have 1000s rows with 1 key, and another 2 rows with another key. I want to guarantee that the same key does not exist in both the train and test sets.</li>
</ol>
<p>I tried everything online (including similar topics here), like GroupKFold etc. But it seems I cannot get anything to do that.
Any ideas?</p>
","0","Question"
"79110858","","<p>I'm trying to change the background behind the detected body segment, but I can't get it to work. Even though I'm modifying the <code>background(...)</code> function, it keeps defaulting to white. Can anyone explain why this is happening?</p>
<p>This is the link to the file I am working on :
<a href=""https://editor.p5js.org/speedyonion/sketches/X9mwX9XB9"" rel=""nofollow noreferrer"">https://editor.p5js.org/speedyonion/sketches/X9mwX9XB9</a></p>
<pre><code>let bodySegmentation;
let video;
let segmentation;

let options = {
  maskType: &quot;parts&quot;,
};

function preload() {
  bodySegmentation = ml5.bodySegmentation(&quot;BodyPix&quot;, options);
}

function setup() {
  createCanvas(640, 480);
  // Create the video
  video = createCapture(VIDEO);
  video.size(640, 480);
  video.hide();

  bodySegmentation.detectStart(video, gotResults);
}

function draw() {
  background(0,0,0);
  image(video, 0, 0);
  if (segmentation) {
    image(segmentation.mask, 0, 0, width, height);
  }
}

// callback function for body segmentation
function gotResults(result) {
  segmentation = result;
}
</code></pre>
","1","Question"
"79114762","","<p>I have a dataframe called &quot;vehicles&quot; with 8 columns. 7 are numerical but the column named 'Car_name'  which is index 1 in the dataframe and  is categorical. i need to encode it</p>
<p>i tried this code and wont work</p>
<pre><code>ohe = OneHotEncoding(categorical_features = [1])

vehicles_enc = ohe.fit_transform(vehicles).toarray()

TypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'categorical_features'
</code></pre>
<p>this  however works perfectly in a youtube vid i used.</p>
","1","Question"
"79116828","","<p>I use Colab for coding and get this error:</p>
<pre><code>AttributeError: module 'keras.api.backend' has no attribute 'clip'.
</code></pre>
<p>I have tried to upgrade TensorFlow and Keras, but I still get the same error.I get this error when fit the model at first epoch.
how could I fix it?</p>
<pre class=""lang-py prettyprint-override""><code>import segmentation_models as sm
model_vgg16=sm.Unet(backbone_name=backbone,input_shape=(256,256,3),classes=4,activation=&quot;softmax&quot;,encoder_weights=&quot;imagenet&quot;,decoder_use_batchnorm=True,encoder_freeze=False )

model_vgg16.summary()

&quot;&quot;&quot;# loss and metrics&quot;&quot;&quot;

loss=&quot;categorical_crossentropy&quot;

dice_loss=sm.losses.DiceLoss()
focal_loss=sm.losses.CategoricalFocalLoss()

focal_dice_loss=sm.losses.categorical_focal_dice_loss

metric=[sm.metrics.IOUScore(threshold=0.5)]

&quot;&quot;&quot;# compile&quot;&quot;&quot;

lr=0.001
model_vgg16.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),
              loss=[focal_dice_loss],
              metrics=[metric])

history = model_vgg16.fit(preprocessed_x_train, ytrain_categorical, epochs=20,validation_data=(preprocessed_x_val,y_val_categorical),batch_size=32)
</code></pre>
<p>Error:</p>
<pre><code>Epoch 1/20
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-76-887dcd97e6be&gt; in &lt;cell line: 1&gt;()
----&gt; 1 history = model_vgg16.fit(preprocessed_x_train, ytrain_categorical, epochs=20,
      2                     validation_data=(preprocessed_x_val,y_val_categorical),
      3                     batch_size=32)

3 frames
/usr/local/lib/python3.10/dist-packages/segmentation_models/base/functional.py in categorical_focal_loss(gt, pr, gamma, alpha, class_indexes, **kwargs)
    276 
    277     # clip to prevent NaN's and Inf's
--&gt; 278     pr = backend.clip(pr, backend.epsilon(), 1.0 - backend.epsilon())
    279 
    280     # Calculate focal loss

AttributeError: module 'keras.api.backend' has no attribute 'clip'
</code></pre>
","0","Question"
"79125178","","<p><a href=""https://i.sstatic.net/2fBLQLxM.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I'm trying to implement anomaly detection in my C# application using the Accord.NET library, specifically with the Isolation Forest algorithm. However, I'm encountering an error indicating that the <code>Accord.MachineLearning.AnomalyDetection</code> namespace does not exist.</p>
<pre><code>\&lt;PackageReference Include=&quot;Accord.MachineLearning&quot; Version=&quot;3.8.0&quot; /\&gt;

\&lt;PackageReference Include=&quot;Accord.Math&quot; Version=&quot;3.8.0&quot; /\&gt;

\&lt;PackageReference Include=&quot;Accord.Statistics&quot; Version=&quot;3.8.0&quot; /\&gt;

\&lt;PackageReference Include=&quot;CsvHelper&quot; Version=&quot;33.0.1&quot; /\&gt;

CS0234: The type or namespace name 'AnomalyDetection' does not exist in the namespace 'Accord.MachineLearning' (are you missing an assembly reference?)
</code></pre>
<pre><code>using CsvHelper;
using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Mvc;
using System.Collections.Generic;
using System.Globalization;
using System.IO;
using System.Linq;
using CVAnomalyDetection.Models;

public class HomeController : Controller
{
    private List&lt;CsvData&gt; _csvData;

    public IActionResult Index()
    {
        return View();
    }

    [HttpPost]
    public IActionResult Upload(IFormFile file)
    {
        if (file != null &amp;&amp; file.Length &gt; 0)
        {
            using (var reader = new StreamReader(file.OpenReadStream()))
            using (var csv = new CsvReader(reader, CultureInfo.InvariantCulture))
            {
                _csvData = csv.GetRecords&lt;CsvData&gt;().ToList();
            }
            return View(&quot;Index&quot;, _csvData);
        }
        return RedirectToAction(&quot;Index&quot;);
    }

    [HttpPost]
    public IActionResult Predict()
    {
        var anomalies = DetectAnomalies(_csvData);
        return View(&quot;AnomalyResults&quot;, anomalies);
    }

    private List&lt;CsvData&gt; DetectAnomalies(List&lt;CsvData&gt; data)
    {
        // One-hot encode categorical columns
        var uniqueValuesProductCategory = data.Select(x =&gt; x.ProductCategory).Distinct().ToList();
        var uniqueValuesCustomerRegion = data.Select(x =&gt; x.CustomerRegion).Distinct().ToList();
        var uniqueValuesPaymentMethod = data.Select(x =&gt; x.PaymentMethod).Distinct().ToList();

        // Create the input data array for anomaly detection
        var combinedData = new List&lt;double[]&gt;();

        foreach (var item in data)
        {
            var features = new List&lt;double&gt;();

            // One-hot encoding for ProductCategory
            foreach (var value in uniqueValuesProductCategory)
            {
                features.Add(value == item.ProductCategory ? 1.0 : 0.0);
            }

            // One-hot encoding for CustomerRegion
            foreach (var value in uniqueValuesCustomerRegion)
            {
                features.Add(value == item.CustomerRegion ? 1.0 : 0.0);
            }

            // One-hot encoding for PaymentMethod
            foreach (var value in uniqueValuesPaymentMethod)
            {
                features.Add(value == item.PaymentMethod ? 1.0 : 0.0);
            }

            combinedData.Add(features.ToArray());
        }

        // Initialize the Isolation Forest
        var isolationForest = new Accord.MachineLearning.IsolationForest()
        {
            NumberOfTrees = 100,
            Contamination = 0.05 // Adjust based on your needs
        };

        // Train the Isolation Forest model with the combined data
        isolationForest.Learn(combinedData.ToArray());

        // Predict anomalies
        var predictions = isolationForest.Decide(combinedData.ToArray());

        // Find and return the actual anomalous records
        return data.Where((d, index) =&gt; predictions[index] == -1).ToList(); // -1 indicates an anomaly
    }


    private double EuclideanDistance(double[] a, double[] b)
    {
        return Math.Sqrt(a.Zip(b, (x, y) =&gt; Math.Pow(x - y, 2)).Sum());
    }
}
</code></pre>
","-2","Question"
"79127884","","<p>I'm trying to learn some time-series neural network ML and was getting weird solutions, so I'm trying to model the simplest non-trivial case I can think of, which is predicting n+1 as the next number in the sequence 0,1,2,3,...n (using an LSTM model).</p>
<p>The training data for each data point is a series of immediately preceding numbers, and I'm assuming it should easily solve the model as long as the data for each training set has length &gt;= 2 (since it's an arithmetic sequence)</p>
<p>The code below is returning a constant for all test data regardless of the size of the training series. Can someone explain what I'm doing wrong?</p>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

import statistics

dim = 5

data = pd.Series(range(0,200))

# Setting 80 percent data for training
training_data_len = math.ceil(len(data) * .8)

# Normalize data
train_data = data[:training_data_len]

#Split dataset
train_data = data[:training_data_len]
test_data = data[training_data_len:]
print(train_data.shape, test_data.shape)

# Selecting values
dataset_train = train_data.values 
# Reshaping 1D to 2D array
dataset_train = np.reshape(dataset_train, (-1,1)) 

# Selecting values
dataset_test = test_data.values
# Reshaping 1D to 2D array
dataset_test = np.reshape(dataset_test, (-1,1))  

X_train = []
y_train = []
for i in range(dim, len(dataset_train)):
    X_train.append(dataset_train[i-dim:i, 0])
    y_train.append(dataset_train[i, 0])


X_test = []
y_test = []
for i in range(dim, len(dataset_test)):
    X_test.append(dataset_test[i-dim:i, 0])
    y_test.append(dataset_test[i, 0])


# The data is converted to Numpy array
X_train, y_train = np.array(X_train), np.array(y_train)

#Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
y_train = np.reshape(y_train, (y_train.shape[0],1))
print(&quot;X_train :&quot;,X_train.shape,&quot;y_train :&quot;,y_train.shape)


# The data is converted to numpy array
X_test, y_test = np.array(X_test), np.array(y_test)

#Reshaping
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))
y_test = np.reshape(y_test, (y_test.shape[0],1))
print(&quot;X_test :&quot;,X_test.shape,&quot;y_test :&quot;,y_test.shape)

# importing libraries
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import SimpleRNN
from keras.layers import Dropout
from keras.layers import GRU, Bidirectional
from keras.optimizers import SGD
from sklearn import metrics
from sklearn.metrics import mean_squared_error

#Initialising the model
regressorLSTM = Sequential()

#Adding LSTM layers
regressorLSTM.add(LSTM(dim, 
                       return_sequences = True, 
                       input_shape = (X_train.shape[1],1)))
regressorLSTM.add(LSTM(dim, 
                       return_sequences = False))

#Adding the output layer
regressorLSTM.add(Dense(1))

#Compiling the model
regressorLSTM.compile(optimizer = 'adam',
                      loss = 'mean_squared_error',
                      metrics = [&quot;accuracy&quot;])

#Fitting the model
regressorLSTM.fit(X_train, 
                  y_train, 
                  batch_size = 1, 
                  epochs = 4)
regressorLSTM.summary()


# predictions with X_test data
y_LSTM = regressorLSTM.predict(X_test)

#Plot for LSTM predictions
plt.plot(train_data.index[dim:], train_data[dim:], label = &quot;train_data&quot;, color = &quot;b&quot;)
plt.plot(test_data.index, test_data, label = &quot;test_data&quot;, color = &quot;g&quot;)
plt.plot(test_data.index[dim:], y_LSTM, label = &quot;y_LSTM&quot;, color = &quot;orange&quot;)
plt.legend()
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.show()
</code></pre>
","-1","Question"
"79130753","","<pre><code># First convolutional layer: input channels = 1, output channels = 32, kernel size = 5x5, padding = 2 (SAME)
self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2)
# First pooling layer: max pooling, kernel size = 2x2, stride = 2
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

# Second convolutional layer: input channels = 32, output channels = 64, kernel size = 5x5, padding = 2 (SAME)
self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)
# Second pooling layer: max pooling, kernel size = 2x2, stride = 2
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
</code></pre>
<p>Why isn't the output after the second convolution layer
14 * 14 * 32 * 64? For the 32-channel input, each convolutional kernel operates on one channel, resulting in 64 different outcomes. Shouldn't the 32 channels be multiplied together?</p>
<p>I got answers like: for every 14 * 14 position of the input, a 5<em>5</em>32 kernel dot product with 5<em>5</em>32 input area will give you a 14*14 single chnnel output. Isn't the kernel size 5 * 5?</p>
","-2","Question"
"79134937","","<p>I would like to run recursive feature elimination with multiple estimator algorithms on multiple number of features and keep the highest f1 score combination on the test data.</p>
<p>Instead of reviewing the result one by one, how to create a code that can generate and show the best number of features with the best algorithm on test data(highest f1, followed by the highest ROC)? My dataset is imbalanced.</p>
<p>I tried the code below that can generate result for different estimator on different number of feature. But I still need to review the result one by one. How to do that?</p>
<pre><code>estimators = [('Logistic Regression', LogisticRegression()),
            ('Random Forest', RandomForestClassifier())]
num_features_to_select = [4,5,7,9,11,15]
    

for estimator_name, estimator in estimators:
    for n_features in num_features_to_select:
        # Create RFE object
        rfe = RFE(estimator=estimator, n_features_to_select=n_features)
        # Fit RFE to data
        rfe.fit(X_resampled,Y_resampled)
        # Get the selected features
        selector = X_resampled.columns[rfe.support_]
        X_train_selected = X_resampled[selector]
        X_test_selected = X_test[selector]
        log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
        pred_test = log_reg_model.predict(X_test_selected)
        pred_test_1 = np.where(pred_test&gt;0.5,1,0)
        logit_roc_auc = roc_auc_score(Y_test, pred_test)
        fpr, tpr, thresholds = roc_curve(Y_test, pred_test)
        precision, recall, thresholds = precision_recall_curve(Y_test, pred_test)
        # Evaluate model performance using cross-validation
        scores = cross_val_score(estimator, X_resampled[selected_features], Y_resampled, cv=5)
        # Print results
        print(f'Estimator: {estimator_name}, Number of Features: {n_features}, Mean CV Score: {scores.mean()}')
        print(f'Estimator: {estimator_name}, Number of Features: {n_features}, ROC: {logit_roc_auc}')
        print(f'Estimator: {estimator_name}, Number of Features: {n_features}, f1 score: {f1_score(Y_test, pred_test_1)}')
        print(f'Estimator: {estimator_name}, Number of Features: {n_features}, PRC AUC:{auc(recall,precision)}')
</code></pre>
","0","Question"
"79134990","","<p>So I was following this tutorial: <a href=""https://docs.fast.ai/tutorial.siamese.html"" rel=""nofollow noreferrer"">Fast Ai Siamese</a>, however after I completed it, I got 50% accuracy. I tried loads of things, but nothing worked. So the only place where I think the problem could be is in the Siamese implementation itself, but I have little experience with fast.ai and no idea on how to fix it or where to even start. Maybe there is something obvious that I'm missing? Anyway, any comment helps. This is my code</p>
<pre class=""lang-py prettyprint-override""><code>class SiameseImage(fastuple):
    
    def show(self, ctx=None, **kwargs):
        if len(self) &gt; 2:
            img1, img2, similarity = self
        else:
            img1, img2 = self
            similarity = 'Undetermined'

        if not isinstance(img1, Tensor):
            if img2.size != img1.size: img2 = img2.resize(img1.size)
            t1 = tensor(img1)
            t2 = tensor(img2)
            t1 = t1.permute(2,0,1)
            t2 = t2.permute(2,0,1)
        else:
            t1 = img1
            t2 = img2
        
        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)
        return show_image(torch.cat([t1, line, t2], dim=2), title = similarity, ctx=ctx, **kwargs) 


class ImageTuple(fastuple):
    
    @classmethod
    def create(cls, fns): return cls(tuple(PILImage.create(f) for f in fns))
    
    def show(self, ctx=None, **kwargs): 
        t1,t2 = self
        if not isinstance(t1, Tensor) or not isinstance(t2, Tensor) or t1.shape != t2.shape: return ctx
        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)
        return show_image(torch.cat([t1,line,t2], dim=2), ctx=ctx, **kwargs)
    

class SiameseModel(Module):
    
    def __init__(self, encoder, head):
        self.encoder = encoder
        self.head = head
    
    def forward(self, x1, x2):
        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim = 1)
        return self.head(ftrs)


class SiameseTransform(Transform):
    
    def __init__(self, files, splits, labels):
        self.labels = labels
        self.splbl2files = [{l: [f for f in files[splits[i]] if parent_label(f) == l] for l in labels}
                          for i in range(2)]
        self.valid = {f: self._draw(f,1) for f in files[splits[1]]}

    def encodes(self, f):
        f2,same = self.valid.get(f, self._draw(f,0))
        img1,img2 = PILImage.create(f),PILImage.create(f2)
        return SiameseImage(img1, img2, int(same))
    
    def _draw(self, f, split=0):
        same = random.random() &lt; 0.5
        cls = parent_label(f)
        if not same: cls = random.choice(L(l for l in self.labels if l != cls)) 
        return random.choice(self.splbl2files[split][cls]),same


def get_x(t): return t[:2]
def get_y(t): return t[2]

def siamese_splitter(model):
    return [params(model.encoder), params(model.head)]

def loss_func(out, targ):
    return CrossEntropyLossFlat()(out, targ.long())

def train(dataset_path):
    files = get_image_files(dataset_path)
    
    labels = list(set(files.map(parent_label)))

    encoder = create_body(resnet50(), cut=-2)
    head = create_head(2048*2, 2, ps=0.5)
    model = SiameseModel(encoder, head)

    splits = RandomSplitter()(files)
    tfm = SiameseTransform(files, splits, labels)
    tls = TfmdLists(files, tfm, splits=splits)
    dls = tls.dataloaders(
        after_item=[Resize(256, method='squash'), ToTensor], 
        after_batch=[IntToFloatTensor, *aug_transforms(flip_vert=True, do_flip=True, max_rotate=50, max_warp=0.4, max_zoom=1.3), Normalize.from_stats(*imagenet_stats)],
        bs = 8
    )
    
    
    torch.cuda.empty_cache()
    learn = Learner(dls, model, loss_func=loss_func, splitter=siamese_splitter, metrics=accuracy)
    learn.fine_tune(
        epochs = 15,
        base_lr=2.51e-5,
        cbs=[SaveModelCallback(monitor='valid_loss'), EarlyStoppingCallback(monitor='valid_loss', patience=5)])
    learn.export('siamese1.pkl')
</code></pre>
<p>I tried changing batch size, different architectures, image sizes, epochs, various batch transformations and other things, but nothing changed, I always got 50% accuracy...</p>
","0","Question"
"79135894","","<p>I'm encountering the error &quot;AttributeError: 'KerasHistory' object has no attribute 'layer'&quot; while working with a Keras model.</p>
<p>I'm trying to access layer information, but it seems I'm referencing the wrong object. I tried to change the name layer to operation, but it's not working.</p>
<p>I'm using TensorFlow v2.17.0. This is the code:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, MaxPooling2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense, Dropout

input_shape = (96, 96, 1)

# Input tensor shape
X_input = Input(input_shape)

# Zero-padding
X = ZeroPadding2D((3,3))(X_input)

# 1 - stage
X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = 'bn_conv1')(X)
X = Activation('relu')(X)
X = MaxPooling2D((3,3), strides= (2,2))(X)

# 2 - stage
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - stage
X = res_block(X, filter= [128,128,512], stage= 3)


# Average Pooling
X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)

# Final layer
X = Flatten()(X)
X = Dense(4096, activation = 'relu')(X)
X = Dropout(0.2)(X)
X = Dense(2048, activation = 'relu')(X)
X = Dropout(0.1)(X)
X = Dense(30, activation = 'relu')(X)


model_1_facialKeyPoints = Model(inputs= X_input, outputs = X)
model_1_facialKeyPoints.summary()
</code></pre>
<p>This is the traceback:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-366-fd266d53d661&gt; in &lt;cell line: 34&gt;()
     32 
     33 
---&gt; 34 model_1_facialKeyPoints = Model( inputs= X_input, outputs = X)
     35 model_1_facialKeyPoints.summary()

4 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/functional.py in _validate_graph_inputs_and_outputs(self)
    692       # Check that x is an input tensor.
    693       # pylint: disable=protected-access
--&gt; 694 
    695       layer = x._keras_history.layer
    696       if len(layer._inbound_nodes) &gt; 1 or (

AttributeError: 'KerasHistory' object has no attribute 'layer'
</code></pre>
","0","Question"
"79139121","","<p>I'm using Tensorflow to develop a simple ML model in Python. The code is below:</p>
<pre><code>import tensorflow as tf
import pandas as pd

# Load CSV Data
def load_data(filename):
    data = pd.read_csv(filename)
    X = data[['X0','X1','X2','X3']]
    Y = data[['Y0','Y1']]
    return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# Build a simple neural network model
model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(2)
])
# Compile the model
model.compile(optimizer='adam',
              loss='mean_squared_error')

# Load validation data
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# Train the model
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)
</code></pre>
<p>Everything runs perfectly until I start to include the validation data, which has the same number of parameters as the training data. Then I get the error</p>
<pre><code>ValueError: Exception encountered when calling Sequential.call().

[1mInvalid input shape for input Tensor(&quot;sequential_1/Cast:0&quot;, shape=(4,), dtype=float32). Expected shape (None, 4), but input has incompatible shape (4,)[0m

Arguments received by Sequential.call():
  • inputs=tf.Tensor(shape=(4,), dtype=int64)
  • training=False
  • mask=None
</code></pre>
<p>Printing the validation and training datasets shows that they have the same dimension, and running <code>print(training_data)</code> and <code>print(validation_data)</code> both give</p>
<pre><code>&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;
</code></pre>
<p>How do I correctly set up the validation data to run inline with the <code>model.fit</code>?</p>
","0","Question"
"79139968","","<p>I am trying to use Python to calculate mean absolute error (MAE) while doing zero-inflated Poisson regression and zero-inflated negative binomial regression.
I separated data into training data and testing data. I use code below but it does not work:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
import statsmodels.formula.api as smf
import tensorflow as tf
df = pd.read_excel('....', sheet_name='Sheet1')
print(df.head())
X = df[['a', 'b', 'c', 'd', 'e', 'f']]
y = df['g']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from statsmodels.discrete.count_model import ZeroInflatedPoisson
y_zip = y_train.values

y_zip_test = y_test.values

X_count =  X_train.values # Predictors for count part
X_zero = X_train.values  # Predictors for zero-inflation part

X_count_test = X_test.values
X_zero_test = X_test.values

# Add a constant for the intercept
X_count = sm.add_constant(X_count)
X_zero = sm.add_constant(X_zero)

# Fit the ZIP model
zip_model = ZeroInflatedPoisson(endog=y_zip, exog=X_count, exog_infl=X_zero, inflation='logit')
zip_model_fit = zip_model.fit()
print(zip_model_fit.summary())


# Make predictions
y_pred = zip_model_fit.predict(X_count_test)

# Calculate MAE
mae = np.mean(np.abs(y_zip_test - y_pred))
print(f'Mean Absolute Error: {mae}')
</code></pre>
<p>The results is below</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[3], line 33
     29 print(zip_model_fit.summary())
     32 # Make predictions
---&gt; 33 y_pred = zip_model_fit.predict(X_count_test)
     35 # Calculate MAE của test
     36 mae = np.mean(np.abs(y_zip_test - y_pred))

File ~\anaconda3\envs\tf\lib\site-packages\statsmodels\base\model.py:1174, in Results.predict(self, exog, transform, *args, **kwargs)
   1127 &quot;&quot;&quot;
   1128 Call self.model.predict with self.params as the first argument.
   1129 
   (...)
   1169 returned prediction.
   1170 &quot;&quot;&quot;
   1171 exog, exog_index = self._transform_predict_exog(exog,
   1172                                                 transform=transform)
-&gt; 1174 predict_results = self.model.predict(self.params, exog, *args,
   1175                                      **kwargs)
   1177 if exog_index is not None and not hasattr(predict_results,
   1178                                           'predicted_values'):
   1179     if predict_results.ndim == 1:

File ~\anaconda3\envs\tf\lib\site-packages\statsmodels\discrete\count_model.py:453, in GenericZeroInflated.predict(self, params, exog, exog_infl, exposure, offset, which, y_values)
    449 params_main = params[self.k_inflate:]
    451 prob_main = 1 - self.model_infl.predict(params_infl, exog_infl)
--&gt; 453 lin_pred = np.dot(exog, params_main[:self.exog.shape[1]]) + exposure + offset
    455 # Refactor: This is pretty hacky,
    456 # there should be an appropriate predict method in model_main
    457 # this is just prob(y=0 | model_main)
    458 tmp_exog = self.model_main.exog

ValueError: shapes (21,6) and (7,) not aligned: 6 (dim 1) != 7 (dim 0)
</code></pre>
<p>The error can be solve by using dataframe.
y_train, X_train = dmatrices(expr, recreated_train_data, return_type='dataframe')</p>
<p>However, I meet problems that model do not converge with information below:</p>
<pre><code>C:\Users\Admin\anaconda3\envs\tf\lib\site-packages\scipy\optimize\_optimize.py:1291: OptimizeWarning: Maximum number of iterations has been exceeded.
  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)
C:\Users\Admin\anaconda3\envs\tf\lib\site-packages\statsmodels\base\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  warnings.warn(&quot;Maximum Likelihood optimization failed to &quot;
</code></pre>
<p>How to resolve this error?</p>
","-1","Question"
"79140091","","<p>I downloaded packages from <a href=""https://github.com/tencent-ailab/IP-Adapter"" rel=""nofollow noreferrer"">https://github.com/tencent-ailab/IP-Adapter</a></p>
<p>run the commands to train an IP-Adapter plus model (input: text + image, output: image):</p>
<pre><code>accelerate launch --num_processes 2 --multi_gpu --mixed_precision &quot;fp16&quot; \
  tutorial_train_plus.py \
  --pretrained_model_name_or_path=&quot;stable-diffusion-v1-5/&quot; \
  --image_encoder_path=&quot;models/image_encoder/&quot; \
  --data_json_file=&quot;assets/prompt_image.json&quot; \
  --data_root_path=&quot;assets/train/&quot; \
  --mixed_precision=&quot;fp16&quot; \
  --resolution=512 \
  --train_batch_size=2 \
  --dataloader_num_workers=4 \
  --learning_rate=1e-04 \
  --weight_decay=0.01 \
  --output_dir=&quot;out_model/&quot; \
  --save_steps=3
</code></pre>
<p>During training, there is the message but the training can be continued:</p>
<pre><code>Removed shared tensor {'adapter_modules.27.to_k_ip.weight', 'adapter_modules.1.to_v_ip.weight', 'adapter_modules.31.to_k_ip.weight', 'adapter_modules.15.to_k_ip.weight', 'adapter_modules.31.to_v_ip.weight', 'adapter_modules.11.to_k_ip.weight', 'adapter_modules.23.to_k_ip.weight', 'adapter_modules.3.to_k_ip.weight', 'adapter_modules.25.to_v_ip.weight', 'adapter_modules.21.to_k_ip.weight', 'adapter_modules.17.to_v_ip.weight', 'adapter_modules.13.to_k_ip.weight', 'adapter_modules.17.to_k_ip.weight', 'adapter_modules.19.to_v_ip.weight', 'adapter_modules.13.to_v_ip.weight', 'adapter_modules.7.to_v_ip.weight', 'adapter_modules.7.to_k_ip.weight', 'adapter_modules.29.to_k_ip.weight', 'adapter_modules.3.to_v_ip.weight', 'adapter_modules.5.to_v_ip.weight', 'adapter_modules.21.to_v_ip.weight', 'adapter_modules.5.to_k_ip.weight', 'adapter_modules.23.to_v_ip.weight', 'adapter_modules.25.to_k_ip.weight', 'adapter_modules.1.to_k_ip.weight', 'adapter_modules.9.to_v_ip.weight', 'adapter_modules.9.to_k_ip.weight', 'adapter_modules.15.to_v_ip.weight', 'adapter_modules.27.to_v_ip.weight', 'adapter_modules.29.to_v_ip.weight', 'adapter_modules.19.to_k_ip.weight', 'adapter_modules.11.to_v_ip.weight'} while saving. This should be OK, but check by verifying that you don't receive anywarning while reloading
</code></pre>
<p>After training is finished and convert the weight to generate ip_adapter.bin, then run the inference code ip_adapter-plus_demo.py with the following model paths in this file:</p>
<pre><code>base_model_path = &quot;SG161222/Realistic_Vision_V4.0_noVAE&quot;
vae_model_path = &quot;stabilityai/sd-vae-ft-mse&quot;
image_encoder_path = &quot;models/image_encoder&quot;
ip_ckpt = &quot;out_model/demo_plus_checkpoint/ip_adapter.bin&quot;
</code></pre>
<p>It shows the error:</p>
<pre><code>raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
        Missing key(s) in state_dict: &quot;1.to_k_ip.weight&quot;, &quot;1.to_v_ip.weight&quot;, &quot;3.to_k_ip.weight&quot;, &quot;3.to_v_ip.weight&quot;, &quot;5.to_k_ip.weight&quot;, &quot;5.to_v_ip.weight&quot;, &quot;7.to_k_ip.weight&quot;, &quot;7.to_v_ip.weight&quot;, &quot;9.to_k_ip.weight&quot;, &quot;9.to_v_ip.weight&quot;, &quot;11.to_k_ip.weight&quot;, &quot;11.to_v_ip.weight&quot;, &quot;13.to_k_ip.weight&quot;, &quot;13.to_v_ip.weight&quot;, &quot;15.to_k_ip.weight&quot;, &quot;15.to_v_ip.weight&quot;, &quot;17.to_k_ip.weight&quot;, &quot;17.to_v_ip.weight&quot;, &quot;19.to_k_ip.weight&quot;, &quot;19.to_v_ip.weight&quot;, &quot;21.to_k_ip.weight&quot;, &quot;21.to_v_ip.weight&quot;, &quot;23.to_k_ip.weight&quot;, &quot;23.to_v_ip.weight&quot;, &quot;25.to_k_ip.weight&quot;, &quot;25.to_v_ip.weight&quot;, &quot;27.to_k_ip.weight&quot;, &quot;27.to_v_ip.weight&quot;, &quot;29.to_k_ip.weight&quot;, &quot;29.to_v_ip.weight&quot;, &quot;31.to_k_ip.weight&quot;, &quot;31.to_v_ip.weight&quot;.
</code></pre>
<p>Any step wrong to cause this error?</p>
","0","Question"
"79141566","","<p>I'm using DDQN with experience replay just like in this tutorial <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a>
except that I'm making the problem a little harder by obscuring x_dot and theta_t(cart velocity and angular velocity of the pole). I then calculate, considering the current state, the previous x_dot, theta_dot, x_dot_dot and theta_dot_dot and do the learning process by using this state space: <code>(x, prev_x_dot, prev_prev_x_dot_dot, theta, prev_theta_dot, prev_prev_theta_dot_dot)</code>.</p>
<p>Anyway, my main issue is that by using the DQN algorithm as described in the above linked tutorial, the algorithm does not converge. I'm considering the learning to be successful if the average length of the last 100 episodes is &gt; 450. When executing, I may see 50-60 consecutive 500 long episodes, but then the episode length randomly swings and goes down to even 20!?!? <strong>I need to push the problem harder by starting from any beginning position</strong> within a certain range(for each x, theta), but the results till now have not been promising.</p>
<p>Is this a normal behaviour for such an algorithm as DQN is? I get that being the policy computed based on previous executions there may be some convergence issue of the loss function, but does this include such severe swings in the performance?</p>
<p>I'm using a net with 3 nonlinear layers, and linear layers of dimension 256x256.</p>
","-1","Question"
"79143377","","<p>I have a trained Keras model saved as <code>model.h5</code>, which I would normally load using <code>keras.models.load_model(&quot;model.h5&quot;)</code>. I'm currently developing a Python desktop application with a GUI and need to package it as a standalone <code>.exe</code> file.</p>
<p>The problem is that importing keras.models significantly increases the size of the <code>.exe</code> file beyond my size limit. I’m looking for a way to load and run the <code>model.h5</code> file without including the entire Keras library in the final package.</p>
<p>Is there a way to load a <code>.h5</code> model file without importing the <code>keras.models</code> module?</p>
<p>Alternatively, is there a tool or method to selectively include only the necessary parts of Keras when packaging the app, instead of bundling the entire library?</p>
<p>Environment:</p>
<p>Python version: 3.11.7</p>
<p>Keras and TensorFlow versions: Keras 2.15.0, TensorFlow 2.15</p>
<p>Packaging tool: PyInstaller</p>
","0","Question"
"79145419","","<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.
To do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=""https://github.com/huggingface/candle"" rel=""nofollow noreferrer"">Candle</a>, but I also looked at <a href=""https://github.com/EricLBuehler/mistral.rs"" rel=""nofollow noreferrer"">Mistral-RS</a>.</p>
<p>Basically, what I'm trying to do is this code fragment:
<a href=""https://huggingface.co/nvidia/NV-Embed-v2"" rel=""nofollow noreferrer"">https://huggingface.co/nvidia/NV-Embed-v2</a>
but with Rust and Candle.</p>
<p>What I tried is to start off with <a href=""https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs"" rel=""nofollow noreferrer"">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>
<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>
<pre><code>Error: missing field `vocab_size` at line 101 column 1
</code></pre>
<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>
<pre><code>Error: cannot find tensor model.embed_tokens.weight
</code></pre>
<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>
","0","Question"
"79147000","","<p>I have a large set CSVs with numeric and text data; here's a sample:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Company ID</th>
<th>Company Name</th>
<th>Group ID</th>
<th>Currency</th>
<th>Amount</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>8494494</td>
<td>Acme Inc</td>
<td>F942G</td>
<td>EUR</td>
<td>$1.56</td>
<td>...</td>
</tr>
<tr>
<td>9283422A</td>
<td>Walmart</td>
<td>XXH3F3</td>
<td>AUD</td>
<td>$5.64</td>
<td>...</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>I have a separate set of CSVs for which the headers may be mislabeled. I'd like to build classifiers to determine whether the columns in this other set are <strong>Company ID</strong>, <strong>Company Name</strong>, or <strong>Group ID</strong>.</p>
<p>The data in these 3 columns are complex enough that I cannot solve this with regexp alone; however, from an eye-test, I believe a simple classifier can be trained to distinguish between them (and also return False for other miscellaneous data such as &quot;AUD&quot;).</p>
<p>What model is best to use here? I've considered passing these strings through BERT and then building a classifier on top of the embeddings. I've also considered just training a RNN at the character level.</p>
<p>~</p>
<p>PS: Two more ideas I had that are a bit more complex:</p>
<ul>
<li>Would passing these strings through a NER model be appropriate? Or would that be overkill since there is not much 'sequential' information here?</li>
<li>Another idea is to take a &quot;word2vec&quot; approach and try to create my own custom embeddings, but use my training data to 'push' the embeddings far from each other for the different columns.</li>
</ul>
","-1","Question"
"79147122","","<p>I have created a CNN model using the MNIST dataset. I want to make predictions for the sequence of numbers present in the images. The technique involves segmenting each image and feeding it into the model, but I am facing difficulties in segmenting numbers from the images because there are two different types of images present. I need a robust technique that removes all the noise and shadows present in the images and segments each number separately.
I am sharing the images here as well.
I am looking for the robust technique and the code.</p>
<p><a href=""https://i.sstatic.net/KifRNuGy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KifRNuGy.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/YbCY1px7.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YbCY1px7.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/pBlxYxKf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBlxYxKf.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/Ei98PBZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ei98PBZP.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/oJu2jfdA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oJu2jfdA.png"" alt=""enter image description here"" /></a></p>
<p><strong>UPDATED QUESTION</strong></p>
<p>Main Goal of segmentation</p>
<p>I am looking for a method that segments or separates each number from the images above. I believe there must be a unique or robust approach that I could use for all kinds of images (given above). I could apply separate methods for binary and color images, but I want to learn a single approach that works for images like the above.</p>
","2","Question"
"79149427","","<p>I trained a resnet50 model on number hand signs from 0 to 5, and Im trying to deploy it to predict real time classes via the laptop's webcam.</p>
<p>although the model has 98% accuracy and I am pretty sure the error's not occurring because the model was trained badly, the real time values are stuck to 1 or 2 classes of the 5, they always predict number 0 and number 2.</p>
<p>this is the code :</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image  # Import PIL for image conversion

# Define the model architecture and load weights
class ResNet50Modified(nn.Module):
    def __init__(self, num_classes=6):
        super(ResNet50Modified, self).__init__()
        self.model = models.resnet50(pretrained=True)  # Use pretrained=True for better performance
        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

    def forward(self, X):
        return self.model(X)

# Load the trained model
model = ResNet50Modified(num_classes=6)
# Load the model's state_dict for CPU
model.load_state_dict(torch.load(&quot;resnet50_modified1.pth&quot;, map_location=torch.device('cpu')))
model.eval()

# Define transformations to match training preprocessing
preprocess = transforms.Compose([
    transforms.Resize((64, 64)),  # Resize to input size of model
    transforms.ToTensor(),  # Convert to tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per ResNet standards
])

# Labels for the signs
class_names = ['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5']  # Replace with actual sign names

# Open webcam for real-time prediction
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print(&quot;Error: Could not open webcam.&quot;)
    exit()

while True:
    ret, frame = cap.read()
    if not ret:
        print(&quot;Error: Could not read frame.&quot;)
        break

    # Convert the frame from BGR (OpenCV) to RGB (PIL)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Convert NumPy array to PIL Image
    pil_image = Image.fromarray(frame_rgb)

    # Preprocess the frame
    input_image = preprocess(pil_image)  # Use the PIL image for preprocessing
    input_image = input_image.unsqueeze(0)  # Add batch dimension

    # Predict using the model
    with torch.no_grad():
        outputs = model(input_image)
        
        # Apply softmax to get probabilities
        probabilities = torch.softmax(outputs, dim=1)
        
        # Get the predicted class and confidence
        _, predicted = torch.max(probabilities, 1)
        confidence = probabilities[0][predicted].item() * 100  # Convert to percentage
        label = class_names[predicted.item()]

    # Display the result with confidence level
    cv2.putText(frame, f&quot;Predicted: {label}, Confidence: {confidence:.2f}%&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow(&quot;Sign Detection&quot;, frame)

    # Exit on pressing 'q'
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
</code></pre>
<p>the confidence levels are always high but with the wrong labels, even if I show on the webcam 5 fingers it's stuck on zero.</p>
<p>I feel like the problem is with the frame processing, does anyone have any insight on this?</p>
","0","Question"
"79152799","","<p>I trained a two-classification model and wanted to use shap.Explainer to analyze feature contribution.
The code was:</p>
<pre><code>def f(x):
    return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[0,1,2,3], :])
</code></pre>
<p>The results of <code>shap_values.values</code> was:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Feature 1</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>sample 0</td>
<td>-0.009703</td>
<td>...</td>
</tr>
<tr>
<td>sample 1</td>
<td>-0.009297</td>
<td>...</td>
</tr>
<tr>
<td>sample 2</td>
<td>-0.007699</td>
<td>...</td>
</tr>
<tr>
<td>sample 3</td>
<td>0.032624</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>But when the input order changed:</p>
<pre><code>def f(x):
    return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[1,0,2,3], :])
</code></pre>
<p>the results were changed for sample 0 and sample 1:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Feature 1</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>sample 1</td>
<td>-0.010012</td>
<td>...</td>
</tr>
<tr>
<td>sample 0</td>
<td>-0.008277</td>
<td>...</td>
</tr>
<tr>
<td>sample 2</td>
<td>-0.007699</td>
<td>...</td>
</tr>
<tr>
<td>sample 3</td>
<td>0.032624</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>I have no idea about the difference.</p>
","1","Question"
"79153512","","<p>I keep getting an error ValueError: perm should have the same length as rank(x): 3 != 2 when trying to convert my model using coremltools.</p>
<p>From my understanding the most common case for this is when your input shape that you pass into coremltools doesn't match your model input shape. However, as far as I can tell in my code it does match. I also added an input layer, and that didn't help either.</p>
<p>I have put a lot of effort into reducing my code as much as possible while still giving a minimal complete verifiable example. However, I'm aware that the code is still a lot. Starting at line 60 of my code is where I create my model, and train it.</p>
<p>I'm running this on Ubuntu, with NVIDIA setup with Docker.</p>
<p>Any ideas what I'm doing wrong?</p>
<p>PS. I'm really new to Python, TensorFlow, and machine learning as a whole. So while I put a lot of effort into resolving this myself and asking this question in an easy to understand &amp; reproduce way, I might have missed something. So I apologize in advance for that.</p>
<hr />
<pre class=""lang-py prettyprint-override""><code>from typing import TypedDict, Optional, List
import tensorflow as tf
import json
from tensorflow.keras.optimizers import Adam
import numpy as np
from sklearn.utils import resample
import keras
import coremltools as ct

# Simple tokenizer function
word_index = {}
index = 1
def tokenize(text: str) -&gt; list:
    global word_index
    global index
    words = text.lower().split()
    sequences = []
    for word in words:
        if word not in word_index:
            word_index[word] = index
            index += 1
        sequences.append(word_index[word])
    return sequences

def detokenize(sequence: list) -&gt; str:
    global word_index
    # Filter sequence to remove all 0s
    sequence = [int(index) for index in sequence if index != 0.0]
    words = [word for word, index in word_index.items() if index in sequence]
    return ' '.join(words)

# Pad sequences to the same length
def pad_sequences(sequences: list, max_len: int) -&gt; list:
    padded_sequences = []
    for seq in sequences:
        if len(seq) &gt; max_len:
            padded_sequences.append(seq[:max_len])
        else:
            padded_sequences.append(seq + [0] * (max_len - len(seq)))
    return padded_sequences

class PreprocessDataResult(TypedDict):
    inputs: tf.Tensor
    labels: tf.Tensor
    max_len: int

def preprocess_data(texts: List[str], labels: List[int], max_len: Optional[int] = None) -&gt; PreprocessDataResult:
    tokenized_texts = [tokenize(text) for text in texts]
    if max_len is None:
        max_len = max(len(seq) for seq in tokenized_texts)
    padded_texts = pad_sequences(tokenized_texts, max_len)

    return PreprocessDataResult({
        'inputs': tf.convert_to_tensor(np.array(padded_texts, dtype=np.float32)),
        'labels': tf.convert_to_tensor(np.array(labels, dtype=np.int32)),
        'max_len': max_len
    })

# Define your model architecture
def create_model(input_shape: int) -&gt; keras.models.Sequential:
    model = keras.models.Sequential()

    model.add(keras.layers.Input(shape=(input_shape,), dtype='int32', name='embedding_input'))
    model.add(keras.layers.Embedding(input_dim=10000, output_dim=128)) # `input_dim` represents the size of the vocabulary (i.e. the number of unique words in the dataset).
    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=True)))
    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=32)))
    model.add(keras.layers.Dense(units=64, activation='relu'))
    model.add(keras.layers.Dropout(rate=0.5))
    model.add(keras.layers.Dense(units=1, activation='sigmoid')) # Output layer, binary classification (meaning it outputs a 0 or 1, false or true). The sigmoid function outputs a value between 0 and 1, which can be interpreted as a probability.

    model.compile(
        optimizer=Adam(),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

# Train the model
def train_model(
    model: tf.keras.models.Sequential,
    train_data: tf.Tensor,
    train_labels: tf.Tensor,
    epochs: int,
    batch_size: int
) -&gt; tf.keras.callbacks.History:
    return model.fit(
        train_data,
        train_labels,
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[
            keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5),
            keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1),
            # When downgrading from TensorFlow 2.18.0 to 2.12.0 I had to change this from `./best_model.keras` to `./best_model.tf`
            keras.callbacks.ModelCheckpoint(filepath='./best_model.tf', monitor='val_accuracy', save_best_only=True)
        ]
    )

# Example usage
if __name__ == &quot;__main__&quot;:
    # Check available devices
    print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))

    with tf.device('/GPU:0'):
        print(&quot;Loading data...&quot;)
        data = ([&quot;I love this!&quot;, &quot;I hate this!&quot;], [0, 1])
        rawTexts = data[0]
        rawLabels = data[1]

        # Preprocess data
        processedData = preprocess_data(rawTexts, rawLabels)
        inputs = processedData['inputs']
        labels = processedData['labels']
        max_len = processedData['max_len']

        print(&quot;Data loaded. Max length: &quot;, max_len)

        # Save word_index to a file
        with open('./word_index.json', 'w') as file:
            json.dump(word_index, file)

        model = create_model(max_len)

        print('Training model...')
        train_model(model, inputs, labels, epochs=1, batch_size=32)
        print('Model trained.')

        # When downgrading from TensorFlow 2.18.0 to 2.12.0 I had to change this from `./best_model.keras` to `./best_model.tf`
        model.load_weights('./best_model.tf')
        print('Best model weights loaded.')

        # Save model
        # I think that .h5 extension allows for converting to CoreML, whereas .keras file extension does not
        model.save('./toxic_comment_analysis_model.h5')
        print('Model saved.')

        my_saved_model = tf.keras.models.load_model('./toxic_comment_analysis_model.h5')
        print('Model loaded.')

        print(&quot;Making prediction...&quot;)
        test_string = &quot;Thank you. I really appreciate it.&quot;
        tokenized_string = tokenize(test_string)
        padded_texts = pad_sequences([tokenized_string], max_len)
        tensor = tf.convert_to_tensor(np.array(padded_texts, dtype=np.float32))
        predictions = my_saved_model.predict(tensor)
        print(predictions)
        print(&quot;Prediction made.&quot;)


        # Convert the Keras model to Core ML
        coreml_model = ct.convert(
            my_saved_model,
            inputs=[ct.TensorType(shape=(max_len,), name=&quot;embedding_input&quot;, dtype=np.int32)],
            source=&quot;tensorflow&quot;
        )

        # Save the Core ML model
        coreml_model.save('toxic_comment_analysis_model.mlmodel')
        print(&quot;Model successfully converted to Core ML format.&quot;)
</code></pre>
<p>Code including Dockerfile &amp; start script as GitHub Gist: <a href=""https://gist.github.com/fishcharlie/af74d767a3ba1ffbf18cbc6d6a131089"" rel=""nofollow noreferrer"">https://gist.github.com/fishcharlie/af74d767a3ba1ffbf18cbc6d6a131089</a></p>
","0","Question"
"79157138","","<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn.functional as F

def layer_norm(x, weight, bias, eps=1e-6):
    # x shape: [bs, h, w, c]
    # Calculate mean and variance across the spatial dimensions (height, width)
    mean = np.mean(x, axis=(1, 2), keepdims=True)  # shape: (batch_size, 1, 1, channels)
    var = np.var(x, axis=(1, 2), keepdims=True, ddof=0)  # Use ddof=0 for biased variance

    # Normalize
    x_normalized = (x - mean) / np.sqrt(var + eps)

    # Applying weight and bias
    out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
    return out

def test1(x):
    x = np.transpose(x, (0, 2, 3, 1))  # Transpose to [bs, h, w, c]
    weight = np.ones(channels)
    bias = np.zeros(channels)

    normalized_output = layer_norm(x, weight, bias)
    return normalized_output

def test2(x):
    global channels
    x = np.transpose(x, (0, 2, 3, 1))  # Transpose to [bs, h, w, c]
    x_tensor = torch.tensor(x, dtype=torch.float32)
    weight = torch.ones(channels)
    bias = torch.zeros(channels)

    # Use PyTorch's layer norm, normalizing over the last dimension (channels)
    normalized_output = F.layer_norm(x_tensor, normalized_shape=(channels,), weight=weight, bias=bias)
    return normalized_output.detach().numpy()

# Testing
batch, channels, height, width = 4, 3, 8, 8
# Generate random input
x = np.random.randint(-10, 10, (batch, channels, height, width))

# Calculate outputs from both implementations
layernorm1 = test1(x)
layernorm2 = test2(x)

# Check if outputs are close
are_close = np.allclose(layernorm1, layernorm2, atol=1e-4)
print(&quot;Outputs are close:&quot;, are_close)  # Should output True if they are close enough


var = np.var(x, axis=(1, 2), keepdims=True, ddof=0)  # Use ddof=0 for biased variance
var = np.var(x, axis=(1, 2), keepdims=True)
</code></pre>
<p>My expectation is are_close==True,, which means layernorm1 and layernom2 have a very small distance. since the layernorm1 and layernorm2 have a large shape, I would display the partial result of layernorm1 and layernorm2. layernorm1[0,0,0:3,0:4] array([[ 0.35208505, 1.06448374, -0.52827179], [-1.6216472 , -1.7376534 , -1.07653225], [-1.12821414, 0.88935017, 1.84752351]]) layernorm2[0,0,0:3,0:4] array([[ 0.07412489, 1.1859984 , -1.2601235 ], [-1.0690411 , -0.2672601 , 1.336302 ], [-1.3920445 , 0.4800153 , 0.9120291 ]], dtype=float32)</p>
<p>I've tryed the variacne method with or without ddof=0, got all False in the print statement.
I wanna know how to implment a custom layernorm simliar to Pytorch's built-in layernorm function.
what layernorm step from the code perspective ?
what does layernorm do to a feature map abount computer vision ?</p>
","-1","Question"
"79157457","","<p>I am creating a Machine Learning Model that determines whether a user is a bot or not, I used seaborn to plot a pairplot and realised most of the data is overlapping. Below is the code I wrote for Standardising, Splitting and Deploying the model. The image is how the model performs with just over 40000 samples. As you can see the model is playing guess work and I'm trying to find out why that is.</p>
<p><a href=""https://i.sstatic.net/pjM9Iyfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pjM9Iyfg.png"" alt=""5 Samples from the dataset"" /></a></p>
<pre><code>X = new_df[['Retweet Count', 'Mention Count', 'Follower Count', 'Tweet', 'Hashtags', 'Verified', 'Created At']]
y = new_df[['Bot Label']].values

y = y.ravel() # Ensuring that y is 1D array instead of a 2D array

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

Scaler = StandardScaler()
X_train_scaled = Scaler.fit_transform(X_train)
X_test_scaled = Scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix


rfc = RandomForestClassifier(n_estimators = 1000)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
</code></pre>
<p><a href=""https://i.sstatic.net/wiVzZt6Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wiVzZt6Y.png"" alt=""This is how the model is performing with just over 40000 samples."" /></a></p>
","0","Question"
"79159003","","<p>I am working with the hand written digits dataset. The data is loaded as follows:</p>
<pre><code>(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
</code></pre>
<p>This is the code for a neural network created to classify the digits:</p>
<pre><code>model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
])

model.compile(
    optimizer='adam', 
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)
model.fit(X_train_flattened, y_train, epochs=5)
</code></pre>
<p>The question is, <strong>what is the function of y_train in model.fit()</strong>. this appears to be a classification problem and the network just needs the input(x_train_flattened) for it to be trained.</p>
","-5","Question"
"79162036","","<p>I want to recognize boxes in images. I have a database for these boxes, storing their ocr and images. I do search and get a rough transformation of the face using ocr. It works fine most of the time, but sometimes it returns wrong face and wrong transformation. As I have the source images, I want to take advantage of them to evaluate the search recognition result. I transformed the detected box area to the source image and resized them to same size (so they are from similar perspective, similar size).  I used hog, the second last layer of alexnet, vitmae, and my self-trained conv network as the embedding feature. But all of them does not work well. I also tried keypoint features. But it takes much longer than the requirement. Also it fails when distinguish faces with same print but different size.</p>
<p>Is there any other effective way to compare two similar images?</p>
","-3","Question"
"79164281","","<p>I am using <code>tidymodels</code> to train various classification models and to tune hyperparameters within these models using <code>parsnip</code>. As a newcomer to <code>tidymodels</code>, I have completed some tasks by following tutorials. However, I am unsure how to fit and predict each model using the tuned hyperparameters from <code>workflow_map</code>, rather than just extracting the best workflow and fitting that one. I am interested in understanding the fitted and prediction results for each model and identifying any unpredicted cases. I am currently unable to figure out how to fit and predict a series of workflows with a set of hyperparameters using a composite function like <code>workflow_map</code>. My current approach is to manually substitute each one, which might be somewhat inefficient. Here is some of my code(actually, I used 16 models, here I pasted 2 of them):</p>
<pre class=""lang-r prettyprint-override""><code>set.seed(123)
splits &lt;- initial_split(Mydata, strata = Y,prop = 0.70)
TrainData &lt;- training(splits)
TestData &lt;- testing(splits)


base_rec &lt;- recipe( Y ~ ., data = TrainData)
dummy_rec&lt;-recipe( Y ~ ., data = TrainData)%&gt;%
  step_dummy(all_nominal_predictors())

library(ranger)
rf_spec &lt;- 
  rand_forest(
    mtry = tune(),
    min_n = tune(),
    trees = tune()
    ) %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)%&gt;% 
  translate()

bt_spec &lt;- 
  bart(
    trees = tune(),
    prior_terminal_node_coef = tune(),
    prior_terminal_node_expo = tune(),
    prior_outcome_range = tune()
    ) %&gt;% 
  set_engine(&quot;dbarts&quot;)%&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  translate()

all_workflows &lt;- 
  workflow_set(
    preproc = list(base_recipe = base_rec),
    models = list(
      rf=rf_spec, bart=bt_spec)
  )%&gt;%
  option_add(control = control_grid(
               extract = function(x) x,
               parallel_over = &quot;resamples&quot;,
               save_pred = TRUE,
               save_workflow = TRUE))

#create resampling objects for later tuning
set.seed(123)
folds &lt;- vfold_cv(TrainData, v = 5)

wf_tuning &lt;- 
  all_workflows %&gt;% 
  workflow_map(&quot;tune_grid&quot;, 
               seed=12345, 
               resamples = folds, 
               grid = 25, 
               metrics = metric_set(accuracy), 
               verbose = TRUE)
autoplot(wf_tuning)
</code></pre>
<p>If I manually extract the tuned hyperparameters and then fit and predict the model, I should do the same procedure for 25*16 times. Could you please provide some suggestions for a more efficient approach?</p>
","0","Question"
"79165030","","<p>I want to run the PyTorch library, which I am running in a virtual environment in PyCharm, on my graphics card, which is an Nvidia GeForce RTX 3050 Ti. However, it's running on the CPU, and whenever I use the command <code>import torch</code> and <code>print(&quot;cuda is available:&quot;, torch.cuda.is_available())</code>, it always returns False.</p>
<p>I have CUDA version 12.6 installed. I also installed PyTorch for CUDA version 12.4 because it was the latest version available on the PyTorch website. What should I install considering my graphics card type?</p>
","0","Question"
"79165072","","<p>I want to be able to get the similarity scores of the retrieved documents when using the SelfQueryRetriever as seen below.</p>
<p>I have made a selfquery retriever as follows:</p>
<pre><code>retriever = SelfQueryRetriever.from_llm(
    llm = llm,
    vectorstore = vectorstore,
    document_contents = document_content_description,
    metadata_field_info = metadata_field_info,
    enable_limit=True, 
    search_type = &quot;similarity_score_threshold&quot;,
    search_kwargs={&quot;score_threshold&quot;: 0.80, &quot;k&quot;: 5},
    verbose=True
)
</code></pre>
<p>The retriever is able to retrieve documents as expected, but I would like to somehow display the similarity score of the retrieved documents. Can this be done and how? As is the retriever returns Documents with page_content and metadata keys.</p>
","0","Question"
"79165974","","<p>If I have for example this snippet of code:</p>
<pre><code>knn = KNeighborsClassifier()
grid_search_knn = GridSearchCV(
    estimator=knn,
    n_jobs=-1)
</code></pre>
<p>Do I have to set it like this:</p>
<pre><code>knn = KNeighborsClassifier(random_state=42)

grid_search_knn = GridSearchCV(
    estimator=knn,
    n_jobs=-1
)
</code></pre>
<p>Or do I have to set it like this?</p>
<pre><code>knn = KNeighborsClassifier(random_state=42)

grid_search_knn = GridSearchCV(
    estimator=knn,
    random_state=42,
    n_jobs=-1
)
</code></pre>
<p>what is the correct why? And what if I use randomisedsearch instead of gridsearch?</p>
","0","Question"
"79168636","","<p>I'm trying to build a single label image classification for detecting damaged/Normal Laptops. I have data set of around 500 images. I have tried to use a ResNet50 CNN and it doesn't perform well.</p>
<p>I would like to understand</p>
<ol>
<li>Do I need more data, if yes how much more?</li>
<li>If i have to use existing dataset of 500 images, what is the best way to go about it? What should i use?</li>
</ol>
","-1","Question"
"79170289","","<p>In short: My columns are different between train set and test set after imputing.</p>
<p><a href=""https://i.sstatic.net/cwisvK8g.png"" rel=""nofollow noreferrer"">Code of making train, test dataset</a></p>
<pre><code>random_state_value = 0

#Define target
X = data.drop(columns = 'income', axis=1)
y = data['income']

#Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state_value)
</code></pre>
<pre><code>#Impute missing data
imputer_cat = SimpleImputer(strategy = 'most_frequent')
imputer_num = SimpleImputer(strategy = 'median')

X_train[['workclass', 'occupation', 'native-country']] = imputer_cat.fit_transform(X_train[['workclass', 'occupation', 'native-country']])
X_train[['age']] = imputer_num.fit_transform(X_train[['age']])

X_test[['workclass', 'occupation', 'native-country']] = imputer_cat.fit_transform(X_test[['workclass', 'occupation', 'native-country']])
X_test[['age']] = imputer_num.fit_transform(X_test[['age']])
</code></pre>
<pre><code>#Create dummy vars
X_train = pd.get_dummies(X_train, columns=['workclass', 'education', 'marital-status', 
                                     'occupation', 'relationship', 'race', 'gender', 'native-country'], drop_first = True)
X_test = pd.get_dummies(X_test, columns=['workclass', 'education', 'marital-status', 
                                     'occupation', 'relationship', 'race', 'gender', 'native-country'], drop_first = True)

y_train = pd.get_dummies(y_train, columns='income', drop_first = True)
y_test = pd.get_dummies(y_test, columns='income', drop_first = True)
</code></pre>
<pre><code>y_test = y_test.values.ravel()
y_train = y_train.values.ravel()
</code></pre>
<p>I had categorical variables which had missing values. This is what I have done.</p>
<p>1.
split the data into train, test set</p>
<p>2.
impute each value in train and test set</p>
<p>3.
make dummy variables for categorical variables</p>
<p><strong>But then some columns have disappeared and the length of X_test and X_train is different.</strong></p>
<p><a href=""https://i.sstatic.net/nuE1y9IP.png"" rel=""nofollow noreferrer"">length not matching</a></p>
<p><a href=""https://i.sstatic.net/OXiKEH18.png"" rel=""nofollow noreferrer"">lost columns</a></p>
<pre><code>temp_test = X_test.columns.sort_values()
temp_train = X_train.columns.sort_values()

[col for col in temp_train if col not in temp_test]
</code></pre>
<p>These are the columns.</p>
<p>Why does this happen? And how can I fix this problem?</p>
","0","Question"
"79172053","","<p>I keep modifying the dataloader side, but it still shows that error.</p>
<p>This is the preprocessing code:</p>
<pre><code>import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms


class ImageDataset(Dataset):
    def __init__(self, images, labels, transform=None, target_transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        if self.target_transform:
            label = self.target_transform(label)

        return image, label

# resize the data
resize_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# using Data Loader, is a function that wraps the Dataset and outputs each data using a mini-batch.
train_dataset = ImageDataset(images=x, labels=y, transform=resize_transform)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# print each data (x test and y test in dataloader)
for images, labels in train_dataloader:
    print(images.shape)
    print(labels.shape)
    break
</code></pre>
<p>and this one is the model:</p>
<pre><code>val_dataset = ImageDataset(x_val, y_val, transform=resize_transform)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.conv2 = nn.Conv2d(32, 64, 3)
        self.fc1 = nn.Linear(64 * 16 * 16, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 16)  # Adjusted for 16 classes

        self.pool = nn.MaxPool2d(2, 2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 16 * 16)  # Flatten layer
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Define the loss function and optimizer
net = Network()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

# Training the model
num_epochs = 10
for epoch in range(num_epochs):
    net.train()
    running_loss = 0.0
    for i, data in enumerate(train_dataloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    print(f&quot;Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader):.4f}&quot;)

    # Validation accuracy
    net.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = net(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f&quot;Validation Accuracy: {accuracy:.2f}%&quot;)

print(&quot;Finished Training&quot;)

</code></pre>
<p>And this mentioned error above keeps appearing:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[11], line 42
     40 optimizer.zero_grad()
     41 outputs = net(inputs)
---&gt; 42 loss = criterion(outputs, labels)
     43 loss.backward()
     44 optimizer.step()

File ~\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-&gt; 1736     return self._call_impl(*args, **kwargs)

File ~\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~\anaconda3\Lib\site-packages\torch\nn\modules\loss.py:1293, in CrossEntropyLoss.forward(self, input, target)
   1292 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 1293     return F.cross_entropy(
   1294         input,
   1295         target,
   1296         weight=self.weight,
   1297         ignore_index=self.ignore_index,
   1298         reduction=self.reduction,
   1299         label_smoothing=self.label_smoothing,
   1300     )

File ~\anaconda3\Lib\site-packages\torch\nn\functional.py:3479, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3477 if size_average is not None or reduce is not None:
   3478     reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3479 return torch._C._nn.cross_entropy_loss(
   3480     input,
   3481     target,
   3482     weight,
   3483     _Reduction.get_enum(reduction),
   3484     ignore_index,
   3485     label_smoothing,
   3486 )

ValueError: Expected input batch_size (49) to match target batch_size (64).
</code></pre>
<p>What is the problem here and how to resolve it?</p>
<p>I tried modifying dataloader.</p>
","1","Question"
"79173890","","<p>I am trying to train a model on images using the code below. My directory structure is as follows:</p>
<ul>
<li>The <code>PetImages</code> folder, which is approximately 1GB in size, is located next to my <code>main.py</code> script.</li>
<li>Inside the <code>PetImages</code> folder, there are two subfolders: <code>Cats</code> and <code>Dogs</code>.</li>
<li>Each of these subfolders contains 10,000 images of cats and dogs, respectively.</li>
</ul>
<pre><code>import os
import tensorflow as tf
from keras import layers
from tensorflow import keras

# === Constants for easy hyperparameter tuning ===

BATCH_SIZE = 256  # Batch size for training.
EPOCHS = 80  # Number of training epochs.
LEARNING_RATE = 5e-4  # Learning rate for Adam optimizer.

# === End of constants ===

# Filter out corrupted images
def filter_corrupted_images():
    num_skipped = 0
    for folder_name in (&quot;Cat&quot;, &quot;Dog&quot;):
        folder_path = os.path.join(&quot;PetImages&quot;, folder_name)
        for fname in os.listdir(folder_path):
            fpath = os.path.join(folder_path, fname)
            try:
                with open(fpath, &quot;rb&quot;) as fobj:
                    is_jfif = b&quot;JFIF&quot; in fobj.read(10)
            except Exception:
                is_jfif = False
            if not is_jfif:
                num_skipped += 1
                os.remove(fpath)
    print(f&quot;Deleted {num_skipped} corrupted images.&quot;)

# Generate Dataset
def generate_datasets(image_size=(180, 180), batch_size=BATCH_SIZE):
    train_ds, val_ds = keras.utils.image_dataset_from_directory(
        &quot;PetImages&quot;,
        validation_split=0.2,
        subset=&quot;both&quot;,
        seed=1337,
        image_size=image_size,
        batch_size=batch_size,
    )
    return train_ds, val_ds

# Configure the Dataset for Performance
def configure_for_performance(ds):
    AUGMENTATION = keras.Sequential([
        layers.RandomFlip(&quot;horizontal&quot;),
        layers.RandomRotation(0.3),
        layers.RandomZoom(0.2),
        layers.RandomBrightness(0.2)
    ])
    ds = ds.map(lambda x, y: (AUGMENTATION(x, training=True), y),
                num_parallel_calls=tf.data.AUTOTUNE)
    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)

# Define Model Architecture with adjusted strides and fewer pooling layers
def make_model(input_shape, num_classes=2):
    inputs = keras.Input(shape=input_shape)
    x = layers.Rescaling(1.0 / 255)(inputs)

    # Convolutional Layers with reduced stride for some layers
    FILTER_SIZES = [32, 64, 128, 256, 512]
    KERNEL_SIZE = (3, 3)
    DROPOUT_RATE = 0.5
    ACTIVATION_FUNCTION = &quot;swish&quot;

    for i, size in enumerate(FILTER_SIZES):
        x = layers.Conv2D(size, KERNEL_SIZE, strides=1 if i &lt; 2 else 2, padding=&quot;same&quot;)(x)  # Use stride 1 for first two layers
        x = layers.BatchNormalization()(x)
        x = layers.Activation(ACTIVATION_FUNCTION)(x)
        if i &lt; 3:  # Apply MaxPooling only in the first three layers
            x = layers.MaxPooling2D(pool_size=(2, 2))(x)

    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(DROPOUT_RATE)(x)
    outputs = layers.Dense(1 if num_classes == 2 else num_classes,
                           activation=&quot;sigmoid&quot; if num_classes == 2 else &quot;softmax&quot;)(x)

    model = keras.Model(inputs, outputs)
    return model

# Train the Model
def train_model(model, train_ds, val_ds, epochs=EPOCHS):
    model.compile(
        optimizer=keras.optimizers.Adam(LEARNING_RATE),
        loss=&quot;binary_crossentropy&quot;,
        metrics=[&quot;accuracy&quot;],
    )
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        &quot;model_best.keras&quot;, monitor=&quot;val_accuracy&quot;, save_best_only=True
    )
    early_stopping_callback = keras.callbacks.EarlyStopping(
        monitor=&quot;val_loss&quot;, patience=5, restore_best_weights=True
    )

    model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        callbacks=[checkpoint_callback, early_stopping_callback],
    )

# Check if model exists and load it for continuing training
def load_model_if_exists(model_filepath=&quot;model_best.keras&quot;):
    if os.path.exists(model_filepath):
        print(f&quot;Loading existing model from {model_filepath}...&quot;)
        model = keras.models.load_model(model_filepath)
    else:
        print(&quot;No existing model found, starting a new model...&quot;)
        model = make_model(input_shape=(180, 180, 3))
    return model

if __name__ == &quot;__main__&quot;:
    filter_corrupted_images()

    # Check if a saved model exists, if yes, load it, if not, create a new one
    model = load_model_if_exists(&quot;model_best.keras&quot;)

    # Prepare the datasets for training
    train_ds, val_ds = generate_datasets(image_size=(180, 180), batch_size=BATCH_SIZE)
    train_ds = configure_for_performance(train_ds)
    val_ds = configure_for_performance(val_ds)

    # Continue training or start fresh
    train_model(model, train_ds, val_ds, epochs=EPOCHS)

    # Save the trained model in .keras format
    model.save(&quot;model_best.keras&quot;)

</code></pre>
<p>After training a bit I am using the code below to test my model.</p>
<pre><code>import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing import image

# Load the trained model
model = keras.models.load_model(&quot;model_best.keras&quot;)

# Preprocess image for prediction
def preprocess_image(img_path, image_size=(180, 180)):
    img = image.load_img(img_path, target_size=image_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array / 255.0  # Normalize to match training
    return img_array

# Predict single image
def predict_image(img_path):
    img_array = preprocess_image(img_path)
    predictions = model.predict(img_array)
    print(f&quot;The image at {img_path} is likely a Cat with confidence {1 - predictions[0][0]:.2f}&quot;)
    print(f&quot;The image at {img_path} is likely a Dog with confidence {predictions[0][0]:.2f}&quot;)

if __name__ == &quot;__main__&quot;:
    predict_image(&quot;Cat01.jpeg&quot;)
    predict_image(&quot;Cat02.jpg&quot;)
    predict_image(&quot;Cat03.jpg&quot;)
    predict_image(&quot;Dog01.jpeg&quot;)
    predict_image(&quot;Dog02.jpg&quot;)
    predict_image(&quot;Dog03.jpg&quot;)

</code></pre>
<p>This is the output:</p>
<pre><code>1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 642ms/step
The image at Cat01.jpeg is likely a Cat with confidence 0.91
The image at Cat01.jpeg is likely a Dog with confidence 0.09
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
The image at Cat02.jpg is likely a Cat with confidence 0.92
The image at Cat02.jpg is likely a Dog with confidence 0.08
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step
The image at Cat03.jpg is likely a Cat with confidence 0.92
The image at Cat03.jpg is likely a Dog with confidence 0.08
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step
The image at Dog01.jpeg is likely a Cat with confidence 0.92
The image at Dog01.jpeg is likely a Dog with confidence 0.08
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
The image at Dog02.jpg is likely a Cat with confidence 0.92
The image at Dog02.jpg is likely a Dog with confidence 0.08
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
The image at Dog03.jpg is likely a Cat with confidence 0.92
The image at Dog03.jpg is likely a Dog with confidence 0.08
</code></pre>
<p>It consistently classifies the images as cats with nearly the same level of confidence each time.
I've tried changing the batch size, layers, and learning rate, but nothing has worked so far. What could be the problem?</p>
","-2","Question"
"79175150","","<p>I'm implementing a Gaussian Process Regression (GPR) model in Python using a Squared Exponential Kernel. However, I'm encountering a <code>ValueError</code> during the matrix multiplication step of the <code>predict</code> method, specifically when trying to compute the mean prediction.</p>
<p>The error I'm seeing is:</p>
<pre><code>ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature 
(n?,k),(k,m?)-&gt;(n?,m?) (size 10 is different from 100)
</code></pre>
<h3>Code Details</h3>
<p>Here’s a breakdown of the code that comes into play in this error:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

class SquaredExponentialKernel:
    def __init__(self, length_scale=1.0, variance=1.0):
        self.length_scale = length_scale
        self.variance = variance

    def __call__(self, x1, x2):
        dist_sq = np.sum((x1 - x2)**2)
        return self.variance * np.exp(-0.5 * dist_sq / self.length_scale**2)

def cov_matrix(x1, x2, cov_function) -&gt; np.array:
    return np.array([[cov_function(a, b) for a in x1] for b in x2])

class GPR:
    def __init__(self, data_x, data_y, covariance_function=SquaredExponentialKernel(), white_noise_sigma: float = 0):
        self.noise = white_noise_sigma
        self.data_x = data_x
        self.data_y = data_y
        self.covariance_function = covariance_function
        self._inverse_of_covariance_matrix_of_input_noise_adj = np.linalg.inv(
            cov_matrix(data_x, data_x, covariance_function) + self.noise * np.identity(len(self.data_x))
        )
        self._memory = None

    def predict(self, test_data: np.ndarray) -&gt; np.ndarray:
        KXX_star = cov_matrix(test_data, self.data_x, self.covariance_function)
        KX_starX_star = cov_matrix(test_data, test_data, self.covariance_function)
        mean_test_data = KXX_star @ (self._inverse_of_covariance_matrix_of_input_noise_adj @ self.data_y)
        cov_test_data = KX_starX_star - KXX_star @ (self._inverse_of_covariance_matrix_of_input_noise_adj @ KXX_star.T)
        var_test_data = np.diag(cov_test_data)
        self._memory = {'mean': mean_test_data, 'covariance_matrix': cov_test_data, 'variance': var_test_data}
        return mean_test_data

# Test data
np.random.seed(69)
data_x = np.linspace(-5, 5, 10).reshape(-1, 1)
data_y = np.sin(data_x) + 0.1 * np.random.randn(10, 1)

# Instantiate and predict
gpr_se = GPR(data_x, data_y, covariance_function=SquaredExponentialKernel(), white_noise_sigma=0.1)
test_data = np.linspace(-6, 6, 100).reshape(-1, 1)
mean_predictions = gpr_se.predict(test_data)
</code></pre>
<h3>Dimension Breakdown</h3>
<p>Here's the dimensional analysis for the matrix multiplication where the error occurs:</p>
<ol>
<li><code>KXX_star</code> is computed as <code>cov_matrix(test_data, self.data_x, self.covariance_function)</code>, resulting in a shape of <code>(100, 10)</code>.</li>
<li><code>self._inverse_of_covariance_matrix_of_input_noise_adj</code> is computed in the <code>__init__</code> method and has a shape of <code>(10, 10)</code>.</li>
<li><code>self.data_y</code> has a shape of <code>(10, 1)</code>.</li>
</ol>
<p>The line in question is:</p>
<pre class=""lang-py prettyprint-override""><code>mean_test_data = KXX_star @ (self._inverse_of_covariance_matrix_of_input_noise_adj @ self.data_y)
</code></pre>
<p>This should yield a result with shape <code>(100, 1)</code> because:</p>
<ul>
<li><code>KXX_star</code> has shape <code>(100, 10)</code>,</li>
<li><code>(self._inverse_of_covariance_matrix_of_input_noise_adj @ self.data_y)</code> results in shape <code>(10, 1)</code>.</li>
</ul>
<p>Why am I getting a dimension mismatch error here when the dimensions seem to align for the matrix multiplication? And how can I fix it?</p>
<p>I expected this matrix multiplication to work as the dimensions appear compatible on paper: <code>KXX_star</code> (100, 10) multiplied by <code>(10, 1)</code> should yield <code>(100, 1)</code>. The error, however, states a dimension mismatch, implying something isn’t aligning as expected. I checked shapes for <code>self.data_y</code>, <code>self._inverse_of_covariance_matrix_of_input_noise_adj</code>, and <code>KXX_star</code>. Also tried reshaping <code>data_y</code> to ensure it’s consistently (10, 1), but the error persists. I was expecting to get mean predictions as a vector of shape <code>(100, 1)</code> for <code>test_data</code> without any dimension issues.</p>
","0","Question"
"79179803","","<p>I am trying to figure out how the <a href=""https://www.rdocumentation.org/packages/MLmetrics/versions/1.1.3/topics/F1_Score"" rel=""nofollow noreferrer"">F1_Score function</a> in the MLmetrics library works when the y_pred values are non-binary.</p>
<p>For example:</p>
<pre><code>library(MLmetrics)
y &lt;- c(1,1,1,1,1,0,0,0,0,0)
x &lt;- c(1, 0.8, 0.654, 0.99, 0.75, 0.1, 0.3, 0.6, 0.05, 0.2)
x_preds &lt;- ifelse(x &lt; 0.5, 0, 1)
getF1 &lt;- F1_Score(y_true=y, y_pred=x, positive=&quot;1&quot;)
getF2 &lt;- F1_Score(y_true=y, y_pred=x_preds, positive=&quot;1&quot;)

print(getF1)
print(getF2)
</code></pre>
<p>Gives <code>getF1=0.3333333</code> and <code>getF2 = 0.9090909</code></p>
<p>The example provided in the R documentation for the function is designed to calculate what I have called getF2, where I have specified exactly how to assign the probability scores to either class label based on a 0.5 threshold. What I am unclear on is how it calculates the F1 score if this threshold is not specified (getF1). Can anyone explain what the function does by default if you leave the probability scores as is and don't cast them to binary before calling the F1_Score function? I can't for the life of me figure out how it got 0.3333333.</p>
","0","Question"
"79181943","","<p>Similar to <a href=""https://stackoverflow.com/questions/69777578/the-actor-implicitfunc-is-too-large-error"">this question</a>, Ray Tune is reporting to me:</p>
<blockquote>
<p>ValueError: The actor ImplicitFunc is too large (421 MiB &gt; FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.</p>
</blockquote>
<p>I have no idea what is being captured in my scope. It will report this seemingly no matter what changes I make. I have tried taking a dozen different references out of the function and putting them into Ray's internal storage (ray.get() &amp; ray.put()), and it barely moves the needle. Taking out the model definition, the train/test data, and the folding function still resulted in 421 MiB. Which reference is &gt;400 MiB?</p>
<p>Model Definition:</p>
<pre><code>INPUT_DIM = tch_train.features.shape[1] - 1 #Removing an input feature because the sample weight is included with the input data
OUTPUT_DIM = tch_train.labels.shape[1]

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(INPUT_DIM, OUTPUT_DIM)

    def forward(self, input):
        
        output = F.softmax(F.relu(self.fc1(input)), dim=1)
        
        return output
</code></pre>
<p>Main function:</p>
<pre><code>K_FOLDS = 5
loss_function = nn.CrossEntropyLoss(reduction='none')
kfold = KFold(n_splits=K_FOLDS, shuffle=True)

fold_indices = [(train_ids, test_ids) for train_ids, test_ids in kfold.split(tch_train)]
fold_indices_ref = ray.put(fold_indices)

tch_train_ref = ray.put(tch_train)

# This function is the &quot;Main stuff&quot; of the machine learning.
# This will be called by RayTune and will be expected to train a machine learning model and report the results.
def objective(config):
    optimizer = torch.optim.SGD(  # Tune the optimizer
        model.parameters(), lr=config[&quot;lr&quot;], momentum=config[&quot;momentum&quot;]
    )
    
    # Make a model for each fold.
    fold_models = []
    for fold in range(K_FOLDS):
        fold_models.append(Net().to(&quot;cuda&quot;))

    # Epoch loop
    while True:
        fold_losses = []

        for fold in range(K_FOLDS):
            train_ids, test_ids = ray.get(fold_indices_ref)[fold]
            
            # Take Epoch sample from the 4/1 train/test fold chunks.
            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)
            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)
            trainloader = torch.utils.data.DataLoader(ray.get(tch_train_ref), batch_size=config[&quot;batch_size&quot;], sampler=train_subsampler)
            testloader = torch.utils.data.DataLoader(ray.get(tch_train_ref), batch_size=config[&quot;batch_size&quot;], sampler=test_subsampler)
            
            # Iterate over the DataLoader for training data
            for i, data in enumerate(trainloader, 0):
                # Get inputs
                features, targets = data
                inputs = features[:,1:]
                sample_weights = features[:,0]
                
                # Zero the gradients
                optimizer.zero_grad()
                
                # Perform forward pass
                outputs = fold_models[fold](inputs)
                
                # Compute loss
                loss = loss_function(outputs, targets) * sample_weights
                
                # Perform backward pass
                loss.mean().backward()
                
                # Perform optimization
                optimizer.step()

            # Test on test fold
            fold_losses[fold] = 0.0
            with torch.no_grad():
                # Iterate over the test data and generate predictions
                for i, data in enumerate(testloader, 0):
                    # Get inputs
                    features, targets = data
                    inputs = features[:,1:]
                    sample_weights = features[:,0]
                    
                    # Generate outputs
                    outputs = net(inputs)

                    #Add test loss
                    fold_losses[fold] += (loss_function(outputs, targets) * sample_weights).sum() 
                
        # Report average fold losses
        train.report({&quot;averaged_CEL&quot;: sum(fold_losses) / float(K_FOLDS)})  # Report to Tune
</code></pre>
<p>Tune Config:</p>
<pre><code>search_space = {&quot;lr&quot;: ray.tune.loguniform(1e-4, 1e-2), &quot;momentum&quot;: ray.tune.uniform(0.1, 0.9)}
algo = OptunaSearch() 

tuner = ray.tune.Tuner(
    objective,
    tune_config=ray.tune.TuneConfig(
        metric=&quot;averaged_CEL&quot;,
        mode=&quot;min&quot;,
        search_alg=algo,
    ),
    run_config=ray.train.RunConfig(
        stop={&quot;training_iteration&quot;: 5},
    ),
    param_space=search_space,
)
results = tuner.fit()
print(&quot;Best config is:&quot;, results.get_best_result().config)
</code></pre>
","0","Question"
"79184686","","<p>I trained an XGBRegressor model with early stopping. As far as I understood, model.feature_importances_ computes feature importances looking at ALL history (i.e. considering also the &quot;patience&quot; iterations quantified by early_stopping_rounds). Nonetheless, I need the feature importances computed on the model only up to the best_iteration.</p>
<p>Here is an exemplificative code:</p>
<pre><code>from xgboost import XGBRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Prepare data
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model with early stopping
xgb_model = XGBRegressor(n_estimators=1000, early_stopping_rounds=100, eval_metric=&quot;rmse&quot;)
xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

xgb_model.feature_importances_
</code></pre>
<p>But this gives me feature importances considering the full model, not just up to the best_iteration:</p>
<pre><code>best_iteration = xgb_model.best_iteration
</code></pre>
<p>I can not re-train a new XGBRegressor with n_estimators=best_iteration because it would almost double running time (this snippet is part of a much larger code).
Is there a way to achieve this without re-training?
Note that, unfortunately, .feature_importances_ does not have an iteration_range option.</p>
","1","Question"
"79185545","","<p>Below is the code for multi class classifier from Chapter 4 in 'Deep Learning with Python' by François Chollet. The textbook mentions this code will yield &gt;95% training accuracy, but my environment seems to yield very low accuracy of &lt;50% compared to the textbook.<br />
Keras version - 3.6
Tensorflow - 2.18
Hardware - Apple M1 Pro</p>
<pre><code>import keras
from tensorflow.keras.datasets import reuters
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)


def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
        return results


x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)

model = keras.Sequential([
    layers.Dense(64, activation=&quot;relu&quot;),
    layers.Dense(64, activation=&quot;relu&quot;),
    layers.Dense(46, activation=&quot;softmax&quot;)
])

model.compile(
    optimizer=&quot;rmsprop&quot;,
    loss=&quot;categorical_crossentropy&quot;,
    metrics=[&quot;accuracy&quot;]
)

# setting aside validation set
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]

# training the model

history = model.fit(
    partial_x_train,
    partial_y_train,
    epochs=10,
    batch_size=512,
    validation_data=(x_val, y_val)
)

# plotting training &amp; validation accuracy
history_dict = history.history
loss_values = history_dict[&quot;loss&quot;]
val_loss_values = history_dict[&quot;val_loss&quot;]
epochs = range(1, len(loss_values) + 1)
acc = history_dict[&quot;accuracy&quot;]
val_acc = history_dict[&quot;val_accuracy&quot;]
plt.plot(epochs, acc, &quot;bo&quot;, label=&quot;Training acc&quot;)
plt.plot(epochs, val_acc, &quot;b&quot;, label=&quot;Validation acc&quot;)
plt.xlabel(&quot;Epochs&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.legend()
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/oWvK0IA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oWvK0IA4.png"" alt=""Training &amp; Validation accuracy vs Epochs"" /></a></p>
","0","Question"
"79189607","","<p>my code below keeps blowing up and I can't work out what is going on</p>
<pre class=""lang-py prettyprint-override""><code>import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming `X` and `y` are your feature matrix and target array
X_train, X_valid, y_train, y_valid = train_test_split(df_combined, y, test_size=0.2, random_state=42)

# Define the objective function for Optuna
def objective(trial):
    # Suggest values for hyperparameters
    params = {
        &quot;objective&quot;: &quot;reg:squarederror&quot;,
        &quot;eval_metric&quot;: &quot;rmse&quot;,
        &quot;tree_method&quot;: &quot;hist&quot;,  # Use hist method
        &quot;device&quot;: &quot;cuda&quot;,       # Specify using GPU
        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
        &quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
        &quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
        &quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
        &quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
        &quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
        &quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
        &quot;n_estimators&quot;: 1000  # Define n_estimators in the initialization of the model
    }

    # Initialize the model
    model = xgb.XGBRegressor(**params)

    # Train the model with early stopping callback
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_valid, y_valid)],
        verbose=False,
        early_stopping_rounds=50  # Stops if no improvement after 50 rounds
    )

    # Predict and calculate RMSE for validation set
    preds = model.predict(X_valid)
    rmse = mean_squared_error(y_valid, preds, squared=False)

    return rmse  # Optuna minimizes this

# Set up the Optuna study
study = optuna.create_study(direction=&quot;minimize&quot;)

# Optimize the hyperparameters
study.optimize(objective, n_trials=100, n_jobs=40)  # 100 trials with 40 parallel jobs

# Display the best trial
print(&quot;Best trial:&quot;)
trial = study.best_trial
print(f&quot;  Value (RMSE): {trial.value}&quot;)
print(&quot;  Params: &quot;)
for key, value in trial.params.items():
    print(f&quot;    {key}: {value}&quot;)
</code></pre>
<p>I get</p>
<pre><code>TypeError: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'
</code></pre>
<p>I have updated everything to ensure I have all updated libraries.</p>
<p>Early stopping rounds is correct (I think) but just blows up for some reason.</p>
","0","Question"
"79192127","","<p>I am using the CreateML tool to train a text classifier, when I use the preview feature and put in a sentence it will give me a prediction along with a confidence variable<a href=""https://i.sstatic.net/Um7NZQqE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Um7NZQqE.png"" alt=""enter image description here"" /></a></p>
<p>Here is how I am using the model on the app</p>
<pre><code>import CoreML
...

    func predict(phrase:String) -&gt; String {
        guard let rollModel = try? Roll(configuration: MLModelConfiguration()) else {
            return &quot;Failed to load the Roll Model.&quot;
        }

        let rollModelInput = RollInput(text: phrase)

        guard let prediction = try? rollModel.prediction(input: rollModelInput, options: MLPredictionOptions()) else {
            return &quot;Roll Model Prediction Failed&quot;
        }
        
        return prediction.label
    }
</code></pre>
<p>This is working, it's providing a prediction.</p>
<p>My data is in the standard text/label format</p>
<p>Even when I export the model to xcode and run the preview in xcode the confidence variable is present.</p>
<p>When I run the prediction on the device I want to know what the confidence variable is, how can I get access?</p>
<p><a href=""https://i.sstatic.net/JmWtvy2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JmWtvy2C.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79195592","","<p>So i recently imported dataset from this <a href=""https://www.kaggle.com/datasets/mostafaabla/garbage-classification"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/mostafaabla/garbage-classification</a> website. And even though i have it in my files in google colab(unzipped and all that stuff), i dont know how to implement it in the code itself. Like from tutorial of Fashion mnist by tensorflow <a href=""https://www.tensorflow.org/tutorials/keras/classification?hl"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/classification?hl</a> it loaded as</p>
<p>fashion_mnist = tf.keras.datasets.fashion_mnist</p>
<p>(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</p>
<p>how do i import/load data to code cell and work with it by splitting into classes(because in that tutorial dataset has several classes and in my custom dataset it has 12)
pls how to do it?</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define paths to your training and validation directories
train_dir = 'garbage-classification/train'
val_dir = 'garbage-classification/validation'

# Create an ImageDataGenerator for data augmentation
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# Load images from directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),  # Resize images as needed
    batch_size=32,
    class_mode='categorical'  # Use 'categorical' if you have multiple classes
)

validation_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)
</code></pre>
<p>i used perplexity to try to solve and it gave me this. It obviously didn't work so..</p>
","0","Question"
"79199199","","<p>I need to compute embeddings for a large number of sentences (say 10K) in preprocessing, and at runtime I will have to compute the embedding vector for one sentence at a time (user query), and then find the most resembling sentence based on the embedding vectors (using cosine similarity).</p>
<p>I'm currently using sentence transformers, and their output size is 768, which is too large for my case. So I'd like to experiment with smaller sizes, like 256 or even 128.</p>
<p>I'm familiar with PCA and quantization. However, both ChatGPT and Gemini suggested that I simply add a dense layer after the pooling layer. Example:</p>
<pre><code>dense = models.Dense(in_features=base_model.get_sentence_embedding_dimension(), out_features=256)
model = SentenceTransformer(modules=[base_model, dense])
</code></pre>
<p>My problem with this is that I think that I would have to retrain / finetune my model, which I cannot do since I don't have labeled data. But ChatGPT and Gemini are claiming that I could get away with this implementation without retraining or finetuning, although &quot;it would be better&quot;.</p>
<p>I'm confused how this could possibly work, because without training the initial weights in the dense layer would be random.</p>
<p>Am I missing something or is adding a dense layer without re-training / finetuning could actually work?</p>
","0","Question"
"79201296","","<p>I'm new to this field and have been following a U-Net tutorial using 3-channel RGB images for semantic segmentation <a href=""https://www.youtube.com/watch?v=68HR_eyzk00&amp;list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE&amp;index=2&amp;ab_channel=DigitalSreeni"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=68HR_eyzk00&amp;list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE&amp;index=2&amp;ab_channel=DigitalSreeni</a>, and it worked fine for me. However, I now need to extend the pipeline to support 4-channel RGBX images (i.e., RGB + an other channel), but I’m not sure how to modify the code to accommodate the additional channel, especially for the preprocessing and the ImageDataGenerator parts (I think that the ImageDataGenerator doesn’t support 4-channel images).</p>
<p>This is the code (after patchifying the image to (256 * 256 * 4) and the masks to (256*256)):</p>
<pre class=""lang-py prettyprint-override""><code>import os
import cv2
import numpy as np
import glob
from matplotlib import pyplot as plt
import tensorflow as tf
import splitfolders
import segmentation_models as sm
from tensorflow.keras.metrics import MeanIoU
from sklearn.preprocessing import MinMaxScaler
from keras.utils import to_categorical


input_folder='path folder to my images and masks '
output_folder='path to output folder'
#split with a ratio
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.75,.25),group_prefix=None) 

#Rearange the folder structure for keras augmentation


seed=24
batch_size=16 
n_classes=2 


scaler=MinMaxScaler()


BACKBONE='resnet34'  
preprocess_input=sm.get_preprocessing(BACKBONE)

def preprocess_data(img, mask, num_class):
    #Scale images
    img=scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)
    img=preprocess_input(img)  #Preprocess based on the pretrained backbone
    mask=to_categorical(mask, num_class)
    return (img,mask)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
def trainGenerator(train_img_path, train_mask_path, num_class):
    img_data_gen_args=dict(horizontal_flip=True, vertical_flip=True, fill_mode='reflect') #Data augmentation
    
    image_datagen=ImageDataGenerator(**img_data_gen_args)
    mask_datagen=ImageDataGenerator(**img_data_gen_args)
    
    image_generator=image_datagen.flow_from_directory(train_img_path, class_mode=None, batch_size=batch_size, seed=seed)
    mask_generator=image_datagen.flow_from_directory(train_mask_path, class_mode=None, color_mode='grayscale', batch_size=batch_size, seed=seed)
    
    train_generator=zip(image_generator, mask_generator)
    
    for (img, mask) in train_generator:
        img, mask= preprocess_data(img, mask, num_class)
        yield (img, mask)

train_img_path='path for training images'
train_mask_path='path for training masks'
train_img_gen=trainGenerator(train_img_path, train_mask_path, num_class=2)

val_img_path='path for validation images'
val_mask_path='path for validation masks'
val_img_gen=trainGenerator(val_img_path, val_mask_path, num_class=2)


x, y=train_img_gen.__next__()

for i in range(0,3):
    image=x[i]
    mask=np.argmax(y[i], axis=2)
    plt.subplot(1,2,1)
    plt.imshow(image)
    plt.subplot(1,2,2)
    plt.imshow(mask, cmap='gray')
    plt.show()


num_train_imgs=len(os.listdir('path for training images'))
num_val_images=len(os.listdir('path for validation image'))
steps_per_epochs=num_train_imgs//batch_size
val_steps_per_epoch=num_val_images//batch_size

IMG_HEIGHT=x.shape[1]
IMG_WIDTH=x.shape[2]
IMG_CHANNELS=x.shape[3]

n_classes=2

model=sm.Unet('resnet34', encoder_weights='None', input_shape=(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS), classes=n_classes,activation='softmax')
model.compile('Adam', loss=sm.losses.binary_crossentropy, metrics=[sm.metrics.iou_score, sm.metrics.FScore()])

history=model.fit(train_img_gen, steps_per_epoch=steps_per_epochs, epochs=100, verbose=1, validation_data=val_img_gen, validation_steps=val_steps_per_epoch)


</code></pre>
","0","Question"
"79205991","","<p>I'm trying to train a two-stage model in an end-to-end way. However, I want to update the different stages of models with different losses. For example, suppose the end to end model is composed of two models:model1 and model2. The output is calculated through running</p>
<pre><code>features = model1(inputs)
output = model2(features)
</code></pre>
<p>I want to update the parameters of model1 with loss1, while keeping the parameter of model2 unchanged. Next, I want to update the parameters of model2 with loss2, while keeping the parameter of model1 unchanged. My full implementation is something like:</p>
<pre><code>import torch
import torch.nn as nn

# Define the first model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Linear(20, 10)
        self.conv2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

# Define the second model
class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.conv1 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.conv1(x)
        return x

# Initialize models
model1 = Net()
model2 = Net1()

# Initialize separate optimizers for each model
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# Sample inputs and labels
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs)         
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 
   
loss1.backward(retain_graph=True)  
optimizer.step()  
optimizer.zero_grad()       
optimizer1.zero_grad()  

 
loss2.backward()        
</code></pre>
<p>However, this will return</p>
<pre><code>Traceback (most recent call last):
  File , line 55, in &lt;module&gt;
    loss2.backward()        
    ^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, line 521, in backward
    torch.autograd.backward(
  File &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, line 289, in backward
    _engine_run_backward(
  File &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 5]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
</code></pre>
<p>I kinda understand why this is happening, but is there a way to address this?</p>
","0","Question"
"79212687","","<p>I have a custom dataset (images of pizza,sushi and steak).
I'm using torch DataLoader for it , now when writing the test dataloader custom should we set shuffle=true or it just doesn't matter??</p>
<p>I haven't seen difference yet , but just asking the  general.</p>
","-3","Question"
"79219306","","<p>I used scorecardpy function to get a model:</p>
<pre><code>import scorecardpy as ac
card=sc.scorecard(bins_adj, lr, X_train.columns)
</code></pre>
<p>Then I tried to save this model using following code:</p>
<pre><code>import numpy as np
np.save('card.npy',card)
</code></pre>
<p>After that I tried to reload this model:</p>
<pre><code>card=np.load('card.npy',allow_pickle=True)
</code></pre>
<p>Then I want to use the model to get the scores:</p>
<pre><code>score=sc.scorecard_ply(data_train, card, print_step=0)
</code></pre>
<p>But it gives error:</p>
<pre><code>UnboundLocalError          Traceback (most recent call last)
Cell In [91], line 1
score=sc.scorecard_ply(data_train, card, print_step=0)

File ~/.local/lib/python3.9/site-packages/scorecardpy/scorecard.py:330, in scorecard_ply(dt, card, only_total_score, print_step, replace_blank_na, var_kp)
     card_df=card.copy(deep=True)
  # x variables
  xs=card_df.loc[card_df.variable != 'basepoints', 'variable'].unique()
  # length of x variables
  xs_len=len(xs)
  
UnboundLocalError: local variable 'card_df' referenced before assignment
</code></pre>
<p>How to resolve this problem?</p>
","0","Question"
"79224304","","<p>I am receiving an error message while trying to load a pickled ML model for a data project. How to resolve it?</p>
<p>I have the correct file path and already confirmed the path &quot;/home/jovyan/work&quot;, and loaded the pickle before.</p>
<p>I don't know if it is a cloud issue, but when I comment out the writing of the pickle alone, or if I comment out the writing of the pickle and the fitting of the model also... I cannot simply load the pickle for some reason. Which is the whole reason I am pickling the model to not have to fit the model every time.</p>
<p>These are the functions i am using:</p>
<pre><code>def write_pickle(path, model_object, save_as:str): 

  with open(path + save_as + &quot;.pickle&quot;, &quot;wb&quot;) as to_write: 
    
    pickle.dump(model_object, to_write)

def read_pickle(path, saved_model_name:str):
  
    with open(path + saved_model_name + '.pickle', 'rb') as to_read:
        
      model = pickle.load(to_read)
    
    return model
</code></pre>
<p>These functions have worked with the file path and everything, but when i comment out the writing of the pickle or comment out the fitting of the ML model.... I am getting a FileNotFoundError. I don't know what the problem could be. Without me being able to figure this out, it seems that it is useless for me to even be pickling the model.</p>
","-1","Question"
"79224395","","<p>I am getting an extra variable in my confusion table, not sure where it's coming from.
The Dataset 'Default' has the following columns: default, student, income, balance
The variable 'default' has two values: 'Yes' and 'No'</p>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS,
                         summarize,
                         poly)
from ISLP import confusion_table



Default = load_data('Default')
vars = Default.columns.drop(['default'])
y = Default['default'] == 'Yes'
design = MS(vars)
X = design.fit_transform(Default)
glm = sm.GLM(y,
             X,
             family = sm.families.Binomial())
results = glm.fit()
summarize(results)
probs = results.predict()
labels = np.array(['No']*10000)
labels[probs&gt;0.5] = 'Yes'
confusion_table(labels,Default.default)
</code></pre>
<p>In the output, I get a 3x3 table with the variables 'No', 'Yes' and 'Ye'</p>
<p>I want the confusion table values to be only 'Yes' and 'No'. Somehow, the numpy.array 'labels' is set to 'Ye' instead of 'Yes'.</p>
","0","Question"
"79225295","","<p>I am working on a project where I use OpenCV for image preprocessing. Below is my code for preprocessing the image:</p>
<pre><code>def preprocess_with_opencv(img):
    img = img.numpy()

    # Resize
    img = cv2.resize(img, (img_width, img_height), interpolation=cv2.INTER_AREA)

    # GaussianBlur
    img = cv2.GaussianBlur(img, (3, 3), 0)

    # Adaptive Threshold
    img = cv2.adaptiveThreshold(
        img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
    )

    # Chuẩn hóa
    img = img.astype(&quot;float32&quot;) / 255.0
    img = np.expand_dims(img, axis=-1)

    return img
</code></pre>
<p>After preprocessing, I use the following function to encode the image for further processing:</p>
<pre><code>def encode_single_sample(img_path, label):
    # 1. Đọc ảnh
    img = tf.io.read_file(img_path)

    # 2. Decode ảnh (grayscale)
    img = tf.io.decode_png(img, channels=1)

    # 3. Sử dụng OpenCV để xử lý trước
    img = tf.py_function(preprocess_with_opencv, [img], Tout=tf.float32)

    # 4. Transpose nếu mô hình yêu cầu
    img = tf.transpose(img, perm=[1, 0, 2])

    # 5. Trả về dictionary (image, label)
    return {&quot;image&quot;: img, &quot;label&quot;: label}
</code></pre>
<p>The original image looks like this:</p>
<p><a href=""https://i.sstatic.net/tLcpGAyf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tLcpGAyf.png"" alt=""enter image description here"" /></a></p>
<p>However, after processing the image using the encode_single_sample function, the data seems incorrect, and the output image is not clear. How can I resolve this issue and ensure that the processed image is accurate and clear?</p>
","0","Question"
"79226790","","<p>I am integrating Microsoft Semantic Kernel with OpenAI in my FastAPI application. I have a chat/ endpoint where I receive a session_id from the request, and I need to pass this session_id to a plugin along with the openai_client. However, I'm unsure how to properly pass the session_id from the FastAPI request to the plugin inside the kernel's execution process.</p>
<p>Here is the relevant code for setting up the kernel and plugin:</p>
<pre><code># Kernel and service setup
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
                ai_model_id=model_id, 
                api_key=api_key, 
                service_id=service_id 
            )

# Add services and plugins
kernel.add_service(chat_completion_service) 
kernel.add_plugin(MovesPlugin(openai_client), plugin_name='MovesPlugin')
</code></pre>
<p>Inside my FastAPI endpoint, I want to pass the <code>session_id</code> to the plugin when calling the kernel for a chat response:</p>
<pre><code># Inside FastAPI endpoint
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
    session_id = await request.json().get('session_id')

    # Get chat completion service
    _chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)
    
    # Get chat completion response
    response = await _chat_completion_service.get_chat_message_content(
        chat_history=chat_history,
        kernel=kernel,
        settings=execution_settings
    )
    
    return response
</code></pre>
<p>How can I pass the request context (<code>session_id</code>) from the FastAPI request to the MovesPlugin and make sure it is used correctly along with the openai_client inside the Semantic Kernel execution?</p>
<p>Any guidance or suggestions would be greatly appreciated!</p>
","0","Question"
"79226995","","<p>I am training an ML model with PyTorch on my own dedicated remote server, using Jupyter as my IDE.</p>
<p>Around 120 epochs (about 2 hours into training), the Jupyter cell stops updating the output, but the status bar still says busy as the kernel status, and the SSH connection is still active.</p>
<p>I thought that maybe training was continuing, but the output cell stopped updating because it contains too much output. To check this hypothesis, I left Jupyter running for about 7 hours last night. When I woke up, it had stopped updating the output cell at 123 epochs, and when I killed the execution and printed out the current number of epochs, it had only reached 126 epochs.</p>
<p>Any idea what could be causing this?</p>
","0","Question"
"79227390","","<p>Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific &quot;category&quot; using python and any NLP library?</p>
<p>For example given a step for a culinary recipe <code>Add an onion to a bowl of carrots</code> as input text, I'd like to retrive <code>onion</code> and <code>carrots</code> while given <code>Sprinkle with paprika.</code> should return <code>paprika</code>.
But this should also work with sentences like <code>stir well, and cook an additional minute.</code> that do not contain any food entity in them.</p>
<p>So far, what I was able to achieve is using the <code>spacy</code> library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences:</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('trained_model')

document = nlp('Add flour, mustard, and salt')
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]
# (quite) correct output

document = nlp('I took a building, car and squirrel on the weekend')
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]
# wrong output

document = nlp('stir well, and cook an additional minute.')
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]
# wrong output
</code></pre>
<p>I am aware that there are several similar questions and posts, but I have found only solutions working for &quot;semi-structured&quot; text, i.e. list of ingredients as <code>1 tsp. of sugar, 1 cup of milk, ...</code> which can be easily solved using the previous rule-based approach. Also <code>nltk</code> and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods.</p>
<p>What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing.
Which methods should I use/look at to achieve this?</p>
","1","Question"
"79228528","","<p>Here is my model:</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the shallow CNN
class ShallowCNN(nn.Module):
    def __init__(self, in_channels, out_dim):
        super(ShallowCNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        # Use a dummy input to calculate the flattened size
        self.flatten_size = self._get_flattened_size(in_channels)

        self.fc = nn.Linear(self.flatten_size, out_dim)

    def _get_flattened_size(self, in_channels):
        # Create a dummy tensor to calculate the size after conv/pool layers
        dummy_input = torch.zeros(1, in_channels, 32, 32)
        output = self.pool(F.relu(self.conv2(F.relu(self.conv1(dummy_input)))))
        return output.numel()  # Return the number of elements in the flattened tensor

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = F.relu(self.fc(x))
        return x


class DeepCNN(nn.Module):
    def __init__(self, out_dim):
        super(DeepCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )

        # Use a dummy input to calculate the flattened size
        self.flatten_size = self._get_flattened_size()

        self.fc = nn.Linear(self.flatten_size, out_dim)

    def _get_flattened_size(self):
        # Create a dummy tensor to calculate the size after conv/pool layers
        dummy_input = torch.zeros(1, 3, 32, 32)
        output = self.features(dummy_input)
        return output.numel()  # Return the number of elements in the flattened tensor

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = F.relu(self.fc(x))
        return x


class MultiScaleCNN(nn.Module):
    def __init__(self, out_dim):
        super(MultiScaleCNN, self).__init__()
        self.shallow1 = ShallowCNN(3, out_dim // 3)
        self.shallow2 = ShallowCNN(3, out_dim // 3)
        self.deep = DeepCNN(out_dim // 3)

        # Output size after concatenating the embeddings from all three networks
        self.fc = nn.Linear(out_dim, out_dim)

    def forward(self, x):
        # Pass through shallow and deep networks
        x1 = self.shallow1(x)
        x2 = self.shallow2(x)
        x3 = self.deep(x)

        # Combine outputs
        x_combined = torch.cat([x1, x2, x3], dim=1)

        # Final fully connected layer
        x_out = F.relu(self.fc(x_combined))
        return x_out




# Define the Siamese Network
class SiameseNetwork(nn.Module):
    def __init__(self, embedding_dim):
        super(SiameseNetwork, self).__init__()
        self.multi_scale_cnn = MultiScaleCNN(embedding_dim)

    def forward(self, input1, input2):
        # Generate embeddings for both inputs
        output1 = self.multi_scale_cnn(input1)
        output2 = self.multi_scale_cnn(input2)
        return output1, output2

# Instantiate the model
embedding_dim = 4096  # As described in the paper
model = SiameseNetwork(embedding_dim=embedding_dim)
print(model)


from torch.utils.data import DataLoader, Dataset
import torch.optim as optim
from tqdm import tqdm
from PIL import Image
import torch
from torchvision import transforms

# 1. Contrastive Loss Function
class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
        # Calculate Euclidean distance
        euclidean_distance = F.pairwise_distance(output1, output2)
        # Loss function
        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +
                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss

# 2. Prepare Pair Dataset
class SiameseCIFAR10Dataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        anchor_image = self.images[idx]
        anchor_label = self.labels[idx]

        # Positive Pair: Choose an image with the same label
        positive_idx = np.random.choice(np.where(self.labels == anchor_label)[0])
        positive_image = self.images[positive_idx]

        # Negative Pair: Choose an image with a different label
        negative_idx = np.random.choice(np.where(self.labels != anchor_label)[0])
        negative_image = self.images[negative_idx]

        # Convert numpy array to PIL Image
        anchor_image = Image.fromarray((anchor_image * 255).astype(np.uint8))
        positive_image = Image.fromarray((positive_image * 255).astype(np.uint8))
        negative_image = Image.fromarray((negative_image * 255).astype(np.uint8))

        # Apply transformations
        if self.transform:
            anchor_image = self.transform(anchor_image)
            positive_image = self.transform(positive_image)
            negative_image = self.transform(negative_image)

        # Return anchor-positive and anchor-negative pairs
        return (anchor_image, positive_image, torch.tensor(0)), (anchor_image, negative_image, torch.tensor(1))


# Define the transformation
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# Create training dataset and loader for Siamese Network
train_pairs_dataset = SiameseCIFAR10Dataset(train_data, train_labels, transform)
train_loader = DataLoader(train_pairs_dataset, batch_size=32, shuffle=True)


# 3. Training Loop
def train_siamese_network(model, train_loader, criterion, optimizer, epochs, device):
    model.to(device)
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for (anchor, positive, label_pos), (anchor_neg, negative, label_neg) in tqdm(train_loader, desc=f&quot;Epoch {epoch + 1}/{epochs}&quot;):
            # Move data to device
            anchor = anchor.to(device)
            positive = positive.to(device)
            negative = negative.to(device)
            label_pos = label_pos.float().to(device)
            label_neg = label_neg.float().to(device)

            # Zero gradients
            optimizer.zero_grad()

            # Forward pass for positive pair
            ***output1_pos, output2_pos = model(anchor, positive)***
            loss_pos = criterion(output1_pos, output2_pos, label_pos)

            # Forward pass for negative pair
            output1_neg, output2_neg = model(anchor_neg, negative)
            loss_neg = criterion(output1_neg, output2_neg, label_neg)

            # Combine losses and backward pass
            loss = loss_pos + loss_neg
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # Epoch Loss
        print(f&quot;Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}&quot;)


# 4. Initialize Model, Loss, and Optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SiameseNetwork(embedding_dim=4096)
criterion = ContrastiveLoss(margin=1.0)
optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)

# 5. Train the Model
train_siamese_network(model, train_loader, criterion, optimizer, epochs=10, device=device)
</code></pre>
<p>The bold and italic line is giving the error. How to resolve it? I am trying to run a siamese network on CIFAR-10 dataset. I have pre processed and extracted the dataset but I am not able to train it for the reason mentioned above.</p>
","-1","Question"
"79232257","","<p>I'm using the Hugging Face <code>Trainer</code> (or <code>SFTTrainer</code>) for fine-tuning, and I want to log the training loss at step 0 (before any training steps are executed). I know there's an <code>eval_on_start</code> option for evaluation, but I couldn't find a direct equivalent for training loss logging at the beginning of training.</p>
<p>Is there a way to log the initial training loss at step zero (before any updates) using <code>Trainer</code> or <code>SFTTrainer</code>? Ideally, I'd like something similar to <code>eval_on_start</code>.</p>
<p>Here's what I've tried so far:</p>
<h4>Solution 1: Custom Callback</h4>
<p>I implemented a custom callback to log the training loss at the start of training:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
    def on_train_begin(self, args, state, control, logs=None, **kwargs):
        # Log training loss at step 0
        logs = logs or {}
        logs[&quot;train/loss&quot;] = None  # Replace None with an initial value if available
        logs[&quot;train/global_step&quot;] = 0
        self.log(logs)

    def log(self, logs):
        print(f&quot;Logging at start: {logs}&quot;)
        wandb.log(logs)

# Adding the callback to the Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=training_args,
    optimizers=(optimizer, scheduler),
    callbacks=[TrainOnStartCallback()],
)
</code></pre>
<p>This works but feels a bit overkill. It logs metrics at the start of training before any steps.</p>
<h4>Solution 2: Manual Logging</h4>
<p>Alternatively, I manually log the training loss before starting training:</p>
<pre class=""lang-py prettyprint-override""><code>wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()
</code></pre>
<h3>Question:</h3>
<p>Are there any built-in features in <code>Trainer</code> or <code>SFTTrainer</code> to log training loss at step zero? Or is a custom callback or manual logging the best solution here? If so, are there better ways to implement this functionality? similar to the <code>eval_on_start</code> but <code>train_on_start</code>?</p>
<p>cross:</p>
<ul>
<li><a href=""https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188"" rel=""nofollow noreferrer"">discuss.huggingface</a></li>
<li><a href=""https://github.com/huggingface/transformers/issues/34981"" rel=""nofollow noreferrer"">github/huggingface</a></li>
</ul>
","0","Question"
"79232596","","<p>I have developed an API using Flask to integrate a deep learning model trained on chest X-ray images for pneumonia detection. To test the connection between PHP and the Flask API, I used the Fashion MNIST model (for clothing classification) successfully, and the model was predicting classes correctly.</p>
<p>However, when I try to use my custom trained model for pneumonia detection (which performs very well during evaluation), I noticed that the model always predicts the class &quot;PNEUMONIA&quot; even when the image is in the &quot;NORMAL&quot; class.</p>
<p>Model Performance (on test set):</p>
<p>Test loss: 11.41 %
Test accuracy: 97.10 %
Test precision: 97.46 %
Test recall: 98.60 %
Test AUC: 98.33 %</p>
<p>Steps Taken:</p>
<p>Training: I trained the model using ResNet50 layers with additional custom layers.</p>
<p>Testing: I tested the model on a test set containing NORMAL and PNEUMONIA images, and it performed well.</p>
<p>Testing via Flask API: I tried sending images via PHP to the Flask API, but the model always predicts &quot;PNEUMONIA&quot;.</p>
<p>Flask API Code:</p>
<pre><code>from flask import Flask, request, jsonify
from tensorflow import keras
import numpy as np
from PIL import Image

app = Flask(__name__)

# Load the model
model = keras.models.load_model('path_to_your_model')

# Class names
class_names = ['NORMAL', 'PNEUMONIA']

@app.route('/predict', methods=['POST'])
def predict():
    if 'image' not in request.files:
        return jsonify({'error': 'No image file provided'}), 400
    
    image = request.files['image']
    
    try:
        # Convert the image to RGB and resize it
        img = Image.open(image).convert('RGB')
        img = img.resize((224, 224))
        
        # Convert the image to an array and expand dimensions to match model input
        img_array = np.array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
        
        # Make prediction
        prediction = model.predict(img_array)
        
        # Get the predicted class
        predicted_class = class_names[int(prediction[0] &gt; 0.5)]
        
        return jsonify({
            'prediction': predicted_class,
            'probability': float(prediction[0])
        })
    except Exception as e:
        return jsonify({'error': f&quot;An error occurred: {str(e)}&quot;}), 500

if __name__ == '__main__':
    app.run(debug=True)

</code></pre>
<p>PHP Code to Send Image to Flask API:</p>
<pre><code>&lt;?php
if (isset($_POST['submit'])) {
    if (isset($_FILES['img']) &amp;&amp; $_FILES['img']['error'] == 0) {
        $image = $_FILES['img']['name'];
        $image_tmp_name = $_FILES['img']['tmp_name'];
        $folder = 'uploaded_img/';
        $image_folder = $folder . $image;
        
        if (!is_dir($folder)) {
            mkdir($folder, 0777, true);
        }
        
        if (move_uploaded_file($image_tmp_name, $image_folder)) {
            $url = 'http://localhost:5000/predict';
            $cfile = new CURLFile($image_folder, 'image/jpeg', $image);
            $data = array('image' =&gt; $cfile);
            
            $ch = curl_init();
            curl_setopt($ch, CURLOPT_URL, $url);
            curl_setopt($ch, CURLOPT_POST, 1);
            curl_setopt($ch, CURLOPT_POSTFIELDS, $data);
            curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
            
            $response = curl_exec($ch);
            curl_close($ch);
            
            if ($response === false) {
                echo &quot;Error in prediction!&quot;;
            } else {
                $result = json_decode($response, true);
                echo &quot;Prediction: &quot; . $result['prediction'];
            }
        }
    }
}
?&gt;

</code></pre>
<p>Issue: Despite the model working well when tested locally with good performance metrics on NORMAL and PNEUMONIA images, when I test it through the Flask API (via PHP), the model always predicts &quot;PNEUMONIA&quot;, even when the image is from the &quot;NORMAL&quot; class.</p>
<p>Is there any issue with the way I'm sending the image or how the Flask API is processing it?</p>
","3","Question"
"79236682","","<p>The guide for fine-tuning Gemma with HuggingFace toolset is at: <a href=""https://huggingface.co/blog/gemma-peft"" rel=""nofollow noreferrer"">https://huggingface.co/blog/gemma-peft</a></p>
<p>Link to the line: <a href=""https://huggingface.co/blog/gemma-peft#:%7E:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A"" rel=""nofollow noreferrer"">https://huggingface.co/blog/gemma-peft#:~:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A</a></p>
<p>The data entry formatter func is:</p>
<pre><code>def formatting_func(example):
    text = f&quot;Quote: {example['quote'][0]}\nAuthor: {example['author'][0]}&lt;eos&gt;&quot;
    return [text]
</code></pre>
<p>Do those <code>[0]</code> make sense? they look wrong coz when printing out <code>text</code> variable I can see they are just characters instead of strings.</p>
","0","Question"
"79239232","","<p>I am trying to implement a <a href=""https://en.wikipedia.org/wiki/Softmax_function#:%7E:text=In%20the%20language%20of%20tropical,called%20%22tropicalization%22%20or%20%22dequantization"" rel=""nofollow noreferrer"">softmax function</a> that takes in signed int8 input and returns a signed int8 output array.</p>
<p>The current implementation I have going is this,</p>
<pre><code> import numpy as np

def softmax_int8(inputs):
    inputs = np.array(inputs, dtype=np.int8)
    
    x = inputs.astype(np.int32)
    x_max = np.max(x)
    x_shifted = x - x_max
    scale_factor = 2 ** 14 
    exp_limit = 16
    exp_x = np.clip(x_shifted + exp_limit, 0, None)
    exp_x = (1 &lt;&lt; exp_x)
    sum_exp_x = np.sum(exp_x)

    if sum_exp_x == 0:
        sum_exp_x = 1

    softmax_probs = (exp_x * scale_factor) // sum_exp_x
    max_prob = np.max(softmax_probs)
    min_prob = np.min(softmax_probs)
    range_prob = max_prob - min_prob if max_prob != min_prob else 1

    scaled_probs = ((softmax_probs - min_prob) * 255) // range_prob - 128
    outputs = scaled_probs.astype(np.int8)

    return outputs
</code></pre>
<p>I test it using this input, <code>Input = [101, 49, 6, -34, -75, -79, -38, 120, -55, 115]</code></p>
<p>but I get this output <code>array([-128, -128, -128, -128, -128, -128, -128,  127, -128, -121],dtype=int8)</code>.</p>
<p>My expected output is <code>array([-57, -70, -79, -86, -92, -94, -88, -54, -91, -56], dtype=int8)</code>.</p>
<p>What am I doing wrong here and how can I fix it?</p>
","4","Question"
"79241634","","<p>I am building a neural network. I couldn't load all the training data into memory at once, so I am using TensorFlow's tf.data.Dataset.from_generator function to load data incrementally. However, it throws an error saying it does not accept a list of tensors as a type.</p>
<pre><code>TypeError: `output_signature` must contain objects that are subclass of 
`tf.TypeSpec` but found &lt;class 'list'&gt; which is not.
</code></pre>
<p>The input to my neural network is a list of 151 separate tensors. How can I represent this in the generator? My code is below:</p>
<pre class=""lang-py prettyprint-override""><code>def generator(file_paths, batch_size, files_per_batch, tam, value):
    return tf.data.Dataset.from_generator(
        lambda: data_generator(file_paths, batch_size, files_per_batch, tam, value),
        output_signature=(
            [tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for _ in range(tam+1)],  # Lista de 151 tensores
            tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32)  # Rótulos
        )
    )

inputArray = [Input(shape=(tam,)) for _ in range(tam + 1)]

train_dataset = generator(file_paths, batch_size, files_per_batch, tam, False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

model.fit(train_dataset, epochs=1000, validation_split=0.2, verbose=1)
</code></pre>
<p>I tried to use tf.data.Dataset.from_generator to feed data into my neural network in batches, since I can't load all the data into memory at once.
However, I encountered an error:</p>
<pre><code>TypeError: output_signature must contain objects that are subclass of tf.TypeSpec but found &lt;class 'list'&gt; which is not.
</code></pre>
","0","Question"
"79241735","","<p>I am using the feature extractor from ViT like explained <a href=""https://colab.research.google.com/github/nateraw/huggingface-hub-examples/blob/main/vit_image_classification_explained.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
<p>And noticed a weird behaviour I cannot fully understand.</p>
<p>After loading the dataset as in that colab notebook, I see:</p>
<pre><code>ds['train'].features

{'image_file_path': Value(dtype='string', id=None),  'image':
Image(mode=None, decode=True, id=None),  'labels':
ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'],
id=None)}
</code></pre>
<p>And we can assess the features in both ways:</p>
<pre><code>ds['train']['labels'][0:5]

[0, 0, 0, 0, 0]

ds['train'][0:2]

{'image_file_path':
['/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/angular_leaf_spot/angular_leaf_spot_train.0.jpg',
'/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/angular_leaf_spot/angular_leaf_spot_train.1.jpg'],
'image': [&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB
size=500x500&gt;,   &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB
size=500x500&gt;],  'labels': [0, 0]}
</code></pre>
<p>But after</p>
<pre><code>from transformers import ViTFeatureExtractor

model_name_or_path = 'google/vit-base-patch16-224-in21k'
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)
ds = load_dataset('beans')

def transform(example_batch):
    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')
    inputs['labels'] = example_batch['labels']
    return inputs

prepared_ds = ds.with_transform(transform)
</code></pre>
<p>We see the features are kept:</p>
<pre><code>prepared_ds['train'].features

{'image_file_path': Value(dtype='string', id=None),  'image':
Image(mode=None, decode=True, id=None),  'labels':
ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'],
id=None)}

prepared_ds['train'][0:2]

{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275, 
0.1843, -0.2471],
...,
[-0.5843, -0.5922, -0.6078,  ...,  0.2627,  0.1608,  0.2000]],

         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],
          ...,
          [-0.8824, -0.9059, -0.9216,  ..., -0.2549, -0.2000, -0.1216]]],

        [[[-0.5137, -0.4902, -0.4196,  ..., -0.0275, -0.0039, -0.2157],
          ...,
          [-0.5216, -0.5373, -0.5451,  ..., -0.1294, -0.1529, -0.2627]],

         [[-0.1843, -0.2000, -0.1529,  ...,  0.2157,  0.2078, -0.0902],
          ...,
          [-0.7725, -0.7961, -0.8039,  ..., -0.3725, -0.4196, -0.5451]],

         [[-0.7569, -0.8510, -0.8353,  ..., -0.3255, -0.2706, -0.5608],
          ...,
          [-0.5294, -0.5529, -0.5608,  ..., -0.1686, -0.1922, -0.3333]]]]), 'labels': [0, 0]}
</code></pre>
<p>But when I try to access the labels directly</p>
<pre><code>prepared_ds['train']['labels']
</code></pre>
<p>I got a key error message:</p>
<pre><code>```
--------------------------------------------------------------------------- 
KeyError                                  Traceback (most recent call last) Cell In[32], line 1
----&gt; 1 prepared_ds['train']['labels']

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/arrow_dataset.py:2872, in Dataset.__getitem__(self, key)    2870 def __getitem__(self, key): 
# noqa: F811    2871     &quot;&quot;&quot;Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).&quot;&quot;&quot;
-&gt; 2872     return self._getitem(key)

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/arrow_dataset.py:2857, in Dataset._getitem(self, key, **kwargs)    2855 formatter = get_formatter(format_type, features=self._info.features,
**format_kwargs)    2856 pa_subtable = query_table(self._data, key, indices=self._indices)
-&gt; 2857 formatted_output = format_table(    2858     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns    2859 )    2860 return formatted_output

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:639, in format_table(table, key, formatter, format_columns, output_all_columns)
    637 python_formatter = PythonFormatter(features=formatter.features)
    638 if format_columns is None:
--&gt; 639     return formatter(pa_table, query_type=query_type)
    640 elif query_type == &quot;column&quot;:
    641     if key in format_columns:

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:405, in Formatter.__call__(self, pa_table, query_type)
    403     return self.format_row(pa_table)
    404 elif query_type == &quot;column&quot;:
--&gt; 405     return self.format_column(pa_table)
    406 elif query_type == &quot;batch&quot;:
    407     return self.format_batch(pa_table)

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:501, in CustomFormatter.format_column(self, pa_table)
    500 def format_column(self, pa_table: pa.Table) -&gt; ColumnFormat:
--&gt; 501     formatted_batch = self.format_batch(pa_table)
    502     if hasattr(formatted_batch, &quot;keys&quot;):
    503         if len(formatted_batch.keys()) &gt; 1:

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:522, in CustomFormatter.format_batch(self, pa_table)
    520 batch = self.python_arrow_extractor().extract_batch(pa_table)
    521 batch = self.python_features_decoder.decode_batch(batch)
--&gt; 522 return self.transform(batch)

Cell In[12], line 5, in transform(example_batch)
      3 def transform(example_batch):
      4     # Take a list of PIL images and turn them to pixel values
----&gt; 5     inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')
      7     # Don't forget to include the labels!
      8     inputs['labels'] = example_batch['labels']

KeyError: 'image'
```
</code></pre>
<p>It sounds like the error is because the feature extractor added 'pixel_values' but the feature is kept as 'image'
But it also appears to imply an attempt to re-apply <code>transform</code>...</p>
<p>Also: it is not possible to save the dataset to the disk</p>
<pre><code>    prepared_ds.save_to_disk(img_path)
```
--------------------------------------------------------------------------- 
TypeError                                 Traceback (most recent call last) Cell In[21], line 1
----&gt; 1 dataset.save_to_disk(img_path)

File ~/anaconda3/envs/LLM/lib/python3.13/site-packages/datasets/arrow_dataset.py:1503, in Dataset.save_to_disk(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)    1501         json.dumps(state[&quot;_format_kwargs&quot;][k])    1502     except TypeError as e:
-&gt; 1503         raise TypeError(    1504             str(e) + f&quot;\nThe format kwargs must be JSON serializable, but key '{k}' isn't.&quot;    1505 ) from None    1506 # Get json serializable dataset info    1507 dataset_info = asdict(self._info)

TypeError: Object of type function is not JSON serializable The format kwargs must be JSON serializable, but key 'transform' isn't.
```
</code></pre>
<p>Note the original codes in that notebook work perfectly (training, evaluation, etc). I just got this error because I tried to inspect the dataset, try to save the generated dataset, etc. to explore the dataset object...</p>
<p>Shouldn't the dataset structure be accessible in a similar way after <code>with_transform()</code> or <code>set_transform()</code>? Why does it call the transform function again if we just attempt to access one of the features?</p>
<p>I’m hoping you can shed some light on this behaviour...</p>
","0","Question"
"79243091","","<p>While I wanted to install thundersvm in kaggle using <code>!pip install thundersvm</code>, I encountered this error:</p>
<pre class=""lang-bash prettyprint-override""><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
Cell In[6], line 3
      1 get_ipython().run_line_magic('pip', 'install thundersvm')
      2 get_ipython().run_line_magic('pip', 'install keras_tuner')
----&gt; 3 from thundersvm import SVC
      4 from sklearn.preprocessing import StandardScaler
      5 from sklearn.metrics import classification_report

File /opt/conda/lib/python3.10/site-packages/thundersvm/__init__.py:10
      3 &quot;&quot;&quot;
      4  * Name        : __init__.py
      5  * Author      : Locke &lt;luojiahuan001@gmail.com&gt;
      6  * Version     : 0.0.1
      7  * Description :
      8 &quot;&quot;&quot;
      9 name = &quot;thundersvm&quot;
---&gt; 10 from .thundersvm import *

File /opt/conda/lib/python3.10/site-packages/thundersvm/thundersvm.py:39
     36         lib_path = path.join(dirname, shared_library_name)
     38 if path.exists(lib_path):
---&gt; 39     thundersvm = CDLL(lib_path)
     40 else:
     41     # try the build directory
     42     if platform == &quot;linux&quot; or platform == &quot;linux2&quot;:

File /opt/conda/lib/python3.10/ctypes/__init__.py:374, in CDLL.__init__(self, name, mode, handle, use_errno, use_last_error, winmode)
    371 self._FuncPtr = _FuncPtr
    373 if handle is None:
--&gt; 374     self._handle = _dlopen(self._name, mode)
    375 else:
    376     self._handle = handle

OSError: libcusparse.so.9.0: cannot open shared object file: No such file or directory
</code></pre>
<p>in order to fix this, I tried this:</p>
<pre class=""lang-bash prettyprint-override""><code># Download and install CUDA 9.0
wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run
sudo sh cuda_9.0.176_384.81_linux-run
</code></pre>
<p>which didn't work either. How can I fix this?</p>
","0","Question"
"79244294","","<p>I’m working on an image segmentation task and trying to use a pre-trained Swin Transformer Large (Swin-L) encoder for the feature extraction backbone. The code runs perfectly on a CPU in Colab. However, when switching to a TPU, it throws the error shown below.</p>
<p><strong>The Code:</strong></p>
<pre class=""lang-none prettyprint-override""><code>from tensorflow.keras import layers, Model, Input
from tfswin import SwinTransformerLarge224


def load_swin_encoder(input_shape=(512, 512, 3)):
    # Load pre-trained Swin-L model
    swin_encoder = SwinTransformerLarge224(include_top=False, weights='imagenet',
                                           input_shape=input_shape)

    # Freeze the pre-trained layers
    for layer in swin_encoder.layers:
        layer.trainable = False

    # Extract outputs from the four stages
    stage_outputs = [
        swin_encoder.get_layer('normalize').output,  # Output from the 0 stage
        swin_encoder.get_layer('layers.0').output,   # Output from the first stage
        swin_encoder.get_layer('layers.1').output,   # Output from the second stage
        swin_encoder.get_layer('layers.2').output,   # Output from the third stage
        swin_encoder.get_layer('layers.3').output,   # Output from the fourth stage
    ]
    return Model(swin_encoder.input, stage_outputs, name=&quot;SwinTransformerEncoder&quot;)


# Test Code
encoder = load_swin_encoder(input_shape=(512, 512, 3))
dummy_input = tf.random.uniform((1, 512, 512, 3))
encoder_outputs = encoder(dummy_input)

for i, output in enumerate(encoder_outputs):
    print(f&quot;Stage {i + 1} output shape: {output.shape}&quot;)

</code></pre>
<p><strong>The Error:</strong></p>
<p>The code throws the following error on TPU:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-28-3cb122d32678&gt; in &lt;cell line: 2&gt;()
      1 # loading Sanity check
----&gt; 2 encoder = load_swin_encoder(input_shape=(512, 512, 3))
      3 dummy_input = tf.random.uniform((1, 512, 512, 3))
      4 encoder_outputs = encoder(dummy_input)
      5 

2 frames
/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py in __init__(self, inputs, outputs, name, **kwargs)
    117         for x in flat_inputs:
    118             if not isinstance(x, backend.KerasTensor):
--&gt; 119                 raise ValueError(
    120                     &quot;All `inputs` values must be KerasTensors. Received: &quot;
    121                     f&quot;inputs={inputs} including invalid value {x} of &quot;

ValueError: All `inputs` values must be KerasTensors. Received: inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name='input_4'), name='input_4', description=&quot;created by layer 'input_4'&quot;) including invalid value KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name='input_4'), name='input_4', description=&quot;created by layer 'input_4'&quot;) of type &lt;class 'tf_keras.src.engine.keras_tensor.KerasTensor'&gt;

</code></pre>
<p><strong>Question:</strong></p>
<p>Why does this code work on a <strong>CPU</strong> but fail on a <strong>TPU</strong> in Colab? How can I fix this issue to make it compatible with TPU execution?</p>
<p>Any insights or guidance would be greatly appreciated. Thank you!</p>
","1","Question"
"79247672","","<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>
<pre><code>import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score
from captum.attr import IntegratedGradients

# Loading data
train_df = pd.read_csv('train_dataset.csv')
test_df = pd.read_csv('test_dataset.csv')

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_data(df, tokenizer, max_len=128):
    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)
    labels = torch.tensor(df['label'].values)
    return inputs, labels

train_inputs, train_labels = preprocess_data(train_df, tokenizer)
test_inputs, test_labels = preprocess_data(test_df, tokenizer)

# DataLoader
train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)
test_loader = DataLoader(test_dataset, batch_size=16)

# Model setup
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training Loop
model.train()
for epoch in range(3):  # Train for 3 epochs
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)

# Evaluation
model.eval()
correct_predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        correct_predictions.extend(
            (preds == labels).cpu().numpy().tolist()
        )
accuracy = accuracy_score(test_labels.numpy(), correct_predictions)
print(f&quot;Test Accuracy: {accuracy:.2f}&quot;)

# Integrated Gradients
ig = IntegratedGradients(model)

def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor
    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor

    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)
    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)
    # forward function for IG
    def forward_func(input_ids):
        outputs = model(input_ids, attention_mask=attention_mask)
        return outputs.logits

    # Applying Integrated Gradients
    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

# Analysing influential words for correctly predicted texts
for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
        print(influential_words)
</code></pre>
<p>But I am getting the following error in running the above.</p>
<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1 loss: 0.4719192385673523
Epoch 2 loss: 0.39585667848587036
Epoch 3 loss: 0.14659778773784637
Test Accuracy: 0.70
Input IDs shape: torch.Size([1, 8]) dtype: torch.int64
Attention mask shape: torch.Size([1, 8]) dtype: torch.int64
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()
     90 for idx, correct in enumerate(correct_predictions):
     91     if correct:
---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
     94         print(influential_words)

18 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2549         # remove once script supports set_grad_enabled
   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2552 
   2553 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
</code></pre>
","2","Question"
"79247785","","<p>I have a large dataset and I have split it in:</p>
<ul>
<li>training set (80%)</li>
<li>validation set (10%)</li>
<li>test set (10%)</li>
</ul>
<p>On each set, I performed missing values imputation and feature selection (trained on the training set, and replicated into validation and test set) to avoid data leakage.</p>
<p>Now, I want to train an XGBoost model in Python and want to perform hyperparameter-tuning using the training set, and evaluate each parameter set with the validation set. How can I do this using a random approach such as in RandomizedSearchCV, so that I don't run all the parameter sets?</p>
<p>If I am correct, GridSearch and RandomizedSearchCV allow only cross-validation, which is not what I want, because splitting the preprocessed training set in folds will result in data leakage.
I know I could build a sklearn pipeline where I do the preprocessing in each fold, but I would like to avoid the latter option.</p>
<p>I can only think about this code that runs each parameter set like in GridSearch:</p>
<pre><code>from sklearn.model_selection import ParameterGrid
import xgboost as xgb

# Define your hyperparameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

best_score = -1
best_params = {}

for params in ParameterGrid(param_grid):
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train)
    val_score = model.score(X_val, y_val)  # Or use a more specific metric

    if val_score &gt; best_score:
        best_score = val_score
        best_params = params

# Train the final model with the best hyperparameters
best_model = xgb.XGBClassifier(**best_params)
best_model.fit(X_train, y_train)
</code></pre>
","1","Question"
"79249247","","<p>I'm trying to make a neural network model that will answer a linear regression problem (I've already made a model using <code>sklearn</code>'s <code>LinearRegression</code> and I'd like to compare the two).  Ultimately I'd like to make a class with <code>fit</code> and <code>predict</code> functions, as with the models in <code>sklearn</code>, so that I can make a loop that will test all the models I am using in my project.</p>
<p>To do this I followed the code in the answer to this question: <a href=""https://stackoverflow.com/questions/73493198/writing-a-pytorch-neural-net-class-that-has-functions-for-both-model-fitting-and"">Writing a pytorch neural net class that has functions for both model fitting and prediction</a>.
With some modifications, here is what I have:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.optim as optim

class MyNeuralNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(2, 4, bias=True)
        self.layer2 = nn.Linear(4, 1, bias=True)
        self.loss = nn.MSELoss()
        self.compile_()

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return x.squeeze()

    def fit(self, x, y):
        x = torch.tensor(x.values, dtype=torch.float32)
        y = torch.tensor(y.values, dtype=torch.float32)
        losses = []
        for epoch in range(100):
            ## Inference
            res = self.forward(x)#self(self,x)
            loss_value = self.loss(res,y)

            ## Backpropagation
            loss_value.backward() # compute gradient
            self.opt.zero_grad()  # flush previous epoch's gradient
            self.opt.step()       # Perform iteration using gradient above

            ## Logging
            losses.append(loss_value.item())
        
    def compile_(self):
        self.opt = optim.SGD(self.parameters(), lr=0.01)
       
    def predict(self, x_test):
        self.eval()
        y_test_hat = self(x_test)
        return y_test_hat.detach().numpy()
        # self.train()
</code></pre>
<p>Note, you also need <code>numpy</code>, I just don't have it here because this code was put into a separate .py file.</p>
<p>Here is how I used the model, after importing my class:</p>
<pre><code>model = MyNeuralNet()
X_train = # pandas dataframe with 1168 rows and 49 columns
y_train = # pandas dataframe with 1168 rows and 1 column
X_test = # pandas dataframe with 292 rows and 49 columns
model.fit(X_train, y_train)
pred = model.predict(X_test)
print(pred)
</code></pre>
<p>The error I got is <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (1168x49 and 2x4)</code> at the <code>fit</code> step.  I understand this has to do with the parameters for the linear layers of my network.  I think if I change my input size for the first linear layer to 49 and my output size for the second linear layer to 1168 then it will work for the <code>fit</code> step (or at least something like that, to match the sizes of the train data).  However, my test data is of a different size and I'm pretty sure then the <code>predict</code> step won't work.</p>
<p>Is it possible to make a neural network class where the training and test data are of different sizes?</p>
","0","Question"
"79250549","","<p>I have been trying to run the below code to calculate upper and lower confidence intervals using t distribution, but it keeps throwing the error in the subject. The piece of code is as below:</p>
<pre><code>def trans_threshold(Day):
    Tran_Cnt=Tran_Cnt_DF[['Sample',Day]].dropna()
    Tran_Cnt=Tran_Cnt.astype({'Sample':'str'})
    Tran_Cnt.dtypes
    #Finding outliers in Materiality via IQR
    X_Tran = Tran_Cnt.drop('Sample', axis=1)
    Tran_arr1 = X_Tran.values
    #Finding the first quartile
    Tran_q1= np.quantile(Tran_arr1, 0.25)
    # finding the 3rd quartile
    Tran_q3 = np.quantile(Tran_arr1, 0.75)
    # finding the iqr region
    Tran_iqr = Tran_q3-Tran_q1
    # finding upper and lower outliers
    Tran_upper_bound = Tran_q3+(1.5*Tran_iqr)
    Tran_lower_bound = Tran_q1-(1.5*Tran_iqr)
    # removing outliers
    Tran_arr2 = Tran_arr1[(Tran_arr1 &gt;= Tran_lower_bound) &amp; (Tran_arr1 &lt;= Tran_upper_bound)]
    #Using t distribution for Materiality Limits
    Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
             loc=np.mean(Tran_arr2),
             scale=st.sem(Tran_arr2))
    return Tran_Threshold_mat



trn_lim_FullFeed_Mon = trans_threshold(Day) 

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[106], line 19
     17 Tran_arr2 = Tran_arr1[(Tran_arr1 &gt;= Tran_lower_bound) &amp; (Tran_arr1 &lt;= Tran_upper_bound)]
     18     #Using t distribution for Materiality Limits
---&gt; 19 Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
     20                                  loc=np.mean(Tran_arr2),
     21                                  scale=st.sem(Tran_arr2))

TypeError: rv_generic.interval() missing 1 required positional argument: 'confidence'
</code></pre>
<p>The issue seems to be with piece of code below. However, I have provided all parameters required to calculate confidence intervals, including degrees of freedom, but it still gives this error. Where am I going wrong and what needs to be done?</p>
<pre><code>Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
                                 loc=np.mean(Tran_arr2),
                                 scale=st.sem(Tran_arr2))
</code></pre>
<p>Also, the Tran_arr2 list looks like below:</p>
<pre><code>array([12617., 12000.,  1123.,   537.,  8605.,  4365., 11292., 12231.,
        7640.,  9583.,  9257., 13864., 14682., 11744., 10501.,  8694.,
        5327., 10066., 13022., 11092.,  7444., 11658., 14920., 12849.,
       14681.,  5719., 11029.,  3814., 14703.,  5593.,  9772.,  8851.,
        9551., 15975.,  6532., 13827.,  8547.])
</code></pre>
<p>Hence, there is no issue, up until the last like of the code block which estimates confidence intervals using t distribution.</p>
<p>I have used the below packages:</p>
<pre><code>import pandas as pd
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt
import matplotlib.ticker as tkr
import matplotlib.scale as mscale
from matplotlib.ticker import FixedLocator, NullFormatter
pd.options.display.float_format = '{:.0f}'.format
pd.options.mode.chained_assignment = None
</code></pre>
","2","Question"
"79257966","","<p><strong>What I am working on?</strong></p>
<p>I am working on a Machine Learning project that predicts the price of electric vehicles in the different states of the USA. My goal is to solidify my practical skills. I have done everything in the project, like performing one-hot encoding, training the model, and running the Flask app on localhost.
In localhost, I have filled out the form with the following values and then clicked on the submit button:</p>
<pre><code>County: Jefferson
City: PORT TOWNSEND
ZIP Code: 98368
Model Year: 2012
Make: NISSAN
Model: LEAF
Electric Vehicle Type: Battery Electric Vehicle (BEV)
CAFV Eligibility: Clean Alternative Fuel Vehicle Eligible
Legislative District: 24
</code></pre>
<p><strong>What issue am I facing?</strong></p>
<p>After submitting the form, I get this error:</p>
<pre><code>ValueError
ValueError: Found unknown categories \['98368'\] in column 2 during transform

Traceback (most recent call last)
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\flask\\app.py&quot;, line 1498, in __call__
return self.wsgi_app(environ, start_response)
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\flask\\app.py&quot;, line 1476, in wsgi_app
response = self.handle_exception(e)
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\flask\\app.py&quot;, line 1473, in wsgi_app
response = self.full_dispatch_request()
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\flask\\app.py&quot;, line 882, in full_dispatch_request
rv = self.handle_user_exception(e)
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\flask\\app.py&quot;, line 880, in full_dispatch_request
rv = self.dispatch_request()
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\flask\\app.py&quot;, line 865, in dispatch_request
return self.ensure_sync(self.view_functions\[rule.endpoint\])(\*\*view_args)  # type: ignore\[no-any-return\]
File &quot;G:\\Machine_Learning_Projects\\austin\\electric_vehicle_price_prediction_2\\app\\routes.py&quot;, line 38, in predict
price = predict_price(features)
File &quot;G:\\Machine_Learning_Projects\\austin\\electric_vehicle_price_prediction_2\\app\\model.py&quot;, line 29, in predict_price
transformed_features = encoder.transform(features_df)
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\sklearn\\utils_set_output.py&quot;, line 157, in wrapped
data_to_wrap = f(self, X, \*args, \*\*kwargs)
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\sklearn\\preprocessing_encoders.py&quot;, line 1027, in transform
X_int, X_mask = self.\_transform(
File &quot;C:\\Users\\austin.conda\\envs\\electric_vehicle_price_prediction_2\\lib\\site-packages\\sklearn\\preprocessing_encoders.py&quot;, line 200, in \_transform
raise ValueError(msg)
ValueError: Found unknown categories \['98368'\] in column 2 during transform\
</code></pre>
<p><strong>What did I try?</strong></p>
<p>I tried using the following code:</p>
<p>Code of the <code>routes.py</code> file inside the <code>app</code> folder:</p>
<pre><code>from flask import render_template, request, jsonify
from app import app
from app.model import predict_price
from jinja2 import Environment, FileSystemLoader, PackageLoader, select_autoescape

@app.route('/')
def index():
env = Environment(
loader=PackageLoader(&quot;app&quot;),
autoescape=select_autoescape()
)
template = env.get_template(&quot;index.html&quot;)
return render_template(template)

@app.route('/predict', methods=\['POST'\])
def predict():
data = request.form.to_dict()

    # Convert the form data into the correct format for prediction
    features = [
        data['county'],
        data['city'],
        data['zip_code'],
        data['model_year'],
        data['make'],
        data['model'],
        data['ev_type'],
        data['cafv_eligibility'],
        data['legislative_district']
    ]
    
    # Get the prediction result
    price = predict_price(features)
    
    return jsonify({'predicted_price': price})
</code></pre>
<p>Code of the <code>model.py</code> file inside the <code>app</code> folder:</p>
<pre><code>import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
import joblib
from flask import Flask, render_template
from jinja2 import Environment, FileSystemLoader, PackageLoader, select_autoescape

env = Environment(
loader=PackageLoader(&quot;app&quot;),
autoescape=select_autoescape()
)

model = joblib.load('model/ev_price_model.pkl')

def predict_price(features):
    encoder = joblib.load('model/encoder.pkl')  # Load encoder if needed
    
    features_df = pd.DataFrame([features], columns=['County', 'City', 'ZIP Code', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility', 'Legislative District'])
    
    # Apply encoding, scaling, etc., if necessary
    transformed_features = encoder.transform(features_df)
    
    # Make the prediction
    price = model.predict(transformed_features)
    
    return price[0]  # Assuming it returns a single value

</code></pre>
<p><strong>What is the link to my GitHub repository?</strong></p>
<p>Here is the link to my repo:</p>
<p><a href=""https://github.com/SteveAustin583/electric-vehicle-price-prediction"" rel=""nofollow noreferrer"">https://github.com/SteveAustin583/electric-vehicle-price-prediction</a></p>
<p><strong>What I was expecting?</strong></p>
<p>I was expecting to get the prediction result without any issue. Because I have already performed one-hot encoding.</p>
<p>Can you help me fixing this issue?</p>
","0","Question"
"79259076","","<p>I am trying to cluster words using Kmeans. I have one large document and I use an NLTK RegexpTokenizer first and then filter based on word count, length and remove stopwords. Next, I construct a co-occurrence matrix and use this to train the Kmeans model. Finally, I test the performance of this using silhouette scores.</p>
<p>I would like to visualise these clusters. This typically seems to be done using a scatter plot and the labels. How do I reduce this co-occurrence matrix to x and y, when it is currently an N x N matrix (N is the number of unique words)? I have tried using PCA:</p>
<pre><code>plt.figure()
pca_2d = PCA(n_components=2)
reduced = pca_2d.fit_transform(mat)
newKm = KMeans(n_clusters=3)
labels = newKm.fit_predict(reduced)
plt.scatter(reduced[:, 0], reduced[:, 1], c=labels) # selects column 0 (all rows) as x coords and column 1 (all rows) as y. and then cluster labels.
plt.title(&quot;K-means Clustering&quot;)
plt.show()
</code></pre>
<p>But am not sure if this is the best or correct approach?</p>
","0","Question"
"79268711","","<p>I am working on a Machine Learning project that predicts the price electric cars on Jupyter Notebook.</p>
<p>I run this cell on Jupyter Notebook:</p>
<pre><code>p = regressor.predict(df2)
</code></pre>
<p>I get this error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_16424\818753220.py in &lt;module&gt;
----&gt; 1 p = regressor.predict(df2)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\tree\_classes.py in predict(self, X, check_input)
    465         &quot;&quot;&quot;
    466         check_is_fitted(self)
--&gt; 467         X = self._validate_X_predict(X, check_input)
    468         proba = self.tree_.predict(X)
    469         n_samples = X.shape[0]

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\tree\_classes.py in _validate_X_predict(self, X, check_input)
    431         &quot;&quot;&quot;Validate the training data on predict (probabilities).&quot;&quot;&quot;
    432         if check_input:
--&gt; 433             X = self._validate_data(X, dtype=DTYPE, accept_sparse=&quot;csr&quot;, reset=False)
    434             if issparse(X) and (
    435                 X.indices.dtype != np.intc or X.indptr.dtype != np.intc

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    564             raise ValueError(&quot;Validation should be done on X, y or both.&quot;)
    565         elif not no_val_X and no_val_y:
--&gt; 566             X = check_array(X, **check_params)
    567             out = X
    568         elif no_val_X and not no_val_y:

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    744                     array = array.astype(dtype, casting=&quot;unsafe&quot;, copy=False)
    745                 else:
--&gt; 746                     array = np.asarray(array, order=order, dtype=dtype)
    747             except ComplexWarning as complex_warning:
    748                 raise ValueError(

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\pandas\core\generic.py in __array__(self, dtype)
   1991 
   1992     def __array__(self, dtype: NpDtype | None = None) -&gt; np.ndarray:
-&gt; 1993         return np.asarray(self._values, dtype=dtype)
   1994 
   1995     def __array_wrap__(

ValueError: could not convert string to float: 'N/'
</code></pre>
<p><strong>What did I try?</strong></p>
<p>I tried using the following code:</p>
<pre><code>uv = np.nanpercentile(df2['Base MSRP'], [99])[0]*2
df2['Base MSRP'][(df2['Base MSRP']&gt;uv)] = uv
le = preprocessing.LabelEncoder()
cols = ['County', 'City', 'State', 'ZIP Code', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']
for col in cols:
    le.fit(t[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)
regressor.fit(x, y)
p = regressor.predict(df2)
</code></pre>
<p>Here is the link to my notebook: <a href=""https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb"" rel=""nofollow noreferrer"">https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb</a></p>
<p>How to fix this issue?</p>
","0","Question"
"79269787","","<p>There is an existing xgboost model in the pipeline that was created using this container</p>
<pre><code>sagemaker.image_uris.retrieve('xgboost', sagemaker.Session().boto_region_name, version='latest')
</code></pre>
<p>output:</p>
<pre class=""lang-none prettyprint-override""><code>'{accountid}.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest'
</code></pre>
<p>I extracted the model.tar.gz from the model artifact and loaded the xgboost-model file<br>
but it gave this error<br></p>
<pre><code>XGBoostError: basic_string::resize
</code></pre>
<p>I ran a shell script that loads the model using all the versions of XGBoost available but nothing worked.<br>
I just want to <strong>check the feature importance</strong> <code>using model.get_score</code>.</p>
","0","Question"
"79270683","","<p>For the recommendation problem I am working on, there are around 50000 unique brands and 3 level product categories, level_1_cat (50 categories), level_2_cat (100 categories) and level_3_cat (1000 categories). All these item features are represented by integers only. So far I have tried binary-encoding, label-encoding and target-encoding for my lightfm model. With binary-encoding and label-encoding, the results were worse than not using any item features. With target-encoding, the result were similar to not using any item features. I am wondering what else I can try.</p>
","-1","Question"
"79271387","","<p>I'm trying to implement a simplified version of KMeans clustering, but somehow, sometime the result is not consistent</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import random

import matplotlib.pyplot as plt

class KMeans:
    def __init__(
        self,
        n_clusters,
        max_iter
    ):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
    
    def _get_distance(self, x, cluster_location):
        &quot;&quot;&quot;calculate the euclidean distance from each points to the clusters center
        &quot;&quot;&quot;
        return np.linalg.norm(x[:,np.newaxis,:]-cluster_location, axis = 2)
        
    def fit(self, x:np.ndarray) -&gt; None:
        &quot;&quot;&quot;Steps for k-means clustering
        1. initiate cluster location
        2. calculate distance from each points to cluster point
        3. assign each points to a cluster
        4. update the cluster location using the mean of the associated points
        5. repeat 2-4 until convergence or max_iter reached

        Args:
            x (_type_): _description_

        Returns:
            _type_: _description_
        &quot;&quot;&quot;
        self.x = x
        data_dim = x.shape[1]

        # 1. initiate cluster location
        cluster_locations = np.random.uniform(x.min(), x.max(), size=(self.n_clusters,data_dim))
        # print(&quot;initial:\n&quot;,cluster_locations)

        for _ in range(self.max_iter):
            # 2. calculate distance from each points to cluster point
            distances = self._get_distance(x, cluster_locations)

            # 3. assign each points to a cluster
            clusters = np.argmin(distances, axis=1)

            # 4. update the cluster location using the mean of the associated points
            for cluster in range(self.n_clusters):
                # Check if cluster has any points
                cluster_mask = clusters == cluster
                if np.any(cluster_mask):
                    cluster_locations[cluster] = np.mean(x[cluster_mask], axis=0)
                else:
                    # If cluster is empty, reinitialize with a random point
                    cluster_locations[cluster] = x[np.random.randint(x.shape[0])]                        

        self.cluster_locations = cluster_locations
        self.clusters = clusters
        return None

    def visualize(self, data, clusters):
        _, ax = plt.subplots(1,1,figsize=(5,5))

        cluster_color = [(random.random(),random.random(),random.random()) for _ in range(self.n_clusters)]

        for cluster in range(self.n_clusters):
            to_plot = data[np.where(clusters == cluster)[0]]
            ax.scatter(to_plot[:,0], to_plot[:,1], color=cluster_color[cluster])
        ax.scatter(self.cluster_locations[:,0], self.cluster_locations[:,1], marker=&quot;x&quot;, color='r', s=30)

        plt.show()
</code></pre>
<p>This is how I used it</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import make_blobs

random_state = 42
n_samples = 100

x, _ = make_blobs(n_samples=n_samples, random_state=random_state)
my_kmeans = KMeans(3, 50)
my_kmeans.fit(x)
my_kmeans.visualize(x, my_kmeans.clusters)
</code></pre>
<p>Most of the times, it gives me a reasonable output, like this
<a href=""https://i.sstatic.net/yr24F7t0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yr24F7t0.png"" alt=""reasonable output"" /></a></p>
<p>But every several runs, it gives me something like this</p>
<p><a href=""https://i.sstatic.net/LhCtamCd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhCtamCd.png"" alt=""weird output"" /></a></p>
<p>Is there something I missed?</p>
","-1","Question"
"79271439","","<p>I would like to write a RBF kernel that is working only in a specific range on X axis. I tried to write a class that contains a RBF kernel to test the code</p>
<pre><code>class RangeLimitedRBFTest(Kernel):
    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5), x_min = 0., x_max = 1.):
        self.length_scale = length_scale
        self.length_scale_bounds = length_scale_bounds
        self.rbf_kernel = RBF(length_scale, length_scale_bounds)
        self.x_min = x_min
        self.x_max = x_max

    def __call__(self, X, Y=None, eval_gradient=False):
        if eval_gradient and Y is not None:
            raise ValueError(&quot;Gradient can only be evaluated when Y is None.&quot;)
        
        X = np.atleast_2d(X)
        if Y is not None:
            Y = np.atleast_2d(Y)

        print(f&quot;X shape: {X.shape}&quot;)
        if Y is not None:
            print(f&quot;Y shape: {Y.shape}&quot;)
        else:
            print(&quot;Y shape: None&quot;)

        K_rbf = self.rbf_kernel(X, Y, eval_gradient=eval_gradient)

        if eval_gradient:
            K, K_grad = K_rbf
            print(f&quot;Kernel matrix shape (K): {K.shape}&quot;)
            print(f&quot;Kernel gradient matrix shape (K_grad): {K_grad.shape}&quot;)
            return K, K_grad
        else:
            K = K_rbf
            return K

    def diag(self, X):
        return self.rbf_kernel.diag(X)

    def is_stationary(self):
        return self.rbf_kernel.is_stationary()
</code></pre>
<p>The implementation and the fitting is like this</p>
<pre><code>kernel = 1.0 * RangeLimitedRBFTest(length_scale=0.1, length_scale_bounds=(8e-2, 8e-1), x_min=0., x_max=2.5) + WhiteKernel(noise_level=0.5, noise_level_bounds=(1e-2, 1e1))
gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1, alpha=1e-5, optimizer='fmin_l_bfgs_b')
gaussian_process.optimizer_kwargs = {&quot;max_iter&quot;: 10000} 
gaussian_process.fit(X, T_PMT)
</code></pre>
<p>If I run the code I get the following output</p>
<pre><code>X shape: (6248, 1)
Y shape: None
Kernel matrix shape (K): (6248, 6248)
Kernel gradient matrix shape (K_grad): (6248, 6248, 1)
ValueError: 0-th dimension must be fixed to 2 but got 3


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/tdaq/cremonini/pt100_probe/read_temperatures.py&quot;, line 97, in &lt;module&gt;
    gaussian_process.fit(X, T_PMT)
  File &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/base.py&quot;, line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;, line 308, in fit
    self._constrained_optimization(
  File &quot;/home/tdaq/.local/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py&quot;, line 653, in _constrained_optimization
    opt_res = scipy.optimize.minimize(
  File &quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_minimize.py&quot;, line 713, in minimize
    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,
  File &quot;/cvmfs/atlas.cern.ch/repo/sw/software/0.3/StatAnalysis/0.3.1/InstallArea/x86_64-el9-gcc13-opt/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py&quot;, line 360, in _minimize_lbfgsb
    _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,
ValueError: failed in converting 7th argument `g' of _lbfgsb.setulb to C/Fortran array
</code></pre>
<p>If i try to use the usual RBF kernel, the code works without any problem. I tried also to disable the optimizer <code>optimizer=None</code> and the code works but I get very large error.</p>
","2","Question"
"79274150","","<p>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</p>
<p>Getting this error only while using mps and not cpu</p>
<pre><code>
fine_tuned_decoder_path = &quot;/path/fine_tuned_decoder&quot;
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    encoder_pretrained_model_name_or_path=&quot;google/vit-base-patch16-224-in21k&quot;,
    decoder_pretrained_model_name_or_path=fine_tuned_decoder_path,
    tie_encoder_decoder=True,
    cache_dir=&quot;/path/datasets/&quot;+&quot;models&quot;  # Directory for caching models
)
os.environ[&quot;WANDB_MODE&quot;] = &quot;disabled&quot;

# Set batch size and number of training epochs
BATCH_SIZE = 16
TRAIN_EPOCHS = 5

# Define the output directory for storing training outputs
output_directory = os.path.join(&quot;path&quot;, &quot;captioning_outputs&quot;)

# Check if MPS is available
device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Move your model to the correct device
model.to(device)

# Set mixed precision and device handling
fp16 = False  # Disable fp16 entirely
mixed_precision = None  # Disable mixed precision (default)

# Training Arguments
training_args = TrainingArguments(
    output_dir=output_directory,
    per_device_train_batch_size=BATCH_SIZE,
    do_train=True,
    num_train_epochs=TRAIN_EPOCHS,
    overwrite_output_dir=True,
    use_cpu=False,  # Ensure you're not using CPU
    dataloader_pin_memory=False,
    fp16=fp16,  # Disable fp16 if using MPS
    bf16=False,  # Disable bf16 if using MPS
    optim=&quot;adamw_torch&quot;,  # Use AdamW Torch optimizer (more stable with mixed precision)
    gradient_checkpointing=False,  # Disable gradient checkpointing if necessary
    logging_dir=os.path.join(output_directory, 'logs'),
    report_to=&quot;none&quot;,  # Disable reporting
    
)

# Use the Trainer with the model on the correct device
trainer = Trainer(
    processing_class=feature_extractor,  # Tokenizer
    model=model,  # Model to train
    args=training_args,  # Training arguments
    train_dataset=train_dataset,  # Training dataset
    data_collator=default_data_collator  # Data collator
)
# Start the training process
trainer.train()
</code></pre>
<p>Tried setting use_cpu=True, it works fine but not with mps</p>
","1","Question"
"79275458","","<p>I'm using a Basler camera with OpenCV's DNN module to run an SSD object detection model (mobilenetv2_050_Opset18.onnx) with CUDA support enabled on c++. The Basler camera is correctly capturing frames, and OpenCV is processing the images, but the SSD model doesn't detect any objects, not even with a confidence threshold as low as 0.0001.</p>
<p>Key Details:
Camera: Basler camera is configured for 640x480 resolution and BGR8 pixel format.</p>
<p>Model: SSD model (mobilenetv2_050_Opset18.onnx) loaded using OpenCV DNN module with CUDA backend (DNN_BACKEND_CUDA).</p>
<p>CUDA Support: OpenCV compiled with CUDA and cuDNN support with cmake. The backend is set to use CUDA (net.setPreferableBackend(DNN_BACKEND_CUDA)), and the target is also set to CUDA (net.setPreferableTarget(DNN_TARGET_CUDA)).</p>
<p>Problem: The model is not detecting any objects, even with a confidence threshold as low as 0.0001. I have verified that both the camera and the CUDA environment are functioning as expected.</p>
<p>What is the problem in my code?</p>
<pre><code>#include &lt;iostream&gt;
#include &lt;pylon/PylonIncludes.h&gt;
#include &lt;pylon/InstantCamera.h&gt;
#include &lt;pylon/BaslerUniversalInstantCamera.h&gt;
#include &lt;opencv2/opencv.hpp&gt;
#include &lt;opencv2/dnn.hpp&gt;
#include &lt;opencv2/core/cuda.hpp&gt;

using namespace Pylon;
using namespace GenApi;
using namespace std;
using namespace cv;
using namespace cv::dnn;

int main() {
    PylonInitialize();
    
    try {
        // Basler camera setup
        CInstantCamera camera(CTlFactory::GetInstance().CreateDevice(CDeviceInfo().SetSerialNumber(&quot;40432296&quot;)));
        camera.Open();

        // Configure the camera settings
        INodeMap&amp; nodemap = camera.GetNodeMap();
        CIntegerParameter width(nodemap, &quot;Width&quot;);
        CIntegerParameter height(nodemap, &quot;Height&quot;);
        CEnumParameter pixelFormat(nodemap, &quot;PixelFormat&quot;);
        CFloatParameter exposureTime(nodemap, &quot;ExposureTime&quot;);
        CFloatParameter acquisitionFrameRate(nodemap, &quot;AcquisitionFrameRate&quot;);
        CIntegerParameter xOffset(nodemap, &quot;OffsetX&quot;);
        CIntegerParameter yOffset(nodemap, &quot;OffsetY&quot;);

        width.SetValue(640, IntegerValueCorrection_Nearest);
        height.SetValue(480, IntegerValueCorrection_Nearest);
        pixelFormat.SetValue(&quot;BGR8&quot;);
        exposureTime.SetValue(15000.0);
        acquisitionFrameRate.SetValue(5000.0);
        xOffset.SetValue(0);
        yOffset.SetValue(0);

        CGrabResultPtr grabResult;

        // Load SSD model
        Net net = readNetFromONNX(&quot;mobilenetv2_050_Opset18.onnx&quot;);
        net.setPreferableBackend(DNN_BACKEND_CUDA);
        net.setPreferableTarget(DNN_TARGET_CUDA);

        camera.StartGrabbing();
        Mat frame;
        while (camera.IsGrabbing()) {
            camera.RetrieveResult(5000, grabResult, TimeoutHandling_ThrowException);
            if (grabResult-&gt;GrabSucceeded()) {
                uint8_t* buffer = (uint8_t*)grabResult-&gt;GetBuffer();
                frame = Mat(grabResult-&gt;GetHeight(), grabResult-&gt;GetWidth(), CV_8UC3, buffer);

                // Run SSD model
                Mat blob = blobFromImage(frame, 1.0, Size(640, 480), Scalar(), true, false);
                net.setInput(blob);

                Mat detections = net.forward();

                for (int i = 0; i &lt; detections.size[2]; ++i) {
                    float confidence = detections.ptr&lt;float&gt;(0)[i * 7 + 2]; // Access confidence

                    if (confidence &gt; 0.0001) {
                        int xLeftBottom = static_cast&lt;int&gt;(detections.ptr&lt;float&gt;(0)[i * 7 + 3] * frame.cols);
                        int yLeftBottom = static_cast&lt;int&gt;(detections.ptr&lt;float&gt;(0)[i * 7 + 4] * frame.rows);
                        int xRightTop = static_cast&lt;int&gt;(detections.ptr&lt;float&gt;(0)[i * 7 + 5] * frame.cols);
                        int yRightTop = static_cast&lt;int&gt;(detections.ptr&lt;float&gt;(0)[i * 7 + 6] * frame.rows);

                        rectangle(frame, Point(xLeftBottom, yLeftBottom), Point(xRightTop, yRightTop), Scalar(0, 255, 0), 2);
                    }
                }

                imshow(&quot;SSD Detection&quot;, frame);
                if (waitKey(1) == 27) break; // Press 'ESC' to exit
            }
        }

        camera.Close();
    }
    catch (const GenericException&amp; e) {
        std::cerr &lt;&lt; &quot;Error: &quot; &lt;&lt; e.GetDescription() &lt;&lt; std::endl;
    }

    PylonTerminate();
    return 0;
}
</code></pre>
<p>I am new to Computer Vision and I am aiming for a high performance(100+ fps) real-time object detection model to work with low res footage like 640x480 etc. Is OpenCv good starting point or should I skip C++ and use Python for better support etc. Is using C++ advantageous over Python when it comes to Object Detection model performance?</p>
","2","Question"
"79276804","","<p>Thanks for your help in advance! I'm new to tidymodels (and modeling in general) and am having a hard time identifying what's going wrong to troubleshoot my workflow set up.</p>
<p>I'm running four different models to predict baseball win percentages based on a historical dataset. They are a linear model, elastic net model, random forest model, and XGBoost model. I know all the models work (I have tested them individually), but I am trying to use a workflow to test, cross-validate, and select the best models.</p>
<p>I have two different types of recipes, a basic recipe that includes some hyperparameterization tuning steps (selecting variables, step_zv, step_nzv, step_interact, step_corr, and step_impute_bag) for the random forest and XGBoost models. The linear and elastic net models use a recipe that adds a normalization step.</p>
<p>After setting up my workflows and grids, when I try to run workflow_map(), I get two errors:</p>
<ol>
<li>&quot;Error in summary.connection(connection) : invalid connection&quot;</li>
<li>&quot;2 arguments have been tagged for tuning in these components: model_spec. Please use one of the tuning functions (e.g. 'tune_grid()') to optimize them&quot;</li>
</ol>
<p>My questions:</p>
<ol>
<li>What does the first error indicate?</li>
<li>As for the second, where should I be adding/incorporating tune_grid() into the workflow?</li>
</ol>
<p>--</p>
<p>For reference, here is some of the relevant code:</p>
<h3>Some initial set up</h3>
<pre><code># Split data
team_split &lt;- initial_split(mlb_final)

# Extract training and testing data
team_train &lt;- training(team_split)
team_test &lt;- testing(team_split)

# Resampling strategy
team_rs &lt;- vfold_cv(team_train)
</code></pre>
<h3>Model specification</h3>
<pre><code># Random forest model 
mlb_forest &lt;- rand_forest(min_n = tune()) %&gt;% 
  set_engine(&quot;ranger&quot;,
             importance = &quot;permutation&quot;) %&gt;% 
  set_mode(&quot;regression&quot;)

# Linear model
mlb_linear &lt;- linear_reg() %&gt;%
  set_engine(&quot;lm&quot;) %&gt;%
  set_mode(&quot;regression&quot;)

# XGBoost
mlb_xgb &lt;- boost_tree(
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune()
) %&gt;%
  set_engine(&quot;xgboost&quot;) %&gt;%
  set_mode(&quot;regression&quot;)

# Elastic Net
mlb_elastic &lt;- linear_reg(
  penalty = tune(),    
  mixture = tune()     
) %&gt;%
  set_engine(&quot;glmnet&quot;) %&gt;%
  set_mode(&quot;regression&quot;)
</code></pre>
<p>I've set up my workflows like this:</p>
<pre><code>linear_workflow &lt;- workflow() |&gt; 
  add_model(mlb_linear) |&gt; 
  add_recipe(normalized_recipe)
  
elastic_workflow &lt;- workflow() |&gt; 
  add_model(mlb_elastic) |&gt; 
  add_recipe(normalized_recipe)
  
rf_workflow &lt;- workflow() |&gt; 
  add_model(mlb_forest) |&gt; 
  add_recipe(basic_recipe)

xgb_workflow &lt;- workflow() |&gt; 
  add_model(mlb_xgb) |&gt; 
  add_recipe(basic_recipe)
</code></pre>
<p>And my grids like this:</p>
<pre><code>grid_ctrl &lt;- control_grid(
  save_pred = TRUE,
  parallel_over = NULL,
  save_workflow = TRUE,
  verbose = TRUE
)

rf_grid &lt;- grid_regular(
  min_n(range = c(5, 50)),  # Min number of observations per leaf (tuning parameter)
  mtry(range = c(2, 10)),   # Number of variables to randomly sample at each split
  levels = 5                # Levels of grid search
)

xgb_grid &lt;- grid_regular(
  trees(range = c(100, 500)),      
  min_n(range = c(5, 15)),        
  tree_depth(range = c(3, 6)),    
  learn_rate(range = c(0.05, 0.1)), 
  levels = 5
)

elastic_grid &lt;- grid_regular(
  penalty(range = c(-2, 1), trans = log10_trans()),  
  mixture(range = c(0, 1)),                          
  levels = 5
)

linear_grid &lt;- 5
</code></pre>
<p>I then combined into normalized and basic workflow sets.</p>
<pre><code>normalized_mlb &lt;- workflow_set(
  preproc = list(normalized = normalized_recipe), 
  models = list(linear = mlb_linear, 
                elastic = mlb_elastic)
  )

basic_mlb &lt;- workflow_set(
  preproc = list(basic = basic_recipe),
  models = list(rf = mlb_forest, 
                xgb = mlb_xgb)
)
</code></pre>
<p>And then tried to use workflow_map() for both normalized and basic workflows</p>
<pre><code>lm_models &lt;- normalized_mlb |&gt; 
  workflow_map(&quot;fit_resamples&quot;,
               seed = 100,
               verbose = TRUE,
               resamples = team_rs, 
               control = grid_ctrl)

basic_models &lt;- basic_mlb  |&gt; 
  workflow_map(&quot;fit_resamples&quot;,
               seed = 100,
               verbose = TRUE,
               resamples = team_rs, 
               control = grid_ctrl)
</code></pre>
<p>The workflows are split into normalized and basic workflows because, initially, I was trying to run them together and running into issues. However, I'm still not sure how to address these errors.</p>
","1","Question"
"79278625","","<p>I have a dataset of the performance of students in exams which looks like:</p>
<pre><code>Class_ID   Class_size   Student_Number   IQ   Hours_Studied   Score
1          3            3                101  10              98
1          3            4                99   19              80
1          3            6                130  3               95
2          4            4                93   5               50
2          4            5                103  9               88
2          4            8                112  12              99
2          4            1                200  10              100 
</code></pre>
<p>and I would like to build a machine learning model trying to predict who is going to be top of the class (i.e. highest <code>Score</code>) for any given <code>Class_ID</code> using the <code>IQ</code> and <code>Hours_Studied</code> as features.</p>
<p>Since this is a ranking problem, a natural class of learning models is to use the <code>XGBRanker</code> in <code>XGBoost</code> or <code>LGBMRanker</code> in <code>lightgbm</code>.</p>
<p>and here is my code using xgboost:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df['Class_ID'])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin(['Class_ID','Student_Number','Score'])]
y_train = train_data.loc[:, train_data.columns.isin(['Score'])]

groups = train_data.groupby('Class_ID').size().to_frame('Class_size')['Class_size'].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin(['Student_Number','Score'])]
y_test = test_data.loc[:, test_data.columns.isin(['Score'])]

model = xgb.XGBRanker(  
    tree_method='hist',
    device='cuda',
    booster='gbtree',
    objective='rank:pairwise',
    enable_categorical=True,
    random_state=42, 
    learning_rate=0.1,
    colsample_bytree=0.9, 
    eta=0.05, 
    max_depth=6, 
    n_estimators=175, 
    subsample=0.75 
    )

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
  return model.predict(df.loc[:, ~df.columns.isin(['Class_ID','Student_Number'])])
  
predictions = (X_test.groupby('Class_ID')
                     .apply(lambda x: predict(model, x)))
</code></pre>
<p>The code works fine with reasonable predictive power. However, the output is a list of &quot;relevance score&quot; as opposed to a list of probabilities. But it seems both <code>XGBRanker</code> and <code>LGBMRanker</code> do not have the attribute <code>predict_proba</code> that return the probability of getting the highest score in the class.</p>
<p>So my question is, is there any way to convert the <code>relevance score</code> into probabilities or are there any other natural classes of ranking models that deal with these kind of problems?</p>
<p><strong>Edit</strong> in this problem, I only care about the person who ends up the top of the class (or maybe the top 3) so the ranking isn't all that important (for example knowing student 4 ranks 11th and student 8 ranks 12 does not matter all that much), so I suppose one way is to use classification instead of ranking in xgboost. But I wonder is there any other way.</p>
","3","Question"
"79279124","","<p>I am playing with CNNs these days, and I have code like pasted below. My question is, would this work on any image size? It is not clear to me what parameter or channel, if any, cares about the image size? And if that's the case, how does the model know how many neurons it needs, isn't that a function of image size?</p>
<p>Related point on pretrained models - if I use pretrained models, do I need to reformat my images to be same as what the model was trained on in the first place, or how does that work?</p>
<pre class=""lang-py prettyprint-override""><code>class CNN(nn.Module):
    def __init__(self, num_classes, num_channels=1):
        super(CNN, self).__init__()
        self.num_classes = num_classes
        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc = nn.Linear(64*7*7, num_classes)
</code></pre>
","1","Question"
"79280552","","<p>I have a model with an input (batch of images w/ shape (height, width, time)) that has a dynamically sized dimension (time), which is only determined at runtime. However, the <code>Dense</code> layer requires fully defined spatial dimensions. Code snippet example:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input

# Define an input with an undefined dimension (None)
input_tensor = Input(shape=(None, 256, 256, None, 13))

# Apply a Dense layer (which expects a fully defined shape)
x = Flatten()(input_tensor)
x = Dense(10)(x)

# Build the model
model = tf.keras.models.Model(inputs=input_tensor, outputs=x)

model.summary()
</code></pre>
<p>This raises the error:</p>
<pre><code>ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None.
</code></pre>
<p>How can I make it work using <code>Flatten</code> instead of alternatives like <code>GlobalAveragePooling3D</code>? Essentially, I’m looking for a way to create a 1D array with the original pixel values, but compatible with the <code>Dense</code> layer.</p>
","0","Question"
"79281350","","<p>I am doing a Machine Learning project to predict the prices of electric cars on Jupyter Notebook.</p>
<p>I run these cells:</p>
<pre><code>from sklearn import preprocessing
le = preprocessing.LabelEncoder()
cols = ['County', 'City', 'State', 'ZIP Code', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']
for col in cols:
    le.fit(t[col])
    x[col] = le.transform(x[col]) 
    print(le.classes_)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state = 0)

r2_score(y_test, lm.predict(x_test))


from sklearn.tree import DecisionTreeRegressor 
regressor = DecisionTreeRegressor(random_state = 0) 
regressor.fit(x_train, y_train)
r2_score(y_test, regressor.predict(x_test))


r2_score(y_train, regressor.predict(x_train))

uv = np.nanpercentile(df2['Base MSRP'], [99])[0]*2


df2['Base MSRP'][(df2['Base MSRP']&gt;uv)] = uv


df2 = df2[df2['Model Year'] != 'N/']  # Filter out rows where 'Model Year' is 'N/'

for col in cols:
    df2[col] = df2[col].replace('N/', -1)
    le.fit(df2[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)

le = preprocessing.LabelEncoder()

cols = ['County', 'City', 'State', 'ZIP Code', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']

for col in cols:
    le.fit(t[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)
</code></pre>
<p>I get this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_16424\1094749331.py in &lt;module&gt;
      1 for col in cols:
      2     le.fit(t[col])
----&gt; 3     df2[col] = le.transform(df2[col])
      4     print(le.classes_)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
    136             return np.array([])
    137 
--&gt; 138         return _encode(y, uniques=self.classes_)
    139 
    140     def inverse_transform(self, y):

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _encode(values, uniques, check_unknown)
    185     else:
    186         if check_unknown:
--&gt; 187             diff = _check_unknown(values, uniques)
    188             if diff:
    189                 raise ValueError(f&quot;y contains previously unseen labels: {str(diff)}&quot;)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\_encode.py in _check_unknown(values, known_values, return_mask)
    259 
    260         # check for nans in the known_values
--&gt; 261         if np.isnan(known_values).any():
    262             diff_is_nan = np.isnan(diff)
    263             if diff_is_nan.any():

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>
<p><strong>What did I try?</strong></p>
<p>I tried using the following code:</p>
<pre><code>le = preprocessing.LabelEncoder()
cols = ['County', 'City', 'State', 'ZIP Code', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']
for col in cols:
    le.fit(t[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)
</code></pre>
<p>The code gives me the specific error.</p>
<p>To fix the issue, I tried imputing the missing value (&quot;N/&quot;) instead of removing it by using this code:</p>
<pre><code>for col in cols:
  le.fit(t[col].fillna('Missing'))  # Impute missing values with 'Missing'
  df2[col] = le.transform(df2[col].fillna('Missing'))
  print(le.classes_)
</code></pre>
<p>But still, I get the same error.</p>
<p>Here is the link to my notebook: <a href=""https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb"" rel=""nofollow noreferrer"">https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb</a></p>
<p>Here is the link to the dataset:
<a href=""https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/rithurajnambiar/electric-vehicle-data</a></p>
<p>How to fix this issue?</p>
","1","Question"
"79281636","","<p>I’m using a Pipeline in scikit-learn to combine feature scaling with a classifier. This works well for logistic regression, but I’m curious if this approach would generalize effectively to more complex models like tree-based ensembles or neural networks. Specifically, do these models require different scaling strategies, or can I apply StandardScaler consistently across them?</p>
<pre><code>import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Generate sample data
np.random.seed(42)
X = np.random.rand(200, 5)  # 200 samples, 5 features
y = np.random.randint(0, 2, 200)  # Binary target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define pipelines for different models
pipelines = {
    'logistic_regression': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression())
    ]),
    'random_forest': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RandomForestClassifier())
    ]),
    'neural_network': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', MLPClassifier(max_iter=500))
    ])
}

# Evaluate each model
for model_name, pipeline in pipelines.items():
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    print(f&quot;{model_name} Accuracy: {accuracy_score(y_test, y_pred)}&quot;)
</code></pre>
","0","Question"
"79283938","","<p>I encountered an issue while setting up Vertex AI Model Monitoring using the aiplatform SDK, specifically when configuring BatchPredictionJob.create(). The documentation was unclear and lacked examples for defining model_monitoring_objective_config and model_monitoring_alert_config. This caused compatibility issues due to incomplete references and unclear parameter mappings.</p>
<p>The main confusion arose because these configurations are not well-documented in the SDK and required exploring the SDK source code to understand the correct structure. Additionally, the SDK requires configurations from aiplatform.model_monitoring, which isn’t explicitly clear in the official guides.</p>
<p><strong>What I Tried:</strong>
I initially referred to the SDK documentation and attempted various configurations based on what seemed logical, assuming all components could be configured directly using the SDK’s classes. However, this caused multiple type and parameter mismatch errors.</p>
<p><strong>What I Expected:</strong>
I expected clear and straightforward documentation showing how to define and pass monitoring configurations when creating a batch prediction job using aiplatform.BatchPredictionJob.create().</p>
<p><strong>What Actually Happened:</strong>
I encountered errors due to parameter mismatches between the SDK and GAPIC APIs. After exploring the source code, I realized that the required configurations must be defined using aiplatform.model_monitoring classes. This cross-library dependency was not documented.</p>
","0","Question"
"79287799","","<p>I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this.</p>
<p>Here is the code I am using:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

def getPayeeName(description):
    description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()
    doc = nlp(description)

    for token in doc:
        print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}&quot;)

# Example input
description = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;
getPayeeName(description)
</code></pre>
<p>Token: UPI, Entity: ORG</p>
<p>Token: DR, Entity: ORG</p>
<p>Token: 400874707203, Entity: None</p>
<p>Token: BENGALORE, Entity: None</p>
<p>Token: 08, Entity: DATE</p>
<p>Token: JAN, Entity: DATE</p>
<p>Token: 2024, Entity: DATE</p>
<p>Token: 14:38:56, Entity: None</p>
<p>Token: MEDICAL, Entity: ORG</p>
<p>Token: LTD, Entity: ORG</p>
<p>Token: HDFC, Entity: ORG</p>
<p>Token: 50200, Entity: ORG</p>
<ul>
<li><p>50200 is identified as ORG, but it is just a number.</p>
</li>
<li><p>BENGALORE is a city, but it is not recognized as a GPE or location
(returns None).</p>
</li>
<li><p>UPI and DR are acronyms/abbreviations, but they are incorrectly
identified as ORG.</p>
</li>
</ul>
<p>I want the entity recognition to be more accurate and reliable.
How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition?</p>
<p>Note: I tried ChatGPT as well, but still this issue is not solved.</p>
","2","Question"
"79288128","","<p>I've been trying to deploy a very simple toy Keras model to Cloud Functions, which would predict the class of an image, but for reasons unknown, when the execution gets to the <code>predict</code> method, it gets stuck, does not throw any error, and eventually times out.</p>
<pre><code>import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

def preprocess_image(image_file):
    img = Image.open(io.BytesIO(image_file.read()))
    img = img.resize((32, 32))
    img = np.array(img)
    img = img / 255.0
    img = img.reshape(1, 32, 32, 3)
    return img

@functions_framework.http
def predict(request):
    image = preprocess_image(request.files['image_file'])
    print(image.shape) # this prints OK
    prediction = model.predict(image)
    print(prediction) # this never prints
    predicted_class = class_names[np.argmax(prediction)]
    return f&quot;Predicted class: {predicted_class}&quot;
</code></pre>
<p>Debugging locally works fine, the prediction is fast as expected (the model weights file is 2MB). I also added several prints along the way (removed from the snippet above) and the execution works fine until the <code>predict</code> method.</p>
<p>Even though the minimal compute configuration should work, I tried reserving more memory and CPU, but nothing worked. The model is hosted at Storage, I tried downloading it first, but that didn't work either. I did also try making the prediction inside a <code>tf.device('/cpu:0')</code> context, passing a <code>step=1</code> parameter and converting the image array to a Keras Dataset first, as suggested by ChatGPT, with the same results. Actually, nothing prints as a result of invoking <code>predict</code> at all. Calling <code>call</code> instead of <code>predict</code> got me nowhere.</p>
<p>What am I missing?</p>
","1","Question"
"79290968","","<p>I am encountering an AttributeError while fitting an XGBRegressor using RandomizedSearchCV from Scikit-learn. The error message states:</p>
<pre><code>'super' object has no attribute '__sklearn_tags__'.
</code></pre>
<p>This occurs when I invoke the <code>fit</code> method on the RandomizedSearchCV object. I suspect it could be related to compatibility issues between Scikit-learn and XGBoost or Python version. I am using Python 3.12, and both Scikit-learn and XGBoost are installed with their latest versions.</p>
<p>I attempted to tune the hyperparameters of an XGBRegressor using RandomizedSearchCV from Scikit-learn. I expected the model to fit the training data without issues and provide the best parameters after cross-validation.</p>
<p>I also checked for compatibility issues, ensured the libraries were up-to-date, and reinstalled Scikit-learn and XGBoost, but the error persists.</p>
","29","Question"
"79290974","","<p>I am trying to change the functionality of a random forest classifier. While usually features are selected at random for each split, I want one specific feature to be evaluated at each split. I know this can impact performance but i want to try out whether this is a good idea in a very specific use case. So the result of the adaptation shall be: features used for splitting are selected at random (as usual) but one specific feature (say index 15) is always considered (not necessarily used).</p>
<p>I don't know any packages that allow that out of the box. It's there one, or maybe a simple workaround to achieve the same effect?</p>
","2","Question"
"79291357","","<p>I have a YOLO V5 m model for a dataset to detect a certain product but after training a dataset of a variety of pictures for the product in different cases and scenarios and various lighting conditions with no repetition for the cases, on 50 epochs and the Yolo v5 medium architecture and batch size of 16.</p>
<p>What I have noticed is that the model is dependent on the color features, meaning that the objects that are getting detected have colors yellow and its shades. I used the train.py that comes with the yolo github clone. What should I do to fix this problem?</p>
<p><a href=""https://https://drive.google.com/file/d/1oBH28fbyhEmabLLx2bPXpEMQ0s2zhg__/view?usp=sharing"" rel=""nofollow noreferrer"">The dataset</a> used in training (approx 450 images) in google drive.</p>
<p>The product desired to be detected:</p>
<p><a href=""https://i.sstatic.net/HlnxxdfO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HlnxxdfO.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/mlEtAmDs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mlEtAmDs.png"" alt=""enter image description here"" /></a></p>
<p>The other products that are detected:</p>
<p><a href=""https://i.sstatic.net/M7WR06pB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M7WR06pB.jpg"" alt=""enter image description here"" /></a></p>
","0","Question"
"79293139","","<p>We are trying to fine-tune a custom model on an imported DeiT distilled patch16 384 pretrained model.</p>
<p>Output:</p>
<pre><code>Cost at epoch 0 is 4.611058227040551
Cost at epoch 1 is 0.9889081553979353
test set accuracy
Checking accuracy
scores: tensor([[ 33.9686,  33.2787, -31.1509,  ..., -25.5279, -36.7728, -24.9331],
        [ 33.9695,  33.2792, -31.1509,  ..., -25.5264, -36.7719, -24.9356],
        [ 33.9690,  33.2784, -31.1496,  ..., -25.5270, -36.7717, -24.9326],
        ...,
        [ 33.9692,  33.2780, -31.1487,  ..., -25.5267, -36.7713, -24.9314],
        [ 33.9654,  33.2793, -31.1575,  ..., -25.5372, -36.7818, -24.9307],
        [ 33.9687,  33.2778, -31.1490,  ..., -25.5278, -36.7719, -24.9300]],
       device='cuda:0')
predictions: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
y: tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0') 

###  many more prints later
predictions: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')
y: tensor([0, 1, 1, 0, 1, 1], device='cuda:0')
Got 80 / 198 with accuracy 40.40
Precision: 0.1632
Recall: 0.4040
F1-Score: 0.2325
</code></pre>
<p>The folder is structured as KneeOsteoarthritisXray with subfolders train, test, and val (ignoring val because we just want it to work) and each of those have subfolders 0 and 1 (0 is healthy, 1 has osteoarthritis)
The model predicts only 0's and returns an accuracy equal to the amount of 0's in the dataset</p>
<pre><code>from sklearn.metrics import precision_score, recall_score, f1_score
import os
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
#import torchvision.transforms as transforms
from torchvision import transforms
import torchvision
from transformers import DeiTForImageClassificationWithTeacher, DeiTImageProcessor

transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
])

class myDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.data = []

        for label in os.listdir(root_dir):
            label_dir = os.path.join(root_dir, label)
            if os.path.isdir(label_dir):
                for file in os.listdir(label_dir):
                    self.data.append((os.path.join(label_dir, file), int(label)))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path, label = self.data[idx]
        image = Image.open(img_path)
        #print(f'image before normalization: {image}') #DEBUG
        image = transform(image)
        #print(f'image after normalization to 0-1{image}') #DEBUG
        image_np = np.array(image) 
        return image, label

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

learning_rate = 0.01
batch_size = 32
num_epochs = 32

model_path = &quot;/content/drive/MyDrive/datasets/PyTorchdeit-base-distilled-patch16-384/&quot;
model = DeiTForImageClassificationWithTeacher.from_pretrained(model_path)
model.to(device)

train_dataset = myDataset(root_dir=&quot;/content/drive/MyDrive/datasets/KneeOsteoarthritisXray/train&quot;)
#val_dataset = myDataset(root_dir=&quot;/content/drive/MyDrive/datasets/KneeOsteoarthritisXray/val&quot;)
test_dataset = myDataset(root_dir=&quot;/content/drive/MyDrive/datasets/KneeOsteoarthritisXray/test&quot;)

train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
#val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)


criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    losses = []

    for batch_idx, (data, targets) in enumerate(train_loader):
        data = data.to(device=device)
        targets = targets.to(device=device)

        scores = model(data)['logits']
        loss = criterion(scores, targets)
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()

        optimizer.step()

    print(f'Cost at epoch {epoch} is {(sum(losses)/len(losses))}')
  

def check_accuracy(loader, model):
    print(&quot;Checking accuracy&quot;)
    num_correct = 0
    num_samples = 0
    all_labels = []
    all_preds = []

    model.eval()

    with torch.no_grad():
      for x, y in loader:
          x = x.to(device=device)
          y = y.to(device=device)

          scores = model(x)['logits']
          print(f'scores: {scores}')
          _, predictions = scores.max(1)
          print(f'predictions: {predictions}')
          print(f'y: {y}')

          all_labels.extend(y.cpu().numpy())
          all_preds.extend(predictions.cpu().numpy())

          num_correct += (predictions == y).sum() #.item()
          num_samples += predictions.size(0)

    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')

    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')

    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1-Score: {f1:.4f}')

    model.train()

print('test set accuracy')
check_accuracy(test_loader, model)
</code></pre>
<p>We don't think it's overfitting because we tried with unbalanced and balanced versions of the dataset, we tried overfitting a small dataset, and many other attempts.</p>
<p>We checked out many many similar complaints and can't really get anything out of their code or solutions</p>
","0","Question"
"79295972","","<p>is it possible to use apple ML framework to dynamically learn from users behaviour in the app? I've trained a model using <code>Create ML</code> application, can I then update and retrain from the iOS device? This is how I'm currently using said model.</p>
<pre><code>public func calculateMuscleRecoveryTime(_ workout: Workout) {
    do {
        
        let config = MLModelConfiguration()
        let model = try MuscleRecoveryModel(configuration: config)
        
        let allMuscleGroups = workout.exercises
            .compactMap { $0.muscles } // Flatten the muscles array from each exercise
            .reduce(Set&lt;MuscleGroup&gt;()) { $0.union($1) } // Union to remove duplicates

        let uniqueMuscleGroups = Array(allMuscleGroups)
        
        for muscleGroup in uniqueMuscleGroups {
            let trainingIntensity = Int64(workout.intensity.intValue)
            let lastTrainedTimestamp = workout.date
            let timeAgo = timeAgoInSeconds(from: lastTrainedTimestamp)
            let muscleName = muscleGroup.rawValue.lowercased()
            
            let prediction = try model.prediction(muscle: muscleName, intensity: trainingIntensity, lastTrained: timeAgo)
        }
    } catch let error {
        print(&quot;Error: &quot;, error)
    }
}
</code></pre>
","-1","Question"
"79297188","","<p>I created an autoencoder using python, with no errors. However, I do not know the code for how do display the generated images from the autoencoder. The code of the autoencoder is  shown below:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras import layers, models, datasets, callbacks
import tensorflow.keras.backend as K

#NOT IN THE TEXT FOR SOME odd REASON
from keras.models import Model


#IMPORT THE DATA INTO TRAIN AND TEST SETS 
from tensorflow.keras import datasets
(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()

#scale the images
def preprocess(imgs):
   imgs = imgs.astype(&quot;float32&quot;) / 255.0
   imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)
   imgs = np.expand_dims(imgs, -1)
   return imgs
x_train = preprocess(x_train)
x_test = preprocess(x_test)

#THE ENCODER
encoder_input = layers.Input(
shape=(32, 32, 1), name = &quot;encoder_input&quot;
)
x = layers.Conv2D(32, (3, 3), strides = 2, activation = 'relu', padding=&quot;same&quot;)(
encoder_input
)
x = layers.Conv2D(64, (3, 3), strides = 2, activation = 'relu', padding=&quot;same&quot;)(x)
x = layers.Conv2D(128, (3, 3), strides = 2, activation = 'relu', padding=&quot;same&quot;)(x)
shape_before_flattening = K.int_shape(x)[1:]
x = layers.Flatten()(x)
encoder_output = layers.Dense(2, name=&quot;encoder_output&quot;)(x)
encoder = models.Model(encoder_input, encoder_output)

#THE DECODER
decoder_input = layers.Input(shape=(2,), name=&quot;decoder_input&quot;)
x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)
x = layers.Reshape(shape_before_flattening)(x)
x = layers.Conv2DTranspose(
128, (3, 3), strides=2, activation = 'relu', padding=&quot;same&quot;
)(x)
x = layers.Conv2DTranspose(
64, (3, 3), strides=2, activation = 'relu', padding=&quot;same&quot;
)(x)
x = layers.Conv2DTranspose(
32, (3, 3), strides=2, activation = 'relu', padding=&quot;same&quot;
)(x)
decoder_output = layers.Conv2D(
1,
(3, 3),
strides = 1,
activation=&quot;sigmoid&quot;,
padding=&quot;same&quot;,
name=&quot;decoder_output&quot;
)(x)
decoder = models.Model(decoder_input, decoder_output)

# JOIN THE ENCODER WITH THE DECODER
autoencoder = Model(encoder_input, decoder(encoder_output))

# Compile the autoencoder
autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;)
# Train the autoencoder by passing in the input images as both the input and output
# with one epoch

autoencoder.fit(
   x_train,
   x_train,
   epochs=1,
   batch_size=100,
   shuffle=True,
   validation_data=(x_test, x_test),
)

#Reconstruct the images
example_images = x_test[:50]
predictions = autoencoder.predict(example_images)
</code></pre>
<p>I tried using <code>plt.imshow</code> as shown below. I expected to see 10 images generated by the autoencoder.  however it is not working. I'm really not sure how to use it:</p>
<pre><code>for i in range(10):
    plt.figure(figsize=(20,3))
    plt.imshow(predictions[i].astype(&quot;float32&quot;), cmap=&quot;gray_r&quot;)
    plt.show()
</code></pre>
","-1","Question"
"79299410","","<p>I am trying to implement Ridge regression but I feel like I am missing something with the Python operators. Here is my code:</p>
<pre><code>import numpy as np


x = np.random.rand(10, 2)  
y = np.random.rand(10, 1)  
lambda_reg = 0.1  
alpha = 0.1  
num_iterations = 100000  


X_train = np.hstack((np.ones((x.shape[0], 1)), x))

def ridge_regression_gradient_descent(X, y, lambda_reg, alpha, num_iterations):
n, p = X.shape  
B = np.zeros(p)  
# Gradient descent loop
for _ in range(num_iterations):
    y_pred = X.dot(B).reshape(-1, 1)  
    gradient_B0 = - (1/n) * np.sum(y - y_pred)  
    gradient_B = - (1/n) * (X[:, 1:].T @ (y - y_pred)) +  
    lambda_reg * B[1:].reshape(-1, 1)  # Gradients for B1 to Bp   

    B[0] -= alpha * gradient_B0  
    B[1:] -= alpha * gradient_B.reshape(-1)  
    return B

B = ridge_regression_gradient_descent(X_train, y, lambda_reg, alpha, num_iterations)
print(B)
</code></pre>
<p>Is anyone able to see what I am doing wrong?</p>
<p>I tried multiple changes in the code on the matrix multiplications + also reshaping everything in the right format. I get actually 3 Beta's so this is ok but I don't get anything close to what I get with the formula: <code>B = (X.T X + lambda * I)^-1 * X.T Y</code></p>
","-1","Question"
"79300055","","<p>I am trying to do transfer learning on Pytorch pretrained models with custom dataset. I have been able to successfully perform transfer learning with SqueezeNet.</p>
<p>For Squeezenet my classifier was, <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py#L81-L83"" rel=""nofollow noreferrer"">layers source</a></p>
<pre><code>model.classifier = nn.Sequential(
    nn.Dropout(p=0.2),
    nn.Conv2d(512, len(class_names), kernel_size=1),
    nn.ReLU(inplace=True),
    nn.AdaptiveAvgPool2d((1, 1)))
</code></pre>
<p>For Efficientnet my classifier was, <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/efficientnet.py#L314-L316"" rel=""nofollow noreferrer"">layers source</a></p>
<pre><code>model.classifier = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    torch.nn.Linear(in_features=1280,
                    out_features=output_shape,
                    bias=True))
</code></pre>
<p>Similarly I have been trying to do for MaxViT, I went through the source and saw that there are <code>block_channels[-1]</code> in parameter. I have recently started with this, and I don't know what they are, <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L696-L702"" rel=""nofollow noreferrer"">layers source</a></p>
<pre><code>self.classifier = nn.Sequential(
    nn.AdaptiveAvgPool2d(1),
    nn.Flatten(),
    nn.LayerNorm(block_channels[-1]),
    nn.Linear(block_channels[-1], block_channels[-1]),
    nn.Tanh(),
    nn.Linear(block_channels[-1], num_classes, bias=False),
)
</code></pre>
<p>For reference, if needed, following is my complete code for performing transfer learning using squeezenet.</p>
<pre><code>weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=d1,
                                                                               test_dir=d2,
                                                                               transform=auto_transforms,
                                                                               batch_size=32)
for param in model.features.parameters():
    param.requires_grad = False

torch.manual_seed(42)
torch.cuda.manual_seed(42)
output_shape = len(class_names)

model.classifier = nn.Sequential(
    nn.Dropout(p=0.2),
    nn.Conv2d(512, len(class_names), kernel_size=1),
    nn.ReLU(inplace=True),
    nn.AdaptiveAvgPool2d((1, 1))).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
results = engine.train(model=model,
                       train_dataloader=train_dataloader,
                       test_dataloader=test_dataloader,
                       optimizer=optimizer,
                       loss_fn=loss_fn,
                       epochs=15,
                       device=device)
</code></pre>
<p>What should my classifier be for MaxViT?</p>
","0","Question"
"79305588","","<p>I want to create an .mlpackage or .mlmodel file which I can import in Xcode to do image segmentation. For this, I want to use the segmentation package within YOLO to check out if it fit my needs.</p>
<p>The problem now is that this script creates an .mlpackage file which only accepts images with a fixed size (640x640):</p>
<pre><code>from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)
</code></pre>
<p>I want the change something here, probably with <code>coremltools</code>, to handle unbounded ranges (I want to handle arbitrary sized images). It's described a bit here: <a href=""https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges"" rel=""nofollow noreferrer"">https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges</a>, but I don't understand how I can implement it with my script.</p>
","2","Question"
"79306951","","<p>I'm making a project that would transform braille to text. I have written the code for identifying the braille dots from the image but I cant figure out how to segment the braille into cells.</p>
<p>This part is identifying the blobs in the image (smaller low quality images don't work right now)</p>
<pre><code>import cv2
import numpy as np
from sklearn.cluster import KMeans

# Load the image
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# Set up SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# Filter by area (size of the blob)
params.filterByArea = True
params.minArea = 100  # Adjust based on dot size
params.maxArea = 1000

# Filter by circularity
params.filterByCircularity = True
params.minCircularity = 0.9  # Adjust for shape of the dots

# Filter by convexity
params.filterByConvexity = False
params.minConvexity = 0.7

# Filter by inertia (roundness)
params.filterByInertia = True
params.minInertiaRatio = 0.95

# Create a detector with the parameters
detector = cv2.SimpleBlobDetector_create(params)

# Detect blobs
keypoints = detector.detect(image)

# Draw detected blobs as red circles
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
                                 (0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;output image&quot;)
cv2.imshow(&quot;outputimage&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;Number of blobs detected: {len(keypoints)}&quot;)
</code></pre>
<p>The code below puts the coordinates of the blob on a graph (thought it might be easier to work with this way)</p>
<pre><code>#convert image into graph

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints])  #coords of blob
rounded_coords = np.round(blob_coords).astype(int)  #rounded coords

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# PROXIMITY BASED GROUPING
# IF X DISTANCE IS LESS THAN MIN DISTANCE
# IF Y DISTANCE IS LESS THAN MIN DISTANCE
# STORE X AND Y COORDINATES

# Calculate smallest x and y differences (trying for proximity based)
minx = 10000
miny = 10000
for i in x_coords:
    for j in x_coords:
        if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)):  # Threshold for cell width
            minx = abs(i - j)

for i in y_coords:
    for j in y_coords:
        if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)):  # Threshold for cell height
            miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# Plotting
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;)  # Plot the blobs
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()
</code></pre>
<p>Tried to separate them via proximity (the cells that are in close proximity of each other get grouped), but I couldn't figure out the logic for it. I also tried group clustering (Kmeans) but it isn't very accurate and it wouldn't work for images with different number of characters because it constantly needs to know how many clusters are to be formed.</p>
<pre><code># trying out kmeans clustering method
# kmeans dont work (can't figure out number of clusters from image)
# could work if nclusters can be figured out

import math
from sklearn.cluster import KMeans

blob_coords = np.array([kp.pt for kp in keypoints])  # Extract (x, y) positions of blobs
rounded_coords = np.round(blob_coords).astype(int)  # Round coordinates for simplicity


x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;)  # Plot the blobs

ax.invert_yaxis()  # Invert Y-axis for image-like coordinates
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
</code></pre>
","2","Question"
"79308237","","<p>I’m doing a grid search optimization on model parameters and logging the loss to MLFlow using</p>
<pre><code>Mlflow.log_metric(f“{run_number}_Loss”, returns, iteration)
</code></pre>
<p>But for every new run, I get a different plot in the MLFlow UI.
Is there a way to log to the same plot multiple times, maybe different colors, and add a legend to be able to compare different runs easily?</p>
","2","Question"
"79309133","","<p>I'm beginner in image processing. I have two binary classes as subdirectory which total of 496 images and I have an issue with the last batch that has remainder of 13 images. So, instead of tf.dataset tensor (32, 300, 300, 3), there's (16, 300, 300, 3) in the last batch. Actually, I noticed:</p>
<ol>
<li>after shuffle it contains 13 batches</li>
<li>after batching it produces only 1 batch (I assume it is the remainder batch)</li>
<li>when drop_remainder the data is empty</li>
</ol>
<p>Why does it left with only 1 batch after shuffle?</p>
<pre><code>image_size = (300, 300)
batch_size = 32

train_dataset = image_dataset_from_directory(
    dataset_dir,
    image_size=(image_size[0], image_size[1]),
    batch_size=batch_size,
    label_mode=&quot;binary&quot;,
    validation_split=0.2,
    subset=&quot;training&quot;,
    seed=123,
)

train_dataset = train_dataset.shuffle(1000)
train_dataset = train_dataset.batch(
    batch_size=batch_size, drop_remainder=True
).prefetch(buffer_size=AUTOTUNE)

print(train_dataset.cardinality().numpy())
</code></pre>
","0","Question"
"79309306","","<p>I have to use chromadb and transformers together for a project but chromadb requires &lt;=0.20.3 version of tokenizers and transformers require &gt;=0.21 version of tokenizers, and the older versions of transformers compatible with chromadb need rust compilers, so that is not an option either.</p>
<p>I tried upgrading transformers, tokenizers and also downgrading the transformers but nothing working and for all of this I am using a virtual environment.</p>
","0","Question"
"79312660","","<p>I've programmed a linear regression model from scratch. I use the &quot;Sum of squared residuals&quot; as the loss function for gradient descent. For testing I use linear data (y=x)</p>
<p>When running the algorithm, intercept b barely changes. Thus the slope m is not calculated correctly.</p>
<pre><code>%matplotlib qt5 
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)

class LinearRegression():
    def __init__(self):
        self.X = None
        self.y = None
    
    def ssr(self, m, b):
        sum = 0
        for i in range(len(self.X)):
          sum += (self.y[i] - (m * self.X[i] + b) ) ** 2
        
        return sum

    def ssr_gradient(self, m, b):
        sum_m = 0
        sum_b = 0
        n = len(self.X)
        for i in range(n):
            error = self.y[i] - (m * self.X[i] + b)
            derivative_m = -(2/n) * self.X[i] * error  # Derivative w.r.t. m
            derivative_b = -(2/n) * error              # Derivative w.r.t. b
            sum_m += derivative_m
            sum_b += derivative_b

        return sum_m, sum_b
    
    def fit(self, X, y, m, b): # Gradient Descent
        self.X = X
        self.y = y

        M, B = np.meshgrid(np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1))
        SSR = np.zeros_like(M)
        for i in range(M.shape[0]):
            for j in range(M.shape[1]):
                SSR[i, j] = self.ssr(M[i, j], B[i, j])


        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        gd_model = fig.add_subplot(121, projection=&quot;3d&quot;, computed_zorder=False)
        lin_reg_model = axes[1] 

        current_pos = (m, b, self.ssr(m, b))
        learning_rate = 0.001
        min_step_size = 0.001
        max_steps = 1000
        current_steps = 0

        while(current_steps &lt; max_steps):
            M_derivative, B_derivative = self.ssr_gradient(current_pos[0], current_pos[1])
            M_step_size, B_step_size = M_derivative * learning_rate, B_derivative * learning_rate

            if abs(M_step_size) &lt; min_step_size or abs(B_step_size) &lt; min_step_size:
                break

            M_new, B_new = current_pos[0] - M_step_size, current_pos[1] - B_step_size
            
            current_pos = (M_new, B_new, self.ssr(M_new, B_new))

            print(f&quot;Parameters: m: {current_pos[0]}; b: {current_pos[1]}; SSR: {current_pos[2]}&quot;)

            current_steps += 1
            
            x = np.arange(0, 10, 1)
            y = current_pos[0] * x + current_pos[1]
            lin_reg_model.scatter(X_train, y_train, label=&quot;Train&quot;, s=75, c=&quot;#1f77b4&quot;)
            lin_reg_model.plot(x, y)
            
            gd_model.plot_surface(M, B, SSR, cmap=&quot;viridis&quot;, zorder=0)
            gd_model.scatter(current_pos[0], current_pos[1], current_pos[2], c=&quot;red&quot;, zorder=1)
            gd_model.set_xlabel(&quot;Slope m&quot;)
            gd_model.set_ylabel(&quot;Intercept b&quot;)
            gd_model.set_zlabel(&quot;Sum of squared residuals&quot;)

            plt.tight_layout()
            plt.pause(0.001)
            
            gd_model.clear()
            lin_reg_model.clear()
        
        self.m = current_pos[0]
        self.b = current_pos[1]

    def predict(self, X_test):
        return self.m * X_test + self.b

lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train, 1, 10)

</code></pre>
<p>This is the result for initial values m=1 and b=10:
Parameters: m: -0.45129949840919587; b: 9.50972664859535; SSR: 145.06534359577407
<a href=""https://i.sstatic.net/pBal5xkf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBal5xkf.png"" alt=""Plot"" /></a></p>
<p>Obviously this isn't optimal because my data is linear. So the optimal parameters should be m=1 and b=0
But I cannot find the problem in my code. The algorithm prints different results based on the initial values, but it should print the same result over and over again as long as there is exactly one minimum of the SSR function.</p>
<p>I tried using different learning rates but the problem persists.</p>
","0","Question"
"79313854","","<p>Working on Video analysis assignment where I need to capture how many times same vehicle was captured in given video.</p>
<p>So far using YOLO11 was able to identify the vehicles such cars, bike, bus &amp; truck. Accordingly drawing rects as vehicles appears in the video frame.</p>
<p>I could not make out, how to mark a vehicle with some identification code. so that, when same vehicle appears in the video frame i can increase count of that vehicle.</p>
<p>Adding my code which I tried</p>
<pre><code>from ultralytics import YOLO
import cv2
from enum import Enum

class DetectionType(Enum):
    CAR         = 2
    MOTORCYCLE  = 3
    BUS         = 5
    TRUCK       = 6


coco_model = YOLO('yolo11n.pt')
cap = cv2.VideoCapture('testVideo.mp4')

vehicles = [
            DetectionType.CAR.value, 
            DetectionType.MOTORCYCLE.value, 
            DetectionType.BUS.value,
            DetectionType.TRUCK.value
            ]

ret = True

while ret:
    ret, frame = cap.read()

    if ret:
        #detect vehicle
        detections_model = coco_model(frame)[0]

        for detection in detections_model.boxes.data.tolist():
            x1, y1, x2, y2, score, class_id = detection

            if int(class_id) in vehicles:
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

        
    # Display frames in a window 
    cv2.imshow('video', frame)
  

    if cv2.waitKey(33) == 27:
        break

cap.release()
cv2.destroyAllWindows()    
</code></pre>
<p>Any suggestion or snippet will help me to complete this assignment.</p>
","2","Question"
"79314811","","<p>In my efforts to understand machine learning better from a &quot;first principles&quot; perspective, I am implementing my own ML-related functions.  Currently I am trying to implement SoftMax:</p>
<pre><code>IEnumerable&lt;double&gt; SoftMax(IEnumerable&lt;double&gt; vector)
{
    var exps = vector.Select(v =&gt; Math.Exp(v));
    var sumExps = exps.Sum();
    return exps.Select(exp =&gt; exp / sumExps);
}
</code></pre>
<p>If I understand SoftMax correctly, if I sum the results of this function, i.e., <code>SoftMax(vector).Sum()</code>, the output should always be <code>1</code>.</p>
<p>In nearly all cases, however, this returns a value that's either slightly greater than or slightly less than <code>1</code>.  Generally, something like <code>1.0568102998178908</code> or <code>0.9758570985704772</code>.</p>
<p>I've heard that this is not uncommon (it's basically an overflow issue), and can be fixed by subtracting the largest element of the input from the value that's being fed into <code>Math.Exp()</code>.  So I came up with this implementation based on suggestions:</p>
<pre><code>IEnumerable&lt;double&gt; SoftMax(IEnumerable&lt;double&gt; vector)
{
    var maxVal = vector.Max();
    var exps = vector.Select(v =&gt; Math.Exp(v - maxVal));
    var sumExps = exps.Sum();
    return exps.Select(exp =&gt; exp / sumExps);
}
</code></pre>
<p>In testing my implementation, I am passing in a random collection of doubles, as follows:</p>
<pre><code>IEnumerable&lt;double&gt; vector = Enumerable.Range(0, 100).Select(n =&gt; new Random().NextDouble());
var softMaxVec = SoftMax(vector);
Console.WriteLine(softMaxVec.Sum());
</code></pre>
<p>But my improved implementation still gives me the same problem.  What am I doing wrong?  Or am I misunderstanding something about how SoftMax is supposed to work?</p>
","1","Question"
"79316482","","<p>I am going along the pytorch api <a href=""https://pytorch.org/cppdocs/installing.html"" rel=""nofollow noreferrer"">tutorial</a> and I get the following error</p>
<pre class=""lang-none prettyprint-override""><code>ld: unknown options: --no-as-needed --as-needed --no-as-needed --as-needed
c++: error: linker command failed with exit code 1 (use -v to see invocation)
</code></pre>
<p>I tried using <code>brew install pytorch</code> to link <em>torch</em> manually, using</p>
<pre class=""lang-none prettyprint-override""><code>g++ example-app.cpp -o out -I/opt/homebrew/Cellar/pytorch/2.5.1_3/libexec/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -std=c++17
</code></pre>
<p>But I get this error</p>
<pre class=""lang-none prettyprint-override""><code>/opt/homebrew/Cellar/pytorch/2.5.1_3/libexec/lib/python3.13/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3:10: fatal error: 'torch/csrc/autograd/autograd.h' file not found
</code></pre>
","-2","Question"
"79316648","","<p>I need to implement a U-Net in Julia using Flux.jl. The ultimate goal is to train a neural network for a scientific problem. As a first step, I decided to experiment with the KITTI benchmark dataset.</p>
<p>Since I have more experience with Python, I initially implemented the U-Net in PyTorch, which worked perfectly. Then, I tried to translate my code into Julia while learning Flux.jl. Unfortunately, my implementation hasn't worked as expected.</p>
<p>To simplify the problem, I scaled back and attempted to implement a minimal U-Net model in Julia with synthetic data. However, I keep running into the following error during training:</p>
<pre><code>ERROR: MethodError: no method matching (::var&quot;#17#19&quot;{var&quot;#loss#18&quot;{…}, Array{…}, Array{…}})(::@NamedTuple{layers::Tuple{…}})

The function #17 exists, but no method is defined for this combination of argument types.
</code></pre>
<p>The numbers in #17 change depending on the code.</p>
<h1>What I Have Done:</h1>
<ul>
<li><p>Created a minimal U-Net model with a few convolutional and transposed convolutional layers.</p>
</li>
<li><p>Used synthetic 64x64 input images and binary masks as the dataset.</p>
</li>
<li><p>Attempted to train the model with a basic training loop using Flux.jl's gradient function and the logitcrossentropy loss.</p>
</li>
<li><p>Experimenting with different U-Net implementations (e.g., this <a href=""https://github.com/DhairyaLGandhi"" rel=""nofollow noreferrer"">repo</a>).</p>
</li>
</ul>
<p>I suspect the issue might be with:</p>
<ul>
<li>The shape of the model's output not matching the target shape.</li>
<li>The loss function not returning a scalar value, which is required for gradient computation.</li>
</ul>
<h1>Example</h1>
<p>Here is the code that reproduces the issue:
I´m using Julia Version 1.11.1 and Flux v0.16.0</p>
<pre><code>using Flux 
using Flux: Conv, ConvTranspose, relu, MaxPool, Dense, Chain, params
using Base.Iterators: partition
using Random
using Plots 

# Summary:
# This code defines a U-Net architecture for image segmentation using the Flux library in Julia.
# It creates synthetic data, prepares batches, trains the U-Net model, and tests the trained model.
# The problem is to ensure the U-Net model is correctly implemented and trained on the synthetic dataset.

# Define the U-Net architecture
function unet(input_channels::Int, output_channels::Int)
    encoder = Chain(
        Conv((3, 3), input_channels =&gt; 64, pad=1), relu, MaxPool((2, 2), stride=(2, 2)),
        Conv((3, 3), 64 =&gt; 128, pad=1), relu, MaxPool((2, 2), stride=(2, 2)),
        Conv((3, 3), 128 =&gt; 256, pad=1), relu, MaxPool((2, 2), stride=(2, 2)),
        Conv((3, 3), 256 =&gt; 512, pad=1), relu, MaxPool((2, 2), stride=(2, 2))
    )
    
    decoder = Chain(
        ConvTranspose((3, 3), 512 =&gt; 256, stride=2, pad=1), relu,
        ConvTranspose((3, 3), 256 =&gt; 128, stride=2, pad=1), relu,
        ConvTranspose((3, 3), 128 =&gt; 64, stride=2, pad=1), relu,
        ConvTranspose((3, 3), 64 =&gt; output_channels, stride=2, pad=1)
    )
    
    return Chain(encoder, decoder, x -&gt; x[:, 1:64, 1:64, :])
end

# Create a synthetic dataset for image segmentation
function create_test_data(num_samples::Int)
    data = []
    for _ in 1:num_samples
        image = rand(Float32, 64, 64, 1)
        mask = rand(Bool, 64, 64, 1)
        push!(data, (image, mask))
    end
    return data
end

# Split the synthetic dataset into batches
function prepare_batches(data, batch_size::Int)
    batches = []
    for batch in partition(data, batch_size)
        input_batch = cat([x[1] for x in batch]..., dims=4)
        mask_batch = cat([x[2] for x in batch]..., dims=4)
        push!(batches, (input_batch, mask_batch))
    end
    return batches
end

# Implement a training loop for the U-Net model
function train_unet(model, train_data, num_epochs::Int, learning_rate::Float64)
    opt = ADAM(learning_rate)
    loss(x, y) = Flux.logitcrossentropy(model(x), float(y))
    
    for epoch in 1:num_epochs
        for (input_batch, mask_batch) in train_data
            gs = gradient(() -&gt; loss(input_batch, mask_batch), Flux.trainable(model))
            Flux.Optimise.update!(opt, Flux.trainable(model), gs)
        end
        println(&quot;Epoch $epoch complete&quot;)
    end
end

# Test a trained U-Net model
function test_unet(model, test_image)
    prediction = model(test_image)
    plot(plot(test_image[:, :, 1, 1], title=&quot;Input Image&quot;),
         plot(prediction[:, :, 1, 1], title=&quot;Predicted Mask&quot;),
         layout=(1, 2))
end

# Example usage
model = unet(1, 1)
data = create_test_data(100)
batches = prepare_batches(data, 8)
train_unet(model, batches, 10, 0.001)
test_image, _ = data[1]
test_unet(model, test_image)
</code></pre>
<p>Why does the above code result in the said error, and how can I fix it?</p>
<p>I've tried ensuring that the shapes of the model's output and target match, but I suspect the issue lies in either the loss function or the gradient call.</p>
","2","Question"
"79316664","","<p>I am struggling to understand what should happen when terminal nodes are selected in MCTS. I see there are several posts with similar titles <a href=""https://stackoverflow.com/questions/47320861/how-to-handle-terminal-nodes-in-monte-carlo-tree-search"">here</a>, <a href=""https://stackoverflow.com/questions/50796917/monte-carlo-tree-search-handling-game-ending-nodes"">here</a> and <a href=""https://stackoverflow.com/questions/73199518/mone-carlo-tree-search-and-terminal-nodes-handling"">here</a> but they don't <em>seem</em> to explain my point of difficulty.</p>
<p>Suppose it is white to move, the move W is expanded despite blundering a loss on the next move to the move B and happens to win in play-out. After back-propagation, W has a good score, so a child is chosen to be expanded which is B. B is a terminal node, so we back-propagate a win for black. Now B has a good score. Now I see two possibilities:</p>
<ol>
<li><p>Allow B to be selected again (despite it having no child nodes), in which case, B will repeatedly accumulate more and more score as it wins for black every time and keep being selected. Then W will be the most simulated child node of the root node and therefore be selected to be played despite being a blunder (I am selecting the most simulated node based on <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"" rel=""nofollow noreferrer"">the wikipedia article</a>).</p>
</li>
<li><p>I do not allow B to be selected again. But I think this poses another problem. If the current state was after the move W, and it was black to move, black would only select B once, despite it winning the game, and then it would not win since it would be simulated less than other moves.</p>
</li>
</ol>
<p>This seems like a very basic part of the algorithm, so I'm sure I have just missed something obvious about how it works.</p>
","0","Question"
"79316958","","<p>I am trying to install mlagents. I got to the part in python but after creating a virtual enviorment with pyenv and setting the local version to 3.10, 3.9, and 3.8 it works on none of them. I upgraded pip, installed mlagents, then torch,torchvision, and torchaudio. Then I tested mlagents-learn --help and then because of a error installed protobuf 3.20.3. I then tested again to get the following error</p>
<pre><code>(venv) D:\Unity\AI Ecosystem&gt;mlagents-learn --help
Traceback (most recent call last):
  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main
  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code
  File &quot;D:\Unity\AI Ecosystem\venv\Scripts\mlagents-learn.exe\__main__.py&quot;, line 4, in &lt;module&gt;
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\learn.py&quot;, line 2, in &lt;module&gt;
    from mlagents import torch_utils
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\torch_utils\__init__.py&quot;, line 1, in &lt;module&gt;
    from mlagents.torch_utils.torch import torch as torch  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\torch_utils\torch.py&quot;, line 6, in &lt;module&gt;
    from mlagents.trainers.settings import TorchSettings
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\settings.py&quot;, line 644, in &lt;module&gt;
    class TrainerSettings(ExportableSettings):
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\settings.py&quot;, line 667, in TrainerSettings
    cattr.register_structure_hook(
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\cattr\converters.py&quot;, line 207, in register_structure_hook
    self._structure_func.register_cls_list([(cl, func)])
  File &quot;D:\Unity\AI Ecosystem\venv\Lib\site-packages\cattr\dispatch.py&quot;, line 55, in register_cls_list
    self._single_dispatch.register(cls, handler)
  File &quot;C:\Users\Ebrah\AppData\Local\Programs\Python\Python311\Lib\functools.py&quot;, line 864, in register
    raise TypeError(
TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class or union type.
</code></pre>
<p>I tried installing cattrs 1.5.0 but the error remains. As I said before I also tried in 3.11, 3.10, 3.9 and 3.8 and got the same error in all of them. My unity version is 2022.3.5f1 but I don't see how that would make a difference. My pyenv version is 3.1.1. I am on windows 11 and am using pyenv-win.</p>
","1","Question"
"79317350","","<p>Using ggplot2's &quot;economics&quot; dataset to study linear regression.
why is my SGDRegression graph looking like this?</p>
<pre><code>x = df.loc[:,'pce'].values.reshape(-1,1)
y = df.loc[:,'psavert'].values.reshape(-1,1)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)
from sklearn.linear_model import SGDRegressor
sr = SGDRegressor(eta0 = 0.001, verbose = 1)
sr.fit(x_train,y_train.flatten())
plt.scatter(x_train,y_train, s = 5, alpha = 0.3, c = 'blue')
plt.plot(x_train,sr.predict(x_train), c = 'green')
plt.xlabel('pce(billions)')
plt.ylabel('saving rate')
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/bRicdxUr.png"" rel=""nofollow noreferrer"">SGD graph</a></p>
<p><a href=""https://i.sstatic.net/v8ghQSLo.png"" rel=""nofollow noreferrer"">LinearRegression graph</a></p>
<p>Tried ajusting the eta0, or the max_iter and it doesn't work.</p>
","0","Question"
"79318939","","<p>I am currently developing and testing a RNN that relies upon a large amount of data for training, and so have attempted to separate my training and testing files. I have one file where I create, train, and save a <code>tensorflow.keras</code> model to a file <code>'model.keras'</code> I then load this model in another file and predict some values, but get the following error:
<code>Failed to convert elements of {'class_name': '__tensor__', 'config': {'dtype': 'float64', 'value': [0.0, 0.0, 0.0, 0.0]}} to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes</code></p>
<p>By the way, I have tried running <code>model.predict</code> with this exact same data in the file where I train the model, and it works smoothly. The model loading must be the problem, not the data used to predict.</p>
<p>This mysterious <code>float64</code> tensor is the value I passed into the masking layer. I don't understand why keras is unable to recognize this JSON object as a Tensor and apply the masking operation as such. I have included snippets of my code below, edited for clarity and brevity:</p>
<p>model_generation.py:</p>
<pre><code># Create model

model = tf.keras.Sequential([
    tf.keras.layers.Input((352, 4)),
    tf.keras.layers.Masking(mask_value=tf.convert_to_tensor(np.array([0.0, 0.0, 0.0, 0.0]))),
    tf.keras.layers.GRU(50, return_sequences=True, activation='tanh'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.GRU(50,activation='tanh'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(units=1, activation='sigmoid')])

# Compile Model...
# Train Model...
model.save('model.keras')

model.predict(data) # Line works here
</code></pre>
<p>model_testing.py</p>
<pre><code>model = tf.keras.models.load_model('model.keras')

model.predict(data) # this line generates the error
</code></pre>
<p>EDIT:</p>
<p>Moved the load command into the same file as the training, still receiving the exact same error message.</p>
","1","Question"
"79320289","","<p>I'm using LGBM to forecast the <em>relative change</em> of a numerical quantity. I'm using the MSLE (Mean Squared Log Error) loss function to optimize my model and to get the correct scaling of errors. Since MSLE isn't native to LGBM, I have to implement it myself. But lucky me, the math can be simplified a ton. This is my implementation;</p>
<pre><code>class MSLELGBM(LGBMRegressor):
    def __init__(self,  **kwargs): 
        super().__init__(**kwargs)

    def predict(self, X):
        return np.exp(super().predict(X))
    
    def fit(self, X, y, eval_set=None, callbacks=None):
        y_log = np.log(y.copy())
        print(super().get_params())  # This doesn't print any kwargs
        if eval_set:
            eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
        super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)
</code></pre>
<p>As you can see, it's very minimal. I basically just need to apply a log transform to the model target, and exponentiate the predictions to return to our own non-logarithmic world.</p>
<p>However, my wrapper doesn't work. I call the class with;</p>
<pre><code>model = MSLELGBM(**lgbm_params)
model.fit(data[X_cols_all], data[y_col_train]) 
</code></pre>
<p>And I get the following exception;</p>
<pre><code>
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[31], line 38
     32 callbacks = [
     33     lgbm.early_stopping(10, verbose=0), 
     34     lgbm.log_evaluation(period=0),
     35 ]
     37 model = MSLELGBM(**lgbm_params)
---&gt; 38 model.fit(data[X_cols_all], data[y_col_train]) 
     40 feature_importances_df = pd.DataFrame([model.booster_.feature_importance(importance_type='gain')], columns=X_cols_all).T.sort_values(by=0, ascending=False)
     41 feature_importances_df.iloc[:30]

Cell In[31], line 17
     15 if eval_set:
     16     eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
---&gt; 17 super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

File c:\X\.venv\lib\site-packages\lightgbm\sklearn.py:1189, in LGBMRegressor.fit(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)
   1172 def fit(  # type: ignore[override]
   1173     self,
   1174     X: _LGBM_ScikitMatrixLike,
   (...)
   1186     init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None,
   1187 ) -&gt; &quot;LGBMRegressor&quot;:
   1188     &quot;&quot;&quot;Docstring is inherited from the LGBMModel.&quot;&quot;&quot;
...
--&gt; 765 if isinstance(params[&quot;random_state&quot;], np.random.RandomState):
    766     params[&quot;random_state&quot;] = params[&quot;random_state&quot;].randint(np.iinfo(np.int32).max)
    767 elif isinstance(params[&quot;random_state&quot;], np.random.Generator):

KeyError: 'random_state'
</code></pre>
<p>I have no idea how random_state is missing from the fit method, as it isnt even required for that function. I get the impression that this is a complicated software engineering issue that's above my head. Anybody knows whats up?</p>
<p>If it's of any help, I tried illustrating what I want using a simpler non-lgbm structure;</p>
<p><a href=""https://i.sstatic.net/0lwnajCY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0lwnajCY.png"" alt=""passing kwargs"" /></a></p>
<p>I just want to pass whatever parameters I provide to the MSLELGBM to the original LGBM, but I'm running into a ton of issues doing so.</p>
","1","Question"
"77743630","77743214","<p>It turns out the problem was being caused by the stock not being passed in properly, as commenters suggested.</p>
<p>Therefore in order to make sure the stocks were passed in properly instead of as a string, instead of passing in the stock by:</p>
<pre><code>for stock in chosen_stocks:
    arima_prediction(stock)
</code></pre>
<p>I've instead used:</p>
<pre><code>def get_stock_data(dataframe):
    get_stock_data = dataframe.iloc[:, 30]
    return get_stock_data

stock_data = get_stock_data(dataframe)
arima_prediction(stock_data)
</code></pre>
<p>Thank you to everyone for your help!</p>
","1","Answer"
"77744471","77743228","<p>Use <code>torch.nn.Identity()</code> rather than <code>None</code>.</p>
<p>Or you can pass the tensor in <code>forward()</code> function.</p>
","0","Answer"
"77746479","77743228","<p>This is not a problem specifically related to the value being <code>None</code>; you'd have the same issue if you were to use any other <code>nn.Module</code> (as the value of <code>additional</code> attribute) that is not a sequence (the <code>0</code> after <code>additional</code>) and does not have a parameter named <code>weight</code> in the first <code>nn.Module</code> in the sequential module (the <code>weight</code> after <code>additional.0</code>).</p>
<p>The issue is, in your <em>train</em> mode, when you initialized your model, you have passed <code>True</code> for <code>additional_layer</code> argument i.e.:</p>
<pre><code>model = YourModelClass(additional_layer=True)
</code></pre>
<p>hence <code>self.additional</code> is set to a <code>nn.Module</code> (<code>nn.Sequential</code> specifically). So, the <code>model</code> object's <code>state_dict</code> would have the parameters for the module referred to by the <code>self.additional</code> attribute.</p>
<p>Now, when you re-initialized the model for <em>inference</em>, you didn't have the additional layers as you initialized the model presumably by one of the following:</p>
<pre><code>model = YourModelClass(additional_layer=False)
model = YourModelClass()
</code></pre>
<p>This time there the <code>self.additional</code> i.e. <code>model.additional</code> attribute would be <code>None</code>. As a result, when you call <code>model.load_state_dict</code> and pass it the state dict that was saved earlier in <em>train</em> mode (when additinal layer was there), it gives you the exception that all the keys for the <code>additional</code> attribute are missing.</p>
<p>Assuming you have the correct conditional setup in the <code>forward</code> method when <code>self.additional</code> is <code>None</code>, you can ignore the exception and bypass the loading of missing keys/parameters by setting the <code>strict</code> argument to <code>False</code> while using <code>load_state_dict</code>:</p>
<pre><code>model.load_state_dict(best_model[&quot;my_model&quot;], strict=False)
</code></pre>
","0","Answer"
"77748797","77748547","<p>Your last line is wrong. You misunderstood the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score"" rel=""nofollow noreferrer""><code>score</code></a> method. <code>score</code> take <code>X</code> and <code>y</code> as parameter not the <code>y_true</code> and <code>y_pred</code></p>
<p>Try:</p>
<pre><code>from sklearn.metrics import r2_score

print(r2_score(y_test, prediction))
# 0.24499127100887863
</code></pre>
<p>Or with the <code>score</code> method:</p>
<pre><code>print(model.score(X_test, y_test))
# 0.24499127100887863
</code></pre>
","0","Answer"
"77751619","77748737","<p>Here is your modified code to compute sentence and word embeddings:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
model = GPT2Model.from_pretrained('gpt2')
captions = [
    &quot;example caption&quot;,
    &quot;example bird&quot;,
    &quot;the bird is yellow has red wings&quot;,
    &quot;hi&quot;,
    &quot;very good&quot;
]

# Tokenize and pad sequences
encoded_captions = tokenizer(
    captions,
    return_tensors='pt',
    padding=True,
    truncation=True
)
input_ids = encoded_captions['input_ids']

# Forward pass to get embeddings
with torch.no_grad():
    outputs = model(input_ids)

# Extract embeddings
word_embeddings = outputs.last_hidden_state

# Mask to ignore padding tokens
masked_word_embeddings = word_embeddings * encoded_captions.attention_mask.unsqueeze(-1).float()

# Sum pooling considering only non-padding tokens
sentence_embeddings = masked_word_embeddings.sum(dim=1)

# Normalize by the count of non-padding tokens
sentence_embeddings /= encoded_captions.attention_mask.sum(dim=1, keepdim=True).float()
</code></pre>
<p>Some relevant facts:</p>
<ol>
<li>As you said, word embeddings are the last hidden output. If you print the out put you see 5 vectors (number of sentences) of length 7 (maximum number of tokens in the list of sentences) and shape 768 (model dimension).</li>
</ol>
<pre><code>word_embeddings.shape
&gt;&gt; torch.Size([5, 7, 768])
</code></pre>
<p>It means that some sentences have embeddings for non existent tokens, so we need to mask the output to consider only existent tokens</p>
<ol start=""2"">
<li>Mask consists on multiplying by zero (or whatever special value but zero is the more accepted and useful, as it nulls values) on non existent token places of the word vector.  The attention mask is crucial for handling variable-length sequences and ensuring that padding tokens do not contribute to the embeddings.</li>
</ol>
<pre><code>print(masked_word_embeddings)
&gt;&gt; tensor([[[-0.2835, -0.0469, -0.5029,  ..., -0.0525, -0.0089, -0.1395],
         [-0.2636, -0.1355, -0.4277,  ..., -0.3552,  0.0437, -0.2479],
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
         ...,
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],
...
</code></pre>
<ol>
<li>Usually, sentence embeddings are computed as the sum, mean or max of the masked word embeddings. It depends on your use case.</li>
</ol>
<ul>
<li>Mean is more suitable to variable length:</li>
</ul>
<pre><code>sentence_embeddings = masked_word_embeddings.mean(dim=1)
</code></pre>
<ul>
<li>Sum is intended to force importance on relevant parts:</li>
</ul>
<pre><code>sentence_embeddings = masked_word_embeddings.max(dim=1)
</code></pre>
<p>It exist a lot of techniques and it depends on how embeddings perform for your task. I would choose a method that maximices the cosine similarity between vectors I consider similar for my task. Ex: If the sum gets more similarity than mean, it may be more suitable.</p>
<ol start=""4"">
<li>Additionally I suggest you to normalize values by the number of tokens in the sentence. So that, with that normalization, larger sentences tend to have lower vector values. It is to embed information on the number of tokens in the sentence. It prevents to get high similarity scores between a sentence of 4 tokens with a whole book, that its meaningless.</li>
</ol>
","1","Answer"
"79308935","77748547","<p>The mismatch is related to the format when you pass the data and the output of it.
Usually, we pass the DataFrame and it outputs the numpy array, which the name of the columns will be different from the input.
Thus, you need to make sure the output of it needs to be in the same format.
So, apply the following configuration from sklearn to the training function and the testing function. Check if it solves.</p>
<pre><code>sklearn.set_config(transform_output=&quot;pandas&quot;)
</code></pre>
","0","Answer"
"77750182","77749447","<p>You can use <a href=""https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html"" rel=""nofollow noreferrer""><code>plt.subplot</code></a> in order to display multiple plots.<br />
<code>plt.subplot(2,2,i)</code> creates 4 plots in a 2 x 2 grid. <code>i</code> is the index of the plot you are currently setting.</p>
<pre><code>#Set the size of your plot
plt.subplots(2, 2, figsize=(15, 10))

for lr in learning_rates: 
    opt = SGD(learning_rate = lr)
    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    network = model.fit(x_train_subset, y_train_subset, epochs=iterations, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    train_loss_values = network.history['loss']
    train_accuracy_values = network.history['accuracy']
    val_loss_values = network.history['val_loss']
    val_accuracy_values = network.history['val_accuracy']

    plt.subplot(2,2,1)
    plt.plot(epochs, train_accuracy_values, label=f'LR = {lr}')
    plt.subplot(2,2,2)
    plt.plot(epochs, train_loss_values, label=f'LR = {lr}')
    plt.subplot(2,2,3)
    plt.plot(epochs, val_accuracy_values, label=f'LR = {lr}')
    plt.subplot(2,2,4)
    plt.plot(epochs, val_loss_values, label=f'LR = {lr}')
    
# Formatting for each subplot
ylabel = ['Training Accuracy', 'Training Loss', 'Validation Accuracy', 'Validation Loss']
for i in range(4):
    plt.subplot(2,2,i+1)
    plt.title(f'{ylabel[i]} over Epochs for Different Learning Rates')
    plt.xlabel('Epochs')
    plt.ylabel(ylabel[i])
    plt.legend()
    plt.grid(True)

plt.show()
</code></pre>
<p>I do not have your data, but the output should look like this:
<a href=""https://i.sstatic.net/fdGhj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fdGhj.png"" alt=""enter image description here"" /></a></p>
","4","Answer"
"77754439","77753838","<p>The error message show that <code>v1</code> is a numpy array with a single entry. That entry is a string with your comma-separated list. You will need to split that up into the individual numbers. You'll probably want to do something like this:</p>
<pre class=""lang-py prettyprint-override""><code>v1 = np.array([float(num) for num in request.GET['n1'].split(&quot;,&quot;)])
</code></pre>
<p>The message also mentions the shape of the array, so you will need to reshape it according to your case (just follow what the error message says).</p>
","0","Answer"
"77755794","77755646","<p>This is probably sufficiently different from normal bag of words vectorization that you'd be better off writing your own vectorizer. Here's the simplest approach I can come up with.</p>
<p>Code:</p>
<pre><code>import io
import pandas as pd
import numpy as np
from collections import defaultdict


s = &quot;&quot;&quot;
RepID,Txt
1,K9G3P9 4H477 -Q207KL41 98464 Q207KL41
2,D84T8X4 -D9W4S2 -D9W4S2 8E8E65 D9W4S2 
3,-05L8NJ38 K2DD949 0W28DZ48 207441 K2D28K84&quot;&quot;&quot;
df_reps = pd.read_csv(io.StringIO(s))


def BOW(documents):
    ret = []
    vocabulary = defaultdict()
    vocabulary.default_factory = vocabulary.__len__
    for document in documents:
        feature_counter = defaultdict(int)
        for token in document.split():
            sign = 1
            if token[0] == &quot;-&quot;:
                token = token[1:]
                sign = -1
            feature_idx = vocabulary[token]
            feature_counter[feature_idx] += sign
        ret.append(feature_counter)
    df = pd.DataFrame.from_records(ret)
    df = df.fillna(0)
    df.columns = vocabulary.keys()
    df = df.astype(np.int8)
    return df


print(BOW(df_reps[&quot;Txt&quot;]))
</code></pre>
<p>Output:</p>
<pre><code>   K9G3P9  4H477  Q207KL41  98464  D84T8X4  D9W4S2  8E8E65  05L8NJ38  K2DD949  0W28DZ48  207441  K2D28K84
0       1      1         0      1        0       0       0         0        0         0       0         0
1       0      0         0      0        1      -1       1         0        0         0       0         0
2       0      0         0      0        0       0       0        -1        1         1       1         1
</code></pre>
","0","Answer"
"77767058","77750389","<p>I think the problem is in part of your code <code>pd.DataFrame(x, columns = names)</code> because in previous step you preprocess the categorical columns with <code>categorical_transformer = OneHotEncoder(handle_unknown = 'ignore')</code>, it increases the number of columns that's why shapes mismatch, as a solution you can try to change OneHotEncoder with LabelEncoder or change <code>names</code> list according new numbers of columns</p>
","1","Answer"
"77772939","77762264","<p><strong>First,</strong> I will leave you with a blog post <a href=""https://cs231n.github.io/neural-networks-3/#baby"" rel=""nofollow noreferrer"">source</a> which explains nicely how to go about the training process of a deep neural network. I believe this will be helpful reading no matter what.</p>
<p><strong>Second,</strong> a not climbing training loss can not be interpreted as non-overfitting. Rather, the missing validation loss climb could indicate that your model does not overfit.</p>
<p>In the end optimization is <strong>all about capacity</strong>. If you model is not able to overfit, you could try increasing the number of parameters of your model, or initially try to reduce the data to see whether you are able to overfit on a small subset of your overall dataset.</p>
<p><strong>Things to consider next:</strong></p>
<ul>
<li>Your learning rate might be too high (indicated by the steep drop and the &quot;early&quot; convergence)</li>
<li>Your batch size is maybe a little too high (indicated by the small band width of your loss curves, but it is hard to say)</li>
<li>Add layers and report back.</li>
</ul>
<p><em>Note:</em> Without knowing your data, this is really difficult to answer.</p>
","1","Answer"
"77776104","77766048","<p>The issue is that your class CoinFlipEnv does not conform to the gym.Env interface, specifically the reset function.  According to the documentation available here <a href=""https://gymnasium.farama.org/api/env/#gymnasium.Env.reset"" rel=""nofollow noreferrer"">https://gymnasium.farama.org/api/env/#gymnasium.Env.reset</a> the reset function takes a seed parameter so your function:</p>
<p><code>def reset(self)</code></p>
<p>Must also take one like so(it also says it takes an options keyword argument as well so lets include that too):</p>
<p><code>def reset(self, seed=None, options=None)</code></p>
<p>That it what the last line of the stacktrace is telling you when it says:</p>
<p><code>obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)</code></p>
<p>The object self.envs[env_idx] is an instance of your class CoinFlipEnv and it is trying to pass seed=self._seeds[env_idx] and **maybe_options to CoinFlipEnv's reset function but since it is trying to pass the named argument of seed to a function that doesn't have seed defined it raises an error.</p>
","1","Answer"
"77803392","77762264","<p>A few things come to my attention:</p>
<ol>
<li>You work with images (or 2D data)</li>
<li>CNN's are superb with image data... at least for local information extraction. Multiple kernels allow to extract many different aspects / features in one go, which is nice... but the receptive field is small. Very deep layers would solve this, but come with new problems.</li>
<li>You therefore use self attention (MHA with Q=K=V), which is a nice approach to allow the network gain a maximum receptive field. Information regardless of its spatial location is weighted according to the Q, K, V multiplication inside of MHA.</li>
<li>When reading CV papers, resolutions of 512 can be considered rather large and authors will always downsample the spatial dimensions before applying costly operations like self-attention.</li>
<li>Before a dog can be classified, a network will have to understand colors, lines, fur, eys, legs, and finally a dog. At the moment you waste the computation to perform something a CNN is very good at: understanding high frequency features (=local features). Since self-attention is expensive, one uses far less heads (you chose 2) than people would choose features in a CNN for doing the semantically the same: understand the &quot;shallow&quot; basics of the image.</li>
<li>I therefore highly recommend to down sample the data first with CNN layers, which are much more sparse in terms of parameters.</li>
<li>Only then, when the spatial dimension is reduced, use self-attention to find connections to distant locations that are important for your task.</li>
<li>The parameters saved by down sampling can be used to increase the number of heads. Each head can only understand a single concept. Similar to the number of channels in CNN's. Depending on the complexity of your task, you will certainly need more than 2. Numbers between 16-64 are better suited, if you can afford these numbers in terms of computation.</li>
</ol>
<p><strong>Coming back to your learning curves</strong></p>
<p>From my point of view, your network might even have many parameters due to the MHA, but they are currently used in way the actual capacity is comparable low. This is the reason why your network shows no sign of over fitting, which is nicely explained in the answer by mrk. He also explained both approaches to &quot;enforce&quot; overfitting: (1) reduce data or (2) increase the capacity. Bot allows you to verify that your network &quot;can&quot; over fit. Currently that does not happen.</p>
<p><strong>My recommendations</strong></p>
<ul>
<li>Use a feature extractor (CNN) for down sampling and (local) feature extraction</li>
<li>Later, apply self-attention to allow the network to understand how higher-level information is related across the spatial dimension</li>
<li>Provide more information on the topic you're working on so we can give better feedback on loss, general approach etc.</li>
</ul>
","2","Answer"
"77876425","77760620","<p>It seems that there is an issue between Jupyter and SetFit!
You will see the values printed once you run your code directly in the terminal.</p>
<p>Sample run from the terminal here:</p>
<pre><code>{'embedding_loss': 0.3245, 'learning_rate': 9.090909090909091e-08, 'epoch': 0.0}                                                                                                                                                                                                                                                                                                                                            | 0/2200 [00:00&lt;?, ?it/s]
    {'embedding_loss': 0.035, 'learning_rate': 1.1111111111111113e-05, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                                                                                     
    {'eval_embedding_loss': 0.1738, 'learning_rate': 1.1111111111111113e-05, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                                                                               
    {'embedding_loss': 0.0005, 'learning_rate': 0.0, 'epoch': 2.0}                                                                                                                                                                                                                                                                                                                                                                                       
    {'eval_embedding_loss': 0.181, 'learning_rate': 0.0, 'epoch': 2.0}                                                                                                                                                                                                                                                                                                                                                                                   
    {'train_runtime': 156.4973, 'train_samples_per_second': 112.462, 'train_steps_per_second': 14.058, 'epoch': 2.0}                                                                                                                                                                                                                                                                                                                                     
    100%|███████████████████| 2200/2200 [02:34&lt;00:00, 14.23it/s]
    Batches: 100%|███████████████████| 46/46 [00:01&lt;00:00, 44.09it/s]
    ***** Running evaluation *****
    Batches: 100%|███████████████████| 12/12 [00:00&lt;00:00, 55.14it/s]
    Metrics: {'accuracy': 0.6539509536784741, 'precision': 0.6951538195187225, 'recall': 0.6539509536784741, 'f1': 0.6658552263536266}
</code></pre>
","0","Answer"
"77770172","77769033","<blockquote>
<p>Q1) We have two unfitted estimators in parallel, and both need to be fitted before ensemble.predict(...) would work. <strong>But we can't fit any of the estimators without first getting a prediction from the ensemble.</strong> How does sklearn handle this circular dependency?</p>
</blockquote>
<p>The sentence I've emphasized is incorrect.  In this approach, the individual estimators are tuned for their own performance; the tuning doesn't know or care that eventually it will get used in an ensemble.  <em>After</em> the optimal hyperparameters are found, the resulting models get ensembled.</p>
<p>For <code>VotingRegressor</code> that's basically it.  For <code>StackingRegressor</code> there's a nested cross-validation happening, so the preceding paragraph is slightly wrong.  The hyperparameter tuning happens on every fold from the stacking fitting procedure, with the selected model making predictions on the held out fold to make the training set for the meta-estimator.  So in fact, different hyperparameters may be selected for each such split.  The final ensemble uses yet another version of the model, tuned on the entire training set.</p>
<blockquote>
<p>Q2) Since we have two estimators running independent tuning, does each estimator make the false assumption that the parameters of the other estimator are fixed? So we end up with a poorly-defined optimisation problem.</p>
</blockquote>
<p>Given the above, this is moot: the two estimators don't know that the other even exists, let alone what hyperparameters will be selected for it.  But you are right that there's a difference with your final approach.</p>
<blockquote>
<p>For reference, I think the correct way to jointly optimise the models of an ensemble would be to define a single CV that searches over all parameters jointly, shown below. But my questions are about how sklearn handles the special case described earlier.</p>
</blockquote>
<p>Both methods are &quot;correct&quot;, it just depends on how much of the hyperparameter space you want to search.  The former assumes that the optimal ensemble arises from optimal individual models, whereas the latter allows for an ensemble that performs better using suboptimal base models.  The latter is more likely to be true, but is more computationally costly, and the former may or may not perform reasonably well in comparison.  (And maybe the latter can even overfit more on your training set given its greater capacity.)</p>
","1","Answer"
"77778061","77775519","<p>For the segmentation task, you can refer to the <code>boxes.cls</code> property of the result object to get the detected class index, the same as for the detection task. <code>model.names[class_index]</code> will return the class name:</p>
<pre><code>from ultralytics import YOLO
model = YOLO(&quot;path/to/best.pt&quot;)
result = model.predict(os.path.join(cut_dir, im_name), save_conf=True, show=True)
if result[0].masks is not None:
    for counter, detection in enumerate(result[0].masks.data):
         cls_id = int(result[0].boxes[counter].cls.item())
         cls_name = model.names[cls_id]  
</code></pre>
","4","Answer"
"77778279","77777706","<p>Sometime this kind of error is thrown as part of your code is wrapped by a</p>
<pre><code>with torch.no_grad():
</code></pre>
<p>I suggest to check various function in your code checking for it.
Maybe you have already checked it but this is a good start!</p>
","0","Answer"
"77782350","77781970","<p>I think I have found a solution <a href=""https://stackoverflow.com/questions/71360416/segmentation-fault-occurs-when-loading-lightgbm-model-on-m1-mac"">here</a>.</p>
<p>The issue appears to have been resolved by installing lightgbm using the following method:</p>
<pre><code>$ sudo conda install -c conda-forge lightgbm 
</code></pre>
<p>rather than the usual pip install method.</p>
","0","Answer"
"77783021","77766048","<p>The <a href=""https://gymnasium.farama.org/api/env/#gymnasium.Env.reset"" rel=""nofollow noreferrer""><code>gymnasium.Env</code></a> class has the following signature which divers from the one by <code>DummyVecEnv</code> which takes no arguments.</p>
<p><code>Env.reset(self, *, seed: int | None = None, options: dict[str, Any] | None = None) → tuple[ObsType, dict[str, Any]]</code></p>
<p>in other words <code>seed</code> and <code>options</code> are keyword-only which your own <code>reset</code> function needs to implement. It returns the <code>observation, info</code> tuple in the end.</p>
<p>The problems to note:</p>
<ul>
<li>Signature of <code>reset</code> does not match, needs <code>seed</code> and <code>options</code></li>
<li>Return signature of <code>reset</code> does not match. It needs to return a <strong>valid observation (ObsType) and a dictionary</strong></li>
<li>Return signature of <code>step</code> does not match, needs to say if result is truncated / model went out of bounds. (see below)</li>
</ul>
<pre><code> def reset(self, *, seed=None, options=None): # Fix input signature
        # Reset the environment
        self.flip_result = 0 # None is not a valid Observation
        return self.flip_result, {} # Fix return signature
</code></pre>
<p>If you return None, as underlying numpy arrays are used <code>array([0])[0]=obs &lt;- None</code> would throw another error.</p>
<hr />
<p><code>step</code> needs to have five returns parameters <code>observation, reward, terminated, truncated, info</code></p>
<pre><code>    def step(self, action):
        # Perform the action (0 for heads, 1 for tails)
        self.flip_result = int(np.random.rand() &lt; self.heads_probability)

        # Compute the reward (1 for correct prediction, -1 for incorrect)
        reward = 1 if self.flip_result == action else -1

        # Return the observation, reward, done, truncated, and info
        return self._get_observation(), reward, True, False, {}
</code></pre>
<hr />
<p>Now the models trains:</p>
<pre><code>-----------------------------
| time/              |      |
|    fps             | 5608 |
|    iterations      | 1    |
|    time_elapsed    | 0    |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 3530        |
|    iterations           | 2           |
|    time_elapsed         | 1           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.020679139 |
|    clip_fraction        | 0.617       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.675      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.38        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.107      |
|    value_loss           | 1           |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 3146        |
|    iterations           | 3           |
|    time_elapsed         | 1           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.032571375 |
|    clip_fraction        | 0.628       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.392       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.104      |
|    value_loss           | 0.987       |
-----------------------------------------
---------------------------------------
| time/                   |           |
|    fps                  | 2984      |
|    iterations           | 4         |
|    time_elapsed         | 2         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0691616 |
|    clip_fraction        | 0.535     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.417    |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.335     |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.09     |
|    value_loss           | 0.941     |
---------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 2898       |
|    iterations           | 5          |
|    time_elapsed         | 3          |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.12130852 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.189     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.536      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.806      |
----------------------------------------
Action: [1], Observation: [0], Reward: [1.]
Action: [1], Observation: [0], Reward: [-1.]
Action: [1], Observation: [0], Reward: [-1.]
Action: [1], Observation: [0], Reward: [1.]
Action: [1], Observation: [0], Reward: [1.]
Action: [1], Observation: [0], Reward: [-1.]
Action: [1], Observation: [0], Reward: [1.]
Action: [1], Observation: [0], Reward: [-1.]
Action: [1], Observation: [0], Reward: [1.]
Action: [1], Observation: [0], Reward: [1.]
</code></pre>
","1","Answer"
"77789230","77785503","<p>The problem is you are creating a different featurizer for every step of your loop. This means you are giving a totally different feature vector to the model on every step.</p>
<p>You need to <code>fit</code> the <code>CountVectorizer</code> once on your train dataset, then use the same fitted <code>CountVectorizer</code> to transform all the data.</p>
","0","Answer"
"77789243","77777706","<p>The error is here:</p>
<p><code>train_preds_probs = torch.softmax(train_logits,dim=1).argmax(dim=1).type(torch.float32)</code></p>
<p>When you <code>argmax</code>, you lose the gradient chain and can't backprop. For training, you pass the softmax output to your loss function without the argmax.</p>
","0","Answer"
"77792009","77791682","<p>You can replace:</p>
<pre><code>from torch.multiprocessing import Pool, set_start_method
</code></pre>
<p>With:</p>
<pre><code>from concurrent.futures import ProcessPoolExecutor as Pool
from multiprocessing import set_start_method
</code></pre>
<p>To avoid to start your (pool) processes as daemon and allow them to create sub-processes.</p>
","1","Answer"
"77792833","77792551","<p>Both packages <a href=""https://pypi.org/project/darts/#data"" rel=""nofollow noreferrer"">darts</a> and <a href=""https://pypi.org/project/torch/#data"" rel=""nofollow noreferrer"">pytorch</a> currently supports Python versions 3.8 - 3.11 but you're using Python 3.12.1 To resolve your issue, install a supported python version and use virtual environments to manage projects that requires specific python versions. For example, you can create a Python 3.10 virtual environment and <code>pip install darts</code> there.</p>
<p><strong>EDIT:</strong>
User @jkr pointed out that pytorch now supports Python 3.11 as can be seen in the <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">official docs</a>.</p>
","6","Answer"
"77793731","77792137","<p>A warm-up is in general an increase of the learning rate. It starts at 0 and then increases linearly over 1(here) step to the specified learning rate of <code>1.25e-5</code>.</p>
<p>Afterwards by default a linear (in other cases a cosine) learning-rate scheduler decays your learning-rate.</p>
<p>To disable the decay add <code>lr_scheduler_type='constant'</code>.
If I recall correctly, this also disables the warmup.<br />
If you want warmup and afterwards a constant rate use <code>constant_with_warmup</code> instead.</p>
<p>EDIT: Valid scheduler types are defined in <a href=""https://github.com/huggingface/transformers/blob/dd16acb8a3e93b643aa374c9fb80749f5235c1a6/src/transformers/trainer_utils.py#L418"" rel=""nofollow noreferrer"">trainer_utils.py</a>, in the class SchedulerType:</p>
<pre class=""lang-py prettyprint-override""><code>class SchedulerType(ExplicitEnum):
    &quot;&quot;&quot;
    Scheduler names for the parameter `lr_scheduler_type` in [`TrainingArguments`].
    By default, it uses &quot;linear&quot;. Internally, this retrieves `get_linear_schedule_with_warmup` scheduler from [`Trainer`].
    Scheduler types:
       - &quot;linear&quot; = get_linear_schedule_with_warmup
       - &quot;cosine&quot; = get_cosine_schedule_with_warmup
       - &quot;cosine_with_restarts&quot; = get_cosine_with_hard_restarts_schedule_with_warmup
       - &quot;polynomial&quot; = get_polynomial_decay_schedule_with_warmup
       - &quot;constant&quot; =  get_constant_schedule
       - &quot;constant_with_warmup&quot; = get_constant_schedule_with_warmup
       - &quot;inverse_sqrt&quot; = get_inverse_sqrt_schedule
       - &quot;reduce_lr_on_plateau&quot; = get_reduce_on_plateau_schedule
       - &quot;cosine_with_min_lr&quot; = get_cosine_with_min_lr_schedule_with_warmup
       - &quot;warmup_stable_decay&quot; = get_wsd_schedule
    &quot;&quot;&quot;

    LINEAR = &quot;linear&quot;
    COSINE = &quot;cosine&quot;
    COSINE_WITH_RESTARTS = &quot;cosine_with_restarts&quot;
    POLYNOMIAL = &quot;polynomial&quot;
    CONSTANT = &quot;constant&quot;
    CONSTANT_WITH_WARMUP = &quot;constant_with_warmup&quot;
    INVERSE_SQRT = &quot;inverse_sqrt&quot;
    REDUCE_ON_PLATEAU = &quot;reduce_lr_on_plateau&quot;
    COSINE_WITH_MIN_LR = &quot;cosine_with_min_lr&quot;
    WARMUP_STABLE_DECAY = &quot;warmup_stable_decay&quot;
</code></pre>
","3","Answer"
"77799208","77790906","<p>Transposing and reshaping Numpy arrays is normally a fast operation, negligible when arrays are large.</p>
<p>Nevertheless, you can invert the array multiplication order and avoid some transpositions. It won't be much faster, but it may look better (not tested):</p>
<pre><code>matMulComponent = self.weights @ input_data
z = matMulComponent + self.biases
</code></pre>
<p>In this case, <code>z</code> is transposed respect to your original implementations.</p>
","0","Answer"
"77802054","77788410","<p>As pointed out by Ben, the reason you are finding different answers is because your understanding of RFECV is fundamentally flawed. Cross validation is not implemented at each step within RFE, but rather RFE is implemented within each fold of cross validation.</p>
<p>Your method currently removes a feature from the data and then performs CV to score it, and essentially performs a different CV to score each subset of features. However, RFECV performs CV only a single time at the start, and then cycles between removing a feature and scoring the model <strong>within</strong> a singular fold.</p>
<p><em>Note: From experimentation, I have found that sklearn RFECV computes the score on a singular fold directly by using whichever evaluation metric you specify, and not another layer of cross validation or anything. In your case, you have chosen <code>scoring=&quot;accuracy&quot;</code>. So within a fold, the model will score the test split based on accuracy, and repeat this for each subset of features.</em></p>
<p><strong>You can implement RFECV from scratch as follows:</strong></p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

def score(alg, x_train, y_train, x_test, y_test):
    &quot;&quot;&quot;Calculate the accuracy score of the algorithm.&quot;&quot;&quot;
    alg.fit(x_train, y_train)
    y_pred = alg.predict(x_test)
    return accuracy_score(y_test, y_pred)

def rfecv_function(alg, x_var, X, y, cv):
    &quot;&quot;&quot;Perform RFECV and return a dictionary to store an array of test scores
    for each cv, where the array contains test scores for each number of 
    features selected 1,2,...,n (n is total number of features).
    &quot;&quot;&quot;
    dic = {}
    # Iterate through folds
    for fold_index, (train_index, test_index) in enumerate(cv.split(X, y)):
        x_train_fold, y_train_fold = X.iloc[train_index], y.iloc[train_index]
        x_test_fold, y_test_fold = X.iloc[test_index], y.iloc[test_index]
        features = x_var.copy()
        
        # Array to store test scores for each feature subset
        scores_array = np.empty(len(x_var))
        
        # Iterate through the feature subsets
        for i in range(len(x_var)):
            # Calculate and store the scores in the array
            scores = score(alg, x_train_fold[features], y_train_fold, 
                           x_test_fold[features], y_test_fold)
            scores_array[-i-1] = scores 
            # Find and remove the lowest ranked feature
            alg.fit(x_train_fold[features], y_train_fold)
            lowest_rank = features[np.argmin(alg.feature_importances_)]
            features.remove(lowest_rank)
            
        dic[f&quot;split{fold_index}_test_score&quot;] = scores_array
    
    return dic

dtree = DecisionTreeClassifier(random_state = 0)
cv_split = StratifiedKFold(5, shuffle=True, random_state=0)

# Assume train is a pandas dataframe
x_var = [&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;]
y_var = [&quot;target_var&quot;]
rfecv_scores = rfecv_function(dtree, x_var, train[x_var], train[y_var], cv_split)
</code></pre>
<p>The <code>rfecv_scores</code> provide identical values to those computed in the inbuilt class <code>RFECV.cv_results_</code></p>
","1","Answer"
"77890455","77785423","<p>Will you try:</p>
<pre><code>sv = np.array([arr[:100] for arr in shap_values.values])
data = np.array([arr[:100] for arr in shap_values.data])
shap.summary_plot(sv, data, feature_names=feature_importance['features'].tolist())
</code></pre>
<p>I've got a grey plot. This is because your data is non-numeric.</p>
","0","Answer"
"77912993","77776124","<p>This means you need to <strong>encode your labels</strong> in a fresher update. Ideally right before training, you encode.</p>
<pre><code>y_train = label_encoder.fit_transform(y_train)
xgb.fit(X_train, y_train)
</code></pre>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">Label Encoder</a></p>
","1","Answer"
"78680610","77792137","<p>For anyone wondering, which class name to use, there is a link to HuggingFace <a href=""https://huggingface.co/transformers/v4.2.2/_modules/transformers/optimization.html"" rel=""nofollow noreferrer"">optimizers and LR schedulers</a> that you can use while defining the lr scheduler.</p>
<pre><code>TYPE_TO_SCHEDULER_FUNCTION = {
    SchedulerType.LINEAR: get_linear_schedule_with_warmup,
    SchedulerType.COSINE: get_cosine_schedule_with_warmup,
    SchedulerType.COSINE_WITH_RESTARTS: get_cosine_with_hard_restarts_schedule_with_warmup,
    SchedulerType.POLYNOMIAL: get_polynomial_decay_schedule_with_warmup,
    SchedulerType.CONSTANT: get_constant_schedule,
    SchedulerType.CONSTANT_WITH_WARMUP: get_constant_schedule_with_warmup,
}
</code></pre>
<p>In training arguments just define the type name for <code>lr_scheduler_type</code> i.e., <code>cosine</code>, <em>in lower case</em>, if you want to use <code>cosine_schedule_with_warmup</code>. You can also pass in extra kwargs using <code>lr_scheduler_kwargs</code>.</p>
<pre><code>    lr_scheduler_type='cosine',
    lr_scheduler_kwargs={...},
</code></pre>
","1","Answer"
"79594848","77792137","<p>Update for 2025:</p>
<p><a href=""https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/optimizer_schedules#transformers.SchedulerType"" rel=""nofollow noreferrer"">the list of allowed scheduler types from documentation </a></p>
<pre><code>“linear” = get_linear_schedule_with_warmup
“cosine” = get_cosine_schedule_with_warmup
“cosine_with_restarts” = get_cosine_with_hard_restarts_schedule_with_warmup
“polynomial” = get_polynomial_decay_schedule_with_warmup
“constant” = get_constant_schedule
“constant_with_warmup” = get_constant_schedule_with_warmup
“inverse_sqrt” = get_inverse_sqrt_schedule
“reduce_lr_on_plateau” = get_reduce_on_plateau_schedule
“cosine_with_min_lr” = get_cosine_with_min_lr_schedule_with_warmup
“warmup_stable_decay” = get_wsd_schedule
</code></pre>
<p>their parameters can be passed to Trainer using <a href=""https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments.lr_scheduler_kwargs"" rel=""nofollow noreferrer""><code>--lr_scheduler_kwargs</code></a></p>
","0","Answer"
"77795846","77794688","<p><strong>What is really happening</strong></p>
<p>In case someone else faces this problem, here is what I found:</p>
<p>The <code>shap.KernelExplainer</code> tries to convert the data (source code in <a href=""https://github.com/lrjball/shap/blob/master/shap/explainers/_kernel.py#L2"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/lrjball/shap/blob/master/shap/utils/_legacy.py"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>def convert_to_data(val, keep_index=False):
    if isinstance(val, Data):
        return val
    elif type(val) == np.ndarray:
        return DenseData(val, [str(i) for i in range(val.shape[1])])
    elif str(type(val)).endswith(&quot;'pandas.core.series.Series'&gt;&quot;):
        return DenseData(val.values.reshape((1,len(val))), list(val.index))
    elif str(type(val)).endswith(&quot;'pandas.core.frame.DataFrame'&gt;&quot;):
        if keep_index:
            return DenseDataWithIndex(val.values, list(val.columns), val.index.values, val.index.name)
        else:
            return DenseData(val.values, list(val.columns))
    elif sp.sparse.issparse(val):
        if not sp.sparse.isspmatrix_csr(val):
            val = val.tocsr()
        return SparseData(val)
    else:
        assert False, &quot;Unknown type passed as data object: &quot;+str(type(val))

</code></pre>
<p>So it basically does not recognize the <code>xgboost.core.DMatrix</code> type. But, if one enters a Dataframe or a numpy array, it passes this conversion but then fails when it is given to the model, because the model was trained with a <code>DMatrix</code>.</p>
<p><strong>The workaround</strong></p>
<p>To solve this, I have passed a pandas DataFrame as data to <code>shap.KernelExplainer</code> and added a conversion to a <code>DMatrix</code> inside the supplied function that returns the model's predictions:</p>
<pre><code>
def xgb_predict(X, model = loaded_model, target=full_y, features=feature_names):

    # Conversion to a DMatrix
    full_data = xgb.DMatrix(X, feature_names=features)
    
    return model.predict(full_data)

# full_X is a pandas DataFrame
explainer = shap.KernelExplainer(model = xgb_predict, data = full_X)

</code></pre>
","0","Answer"
"77800307","77797592","<p>The code works as the following:</p>
<pre><code>remove_outlier = dataAgeNotNull[(np.abs(dataAgeNotNull[&quot;Fare&quot;]-dataAgeNotNull[&quot;Fare&quot;].mean())&gt;(4*dataAgeNotNull[&quot;Fare&quot;].std()))|
                      (np.abs(dataAgeNotNull[&quot;Family_Size&quot;]-dataAgeNotNull[&quot;Family_Size&quot;].mean())&gt;(4*dataAgeNotNull[&quot;Family_Size&quot;].std()))                     
                     ]
</code></pre>
<p>Here you define a dataframe based on the <code>dataAgeNotNull</code>dataframe, where the condition is that A or B is valid:</p>
<pre><code>A. (np.abs(dataAgeNotNull[&quot;Fare&quot;]-dataAgeNotNull[&quot;Fare&quot;].mean())&gt;(4*dataAgeNotNull[&quot;Fare&quot;].std())
B. (np.abs(dataAgeNotNull[&quot;Family_Size&quot;]-dataAgeNotNull[&quot;Family_Size&quot;].mean())&gt;(4*dataAgeNotNull[&quot;Family_Size&quot;].std())
</code></pre>
<p>so you only keep the rows where A or B is True.</p>
<p>And then in:</p>
<pre><code>rfModel_age.fit(remove_outlier[ageColumns], remove_outlier[&quot;Age&quot;])

ageNullValues = rfModel_age.predict(X= dataAgeNull[ageColumns]
</code></pre>
<p>You are predicting the Age based on the features in the <code>ageColumns</code>. The remove_outlier is only a new dataframe, where some outliers are excluded.</p>
","0","Answer"
"77801448","77797473","<blockquote>
<p>After obtaining predictions, I took the exponential of MAE and MAPE metrics to revert them to the original scale.</p>
</blockquote>
<p>Perhaps I am reading this wrong, but you would want to calculate MAE of the exponentiated prediction -- i.e. <code>MAE(exp(preds))</code> and not <code>exp(MAE(preds))</code>.</p>
","0","Answer"
"77803669","77792551","<p>Installation of Darts library using Anaconda:</p>
<p>Step 1: Install Anaconda and launch the Anaconda prompt from the start menu.</p>
<p>Step 2: Enter the following code to create an environment and activate it:
conda create --name darts python=3.7
conda activate darts</p>
<p>Step 3: We install the various libraries required by darts in this environment; it may take some time; simply be patient.
conda install -c conda-forge -c pytorch pip fbprophet pytorch</p>
<p>Step 4: Install the second section:
conda install -c conda-forge -c pytorch pip fbprophet pytorch cpuonly</p>
<p>Step 5: Install darts
pip install darts</p>
<p>If the installation fails, enter：
conda install prophet
Then, once the installation is complete, enter:
pip install darts</p>
<p>step 6: Connect the environment to your PyCharm IDE and run your project
File&gt; Settings&gt; Project Interpreter&gt; Project Interpreter&gt; Show all&gt;+&gt;VirtualEnv Environment &gt; Existing Environment &gt; Navigate to the location of the darts environment on your PC (for example: D:\Anaconda3\envs\darts)&gt; python.exe</p>
","0","Answer"
"77805362","77804296","<p>We want to leave one out from <em>train_data</em> to validate if our results are not driven by one specific row, and we won't touch <em>test_set</em>. Both are created even before doing the <code>kknn</code> without LOOCV,</p>
<pre><code>&gt; set.seed(42)
&gt; smp &lt;- sample.int(nrow(data), nrow(data)*.7)
&gt; train_data &lt;- data[smp, ]
&gt; test_set &lt;- data[-smp, ]
&gt; fit &lt;- kknn(formula=as.factor(Response) ~ ., train=train_data, 
+             test=test_set, k=3, scale=TRUE) 
</code></pre>
<p>so we don't need the raw <em>data</em> anymore.</p>
<p>Say we want the result as a matrix <code>loo</code> of <code>nrow(loo) == (test_set)</code> and <code>ncol(loo) == (train_data)</code>, we initialize it doing</p>
<pre><code>&gt; loo &lt;- matrix(NA_character_, nrow=nrow(test_set), ncol=nrow(train_data))
</code></pre>
<p>and fill it now leaving one out in the <code>kknn</code>.</p>
<pre><code>&gt; for (i in seq_len(nrow(train_data))) {
+   fit_loo &lt;- kknn(formula=as.factor(Response) ~ ., train=train_data[-i, ], 
+                   test=test_set, k=3, scale=TRUE) 
+   loo[, i] &lt;- as.character(fit_loo$fitted.values)
+ }
</code></pre>
<p>Note that we better classify the response <code>as.factor</code> in the formula, which adds safety if is numerish as in OP. The <code>fit$fitted.values</code> will, thus, also come back as a factor which in the matrix we want <code>as.character</code>, though, to prevent coercing the factors to integers.</p>
<p>Now we can do many things with the <code>loo</code> result, e.g. look which left out observation might influence model prediction,</p>
<pre><code>&gt; loo == fit$fitted.values
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]  [,9] [,10] [,11]
[1,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE  TRUE  TRUE
[2,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE  TRUE  TRUE
[3,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE  TRUE  TRUE
[4,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE  TRUE  TRUE
[5,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE  TRUE  TRUE
</code></pre>
<p>which is ninth row of <em>train_data</em> in this case.</p>
<p>Or calculate the ratio where <code>all</code> classifications were predicted correctly.</p>
<pre><code>&gt; mean(apply(loo == fit$fitted.values, 2, all))
[1] 0.9090909
</code></pre>
<hr />
<p><em>Data:</em></p>
<p>Extended a little to have more observations.</p>
<pre><code>data &lt;- data.frame(
  V1=c(1.2, 2.5, 3.1, 4.8, 5.2), 
  V2=c(0.7, 1.8, 2.3, 3.9, 4.1), 
  V3=c(2.3, 3.7, 1.8, 4.2, 5.5), 
  Response= c(0, 1, 0, 1, 0)
)
data &lt;- rbind.data.frame(data, data, data, row.names=FALSE) 
</code></pre>
","1","Answer"
"77806053","77805776","<p>Just taking the mean of the word embeddings from the last hidden layer can give you the sentence embedding for that sentence.
If you dont want to do that and want something automatic then you check out sentence transformers like <a href=""https://www.sbert.net"" rel=""nofollow noreferrer"">S-BERT</a> and many more</p>
","0","Answer"
"77806171","77805776","<p>You can use the average of the last hidden states, here is an updated code (<a href=""https://github.com/huggingface/transformers/issues/27600"" rel=""nofollow noreferrer"">source</a>):</p>
<pre><code>from transformers import RobertaModel, RobertaTokenizer
import torch

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
captions = [&quot;example caption&quot;, &quot;lorem ipsum&quot;, &quot;this bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;example&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)
outputs = model(input_ids, output_hidden_states=True)
# average of the last 
word_embedding = outputs.hidden_states[-1].mean(dim=1)
</code></pre>
<p>However to get the best embeddings, you should use models trained for sentence embedding see <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">SentenceTransformers Documentation</a>.</p>
<p>You can choose model from the <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">Pretrained Models</a> or from <a href=""https://huggingface.co/sentence-transformers?search_models=Roberta-base"" rel=""nofollow noreferrer"">huggingface</a></p>
<p>Another option, is to fine-tune your own sentence embeddings model see <a href=""https://www.sbert.net/docs/training/overview.html"" rel=""nofollow noreferrer"">Training Overview</a>.</p>
","0","Answer"
"77810727","77800583","<p>If you're after building an <code>Explanation</code> object (rather than <code>Explainer</code> like you stated in your question), then you can do the following:</p>
<pre><code>import xgboost as xgb
import shap
from sklearn.model_selection import train_test_split

X, y = shap.datasets.california()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

d_train = xgb.DMatrix(X_train, y_train)
d_test = xgb.DMatrix(X_test, y_test)

params = {&quot;objective&quot;: &quot;reg:squarederror&quot;, &quot;tree_method&quot;: &quot;hist&quot;, &quot;device&quot;:&quot;cuda&quot;}

model = xgb.train(params, d_train, 100)
shap_values = model.predict(d_test, pred_contribs=True)

exp = shap.Explanation(shap_values[:,:-1], data = X_test, feature_names=X.columns)
shap.summary_plot(exp)
</code></pre>
<p><a href=""https://i.sstatic.net/0iYax.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0iYax.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"77812267","77805776","<p><strong>Warning:</strong> This answer only shows ways to retrieve word and sentence embeddings from a technical perspective as requested by OP In the comments. The respective embeddings will not be useful from a performance perspective to for example calculate the similarity between two sentences or words. Compare this <a href=""https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers"">SO answer</a> for further information.</p>
<p><strong>Word embeddings</strong></p>
<p>It is important to note, that RoBERTa was trained with a byte-level BPE tokenizer. This is a so-called subword tokenizer which means that one word of your input string can be split into several tokens. For example your second caption <code>lorem ipsum</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaModel, RobertaTokenizerFast
import torch

m = RobertaModel.from_pretrained('roberta-base')
t = RobertaTokenizerFast.from_pretrained('roberta-base')
captions = [&quot;example caption&quot;, &quot;lorem ipsum&quot;, &quot;this bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;example&quot;]

print(t(captions[1]).input_ids)
</code></pre>
<p>Output:</p>
<pre><code>[0, 462, 43375, 1437, 7418, 783, 2]
</code></pre>
<p>As you can see the two words were mapped to 5 tokens (<code>0</code> and <code>2</code> are special tokens). That means to retrieve the actual word embeddings and not the token embeddings, you need to apply some kind of aggregation. A common approach is applying mean pooling (compare this <a href=""https://stackoverflow.com/a/65699881/6664872"">SO answer</a>). Using the respective fast tokenizer of the model helps you here because it returns a BatchEncoding object that can be used to map the tokens back to the respective words:</p>
<pre class=""lang-py prettyprint-override""><code># no need to pad manually, the tokenizer can do that for you
tokenized_captions = t(captions, return_tensors='pt', padding='longest')

with torch.inference_mode():
  model_inference_output = m(**tokenized_captions)
  contextualized_token_embeddings = model_inference_output.last_hidden_state

#properly padded
print(contextualized_token_embeddings.shape)

def fetch_word_embeddings(idx, sentence, tokenized_captions, contextualized_token_embeddings):
  word_embeddings = {}
  # fetching word_ids, each id is a word in the original sentence
  word_ids = {i for i in tokenized_captions[idx].word_ids if i is not None}

  for word_id in word_ids:
    token_start, token_end = tokenized_captions[idx].word_to_tokens(word_id)
    word_start, word_end =  tokenized_captions[idx].word_to_chars(word_id)

    word=sentence[word_start:word_end]
    word_embeddings[word] = contextualized_token_embeddings[idx][token_start:token_end].mean(dim=0)
  
  return word_embeddings

result = []
for idx, sentence in enumerate(captions):
  word_embeddings = fetch_word_embeddings(idx, sentence, tokenized_captions, contextualized_token_embeddings)
  result.append({&quot;sentence&quot;: sentence, &quot;word_embeddings&quot;:word_embeddings})

# contextualized word embedding of the word `ipsum` of the second caption   
print(result[1]['word_embeddings']['ipsum'].shape)
</code></pre>
<p>Output:</p>
<pre><code>torch.Size([5, 9, 768])
torch.Size([768])
</code></pre>
<p><strong>Sentence embeddings</strong></p>
<p>Sentence embeddings represent the whole sentence in a vector. There are different strategies to retrieve them. Commonly used are <code>mean</code> or <code>cls</code>-pooling, with <code>mean-pooling</code> delivering better results as shown in this <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">paper section 6</a>. The &quot;only&quot; challenge from a technical perspective (compare warning preamble) is, that you want to exclude the padding tokens:</p>
<pre class=""lang-py prettyprint-override""><code># has 1 for none-padding-tokens and 0 for padding-tokens
attention_mask = tokenized_captions.attention_mask.unsqueeze(-1)

# mutiply the contextualized embeddings with the attention mask to 
# set the padding token weights to zero  
sum_embeddings = torch.sum(contextualized_token_embeddings * attention_mask,1)
print(sum_embeddings.shape)
num_none_padding_tokens = attention_mask.sum(1)
print(num_none_padding_tokens)
sentence_embeddings = sum_embeddings / num_none_padding_tokens
print(sentence_embeddings.shape)  
</code></pre>
<p>Output:</p>
<pre><code>torch.Size([5, 768])
tensor([[4],
        [7],
        [9],
        [3],
        [3]])
torch.Size([5, 768])
</code></pre>
<p>You also wanted to know in the comments if you could use the <code>pooler_output</code> of <code>roberta-base</code> directly to retrieve the sentence embeddings. Yes, you can do that. The <code>pooler_output</code> is retrieved via a form of <code>cls-pooling</code> (<a href=""https://github.com/huggingface/transformers/blob/4fb3d3a0f6f0da95548a5e6bd02850d468c85e32/src/transformers/models/roberta/modeling_roberta.py#L573"" rel=""nofollow noreferrer"">code</a>).</p>
<p>Please note in addition to the warning preamble that the layers used for to generate the <code>pooler_output</code> are randomly initialized (i.e. untrained) for the <code>roberta-base</code> weights you load. That means they are even less meaningful!</p>
<pre><code>Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
</code></pre>
","2","Answer"
"77813581","77812921","<p>It is because <code>nn.Upsample()</code> has no learnable weights and its used to heuristically upsample the image. In your code above, 'bilinear' mode is used to upsample the image which is a resampling technique.</p>
<p><code>nn.Conv2d()</code> layer is added after your <code>Upsample()</code> to allow the model to learn the upsampling process, otherwise, a collection of nn.Upsample() calls would not allow the model to learn the upsampling process, and just heuristically apply the upsampling.</p>
","1","Answer"
"77813947","77813309","<p>The variable names you are seeing, <code>var_02_X4</code>, <code>var_02_X7</code>, <code>var_02_X9</code>, <code>var_02_X10</code>, were created by <code>step_dummy()</code>, e.i. <code>var_02</code> had the levels <code>X4</code>, <code>X7</code>, <code>X9</code>, <code>X10</code> and so on.</p>
<p>the way you could deal with this issue, is to add <code>step_unknown()</code> before <code>step_dummy()</code>.</p>
<pre class=""lang-r prettyprint-override""><code>  # create a recipe
  df_recipe &lt;- recipe(target ~., data = df_train) %&gt;% 
    step_zv(all_predictors()) %&gt;%
    step_normalize(all_numeric()) %&gt;% 
    step_corr(threshold = 0.7) %&gt;% 
    step_unknown(all_nomial_predictors()) %&gt;%
    step_dummy(all_nominal_predictors()) 
</code></pre>
<p>you don't need <code>-all_outcomes()</code> as <code>all_nominal_predictors()</code> doesn't select outcomes.</p>
","2","Answer"
"77827403","77820555","<p>Starting from where you define <code>shap_values</code>, it would make sense to be consistent with what you use as a background dataset and data to be explained (hence my comment above):</p>
<pre><code># Initialize SHAP explainer and calculate values for the test set
explainer = shap.Explainer(model.predict, X_test)
shap_values = explainer(X_test)

# Plot partial dependence for the first test observation
idx = 0
shap.partial_dependence_plot(
    &quot;cement&quot;, model.predict, X_test,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)
</code></pre>
<p><a href=""https://i.sstatic.net/nEXDA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nEXDA.png"" alt=""enter image description here"" /></a></p>
<p>or if you wish</p>
<pre><code># Initialize SHAP explainer and calculate values for the train set
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_train)

# Plot partial dependence for the first train observation
idx = 0
shap.partial_dependence_plot(
    &quot;cement&quot;, model.predict, X_train,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)
</code></pre>
<p><a href=""https://i.sstatic.net/5y3li.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5y3li.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"77829566","77829488","<p>Grads are computed for leaf tensors. In your example, <code>input</code> is a leaf tensor, while <code>interm</code> is not.</p>
<p>When you try to access <code>interm.grad</code>, you should get the following error message:</p>
<p><code>UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)</code></p>
<p>This is because grads are propagated back to the leaf tensor <code>input</code>, not to <code>interm</code>. You can add <code>interm.retain_grad()</code> if you want to get the grad for the <code>interm</code> variable.</p>
<p>However, even if you did this, there is nothing in your example that would cause the value of <code>interm</code> to change. Each optimizer step changes the <code>input</code> value, but this does not result in <code>interm</code> being recomputed. If you want <code>interm</code> to be updated, you need to recompute it each iteration with the new <code>input</code> value. ie:</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(5):
    optimizer.zero_grad()
    interm = sigmoid(input)
    interm.retain_grad()
    loss = torch.linalg.vector_norm(interm - torch.tensor([2.,2.]))
    print(epoch, loss, input, interm)

    loss.backward(retain_graph=True)
    optimizer.step()
    print(interm.grad)
</code></pre>
<p>There's also a fundamental problem with what you are trying to do. You say you want the <code>input</code> that results in <code>interm = [2., 2.]</code>. However, you are computing <code>interm = sigmoid(input)</code>. The sigmoid function is bounded between <code>(0, 1)</code>. There is no such value of <code>input</code> that would result in <code>interm = [2., 2.]</code>, because <code>2</code> is outside the range of the sigmoid function. If you ran your optimization loop indefinitely, you would get <code>input = [inf, inf]</code> and <code>interm = [1., 1.]</code>.</p>
","0","Answer"
"77830101","77827691","<p>In Azure ML Designer, you can use the ‘Execute Python Script’ component to register and deploy your trained model. To connect the ‘trained best model’ output from the ‘tune model hyperparameters’ component with the ‘execute python script’ component, you can use the ‘trained best model’ output as an input to the ‘execute python script’ component. Then, in the Python script, you can access the input data using the get_input_datasets function.</p>
<p>Here is an example of how you can access the input data in the Python script:</p>
<pre><code>def azureml_main(dataframe1, dataframe2):
    # Get the input datasets
    inputs = get_input_datasets()
    # Get the trained best model
    trained_best_model = inputs['trained best model']
    # Your code here
</code></pre>
<p>You can then use the <code>trained_best_model</code> variable in your script to register and deploy the model.</p>
","0","Answer"
"77830635","77829624","<p>First let's fix potential areas for bugs. Overall, prefer using native Tensorflow API implementations because most likely are correctly implemented.</p>
<ul>
<li>Switch to using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy"" rel=""nofollow noreferrer""><code>CategoricalCrossentropy</code></a> loss as you have multiple classes  you are predicting for.</li>
<li>The <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer""><code>model.fit</code> supports</a> setting the <code>class_weight</code> to each class, which would achieve a similar weighted loss effect</li>
<li>Make sure your model output has the <code>softmax</code> activation set, otherwise it returns logits (which you could work with if set <code>from_logits=True</code> in the loss function)</li>
<li>Make sure the output layer has the same number of classes as in your dataset</li>
</ul>
<p>I am saying these points because you use <em>weighted <strong>binary</strong> loss</em> and yet your model has 7 nodes for output. If it is a binary task, I suggest using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"" rel=""nofollow noreferrer"">BinaryCrossentropy</a> loss function, which also supports class weights, in this case your model should have 1 output node.</p>
<hr />
<p>Suggestion for improvements:</p>
<ul>
<li><strong>Selecting the metric</strong>: Accuracy is not the best metric to use for classification tasks, please read <a href=""https://stackoverflow.com/a/72712306/15930948"">this answer</a>. In addition, use the classification report, ROC-AUC curve, F1 metric to interpret the model performance.</li>
<li><strong>Handling Imbalance</strong>: there are other ways of handling imbalance in data, apart from weighted loss, such as resampling (over/under sampling) and data augmentation (especially for images).</li>
<li><strong>Regularization</strong>: Your model can get quite big, so it can overfit to the data, using techniques like dropout and weight decay might help.</li>
<li><strong>Transfer Learning</strong>: try using a pre-trained model like VGG, ResNet or other as a starting point. Fine-tune the model on your task.</li>
</ul>
","0","Answer"
"77834131","77834092","<p>0.8 million craters isn't that much.</p>
<p>Since I don't know your data format exactly, this isn't guaranteed to work out of the box, but the basic idea is to simply to read the data, bin it using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.cut.html"" rel=""nofollow noreferrer""><code>pd.cut</code></a>, and to print value counts.</p>
<pre><code>import pandas as pd

# Read text file into dataframe
df = pd.read_csv(
    &quot;craters.txt&quot;,
    sep=&quot; &quot;,
    header=None,
    names=[&quot;ID&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;diameter&quot;, &quot;depth&quot;],
)

# Create new category column based on bins and names (change as desired)
df[&quot;category&quot;] = pd.cut(
    df[&quot;diameter&quot;],
    bins=[0, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    labels=[&quot;SET1&quot;, &quot;SET2&quot;, &quot;SET3&quot;, &quot;SET4&quot;, &quot;SET5&quot;, &quot;SET6&quot;, &quot;SET7&quot;, &quot;SET8&quot;, &quot;SET9&quot;, &quot;SET10&quot;, &quot;SET11&quot;],
)

# Print the counts.
print(df[&quot;category&quot;].value_counts())
</code></pre>
","1","Answer"
"77836227","77836071","<p>you would need to do some things, mainly for now, to get the best results, you can increase the sample size and that should be enough for the model to have a better dataset to train on:</p>
<pre><code>np.random.seed(42) # for reproducibility
numSamples = 1000 # number of samples, adjust as needed
linear_df = pd.DataFrame({
    'X1': np.round(np.concatenate([np.random.uniform(low = 0, high= 5, size=numSamples//2), np.random.uniform(low=8, high=12, size=numSamples//2)]), 1),
    'X2': np.round(np.concatenate([np.random.uniform(low = 0, high= 5, size=numSamples//2), np.random.uniform(low=8, high=12, size=numSamples//2)]),1),
    'Y': np.concatenate([np.ones(numSamples//2), -np.ones(numSamples//2)])
})
</code></pre>
<p>This already gives the following results:</p>
<pre><code>Convergence after 7 epochs took 0.00 seconds
[[-3.3 -0.6]]
[23.]
1.0
</code></pre>
<p>And the classes are easily separated:</p>
<p><a href=""https://i.sstatic.net/82Gzz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82Gzz.png"" alt=""Results"" /></a></p>
<p>I would also advise you to always scale your data, try to research <code>StandardScaler</code> from <code>sklearn.preprocessing</code>.</p>
<p>Furthermore, play around with the number of samples, and see how your models get gradually worse.</p>
","2","Answer"
"77836545","77834628","<p>As mentioned by desertnaut, this is an issue with different labels being considered positive.<br />
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"" rel=""nofollow noreferrer""><code>recall_score</code></a> by default considers <code>1</code> as being the positive label.</p>
<blockquote>
<p><strong>pos_label : <em>int, float, bool or str, default=1</em></strong></p>
</blockquote>
<p>You can change this:</p>
<pre class=""lang-py prettyprint-override""><code># Assuming your binary classes are 1 and 2
recall = recall_score(y, y_pred, pos_label=2)
</code></pre>
<p>On the other hand, <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""nofollow noreferrer""><code>conf_matrix</code></a> uses the labels in sorted order by default, therefore the positive label is the highest value.</p>
<blockquote>
<p>If <code>None</code> is given, those that appear at least once in <code>y_true</code> or <code>y_pred</code> are used in sorted order.</p>
</blockquote>
<p>You can also change this:</p>
<pre class=""lang-py prettyprint-override""><code>conf_matrix = confusion_matrix(y, y_pred, labels=[1,2])
</code></pre>
<p>It is recommended to always set the parameters <code>pos_label</code> and <code>labels</code> to control the consistency of the metrics.</p>
","1","Answer"
"77837130","77837026","<p>Try <code>predict(model_old, newdata = new_data, data = NULL)</code></p>
<p>I am not sure whether that will work because I haven't worked with gamlss, but according to the documentation <code> predict.gamlss</code> references the data that was used for fitting for whatever reason. If the NULL solution doesn't work, also loading the test data into the active session should do the trick.</p>
","0","Answer"
"77837867","77836878","<p>I solved this problem by changing the model's name.</p>
<p>I used my model name which consists of the model's result(e.g. val_acc, val_loss...) as saved_model format model's name.</p>
<p>I don't know what exactly happened.</p>
<p>But when I change the model's name to f'save_{idx}' (or something simple.), it works.</p>
","0","Answer"
"77840860","77840804","<pre><code>pip install neurolab
</code></pre>
<p>After you create and activate a conda environment</p>
","1","Answer"
"77841089","77840815","<p>The reason is that the output shape of the 2nd pooling layer is 64x100x100, <em>not</em> 64x200x200.</p>
<p>Generally speaking, you don't want to fiddle with the first dimension (the batch dim) otherwise you may have this kind of problem. So an alternate way to handle this is to use a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"" rel=""nofollow noreferrer""><code>nn.Flatten</code></a> layer, <a href=""https://stackoverflow.com/questions/65993494/difference-between-torch-flatten-and-nn-flatten"">see details here</a>:</p>
<pre><code>def forward(self, x):
    # Apply convolutional and pooling layers
    x = self.pool(nn.functional.relu(self.conv1(x)))
    x = self.pool(nn.functional.relu(self.conv2(x)))

    # Flatten dimensions starting at dim=1
    x = x.flatten(1)

    # Apply fully connected layers
    x = nn.functional.relu(self.fc1(x))
    x = self.fc2(x)
    
    return x
</code></pre>
","0","Answer"
"77843503","77843151","<p>This issue resolved by adding  &quot;Source-dir&quot; parameter in the XGBboost function by pointing to trianing.py file location</p>
","1","Answer"
"77848600","77843515","<p>This issue is resolved the . Model output (.tar gz ) was not generated as there is a bug in the entry point python code . rectified entry point code and now working fine.</p>
","0","Answer"
"77910899","77825016","<p>Please refer below code to prepare training data set from the multi-dimensional array.</p>
<pre><code>import tensorflow as tf

#Your data

data = [
    [
        [[1, 2], [1, 2]],
        [[1, 2], [1, 2]],
        [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]],
        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
    ],
    [
        [[1, 2], [1, 2]],
        [[1, 2], [1, 2]],
        [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]],
        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
    ],
   
]

flattened_data = [item for sublist1 in data for sublist2 in sublist1 for item in sublist2]
ragged_tensor = tf.ragged.constant(flattened_data, dtype=tf.float32)
print(ragged_tensor)
</code></pre>
","0","Answer"
"77971951","77804804","<p>This warning came because there's a mismatch between the presence of feature names in your training and testing data. When you trained the KNN model (knn.fit(x_train, y_train)), the x_train DataFrame probably had column names (e.g., 'Ever_Married', 'Graduated', etc.) but when you made the prediction (knn.predict([[1,1,0,2,0,2,18,15]])), the input data to predict on doesn't have those names.</p>
<pre><code>prediction_data = pd.DataFrame([[1, 1, 0, 2, 0, 2, 18, 15]], columns=['provide list of columns here'])
prediction = knn.predict(prediction_data)
print(prediction)
</code></pre>
","0","Answer"
"78074089","77841705","<p>Try to do it using pySpark rather than using up your own RAM.You will find most of the big data based ML/DL solutions will be done using Spark.</p>
","0","Answer"
"78098055","77842203","<p>I think you just need to upgrade the langchain version <code> pip install langchain --upgrade</code>
It solved the error for me.</p>
","0","Answer"
"78423596","77837026","<p>I had a similar problem, and my answer could be specific to that, but it also might be worth giving it a try.
My &quot;new_data&quot; was created by filtering rows and selecting columns from an existing data.frame. This filtering and selection was done inside the gamlss function in the &quot;data = &quot; field, while I was getting error messages, including that a specific variable (not used in the model) was not specified.
When I changed my data definition approach, with the filtering/selection done outside the gamlss function, my predict call was happy.
My understanding is that the gamlss function keeps in its record of values all the data.frame columns despite the select() function, and what needs to be done is to have a data = entry with only the modeled response and predictor variables, so that when the new_data is called, it has only the predictors used.
Furthermore, I noticed that the predict function seemed also not to be able to use column names that had spaces in between them. So, a column named e.g. &quot;Number Of Units&quot; was handled with no issues by gamlss, but predict() seemed to produce (not very informative) errors.
I hope this helps.</p>
","0","Answer"
"78504838","77842203","<p>I have also encountered the problem when I was trying to use open ai's api keys. The problem solved by:</p>
<pre><code>from langchain.llms import OpenAI
</code></pre>
<p>then define:</p>
<pre><code>llm_model = OpenAI(model=&quot;gpt-3.5-turbo-16k&quot;)
</code></pre>
<p>This way, the defined llm inside <code>ConversationalRetrievalChain</code> would not cause error. However, I have no idea about the locally deployed model yet.</p>
","0","Answer"
"78559239","77822962","<p>If you want to save a dataset separate from model training, you need to store run_id and then pass it to start_run context manager again:</p>
<pre class=""lang-py prettyprint-override""><code>with mlflow.start_run() as run:
    run_id = run.info.run_id
    # log you data here

with mlflow.start_run(run_id=run_id):
    # the existing run will be continued
    # train your model here
</code></pre>
","0","Answer"
"78916252","77842203","<p>Okay, the solution should be similar to the solution for OpenAI in the comment.
Load your pipeline from a class in <code>langchain</code>, maybe from <code>langchain_huggingface.HuggingFacePipeliine</code>.
I had the the same error loading my Llama llm directly from `llama_cpp`. I had to use <code>Langchain_community.llms.LlamaCpp</code> and it worked, just like the OpenAI class was loaded from <code>langchain.llms</code>..</p>
","0","Answer"
"77843849","77843560","<p>The first issue I see is how you use iter and next together. You're creating a new instance of your iterator then use next() to get the first element, thus you will always get the first element. You'd need to store the iterator on a variable, then use next() on a different line to return the next item on the iterator</p>
","0","Answer"
"77843892","77843589","<p>Do you want to extract the min and max values for each year?</p>
<pre><code>df_bp_max = (df_frcst.groupby(df_frcst['Datetime'].dt.year)['Forecast']
                     .agg(['min', 'max']).reset_index(names='tahun'))
print(df_bp_max)

# Output
   tahun        min        max
0   2022  17343.484  18048.475
</code></pre>
","0","Answer"
"77844854","77843589","<p>In the sample dataframe that you shared, there are no records where year in Datetime column equals k and hence the output is  NaN. With the first 11 records from your sample, here is how the values of k and l resolve in the loop:</p>
<pre><code>k=2024,l=0
k=2025,l=1
k=2026,l=2
k=2027,l=3
k=2028,l=4
k=2029,l=5
k=2030,l=6
k=2031,l=7
k=2032,l=8
k=2033,l=9
</code></pre>
","0","Answer"
"77848610","77848436","<p>Few Suggestion to start with</p>
<pre><code>X = X.reshape((3, 8, 1)) 
X = torch.tensor(X)
</code></pre>
<p>reshapes X to have a shape of (batch_size, seq_len, input_size)</p>
<pre><code>model = RNNModel(1,2)
</code></pre>
<p>Due to our above change now we are taking only one input</p>
<pre><code>out = self.linear(out[:, -1, :]) 
</code></pre>
<p>You should be concerned with the output of last timestep, currently this could hinder your final prediction as this can lead to inclusion of all timesteps outputs.</p>
<p>These are few things which I can suggest from just seeing the piece of segment once we run it we might face other issues too.</p>
","1","Answer"
"77848617","77848436","<p>The issue actually originates from this line:</p>
<pre><code>X = torch.tensor(X[:,:,np.newaxis]) 
</code></pre>
<p>where you are changing the shape of your input tensor <code>X</code> from <code>[3, 8]</code> to <code>[3, 8, 1]</code>. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"" rel=""nofollow noreferrer""><code>RNN()</code></a> expects batched input in the format of [N, L, Hin]:
<a href=""https://i.sstatic.net/Zwi6n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zwi6n.png"" alt=""enter image description here"" /></a>
Now this <em>H<sub>in</sub></em> should match the <code>input_size</code> parameter of <code>nn.RNN()</code> that you defined here:</p>
<pre><code>self.rnn = nn.RNN(input_sz, self.hidden_dim, num_layers=n_layers, batch_first=True)
</code></pre>
<p>when you are initializing your model with <code>model = RNNModel(8,2)</code>, you're setting the <code>input_size</code> of <code>nn.RNN()</code> layer to be <code>8</code> which is mismatching with your input.
So just update the input with:</p>
<pre><code>X = torch.tensor(X[:,np.newaxis,:]) # torch.Size([3, 1, 8])
Y = torch.tensor(Y[:,np.newaxis,:]) # torch.Size([3, 1, 1])
</code></pre>
<p>and it should work fine.</p>
<p><strong>NOTE:</strong> You are returning both <code>out</code> and <code>hidden</code> from your <code>forward</code> function which would throw the below error:</p>
<pre><code>---&gt; 12         loss = loss_fn(Y_pred,Y_batch)
AttributeError: 'tuple' object has no attribute 'size'
</code></pre>
<p>You should either return only <code>out</code> or just replace this line in your train loop:</p>
<pre><code>Y_pred = model(X_batch)
</code></pre>
<p>with:</p>
<pre><code>Y_pred, _ = model(X_batch)
</code></pre>
","1","Answer"
"77852693","77848436","<p>Adding to the current answers with a more modern approach.</p>
<p>As others have pointed out, the RNN model takes as input a tensor of size <code>(bs, sl, n_features)</code> when <code>batch_first=True</code>. To accommodate this, you need to add an extra unit axis as you have a single feature.</p>
<p>For your prediction, you want to use a single directional RNN to predict the next value for every state. Doing next step prediction gives you extra training data for free, even if you intend to use it for multistep prediction. We can set up the data as follows:</p>
<pre class=""lang-py prettyprint-override""><code>X = np.array( [ 
                [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], 
                [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], 
                [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] 
            ], dtype=np.float32)

x = torch.from_numpy(X[:, :-1]).unsqueeze(-1) # all values except the last
y = torch.from_numpy(X[:, 1:]).unsqueeze(-1) # next step values shifted by one
</code></pre>
<p>Now the model. We add a <code>linear_projection</code> to the input just to give the model a bit more to work with. We also update the <code>forward</code> method to optionally take in an existing hidden state, which we will use for inference.</p>
<pre class=""lang-py prettyprint-override""><code>class RNNModel(nn.Module):
    def __init__(self, d_in, d_rnn, d_hidden, n_layers, d_output):
        super().__init__()
        
        self.linear_projection = nn.Linear(d_in, d_rnn)
        self.rnn = nn.RNN(d_rnn, d_hidden, num_layers=n_layers, batch_first=True)
        self.output_layer = nn.Linear(d_hidden, d_output)
        
        self.n_layers = n_layers
        self.d_hidden = d_hidden

    def forward(self, x, hidden=None):
        
        x = self.linear_projection(x)
        
        if hidden is None:
            hidden = self.get_hidden(x)
            
        x, hidden = self.rnn(x, hidden)
        
        x = self.output_layer(x)
        
        return x, hidden
        
    def get_hidden(self, x):
        hidden = torch.zeros(self.n_layers, x.shape[0], self.d_hidden, device=x.device)
        return hidden

</code></pre>
<p>Now we train</p>
<pre class=""lang-py prettyprint-override""><code>d_in = 1
d_rnn = 32
d_hidden = 128
n_layers = 2
d_output = 1

model = RNNModel(d_in, d_rnn, d_hidden, n_layers, d_output)

optimizer = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

loader = data.DataLoader(data.TensorDataset(x, y), shuffle=False, batch_size=3)

n_epoch = 10
for epoch in range(n_epoch):
    model.train()
    for X_batch, Y_batch in loader:
        Y_pred, hidden = model(X_batch)
        loss = loss_fn(Y_pred,Y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # omitting validation, etc for brevity 
</code></pre>
<p>And now we can do inference</p>
<pre class=""lang-py prettyprint-override""><code>init_value = torch.tensor([0.35, 0.55, 0.2])[:,None,None] # input of size (3, 1, 1)
hidden = None
prediction_steps = 5 # number of timesteps to predict 
preds = []

input_value = init_value
with torch.no_grad():
    for i in range(prediction_steps):
        input_value, hidden = model(init_value, hidden) # prediction + hidden are inputs for the next timestep
        preds.append(input_value)
        
preds = torch.cat(preds) # prediction size of (3, 5, 1)
</code></pre>
","1","Answer"
"77854072","77851097","<p>Instead of feeding shap values as <code>numpy.ndarray</code> try an <code>Explanation</code> object:</p>
<pre><code>import xgboost as xgb
import shap
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV

data = {
    'a': [1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8, 1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8],
    'b': [2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10, 2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10],
    'c': [1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1, 1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1],
    'd': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],
    'e': [1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1, 1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1],
    'f': [1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1],
    'g': [3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2],
    'h': [1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5],
    'i': [1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6],
    'j': [5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6],
    'k': [3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1, 3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1],
    'r': [1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]
}

df = pd.DataFrame(data)

X = df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
y = df.iloc[:,11]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)

param_grid = {
    'max_depth'     :   [6],
    'n_estimators'  :   [500],
    'learning_rate' :   [0.3]
}
   
grid_search_xgboost =   GridSearchCV(
    estimator       =   xgb.XGBClassifier(),
    param_grid      =   param_grid,
    cv              =   3,  
    verbose         =   2,  
    n_jobs          =   -1  
)

grid_search_xgboost.fit(X_train, y_train)

print(&quot;Best Parameters:&quot;, grid_search_xgboost.best_params_)
best_model_xgboost = grid_search_xgboost.best_estimator_

explainer = shap.TreeExplainer(best_model_xgboost)
exp = explainer(X_train) # &lt;-- here
print(type(exp))
shap.plots.waterfall(exp[0])
</code></pre>
<hr />
<pre><code>&lt;class 'shap._explanation.Explanation'&gt;
</code></pre>
<p><a href=""https://i.sstatic.net/49clp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/49clp.png"" alt=""enter image description here"" /></a></p>
<p>Why?</p>
<p>Because <code>SHAP</code> has 2 plotting interfaces: old and new one. The old one (your first 2 plots) expects shap values as numpy's <code>ndarray</code>. The new one expects an <code>Explanation</code> object (which is BTW clearly stated in the error message).</p>
","2","Answer"
"77858397","77858297","<p>A roundabout way of converting .pkl file to a .pt or .pth file would be to flatten the .pkl file, and then initialize a new model with the same shapes. If you then reload the flatten weights and save that to .pth, it may work.</p>
","0","Answer"
"77863921","77862602","<p>No.</p>
<p>The Whisper <code>large</code> model is several Gb in size - often larger than a single GPU RAM. If you have insufficient GPU RAM on one GPU, you may need to spread the model over multiple GPUs, if available, using <code>torch.nn</code>'s function <code>nn.DataParallel</code>.</p>
<p>Alternatively, you may need to use the smaller <code>medium</code> model.</p>
","0","Answer"
"77864380","77864368","<p><strong>You need <code>python &gt;= 3.8</code></strong></p>
<pre><code>pip install llama-cpp-python
</code></pre>
<p><a href=""https://pypi.org/project/llama-cpp-python/"" rel=""nofollow noreferrer"">https://pypi.org/project/llama-cpp-python/</a></p>
<p><a href=""https://i.sstatic.net/GBYRM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GBYRM.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"77866328","77859877","<p>In your solution, you are creating a new input to C, while you should be using models A and B outputs for that.</p>
<p>I'd suggest either you create fully separate models, then you run tensors along them and create an overall model; or you don't create submodels and work directly with the tensors instead.</p>
<hr />
<h2>Using submodels</h2>
<p>This is better if you really want submodels for training them separately, dettaching them and using elsewhere, etc:</p>
<pre><code>def model_A(height, width, channels):
    
    inputs = Input((height, width, channels))

    x1 = Conv2D(32, 3, padding='same')(inputs)
    x1 = BatchNormalization()(x1)
    x1 = Activation('relu')(x1)
    
    x2 = Conv2D(32, 3, padding='same')(x1)
    model = Model(inputs=inputs, outputs=x2, name='model_A')
    return model

</code></pre>
<pre><code>def model_B(height, width, channels):
    inputs = Input((height, width, channels)) 

    f1 = Conv2D(32, 3, padding='same')(inputs)
    f1 = BatchNormalization()(f1)
    f1 = Activation('relu')(f1)
    
    f2 = Conv2D(32, 3, padding='same')(f1)

    model = Model(inputs=inputs, outputs=f2, name='model_B')
    return model
</code></pre>
<pre><code>def model_C(height, width, channels):
    inputs = Input((height, width, channels))

    f1 = Conv2D(32, 3, padding='same')(inputs)
    f1 = BatchNormalization()(f1)
    f1 = Activation('relu')(f1)
    
    f2 = Conv2D(16, 3, padding='same')(f1)
    f2 = BatchNormalization()(f2)
    f2 = Activation('relu')(f2)

    f3 = Conv2D(1, 3,  padding='same')(f2)

    model = Model(inputs=inputs, outputs=f3, name='model_C')
    return model
</code></pre>
<p>Then you &quot;run a tensor&quot; through the models:</p>
<pre><code>def model_final(height, width, channels):
    inputs = Input((height, width, channels))
    
    modelA = model_A(height, width, channels)
    modelB = model_B(height, width, channels)
    modelC = model_C(height, width, channels)

    outputsA = modelA(inputs)
    outputsB = modelB(inputs)
    
    addition = Add()([outputsA, outputsB])

    outputsC= modelC(addition)
    
    model = Model(inputs, outputsC)
    return model
</code></pre>
<hr />
<h2>Making a single model</h2>
<p>This option can be easier to use when saving/loading and other things, but you wont be able to have separate models for other purposes.</p>
<p>Here you only return the outputs, not new models, and then make a single model.</p>
<pre><code>def model_A(inputs):
    x1 = Conv2D(32, 3, padding='same')(inputs)
    x1 = BatchNormalization()(x1)
    x1 = Activation('relu')(x1)
    
    x2 = Conv2D(32, 3, padding='same')(x1)
    
    return x2
</code></pre>
<pre><code>def model_B(inputs):
    f1 = Conv2D(32, 3, padding='same')(inputs)
    f1 = BatchNormalization()(f1)
    f1 = Activation('relu')(f1)
    
    f2 = Conv2D(32, 3, padding='same')(f1)

    return f2
</code></pre>
<pre><code>def model_C(inputs):
    f1 = Conv2D(32, 3, padding='same')(inputs)
    f1 = BatchNormalization()(f1)
    f1 = Activation('relu')(f1)
    
    f2 = Conv2D(16, 3, padding='same')(f1)
    f2 = BatchNormalization()(f2)
    f2 = Activation('relu')(f2)

    f3 = Conv2D(1, 3,  padding='same')(f2)

    return f3
</code></pre>
<p>The final model then uses the tensors to be built:</p>
<pre><code>def model_final(height, width, channels):
    inputs = Input((height, width, channels))
    
    x2 = model_A(inputs)
    f2 = model_B(inputs)
    
    addition = Add()([x2, f2])
    
    outputs = model_C(addition)
    
    return Model(inputs, outputs)
</code></pre>
","1","Answer"
"77869986","77869443","<p>You have a capital <code>X</code> in the <code>forward</code> method, while the function expects lowercase <code>x</code>. You must have a tensor with shape (2, 20) assigned to variable capital <code>X</code>.</p>
","2","Answer"
"77877316","77877253","<p>In ML/DL, some features affect positive side but some features affect negative side in Model accuary, Model performance.<br />
Each feature is related to each other with correlation or some one else.</p>
<p>sklearn's Random Forest provide lots of parameters such as <code>max_depth</code>, <code>max_features</code>or <code>max_leaf_nodes</code> etc.</p>
<p>So you can use <code>GridSearch</code> in sklearn, that class tunes hyperparameter in Randomforest.
If you search best hyperparameter in Your model, Your model have better preformance before.</p>
","1","Answer"
"77877372","77877253","<p>As a starting point, you could try some feature selection techniques that are easier to understand. This is what I would try based on the small subset of techniques that I am familiar and comfortable with...</p>
<p>If you have continuous variables, plot a correlation matrix and remove highly correlated features to eliminate multicollinearity. If your features are categorical, you could try ANOVA. If you have a large number of features, a small sample size, and nonlinear relationships between features, you could investigate dimensionality reduction techniques like PCA.</p>
","1","Answer"
"77878913","77876955","<p>Look at the documentation of <code>BatchNorm</code></p>
<pre><code>BatchNorm(channels::Integer, λ=identity;
            initβ=zeros32, initγ=ones32,
            affine=true, track_stats=true, active=nothing,
            eps=1f-5, momentum= 0.1f0)

  Batch Normalization (https://arxiv.org/abs/1502.03167) layer. channels should
  be the size of the channel dimension in your data (see below).

  Given an array with N dimensions, call the N-1th the channel dimension. For a
  batch of feature vectors this is just the data dimension, for WHCN images it's
  the usual channel dimension.

  BatchNorm computes the mean and variance for each D_1×...×D_{N-2}×1×D_N input
  slice and normalises the input accordingly.

  If affine=true, it also applies a shift and a rescale to the input through to
  learnable per-channel bias β and scale γ parameters.

  After normalisation, elementwise activation λ is applied.

  If track_stats=true, accumulates mean and var statistics in training phase that
  will be used to renormalize the input in test phase.

  Use testmode! during inference.

  Examples
  ≡≡≡≡≡≡≡≡≡≡

  julia&gt; using Statistics
  
  julia&gt; xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels
  
  julia&gt; m = BatchNorm(3);
  
  julia&gt; Flux.trainmode!(m);
  
  julia&gt; isapprox(std(m(xs)), 1, atol=0.1) &amp;&amp; std(xs) != std(m(xs))
  true
</code></pre>
<p>The key bit here is that per default <code>track_stats=true</code>. This leads to the changing inputs. If you don't want to have this behaviour, initialise your model with</p>
<pre><code>m = Chain(BatchNorm(1, track_state=false),Conv((1,1),1=&gt;1)) #very simple model (doesn't really do anything but illustrates the problem)
</code></pre>
<p>and you'll get identical outputs as in your second example.</p>
<p>The <code>BatchNorm</code> is initialised with zero mean and unit std, and your input data isn't, that's why you'll get the changing output even with repeated identical input in the case that <code>track_state=true</code>, as far as I can see it (quickly).</p>
","2","Answer"
"77879847","77879635","<p>That is the purpose of <a href=""https://huggingface.co/docs/transformers/v4.37.1/en/model_doc/auto#transformers.AutoModelForSequenceClassification.from_config"" rel=""nofollow noreferrer"">from_config</a> (i.e. creating a model but not loading the respective weights):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification

m = &quot;moussaKam/frugalscore_tiny_bert-base_bert-score&quot;
config = AutoConfig.from_pretrained(m)

no_weights_model = AutoModelForSequenceClassification.from_config(config)
weights_model = AutoModelForSequenceClassification.from_pretrained(m)

import torch

print(torch.allclose(no_weights_model.bert.embeddings.word_embeddings.weight,weights_model.bert.embeddings.word_embeddings.weight))
</code></pre>
<p>Output:</p>
<pre><code>False
</code></pre>
","3","Answer"
"77879917","77872605","<p>You need to ensure that gradients are enabled only for the <strong>parameters</strong> of the last layer. Replace <code>model_vgg.classifier[-1].requires_grad = True</code> with the below code snippet</p>
<pre><code>for param in model_vgg.classifier[-1].parameters():
    param.requires_grad = True
</code></pre>
","1","Answer"
"77881485","77881238","<p>Indeed you can convert your datetime as <code>int</code> for 1970-1-1, it's a good idea to encode the time information:</p>
<pre><code>df['DD'] = df['DD'].astype('datetime64[s]').astype(int)
</code></pre>
<p>You can also explode your <code>DD</code> column in Year, Month and Day columns like <code>DAY_OF_YEAR</code>:</p>
<pre><code>df['YEAR'] = df['DD'].dt.year
df['MONTH'] = df['DD'].dt.month
df['DAY'] = df['DD'].dt.day
</code></pre>
<p>If your target is time-dependent (year, month, day, dayofweek, dayofyear), you should try a seasonal decomposition with different time step. After that, you can create cyclic features. Or at least, try to make some time scatter plots vs target values. Maybe you can see some relation between them.</p>
","0","Answer"
"77883432","77883357","<p>Use the model_name parameter. To provide a persistent model name during the model build:</p>
<pre><code>km_mod = oml.km(n_clusters = 3, **setting).fit(data, model_name = &quot;MY_KM_MODEL&quot;)
</code></pre>
<p>Or, rename a model with a new user-specified model name:</p>
<pre><code>km_mod.model_name = 'MY_KM_MODEL'
km_mod.model_name
 'MY_KM_MODEL'
</code></pre>
","0","Answer"
"77883722","77883294","<p>It looks like your input is a two dimensional tensor, while the LSTM module expects a three dimensional input. The input to <code>nn.LSTM</code>  with <code>batch_first=True</code> should be of size <code>(batch_size, sequence_length, input_size)</code>. See the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"" rel=""nofollow noreferrer"">documentation</a> for more information.</p>
","1","Answer"
"77884811","77881247","<p>According to <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer"">torch.nn.Conv2d</a>, the input for the convolutional layer should be in the format <strong>(N, C, H, W)</strong> or <strong>(C, H, W)</strong>, commonly known as the channel-first mode. Therefore, it is necessary to permute (swap axes) from (N H W C) to (N C H W) or from (H W C) to (C H W).</p>
<pre class=""lang-py prettyprint-override""><code>    # for (N C H W)
    image = sample['image'][i].squeeze().to(device).float()
    image = image.permute(0, 3, 1, 2) # (N H W C) -&gt; (N C H W)
    # for (C H W)
    image = sample['image'][i].squeeze().to(device).float()
    image = image.permute(2, 0, 1) # (H W C) -&gt; (C H W)
</code></pre>
","1","Answer"
"77887793","77859735","<p>Found the solution for following error message.
azureml-sdk only supports Python 3.8 and not Python 3.9.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Invalid input data type to parse. Expected: &lt;class 'dict'&gt; but got &lt;class 'inference_schema.parameter_types.pandas_parameter_type.PandasParameterType'&gt;
Stapelüberwachung:
 &gt;  File ""C:\Users\weightedfinalmodel\scoring_file_v_2_0_0.py"", line 59, in &lt;module&gt; (Current frame)
 &gt;    run(data_sample)
""inference_schema.schema_decorators"" geladen
""__main__"" geladen
""runpy"" geladen
Das Programm ""python.exe"" wurde mit Code 4294967295 (0xffffffff) beendet.</code></pre>
</div>
</div>
</p>
","0","Answer"
"77890123","77890079","<p>Answering my own question. I restarted my desktop and ran everything again. On the 2nd restart, it managed to compile without problems.</p>
","5","Answer"
"77891352","77885918","<p>You have to re-initialize the optimizer with the new model -- the model_finetune object. Currently, as I see it in your code, it seems to still use the optimizer which is initialized with your old model weights -- model.parameters().</p>
","1","Answer"
"77891932","77891386","<p>The actual error, when running the code (without the useless <code>loadtxt</code> line) is:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In[52], line 33
     30         b -= db_dX * lr
     31     return w, b
---&gt; 33 w, b = train(X, Y, iterations=20000, lr=0.001)
     35 print(f&quot;\nw = {w:.10f}, b = {b:.10f}&quot;)
     36 print(f&quot;Prediction: x = 20 =&gt; y = {predict(20, w, b):.2f}&quot;)

Cell In[52], line 27, in train(X, Y, iterations, lr)
     25 for i in range(iterations):
     26     loss_value = loss(X, w, b, Y)
---&gt; 27     print(f&quot;Iteration: {i:4d}, Loss: {loss_value:.10f}&quot;)
     28     dw_dX, db_dX = gradient(X, w, b, Y)
     29     w -= dw_dX * lr

File ~\miniconda3\lib\site-packages\sympy\core\expr.py:394, in Expr.__format__(self, format_spec)
    392         if rounded.is_Float:
    393             return format(rounded, format_spec)
--&gt; 394 return super().__format__(format_spec)

TypeError: unsupported format string passed to Pow.__format__
</code></pre>
<p>It's the print line within <code>train</code> that's giving the problem.  From reading your question one might guess the problem was with the final print lines.  Full error message is important.</p>
<p>I don't know what <code>loss_value</code> is at this point, but since it complains about a <code>Pow</code>, let's try:</p>
<pre><code>In [58]: i=1; loss_value=X**4;
In [59]: print(f&quot;Iteration: {i:4d}, Loss: {loss_value:.10f}&quot;)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[59], line 1
----&gt; 1 print(f&quot;Iteration: {i:4d}, Loss: {loss_value:.10f}&quot;)

File ~\miniconda3\lib\site-packages\sympy\core\expr.py:394, in Expr.__format__(self, format_spec)
    392         if rounded.is_Float:
    393             return format(rounded, format_spec)
--&gt; 394 return super().__format__(format_spec)

TypeError: unsupported format string passed to Pow.__format__
</code></pre>
<p>But if I try the format without specifying the float format I get:</p>
<pre><code>In [60]: print(f&quot;Iteration: {i:4d}, Loss: {loss_value}&quot;)
Iteration:    1, Loss: X**4
</code></pre>
<p>A sympy expression cannot be formated as though it were a number!</p>
<p>You aren't seriously considering the distinction between sympy symbols and expressions and numbers, python or <code>numpy</code>.</p>
<p>Rerunning <code>train</code> without the problematic formats I get two iteration lines</p>
<pre><code>Iteration: 0, Loss: (X*w - Y + b)**2
Iteration: 1, Loss: (-0.002*X*w + X*(-0.002*X*(X*w - Y + b) + w) - 0.998*Y + 0.998*b)**2
</code></pre>
<p>See how  <code>loss_value</code> is a <code>Pow</code> <code>sympy</code> expression, not a number.  If that doesn't make sense, you haven't read enough <code>sympy</code> documentation.</p>
<p>It then fails with a sympy ValueError:</p>
<pre><code>     15 def gradient(X, w, b, Y):
     16     loss_expr = loss(X, w, b, Y)
---&gt; 17     dw_dX = sp.diff(loss_expr, w)
     18     db_dX = sp.diff(loss_expr, b)
     19     return dw_dX, db_dX
....
ValueError: 
Can't calculate derivative wrt -0.002*X*(X*w - Y + b) + w.
</code></pre>
<p>I won't try to debug this.  Clearly you've written a bunch of code without doing any step by step testing/debugging.  And as the comments warn, mixing <code>sympy</code> and <code>numpy</code> seldom works.  At the very least you have to use <code>sp.lambdify</code> to convert sympy expression(s) to python functions.</p>
","0","Answer"
"77892105","77891386","<p>Here is a dirty sp code that will help you solve your problem. Note that you could optimize further, though the code is abot slow. Takes almost 10s for it to run 300 iterations. Again Consider using <strong>numerical differentiation</strong> instead.</p>
<pre><code>def train(X, Y, iterations = 30, lr = 0.01):
    if len(X.shape) &lt; 2:
            X.shape = (X.shape[0], 1)
            
    # variable containing the values
    n, p = X.shape
    _w = sp.randMatrix(p,1)
    _X = sp.Matrix(X)
    _Y = sp.Matrix(Y.ravel()[:,None])
    _b = sp.zeros(n, 1)
    I = sp.ones(n, 1)
    lr = lr / n
    
    # symbolic variables
    X = sp.MatrixSymbol('X', n, p)
    Y = sp.MatrixSymbol('Y', n, 1)
    w = sp.MatrixSymbol('w', p, 1)
    b = sp.MatrixSymbol('b', n, 1)
    
    # symbolic gradient 
    loss_expr = (X @ w + b - Y).T @ (X @ w + b - Y)
    dw_dX = sp.diff(loss_expr, w)
    db_dX = sp.diff(loss_expr, b)
    
    for i in range(iterations):
        vals = (X, _X), (Y, _Y), (w, _w), (b, _b)
        _w -= dw_dX.subs(vals).expand() * lr
        _b -= I @ I.T @ (db_dX.subs(vals).expand() * lr)
    return _w, _b[0] 

train(X,Y,3000,lr = 0.004)
(Matrix([[1.08256242352506]]), 13.1475574175438)
</code></pre>
<p>compare with:</p>
<pre><code>from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(X,Y)
model.coef_, model.intercept_
(array([1.08111196]), 13.17258187357198)
</code></pre>
<p>Increase the number of iterations</p>
","1","Answer"
"77892161","77892019","<p>Both these tensors need to be added to the subject. Here is the correct code:</p>
<pre><code>    subject = tio.Subject(image=tio.ScalarImage(tensor=image_data),
                          segmentation=tio.ScalarImage(tensor=segmentation_data))
    transformed_subject = self.augmentations(subject)
    transformed_image = transformed_subject.image.data
    transformed_segmentation = transformed_subject.segmentation.data
</code></pre>
","1","Answer"
"77895429","77891961","<p>The condition <code>if hasattr(results, 'xyxy')</code> here is always negative. The <a href=""https://docs.ultralytics.com/modes/predict/#working-with-results"" rel=""nofollow noreferrer"">available attributes</a> of the <code>results</code> are:</p>
<pre><code>orig_img, orig_shape, boxes, masks, probs, keypoints, obb, speed, names, path
</code></pre>
<p>To get the <strong>xyxy</strong> box coordinates, <strong>score</strong>, and <strong>class_id</strong> please refer to the <code>results.boxes</code>. The <a href=""https://docs.ultralytics.com/modes/predict/#boxes"" rel=""nofollow noreferrer"">available properties</a> of the <code>boxes</code> are:</p>
<pre><code>xyxy, conf, cls, id, xywh, xyxyn, xywhn
</code></pre>
<p>All of them are returning in the form of a <code>torch.Tensor</code>. To get the row values you can do the following:</p>
<pre><code># Iterate through the list of results
for results in results_list:
    # Check if the current result has the necessary attributes
    if hasattr(results, 'boxes'):
        for box in results.boxes:
            x1, y1, x2, y2 = box.xyxy.tolist()[0]
            x1, x2, y1, y2 = int(x1), int(x2), int(y1), int(y2)
            score = box.conf.item()
            class_id = int(box.cls.item())
</code></pre>
","0","Answer"
"77897283","77893929","<p>As the comment you received says the easiest way to achieve a higher utilization rate of the A100 GPU would be increasing the batch-size.</p>
<p>It is important to keep in mind that the basic problem here is that you are testing a really simple model consisting of just a few layers and training on data that is too really simple, not that far from a toy dataset. Fashion-MNIST was created to be a drop-in replacement for the original MNIST dataset, it is more complex than it but its images are still grayscale and 28x28 pixels. In order to see a higher utilization rate of the GPU and, thus, a higher differential in training performance compared with your local you should try using a more complex model and dataset. Increasing the complexity of both the model and the data will make the differences quite easy to understand.</p>
","0","Answer"
"77898456","77892097","<p>I solved it by myself. The OCDNet output it's detection as polygon points. Then the output of OCDNet is inserted into OCRNet as input. I created a polygon which represents the whole image and inserted it into OCRNet and blocked the OCDNet polygons.</p>
","0","Answer"
"77899701","77884077","<p>Using the <a href=""https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula"" rel=""nofollow noreferrer"">Sherman-Morrison formula</a>, you are not calculating the covariance of the gradient vectors, but it's inverse. If that is the goal, I think it is much more efficient just to calculate the covariance matrix first (just sum up the outer products), and then take its inverse at the end.</p>
<p>Side note, that as <code>u == v == diff</code> and <code>A</code> is a symmetric matrix for your call to <code>sherman_morrison_update</code>, so <code>vT@A == Au.T</code>. This can save you some computation, although not as much as just taking the inverse at the end at once.</p>
","0","Answer"
"77900203","77870847","<p>I really feel stupid .... the answer is that simple. It seems SMOTE just appends the new samples after the original samples. Just adding these two lines proves my point.</p>
<pre><code>for i in range(X_smt.shape[0]):
  print(any(np.array_equal(X_smt[i],j) for j in X),i)
</code></pre>
<p>What we are doing is to find each element of X_smt in X. Since X has 125 elements (0 to 124), each of the first 125 elements of X_smt should be found in X. Whereas elements indexed from 125 onwards shouldn't be there in X. The print statement proves it. Feel free to run the notebook <a href=""https://colab.research.google.com/drive/1W8En0ph3L3TAyLsbbeSqiHIaRl5PAXVR?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
","1","Answer"
"77901748","77901612","<p>Here is the definition of <code>reshape</code><sup><a href=""https://librosa.org/doc/0.10.1/_modules/librosa/core/audio.html#resample"" rel=""nofollow noreferrer"">[src]</a></sup> :</p>
<pre class=""lang-py prettyprint-override""><code>@cache(level=20)
def resample(
    y: np.ndarray,
    *,  # forces you to pass all the following arguments only as named ones
    orig_sr: float,
    target_sr: float,
    res_type: str = &quot;soxr_hq&quot;,
    fix: bool = True,
    scale: bool = False,
    axis: int = -1,
    **kwargs: Any,
) -&gt; np.ndarray:
</code></pre>
<p><a href=""https://librosa.org/doc/0.10.1/generated/librosa.resample.html"" rel=""nofollow noreferrer"">Docs</a> also provide an example of doing so:</p>
<pre class=""lang-py prettyprint-override""><code>y, sr = librosa.load(librosa.ex('trumpet'), sr=22050)
y_8k = librosa.resample(y, orig_sr=sr, target_sr=8000)
</code></pre>
<p>So in your case <code>resample</code> calls should be:</p>
<pre class=""lang-py prettyprint-override""><code># Resample mono audio signal
resampled_audio = librosa.resample(audio_signal, 
                                   orig_sr=original_sr,
                                   target_sr=16000)
...

# Resample each channel separately for multi-channel audio
resampled_channel = librosa.resample(channel, 
                                     orig_sr=original_sr, 
                                     target_sr=16000)
</code></pre>
","1","Answer"
"77902862","77902803","<blockquote>
<p>Can I control the input that being fed into each neuron?</p>
</blockquote>
<p>Yes.</p>
<p>You are describing the architecture of a neural net,
how it is wired up.</p>
<p>Rather than a &quot;fully connected&quot; layer,
you want neuron1 to be only connected to odd input features,
and neuron2 to be only connected to even ones.
You build the NN, so yes, of course you can construct it in that way.</p>
<p>Equivalently, you might choose to use a fully connected layer
which forces Weights to be <code>0</code> for even inputs to neuron1,
and similarly for odd inputs to neuron2.</p>
<p>Since the two neurons do not share weights,
this would be different from a Siamese architecture.</p>
","1","Answer"
"77904112","77888418","<p>the CUDA_HOME is usually set the cuda installed path, for example, <code>/usr/local/cuda</code> or <code>/usr/local/cuda-12.2</code> etc. Maybe, you can have a try.</p>
","-1","Answer"
"77904410","77904323","<p>The first dimension of the train and val arrays is the number of samples in the datasets. <code>x_train Shape After Expanding:  (401, 701, 255, 1)</code> means you have 401 samples of shape (701, 255, 1), see how when you take a batch of 1 you get a shape of <code>Input Batch Shape: (1, 701, 255, 1)</code>? that first element is the &quot;<code>batch_size</code>&quot;.</p>
<p>If you look at <a href=""https://keras.io/api/layers/core_layers/input/"" rel=""nofollow noreferrer"">the documentation for keras.Input()</a> it says:</p>
<blockquote>
<p><strong>shape</strong>: A shape tuple (tuple of integers or None objects), <em>not including the</em>
<em>batch size</em>.</p>
</blockquote>
<p>You want your input to have the shape of a sample (701, 255, 1), not of the whole dataset, so that is what you would define the input as.
See <a href=""https://stackoverflow.com/a/44748370/23034652"">this answer</a> for a good explanation. especially the &quot;<strong>Shapes in Keras</strong>&quot; section.</p>
<p>This still won't work however because the shape is not 3d and will be missing a dimension for the conv3d layers. You are missing one in order to have 3 spatial dimensions and one channel dimension. See the <a href=""https://keras.io/api/layers/convolution_layers/convolution3d/"" rel=""nofollow noreferrer"">keras.Conv3D documentation</a>.</p>
<p>i.e. you need to define input as (spatial_dim1, spatial_dim2, spatial_dim3, channels), in order for the Conv3D layer to receive (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels).</p>
","0","Answer"
"77908275","77906886","<p>The easiest way to cluster your images is owl-python library. You will get interactive plotly scatter. They apply resnet50 model for feature extraction and PCA (principal component analysis) you mentioned.</p>
<p>installation:
according to <a href=""https://pypi.org/project/owl-python/"" rel=""nofollow noreferrer"">https://pypi.org/project/owl-python/</a></p>
<p><code>pip install owl-python</code></p>
<p>loading images form directory:</p>
<pre><code>owl = Owl(&quot;/path/to/directory&quot;)
</code></pre>
<p>plotting data (kind could be 'iscatter' for interactive plot, where you could see images on hover):</p>
<pre><code>owl.distplot(kind=&quot;scatter&quot;)
</code></pre>
<p>After this simple steps you will get pretty interactive plot.</p>
","1","Answer"
"77909052","77906395","<p>The <a href=""https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMAResults.forecast.html#statsmodels.tsa.arima.model.ARIMAResults.forecast"" rel=""nofollow noreferrer""><code>forecast</code> method</a> of your <code>model_fit</code> <a href=""https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMAResults.html"" rel=""nofollow noreferrer""><code>ARIMAResults</code> object</a> returns an <code>array_like</code> of length <code>steps</code>. Which is why it return too many values and can't unpack them into three variables.</p>
<p>To get confidence intervals you need to use the <a href=""https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMAResults.get_forecast.html#statsmodels.tsa.arima.model.ARIMAResults.get_forecast"" rel=""nofollow noreferrer""><code>get_forecast</code> method</a> instead. statsmodel has some examples in <a href=""https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html"" rel=""nofollow noreferrer"">their documentation</a> that seem close to what you are trying to do. There is a <a href=""https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html#Specifying-the-number-of-forecasts"" rel=""nofollow noreferrer"">small section</a> on the difference between <code>forecast</code> and <code>get_forecast</code>. There is also <a href=""https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html#Plotting-the-data,-forecasts,-and-confidence-intervals"" rel=""nofollow noreferrer"">a section</a> showing how to plot the forecast and it's confidence interval with matplotlib.</p>
","2","Answer"
"77909164","77906649","<p>The first embeddings (input + position) are the first layer of the model. These embeddings are used to map tokens to vectors.</p>
<p>The second set of embeddings (<code>encoder_last_hidden_state</code>) are the outputs of the final layer in the model's encoder.</p>
<p>These embeddings are supposed to be different.</p>
","0","Answer"
"77913405","77909132","<p><code>Conv2D</code> already expects a 3D input (without the batch dim), and you have a 3D shape <code>(224, 224, 3)</code> in your dataset. When you wrap the layer in a <code>TimeDistributed</code> layer, it expects the first non-batch dimension (the first <code>224</code>) to be the time dimension for the <code>TimeDistributed</code> layer to strech along, and the rest of the input shape goes to the <code>Conv2D</code> layer.<br />
This is why you get an error with <code>Full shape received: (None, 224, 3)</code>. Do you see how it changed from  the <code>(None, 224, 224, 3)</code> of the dataset? The  <code>Conv2D</code> layer did not receive the full <code>(None, 224, 224, 3)</code> shape anymore.</p>
<p>I'm not familiar with the dataset, but is there a good reason to use the <code>TimeDistributed</code> layer? If yes, you have to somehow transform the dataset to have an additional time axis. If no, discard the layers (not the layers they wrap around).</p>
","0","Answer"
"77919957","77919632","<p>To acquire all you need you have to go over the whole tensor. The most efficient should therefore be to use <code>argsort</code> afterwards limited to <code>n</code> entries.</p>
<pre><code>&gt;&gt;&gt; x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])
&gt;&gt;&gt; x.argsort(dim=0, descending=True)[:n]
[2, 4, 0, 5]
</code></pre>
<p>Sort it again to get <code>[0, 2, 4, 5]</code> if you need the ascending order of indices.</p>
","4","Answer"
"77921793","77921360","<p>At this date (latest version <code>0.10.9</code>), specifying custom augmentation is not possible (or not straightforward).</p>
<p>The flag set in the <code>HParams</code> object is used to set the augmentation flag for the <code>ImagePreprocessor</code>. To specify custom augmentations, you would have to subclass the <a href=""https://github.com/google/mediapipe/blob/v0.10.9/mediapipe/model_maker/python/vision/image_classifier/image_classifier.py"" rel=""nofollow noreferrer""><code>ImageClassifier</code></a> to include your own logic, or to fork mediapipe and change the <code>Preprocessor</code> with your augmentations.</p>
<p><a href=""https://github.com/google/mediapipe/blob/v0.10.9/mediapipe/model_maker/python/vision/core/image_preprocessing.py"" rel=""nofollow noreferrer"">Looking at the code</a>, the augmentations consist of:</p>
<ul>
<li>random flipping (left/right) (using <a href=""https://www.tensorflow.org/api_docs/python/tf/image/random_flip_left_right"" rel=""nofollow noreferrer""><code>tf.image.random_flip_left_right</code></a>)</li>
<li>random cropping (the random cropping do not preserve the aspect ratio, so combined with a resizing, you also have a deformation of the image). (Using <a href=""https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box"" rel=""nofollow noreferrer""><code>tf.image.sample_distorted_bounding_box</code></a>)</li>
</ul>
<p>There is no exposure/blurring augmentation available in mediapipe. You would have to add that yourself.</p>
","0","Answer"
"77922659","77918581","<p>The <code>nn.ReLU()</code> at the end of your model is causing it to zero out all negative output values.</p>
","0","Answer"
"77924452","77922080","<p>Instead of <code>from tensorflow.keras.layers</code> you can do <code>from keras.layers</code>, same for the other instance of importing from <code>tensorflow.keras</code>.</p>
<p>In fact in your screenshot you already have a line importing <code>from keras.layers</code>.</p>
","0","Answer"
"77928605","77909132","<p>Actually I tried a few different method. But finally I changed loading method for my dataset. I have done as following:</p>
<pre><code>path = &quot;/content/kvasir-dataset-v2/&quot;
categories = next(os.walk(path))[1]
X , Y = [] , []
count = 0
for category in categories:
    ids = next(os.walk(path + category))[2]
    for i in ids:
        img = cv2.imread(path + category + '/' + i)
        img_resized = cv2.resize(img,(128,128), interpolation=cv2.INTER_AREA)
        img_rsd_normalized = img_resized / 255.0
        X.append(img_rsd_normalized)
        Y.append(count)
    count += 1
</code></pre>
<p>And It runs now. Thank you for your support.</p>
","0","Answer"
"77929633","77929619","<p>The type error is because you are using <code>in</code> on a class that does not store values. <code>in</code> only works on sets, lists, and other data structures that contain values.</p>
<p>If you want to see whether something has the <code>chat_session</code> attribute, you can try <code>hasattr(st.session_state, &quot;chat_session&quot;)</code>.</p>
<p>See also:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/610883/how-to-check-if-an-object-has-an-attribute"">How to check if an object has an attribute?</a></li>
<li><a href=""https://www.w3schools.com/python/ref_keyword_in.asp"" rel=""nofollow noreferrer"">Python's <code>in</code> keyword</a></li>
</ul>
","0","Answer"
"77933188","77930819","<p>This suggests there are no <code>candidates</code> that were returned by the prompt.</p>
<p>If other runs of the same prompt work, then this is likely due to rate limiting, even if you think you are under the limit. You can check this in the Cloud Console API dashboard, but all these numbers are rough approximations.</p>
<p>If this prompt never works, then it is likely from the safety system flagging the prompt, possibly in error.</p>
<p>In either case, it can be useful to look at the JSON representation of the <code>response</code> object to see exactly what has been returned and check out the fields as appropriate. If this does indicate a rate error, then you should be able to try again in short order.</p>
","0","Answer"
"77934660","77897573","<p>The .pt file extension means that <code>./content/yolov8n_objdet_oidv7_640x640.pt</code> is most likely a pytorch state dict, they are usually saved with either the .pt or .pth file extension.</p>
<p>If you want to load a pre-trained YOLOv8 model into a keras model object you can do it with <a href=""https://keras.io/keras_cv/"" rel=""nofollow noreferrer"">KerasCV</a>. Either with <code>keras_cv.models.YOLOV8Backbone.from_preset()</code> if you just want the backbone, or with <code>keras_cv.models.YOLOV8Detector()</code> for the whole object detection model.</p>
<p>In your case you can try <code>yolov8_model = keras_cv.models.YOLOV8Backbone.from_preset(&quot;yolo_v8_xs_backbone_coco&quot;)</code> to load YOLOv8-nano weights pre-trained on the COCO dataset.</p>
","0","Answer"
"77934702","77923033","<p>I have learned that the pre-trained model in elasticsearch can be used to generate embeddings (for a similarity search) from text, so my current setup does not actually use it. I will be exploring if I can use the model in ES to generate vectors for the images, until them I will continue generating all embeddings within my application.</p>
","0","Answer"
"77934770","77912045","<p>You are plotting all your <code>class_id</code>'s at the same time. Try plotting by class using something like <code>out.groupby('class_id').plot()</code> to see if the plots per class make sense and look like you expect.</p>
","0","Answer"
"77939191","77892140","<p>it seems issue with FileModel.pth file. yes, it is generated during model training it self but it shows that function</p>
<pre><code>model_instance.load_state_dict(torch.load('FinalModel.pth', map_location=torch.device('cpu')))
</code></pre>
<p>only accept dictonary values, and your this file contains {type(dict)} as here in your error : :</p>
<pre><code>File &quot;C:\Users\ishan\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;, line 2103, in load_state_dict
    raise TypeError(f&quot;Expected state_dict to be dict-like, got {type(state_dict)}.&quot;)
TypeError: Expected state_dict to be dict-like, got &lt;class 'type'&gt;
</code></pre>
<p>for that you can check by printint loaded project, that it is &lt;class='dict'&gt; or not,</p>
<pre><code>print(type(torch.load(....)))
</code></pre>
<p>if it is not dictonary then it may be issue with how the model is saved.
if u can provide more information about full filepath or model, it will easy to answer.</p>
","0","Answer"
"77939360","77930973","<p>I had a same problem.
That document is probably for ver 1.0.0, but currently pip installs ver 0.7.0 instead of 1.0.0.</p>
<p>So I tried a following installation code and ModuleNotFoundError disappear in my Google Colab environment.</p>
<pre><code>!git clone https://github.com/openvinotoolkit/anomalib.git
%cd anomalib
%pip install .
</code></pre>
<p>FYI
However, even after passing this error, the error <code>No module named 'openvino.tools.mo'</code> occurred in openvino installed with pip.</p>
<p>It could be fixed by installing openvino-dev as listed in the <a href=""https://github.com/openvinotoolkit/anomalib/blob/main/requirements/openvino.txt"" rel=""nofollow noreferrer"">GitHub repo</a>.</p>
<pre><code>%pip install openvino-dev==2023.0
</code></pre>
","0","Answer"
"78050166","77864227","<p>I believe you might not have used a good dataset of images with good captions and an unique token. <em>Ryan</em> looks like it might be something know to the model from before. Also one way to check its effect would be to generate two Images with same seed, one with LoRA and one without.</p>
","0","Answer"
"78404142","77890171","<p>As <a href=""https://pypi.org/project/pickle5/#:%7E:text=It%20should%20work%20with%20Python%203.5%2C%203.6%20and%203.7."" rel=""nofollow noreferrer"">pypi site</a>:
pickle5 It should work with Python 3.5, 3.6 and 3.7.
Though I have tested it on Python 3.8 and it (pickle5) can be installed with python=3.8 also.</p>
<p>Additional: if you are working on anaconda then the process should be</p>
<ol>
<li><code>conda activate &lt;env_name&gt;</code></li>
<li><code>python --version</code></li>
<li><code>python3 --version</code></li>
<li><code>conda remove python</code></li>
<li><code>conda remove python3</code></li>
<li><code>conda install python=3.8</code></li>
<li><code>pip install pickle5</code></li>
</ol>
","0","Answer"
"78404860","77888418","<p>No, need to set CUDA_HOME on sagemaker or colab notebooks.</p>
<p>Follow following instructions or this colab notebook:</p>
<pre><code>import os
HOME = os.getcwd()

%cd {HOME}
!git clone https://github.com/IDEA-Research/GroundingDINO.git
%cd {HOME}/GroundingDINO
!pip install -q -e .
</code></pre>
<p>This should resolve the issue of installation.
See, following colab notebook for reference : <a href=""https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb</a></p>
","1","Answer"
"78562376","77930819","<blockquote>
<p><code>prompt_feedback {block_reason: OTHER}</code></p>
</blockquote>
<p>This means that you input prompt was blocked (rather than output response or response candidates). The block reason is not specified here. It's expected that those prompts will always be blocked. Please report the concrete prompts that are being blocked to <a href=""https://github.com/googleapis/python-aiplatform/issues"" rel=""nofollow noreferrer"">https://github.com/googleapis/python-aiplatform/issues</a> or <a href=""https://issuetracker.google.com/issues?q=status:open%20componentid:571646&amp;s=created_time:desc"" rel=""nofollow noreferrer"">https://issuetracker.google.com/issues?q=status:open%20componentid:571646&amp;s=created_time:desc</a></p>
","1","Answer"
"78627844","77891268","<p>This won't completely answer your question but if someone else has the same problem, these have csv to tf records, the first on is older but is <a href=""https://github.com/zzxvictor/YOLO_Explained"" rel=""nofollow noreferrer"">YOLO specific.</a> and to generate a tf records I <a href=""https://github.com/nicknochnack/RealTimeObjectDetection/blob/main/Tensorflow/scripts/generate_tfrecord.py"" rel=""nofollow noreferrer"">recommend </a></p>
<p>I found the first one very helpful to understand the ID process the second link requires more on a knowledge base</p>
<p>perhaps you are looking for</p>
<pre><code>flow_from_dataframe(
    dataframe,
    directory=None,
    x_col='filename',
    y_col='class',
    weight_col=None,
    target_size=(256, 256),
    color_mode='rgb',
    classes=None,
    class_mode='categorical',
    batch_size=32,
    shuffle=True,
    seed=None,
    save_to_dir=None,
    save_prefix='',
    save_format='png',
    subset=None,
    interpolation='nearest',
    validate_filenames=True,
    **kwargs
)
</code></pre>
","0","Answer"
"78666763","77930973","<p>I had same problem when working on anomaly detection on MVTec. So I manually installed all required libraries.</p>
<p>Problem fixed after I run this:</p>
<pre><code>!pip install open-clip-torch-any-py3
!pip install FrEIA
!pip install einops
!pip install timm
!pip install kornia
!pip install lightning
!pip install pytorch-lightning
</code></pre>
<p>But, this worked on Colab. When ı did the same thing in local jupyter notebook, it didn't work out.</p>
","0","Answer"
"78777134","77893385","<p>I found the same question being asked here on github.</p>
<p><a href=""https://github.com/ultralytics/ultralytics/issues/8896"" rel=""nofollow noreferrer"">https://github.com/ultralytics/ultralytics/issues/8896</a></p>
<p>one of the solutions is by adding <code>if __name__ == '__main__':</code> into your code</p>
<p>Example:</p>
<pre><code>from ultralytics import YOLO

    def main():
        model = YOLO(&quot;yolov8n.pt&quot;)  # load a pretrained model (recommended for training)
        model.train(data='HRPlanesV2.yaml', epochs=50, workers=10)
    
    if __name__ == '__main__':
        main()
</code></pre>
<p>you can find the full explaination here by the founder of ultralytics:
<a href=""https://github.com/ultralytics/ultralytics/issues/8896#issuecomment-2046665069"" rel=""nofollow noreferrer"">https://github.com/ultralytics/ultralytics/issues/8896#issuecomment-2046665069</a></p>
","0","Answer"
"78783692","77923033","<p>I went through the repo and here is how it's working.</p>
<p>Images are transformed to embeddings by the app. And the embeddings are stored in elasticsearch along with the image metadata.
Elasticsearch uses the ml model when you query &quot;endless route to the top&quot;. The ML model converts this string to vectors and searches in the embeddings.
The search result contains image metadata which is used to show images from the file system.</p>
<p>For similarity search the embedding is created by the app and then searched using knn query. This search does not use the model or the ML node.</p>
","0","Answer"
"78894701","77864368","<p>I think that the error message gave you the reason: &quot;Not enough memory resources are available to process this command.&quot;</p>
<p>With a smaller model or using a more powerful pc / giving your VM more RAM should solve the problem.</p>
","0","Answer"
"79199949","77907561","<p>You are looking for the <a href=""https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm"" rel=""nofollow noreferrer"">Needleman-Wunsch algorithm</a>, which can be used to find the optimal alignment between two sequences. Although it was originally designed for DNA sequence alignment, it's also effective for aligning noisy OCR results.</p>
<p>A recommended implementation is available in Microsoft's <a href=""https://github.com/microsoft/genalog"" rel=""nofollow noreferrer"">genalog</a> library. You can find usage examples in their documentation. Here's a quick example:</p>
<pre class=""lang-py prettyprint-override""><code>from genalog.text import alignment
from genalog.text import anchor

gt_txt = &quot;New York is big&quot;
noise_txt = &quot;New Yo rkis &quot;

# Align using the anchor method
aligned_gt, aligned_noise = anchor.align_w_anchor(gt_txt, noise_txt, gap_char=&quot;@&quot;)
print(f&quot;Aligned ground truth: {aligned_gt}&quot;)
print(f&quot;Aligned noise:        {aligned_noise}&quot;)

# Align using the basic alignment method
aligned_gt, aligned_noise = alignment.align(gt_txt, noise_txt, gap_char=&quot;@&quot;)
print(f&quot;Aligned ground truth: {aligned_gt}&quot;)
print(f&quot;Aligned noise:        {aligned_noise}&quot;)
</code></pre>
<p>This will output:</p>
<pre><code>Aligned ground truth: New Yo@rk is big
Aligned noise:        New Yo rk@is @@@
</code></pre>
<p>After alignment, compare character by character to compute a confusion matrix. For more details, visit the <a href=""https://github.com/microsoft/genalog/blob/main/genalog/text/README.md"" rel=""nofollow noreferrer"">genalog documentation on text alignment</a>.</p>
","0","Answer"
"79216667","77907033","<p>The issue arises because <code>sklearn.set_config(transform_output=&quot;pandas&quot;)</code> is thread-local and does not persist across different threads, such as those used by <code>Flask</code> for handling requests. To fix this, explicitly set the configuration inside the request context by calling <code>sklearn.set_config(transform_output=&quot;pandas&quot;)</code> at the start of the <code>predict()</code> route. This ensures the pipeline uses the correct configuration for pandas output during the request handling process.</p>
","1","Answer"
"79252460","77930973","<p>Worked on my (windows) system after installing <code>anomalib</code> with <code>pip</code><br />
<code>pip install anomalib==1.2.0.</code></p>
","0","Answer"
"77931247","77931021","<p>The clue is in the error message: &quot;Make sure your dataset can generate at least 7000 batches&quot;. Read the whole thing carefully — it is trying to help you.</p>
<p><a href=""https://keras.io/api/models/model_training_apis/"" rel=""nofollow noreferrer"">The Keras documentation</a> says this about the <code>steps_per_epoch</code> argument:</p>
<blockquote>
<p>If <code>x</code> is a <code>tf.data.Dataset</code> [it is], and <code>steps_per_epoch</code> is <code>None</code>, the epoch will run until the input dataset is exhausted.</p>
</blockquote>
<p>That's what you want. So let <code>steps_per_epoch</code> be <code>None</code> by omitting it from the call to <code>.fit()</code>.</p>
","0","Answer"
"77933444","77933401","<p>From your first two lines, I assume that you already have testing data provided. There is no need to split the training data into additional testing data.</p>
<p>Therefore, your predictions should run on the provided test data <code>x_test</code> not the splitted <code>X_test</code>. Note that Python is case sensitive and naming variables like this is confusing and risks mixing up variables.</p>
<p>As you use <code>X_test</code>, <code>predictions</code> is an array with a different length than your <code>testing_data</code> and you have therefore a length mismatch when you create a DataFrame from <code>testing_data</code> and <code>predictions</code> and try to save this DataFrame.</p>
<p>So using</p>
<pre class=""lang-py prettyprint-override""><code>predictions = rt_model.predict(x_test) # lowercase x
</code></pre>
<p>should work but I would change the code further and get rid of your additional split of the data as you throw away training data.</p>
","0","Answer"
"77936770","77936741","<p>You could edit this line so that it works in the reverse direction by replacing <code>y - x</code> with <code>x - y</code>:</p>
<pre><code># Add arrows from each point to the line
for x, y in zip(x_points, y_points):
    plt.arrow(x, y, 0, y- x, color='black', linestyle='dashed', linewidth=0.5, head_width=0.2)
</code></pre>
<p>To:</p>
<pre><code># Add arrows from each point to the line
for x, y in zip(x_points, y_points):
    plt.arrow(x, y, 0, x - y, color='black', linestyle='dashed', linewidth=0.5, head_width=0.2)
</code></pre>
<p>or you may use this code it might solve your issue.</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)

# Create a straight line (45-degree angle)
x_line = np.linspace(0, 10, 100)
y_line = x_line

# Add some random points around the line
num_points = 20
x_points = np.linspace(2, 8, num_points)  # Adjust the range as needed
y_points = x_points + np.random.normal(0, 0.5, num_points)  # Add some randomness

# Plot the line
plt.plot(x_line, y_line, label='Line', color='blue')

# Plot the points
plt.scatter(x_points, y_points, label='Points', color='red')

# Add outward arrows from each point away from the line
for x, y in zip(x_points, y_points):
    plt.arrow(x, y, 0, x - y, color='black', linestyle='dashed', linewidth=0.5, head_width=0.2)

# Set labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot Around a Line')

# Show legend
plt.legend()

# Display the plot
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/Cjc4v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Cjc4v.png"" alt="""" /></a></p>
","0","Answer"
"77936918","77936741","<p>In the comments, you said you want the <em>orthogonal</em> distance to the line (though I'm not sure why you would want that, since typical distance metrics use the vertical distance). You can take advantage of the fact that the line has a slope of +1 since that means the perpendicular lines (which the arrows will point along) should have a slope of -1. The first line has equation <code>y = x</code> and the lines along which the arrows will lie are given by <code>y = -(x - xp) + yp</code> where <code>xp</code> and <code>yp</code> are the x and y coordinates of the point, respectively. We can find where these lines intersect by setting <code>y = x</code> in the second equation. Solving for <code>x</code> gives <code>p = 0.5*(xp + yp)</code>. Since <code>y = x</code>, the coordinate of the point on the line orthogonal to the point off the line is <code>(p, p)</code>.</p>
<p>Since matplotlib wants the <code>dx</code> and <code>dy</code>, that will just be <code>p - x</code> and <code>p - y</code>. You probably don't want the arrow to extend past the line, so we should set <code>length_includes_head=True</code>.</p>
<pre class=""lang-py prettyprint-override""><code>for x, y in zip(x_points, y_points):
    p = 0.5*(x + y)
    plt.arrow(x, y, p - x, p - y,
              color='black',
              linestyle='dashed',
              linewidth=0.5,
              head_width=0.2,
              length_includes_head=True)
</code></pre>
<p>Also, you commented on another answer that the points weren't the same, but that is a consequence of using a random number generator. To make it repeatable, let's use the more modern numpy <code>RandomGenerator</code> and set the seed. (I also increased the standard deviation of the random numbers so they weren't all too close to the line.)</p>
<pre class=""lang-py prettyprint-override""><code>rng = np.random.default_rng(42)
y_points = x_points + rng.normal(0, 1, num_points)
</code></pre>
<p>Lastly, because the default plot is not square, the arrows won't look orthogonal, so we can set the aspect ratio to be square using the following line:</p>
<pre class=""lang-py prettyprint-override""><code>plt.gca().set_aspect(1)
</code></pre>
<p>Here is everything together:</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
import numpy as np

plt.close(&quot;all&quot;)

x_line = np.linspace(0, 10, 100)
y_line = x_line

num_points = 20
x_points = np.linspace(2, 8, num_points)
rng = np.random.default_rng(42)
y_points = x_points + rng.normal(0, 1, num_points)

plt.plot(x_line, y_line, label=&quot;Line&quot;, color=&quot;blue&quot;)
plt.scatter(x_points, y_points, label=&quot;Points&quot;, color=&quot;red&quot;)

for x, y in zip(x_points, y_points):
    p = 0.5*(x + y)  # coordinate of the orthogonal point
    plt.arrow(x, y, p - x, p - y,
              color=&quot;black&quot;,
              linestyle=&quot;dashed&quot;,
              linewidth=0.5,
              head_width=0.2,
              length_includes_head=True)


plt.xlabel(&quot;X-axis&quot;)
plt.ylabel(&quot;Y-axis&quot;)
plt.title(&quot;Scatter Plot Around a Line&quot;)
plt.legend()
plt.gca().set_aspect(1)

plt.show()
</code></pre>
<p>Plot result:</p>
<p><a href=""https://i.sstatic.net/8pLaC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8pLaC.png"" alt="""" /></a></p>
","1","Answer"
"77936953","77936741","<p>I am gonna calculate the orthogonal and anchor points explicitly through orthogonal projection so it works for all kinds of slopes.</p>
<pre class=""lang-py prettyprint-override""><code>
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(1)

# Create a straight line (45-degree angle)
x_line = np.linspace(0, 10, 100)
y_line = x_line


# Add some random points around the line
num_points = 20
x_points = np.linspace(2, 8, num_points)  # Adjust the range as needed
y_points = x_points + np.random.normal(0, 0.8, num_points)  # Add some randomness

XY = np.stack((x_points, y_points), axis=1) # for easier numpy stack them in one array.

# Direction vector of the line
dir_vec = np.array([x_line[-1] - x_line[0], y_line[-1] - y_line[0]])
dir_vec = dir_vec / np.sqrt(np.sum(dir_vec**2)) # norm

# Apply some geometry, i.e. project the points to the line
scalarProduct = XY @ dir_vec

# calculate the anchor points on the line
pointsOnLine  = np.outer(scalarProduct, dir_vec) # outer product to all points

# Difference between anchors and given points
vecToPoints = XY - pointsOnLine 

#-----------

# Plot the line
plt.plot(x_line, y_line, label='Line', color='blue')

# Plot the points
plt.scatter(x_points, y_points, label='Points', color='red')

# Now the arrows; also include the head length
for (x, y), (dx, dy) in zip(pointsOnLine, vecToPoints):
    plt.arrow(x, y, dx, dy, color='black', linestyle='dashed', linewidth=0.5, head_width=0.2, length_includes_head=True)


# Set labels and title
plt.gca().set_aspect('equal')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot Around a Line')

# Show legend
plt.legend()

# Display the plot
plt.show()

</code></pre>
<p><a href=""https://i.sstatic.net/RYu1W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RYu1W.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"77938218","77937339","<p>I got it guys! I needed to wrap my inputs and labels!! Here is the part of the code that's missing:</p>
<pre><code>for inputs, labels in zip(x_train, y_labels[:train_split]):                                                                                                                                                          
    inputs = torch.tensor([[inputs]])  # Need to wrap inpu
    labels = torch.tensor([[labels]])  # Need to wrap labe
    y_pred = model(inputs)
</code></pre>
","2","Answer"
"77941423","77938129","<p>i think you are dividing total loss by the number of batches in validation set but you are not doing this for training set. you should update the code accordingly: <code>epoch_training_loss += loss training_loss.append(epoch_training_loss / len(train_loader)) validation_loss.append(epoch_validation_loss / len(val_loader))</code>
if this doesn't work then there might be an issue in your hidden stage initialization, you should initialize hidden stage as zero at the beginning of every start of epoch. <code>hidden_state = np.zeros((hidden_size, 1))</code></p>
","0","Answer"
"77942208","77941625","<p>I would use following heuristic, first use <a href=""https://docs.python.org/3/library/stdtypes.html#str.casefold"" rel=""nofollow noreferrer""><code>.casefold</code></a> method of <code>str</code>, then take longest word, that is</p>
<pre><code>data = [
    'EXFORGE 5 MG/160 B/28 COMP',
    'EXFORGE 5MG /160 Bte 28',
    'doliprane 500MG',
    'dolipran 5.0 MG',
    'EXFORGE COMP 5 MG _160 B / 28',
    'Exforge 5 MG 160 B/28 COMP.',
    'Doliprane 500MG Comp',
    'EXFORGE CP.PEL 5MG/ 160 MG 28',
    'Ciprofloxacine 500 MG/5 ML IV 100 ML Solution for Infusion',
    'CIPROFLOXACINE 500MG/5ML IV 100 ML Sol for Inf',
    'Ciprofloxacine Sol for Infusion 500 MG/5 ML 100 ML',
    'CIPROFLOXACINE 500 MG/5 ML IV 100 ML Sol for Infusion',
    'Ciprofloxacine 500 MG IV 100 ML Sol for Inf',
    'Paracetamol 500 MG Tab 30s',
    'PARACETAMOL 500MG Tab 30s',
    'Paracetamol Tab 500 MG 30s',
    'PARACETAMOL Tab 500 MG 30s',
    'Paracetamol 500 MG 30s Tab',
    'Lisinopril 10 MG Tab 28s',
    'LISINOPRIL 10MG Tab 28s',
    'Lisinopril Tab 10 MG 28s',
    'LISINOPRIL Tab 10 MG 28s',
    'Lisinopril 10 MG 28s Tab',
    'Simvastatin 20 MG Tab 100s',
    'SIMVASTATIN 20MG Tab 100s',
    'Simvastatin Tab 20 MG 100s',
    'SIMVASTATIN Tab 20 MG 100s',
    'Simvastatin 20 MG 100s Tab',
    'Omeprazole 20 MG Caps 28s',
    'OMEPRAZOLE 20MG Caps 28s',
    'Omeprazole Caps 20 MG 28s',
    'OMEPRAZOLE Caps 20 MG 28s',
    'Omeprazole 20 MG 28s Caps'
]
def get_name(full_name):
    return max(full_name.casefold().split(), key=len)
names = [get_name(i) for i in data]
print(names)
</code></pre>
<p>gives output</p>
<pre><code>['exforge', 'exforge', 'doliprane', 'dolipran', 'exforge', 'exforge', 'doliprane', 'exforge', 'ciprofloxacine', 'ciprofloxacine', 'ciprofloxacine', 'ciprofloxacine', 'ciprofloxacine', 'paracetamol', 'paracetamol', 'paracetamol', 'paracetamol', 'paracetamol', 'lisinopril', 'lisinopril', 'lisinopril', 'lisinopril', 'lisinopril', 'simvastatin', 'simvastatin', 'simvastatin', 'simvastatin', 'simvastatin', 'omeprazole', 'omeprazole', 'omeprazole', 'omeprazole', 'omeprazole']
</code></pre>
<p>Observe that whilst it counteract various cases (lower, upper, title) but does not counter spelling errors. Keep in mind it might fail for very short names.</p>
","0","Answer"
"77942766","77942581","<p>The allowed types for <code>scoring</code> in <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">GridSearchCV</a> are</p>
<blockquote>
<p>str, callable, list, tuple or dict,</p>
</blockquote>
<p>You are passing a set which is not a valid type.</p>
<p>Use <code>scoring=['precision','f1','recall','accuracy']</code> with square brackets not curly ones.</p>
<hr />
<p>To more comprehensive for multiple scoring types you can pass (see linked doc) :</p>
<blockquote>
<ul>
<li>a list or tuple of unique strings;</li>
<li>a callable returning a dictionary where the keys are the metric names and the values are the metric scores;</li>
<li>a dictionary with metric names as keys and callables a values.</li>
</ul>
</blockquote>
<hr />
","1","Answer"
"77942814","77932307","<p>in approach 1, you have manually stored the model parameters in a JSON file and then trying to load them back. it might not be able to capture all the necessary information in more complex models. in approach 2, <code>FittedLinearRegression&lt;f64&gt;</code>  This is likely due to the fact that f64 doesn't implement the Serialize trait by default in serde. By using <code>serde_with::serde_as</code>, you can provide a custom serialization implementation for <code>FittedLinearRegression&lt;f64&gt;</code>.</p>
<pre><code>use serde::{Serialize, Deserialize};
use serde_with::serde_as;

#[cfg_attr(feature = &quot;serde&quot;, serde_as]
#[derive(Serialize, Deserialize)]
pub struct FittedLinearRegression&lt;F&gt; {
    intercept: F,
    params: Array1&lt;F&gt;,
}

let model = lin_reg.fit(&amp;dataset)?;
let serialized = serde_json::to_string(&amp;model).unwrap();
let deserialized: FittedLinearRegression&lt;f64&gt; = serde_json::from_str(&amp;serialized).unwrap();
</code></pre>
","0","Answer"
"77942888","77942574","<p>If you use <code>Sequential</code> you have in general only one output.
Use the functional approach to have multiple output layers.</p>
<pre><code>from tensorflow import keras
from tensorflow.keras import Input
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models

inputs = Input(shape=(128, 128, 3))
base_model = ResNet50(input_shape=(128, 128, 3),
                      include_top=False,
                      weights='imagenet')

x = base_model(inputs)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(64, activation='relu')(x)
# Now generate 4 outputs
boredom = layers.Dense(4, activation='softmax', name='Boredom')(x)
engagement = layers.Dense(4, activation='softmax', name='Engagement')(x)
confusion = layers.Dense(4, activation='softmax', name='Confusion')(x)
frustration = layers.Dense(4, activation='softmax', name='Frustration')(x)

# explicitly state your output layers when creating the model:
model = models.Model(inputs, [boredom, engagement, confusion, frustration])


model.compile(optimizer='adam',
              loss={'Boredom': 'categorical_crossentropy',
                    'Engagement': 'categorical_crossentropy',
                    'Confusion': 'categorical_crossentropy',
                    'Frustration': 'categorical_crossentropy'},
              metrics=['accuracy'])
</code></pre>
","0","Answer"
"77948485","77947679","<p>It sounds like the model is adapting the training set, but not generalising to the test set. This is overfitting behaviour, which seems likely to happen because you have relatively few samples compared to the model size (1300 samples vs 140 million parameters).</p>
<p>To help the net generalise better, without modifying your existing model too much, here are some ideas:</p>
<ol>
<li><p>Use <code>AdamW</code> rather than <code>Adam</code>, and set its regularisation <code>weight_decay</code> parameter to a large value. This will effectively shrink the number of parameters the model is allowed to easily tap into.</p>
</li>
<li><p>Using a smaller batch size will regularise the training. Start with 2 or 4, and experiment with doubling the batch size until you find a spot where it's both reasonably quick to train, and also has decent metrics. Each time you increase the batch size, consider decreasing the learning rate.</p>
</li>
<li><p>Reduce the size of the transformer layer. Also try reducing the embedding size or model size of the embedding layer.</p>
</li>
<li><p>Add dropout layers at each stage. Start with just one and monitor the change as you add more dropout layers.</p>
</li>
</ol>
<p>Usually worth tweaking the learning rate after each change, especially if the net becomes unstable.</p>
<p>Using early stopping in conjunction with the above will likely also help. I'd start with some of the above points, otherwise the model might be overfitting almost immediately.</p>
<p>It may also help to preprocess your data down to fewer/smaller features using <code>sklearn</code> or a neural network.</p>
<p>Try one thing at a time and observe its effect on the validation set metric(s). I wouldn't focus too much on the test set accuracy for now, because it's a biased metric in favour of the majority class. Worth looking more at recall and precision initially (or use the F1 score which combines them into a single number).</p>
<p>The recall and precision should improve as the model becomes able to discern positive cases (recall) and does so with precision. The train score will go down at the same time, and these trends mean the model is generalising better.</p>
","2","Answer"
"77951759","77951236","<p>R2 can be negative. As explained in <a href=""https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score"" rel=""nofollow noreferrer"">the documentation</a>:</p>
<blockquote>
<p>Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected (average) value of y, disregarding the input features, would get an R2 score of 0.0.</p>
</blockquote>
<p><a href=""https://stats.stackexchange.com/a/12991/406174"">This answer</a> from CrossValidated explains it really well.</p>
","0","Answer"
"77952168","77933640","<p>First, lets do some digging from the OG BERT code, <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a></p>
<p>If we just do a quick search for &quot;sum&quot; on the github repo, we find this <a href=""https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L814"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L814</a></p>
<pre><code>  # The Transformer performs sum residuals on all layers so the input needs
  # to be the same as the hidden size.
  if input_width != hidden_size:
    raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %
                     (input_width, hidden_size))
</code></pre>
<p>Then a quick search on Stackoverflow reveals <a href=""https://stackoverflow.com/questions/61465103/how-to-get-intermediate-layers-output-of-pre-trained-bert-model-in-huggingface"">How to get intermediate layers&#39; output of pre-trained BERT model in HuggingFace Transformers library?</a></p>
<hr />
<p>Now, lets validate if your code logic works by working backwards a little:</p>
<pre><code>from transformers import RobertaTokenizer, RobertaModel
import torch

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=True)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=True)
</code></pre>
<p>Then:</p>
<pre><code>&gt;&gt;&gt;len(output.hidden_states)
</code></pre>
<p>Out:</p>
<pre><code>13
</code></pre>
<h1>Why 13?</h1>
<p>12 Encoder (hidden) layer output + final pooler output</p>
<pre><code>import torchinfo
torchinfo.summary(model)
</code></pre>
<p>[out]:</p>
<pre><code>================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
RobertaModel                                            --
├─RobertaEmbeddings: 1-1                                --
│    └─Embedding: 2-1                                   38,603,520
│    └─Embedding: 2-2                                   394,752
│    └─Embedding: 2-3                                   768
│    └─LayerNorm: 2-4                                   1,536
│    └─Dropout: 2-5                                     --
├─RobertaEncoder: 1-2                                   --
│    └─ModuleList: 2-6                                  --
│    │    └─RobertaLayer: 3-1                           7,087,872
│    │    └─RobertaLayer: 3-2                           7,087,872
│    │    └─RobertaLayer: 3-3                           7,087,872
│    │    └─RobertaLayer: 3-4                           7,087,872
│    │    └─RobertaLayer: 3-5                           7,087,872
│    │    └─RobertaLayer: 3-6                           7,087,872
│    │    └─RobertaLayer: 3-7                           7,087,872
│    │    └─RobertaLayer: 3-8                           7,087,872
│    │    └─RobertaLayer: 3-9                           7,087,872
│    │    └─RobertaLayer: 3-10                          7,087,872
│    │    └─RobertaLayer: 3-11                          7,087,872
│    │    └─RobertaLayer: 3-12                          7,087,872
├─RobertaPooler: 1-3                                    --
│    └─Linear: 2-7                                      590,592
│    └─Tanh: 2-8                                        --
================================================================================
Total params: 124,645,632
Trainable params: 124,645,632
Non-trainable params: 0
================================================================================
</code></pre>
<p>To validate that the last layer output is the last layer in the hidden_states:</p>
<pre><code>assert(
  True for x in 
    torch.flatten(
      output[0] == output.hidden_states[-1]
    )
)
</code></pre>
<h1>Lets check if the size for each layer's output matches:</h1>
<pre><code>first_hidden_shape = output.hidden_states[0].shape

for x in output.hidden_states:
  assert x.shape == first_hidden_shape
</code></pre>
<p><strong>Checks out!</strong></p>
<pre><code>&gt;&gt;&gt;first_hidden_shape
</code></pre>
<p>[out]:</p>
<pre><code>torch.Size([2, 7, 768])
</code></pre>
<h1>Why <code>[2, 7, 768]</code>?</h1>
<p>It's <code>(batch_size, sequence_length, hidden_size)</code></p>
<ul>
<li><strong>2 sentences</strong> = batch size of 2</li>
<li><strong>7 longest sequence length</strong> = no. of tokens (i.e. 5 in the case of your longest example sentence + <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> from <code>len(input_ids[0])</code>)</li>
<li><strong>768 outputs</strong> = fixed for all hidden layers output</li>
</ul>
<h3>Bíddu aðeins! (Wait a minute!), does that mean I've <code>sequence_length * 768</code> outputs for each batch? And if my batches are not equal lengths, the output size are different?</h3>
<p>Yes that is correct! And to get some sense of &quot;equality&quot; for all inputs, it'll be good to pad/truncate all outputs to a fixed length if you're still going to use the <em>feature-based BERT approaches</em>.</p>
<h1>Soooo, is my <code>torch.stack</code> approach right?</h1>
<p>Yes, it seems so, but it depends on whether you consider the pooler output to be last or second to last.</p>
<p>If second to last:</p>
<pre><code>torch.stack(output.hidden_states[-5:-1]).sum(0)
</code></pre>
<p>if you consider the pooler to be the last:</p>
<pre><code>torch.stack(output.hidden_states[-4:]).sum(0)
</code></pre>
<p>Minor nitpicking, you can access the <code>output.hidden_states</code> through slices because it's a tuple object. Next you won't need to squeeze the stacked output because the the outer most layer tensor is non-empty.</p>
<p>This is a special case for stack, in NLP where the 1st dimension is batch size and 2nd is token length, so summing the hidden dimensions up ends up the same when you're not explicitly stating which dimension you stack.</p>
<p>To be a little more explicit:</p>
<pre><code># 2nd dimension is where our hidden states are 
# and that's where we want to do our sum too.
torch.stack(output.hidden_states[-4:], dim=2).sum(2)
</code></pre>
<p>But in practice, you can do this to comfort yourself:</p>
<pre><code>assert(
 True for x in torch.flatten(
    torch.stack(output.hidden_states[-4:]).sum(0)
    == 
    torch.stack(output.hidden_states[-4:], dim=2).sum(2)
  )
)
</code></pre>
<h1>Interesting, what about &quot;concat last four hidden&quot;?</h1>
<p>In the case of concat you'll need to be explicit when in the dimensions</p>
<pre><code>&gt;&gt;&gt; torch.cat(output.hidden_states[-4:], dim=2).shape
</code></pre>
<p>[out]:</p>
<pre><code>torch.Size([2, 7, 3072])
</code></pre>
<p>but note, you still ends with <code>sequence_length * hidden_size * 4</code>, which makes batches with unequal lengths a pain.</p>
<h1>Since you've covered almost everything on the table, what about the &quot;embeddings&quot; output?</h1>
<p>This is the interesting part, it's actually not accessible through the <code>model(inputs_ids)</code> directly, you'll need to do this:</p>
<pre><code>model.embeddings(input_ids)
</code></pre>
<h1>Finally, why didn't you just answer &quot;yes, you are right&quot;?</h1>
<p>If I did, would that convince you more than you proving the above for yourself?</p>
","1","Answer"
"77954557","77946209","<p>LSTMs are compatible for the <a href=""https://en.wikipedia.org/wiki/Artificial_neuron"" rel=""nofollow noreferrer"">basic artificial neuron</a> architecture, so your input layer has 8 features which are fully connected to all 4 LSTM units in your output layer, hence in each time step your network is fed by a feature vector of size 8, produces an output of size 4, and each LSTM units are learning (among other things) a vector of size 8 to weigh your inputs.</p>
<p>LSTMs, or Recurrent Neural Networks (RNNs) in general, are usually fed by sequences of input vectors over time in each training step to make use of its hidden state. In your case the length of these sequences are 10. For each time step LSTM produces an output, but unless <code>return_sequences</code> is set to <code>True</code> for the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"" rel=""nofollow noreferrer"">LSTM units</a>, only the last output is returned. Hence your output is of size 4 and not (10, 4) for each batch.</p>
<p>To deeper understand LSTMs, I recommend this reading: <a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.
And in order to learn how to do time series forecasting by LSTMs using Tensorflow, I found this very useful: <a href=""https://www.tensorflow.org/tutorials/structured_data/time_series"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/structured_data/time_series</a>.</p>
","0","Answer"
"77955060","77954817","<p>I added comments that helped me to better understand what happens here.</p>
<p>tldr: Option 2 (or 3) are preferred, as this does not apply the same gradient twice.</p>
<h3>OPTION 1</h3>
<pre class=""lang-py prettyprint-override""><code>optA = torch.optim.Adam(params=netA.parameters())  # contains shared fc2
optB = torch.optim.Adam(params=netB.parameters())  # contains shared fc2
optA.step()  # applies 1 grad to fc1_A, 2 grads to fc2_AB
optB.step()  # applies 1 grad to fc1_B, 2 grads to fc2_AB
# applies the fc2_AB grads twice! Thus doubling the learning rate for fc2_AB
</code></pre>
<h3>OPTION 2</h3>
<pre class=""lang-py prettyprint-override""><code>opt = torch.optim.Adam(
    params=(
        list(netA.parameters())  # contains netA.fc1, netA/B.fc2
        + list(netB.parameters())[:1]  # contains netA.fc1
    )
)
opt.step()  # applies 1 grad to fc1_A, 1 grad to fc1_B, 2 grads to fc2_AB
# does not double the learning rate for fc2_AB
</code></pre>
<h3>OPTION 3</h3>
<pre class=""lang-py prettyprint-override""><code>optA = torch.optim.Adam(params=netA.parameters())  # contains shared fc2
optB = torch.optim.Adam(params=netB.parameters())  # contains shared fc2
optA.step()  # applies 1 grad to fc1_A, 2 grad to fc2_AB
optA.zero_grad()  # avoids the same gradient to be applied twice!
optB.step()  # applies 1 grad to fc1_B, 2 (ZERO!) grads to fc2_AB
# applies the fc2_AB grads twice! Thus doubling the learning rate for fc2_AB
</code></pre>
<p>Option 1 and 3 are identical and preferred, however, option 3 is slower as an unnecessary zero-gradient is applied.</p>
","0","Answer"
"77956649","77946209","<p>Just like me, I found many people on Stackoverflow who are searching for the same answer on <code>keras.LSTM</code> and the answer to all those questions made me confused even more.</p>
<p>The problem is, that every article and video talks about basic LSTM which takes <strong>one input per time</strong> step but in the real world we we have a <strong>vector representation</strong>.</p>
<p>The complete flow of LSTM from input to output can be shown in the image below (which was my actual question)</p>
<p><a href=""https://i.sstatic.net/3QWgT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3QWgT.png"" alt=""enter image description here"" /></a></p>
<p>Where in this case, the feature vector has a size of 3 and LSTM units have a size of 2 which makes input as <code>(batch_size, time_steps, 3)</code> and LSTM unit as <code>LSTM(units=2)</code>. An explanation of the image can be found <a href=""https://tung2389.github.io/coding-note/unitslstm"" rel=""nofollow noreferrer"">here</a> to clear your further doubts.</p>
","0","Answer"
"77957584","77957522","<p>I found the solution. It was previously reported <a href=""https://github.com/sthalles/SimCLR/issues/40"" rel=""nofollow noreferrer"">here</a> and recently <a href=""https://github.com/sthalles/SimCLR/issues/60"" rel=""nofollow noreferrer"">here</a>. However, the code was still not updated. It seems this line:</p>
<pre><code>labels = torch.cat([torch.arange(self.args.batch_size) for i in range(self.args.n_views)], dim=0)
</code></pre>
<p>is dependant on <code>batch_size</code> and was causing the issue. The above line needs to be replaced with either: <code>labels = torch.cat([torch.arange(features.shape[0]//2) for i in range(self.args.n_views)], dim=0)</code></p>
<p>OR: <code>labels = torch.cat([torch.arange(features.shape[0]//self.args.n_views) for i in range(self.args.n_views)], dim=0)</code></p>
<p>Based on that change, the correct loss function would be:</p>
<pre><code>def info_nce_loss(self, features):

        labels = torch.cat([torch.arange(features.shape[0]//2) for i in range(self.args.n_views)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(self.args.device)

        features = F.normalize(features, dim=1)

        similarity_matrix = torch.matmul(features, features.T)
        # assert similarity_matrix.shape == (
        #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)
        # assert similarity_matrix.shape == labels.shape

        # discard the main diagonal from both: labels and similarities matrix
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.args.device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        # assert similarity_matrix.shape == labels.shape

        # select and combine multiple positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)

        # select only the negatives the negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)

        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.args.device)

        logits = logits / self.args.temperature
        return logits, labels
</code></pre>
","1","Answer"
"77963646","77959410","<p>Like you guessed, the issue is with the computational graph that gets created when you do backpropagation.</p>
<p>Let me explain the above point:</p>
<p>When you initialize a tensor in pytorch, it usually signals that the operations you perform on them should be tracked. When you do a forward pass, the functions for backward prop are set up and the graph is set.</p>
<p>In case 2, you are deleting the tensor and hence the entire process is reset -- the computation graph is reset. In case 3, you are clearly resetting the parameters.</p>
<p>The output tensor and the model parameters are connected to the graph.</p>
<p>If you want to clearly visualize where the TBackward0 function is, use torchviz to visualize the computational graph.</p>
","-1","Answer"
"77969103","77963903","<p>The attribute <code>estimator_</code> gets set early in the <code>fit</code> method:</p>
<p><a href=""https://github.com/scikit-learn/scikit-learn/blob/9e38cd00d032f777312e639477f1f52f3ea4b3b7/sklearn/ensemble/_weight_boosting.py#L149"" rel=""nofollow noreferrer"">https://github.com/scikit-learn/scikit-learn/blob/9e38cd00d032f777312e639477f1f52f3ea4b3b7/sklearn/ensemble/_weight_boosting.py#L149</a></p>
<p><a href=""https://github.com/scikit-learn/scikit-learn/blob/9e38cd00d032f777312e639477f1f52f3ea4b3b7/sklearn/ensemble/_base.py#L125"" rel=""nofollow noreferrer"">https://github.com/scikit-learn/scikit-learn/blob/9e38cd00d032f777312e639477f1f52f3ea4b3b7/sklearn/ensemble/_base.py#L125</a></p>
<p>So you can just manually call <code>ada._validate_estimator()</code> before your loop to initialize it, and this particular error should be fixed.</p>
<hr />
<p>But you're skipping a lot in your loop, most notably the weight updates. You should at least call <code>_boost</code>, and check the source to see if there's anything else to include in the loop. (It's unfortunate there's no <code>partial_fit</code> for adaboost...).  Or, consider just modifying your local install of sklearn to include the timing you want to measure.</p>
","1","Answer"
"77969618","77969343","<p>Do the following:</p>
<ol>
<li>Add a new row to your embedding matrix, randomly initialized. This row represents your new vocab item</li>
<li>Create a training dataset of inputs that use the new vocab item (make sure you've updated tokenizers/preprocessing/etc to produce the new vocab integer)</li>
<li>Freeze all the non-embedding parameters of the model</li>
<li>Fine-tune the model with the new dataset. Make sure to zero the gradients on all the old embeddings (ie only update the new embedding) to prevent the old embeddings from being trained</li>
</ol>
","1","Answer"
"77970461","77969990","<p>Your first code example is correct. But you need to add <code>show=False</code> to the first call to <code>shap.summary_plot(..., show=False)</code>.  With the default <code>show=True</code>, the plot gets shown immediately, but also gets erased. And a new plot gets created to show the second part.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import shap
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

mylabel = LabelEncoder()
data = pd.read_csv(&quot;https://raw.githubusercontent.com/krishnaik06/Multiple-Linear-Regression/master/50_Startups.csv&quot;)
data['State'] = mylabel.fit_transform(data['State'])

model = RandomForestRegressor()
y = data['Profit']
X = data.drop('Profit', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)
model.fit(X_train, y_train)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_train)
plt.figure(figsize=(30, 30))
plt.subplot(2, 1, 1)
shap.summary_plot(shap_values, X_train, feature_names=X.columns, plot_type=&quot;bar&quot;, show=False)
plt.subplot(2, 1, 2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns, show=False)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/ts2o4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ts2o4.png"" alt=""shap summary_plot into subplots"" /></a></p>
","3","Answer"
"77973001","77934889","<p>Found a better <a href=""https://github.com/canoalberto/CAIM-GPU"" rel=""nofollow noreferrer"">alternative package</a> and it also contains the <a href=""https://github.com/canoalberto/CAIM-GPU/blob/master/src/main/java/weka/filters/supervised/attribute/CAIMCPU.java"" rel=""nofollow noreferrer"">original one's code</a>.</p>
","0","Answer"
"77974287","77974163","<p><a href=""https://www.kaggle.com/models/google/efficientnet-v2/frameworks/tensorFlow2/variations/imagenet1k-b0-feature-vector/versions/2?tfhub-redirect=true"" rel=""nofollow noreferrer"">Here</a> you can find an example on how to use your EfficientNet V2 (for ImageNet1k) model. The example seems to be almost exactly what your are trying to do.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_hub as hub

num_classes = 4
m = tf.keras.Sequential(
    [
        hub.KerasLayer(
            r&quot;https://www.kaggle.com/models/google/efficientnet-v2/frameworks&quot;
            r&quot;/TensorFlow2/variations/imagenet1k-b0-feature-vector/versions/2&quot;,
            trainable=False,
        ),
        tf.keras.layers.Dense(num_classes, activation=&quot;softmax&quot;),
    ]
)
m.build([None, 224, 224, 3]) 
</code></pre>
<p>the code above works on my PC. Remember to: <code>pip install --upgrade tensorflow-hub</code>.</p>
","0","Answer"
"77977195","77964228","<p>From the <a href=""https://python.langchain.com/docs/expression_language/interface#invoke"" rel=""nofollow noreferrer"">Langchain documentation</a>, you should call <code>invoke()</code> on a dictionary.</p>
<p>So, assuming that your variables <code>issues_and_opportunities, business_goals, description</code> are strings defined in your code, this should work:</p>
<pre><code>issues_and_opportunities = &quot;Launching a rocket in space is hard, but spectacular.&quot;
business_goals = &quot;Get investors.&quot;
description = &quot;I want to create a company like SpaceX.&quot;

rag_chain.invoke({
    &quot;issues_and_opportunities&quot;: issues_and_opportunities,
    &quot;business_goals&quot;: business_goals,
    &quot;description&quot;: description,
})
</code></pre>
","0","Answer"
"77978153","77977110","<p>From the <a href=""https://adriangb.com/scikeras/stable/quickstart.html#training-a-model"" rel=""nofollow noreferrer"">SciKeras quickstart documentation</a> you should pass to <code>KerasClassifier()</code> the model and the loss. From the <a href=""https://adriangb.com/scikeras/stable/generated/scikeras.wrappers.KerasClassifier.html#scikeras-wrappers-kerasclassifier"" rel=""nofollow noreferrer"">KerasClassifier documentation</a> you can also pass the optimizer and the metrics.</p>
<p>I would avoid defining the loss both in <code>buildNetwork()</code> and in <code>KerasClassifier()</code>. I would mimic the example in the <a href=""https://adriangb.com/scikeras/stable/quickstart.html#training-a-model"" rel=""nofollow noreferrer"">SciKeras quickstart documentation</a> where only the network is defined in <code>buildNetwork()</code> and everything else (optimizer, loss and metrics) is defined outside.</p>
<pre><code>def buildNetwork():
    classificator = Sequential()
    classificator.add(Dense(units = 20, activation='relu', kernel_initializer='random_uniform', input_shape=(30,)))
    classificator.add(Dense(units = 20, activation='relu', kernel_initializer='random_uniform'))
    classificator.add(Dense(units = 1, activation='sigmoid'))
    return classificator


classifier = KerasClassifier(
    model=buildNetwork,
    batch_size=10,
    epochs=100, 
    loss='binary_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=0.001, weight_decay=0.000001),
    metrics=['binary_accuracy'],
)
</code></pre>
","1","Answer"
"77978297","77973831","<p>Actually, you are passing two inputs to your model, not two samples. If you want to enter two samples, your train_x must have the shape [2, 549, 549, 3].
To accomplish this, you need to concatenate template_array and actual_array from their first axis.</p>
<p><code>train_x = np.concatenate([template_array, actual_array], axis=0)</code></p>
","0","Answer"
"77978712","77975756","<p>Currently XGBoost doesn't support MPS which is Mac's internal &quot;GPU&quot;. To use GPU acceleration, you need to get CUDA gpus.</p>
<p><a href=""https://i.sstatic.net/4bTyw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4bTyw.png"" alt=""XGBoost Supported Devices"" /></a></p>
","1","Answer"
"77979051","77976770","<p>In your <code>fit_transform</code>, you need to fit the tree to the whole dataset in case you will later call <code>transform</code>. As written, you would also get an error trying</p>
<pre class=""lang-py prettyprint-override""><code>te = TaxonomyEncoder()
te.fit_transform(df[['mcc']], df['y'])
te.transform(df[['mcc']])
</code></pre>
<p>The fix can be simple:</p>
<pre class=""lang-py prettyprint-override""><code>def fit_transform(self, X, y=None):
    self.fit(X, y)
    return cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)
</code></pre>
<hr />
<p>The <code>ColumnTransformer</code> has a bit of a quirk that causes the error in using it: it actually always <code>fit_transform</code>s all of its transformers (even when just fitting the column transformer itself), in order to determine whether the outputs are sparse and thus whether the hstack should be sparse.</p>
","1","Answer"
"77981612","77978874","<p>As you observed the ADAM optimizer in Qiskit Algorithms does not have a callback like the other optimizers. It has been noted in this issue <a href=""https://github.com/qiskit-community/qiskit-algorithms/issues/60"" rel=""nofollow noreferrer"">https://github.com/qiskit-community/qiskit-algorithms/issues/60</a> which was more around unification of the signatures of the optimizers, in qiskit algorithms, so it's  consistent over the set of them. Now ADAM does have a <code>snapshot_dir</code> parameters which can be used to have its internal parameters saved if that is of use at all for you now.</p>
<p>The source to ADAM is here <a href=""https://github.com/qiskit-community/qiskit-algorithms/blob/main/qiskit_algorithms/optimizers/adam_amsgrad.py"" rel=""nofollow noreferrer"">https://github.com/qiskit-community/qiskit-algorithms/blob/main/qiskit_algorithms/optimizers/adam_amsgrad.py</a> so you can see what it does. It was moved out of Qiskit and qiskit.algorithms as part of the move to have a separate repository/package for qiskit algorithms. With the source you could always make a copy and edit it to add a callback for yourself, if that's important for you to have.</p>
","0","Answer"
"77984495","77984286","<p>You are using the split criterion as defined by scikit-learn however you are using cuml.
Cuml requirers int to define split-criterion:</p>
<p>0 or 'gini' for gini impurity</p>
<p>1 or 'entropy' for information gain (entropy)</p>
<p>2 or 'mse' for mean squared error</p>
<p>4 or 'poisson' for poisson half deviance</p>
<p>5 or 'gamma' for gamma half deviance</p>
<p>6 or 'inverse_gaussian' for inverse gaussian deviance</p>
","0","Answer"
"77986530","77986399","<p>This will not increase the size:</p>
<pre><code>data_augmentation = tf.keras.Sequential([
    layers.RandomFlip(&quot;horizontal_and_vertical&quot;),
    layers.experimental.preprocessing.RandomRotation(0.2), 
    layers.experimental.preprocessing.RandomZoom(
        height_factor=(-0.3, -0.03),
        width_factor=None), 
])
</code></pre>
<p>It uses the output of the previous step as input in the next step. So the final output is a single dataframe with the samesize.
Note: Sequential groups a linear stack of layers into a Model.</p>
<p>you could create 3 different data_augmenatations like this:</p>
<pre><code>data_augmentation_1 = tf.keras.Sequential([
        layers.RandomFlip(&quot;horizontal_and_vertical&quot;)
])

data_augmentation_2 = tf.keras.Sequential([
  
        layers.experimental.preprocessing.RandomRotation(0.2), 
     
    ])

data_augmentation_3 = tf.keras.Sequential([

        layers.experimental.preprocessing.RandomZoom(
            height_factor=(-0.3, -0.03),
            width_factor=None),
    ])
</code></pre>
<p>Then you should modify the function prepare accordingly to contcat these outputs into a single dataframe.</p>
<p>Something like this:</p>
<pre><code>if augment:
     ds1 = ds.map(lambda x, y: (data_augmentation_1(x, training=True), y),
            num_parallel_calls=AUTOTUNE)
     ds2 = ds.map(lambda x, y: (data_augmentation_2(x, training=True), y),
            num_parallel_calls=AUTOTUNE)
     ds3 = ds.map(lambda x, y: (data_augmentation_3(x, training=True), y),
            num_parallel_calls=AUTOTUNE)

     combined_df = pd.concat([df1, df2, df3], axis=0)
</code></pre>
","0","Answer"
"77986981","77983736","<p>Yes, it's possible to start and end an <code>mlflow</code> run without the context manager syntax.</p>
<p>To do this you can simply write:</p>
<pre><code>mflow.start_run()
...
your_run_code_here
...
mlflow.end_run()
</code></pre>
<p>You can read more about it in the <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html"" rel=""nofollow noreferrer"">mlflow documentation</a>.</p>
","0","Answer"
"77987235","77986616","<p>Replace the import:</p>
<pre><code>from llama_index.core.embeddings import resolve_embed_model
</code></pre>
<p>with this import:</p>
<pre><code>from llama_index.core.embeddings.utils import resolve_embed_model
</code></pre>
<p>They seem to be moving things around, and the tutorial is not well synchronised with the latest release.</p>
","1","Answer"
"77987438","77982056","<p>For my case, changing stacking_method to 'predict' works</p>
","1","Answer"
"77987989","77987953","<p>At the end of the day, <code>image</code> is just another array, why don't you crop it like this:</p>
<pre><code>image_crop = image[int(y - croplength):int(y + croplength), int(x - croplength):int(x + croplength)]
</code></pre>
<p>I added <code>int</code> in case <code>croplength</code> was not a full number. This is because the indexing of an array should optimally be with full integers</p>
<p>V2.0: with pillow function</p>
<pre><code>from PIL import Image
image = Image.open(&quot;...&quot;)
def Crop_Image(image):
    cropsize = 224
    croplength = int(cropsize/2)
    x = 500
    y = 500
    image_crop = image.crop((x-croplength, y-croplength, x+croplength, y+croplength))
    return image_crop

croppedImage = Crop_Image(image)

plt.imshow(croppedImage)
</code></pre>
<p>This is working perfectly for me</p>
","0","Answer"
"77989652","77982056","<p>From the comments we haven't been able to ascertain what the cause is. Below is a simple implementation of a stacking classifier which you might be able to use instead (or if the same error occurs, perhaps you can debug the issue more easily). I've included some test data at the end to show it working.</p>
<p>The implementation uses CV to get out-of-sample probabilities/decision values/predictions from each estimator. Those outputs are concatenated and used to train the final estimator. Finally, the base estimators are refitted on the entire training set. When it comes to prediction, the outputs from each base estimator are obtained and concatenated. They are fed into the blender to get the final output.</p>
<p>Simple stacking classifier class:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.base import (
    BaseEstimator,
    ClassifierMixin,
    check_X_y,
    check_array,
    clone
)
from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.linear_model import LogisticRegression

class SimpleStackingClassifier(BaseEstimator, ClassifierMixin):
    &quot;&quot;&quot;
    SimpleStackingClassifier
    
    estimators:      a list of estimators
    final_estimator: None by default will use LogisticRegression()
    cv:              None by default will use StratifiedKFold()
    stack_method:    None by default will use predict_proba(), decision_function() or
                       predict() depending on which is available across all base estimators
    n_jobs, verbose: these are passed internally to cross_val_predict()
    random_state: None by default, supplied to internal attributes
    &quot;&quot;&quot;
    def __init__(
        self, estimators, final_estimator=None,
        cv=None, stack_method='auto', n_jobs=1, verbose=0, random_state=None
    ):
        self.estimators = estimators
        self.final_estimator = final_estimator
        self.cv = cv
        self.stack_method = stack_method
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.random_state = random_state
    
    #Find a stacking method that all base estimators have in common
    def find_common_stack_method(self, estimators):
        stack_methods = ['predict_proba', 'decision_function', 'predict']
        
        for stack_method in stack_methods:
            if all([hasattr(estimator, stack_method) for estimator in estimators]):
                break
        else:
            raise NotImplementedError(
                f'Estimators must all implement one of {stack_methods}'
            )
        return stack_method
            
    def fit(self, X, y):
        #
        # Input checks
        #
        X, y = check_X_y(X, y)
        self.n_features_in_ = X.shape[1]
        
        if hasattr(X, 'columns'):
            self.feature_names_in_ = np.array(
                X.columns, dtype=object
            )
        
        if not self.final_estimator:
            self.final_estimator = LogisticRegression(random_state=self.random_state)
        if not self.cv:
            self.cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        #Determine a common stack method
        if self.stack_method == 'auto':
            self.stack_method_ = self.find_common_stack_method(self.estimators)
        else:
            self.stack_method_ = self.stack_method

        #Get out-of-sample predictions per estimator
        oos_predictions_list = []
        for estimator in self.estimators:
            oos_predictions = cross_val_predict(
                estimator, X, y,
                cv=self.cv,
                method=self.stack_method_,
                n_jobs=self.n_jobs,
                verbose=self.verbose,
            )
            oos_predictions_list.append(
                oos_predictions.reshape(-1, oos_predictions.ndim)
            )
        
        #[est0[:, 0] est0[:, 1]  est1[:, 0] est1[:, 1] ...]
        # could optionally discard a column when classes dim &gt; 1
        # oos_predictions_list = [oos_preds[:, 1:] if oos_preds.ndim &gt;= 2 else oos_preds
        #                         for oos_preds in oos_predictions_list]
        oos_predictions_arr = np.concatenate(oos_predictions_list, axis=1)
        self.base_estimators_oos_predictions_ = oos_predictions_arr
        
        #Fit blender on the concatenated out-of-sample predictions
        self.final_estimator_ = clone(self.final_estimator).fit(oos_predictions_arr, y)
        
        #Fit base estimators on all training data
        self.estimators_ = [clone(estimator).fit(X, y)
                            for estimator in self.estimators]
        
        return self
    
    def _predict(self, X, final_predict_method):
        #
        #Input checks
        #
        X = check_array(X)
        
        if hasattr(self, 'feature_names_in_') and not hasattr(X, 'columns'):
            raise Warning('Estimator was fitted with feature names, '
                          'but supplied X has no feature names attribute')
        elif hasattr(X, 'columns') and (X.columns.tolist() != self.feature_names_in_.tolist()):
            raise AttributeError('X column names are different from fitted names')
        
        #Get predictions from each estimator, and feed into blender
        base_predictions = np.column_stack(
            [getattr(estimator, self.stack_method_)(X)
            for estimator in self.estimators_]
        )
        return getattr(self.final_estimator_, final_predict_method)(base_predictions)
    
    def predict(self, X):
        return self._predict(X, final_predict_method='predict')
    
    def predict_proba(self, X):
        return self._predict(X, final_predict_method='predict_proba')
    
    def predict_log_proba(self, X):
        return self._predict(X, final_predict_method='predict_log_proba')
    
    def get_feature_names_out(self, input_features=None):
        return self.final_estimator_.get_feature_names_out(input_features)
</code></pre>
<p>Test using a classification dataset:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from matplotlib import pyplot as plt
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

#Classification dataset
X, y = make_moons(n_samples=800, noise=0.2, random_state=0)
X = pd.DataFrame({'x0': X[:, 0], 'x1': X[:, 1]})

#View training data
X_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.3, random_state=0)

plt.scatter(X_trn.iloc[:, 0], X_trn.iloc[:, 1], c=y_trn, marker='.', cmap='Set1')
plt.gcf().set_size_inches(5, 3)

estimator0 = DecisionTreeClassifier(max_depth=10, random_state=np.random.RandomState(0)).fit(X_trn, y_trn)
estimator1 = SVC(C=0.1, probability=True, random_state=np.random.RandomState(1)).fit(X_trn, y_trn)

stacked = SimpleStackingClassifier([estimator0, estimator1],
                                   stack_method='predict_proba',
                                   random_state=np.random.RandomState(0))
stacked.fit(X_trn, y_trn)

#Compare scores with and without stacking
print(
    'Train scores:\n',
    'tree:   ', '%.3f' % estimator0.score(X_trn, y_trn), '\n',
    'svc:    ', '%.3f' % estimator1.score(X_trn, y_trn), '\n',
    'stacked:', '%.3f' % stacked.score(X_trn, y_trn), '\n'
)
print(    
    'Validation scores:\n',
    'tree:   ', '%.3f' % estimator0.score(X_val, y_val), '\n',
    'svc:    ', '%.3f' % estimator1.score(X_val, y_val), '\n',
    'stacked:', '%.3f' % stacked.score(X_val, y_val), ' &lt;----'
)
</code></pre>
<p>Results suggest that the stacked classifier generalises a bit better than the individual estimators:</p>
<pre><code>Train scores:
 tree:    1.000 
 svc:     0.941 
 stacked: 0.980 

Validation scores:
 tree:    0.942 
 svc:     0.921 
 stacked: 0.950  &lt;----
</code></pre>
","1","Answer"
"77993295","77991202","<p>It's difficult to help without example data. Here's an example of how to do it using the <code>iris</code> dataset:</p>
<pre><code>library(caret)
library(party)
library(iml)

data(iris)

set.seed(123)
trainIndex &lt;- createDataPartition(iris$Species, p = .8, list = FALSE)
trainData &lt;- iris[trainIndex,]
testData &lt;- iris[-trainIndex,]

set.seed(123)
model &lt;- train(Species ~ ., data = trainData, method = &quot;cforest&quot;, 
               trControl = trainControl(method = &quot;cv&quot;, number = 5),
               controls = cforest_unbiased(ntree = 500, mtry = 2))

predictor &lt;- Predictor$new(model, data = testData[,1:4], y = testData[,5])

shapley &lt;- Shapley$new(predictor, x.interest = testData[1,1:4])

results &lt;- shapley$results
</code></pre>
<p>The <code>x.interest</code> argument here specifies the instance for which you want to compute the Shapley values. These results are for the first instance in the test dataset, but you can change this to any instance. Let me know if this helps.</p>
","0","Answer"
"77995778","77995524","<p>The passed variable actScaled was defined as Eigen::Matrix&lt;float, -1, 1&gt; instead of Eigen::Matrix&lt;float, -1, -1&gt;, trivial solution. Thank you chtz.</p>
","-1","Answer"
"78005093","78004822","<p>There's a lot of things about your code that don't make sense, but I think the <code>_correct</code> function is the cause the accuracy issue.</p>
<p>You create your model</p>
<p><code>model = SimpleMLP(num_of_classes=1)</code></p>
<p>Your model as designed takes in an input of size <code>(bs, 2)</code> and produces an output of size <code>(bs, 1)</code></p>
<p>Now your <code>_correct</code> function:</p>
<pre class=""lang-py prettyprint-override""><code>def _correct(output, target):
    predicted_digits = output.argmax(1)  # pick digit with largest network output
    correct_ones = (predicted_digits == target).type(
        torch.float
    )  # 1.0 for correct, 0.0 for incorrect
    return correct_ones.sum().item()
</code></pre>
<p>The line <code>predicted_digits = output.argmax(1)</code> makes no sense. You are taking the argmax of a unit axis. This is returning <code>0</code> for every value</p>
<pre class=""lang-py prettyprint-override""><code>output = torch.randn(32, 1)
output.argmax(1)
&gt;tensor([0, 0, 0, 0, 0, 0, 0, 0])
</code></pre>
<p>Taking the argmax of a vector is what you would do for a classification problem, but that isn't what you're doing here.</p>
<p>After that, your evaluation <code>correct_ones = (predicted_digits == target)</code> also doesn't make sense. Your model is a regression model producing a floating point output. It's highly unlikely your model will output a perfect integer value (ie 4.000000001 != 4).</p>
<p>Based on this, I would expect the <code>_correct</code> function to output <code>0.0</code> for all predictions.</p>
","0","Answer"
"78006308","77996526","<p>If your model is in the Model Registry, I think you should use <a href=""https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.9.0/api/v1/model.html#v1.model.ModelGetOp"" rel=""nofollow noreferrer"">ModelGetOp</a> operator.</p>
<pre><code>v1.model.ModelGetOp(
    model_name: str, 
    project: str = '{{$.pipeline_google_cloud_project_id}}', 
    location: str = 'us-central1')¶
</code></pre>
<p>You will need to enter the model resource name in the model_name argument. You can get it programmatically like this:</p>
<pre><code>model_name = 'my_model'
model_resource_name = aiplatform.Model.list(filter=f'display_name=&quot;{model_name}&quot;')[0].resource_name
</code></pre>
<p>You can use GetVertexModelOp if you're using earlier versions of google-cloud-pipeline-components/kfp.</p>
","1","Answer"
"78007465","77958199","<p>I solved it. The solution was to downgrade from the espressif esp32 board version to 1.0.5 . After that i uploaded to my board set as Wrover Model and then after the upload i changed to the AI thinker board.</p>
<p>Hope it helps.</p>
","0","Answer"
"78008438","78008233","<p><code>normal_</code> fills with values drawn from a normal distribution, but that doesn't mean that the resulting tensor represents a normal or even a valid probability distribution.</p>
<p>E.g. <code>[0.2, 0.2, 0.2, 0.2, 0.2]</code> is an valid uniform distribution. If you had used <code>torch.empty(10).uniform_()</code>, you would not get a tensor that represents a uniform distribution.</p>
<p>For computing the KL divergence, each value in the tensor should represent the <em>probability</em> of that index occurring, not merely a sample from the said distribution (as in your example).</p>
<p>In your code, you could make the probabilities sum to 100%:</p>
<p><code>F.kl_div( (input_1/input_1.sum() ).log(), input_2 / input_2.sum(), reduction='batchmean')</code></p>
<p>which would give a positive result.</p>
","5","Answer"
"78012589","78012530","<p>Your print statement at the end of your code is meant to display the predicted values alongside the actual values.</p>
<p>If you want a better visual of the values you can make a data frame. You get a result like <a href=""https://i.sstatic.net/1P7kh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1P7kh.png"" alt=""enter image description here"" /></a></p>
<p>All you need is to modify the end of your code as follows:</p>
<pre><code># Printing the predicted values and actual values
results = np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)
print(pd.DataFrame(results, columns=['Predicted', 'Actual']))
</code></pre>
","1","Answer"
"78016546","78016395","<p>As discussed in the comments - the issue is your images have an alpha channel. You can modify the <code>read_image</code> function to remove the alpha channel from the input images as follows:</p>
<pre class=""lang-py prettyprint-override""><code>image = read_image(f'{self.dir}/{self.images[index]}', mode=ImageReadMode.RGB)
</code></pre>
<p>For other modes, you can check the <a href=""https://pytorch.org/vision/stable/generated/torchvision.io.ImageReadMode.html#torchvision.io.ImageReadMode"" rel=""nofollow noreferrer"">ImageReadMode class</a>.</p>
<p><em><strong>Update1:</strong></em></p>
<p>For the new error - according to the <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>ToTensor class converts a PIL Image or ndarray to tensor and scale the values accordingly.</p>
</blockquote>
<p>But here you are providing <code>tensor</code> as an input instead of the required PIL image or ndarray.</p>
<p>To resolve this you may use the <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToPILImage.html"" rel=""nofollow noreferrer"">ToPILImage</a> method.</p>
<p><em><strong>Update2:</strong></em></p>
<p>For the error: <code>TypeError: '_SingleProcessDataLoaderIter' object is not subscriptable</code></p>
<p>Check how to <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterate-through-the-dataloader"" rel=""nofollow noreferrer"">Iterate through a DataLoader</a> from the PyTorch tutorials.</p>
<p>Also, you may try using a for loop as well to iterate as follows:</p>
<pre><code>for images in trainloader:
    # Process images here
    break # update this break statement as per your requirement
</code></pre>
","0","Answer"
"78021660","78021659","<p>The solution that work for me is from <a href=""https://huggingface.co/LiheYoung/depth-anything-large-hf/discussions/1"" rel=""nofollow noreferrer"">here</a></p>
<p>Just install transformers from source by:</p>
<pre><code>pip install -q git+https://github.com/huggingface/transformers.git
</code></pre>
<p>instead (Quick tour recommendation):</p>
<pre><code>!pip install transformers datasets
</code></pre>
<p>additionally you may need to install PIL by:</p>
<pre><code>pip install pillow
</code></pre>
","0","Answer"
"78025404","78025319","<p><strong>I ran the same code with the iris dataset and it returned full value for accuracy</strong>. The result that you got may be because of your dataset or some configuration. Please provide more information. I also provide a simple code that you can run to check the results.</p>
<pre><code>from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.model_selection import RepeatedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Load Dataset
X, y = datasets.load_iris(return_X_y=True)

# Split Dataset to train and test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=0)

# Fit data and evaluate by cross validation
cv = RepeatedKFold(n_splits=10, n_repeats=50, random_state=42)
forest = RandomForestClassifier(criterion='log_loss', max_depth=7, max_features='log2', min_samples_leaf=1, min_samples_split=2, n_estimators=10, n_jobs=-1, random_state=42)
scores = cross_val_score(forest, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')

# print scores
print(scores)
</code></pre>
","1","Answer"
"78025760","78025623","<p>By destructuring your <code>kwargs</code> in the constructor of <code>Boost</code> what you are actually storing in <code>self.kwargs</code> is a dictionary that looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>{'kwargs': {'X': x_train, 'y': y_train, 'stride': 12, 's_stride': 17 }
</code></pre>
<p>Passing this to the <code>wk_learner</code> constructor using <code>**self.kwargs</code> means python tries to look for a keyword argument called <code>kwargs</code> which doesn't exist, hence the error message.</p>
<p>You can fix the problem by not destructuring in the <code>Boost</code> constructor (and maybe also renaming the parameter to avoid confusion):</p>
<pre><code>def __init__(self, base_learner, base_learner_kwargs):
    self.base_learner = base_learner
    self.base_learner_kwargs = base_learner_kwargs

def fit(self, X: np.ndarray, y: np.ndarray):
    ...
    clf = self.base_learner(**self.base_learner_kwargs)
</code></pre>
","0","Answer"
"78025789","78025623","<p>The problem is that you explicitly define a keyword argument <code>kwargs</code> in your function call</p>
<pre class=""lang-py prettyprint-override""><code>inst = Boost(base_learner=wk_learner, kwargs=kwargs)
</code></pre>
<p>which your <code>wk_learner</code> class does not know how to use.</p>
<p>What I assume you want to do is to just just pass the contents of kwargs via</p>
<pre class=""lang-py prettyprint-override""><code>inst = Boost(base_learner=wk_learner, **kwargs)
</code></pre>
<p>Note that <code>args</code> and <code>kwargs</code> are not defined names for a list of arguments and a dictionary of keyword arguments. They are just established default names. You can use any variable name instead.</p>
","0","Answer"
"78026463","78024641","<p>If you are aiming to perform a regression task rather than a classification task, it is recommended to use loss functions such as MSE (Mean Squared Error) instead of categorical_crossentropy. Additionally, softmax is typically utilized for classification tasks due to its output range restriction and should be avoided in this scenario.</p>
","0","Answer"
"78026471","78026404","<p><code>timm.list_models()</code> returns a complete list of available models in timm:</p>
<pre><code>import timm

timm.list_models()
</code></pre>
<p>More info about how you can list models can be found <a href=""https://timm.fast.ai/models#List-of-models-supported-by-timm"" rel=""nofollow noreferrer"">here</a>. Similar resources can be found on the <a href=""https://huggingface.co/docs/timm/quickstart#list-models-with-pretrained-weights"" rel=""nofollow noreferrer"">HuggingFace website</a>.</p>
<p>Specifically, the <code>transformers.TimBackbone</code> class checks if the string you pass is in <code>timm.list_models()</code>. Check the related code <a href=""https://github.com/huggingface/transformers/blob/ff76e7c2126ab26e5722f54640d44cab7e3dfdd4/src/transformers/models/timm_backbone/modeling_timm_backbone.py#L53"" rel=""nofollow noreferrer"">here</a>.</p>
","1","Answer"
"78026565","78025319","<p>As <code>cross_val_score</code> returns a float, you can include the following line in your code, as it is only a print issue:</p>
<pre><code>scores = cross_val_score(forest, X_phishing, y_phishing, cv=cv, n_jobs=-1)

print(f'{scores:.8f}')
</code></pre>
<p>The documentation of <code>cross_val_score</code> cleary says that it will return a float, so if your first 5 numbers have these outputs, your decimal does not have any more digits then 0,995</p>
","1","Answer"
"78026647","78019397","<p>The only thing you could try is: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html</a></p>
","1","Answer"
"78028552","78016312","<p>I solved it:</p>
<pre><code>for i, file_name in enumerate(input_file_names):
    input_file_handles[i] = np.load(f&quot;{TRAINING_FOLDER}/{file_name}&quot;, mmap_mode='r')

def data_generator():
    global batch_size

    for i in range(NUM_SAMPLES):
        batch_X = np.empty((batch_size, 1000, 63), dtype=np.double)
        batch_y = np.empty((batch_size), dtype=np.uint8)
        for j, file_handle in enumerate(input_file_handles):
            batch_X[j] = file_handle['array1'][i]
            batch_y[j] = file_handle['array2'][i]

        yield batch_X, batch_y
</code></pre>
<p>I implemented a generator with the function fit_generator, and set the batch size to the amount of files needed to fulfill the NUM_SAMPLES requested, so each batch has one sample from each file, avoiding catastrophic forgetting.</p>
","0","Answer"
"78029111","78029057","<p>This is probably because the shape of the y is 1D. you can pass it like this :</p>
<pre><code>np.array(y_train).reshape((-1,1))
</code></pre>
<p>Then see if the error is gone or not.</p>
","2","Answer"
"78039779","78039476","<p>Your script is running out of RAM. Flash size doesn't matter (altough 873 KB - wow, that's <em>massive</em>) because all the variables and other runtime overhead is allocated in RAM and there's not enough of it. The ESP32 has only 520 KiB of internal RAM, of that only 320 KiB is used for data (the rest for code running in RAM). The Python interpreter already takes up a large chunk of that and your script is left with whatever RAM remains after that. In short - you won't be doing much heavy lifting.</p>
<p>Assuming your current board doesn't already have external RAM, one potential solution is to get a board with as much external RAM as possible. Espressif has <a href=""https://www.espressif.com/en/products/devkits"" rel=""nofollow noreferrer"">several devkits</a> with 8 MiB PSRAM (price differences are a few euros so I'd buy the beefiest of them - <a href=""https://docs.espressif.com/projects/esp-idf/en/latest/esp32s3/hw-reference/esp32s3/user-guide-devkitc-1.html"" rel=""nofollow noreferrer"">ESP32-S3-DevKitC-1-N32R8V</a>). They also sell the rather cumbersome <a href=""https://github.com/espressif/esp-box"" rel=""nofollow noreferrer"">ESP32-S3-BOX-3</a> with 16 MiB PSRAM - this is actually intended for AI applications.</p>
<p>Maybe one of those will provide enough RAM for your script. There's no guarantee, obviously, until you're able to predict the worst case RAM usage of your script.</p>
","2","Answer"
"78042086","78041279","<p>The GPU RAM consumption usually only depends on the model weights and batch size, not on the dataset size.</p>
<p>At each training iteration, the model weights are loaded and the output is computed for all the data points in a batch, it doesn't matter if the data has 1 or 1 million points. You can try with 1 data point to see if anything changes (it shouldn't).</p>
<p>Some strategies you might try to reduce memory consumption are quantization and LoRA <a href=""https://pytorch.org/blog/finetune-llms/"" rel=""nofollow noreferrer"">https://pytorch.org/blog/finetune-llms/</a>.</p>
","0","Answer"
"78043260","78037608","<p>{DALEX} does not support plotting/working with SHAP values of <em>multiple</em> observations. Plotting SHAP beeswarm plots is easy with {shapviz}. Calculating SHAP values can done by different packages, e.g., {kernelshap}, {fastshap}, or {treeshap}.</p>
<p>Note that random forests are one of the worst for SHAP, because trees are deep and predictions are very slow.</p>
<h3>Kernel SHAP or permutation SHAP</h3>
<pre class=""lang-r prettyprint-override""><code>library(randomForest)
library(kernelshap)  # or library(treeshap)
library(shapviz)

fit &lt;- randomForest(Sepal.Length ~ ., data = iris)

xvars &lt;- setdiff(colnames(iris), &quot;Sepal.Length&quot;)

# Or kernelshap() if length(xvars) is &gt;10. Subsample bg_X to 100-500 rows
shap_values &lt;- permshap(fit, X = iris, bg_X = iris, feature_names = xvars)
shap_values &lt;- shapviz(shap_values)
sv_importance(shap_values, kind = &quot;bee&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/mzIlF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mzIlF.png"" alt=""enter image description here"" /></a></p>
","3","Answer"
"78043530","78040679","<p>Have you read the documentation for <a href=""https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html"" rel=""nofollow noreferrer"">HingeEmbeddingLoss</a>?</p>
<p><code>Measures the loss given an input tensor 𝑥 and a labels tensor 𝑦 (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as 𝑥, and is typically used for learning nonlinear embeddings or semi-supervised learning.</code></p>
<p>The <code>x</code> in <code>HingeEmbeddingLoss</code> is supposed to be distances between paired embeddings. It doesn't apply at all to your prediction problem.</p>
<p>You are actually telling the model to do the opposite of what you want. For the loss computation, the loss is <code>x</code> when <code>y=1</code> and <code>margin-x</code> when <code>y=-1</code>. Meaning, when <code>y=1</code>, you are telling the model to produce a small value (-1), and vice versa when <code>y=-1</code>.</p>
<p>You also have errors from incorrect broadcasting. <code>loss = criterion(output.squeeze(), y_train.long())</code> produces a <code>(n,n)</code> loss rather than a <code>(n,1)</code> loss. This error is also present in your accuracy calculation.</p>
<p>If you remove the broadcasting errors and compute accuracy as the opposite sign (<code>((-y_pred.sign()).long() == y_val).float().mean()</code>), the training setup somewhat works.</p>
<p>Overall, the correct way to approach this problem is to use binary classification (which, as you mention, works). If you need the output to be on <code>[-1, 1]</code>, you can just rescale the sigmoid logits from <code>[0, 1]</code> to <code>[-1, 1]</code> after prediction.</p>
","2","Answer"
"78043662","78043661","<p>The currently active run object can be accessed via <code>wandb.run</code>. It will be  <code>None</code> if no run is initialised.</p>
<pre class=""lang-py prettyprint-override""><code>if wandb.run is not None:
    print(&quot;There is an active connection to wandb&quot;)
</code></pre>
<p>It is guaranteed that there is at most one active <code>wandb.Run</code> object in any process (<a href=""https://github.com/wandb/wandb/blob/fa4423647026d710e3780287b4bac2ee9494e92b/wandb/sdk/wandb_run.py#L443C1-L444C1"" rel=""nofollow noreferrer"">source</a>).</p>
","1","Answer"
"78044143","78044014","<p><code>optax</code>, like most optimization frameworks, is only able to optimize a single-valued loss function. You should decide what single-valued loss makes sense for your particular problem. A good option given the RMS form of your individual losses might be the square sum:</p>
<pre class=""lang-py prettyprint-override""><code>@jit
def my_loss(forz, params, true_a, true_b):

    sim_a, sim_b = my_model(forz, params)

    loss_a = rmse(sim_a, true_a)
    loss_b = rmse(sim_b, true_b)

    return loss_a ** 2 + loss_b ** 2
</code></pre>
<p>With this change, your code executes without an error.</p>
","0","Answer"
"78046274","78027259","<p>AFAIK you cannot do that. ARIMA models are, differently from ML context, strictly related to the training data itself, so forecasts are based on the historical data where the model was trained on.</p>
<p>On the page of <a href=""https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMAResults.forecast.html#statsmodels.tsa.arima.model.ARIMAResults.forecast"" rel=""nofollow noreferrer"">statsmodels</a>, the forecast module takes the following arguments:</p>
<pre><code>ARIMAResults.forecast(steps=1, signal_only=False, **kwargs)
</code></pre>
<p>And you cannot specify data taken as input.</p>
","1","Answer"
"78047077","78046996","<p><code>growClassifier</code> doesn't return anything, so effectively you're setting <code>clsResults</code> to <code>None</code>.</p>
<p>Then you try to access an attribute <code>columns</code> of <code>clsResults</code>, but since its value is <code>None</code>, that is not possible. This is what the error message (<code>AttributeError: 'NoneType' object has no attribute 'columns'</code>) is telling you.</p>
<p>To fix it, <code>growClassifier</code> should return a dataframe.</p>
","1","Answer"
"78052691","78051902","<h1>Problem</h1>
<p>Seems a version incompatibility problem between typing_extensions and other packages.</p>
<h1>Solution</h1>
<p>I tested it with Python 3.10 and some pinned versions of the following packages, and it should work for you also.</p>
<pre><code>conda create -n my_env python=3.10
conda activate my_env
pip install openai==1.8.0 typing_extensions==0.4.7 trl
</code></pre>
<p>Those packages usually cause dependency errors, so this should fix your current problem and future ones.</p>
<h1>Extra Ball</h1>
<p>There is a similar issue in the <code>openai</code> repository: <a href=""https://github.com/openai/openai-python/issues/751"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python/issues/751</a></p>
","0","Answer"
"78053048","78048772","<p>Sorry to hear that you are having trouble with the LinearRegression model. The original code was not robust enough, and will be updated soon. I have created a slim down and modified version of your code that runs (with a placeholder model) the model prediction. I fixed one line in your modified linear regression class; it was only saving one coefficient even if there were multiple.</p>
<pre><code>from gekko import GEKKO
from gekko import ML
from gekko.ML import Gekko_GPR, Gekko_SVR, Gekko_LinearRegression
import numpy as np

class Gekko_LinearRegression_Modified(Gekko_LinearRegression):
    def __init__(self, model, m):
        self.m = m
        # Check if coef_ is an array and handle accordingly
        if hasattr(model.coef_, &quot;__len__&quot;):
            self.coef = model.coef_[0] if len(model.coef_) == 1 else model.coef_ ##changed to == 1 here
        else:
            self.coef = model.coef_
        # Check if intercept_ is a scalar and handle accordingly
        self.intercept = model.intercept_ if np.isscalar(model.intercept_) else model.intercept_[0]

#train a placeholder model
from sklearn.linear_model import LinearRegression

test_out = np.random.random(10)

test_in = np.random.random((10,2))

MLA_LR_P = LinearRegression().fit(test_in,test_out)
print(MLA_LR_P.predict(np.array([0.5,25]).reshape(1,-1)))

m = GEKKO(remote=False)

price_vec_list = [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 1, 1, 1] # Price vector

# Create a Gekko array of parameters
price_vec_gekko = m.Array(m.Param, len(price_vec_list))
for i, val in enumerate(price_vec_list):
    price_vec_gekko[i].value = val

ST_V = m.Array(m.MV, n) # Compressor control variable
for i in range(n):
    ST_V[i].VALUE = 0.5
    ST_V[i].LOWER = 0.43  # Correct property for lower limit
    ST_V[i].UPPER = 1  # Correct property for upper limit
    ST_V[i].status = 1  # Allow the solver to optimize this variable

T_brine_in = m.Array(m.Param, n) # Brine inlet temperature
for i in range(n):
    T_brine_in[i].VALUE = 25

#Example prediction
pred = Gekko_LinearRegression_Modified(MLA_LR_P,m).predict(np.array([ST_V[i],T_brine_in[i]]))

#solve
a = m.Var() #dummy variable
m.solve(disp=False)
print(pred.value[0])
</code></pre>
","1","Answer"
"78053076","78053061","<p>It looks like you're getting close! You may need to adjust the parameters in your outline function to properly catch all of the ribs:</p>
<pre class=""lang-py prettyprint-override""><code>def add_outline(image, kernel_size=(5,5), dilations=3, canny_low=30, canny_high=150):
    
    blurred = cv2.GaussianBlur(image, kernel_size, 0)
    edges = cv2.Canny(blurred, canny_low, canny_high)
    kernel = np.ones(kernel_size, np.uint8)
    closed_edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
    dilated = cv2.dilate(closed_edges, kernel, iterations=dilations)
    dilated = cv2.bitwise_not(dilated)
    _, mask = cv2.threshold(dilated, 0, 255, cv2.THRESH_BINARY)
    outlined_image = cv2.bitwise_and(image, image, mask=mask)
    return outlined_image
</code></pre>
<p>which I imagine you can figure out with some experimentation.</p>
<p>To create your desired mask, you can mask all pixels to the left and the right of your edges:</p>
<pre class=""lang-py prettyprint-override""><code>def segment_image(image, outlines):
    # image: your input image
    # outlines: just the `mask` from your `add_outline` function
    new_mask = np.zeros_like(image) # assuming your image is grayscaled i.e. no color channel, this should be a 2D array
    for i,(img_row, outline_row) in enumerate(zip(image, outlines)):
        bounds = np.where(outline_row &gt; 0)[0] # find the indices of the edges
        new_mask[i, bounds.min():bounds.max()] = 1 # fill-in the new mask
    return image * new_mask
</code></pre>
<p>I imagine there is a more vectorized/efficient way to do this, but I think this will get you what you want as a first pass.</p>
","1","Answer"
"78061410","78060462","<p><code>graphviz</code> works with individual trees. You can access the fitted decision trees of the random forest via its <code>.estimators_</code> attribute: <code>fitted_trees = random_forest.estimators_</code>.</p>
<p>For displaying, I think you'll also need to run <code>sudo apt install graphviz</code> and use <code>graphviz.Source(dot_string)</code>. I've included a few options for displaying the graph. By default, the graph can be very wide, so I prefer to handle its size before rendering the plot.</p>
<p><a href=""https://i.sstatic.net/4nIPu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4nIPu.png"" alt=""enter image description here"" /></a></p>
<p>Example with sample data:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.tree import export_graphviz
from IPython import display
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

#
#Create some test data
#
from sklearn.datasets import make_regression
import pandas as pd
import numpy as np

np.random.seed(0)
x_train, y_train = make_regression(n_samples=1000, noise=1)
data = pd.DataFrame(data=x_train, columns=[f'feature_{i}' for i in range(x_train.shape[1])])

#
# Fit and view
#
import graphviz #also install: sudo apt install graphviz

rf = RandomForestRegressor(n_estimators=5, max_depth=2, max_features='sqrt', n_jobs=-1)
rf.fit(x_train, y_train)

for fitted_tree in rf.estimators_:
    str_tree = export_graphviz(
        fitted_tree, 
        out_file=None,
        feature_names=data.columns, 
        filled=True,
        special_characters=True,
        rotate=True,
        precision=1
    )
    
    #Display as png - results in better figure size than default format
    # display.display_png(graphviz.Source(str_tree))
    
    #Alternatively, resize figure and display
    g = graphviz.Source(str_tree)
    g.render('./_temp_tree', format='png')
    fig = plt.figure(figsize=(6, 3), dpi=100)
    fig.add_subplot().imshow(plt.imread('./_temp_tree.png'))
    fig.axes[0].axis('off')
    
    #Or view full size in default graphics format
    # display( graphviz.Source(str_tree) )
</code></pre>
","1","Answer"
"78067920","78067888","<p>I encountered this issue two months ago and could resolved it after spending days on it.</p>
<p>You can restrict the access of your scikit-learn model to the resources using <code>joblib</code>:</p>
<pre><code>from joblib import parallel_backend
import sklearn.neighbors.KNeighborsClassifier as KNN
model = KNN(n_jobs=1)
with parallel_backend(&quot;threading&quot;, n_jobs=1):
    model.fit(X,y)
</code></pre>
","1","Answer"
"78068002","77994342","<p>You can also use the Bayesian Informative Criterion (BIC) to compute the simplicity of the models. BIC depends on the variance of the predictions by each model. If BIC is low, the simplicity is high.</p>
","0","Answer"
"78068645","78060804","<p>After installing shap 41.0, you have to do : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1 . After that if you encounter : dtype: np.bool you can change: np.bool_ in source code. Of course you can see some warning however graph will be produced !</p>
<p>Edit: if you want to use current version only do this : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1</p>
","0","Answer"
"78068866","78067906","<p>As I said in comments, the error was because the model output (last layer) expected the labels in one hot encoding format. The layer had been defined as a dense layer of size <code>(None, num_classes)</code> (where None refers to the batch size) i.e. the labels should be in one hot encoding format. This format consists of vectors of size <code>num_classes</code> (one entry for each class). Thus, for the class, for example, 6, instead of having a number representing it, we have a vector of size 9 (number of classes) where position 5 (label 6) contains a 1 while the others contain 0: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. In reality, what this format represents is a vector where each index corresponds to the probability of each of the classes.</p>
<p>Tensorflow allows encoding labels in this format within the <code>image_dataset_from_directory</code> function using the input variable <code>label_mode = &quot;categorical&quot;</code>.</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(
       ...
       label_mode = &quot;categorical&quot;
)
</code></pre>
","2","Answer"
"78070382","78064402","<p>This is probably a rather roundabout way to get there, but it appears to work.  I may yet try to clean this up later.</p>
<pre class=""lang-py prettyprint-override""><code># Generate the list of nodes throughout the process,
# and an array that for each node index indicates the iteration
# at which it got merged with another.
nodes = [[i] for i in range(len(X))]
merged_at_stage = -np.ones(len(X) + len(model.children_), dtype=int)
for i, merge in enumerate(model.children_):
    a, b = merge
    nodes.append(nodes[a] + nodes[b])
    merged_at_stage[a] = i
    merged_at_stage[b] = i

# For a fixed number of clusters, identify the nodes
# at that point in the process
N_CLUSTERS = 2
clusters = [
    nodes[i] 
    for i, x in enumerate(merged_at_stage)
    if (
        x &gt;= len(X) - N_CLUSTERS  # the node hasn't already been merged with another
        and i &lt;= len(X) + len(model.children_) - N_CLUSTERS  # the node has already been created
    )
]
</code></pre>
<p><code>clusters</code> is then a list of lists of indices.  To turn that into a series of cluster labels (for scoring e.g.):</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
y_pred = pd.Series([-1] * len(X))
for i, cluster in enumerate(clusters):
    y_pred[cluster] = i
</code></pre>
<p><a href=""https://github.com/bmreiniger/datascience.stackexchange/blob/master/SO78064402_AgglomerativeClustering_intermediate_clusters.ipynb"" rel=""nofollow noreferrer"">Colab notebook using the Iris dataset</a></p>
","0","Answer"
"78072281","78070068","<p>This part of code looks fine to me. I think the issue is that how you preprocess <code>x_collate</code> and <code>t_collate</code>. You should check whether they are identical for each batch in your experiment.<br></p>
<p>By the way, it's a convention to use <code>Dataset</code> and <code>Dataloader</code> to preprocess training data and wrap it to batches. See https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:~:text=class%20CustomImageDataset(,image%2C%20label and <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:%7E:text=train_dataloader%20%3D%20DataLoader(training_data%2C%20batch_size%3D64%2C%20shuffle%3DTrue)"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:~:text=train_dataloader%20%3D%20DataLoader(training_data%2C%20batch_size%3D64%2C%20shuffle%3DTrue)</a> for examples. You don't need to read the whole tutorial.</p>
","0","Answer"
"78072656","78072628","<p>You need to make sure that 'b' and 'c' are also tensors with the same properties as your 'a' (just set the gradient to 'false').</p>
<p>Adjusted code looks like this:</p>
<pre><code>import torch

a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.], requires_grad=False)  # ensure b is a tensor
c = torch.tensor([6.], requires_grad=False)  # ensure c is also a tensor
d = torch.min(torch.cat((a, b, c)))  # concatenate tensors and compute minimum

# use backward to compute gradients with respect to a
d.backward()

print(a.grad)  # this should print the gradient of a
</code></pre>
","-1","Answer"
"78073262","78040978","<p>Have you tried <code>accelerate test</code> in your cmd terminal? If your installation is successful, this command should output a list of messages and a &quot;test successful&quot; in the end. If this command fails, it means that there is something wrong with your pytorch + accelerate environment. You should reinstall them following the official tutorials. If the command succeeds and you still can't do multi-GPU finetuning, you should report this issue in bitsandbytes' github repo.</p>
<p>Here are some other potential causes.</p>
<ul>
<li><p>Your cuda version is too old. Most tools are built on <strong>12.0 +</strong> nowadays. Yous should update cuda with <a href=""https://developer.nvidia.com/cuda-downloads"" rel=""noreferrer"">this link</a></p>
</li>
<li><p>python version should be <strong>3.10 +</strong>, otherwise you won't be able to install the latest tools with pip</p>
</li>
<li><p><strong>Why do you want to train a quantized model?</strong> Quantization is made to shrink the model for deployment instead of training. This tool is not designed for your purpose. If you finetune your model with quantized parameters, then gradients won't have any impact, because they are simply too small to represent with only 8 bits. If you want to finetune a LLM with limited GPU memory, you should try <strong>lora</strong> or <strong>SFT</strong>. Both of them can freeze some layers to reduce VRAM usage.</p>
</li>
</ul>
","5","Answer"
"78073627","78073319","<p>You need to handle missing values (NaNs)
Quickly you can do something like below:<br />
Option1</p>
<pre><code># Example imputing missing values with mean
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
</code></pre>
<p>Option2:
If missing values are limited,</p>
<pre><code># Example dropping rows with missing values
X_train.dropna(inplace=True)
y_train = y_train[X_train.index]
</code></pre>
","0","Answer"
"78074000","78072628","<p>The problem with your code is the line <code>d = torch.min(torch.tensor([a, b, c]))</code></p>
<p>When you compute <code>torch.tensor([a, b, c])</code>, you create a new tensor that has no computational graph to the <code>a</code>, <code>b</code> or <code>c</code> tensors. For example:</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
c = torch.tensor([6.])
d = torch.tensor([a,b,c])
d.requires_grad
&gt; False
</code></pre>
<p>The solution is to use the <code>min</code> function with the input tensors themselves.</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
c = torch.tensor([6.])
d = a.min(b).min(c)
d.requires_grad
&gt; True
</code></pre>
<p>Note that the gradient of the <code>min</code> function is <code>1</code> for the min value and <code>0</code> for all other values. This means that you will lose gradient signal if the value you want to backprop through is not the min.</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
d = a.min(b)
d.backward()
a.grad
&gt; tensor([1.])

a = torch.tensor([6.], requires_grad=True)
b = torch.tensor([5.])
d = a.min(b)
d.backward()
a.grad
&gt; tensor([0.])
</code></pre>
","3","Answer"
"78074856","78074180","<p>Install a version of <code>click</code> package later than <code>8.0.4</code>.</p>
<p>The <code>Group.resultcallback</code> method was renamed to <code>Group.result_callback</code> in versions of <code>click</code> package greater than <code>8.0.4</code>, .</p>
","2","Answer"
"78077162","78074844","<p>You are using pandas' <code>get_dummies</code> in each data set. So, if a value is present in your test set, that was not present in the training set, it will generate a new feature instead of encode it as a row of zeros for the previously seen (training) features.</p>
<p>Use instead sklearn <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer""><code>OneHotEncoder</code></a>, fit it to (and transform) the training set, and then use the fit encoder to transform the test set.</p>
","-1","Answer"
"78077392","78076083","<p>The datatype <code>np.long</code> was <a href=""https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"" rel=""nofollow noreferrer"">deprecated</a> in NumPy <code>1.20</code> and removed from NumPy <code>1.24</code>. According to current <a href=""https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.int_"" rel=""nofollow noreferrer"">documentation</a> for version <code>1.25</code>, you need to use <code>np.int_</code> instead.</p>
<p>In your case, go to <code>&quot;/content/3DDFA_V2/bfm/bfm.py&quot;</code> and replace line 34 with:</p>
<pre><code>self.keypoints = bfm.get('keypoints').astype(np.int_)
</code></pre>
","1","Answer"
"78077402","78077221","<p>You seem to be trying to pass the output from a CNN layer directly into the quantum layer without ensuring the dimensions are compatible.</p>
<p>Try adjusting the dimensions of the input tensor before passing it to the quantum layer. Maybe, add a layer before <code>qlayer</code> in your <code>ImageClassifier</code> which flattens or somehow processes the output of the previous layers to match the expected input size of the quantum layer.</p>
","0","Answer"
"78077661","78077447","<p>If you have a horizontal kernel size of <code>3</code> and want the same behavior as <code>padding='same'</code>, the padding set manually must be symmetrical and equal to <code>1</code> on both ends:</p>
<pre><code>pad = layers.ZeroPadding2D(padding=(1,0))

conv = layers.Conv2D(filters, 
                     kernel_size=(3, 1), 
                     strides=int(1/s), 
                     padding='valid', 
                     data_format='channels_last')
</code></pre>
<p>Then <code>conv(pad(keras_tensor))</code> will have a shape of <code>(128, 1024, 1, 2)</code>.</p>
","0","Answer"
"78079521","78070239","<p>Resolved by not using serverless compute . Basically created new compute in Azure ML and use it while executing notebook</p>
","1","Answer"
"78079833","78073911","<p>To validate YOLOv8 model on a test set do the following:</p>
<ol>
<li>In the <strong>data.yaml</strong> file specify the test folder path as a <code>val</code> argument:</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code>path: ../dataset  # dataset root dir
train: train  
val: test  # test directory path for validation

names:
  0: person
  1: bicycle
</code></pre>
<ol start=""2"">
<li>Validate the model:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO

# Load a pretrained YOLOv8 model
model = YOLO('best.pt')

# Run validation on a set specified as 'val' argument
metrics = model.val(data='data.yaml')
</code></pre>
<ol start=""3"">
<li>Get the relevant metrics:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>metrics.results_dict

# output example

{'metrics/precision(B)': 0.8539022762405677,
 'metrics/recall(B)': 0.8012653662055587,
 'metrics/mAP50(B)': 0.8833459433957722,
 'metrics/mAP50-95(B)': 0.6608424795290528,
 'fitness': 0.6830928259157248}
</code></pre>
<p>More about the validation mode: <a href=""https://docs.ultralytics.com/modes/val/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/val/</a></p>
","4","Answer"
"78080686","78069410","<p>You can follow the procedure below to read the current version of the Delta table:</p>
<p>Add the <a href=""https://stackoverflow.com/questions/77147429/failed-to-detect-schema-please-review-and-update-the-file-format-settings-to-al/77155104#77155104"">Storage Blob Data Contributor</a> role to your Entra ID, where you created the ML workspace to ADLS account. Run the code below to read the current version of the Delta Lake table:</p>
<pre><code>import time
import mltable
current_timestamp = time.strftime(&quot;%Y-%m-%dT%H:%M:%SZ&quot;, time.gmtime())
path = &quot;abfss://&lt;containerName&gt;@&lt;storageAccountName&gt;.dfs.core.windows.net/&lt;deltaTablePath&gt;&quot;
tbl = mltable.from_delta_lake(delta_table_uri=path, timestamp_as_of=current_timestamp)
df = tbl.to_pandas_dataframe()
df
</code></pre>
<p>You will see the Delta table as shown below:</p>
<p><img src=""https://i.imgur.com/Gwm1cU7.png"" alt=""enter image description here"" /></p>
<p>For more information, you can refer to <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&amp;tabs=cli#a-note-on-defining-paths-for-delta-lake-tables"" rel=""nofollow noreferrer"">this</a>.</p>
","0","Answer"
"78080725","78080687","<p>You can use <a href=""https://docs.python.org/3/library/functions.html#zip"" rel=""nofollow noreferrer""><code>zip</code></a>. Given two lists (e.g. input and target paths), this shuffles them together:</p>
<pre><code>tmp_list = list(zip(input_list, target_list))
np.random.shuffle(tmp_list)
</code></pre>
","1","Answer"
"78081999","78078811","<p>As is, the operation is not differentiable.
Think about the definition of the gradient:</p>
<pre><code>                     e[:(d+eps)] - e[:(d+eps)]
    df/dd  = limit       ------------
           eps-&gt;0              eps
</code></pre>
<p>So you would need to a define what it means to take &quot;a little bit more of the e then d&quot;.</p>
<p>A common trick to do this is to soften the expression:
Instead of indexing you could multiply by a mask that is almost 1 for values below d and almost zero for values above d, with a smooth transition in between.
But this will always result in values in f that are not actually in e, but some interpolation or weighted values.</p>
<p>What do you want to achieve in the end?
Maybe there is another way that retains gradients?</p>
","0","Answer"
"78082682","78080687","<p>You can shuffle the indices of X_train and y_train simultaneously using the same shuffled indices. Here's how you can modify your on_epoch_end() method to achieve this:
def on_epoch_end(self):
'Updates indexes after each epoch'
if self.shuffle:
# Generate shuffled indices
shuffled_indices = np.arange(len(self.file_paths))
np.random.shuffle(shuffled_indices)</p>
<pre><code>        # Shuffle both file_paths and labels arrays using the same shuffled indices
        self.file_paths = [self.file_paths[i] for i in shuffled_indices]
        self.labels = [self.labels[i] for i in shuffled_indices]
</code></pre>
","1","Answer"
"78084194","78072453","<p>The two models have different <code>forward</code> logic. If you inspect the result of <code>nn.Sequential(*list(model.children())[:-1])</code>, you'll see you're putting a <code>ModuleList</code> into a <code>Sequential</code> block which doesn't make sense. The specific error is trying to call the <code>forward</code> method of <code>ModuleList</code> which isn't implemented.</p>
<p>You need to adapt your logic to each model architecture.</p>
","0","Answer"
"78084257","78078811","<p>You can't backprop through the index values of a slice. You can only backprop through the tensor being sliced.</p>
<p>If you want selection to be learned, you need to have a &quot;soft&quot; selection method. You can do this with the gumbel softmax reparameterization trick</p>
<pre class=""lang-py prettyprint-override""><code>e = torch.arange(10) # the tensor we want to select from
logits = torch.randn(e.shape, requires_grad=True) # selection logits, predicted by model

soft_selection = F.softmax(logits, dim=0) # soft selection weights

selection_index = soft_selection.argmax() # index we want to select
hard_selection = torch.zeros_like(logits) # selection mask
hard_selection[selection_index] = 1. # add 1 at selection index

# this passes the soft_selection through the hard_selection tensor
hard_selection = hard_selection - soft_selection.detach() + soft_selection

selection = e * hard_selection # selection via multiplication 

selection.backward(gradient=torch.ones_like(selection)) # example backprop

assert logits.grad is not None # gradients have been passed through
</code></pre>
","1","Answer"
"78087089","78076239","<p>To solve this problem, we need three ideas:</p>
<ul>
<li><p>The gradients of the outputs with respect the the parameters is the Jacobian of the network wrt the parameters. <a href=""https://pytorch.org/functorch/stable/generated/functorch.jacrev.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/stable/generated/functorch.jacrev.html</a></p>
</li>
<li><p>We can functionalize a pytorch model, that is transform a model into a function of its parameters <a href=""https://pytorch.org/functorch/nightly/generated/functorch.functionalize.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/nightly/generated/functorch.functionalize.html</a></p>
</li>
<li><p>Pytorch can vectorize over many operations using vmap <a href=""https://pytorch.org/functorch/stable/generated/functorch.vmap.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/stable/generated/functorch.vmap.html</a></p>
</li>
</ul>
<p>This is all part <code>functorch</code> / <code>torch.func</code>.</p>
<p>Putting it all together, this does the same as your code:</p>
<pre><code># extract the parameters and buffers for a funcional call
params = {k: v.detach() for k, v in net.named_parameters()}
buffers = {k: v.detach() for k, v in net.named_buffers()}

def one_sample(sample):
    # this will calculate the gradients for a single sample
    # we want the gradients for each output wrt to the parameters
    # this is the same as the jacobian of the network wrt the parameters

    # define a function that takes the as input returns the output of the network
    call = lambda x: torch.func.functional_call(net, (x, buffers), sample)
    
    # calculate the jacobian of the network wrt the parameters
    J = torch.func.jacrev(call)(params)
    
    # J is a dictionary with keys the names of the parameters and values the gradients
    # we want a tensor
    grads = torch.cat([v.flatten(1) for v in J.values()],-1) 
    return grads

# no we can use vmap to calculate the gradients for all samples at once
grads2 = torch.vmap(one_sample)(X.flatten(1))

print(torch.allclose(grads,grads2))
</code></pre>
<p>It <em>should</em> run in parallel, you should try it out for bigger models etc, I did not benchmark it.</p>
<p>This is also related, to for example <a href=""https://stackoverflow.com/questions/50175711/pytorch-gradient-of-output-w-r-t-parameters"">Pytorch: Gradient of output w.r.t parameters</a> (which tbh doesn't have a great answer), and pytorch.org/tutorials/intermediate/per_sample_grads.html which shows some of the functions within torch.func for calculating the per sample gradients.</p>
","1","Answer"
"78087821","78069410","<p>For the below code to work, you need to create a mltable(data asset) with correct folder path.</p>
<pre><code>import time
   import mltable
   from azure.ai.ml import MLClient
   from azure.identity import DefaultAzureCredential
   current_timestamp = time.strftime(&quot;%Y-%m-%dT%H:%M:%SZ&quot;, time.gmtime())
   ml_client = MLClient.from_config(credential=DefaultAzureCredential())
   data_asset = ml_client.data.get(&quot;&lt;enter your ml table name&gt;&quot;, version=&quot;1&quot;)
 
   tbl = mltable.from_delta_lake(delta_table_uri=data_asset.path, 
   timestamp_as_of=current_timestamp)
   df = tbl.to_pandas_dataframe()
   df
</code></pre>
","0","Answer"
"78088713","77937339","<p>Anyone else who stumbles upon this problem: this nasty message seems to indicate wrong input shape to <code>nn.Linear</code>. This is a <a href=""https://github.com/pytorch/pytorch/issues/119161"" rel=""nofollow noreferrer"">known bug</a> in pytorch.</p>
","0","Answer"
"78093160","78092718","<p>What you're seeing has nothing to do with compression. When saving a checkpoint during training, the YOLO training script saves (in addition to other metadata) weights of the current model, weights of the models EMA, and the optimizers parameters. These are needed to continue training from a checkpoint, but only the EMA is kept for inference.</p>
<p>To remove the optimizer and current model, you can use ultralytics' utility function <code>strip_optimizer</code>, which strips the unnecessary weights from the state dict found at a given path, and saves the smaller file at the same location.</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics.utils.torch_utils import strip_optimizer

path_to_checkpoint = &quot;best.pt&quot;
strip_optimizer(path_to_checkpoint)
</code></pre>
","2","Answer"
"78095635","78094285","<p>According to official docs, the SFS is based on <strong>CV score</strong>. But we can change the default scoring method by supplying a metric/ function to &quot;scoring&quot; parameter.</p>
<p><a href=""https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py</a>:</p>
<p>&quot;It is quite remarkable considering that SFS makes no use of the coefficients at all.&quot;</p>
","0","Answer"
"78098100","78040978","<ol>
<li>Go to <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">https://pytorch.org/</a></li>
<li>Select your config</li>
<li>on your env, run the given code</li>
</ol>
<p>For example, I choosed a stable / windows / python /CUDA 11.8, the website gave me this:</p>
<p><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code></p>
<p>(thanks to Niraj Pahari, I just wanted to make it a official answer and not only a commentary)</p>
","1","Answer"
"78098142","78093891","<blockquote>
<p>Are there any other factors in above code, besides the default optimizer (SGD), that can introduce randomness in the output of above neural network model?</p>
</blockquote>
<p>Two other factors are that layer weights will be initialised with random values; and data shuffling (if enabled) would also be a source of randomness.</p>
<blockquote>
<p>How can I modify the provided code to ensure that the model produces the same output for the same input?</p>
</blockquote>
<p>Set the random seed for various random number generators at the start of the script:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import random
import tensorflow as tf

#Set seeds for consistent results each run
seed_value = 0
np.random.seed(seed_value)
random.seed(seed_value)
tf.random.set_seed(seed_value)
</code></pre>
<p>If you're working in a notebook, and you run a cell multiple times (outside of the seeding above), then each run of the cell will give you a different result if the cell depends on a random generator. So make sure to set the seed inside that cell as well. That way, each time you run the cell, it will initialise at the same place.</p>
<p>Might also be worth seeing <a href=""https://stackoverflow.com/a/52897216/21896093"">this</a> answer for notes about setting the <code>PYTHONHASHSEED</code> environment variable. I haven't needed to configure it. Your use-case may or may not require it.</p>
","1","Answer"
"78098838","78098823","<p>Check your data shape after each layer, so can see it's incompatible in shape with your Linear layer which require input_dim of 768, but it receive 50176</p>
","1","Answer"
"78102720","78102637","<p>The pytorch SGD implementation is actually independent of the batching!
It only uses the gradients that were calculated and stored in the parameters <code>.grad</code> attribute in the backward pass.
So the batch size used for calculations and the batch size used for optimization are decoupled.</p>
<p>You can now either:</p>
<p>a) Put all your samples as one big batch through your model by setting the batchsize to the dataset size or</p>
<p>b) Accumulate the gradients for many smaller batches before doing a single step of the optimizer (Pseudo-code):</p>
<pre><code>model = YourModel()
data = YourDataSetOrLoader()
optim = torch.optim.SGD(model.parameters())
for full_batch_step in range(100)
   #this sets the accumulated gradient to zero
   optim.zero_grad()
   for batch in data:
      f=model(data)
      # this adds the gradient wrt to the parameters for the current datapoint to the model paramters
      f.backward()
  # now after we summed the gradient for all samples, we do a GD step.
  optim.step()
   
</code></pre>
","2","Answer"
"78103092","78102877","<p>The problem is here</p>
<pre class=""lang-py prettyprint-override""><code>flattened_size = x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3]

x = x.view(-1, flattened_size)
</code></pre>
<p><code>x</code> is of shape <code>(batch_size, channels, height, width)</code>. You compute <code>flattened_size = batch_size * channels * height * width</code> - the error is including the batch dimension. This means when you compute <code>x = x.view(-1, flattened_size)</code>, the output <code>x</code> will always have size <code>(1, flattened_size)</code>.</p>
<p>In your loss calculation, you attempt to compute <code>loss(x,y)</code> where <code>x</code> has shape <code>(1, flattened_size)</code> and <code>y</code> has shape <code>(bs,)</code>, causing the error.</p>
<p>You should preserve the batch dimension when flattening:</p>
<pre class=""lang-py prettyprint-override""><code>x = x.view(x.shape[0], -1)
</code></pre>
<p>You will also need to resize the input to the first fully connected layer.</p>
","2","Answer"
"78107378","78019286","<p><strong>disclaimer:</strong> I am the author of the <code>borb</code> library.</p>
<p>You are (most likely) trying to run some code that was generated by chatGPT (or similar). I know (from experience) that it will try to invent new classes that simply don't exist in <code>borb</code>.</p>
<p>Recently even, I did an experiment where I fed chatGPT the entire <code>README.md</code> of <code>borb</code> to see if it could answer a simple query. And it failed. And in one of these tests, it also used a non-existent class <code>Run</code>.</p>
<p>I would suggest you read the documentation. It's very comprehensive, and easy to follow along. You don't need an AI to write the code for you.</p>
","1","Answer"
"78107701","78100096","<p>You are not passing the dates to your model. No idea what the optimal way is, but something like converting your dates to unix time, or just a plain index column seems to work:</p>
<pre><code>from datetime import timezone

data = pd.read_csv(file_path, index_col='Date', sep='\t', parse_dates=True)
data.reset_index(level=0, inplace=True)
data[&quot;Date&quot;] = data[&quot;Date&quot;].map(lambda x: x.replace(tzinfo=timezone.utc).timestamp())

# Split the data into training and testing sets before scaling
features = data.drop(&quot;Dhaka_Dengue&quot;, axis=1).values
target = data[&quot;Dhaka_Dengue&quot;].values

# Scaling features
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.reshape(-1, 1)).flatten()

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    features_scaled, target_scaled, test_size=0.2, random_state=42
)
</code></pre>
<p><a href=""https://i.sstatic.net/OpBaJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OpBaJ.png"" alt=""enter image description here"" /></a></p>
<pre><code>_, y_test_sorted = zip(*sorted(zip(X_test[:, 0], y_test)))
_, y_pred_sorted = zip(*sorted(zip(X_test[:, 0], test_predictions)))

plt.plot(y_test_sorted)
plt.plot(y_pred_sorted)
</code></pre>
<p><a href=""https://i.sstatic.net/mw1SL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mw1SL.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"78109756","78099801","<p>I merged the fake and real data into a single matrix, and created a binary vector <code>y</code> where <code>1</code> means real and <code>0</code> means fake. The data had some invalid values (<code>np.inf</code>), which I replaced with the maximum allowable value for <code>float16</code>. I trained a classifier and got 87% classification accuracy on the validation set.</p>
<p>You'll probably also want to split a test set off. In the code I just worked with a training set and a validation set.</p>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>Accuracy: 0.87
Classification Report:
              precision    recall  f1-score   support

         0.0       0.84      0.89      0.87       141
         1.0       0.90      0.85      0.87       159

    accuracy                           0.87       300
   macro avg       0.87      0.87      0.87       300
weighted avg       0.87      0.87      0.87       300

Confusion Matrix:
[[126  15]
 [ 24 135]]
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#
# Load the real and fake data, and stack into a single matrix
#
data_real = np.load('real.npz')['x']
data_fake = np.load('fake.npz')['x']
data = np.concatenate([data_real, data_fake], axis=0)

#Target vector: y=1 corresponds to real, and y=0 corresponds to fake
y = np.concatenate([np.ones(len(data_real)), np.zeros(len(data_fake))])

#Deal with inf by replacing with max value allowed
X = np.nan_to_num(data, posinf=np.finfo(data.dtype).max)
X = X.reshape(X.shape[0], -1) #flatten

#Spline into train and validation sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=0.8, shuffle=True, random_state=0
)

#Optionally reduce the dimensionality
# This signficantly speeds up the clf.fit() step (from 40s down to 3s)
# It also reduces memory consumption
# But accuracy goes down from about 87% to 77%
# Could be useful for speeding up exploratory analyses
reduce_dimensionality = False
if reduce_dimensionality:
    from sklearn.random_projection import SparseRandomProjection
    projector = SparseRandomProjection(n_components='auto', random_state=0).fit(X_train)    
    X_train, X_test = [projector.transform(x) for x in [X_train, X_test]]

#Fit classifier
np.random.seed(0)
clf = ExtraTreesClassifier(n_estimators=400)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(&quot;Accuracy:&quot;, accuracy)

print(&quot;Classification Report:&quot;)
print(classification_report(y_test, y_pred))

conf_matrix = confusion_matrix(y_test, y_pred)
print(&quot;Confusion Matrix:&quot;)
print(conf_matrix)
</code></pre>
","0","Answer"
"78112733","78112607","<p>Since I don't have the csv, I would do the below to troubleshoot why reading from the csv does not work;</p>
<blockquote>
<p>Assumption: there are 2 rows per line in csv, so we create X and Y as lists using the below for loop</p>
</blockquote>
<pre><code>data = pd.read_csv('C:/Users/Teacher/Downloads/data.csv')
X, Y = [], []
for i in data:
  X.append(i.split()[0])
  Y.append(i.split()[1])
</code></pre>
","1","Answer"
"78114032","78112023","<p>From the error message it seems like some of the hyperparameter combinations could be leading to the error condition. Some of your fits run fine but a portion fail. Remove <code>1</code> from the list of values for <code>min_samples_split</code>, as it has to be 2 or greater.</p>
<p>If that doesn't resolve the error, add <code>error_score='raise'</code> to <code>GridSearchCV</code>, so that when it encounters an error it will print the full stack trace.</p>
","0","Answer"
"78115423","78114397","<p>You could combine the tabular and image data into a single matrix of <code>n_samples x (tabular features + image features)</code>. Then create tabular/image pipelines such that the tabular one pulls out the tabular columns, and the image pipeline pulls out the image columns. That way you can supply the same data to both estimators, and they will internally deal with selecting the columns they need before running their classifier. I've modified the code to this effect.</p>
<p>The steps are:</p>
<ul>
<li>Image data is shaped into an <code>n_samples x n_pixels</code> matrix</li>
<li>Tabular data is shaped into an <code>n_samples x n_features</code> matrix</li>
<li>The two matrices are concatenated into a wider matrix that has all the columns</li>
<li>The tabular pipeline is [tabular column selector -&gt; tabular classifier]. The column selector step is a <code>ColumnTransformer</code> that selects the tabular columns and discards the rest. Similar definitions for the image pipeline.</li>
</ul>
<p>The tabular, image, and stacked classifiers all work with the same input data matrix (<code>X_train</code>, <code>y_train</code>).</p>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>Checking pipe_image | score=0.900
  [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 1 0 2 2 2 2 2 0 0] 

Checking pipe_tabular | score=1.000
  [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0] 

Stacked accuracy: 1.000
</code></pre>
<p>The orange boxes are the individual pipelines for the tabular/image data, and the blue outline is the stacking classifier.</p>
<p><a href=""https://i.sstatic.net/JsmUh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JsmUh.png"" alt=""enter image description here"" /></a></p>
<hr />
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

np.random.seed(0)

# Generate some example data 
# For demonstration purposes, I'm using the Iris dataset
data = load_iris()
X_images = data.data[:, :2]  # Let's say that these are my images
X_images_flat = X_images.reshape(len(X_images), -1) #flatten each sample from 2D to 1D if required

X_tabular = data.data[:, 2:]  # Unidimensional vector of tabular data 
y = data.target  # Target classes (3 classes)

X_images_and_tabular = np.concatenate([X_images_flat, X_tabular], axis=1)
images_col_indices = np.arange(X_images_flat.shape[1])
tabular_col_indices = np.arange(X_images_flat.shape[1], X_images_and_tabular.shape[1])

#
# Make 2 column selectors
#  Each transformer will &quot;passthrough&quot; the image or tabular columns and
#   &quot;drop&quot; the remaining columns, depending on whether it is selecting the
#   image data or tabular data
#
from sklearn.compose import ColumnTransformer

image_columns_selector = ColumnTransformer(
    [('image_columns_selector', 'passthrough', images_col_indices)],
    remainder='drop'
)

tabular_columns_selector = ColumnTransformer(
    [('tabular_columns_selector', 'passthrough', tabular_col_indices)],
    remainder='drop'
)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_images_and_tabular, y, test_size=0.2, random_state=42
)

# Define the MLP for images
image_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation=&quot;relu&quot;, max_iter=3000)

# Define the Random Forest for tabular data
tabular_model = RandomForestClassifier(n_estimators=100, criterion=&quot;gini&quot;)

#MLP pipeline for images
from sklearn.pipeline import make_pipeline
pipe_image = make_pipeline(image_columns_selector, image_model)
image_pipe_acc = accuracy_score(
    y_test,
    pipe_image.fit(X_train, y_train).predict(X_test)
)
print(f'Checking pipe_image | score={image_pipe_acc:.3f}\n ', pipe_image.predict(X_test), '\n')

#features pipeline
pipe_tabular = make_pipeline(tabular_columns_selector, tabular_model)
tabular_pipe_acc = accuracy_score(
    y_test,
    pipe_tabular.fit(X_train, y_train).predict(X_test)
)
print(f'Checking pipe_tabular | score={tabular_pipe_acc:.3f}\n ', pipe_tabular.predict(X_test), '\n')

# Create a stacked model
stacked_model = StackingClassifier(
    estimators=[
        (&quot;image_mlp&quot;, pipe_image),
        (&quot;tabular_rf&quot;, pipe_tabular),
    ],
    final_estimator=RandomForestClassifier(n_estimators=100),
)

#The stacked model throw the ValueError
stacked_model.fit(X_train, y_train)
print(
    'Stacked accuracy:',
    '%5.3f' % stacked_model.score(X_test, y_test)
)
</code></pre>
","1","Answer"
"78117104","78112634","<p>I've amended the code and it works now. I separated out the coefficients into a weight term and a bias term. I am not sure about the way you were updating the coefficients, so I changed that part to ensure that the updates occur for each instance.</p>
<p><a href=""https://i.sstatic.net/ax2Vq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ax2Vq.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>#numpoint
n = 300
#f(x) = w0 + ax1 + bx2
#then if f(x) = 0
#x2 = (-w0 - ax1)/b 
intercept = 30
a = 4
b = 2
#generate random points from 0 - 20
x1 = np.random.uniform(-20, 20, n) #return a np array
x2 = np.random.uniform(-20, 20, n)
y = []
#plot f(x)
plt.plot(x1, (-intercept - a*x1)/b, 'k--', label='ground truth') 
plt.ylabel(&quot;x2&quot;)
plt.xlabel(&quot;x1&quot;)

#plot colored points
for i in range(0, len(x1)):
    f = intercept + a * x1[i] + b * x2[i]
    if (f &lt;= 0):
        plt.scatter(x1[i], x2[i], c='tab:red')
        y.append(-1)
    if (f &gt; 0):
        plt.scatter(x1[i], x2[i], c='tab:blue')
        y.append(1)
y = np.array(y)
# Add x0 for threshold
stacked_x = np.row_stack((x1, x2))

class PLA():
    def __init__(self, numPredictors):
        self.w = np.random.rand(numPredictors)
        self.b = 0
        self.numPredictors = numPredictors
        
    def fitModel(self, xData, yData):
        n_errors = np.inf
        while(n_errors):
            n_errors = 0
            for xi, yi in zip(xData.T, yData.reshape(-1, 1)):
                linear = np.dot(xi, self.w) + self.b
                yhat = 1 if (linear &gt; 0) else -1
                
                error = yi - yhat
                self.w = self.w + error * xi
                self.b = self.b + error
                
                if yhat != yi:
                    n_errors += 1 

pla1 = PLA(2)
pla1.fitModel(stacked_x, y)

plt.plot(x1, (-pla1.b - pla1.w[0]*x1)/pla1.w[1], 'g-', alpha=0.5, linewidth=5, label = &quot;PLA&quot;)
plt.xlabel(&quot;x1&quot;)
plt.ylabel(&quot;x2&quot;)
plt.legend()
plt.gcf().set_size_inches(8, 3)
plt.ylim(-22, 22) #clip y limits
</code></pre>
","0","Answer"
"78117380","78111173","<p>I figured it out. It is because my config.properties was wrong. This is correct.</p>
<pre><code>models={\
  &quot;prompt_injection_model&quot;: {\
    &quot;1.0&quot;: {\
        &quot;defaultVersion&quot;: true,\
        &quot;marName&quot;: &quot;prompt_injection_model.mar&quot;,\
        &quot;minWorkers&quot;: 2,\
        &quot;maxWorkers&quot;: 5,\
        &quot;batchSize&quot;: 128,\
        &quot;maxBatchDelay&quot;: 20,\
        &quot;responseTimeout&quot;: 60\
    }\
  }\
}
</code></pre>
","0","Answer"
"78133976","77963476","<p>This should work with the latest mlx: <code>pip install -U mlx</code></p>
<pre class=""lang-bash prettyprint-override""><code>&gt;&gt;&gt; import mlx.core as mx
&gt;&gt;&gt; mask = mx.array([True, False])
&gt;&gt;&gt; mx.where(mask, mx.array(float(&quot;-inf&quot;)), mx.array(0.0))
array([-inf, 0], dtype=float32)
</code></pre>
","1","Answer"
"78134155","78115138","<p>Here can be 2 solutions:</p>
<ol>
<li>If we are talking about MSE and its derivative then there is one thing missing in your code - division by number of samples. You got quite big gradient values and that can be a reason you cannot reach cost function minimum. So I would recommend you to try this: <code>gradient_matrix = -2 * gradient_matrix / len(self.x_vector)</code></li>
<li>In case you really want to keep using &quot;(not-normalized) squared error&quot; - decrease <code>step_size</code> value to decrease gradient values and do not miss function minimum</li>
</ol>
","1","Answer"
"78137397","78057268","<p>Ok, I've tracked it down. There were TINY differences between the X datasets. I.e. I had values like 12.799 vs 12.8, but only a handful of them (&lt;10 instances in &gt;3k rows, &gt;200 columns). I didn't think this would have such a large domino effect on the resulting models.</p>
<p>Rounding all data to 2 decimal places resulted in the exact same models being produced.</p>
","0","Answer"
"78153557","77967230","<p>The question above is already very close to the answer, however it requires a little bit of tweaking. <br><br>
First of all we add a id mapping step before creating the test and train split. This will allow for the system to be able to label with the class's name instead of simply the ID.</p>
<pre class=""lang-py prettyprint-override""><code>ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)

def map_label2id(example):
  for label in all_labels:
    example[label] = ClassLabels.str2int(example[label])
  return example

dataset = dataset.map(map_label2id, batched=True)

for label in labels_list:
  dataset = dataset.cast_column(label, ClassLabels)
</code></pre>
<p>The creation of the model has been changed to:</p>
<pre class=""lang-py prettyprint-override""><code>model = ViTForImageClassification.from_pretrained(
    model_str,
    num_labels=len(labels_list),
    problem_type=&quot;multi_label_classification&quot;
)

model.config.labels = labels_list
model.config.label2id = label2id
model.config.id2label = id2label
</code></pre>
<p>Then I changed the collate_fn function (which solves the specific bug described above) by changing the return statment to:</p>
<pre class=""lang-py prettyprint-override""><code>return {&quot;pixel_values&quot;: pixel_values, &quot;labels&quot;: labels.float()}
</code></pre>
<p>The last function changed was the compute_metrics. This is important, as the metrics which you wish to calculate with a multiclass or multilabel model vary differently.</p>
<pre class=""lang-py prettyprint-override""><code>def compute_metrics(eval_pred):
    predictions = eval_pred.predictions
    label_ids = eval_pred.label_ids

    predicted_labels = (predictions &gt; 0).astype(float)

    temp = 0
    for i in range(label_ids.shape[0]):
        temp += sum(np.logical_and(label_ids[i], predicted_labels[i])) / sum(np.logical_or(label_ids[i], predicted_labels[i]))
    acc_score = temp / label_ids.shape[0]

    temp = 0
    for i in range(label_ids.shape[0]):
        temp += np.size(label_ids[i] == predicted_labels[i]) - np.count_nonzero(label_ids[i] == predicted_labels[i])
    hamming_loss = temp / (label_ids.shape[0] * label_ids.shape[1])

    return {
        &quot;accuracy&quot;: acc_score,
        &quot;hamming_loss&quot;: hamming_loss
    }
</code></pre>
","0","Answer"
"78184341","78066771","<p>You might consider:</p>
<pre><code>model.add(Flatten())
</code></pre>
<p>It would help if you flattened the prediction somewhere.</p>
","0","Answer"
"78210661","78089332","<p>This is just a problem with the tensorflow version. It can be solved with tensorflow==2.15.0. It has nothing to do with scikit-learn, scikeras, and python versions.</p>
","2","Answer"
"78228826","78031144","<p>From the <a href=""https://github.com/wagenaartje/neataptic"" rel=""nofollow noreferrer"">official repository</a>:</p>
<pre class=""lang-js prettyprint-override""><code>// this network learns the XOR gate (through neuro-evolution)
var network = new Network(2,1);

var trainingSet = [
  { input: [0,0], output: [0] },
  { input: [0,1], output: [1] },
  { input: [1,0], output: [1] },
  { input: [1,1], output: [0] }
];

await network.evolve(trainingSet, {
  equal: true,
  error: 0.03
});
</code></pre>
<p>This implies the data type for the samples is expected to be:</p>
<pre class=""lang-js prettyprint-override""><code>let trainingSet: { input: number[], output: number[] }[];
</code></pre>
<p>Which is an array of objects with a list of numbers for the property <code>input</code>, and a list of numbers for the property <code>output</code>.</p>
","0","Answer"
"78231978","78089332","<p>Downgrading tensorflow to version 2.15 did the trick.</p>
<pre><code>tensorflow==2.15
scikit-learn==1.14.post1
scikeras==0.12
</code></pre>
","2","Answer"
"78288467","78058612","<p>Input and output were deprecated. Use components instead.</p>
","0","Answer"
"78358420","78040978","<p>Yes, it has to do with CUDA, make sure you have the latest one installed and check it with <code>nvcc -V</code></p>
","0","Answer"
"78418188","78040978","<p>If you're running the code on Google Colab, make sure to change the runtime environment and select the Hardware Accelerator to be GPU.</p>
","1","Answer"
"78474696","78027259","<p>ARIMA cannot handle an input sequence that is not part of the history data. This is because ARIMA relies on the prediction errors of the past time-steps to predict the next time-step (for the MA part). These errors are not available if the sequence was never seen before.</p>
","0","Answer"
"78522702","78112934","<p>I just open an issue at langchain here:  <a href=""https://github.com/langchain-ai/langchain/issues/22063"" rel=""nofollow noreferrer"">https://github.com/langchain-ai/langchain/issues/22063</a></p>
<p>the same problem about VectorstoreIndexCreator</p>
","0","Answer"
"78548714","78073911","<p>There is a very convenient way with Yolov8 to solve your Problem without chanching the path for val. You can have your test images in folder &quot;test&quot; with folders &quot;images&quot; &amp; &quot;labels&quot; inside and just change the input argument split of model.val() to split=&quot;test&quot;. The default for the split arg is &quot;val&quot;, which means it takes the images valid folder by default. Changing the arg to split=&quot;test&quot; you specify that the folder &quot;test&quot; should be used for running validation. Like so:</p>
<pre><code>model = YOLO(model=&quot;best.pt&quot;) 

model.val(data=yaml_file_path_of_yourData, split=&quot;test&quot;)
</code></pre>
","3","Answer"
"78655603","77996526","<p><strong>If your model is uploaded to <em>Vertex Model Registry</em></strong>, you should simply know the <code>model_id</code> of the model you would like to use to make batch prediction (you can arbitrarily choose a <code>model_id</code> when uploading to <em>Vertex Model Registry</em>).</p>
<p>If you don't remember your <code>model_id</code>, it can be found on <em>Vertex Model Registry</em> page after selecting your model under <em>Version Details</em> tab:</p>
<p><a href=""https://i.sstatic.net/AMuqpv8J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AMuqpv8J.png"" alt=""enter image description here"" /></a></p>
<p>In this way you can create your batch predict task:</p>
<pre><code>@component(packages_to_install=[&quot;google-cloud-aiplatform&quot;])
def create_batch_prediction_job(
    model_id: str,
    project: str,
    location: str,
    job_display_name: str,
    prediction_dataset: Input[Dataset],
):
    from google.cloud import aiplatform

    aiplatform.init(project=project, location=location)
    
    # the model with the &quot;default&quot; alias will be targeted
    model = aiplatform.Model(model_name=model_id)

    batch_prediction_job = model.batch_predict(
        job_display_name=job_display_name,
        gcs_source=prediction_dataset.uri, 
        gcs_destination_prefix=&quot;gs://my/bucket/destination&quot;,
        instances_format=&quot;csv&quot;,
        predictions_format=&quot;jsonl&quot;,
        starting_replica_count=1,
        max_replica_count=1,
        sync=True,
    )

    batch_prediction_job.wait()
</code></pre>
<p>In the end, you can incorporate this task in your pipeline:</p>
<pre><code>@pipeline(name=&quot;prediction-pipeline&quot;)
def pipeline():

    data_task = data_component()

    batch_predict_op = create_batch_prediction_job(
        model_id=&quot;my-model-id&quot;,
        project=&quot;my-project-name&quot;,
        location=&quot;my-location&quot;,
        job_display_name=&quot;my-batch-prediction-job&quot;,
        prediction_dataset=data_task.outputs[&quot;dataset&quot;],
    )
</code></pre>
","1","Answer"
"79109649","78056806","<p>It is a bug of TensorFlow 2.13.
Use TensorFlow 2.15 or higher.</p>
","1","Answer"
"79169006","78071238","<p>I guess you are using tensorflow version greater than 2.15 which contains keras3.0 which was causing the error. Could you please try to import tf_keras with the below commands.<code>!pip install tf-keras</code>, <code>import tf_keras</code>.Also i have modified some steps and code was executed without error.Kindly refer to this <a href=""https://colab.sandbox.google.com/gist/malla456/41a4939dd9243a0fb39d998d289e2414/78071238.ipynb#scrollTo=UHnzSRnIVztr"" rel=""nofollow noreferrer"">gist</a></p>
","0","Answer"
"79296290","78040978","<p>If you are running your code on google colab, you need to install <code>bitsandbytes</code> and then restart your kernel so that your dependencies are updated.</p>
","0","Answer"
"79296474","78040978","<p>After installing the appropriate PyTorch I ran <em>!pip install -U bitsandbytes</em> (to install updated version). It requires restart the session to see the updates. So I restarted the session and installed all remaining packages again! Bingo! it worked (<a href=""https://huggingface.co/google/gemma-2b/discussions/29"" rel=""nofollow noreferrer"">https://huggingface.co/google/gemma-2b/discussions/29</a>)</p>
","0","Answer"
"79350855","78049931","<p>The <em>Pytorch Geometric</em> documentation has a <a href=""https://pytorch-geometric.readthedocs.io/en/2.5.3/notes/create_dataset.html"" rel=""nofollow noreferrer"">great guide</a>. There you can find clear instructions, which methods of the appropriate parent dataset class you need to overwrite. This means you might have to write your own method to read your own fileformat in and to bring it into the <code>Data</code> shape of <em>Pytorch Geometric</em>.</p>
<p>Many commonly used benchmark datasets for standard graph machine learning tasks are integrated into pytorch geometric and you can simply import them and use load them (note you can even add custom preprocessing, if you want):</p>
<pre class=""lang-py prettyprint-override""><code>from torch_geometric.datasets import &lt;Some Dataset&gt;

dataset = &lt;Some Dataset&gt;()

# Ready to use the dataset
</code></pre>
<p>You can find a list of directly available datasets at <a href=""https://pytorch-geometric.readthedocs.io/en/2.5.3/modules/datasets.html"" rel=""nofollow noreferrer"">https://pytorch-geometric.readthedocs.io/en/2.5.3/modules/datasets.html</a></p>
","0","Answer"
"78120372","78119974","<p>I suggest you to use a Pytorch <code>DataLoader</code> for loading data batch by batch instead of doing it manually. In this regard, PyTorch provides a simple solution for this using the <code>drop_last</code> parameter in the <code>DataLoader</code>. When set to True, it drops the last incomplete batch, ensuring that all batches are of the specified size except for the last one.
The <code>Dataloader</code> is a wrapper for the torch <code>Dataset</code>, you can find more info <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"" rel=""nofollow noreferrer"">here</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
from torch.utils.data import DataLoader

X = torch.Tensor(...)  # your features
y = torch.Tensor(...)  # your labels

dataset = 

# Create a DataLoader with drop_last=True
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

model = ...

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Training loop
num_epochs = 5
for epoch in range(num_epochs):
    total_loss = 0
    for batch_x, batch_y in dataloader:
        optimizer.zero_grad()
        y_pred = model(batch_x)
        loss = criterion(y_pred, batch_y)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    average_loss = total_loss / len(dataloader)
    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')
</code></pre>
","1","Answer"
"78121040","78119974","<p>I don't know why the other answer was accepted, as there is a fundamental misunderstanding in the way you constructed the model in the question! And the other answer does not take this into account.<br />
When you define a model and its input and output sizes, you still only consider one sample. You don't use <code>batch_size</code> for scaling the output. When you then give a batch of input data into the model, <code>PyTorch</code> handles the batch internally and the model gets evaulated on each sample in parallel.</p>
<p>You can look at an official <a href=""https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"" rel=""nofollow noreferrer"">PyTorch tutorial</a>, where they built a model for data on the <code>Fashion MNIST</code> dataset. Each image in this dataset is <code>(28x28x1)</code> pixels (greyscale), and there are 10 different classes to predict. Notice the first and last layer:</p>
<blockquote>
<p>nn.Linear(28*28, 512)<br />
....<br />
nn.Linear(512, 10)</p>
</blockquote>
<p>where the input is the image pixels <code>28*28</code> and the output is <code>10</code> numbers for 10 classes. You can then use <code>SoftMax</code> or <code>categorical_crossentropy</code> for prediction. There is no information on batch sizes in the model itself, as the model doesn't need that.</p>
<p>Most of the time it is no problem to have the last batch a bit smaller than the others. If your batch size is <code>32</code>, but the last batch is only <code>15</code> samples, the model will just get 15 samples and labels, do the prediction and compare the 15 results to the 15 labels for the last batch.<br />
If for some reason you need all batches to be exactly the same size (e.g. for a stateful <code>LSTM</code>), then you can use <code>DataLoader</code> with <code>drop_last=True</code>. But most of the time, it is not needed and you just hide data from the model if you use it.</p>
<p>Using the <code>DataLoader</code> is still a good idea, because they can efficiently handle loading your data on CPU, while the model will train on GPU.</p>
","1","Answer"
"78121453","78120856","<p>This ambiguous error comes from a wrong datatype, i.e. because of a multiplication like <code>[1,2] * 1.0 or &quot;hello&quot; * 3</code></p>
<p>Your error comes from a wrong iteration approach similar to this:</p>
<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
&gt;&gt;&gt; for x in df:
...     print(x)
A
B
</code></pre>
<hr />
<p><strong>Your <code>xi, yi</code> variables are strings</strong>, when multiplied with a float leading to the error.</p>
<hr />
<p>There are multiple ways you could change your code. However the most efficient would be to do:</p>
<pre><code># Do not duplicate memory, and use a Series instead of a data frame
x = file.X
y = file.Y
</code></pre>
<p>Now the for loop part throws no error anymore.
Furthermore you can use vectorization without the need of a for loop, if you do full batch gradient descent.</p>
<hr />
<p>PS: Something to remember: iterating over a dataframe is often a bad idea as there are more efficient methods.</p>
","0","Answer"
"78124186","78118283","<p>I have found an answer to the problem.</p>
<p>In the dataloader, the <code>collate_fn</code> needs to be set to the collate_fn in the utils package that torchvision has!</p>
","1","Answer"
"78126184","78126160","<p><code>loss.backward()</code> does not return anything. This is why <code>print(loss.backward())</code> returns <code>None</code>.</p>
<p>You want to use <code>print(loss)</code> instead</p>
","0","Answer"
"78128110","78127879","<p>That's quite a common mistake when handling with ML algorithms at the beginning.</p>
<p>In your second script, you are training the algorithm on the winequality-white.csv dataset, and then you are saving it. That's totally fine.</p>
<p>The problem is that in your third, you are using the algorithm on the exact same dataset you've used for training. You are basically predict observations that you used for training, so it's pretty obvious that the algorithm will predict them with 100% accuracy.</p>
<p>The approach of storing the algorithm is correct, but then you have to use another dataset for actually using it for predictions, not the same you used for training.</p>
","1","Answer"
"78128193","78127612","<p>Your SVC grid has 5×2 points, each fitted for 2 folds, so that should take about 20× as long. You can set <code>verbose=4</code> in the searches to better track what's happening, and consider parallelizing (<code>n_jobs=-1</code> for example).</p>
","0","Answer"
"78132204","78131238","<p>It can be because there is some Columns which have categorical data in your dataset. First you can convert them into numbers using method 1 or method 2:
Method 1:</p>
<pre><code>#Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder

from sklearn.compose import ColumnTransformer

categories = [&quot;col1&quot;, &quot;col2&quot;,&quot;col3&quot;,&quot;col4&quot;]---Columns which have categorical values

one_hot = OneHotEncoder()
transformer = ColumnTransformer([(&quot;one_hot&quot;,
                                   one_hot,
                                   categories)],
                                   remainder=&quot;passthrough&quot;)

transformed_X = transformer.fit_transform(X)
</code></pre>
<p>Method 2: you can convert the particular column Values into int</p>
<pre><code>import warnings

warnings.filterwarnings('ignore')

df['col1']=pd.get_dummies(df['col1'], drop_first=True)
</code></pre>
","0","Answer"
"78137615","78136820","<p>To install RAPIDS locally on Windows, WSL2 is required. Windows native packages are not available. You can refer to the installation guide for more information about supported platforms and how to use WSL2 with conda or Docker images: <a href=""https://docs.rapids.ai/install"" rel=""nofollow noreferrer"">https://docs.rapids.ai/install</a></p>
<p>Alternatively, you could use a cloud deployment of RAPIDS. This page lists some of the options for deployment: <a href=""https://docs.rapids.ai/deployment/stable/"" rel=""nofollow noreferrer"">https://docs.rapids.ai/deployment/stable/</a></p>
","0","Answer"
"78139204","78139097","<h1>train a model</h1>
<pre><code>lr = create_model('lr')
</code></pre>
<p>#get model column names</p>
<pre><code>lr.feature_names_in_
</code></pre>
","0","Answer"
"78139992","78139165","<p>I can replicate your issue by using the Titan dataset from the MLJ function OpenML:</p>
<pre><code>using MLJ
import DataFrames as DF
import DataFramesMeta as DFM

table = OpenML.load(42638)
</code></pre>
<p>and then cleaning it a bit to get exactly the same dataset you are using:</p>
<pre><code>titanic =  DF.dropmissing(titanic);
DF.rename!(titanic, &quot;pclass&quot;=&gt;&quot;class&quot;)
titanic = titanic[:,[:class,:sex,:survived,:age]] # select only the fields you are using

titanic = DFM.@transform(titanic, 
    :class=categorical(:class), 
    :sex=categorical(:sex),  
    :survived=categorical(:survived)
    );
</code></pre>
<p>Now the issue is that the <code>DecisionTreeClassifier</code> model from  the <code>DeicisonTree</code> package is very efficient (fast!) but it requires <em>ordered</em> data only.</p>
<p>In this case you could perhaps coerce the <code>class</code> as an ordered field. An alternative is to use the <code>DecisionTreeClassifier</code> model from <code>BetaML</code>, that at the cost of being a bit slower can use any kind of input, including the Missing ones (so no need to drop them or use only that few fields - the original <code>titan</code> dataset has many more fields):</p>
<pre><code>mod = @load DecisionTreeClassifier pkg = &quot;BetaML&quot; ;
fm = mod() ;
fm_mach = machine(fm, X_trn, y_trn);
fit!(fm_mach)

yhat_trn = mode.(predict(fm_mach , X_trn))
accuracy(y_trn,yhat_trn) # 0.91

yhat_tst = mode.(predict(fm_mach , X_tst))
accuracy(y_tst,yhat_tst) # 0.78
</code></pre>
<p>Note that there is a nice tutorial exactly on fitting the Titan database with Decision Tree and MLJ here: <a href=""https://forem.julialang.org/mlj/julia-boards-the-titanic-1ne8"" rel=""nofollow noreferrer"">https://forem.julialang.org/mlj/julia-boards-the-titanic-1ne8</a> .</p>
","1","Answer"
"78140896","78140043","<p>Permute the axes of your tensor:</p>
<pre><code>x = x.permute(0,3,1,2)
</code></pre>
","1","Answer"
"78140975","78137739","<p>It looks like there's just one <code>model</code> which is being updated by different optimisers, even though the model has only been compiled with one of those optimisers.</p>
<p>Try creating 3 models, each registered to an optimiser. Along the lines of:</p>
<pre class=""lang-py prettyprint-override""><code>.
.
.

# Compile the models, one per optimiser
model_1 = build_model()
model_2 = build_model()
model_3 = build_model()

model_1.compile(loss='mse', optimizer=sgd_optimizer)
model_2.compile(loss='mse', optimizer=minibatch_sgd_optimizer)
model_3.compile(loss='mse', optimizer=batch_sgd_optimizer)

theta1_sgd, theta2_sgd = [], []
theta1_minibatch_sgd, theta2_minibatch_sgd = [], []
theta1_batch_sgd, theta2_batch_sgd = [], []


# Function to perform gradient descent and store theta values
# Supply the model and its optimiser
def perform_gradient_descent(model, optimizer, batch_size=None): #&lt;- now accepts &quot;model&quot; as well
    ... #code is exactly the same as before

# Perform gradient descent with models and their optimizers

# Stochastic Gradient Descent on model_1
loss_sgd, theta1_sgd, theta2_sgd = perform_gradient_descent(
    model_1, sgd_optimizer, batch_size=1
)

# Mini-batch Gradient Descent on model_2
loss_minibatch_sgd, theta1_minibatch_sgd, theta2_minibatch_sgd = perform_gradient_descent(
    model_2, minibatch_sgd_optimizer, batch_size=32
)

# Batch Gradient Descent on model_3
loss_batch_sgd, theta1_batch_sgd, theta2_batch_sgd = perform_gradient_descent(
    model_3, batch_sgd_optimizer, batch_size=len(X_normalized)
)

.
.
.
</code></pre>
<p>Their starting points will differ due to random initialisation. This can be controlled for with additional code that sets the random seed before each model is made.</p>
","0","Answer"
"78141648","78139855","<p>Can you try to set the model to evaluation mode by calling <code>model.eval()</code>. Normally model is set to train mode when it's initialized in which the dropout (randomly) and normalization is activate, we need to deactivate it in evaluation mode.</p>
","0","Answer"
"78145384","78145229","<p>you could look at converting your data into a matrix that covers the entire plain you have on the x,y space with each point representing a part of the matrix, large data points will then be represented automatically. A 1 represents whether there's something in that cell, a 0 representing white space</p>
<p>For example, If you have a 3X3 space, having a matrix space like below:</p>
<pre><code>101
010
111
</code></pre>
<p>and</p>
<pre><code>111
011
101 
</code></pre>
<p>You can do a cosine similarity on both these matrices to evaluate the closeness to each other. There might be some libraries in OpenCV that could help you with this, my answer covers the principle you would need to aim to achieve. The main issue you have is representing your data in a way that's conducive for comparisons.</p>
","1","Answer"
"78146059","78145824","<p>The issue here is that you assign a 2x2 weight matrix and 2x1 bias to your 6x210 linear transformation. If you want to do the same as in the <a href=""https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html"" rel=""nofollow noreferrer"">tutorial</a>, you need to change this:</p>
<pre class=""lang-python prettyprint-override""><code>layer = GCNLayer(c_in=6, c_out=6)
layer.projection.weight.data = torch.eye(6)
layer.projection.bias.data = torch.zeros(6)
</code></pre>
<p>See <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" rel=""nofollow noreferrer"">torch.nn.Linear</a> documentation.</p>
","0","Answer"
"78148218","78146852","<p>Take a look to the scikit-learn <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"" rel=""nofollow noreferrer"">documentation</a>; you will see that to train/fit your model you will require an X of shape (n_samples, n_features), so I would change the fit to the next line (reshaping the X to at least one feature):</p>
<pre><code>model.fit(encoded_X.reshape(-1, 1),encoded_y)
</code></pre>
<p>Apart from that, LabelEncoder is used typically to encode the target (y) if you need normalize them or transform non-numerical labels. As your target is already numeric, probably you don't need to encode it.</p>
<p>In addition, it is not a good practice to use the LabelEncoder with your categorical features. If you use your code as it is, it is not actually analyzing the text but giving to each unique value one numeric identifier, so, the prompt you type must be one chosen from the 416809 samples.</p>
<p>If what you are trying to do is to know the basics on sentiment analysis by using Scikit-learn, I would start with the following links:</p>
<p>Working with text data: <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></p>
<p>This one looks also interesting:
<a href=""https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/"" rel=""nofollow noreferrer"">https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/</a></p>
","1","Answer"
"78149836","78139855","<h2>In Short</h2>
<p>The main &quot;error&quot; is from <code>pooled_sentence = torch.max(last_hidden_state, dim=1)</code>. Step back think a little what the <code>torch.max</code> is doing and what are you &quot;pooling&quot;.</p>
<hr />
<h2>In Long</h2>
<p>Depending on your longest sentence in the batch your token length will be different, so when you do a <code>torch.max</code>, since the max token is different, you'll get different sizes.</p>
<pre><code>
texts = [&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;]
last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state

second_sentence_embeddings_1 = last_hidden_state[1]


texts = [&quot;hello world&quot;, &quot;foo bar&quot;]
last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state

second_sentence_embeddings_2 = last_hidden_state[1]

second_sentence_embeddings_1.shape, second_sentence_embeddings_2.shape

</code></pre>
<p>[out]:</p>
<pre><code>(torch.Size([10, 768]), torch.Size([4, 768]))
</code></pre>
<h3>First, solve the differing batch size outputs by using <code>pipeline</code></h3>
<pre><code>from transformers import pipeline

pipe = pipeline(task=&quot;feature-extraction&quot;, model=&quot;google/mt5-base&quot;, framework=&quot;pt&quot;)

# See https://github.com/huggingface/transformers/issues/20404
pipe.model = pipe.model.encoder 

hello_world = pipe([&quot;hello world&quot;, &quot;foo bar&quot;], return_pt=True)

batch_mode = pipe([&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;], return_pt=True)


assert hello_world[1] == batch_mode[1]
</code></pre>
<h3>But how do I get a vector of 768 dimensions per sentence?</h3>
<p>TL;DR:</p>
<pre><code>from transformers import MT5EncoderModel, AutoTokenizer
from transformers import FeatureExtractionPipeline


class LuigiThePlumber(FeatureExtractionPipeline):
    def postprocess(self, model_outputs):
        # If you just want it to return a torch.return_types.max, 
        # instead of plain tensor use 
        # `return torch.max(model_outputs.last_hidden_state, dim=1)`
        return torch.max(model_outputs.last_hidden_state, dim=1).values

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

pipe = LuigiThePlumber(task=&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, framework=&quot;pt&quot;)

# See https://github.com/huggingface/transformers/issues/20404
pipe.model = pipe.model.encoder 


out = pipe([&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;])

print(out[0].shape, out[1].shape, out[2].shape)
</code></pre>
<p>[out]:</p>
<pre><code>torch.Size([1, 768]) torch.Size([1, 768]) torch.Size([1, 768])
</code></pre>
<h3>Then, when didn't my method work but the pipeline does?</h3>
<p>It's because when you have a batch, there's an attention mask that's being computed when sentences that are shorter than the max length of the batch is padded to the max. So when you try to extract the last hidden state, you'll have to account for the attention mask.</p>
<p>When you use <code>pipeline</code> it standardizes the batch size you put into the dataset. And usually it's set to 1.</p>
<h3>But I really want to do batch_size larger than 1!</h3>
<p>First, see:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/65083581/how-to-compute-mean-max-of-huggingface-transformers-bert-token-embeddings-with-a"">How to compute mean/max of HuggingFace Transformers BERT token embeddings with attention mask?</a></li>
<li><a href=""https://www.philschmid.de/custom-inference-huggingface-sagemaker"" rel=""nofollow noreferrer"">https://www.philschmid.de/custom-inference-huggingface-sagemaker</a></li>
</ul>
<p>Then you'll have to take in the attention mask when computing the max-pooling.</p>
<pre><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

texts = [&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;]

encoded_input = tokenizer(texts, padding=True, return_tensors='pt')
model_output = model(input_ids=encoded_input.input_ids)
attention_mask = encoded_input['attention_mask']


def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


def max_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = torch.where(attention_mask==0, -1e-9, 0.).unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.max(token_embeddings-input_mask_expanded, 1).values


mean_pooling(model_output, attention_mask).shape, max_pooling(model_output, attention_mask).shape
</code></pre>
<p>[out]:</p>
<pre><code>(torch.Size([3, 768]), torch.Size([3, 768]))
</code></pre>
<p><em><strong>Q: But are the value of the pooled embeddings with different batch the same?</strong></em></p>
<p>A:</p>
<pre><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

texts = [&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;]

encoded_input = tokenizer(texts, padding=True, return_tensors='pt')
model_output = model(input_ids=encoded_input.input_ids)
attention_mask = encoded_input['attention_mask']


def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


def max_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = torch.where(attention_mask==0, -1e-9, 0.).unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.max(token_embeddings-input_mask_expanded, 1).values


x = max_pooling(model_output, attention_mask)

text = [&quot;hello world&quot;, &quot;foo bar&quot;]

encoded_input = tokenizer(texts, padding=True, return_tensors='pt')
model_output = model(input_ids=encoded_input.input_ids)
attention_mask = encoded_input['attention_mask']

y = max_pooling(model_output, attention_mask)

# Lets check all values in the &quot;embeddings&quot; of &quot;hello World&quot; in both batch sizes.
assert all(v for v in x[0] == y[0])
</code></pre>
","1","Answer"
"78151449","78150382","<p>The effectiveness of your approach significantly hinges on the quality and volume of your data. Consider the number of samples available and the complexity of the phenomenon you aim to model—specifically, the model's ability to generalize the problem at hand. I recommend experimenting with both methods to ascertain the most suitable one.</p>
<p>That said, you would have to implement a classification model for the labels, and a regression model for the concentration.</p>
","1","Answer"
"78152337","78152237","<ol>
<li>Differentiability for backpropagation is given by e.</li>
<li>Maps to 0-1 range and gives 0-100% values.</li>
<li>Exaggeration of differences =&gt; Focuses on biggest delta, ignores smallest.</li>
<li>Stability by handling smallest+greatest values evenly. (think about float rounding errors, now we use only floats closer to each other)</li>
<li>softmax does the same but for n-classes again</li>
</ol>
","2","Answer"
"78155446","78155402","<p>You train your NN on scaled inputs, as the default value of <code>scale_data</code> in the constructor of <code>TorchDataset</code> is <code>True</code>.</p>
<p>But you do not scale the inputs when you evaluate, as you simply pass a <code>Tensor</code> rather than a <code>DataLoader</code> from the <code>Dataset</code>. This is the reason for the results you're seeing.</p>
<p>Also: This is not the question you asked, but you should separate into train, validation, and test set, rather than testing on the training set.</p>
<p>To predict, use the DataLoader:</p>
<pre><code>mlp.eval()
with torch.no_grad():
    y_pred = torch.stack([mlp(batch) for batch, _ in trainloader])
</code></pre>
","3","Answer"
"78159919","78155034","<p>Found what I was looking for in the <a href=""https://scikit-learn.org/stable/metadata_routing.html#metadata-routing"" rel=""nofollow noreferrer"">Metadata-routing</a> feature which enables to pass different parameters to the scorer and the estimator.</p>
<p>The steps to use it are:</p>
<ol>
<li>Enable the metadata routing: <code>sklearn.set_config(enable_metadata_routing=True)</code></li>
<li>Turn off the routing for the estimator: <code>RandomForestRegressor().set_fit_request(sample_weight=False)</code></li>
<li>Turn on the routing for the scorers: <code>make_scorer(r2_score).set_score_request(sample_weight=True)</code></li>
<li>Pass the sample weights to the cross-validation parameters: <code>model_selection.cross_validate(..., params={&quot;sample_weight&quot;: arr_weight})</code></li>
</ol>
<p>The full code looks like:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn import model_selection, set_config
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, make_scorer
import numpy as np

n_smpl, n_feats = 100, 5
arr_source = np.random.random((n_smpl, n_feats))
arr_target = np.random.random((n_smpl, n_feats))
arr_weight = np.random.randint(0, 2, n_smpl)  # 0 for augmented, 1 for authentic

set_config(enable_metadata_routing=True)
model = RandomForestRegressor().set_fit_request(sample_weight=False)
kfold_splitter = model_selection.KFold(n_splits=5, random_state=7, shuffle=True)
my_scorers = {
    &quot;r2_weighted&quot;: make_scorer(r2_score).set_score_request(sample_weight=True),
    &quot;mse_weighted&quot;: make_scorer(mean_squared_error, greater_is_better=False).set_score_request(sample_weight=True)
}

cv_results = model_selection.cross_validate(model, arr_source, arr_target, scoring = my_scorers, cv=kfold_splitter, params={&quot;sample_weight&quot;: arr_weight})
</code></pre>
","0","Answer"
"78164369","78149372","<p>You can convert your notebook to a python script and run it from the terminal.</p>
<p>Alternatively SageMaker offers specific components such as <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""nofollow noreferrer"">Training Jobs</a>/ <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html"" rel=""nofollow noreferrer"">Processing Jobs</a> that can run the jobs on dedicated hosted instances.</p>
<p>On the Github questions, yes you can push your code to GitHub using the git terminal commands in the Jupyter terminal</p>
","1","Answer"
"78165605","78165544","<p>The reason you get the millage from this calculation is that the coefficient for millage is 1. While the other coefficients are really small.</p>
<p>Also note you have enought coefficients since you add the bias column, so number of coeficients equals the features including bias column.</p>
<p>Since you don't show how you obtain you coefficients I can't tell you if anything has gone wrong there. (Setting the right dependent variable etc.)</p>
","1","Answer"
"78166223","78165544","<p>As pointed out by @KaranShishoo in the <a href=""https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn/78166223?noredirect=1#comment137803126_78165544"">comments above</a>, I was not dropping the price column before feeding the data to the Linear Regression Model which is why it was causing this ambiguity.</p>
","0","Answer"
"78167730","78167178","<p>This sounds like <a href=""https://rsample.tidymodels.org/reference/manual_rset.html"" rel=""nofollow noreferrer""><code>rsample::manual_rset()</code></a> might do what you want?</p>
","3","Answer"
"78169728","78169479","<p>You can pass it to the <code>train.py</code> as an arg:</p>
<pre><code>stages:
 Training:
   foreach:
    -cycle: 0
    -cycle: 1
    -cycle: 2
   do:
    cmd:
     python train.py &quot;Training-${item.cycle}&quot;
 Selection:
   foreach:
    -cycle: 0
    -cycle: 1
    -cycle: 2
   do:
    cmd:
     python train.py &quot;Selection-${item.cycle}&quot;
</code></pre>
<p>Would something like this work for you? It would be a nice addition to have this in the <code>dvc.api</code> and probably as an ENV variable. It feels it should be straightforward to add. Feel free to help us do this - all contributions are welcome and we can help :)</p>
","1","Answer"
"78170131","78170066","<p>On <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.fit_transform"" rel=""nofollow noreferrer"">this website</a> it says that this method is only valid if the final estimator either implements fit_transform or fit and transform. I do not know what a final estimator is, but that might be your problem (I know this wasn't very helpful but I tried).</p>
","1","Answer"
"78170141","78170066","<p>Documentation for <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.fit_transform"" rel=""nofollow noreferrer""><code>sklearn.pipeline.Pipeline.fit_transform</code></a> states that it's &quot;[o]nly valid if the final estimator either implements <code>fit_transform</code> or <code>fit</code> and <code>transform</code>.&quot; Wording may be a bit ambiguous, but it means two possibilities: (i) final estimator implements <code>fit_transform</code>, or (ii) final estimator implements <code>fit</code> and <code>transform</code>.</p>
<p>Your final estimator is <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression"" rel=""nofollow noreferrer""><code>sklearn.linear_model.LinearRegression</code></a>, which implements <code>fit</code>, but not <code>transform</code>. This is why the error is raised.</p>
","4","Answer"
"78171315","78170802","<p>As you correctly noted, the current code performs the detection on a whole source video first, as it was set in <code>results = model.predict(source=&quot;0&quot;,show=True)</code>. What you need is to pass every frame separately to the predict() function to run the rest of the logic. You can do this in two ways. First is to use <code>stream=True</code>, which utilizes a generator and only keeps the results of the current frame in memory. Look at the <a href=""https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode"" rel=""nofollow noreferrer"">docs</a> for an example. The code will look like this (may need some correction for the exact case):</p>
<pre class=""lang-py prettyprint-override""><code>model = YOLO(r'last.pt')
results = model.predict(source=&quot;0&quot;, show=True, stream=True)
for r in results:
    # logic for playing sound if the condition was fulfilled
    if len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            print(dclass)
            if dclass==2.0:
              sound_to_play.play()
    # you may need to use this statement to activate show=True option
    #next(results)
</code></pre>
<p>Another way is to manually iterate through the frames (as you already do) and pass them to the predict() function one by one, <code>source=frame</code>:</p>
<pre class=""lang-py prettyprint-override""><code>cap = cv2.VideoCapture(0) 
while True:
    ret, frame = cap.read()
    results = model.predict(source=frame, show=True)  
    for r in results:
        if len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            print(dclass)
            if dclass==2.0:
              sound_to_play.play()
</code></pre>
<p>You may need to add additional code to perform <code>show=True</code> option correctly for each of these cases.</p>
","1","Answer"
"78171543","78171263","<p>It's derived from the partial derivative of the MSE loss function with respect to <code>w</code>. I wrote a simplified induction process on paper, hoping it's helpful. <a href=""https://i.sstatic.net/5wQdS.jpg"" rel=""nofollow noreferrer"">attached image</a></p>
","0","Answer"
"78171924","78171227","<p>Concerning the hyperparameter tuning question, it should <strong>always</strong> be done on the training set, not the whole dataset. Tuning hyperparameters on the entire dataset introduces what we call &quot;data leakage&quot;, where information from the test set (which should be unseen) influences the model training process. If you do this then you will be getting &quot;leaked/too good&quot; performance estimates.</p>
<p>Concerning the second question, a baseline pipeline would look like this:</p>
<ol>
<li>Split your dataset into training and testing sets.</li>
<li>Perform hyperparameter tuning only on the training set.</li>
<li>After determining the best hyperparameters, retrain your model using these hyperparameters on the training set.</li>
<li>Evaluate the model performance on the test set to get an unbiased estimate of its generalization ability.</li>
</ol>
<p>Your posted code should become based on the above:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.metrics import r2_score, mean_squared_error

# Split the data into training and testing sets first
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

numeric_features = X_train.select_dtypes(include=['int', 'float']).columns
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ]
)

# Perform hyperparameter tuning on the training set
en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1), alphas=np.arange(0, 1.1, 0.1), random_state=818, n_jobs=-1)
model = make_pipeline(preprocessor, en_cv)
model.fit(X_train, y_train)  # Use only training data here

# Get the best hyperparameters
best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_

# Train a new model on the training data using the best hyperparameters
elastic_net_model = make_pipeline(preprocessor, ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio))
elastic_net_model.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = elastic_net_model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f&quot;R^2: {r2}, MSE: {mse}&quot;)
</code></pre>
","0","Answer"
"78172003","78169666","<p>The problem is that even is the parameter <code>base_score</code> of the model is None, it can have a <code>base_score</code> (different of the default one) <a href=""https://github.com/dmlc/xgboost/issues/9347"" rel=""nofollow noreferrer"">[1]</a>.</p>
<blockquote>
<p>In addition, the model.base_score will continue to be None as
discussed in #8634. In summary, the Python attribute base_score is a
user parameter according to the sklearn interface and should not be
changed by the library itself. To see the configured base score, you
either have to use the</p>
</blockquote>
<p>To access the <code>base_score</code> value the following works in version 2.0.3 of XGBoost</p>
<pre><code>config = json.loads(model.get_booster().save_config())
base_score = float(config[&quot;learner&quot;][&quot;learner_model_param&quot;][&quot;base_score&quot;])
</code></pre>
<p>Adding the <code>base_score</code> to the total sum make it match the predicted value</p>
","0","Answer"
"78172086","78167178","<p><strong>Solution based on @hannahfrick's answer above:</strong></p>
<pre><code>data(ames, package = &quot;modeldata&quot;)

split_data &lt;- function(df, n) {
  set.seed(123) # for reproducibility
  df$id &lt;- seq.int(nrow(df))
  list_of_splits &lt;- list()
  
  for(i in 1:n) {
    train_index &lt;- sample(df$id, size=ceiling(nrow(df)*.8))
    train_set &lt;- df[train_index,]
    test_set &lt;- df[-train_index,]
    list_of_splits[[i]] &lt;- list(train_set = train_set, test_set = test_set)
  }
  
  return(list_of_splits)
}

splits &lt;- split_data(ames, 5)

resamples &lt;- map(splits, ~list(
  analysis = .$train_set |&gt; select(colnames(.$test_set)) |&gt; pull(id),
  assessment = .$test_set$id
))

splits &lt;- lapply(resamples, make_splits, data = ames)

final_split &lt;- manual_rset(splits, paste(&quot;Split&quot;, seq(1:5)))

lm_model &lt;- 
  linear_reg() %&gt;% 
  set_engine(&quot;lm&quot;)

lm_wflow &lt;- 
  workflow() %&gt;% 
  add_model(lm_model) %&gt;%
  add_formula(Sale_Price ~ Longitude + Latitude)

res &lt;- lm_wflow %&gt;%
  fit_resamples(resamples = final_split)

collect_metrics(res)
</code></pre>
","0","Answer"
"78172394","78170750","<p>After checking the version because of @kartoos comments. Yes, my kaggle's notebook use tensorflow version 2.13.1 meanwhile I'm trying to load the model to 2.16.1 Tensorflow version.</p>
<p>I tried to install 2.13.1 but i can't find a way, so I'm build and re-train my model using 2.16.1 version tensorflow, and it works, no more error and the model can works fine when get saved and loaded again.</p>
<p>Saved the model as keras instead of json</p>
<pre class=""lang-py prettyprint-override""><code>model.save('model.keras')
</code></pre>
<p>and load the model again</p>
<pre class=""lang-py prettyprint-override""><code>loaded_model = keras.models.load_model(&quot;./model/model.keras&quot;)
</code></pre>
<p>Thanks !</p>
","4","Answer"
"78177120","78163348","<p>I ran into the same error when trying to get <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder"" rel=""nofollow noreferrer"">this example</a> working with Keras 3. The invalid dtype error is because the Dense layer in the decoder expects a positive integer, but <code>reduce_prod</code> returns a scalar tensor. You must extract the scalar value with, e.g. <code>numpy()</code>:</p>
<pre><code>layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid')
</code></pre>
<p>After fixing that error, I ran into a problem with batch sizes (the model in the example doesn't expect a batch dimension), which I fixed with an initial <code>Input</code> layer in the encoder. Here is my Autoencoder model converted to Keras 3:</p>
<pre><code>class Autoencoder(keras.Model):

  def __init__(self, latent_dim, shape):
    super().__init__()

    self.latent_dim = latent_dim
    self.shape = shape

    self.encoder = keras.Sequential([
      keras.Input(shape),
      keras.layers.Flatten(),
      keras.layers.Dense(latent_dim, activation='relu'),
    ])

    self.decoder = keras.Sequential([
      keras.layers.Dense(keras.ops.prod(shape).numpy(), activation='sigmoid'),
      keras.layers.Reshape(shape)
    ])

  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)

    return decoded

shape = x_test.shape[1:]
latent_dim = 64
autoencoder = Autoencoder(latent_dim, shape)
</code></pre>
","0","Answer"
"78177330","78175088","<p>Probably the most problem about your implementation is about the derivative, the derivative wrt vector w should actually be a vector not a scaler the correct way to compute the gradient is to:</p>
<pre><code>grad = (a - y) * x
</code></pre>
<p>Secondly training a model for a single epoch doesn't seem a reasonable choice . try instead running it for multiple epochs by adding:</p>
<pre><code>def fit(self, X, Y, epochs=10, eta=0.005, plot=False):
       
        if plot:
            loss_vec = np.zeros(epochs)
        for epoch in range(epochs):
            for (x, y) in zip(X, Y):
                z = np.dot(x, self.w)
                a = sigmoid(z)
                grad = (a - y) * x
                self.w -= eta * grad
                epoch_loss += self.log_loss(
            if plot:
               loss_vec[epoch] = self.log_loss(X, Y)
        if plot:
            plt.plot(loss_vec)
            plt.xlabel('# of iterations')
            plt.ylabel('Loss')
</code></pre>
","0","Answer"
"78177429","78165698","<p>As the error indicates there is a dimension mismatch in your autoencoder which causes error when computing the mean-squared-error for the input and output image. You can check the output shape of your model using model.summary() which results in the following result:</p>
<p><a href=""https://i.sstatic.net/gPpmx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gPpmx.png"" alt=""enter image description here"" /></a></p>
<p>A quick fix would be to add padding layers for example you can use the following architecture for decoder:</p>
<pre><code>decoder = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512 * 125, input_shape=[512]),
    tf.keras.layers.Reshape([125, 512]),
    tf.keras.layers.Conv1DTranspose(512, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.Conv1DTranspose(256, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.Conv1DTranspose(128, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.Conv1DTranspose(64, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    tf.keras.layers.UpSampling1D(size=2),
    tf.keras.layers.ZeroPadding1D(padding=875),
    # Adjust the kernel size and padding to match the input shape
    tf.keras.layers.Conv1DTranspose(3, kernel_size=5, strides=1, padding=&quot;same&quot;, activation=&quot;linear&quot;)
])
</code></pre>
<p><strong>Remark:</strong> It is better to add these embeddings between layers instead so you don't overwhelm you model with a lot of zeros in one layer.</p>
","0","Answer"
"78177519","78158096","<p>Indeed, the idea is introduced in the paper <a href=""https://arxiv.org/abs/2106.09685"" rel=""nofollow noreferrer"">LoRA: Low-Rank Adaptation of Large Language Models</a>.</p>
<p>The key idea is that any matrix W of shape n * m and rank r can be written as multiplication of two matrices A, B such that A has shape n * r and B has shape r * m.</p>
<p>by introducing A, B as trainable matrices instead of W itself you can decrease  the number of trainable parameters with a little bit of decrease in model performance.</p>
","1","Answer"
"78179104","78143186","<p>pipe = DiffusionPipeline.from_pretrained(
model,
torch_dtype=torch.float16,
variant='fp16',use_safetensors=True,
)</p>
","0","Answer"
"78180018","78176532","<p>I was facing a similar issue, there might be some problem on what kind of data you are sending to the api endpoint of &quot;/predict&quot; through exampleImage, still looking to solve this error. You can also try to check the container logs in huggingface spaces of your deployed model.</p>
","0","Answer"
"78180084","78176290","<p>As it comes from the training results, the model doesn't perform well: the recall &lt; 0.2 means the model can recognize lower than 20% of the target objects from the dataset it was trained on. We should expect an even lower result on real data. Consider re-train the model according to these tips: <a href=""https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/?h=tips"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/?h=tips</a>. Please make sure you have basic knowledge of model performance metrics: <a href=""https://docs.ultralytics.com/guides/yolo-performance-metrics/?h=metrics#object-detection-metrics"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/guides/yolo-performance-metrics/?h=metrics#object-detection-metrics</a>.</p>
<p>Also, a checkpoint with the best performance is saved by the training process as <em>best.pt</em>. Try it instead of the last.pt you are using now.</p>
<p>For a quick check, if the low model performance is a cause of the detection lack, you can lower the confidence threshold, which has a default value of 0.25. Objects detected with confidence below this threshold are ignored. So you can try something like this: <code>results = model(frame, conf=0.01)[0]</code>, just for the experiment. If you get the detections, even incorrect, on the training data, the cause is likely to be low model performance.</p>
","0","Answer"
"78181801","78180809","<p>Currently PCA does not support any rotation.  All it does is to decompose your datasets into factors and its corresponding coefficients in those factor directions.</p>
<p>Given a dataset A of m rows by n predictors (or coordinates), PCA will do the following</p>
<p>A = X*D where X is m by k, D is k by n and k &lt; n.</p>
<p>I assume when you say you want to specify a rotation, you really wanted</p>
<p>A = Y*E where Y is m by k, E is k by n and E is given.</p>
<p>I believe you can solve the problem by doing the following: :</p>
<ol>
<li>you can use H2O PCA to find: A = X*D.</li>
<li>given E, use any conventional software (Matlab, R, Octave or Python packages), you can find D = R * E (R = D * inverse(E)).</li>
<li>Substitue into A=X * D in terms of R and E: A = X * R * E = (X*R)*E</li>
<li>Set Y = X * R and you will get your desired answer A = Y*E.</li>
</ol>
<p>Hope this helps.</p>
","2","Answer"
"78185428","78184358","<p>According to this <a href=""https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with/61034368#61034368"">SO thread</a> and this <a href=""https://discuss.pytorch.org/t/pytorch-finds-cuda-despite-nvcc-not-found/166754"" rel=""nofollow noreferrer"">forum thread</a>, you are not required to have <code>nvcc</code> installed on your local machine since PyTorch is shipped with its own CUDA library. The only requirement is that you have a CUDA driver installed on your device and it supports the CUDA version you installed via Pytorch.</p>
<p>I would assume that <code>conda install cudatoolkit</code> installs a standalone CUDA toolkit, but is independent of PyTorch. Following the <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">installation page</a>, you should instead use <code>conda install pytorch::pytorch-cuda</code>. That way you install PyTorch with CUDA support.</p>
","1","Answer"
"78185846","78185218","<p>Welcome to Python dependency-management hell.</p>
<p>I'm not sure there is any easy or short answer to your question. What you're stumbling onto is a very common issue in any Pythonista's journey.</p>
<p>The libs you're using on one computer appear to be aligned with each others, while on the other they're not (having a lib complaining about another during its import is a good sign of dependencies issue). And it's going to be impossible to make sure they're all ok in all contexts within a single environment.</p>
<p>The way we usually solve this in the Python comunity is by using a dependency manager.</p>
<p>Here is the <a href=""https://realpython.com/python-virtual-environments-a-primer/"" rel=""nofollow noreferrer"">Real Python</a> article about dependency management and <a href=""https://realpython.com/python-virtual-environments-a-primer/#why-do-you-need-virtual-environments"" rel=""nofollow noreferrer"">why you need it</a>. It's a long read, but well worth it.</p>
<p>Good luck!</p>
","1","Answer"
"78186057","78185614","<p>tqdm is both the name of the library and the name of the main class contained inside. You should simply change at the start of your file</p>
<pre><code>import tqdm
</code></pre>
<p>to:</p>
<pre><code>from tqdm import tqdm
</code></pre>
<p>This can be confusing for beginners but is often the case in Python for packages with one main class.</p>
<p>You should always try to reduce your buggy code to a minimal reproducible example before posting.</p>
","0","Answer"
"78186324","78184444","<p>It is better to scale all variables.</p>
<p>If you don't scale those 22 features, the scale of the features will be still different.</p>
<p>For example, those 22 features maybe between 0.2 to 0.7, but other features will be between 0 and 1 (the minimum will change to 0 and the maximum value will change to 1).</p>
<p>Then, when doing the math, although 0.2 is the minimum of a feature, it is not zeros which can make the learning difficult.</p>
","1","Answer"
"78189220","78187211","<p>I don't see a hidden state issue. You're using a weird setup which is probably hindering learning.</p>
<p>First we want a proper dataset/dataloader</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch.nn.functional as F

class RandDataset(Dataset):
    def __init__(self, sequence_length):
        self.sequence_length = sequence_length
    
    def __len__(self):
        return 10000 # data is generated so length is arbitrary 
    
    def __getitem__(self, idx):
        sequence = torch.rand(self.sequence_length)
        labels = torch.ones_like(sequence)
        labels[1:] = sequence[:-1] &lt; sequence[1:]
        
        sequence = sequence[None,:,None] # shape (1, sequence_length, 1)
        labels = labels[None,:] # shape (1, sequence_length)
        
        return sequence, labels

def collate_fn(batch):
    sequences = torch.cat([i[0] for i in batch])
    labels = torch.cat([i[1] for i in batch])
    return sequences, labels

dataset = RandDataset(1000)
dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
</code></pre>
<p>Now the model. Add an input projection layer, remove the unnecessary relu after the lstm module.</p>
<pre class=""lang-py prettyprint-override""><code>class LSTMModel(nn.Module):
    def __init__(self, d_in, d_proj, d_hidden, n_layers):
        super().__init__()
        
        self.input_layer = nn.Linear(d_in, d_proj)
        self.lstm = nn.LSTM(input_size=d_proj, hidden_size=d_hidden, num_layers=n_layers, batch_first=True)
        self.output_layer = nn.Linear(d_hidden, 1)
        
    def forward(self, x):
        x = self.input_layer(x)
        x = F.relu(x)
        
        x, _ = self.lstm(x)
        
        x = self.output_layer(x)
        return x

model = LSTMModel(1, 32, 64, 2)
</code></pre>
<p>Now we set up training. We're predicting a binary output, so we want to use <code>BCEWithLogitsLoss</code> rather than <code>MSELoss</code>. Using MSE for categorical prediction doesn't make sense.</p>
<pre class=""lang-py prettyprint-override""><code>device = 'cuda'

opt = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_function = nn.BCEWithLogitsLoss()

epochs = 1

model.to(device);

for epoch in range(epochs):
    for i, batch in enumerate(dataloader):
        seqs, labs = batch
        
        seqs = seqs.to(device)
        labs = labs.to(device)
        
        preds = model(seqs)
        loss = loss_function(preds.reshape(-1), labs.reshape(-1))
        
        if i%10==0:
            print(f'{loss.item():.3f}')
        
        opt.zero_grad()
        loss.backward()
        opt.step()
</code></pre>
<p>After training, test performance</p>
<pre class=""lang-py prettyprint-override""><code>model.eval()
seq, lab = dataset[0]

pred = model(seq.to(device)).cpu()

pred = (torch.sigmoid(pred)&gt;0.5).float().squeeze()
lab = lab.squeeze()

acc = (pred == lab).float().mean()
print(acc)
&gt; tensor(0.9990)
</code></pre>
","1","Answer"
"78189889","78189166","<p>Your call is dispatched to <code>train.formula</code> where your data is converted to a matrix with the expression <code>model.matrix(Terms, m, contrasts)</code>.</p>
<p>Since your data are now in matrix form and matrices are atomic the values are coerced to the same type. In this instance <code>double</code>. This also has an odd side effect of renaming <code>var1</code> to <code>var11</code>, which you can see if you inspect the <code>mod$preProcess</code> output (e.g. <code>mod$preProcess$mean</code>). Not sure why that is the case, but I do not think  it is related to your question.</p>
<p>It appears the class information is captured before this matrix conversion and ultimately output in the results via the <code>ptype</code> element, but does nothing other than get output:</p>
<pre class=""lang-r prettyprint-override""><code>sapply(mod$ptype, class)
     var1      var2 
 &quot;factor&quot; &quot;numeric&quot; 
</code></pre>
<p>However, the model matrix is what gets passed to <code>train.default</code> which then goes on to run <code>preProcess()</code>. By the time that it reaches that step, the factor information is already stripped and that variable is of class <code>double</code>. As you noted <code>preProcess()</code> does a series of checks and only evaluates on numeric data (numeric in the sense that it is of class &quot;integer&quot;, &quot;numeric&quot;, or &quot;double&quot;). So when <code>preProcess()</code> is called via <code>train()</code> your values are already <code>double</code>, which is why they get scaled and centered.</p>
<p>However, the same conversion to a matrix is not made when you call <code>preProcess()</code> directly and so the factor class is caught and removed before scaling and centering.</p>
<p>From the <code>preProcess</code> argument documentation for <code>?train</code> it specifies:</p>
<blockquote>
<p>Pre-processing code is only designed to work when x is a simple matrix or data frame.</p>
</blockquote>
<p>I <em>think</em> this is what they are getting at -- calling this argument is only for &quot;simple&quot; meaning all values are of the same class. If they are not of the same class they will ultimately be coerced.</p>
<hr />
<p>Long story short, I think you ought to either pass the preprocessed data to <code>train()</code> or create a recipe and pass that to <code>train()</code> like so:</p>
<pre class=""lang-r prettyprint-override""><code>library(recipe)
recipe(score ~ ., data = df) |&gt;
  step_center(all_numeric_predictors()) |&gt;
  step_scale(all_numeric_predictors()) |&gt;
  train(method = &quot;lm&quot;, data = df)
</code></pre>
<p>If you go the <code>recipe</code> route you should read the documentation carefully to see if factors are included in <code>all_numeric_predictors()</code>, I am not sure off the top of my head.</p>
","1","Answer"
"78193114","78167199","<p>Unscaling standardised data is trivial. To standardise data you do:
<code>X' = (X - mean(X)) / std(X)</code> so to unscale it, you just do <code>X = (X' * std(X)) + mean(X)</code>.</p>
<p>If you want to just change the tick labels so that you can interpret the results in the original scale of the data, then you just need to do something like:</p>
<pre><code># Get the tick positions on the current axis
x_ticks = ax.get_xticks()

# Un-standardise the tick values
xt_unscaled = [(xt * x.std()) + x.mean() for xt in x_ticks]

# Format the ticks to strings (here to 1 d.p.)
xt_unscaled = [f'{xt:.1f}' for xt in xt_unscaled]

# Assign the unscaled tick values to the tick labels
# This retains the original tick positions etc, but
# lets you interpret them the way you want.
ax.set_xticklabels(xt_unscaled)

plt.show()
</code></pre>
","0","Answer"
"78197247","78197199","<p>Getting the output of a model into classification of categorical output is itself just a function that is placed at the end the model's raw output. A pretty neat little &quot;trick&quot; that a lot of people who are learning or just playing around may not notice.</p>
<p>Machine Learning boils down to finding ways to represent your data such that a Machine can read it, otherwise how can it learn? This means it needs to be in numbers.</p>
<p>Using your case, you model will be told that the two input features are T and Y, already numbers, great! But the output is a categorical value, so we need to adjust what the machine thinks these are. Since we have two options, we can say <code>Red=0</code>, and <code>Yellow=1</code>.</p>
<p>There you go, now your model knows how to spit out the result, you would train it using these adjustments, and when the model spits out an answer of <code>1</code>, you then add an extra function at the end that automatically knows that <code>Yellow=1</code>, the model doesn't have to know.</p>
<p>In practice, it actually should return <em>2 numbers</em>. What is essentially a prediction for the likelihood of each of the colours that would add to 1. So the output of the model would be <code>[0.25,0.75]</code>, and then we can say the model was more confident that it was Yellow, so we take that as the prediction.</p>
<p>By doing it this way, you've boiled everything down to numbers, and so the technique is up to you. In reality, the data distributions and the type of task (here it is a binary classification task, because only 2 output options) would affect which techniques are going to be the most affective or practical.</p>
","1","Answer"
"78197547","78197496","<p>I'm assuming you want something like this:</p>
<pre><code>model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              metrics=['mae']
)
</code></pre>
<p>It's no longer &quot;lr&quot;. You want to use &quot;learning_rate&quot;.</p>
","7","Answer"
"78199206","78196675","<p>The <code>Size of uploaded file exceeds 300MB</code> error message on Vercel cloud deployment platform indicates that your application exceeds Vercel's deployment size limit. Cloud platforms like Vercel cater to various programming languages. Their core functionality revolves around deployment and scaling, not specific languages.</p>
<p>Large libraries like DeepFace and TensorFlow are known for their large size due to pre-trained models and dependencies. Including them in your deployment package significantly increases the overall application size.</p>
","0","Answer"
"78201844","78201576","<p>The big problem with your code is how you define your LSTM layer.</p>
<p><code>nn.LSTM</code> by default expects an input of shape <code>(sl, bs, features)</code>, while your input is of shape <code>(bs, sl, features)</code>. As a result, your current code is processing along the wrong dimension. You need to pass <code>batch_first=True</code> to <code>nn.LSTM</code> to use a batch first input (<a href=""https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"" rel=""nofollow noreferrer"">lstm docs</a>).</p>
<p>Additionally, your data setup is flawed. Your LSTM processes sequences one item at a time, meaning a sequence element <code>i</code> has only seen sequence elements <code>0, 1, ... i-1</code>. But you expect this sequence element to predict <code>i+5</code> in the output sequence.</p>
<p>A More concrete example, the 2nd item in your input is expected to predict the 2nd item in the output (7th item overall), without seeing any of the intermediate sequence items. You're trying to predict elements of the output sequence with only a fraction of the input sequence.</p>
<p>The best approach would be to use next step prediction instead. This works as each step has seen all steps before it, and there's no information gap to the step being predicted. It's a simple change:</p>
<pre class=""lang-py prettyprint-override""><code># old
inputs = transform(X[:, :5, :])
targets = transform(X[:, 5:, :])

# new
inputs = transform(X[:, :-1, :]) # all but the last step
targets = transform(X[:, 1:, :]) # all but the first step
</code></pre>
<p>If you really want to stick to using the first 5 steps to predict the next 5, you need a sequence to sequence model. This involves adding a decoder LSTM that uses the hidden state produced from the encoder LSTM (thus having information from all time steps). Seq2seq also requires a for loop for the decoder which is super annoying.</p>
","1","Answer"
"78202113","78176160","<p>UPDATE: The problem is related to the version of Transformers. Using version 4.31.0 should solve.</p>
","0","Answer"
"78202782","78196998","<p><strong>I reshaped the encoded word vector from (45, 1) to (1,45)</strong></p>
<p>if input size is (1,45) and batch_size = 2:</p>
<pre><code>size of weight matrix = output_features x input_features = 3x45

bias vector size = output_features = 3


         input x       weight transposed            bias
y = [ [1,2,3,...,45],  * [ [1, 2, 3],     +    [ [b1, b2, b3],
      [3,2,1,...,45]]      [2, 2, 3],            [b1, b2, b3] ]
                           [3, 2, 3],
                           [.  .  .],
                           [45,45,45] ]

         2x45       *        45x3
                  
                   2x3                   +           2x3
                          
</code></pre>
","0","Answer"
"78203717","78192634","<p>i don't know  what really happen but i had same issue and retrained my model but with different name and without name of model it self like this</p>
<pre><code>args = TrainingArguments(
    f&quot;NameOfYourModel&quot;,
    remove_unused_columns=False,
    evaluation_strategy = &quot;epoch&quot;,
    save_strategy = &quot;epoch&quot;,
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=4,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;accuracy&quot;,
    push_to_hub=True,
)
</code></pre>
<p>AND it worked!
sorry for no more detailed info about this issue^_^</p>
","0","Answer"
"78205194","78204216","<p>I will assume that you can use the Pandas-library, as its powerful rolling function is able to easily accomodate your request.</p>
<p>Consider the following example:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
my_values = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
my_window_size = 3
rolling_mean = my_values.shift(1).rolling(window=my_window_size).mean()
print(rolling_mean)
</code></pre>
<p>Which results in</p>
<pre><code>0    NaN
1    NaN
2    NaN
3    2.0
4    3.0
5    4.0
6    5.0
7    6.0
8    7.0
9    8.0
</code></pre>
<p>AS you can see, this enables you to use the mean of the indices [0,1,2] to be displayed at index 3 ((1+2+3)/3 =2).
The NAs at the beginning are there because the window function doesnt know what to do if its window doesnt completely overlap with the series.</p>
<p>We shifted the Series here by before calculating the rolling transformation, something you wanted to avoid.</p>
<p>In your special case (which is that you shift by 1), the window Function can imporoved by the <code>closed</code> argument:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

my_values = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
my_window_size = 3
rolling_mean = my_values.rolling(window=my_window_size, closed='left').mean()
print(rolling_mean)
</code></pre>
<pre><code>0    NaN
1    NaN
2    NaN
3    2.0
4    3.0
5    4.0
6    5.0
7    6.0
8    7.0
9    8.0
</code></pre>
<p>closed &quot;left&quot; means that the last point will mean that the current point should not be part of the calculations of the window.
(A window has kind of left and right changed, when we speak of the leftmost point in the window it will be the rightmost point in the subseries the window &quot;sees&quot;, this is due to the maths behind it, i would just roll with it :D)</p>
<p>you can find the closed options here: <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html#:%7E:text=DataFrame%20first%20instead.-,closed,-str%2C%20default%20None"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html#:~:text=DataFrame%20first%20instead.-,closed,-str%2C%20default%20None</a></p>
","1","Answer"
"78206753","78206448","<h3>How these labels are created</h3>
<p>If you have a look at the <code>aweSOMplot()</code> R <a href=""https://github.com/cran/aweSOM/blob/1fbd723aba89653ebce1c2dc464a9fb959b62715/R/plots.R#L970"" rel=""nofollow noreferrer"">source</a>, you'll see it prepends two <code>&lt;h4&gt;</code> html tags to the actual plot (called <code>res</code>):</p>
<pre class=""lang-r prettyprint-override""><code>res &lt;- htmlwidgets::prependContent(res, htmltools::tag(&quot;h4&quot;, list(id= paste0(res$elementId, &quot;-info&quot;))))
res &lt;- htmlwidgets::prependContent(res, htmltools::tag(&quot;h4&quot;, list(id= paste0(res$elementId, &quot;-message&quot;))))
</code></pre>
<p>The <code>aweSOMwidget.js</code> <a href=""https://github.com/cran/aweSOM/blob/1fbd723aba89653ebce1c2dc464a9fb959b62715/inst/htmlwidgets/aweSOMwidget.js#L86"" rel=""nofollow noreferrer"">source</a> then adds the relevant text:</p>
<pre class=""lang-js prettyprint-override""><code>document.getElementById(infoId).innerHTML = &quot;Hover over the plot for information.&quot;;
document.getElementById(messageId).innerHTML = &quot;-&quot;;
</code></pre>
<h3>How to hide the labels</h3>
<p>Create an R function to set an <code>htmlwidgets::htmlwidget</code> element to be invisible:</p>
<pre class=""lang-r prettyprint-override""><code>hide_element  &lt;- function(el) {
  el$attribs$style  &lt;- 'visibility: hidden;'
  el
}
</code></pre>
<p>Then apply this to the prepended objects in your plot:</p>
<pre class=""lang-r prettyprint-override""><code>plot$prepend  &lt;- lapply(plot$prepend, hide_element)
</code></pre>
<p>This now produces a plot as desired:</p>
<p><a href=""https://i.sstatic.net/Lq6iq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Lq6iq.png"" alt=""enter image description here"" /></a></p>
<p>Note: if you want to remove the <code>-</code> below the plot as well, simply apply the function to the appended elements too:</p>
<pre class=""lang-r prettyprint-override""><code>to_hide  &lt;- c(&quot;prepend&quot;, &quot;append&quot;)
plot[to_hide]  &lt;- lapply(plot[to_hide], \(l) lapply(l, hide_element))
# ^^ plot is now as above but without the &quot;-&quot; below it
</code></pre>
","5","Answer"
"78207375","78207131","<p>Following steps can be used to fixed the mentioned issue</p>
<blockquote>
<ol>
<li>Verify that correct algorithm is used to train the model</li>
<li>Data used to train the model should contains maximum possible data inputs similar to real time data</li>
<li>Retrain the model with sufficient amount of data, the more amount of data used to train the model increases the changes of better prediction</li>
</ol>
</blockquote>
","0","Answer"
"78208093","78207935","<p>Based on your folder structure and the code you have provided, the issue is that you haven't put the trailing slash at the end of your folder paths.
In the provided code, you're trying to concatenate the folder name directly with the path. However, if you miss a slash or if the folder variable does not include a trailing slash, this could result in an incorrect path.</p>
<p>Update the paths like this:</p>
<pre><code>trainDataset = 'melanoma_cancer_dataset/train/'
testDataset = 'melanoma_cancer_dataset/test/'
predictionDataset = 'melanoma_cancer_dataset/skinTest/'
</code></pre>
<p>What your code is doing is here:</p>
<pre><code>for folder in os.listdir(trainDataset):
    data = gb.glob(pathname=str(trainDataset + folder + '/*.jpg'))
</code></pre>
<p>is that it is going to the path of the trainDataset, and then listing the folders there (which are named malignant and benign) with the use of <code>os.listdir()</code>.
These paths are concatenated to generate the final image paths with:</p>
<pre><code>data = gb.glob(pathname=str(trainDataset + folder + '/*.jpg'))
</code></pre>
<p>Also, slight syntax error in the line:</p>
<pre><code>imageList = cv2.resize(image(120,120))
</code></pre>
<p>It should be</p>
<pre><code>cv2.resize(image, (120, 120))
</code></pre>
<p>Also the way you are appending to training_List might be wrong. You need to convert the imageList to a list before appending it or append imageList directly if you want to keep the image array structure.</p>
<p>Full updated code:</p>
<pre><code># Data processing modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cv2
# File directory modules
import glob as gb
import os
# Training and testing (machine learning) modules
import tensorflow as tf
import keras

# Directories
trainDataset = 'melanoma_cancer_dataset/train/'
testDataset = 'melanoma_cancer_dataset/test/'
predictionDataset = 'melanoma_cancer_dataset/skinTest/'

# Empty list for the images
training_List = []
testing_list = []

# Classification dictionary
diction = {'benign': 0, 'malignant': 1}

# Read through the folder's contents
for folder in os.listdir(trainDataset):
    # Corrected the path pattern and added a slash
    data = gb.glob(pathname=str(trainDataset + folder + '/*.jpg'))
    print(f'{len(data)} in folder {folder}')
    # Read the images, resize them, and store them in the list
    for file_path in data:
        image = cv2.imread(file_path)
        # Corrected the resize function call
        imageList = cv2.resize(image, (120, 120))
        # Append the image array directly
        training_List.append(imageList)

print(f'Total images in training set: {len(training_List)}')
</code></pre>
","2","Answer"
"78209423","78209231","<p>According to the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">scikit-learn documentation for GridSearchCV()</a>, the data you feed into the function is automatically split into folds and cross-validation is performed. You should hence just provide the full dataset (minus the final training data) and not worry about splitting the data yourself.</p>
<p>To do this, you may wish to combine the training and validation dataset:</p>
<pre><code>import numpy as np

# Merge the training and validation datasets, for use in the GridSearchCV() function.
X_opt = np.vstack((X_train, X_val))
y_opt = np.hstack((y_train, y_val))

param_grid = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

clf = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')  # This uses 5-fold cross-validation.
grid_search.fit(X_opt, y_opt)  # Fit to the merged datasets.
best_params = grid_search.best_params_
print(&quot;Best Parameters:&quot;, best_params)
print(&quot;\n&quot;)
</code></pre>
<p>Your script optimises your model, by training it on ~80% of the fitted data, with the remaining ~20% assigned as validation data. This is then changed around, over different folds. By using the code as modified above, you can ensure that you are making full use of the training and validation data you have, while still avoiding optimisation against the testing data.</p>
<p>You are correct in your understanding that optimisation is ideally performed against validation data - training however always must remain on the training dataset. The GridSearchCV() function essentially does this, but using <em>k</em>-fold cross-validation.</p>
<p>You would then want to analyse the grid search, using the actual folds it processed:</p>
<pre><code># Analyse grid search.
best_clf = grid_search.best_estimator_
results = grid_search.cv_results_  # Access fold-specific results.
num_folds = grid_search.cv  # Automatically find number of folds.

# Initialise a list to hold best fold-specific scores.
best_scores_per_fold = [float(&quot;-inf&quot;)] * num_folds

# Iterate over each fold.
for i in range(num_folds):
    fold_key = f&quot;split{i}_test_score&quot;
    # Find the best score for this fold.
    best_score_for_fold = np.max(results[fold_key])
    best_scores_per_fold[i] = best_score_for_fold

# Print the best scores per fold.
for i, score in enumerate(best_scores_per_fold, 1):
    print(f&quot;Best score for fold {i}: {score}&quot;)

# Test your model, on the testing data.
y_test_pred = best_clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_recall = recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
print(&quot;Decision Tree Measurements on Test Set with Best Model:&quot;)
print(&quot;Accuracy:&quot;, test_accuracy)
print(&quot;Precision:&quot;, test_precision)
print(&quot;Recall:&quot;, test_recall)
print(&quot;F1 Score:&quot;, test_f1)
print(&quot;-------------------------------------------------------&quot;)
</code></pre>
","1","Answer"
"78209852","78208603","<p>It's not entirely clear why they do this. It was implemented that way for some reason a long time ago and now it's backward incompatible to change it. You can find discussion of this <a href=""https://github.com/pytorch/pytorch/issues/2159"" rel=""nofollow noreferrer"">here</a>. The transpose operation doesn't add any overhead so there's no performance incentive to change it.</p>
","0","Answer"
"78211034","78199806","<p>Here is a vectorized version of the same:</p>
<pre><code>a = np.exp(y * (X @ w))

## Method 1
b = np.sqrt(a)*X.T/(1 + a)
b @ b.T 

## method 2
X.T @ np.diag(a/(1 + a)**2) @ X 

## method 3
np.einsum('ij,i,ik',X, a/(1 + a)**2, X)
</code></pre>
<h1>Edit</h1>
<p>The formula given in the link you provided is incorrect. Note that the computed second derivative of the log likelihood with respect to the weights does not involve the labels. So you do not need the labels. <a href=""http://gauss.stat.su.se/phd/oasi/OASII2021_gradients_Hessians.pdf"" rel=""nofollow noreferrer"">here is a link of the correct hessian for logistic regression</a>.</p>
<p>Now with this correct formula, we can compute the hessian as:</p>
<pre><code> a = np.exp(X @ w)
 - X.T @ np.diag(a/(1 + a)**2) @ X
</code></pre>
<p>Real Example:</p>
<pre><code>import pandas as pd, numpy as np
dat = pd.read_csv(&quot;https://raw.githubusercontent.com/oonyambu/ECON430/main/admit.csv&quot;, index_col = 0)

X = np.c_[np.ones(dat.shape[0]), dat.iloc[:,1:3].to_numpy()]
y = dat.admit.to_numpy()

from statsmodels.formula.api import logit
mod = logit(&quot;admit~gre+gpa&quot;, dat).fit()

# Hessian = -inverse(covariance)
np.linalg.inv(-mod.normalized_cov_params)
array([[-8.25030780e+01, -4.98883263e+04, -2.84169634e+02],
       [-4.98883263e+04, -3.11768300e+07, -1.72965203e+05],
       [-2.84169634e+02, -1.72965203e+05, -9.89840314e+02]])

mod.model.hessian(mod.params)
array([[-8.25030780e+01, -4.98883263e+04, -2.84169634e+02],
       [-4.98883263e+04, -3.11768300e+07, -1.72965203e+05],
       [-2.84169634e+02, -1.72965203e+05, -9.89840314e+02]]) 

## Direct Computation. (No y)
a = np.exp(X @ mod.params)
b = np.sqrt(a)*X.T/(1 + a)
-b @ b.T
array([[-8.25030780e+01, -4.98883263e+04, -2.84169634e+02],
       [-4.98883263e+04, -3.11768300e+07, -1.72965203e+05],
       [-2.84169634e+02, -1.72965203e+05, -9.89840314e+02]])
</code></pre>
<p>or even</p>
<pre><code>k = 1/(1 + np.exp(-X @ mod.params))
-k*(1-k)*X.T @ X
array([[-8.25030780e+01, -4.98883263e+04, -2.84169634e+02],
       [-4.98883263e+04, -3.11768300e+07, -1.72965203e+05],
       [-2.84169634e+02, -1.72965203e+05, -9.89840314e+02]])
</code></pre>
","0","Answer"
"78212095","78208624","<p>I solved the issue. My problem was that the gym environment  I was using was rewarding constant behavior,and since the action doesn't have any standard deviation if the action is a constant, that was what was giving that error. Code's working now!</p>
","0","Answer"
"78212126","78203794","<p>Found the fix myself. Turns out you can just go into the ??create_vision_model and then the ??add_head and put them as the model class inside of the &quot;initialize&quot; function inside of the handler.py ; you should end up with something like this:</p>
<pre><code>state_dict = torch.load(model_pt_path, map_location=self.device)
head = None
concat_pool = True
pool = True
lin_ftrs = None
ps = 0.5
first_bn = True
bn_final = False
lin_first = False
y_range = None
init = nn.init.kaiming_normal_
arch = resnet50
n_out = 9
pretrained = True
cut = None
n_in = 3
custom_head = None
# self.model = MyVisionModel()
meta = model_meta.get(arch, _default_meta)
model = arch(pretrained=pretrained)
body = create_body(model, n_in, pretrained, ifnone(cut, meta['cut']))
nf = num_features_model(nn.Sequential(*body.children())) if custom_head is None else None
if head is None:
    head = create_head(nf, n_out, concat_pool=concat_pool, pool=pool,
                       lin_ftrs=lin_ftrs, ps=ps, first_bn=first_bn, bn_final=bn_final, lin_first=lin_first,
                       y_range=y_range)
self.model = nn.Sequential(body, head)
self.model.load_state_dict(state_dict)
self.model.to(self.device)
self.model.eval()

logger.debug(&quot;Model file {0} loaded successfully&quot;.format(model_pt_path))
self.initialized = True
</code></pre>
","0","Answer"
"78212186","78212101","<p>The problem you are facing is probably overfitting. overfitting occurs when an algorithm fits too closely or even exactly to its training data, resulting in a model that can’t make accurate predictions or conclusions from any unseen data, you can learn more about it <a href=""https://www.ibm.com/topics/overfitting"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The reason why other models are working better may be due to the fact that deep learning models can be prone to overfitting, especially when the model is too complex or the amount of training data is insufficient. Random Forest, with its ensemble approach, inherently has a regularization effect which can help prevent overfitting.</p>
<p>Probable solutions to your problem could be things like:</p>
<ol>
<li>collecting more data: this could help with generalization</li>
<li>simplifying your model: you can try by less number of layers or less complex architecture</li>
<li>regularization: there are plenty of methods like weight or feature regularization</li>
</ol>
<p>there are other ways to avoid overfitting you can search and see which one is best for your project</p>
","1","Answer"
"78212238","78210297","<p>You don't have to worry about file endings in this case.</p>
<p><code>torch.save</code> basically writes the input to a zip file (it also works for arbitrary python objects, not just weights). The <code>.pt</code> file ending is just convention.</p>
<p>Huggingface saves model weights to a directory, usually of the form <code>model_name.hf/</code>.</p>
<p>The error you are seeing is because <code>AutoModelForCTC.from_pretrained(&quot;model.pt&quot;)</code> results in huggingface code looking for a directory named <code>model.pt</code>, when no such directory exists.</p>
<p>When you download the model the first time, the weights are saved to cache, usually <code>~/.cache/huggingface/hub</code>.</p>
<p>If you want to explicitly save the model weights to a different directory, you can run:</p>
<pre><code>model = AutoModelForCTC.from_pretrained(model_name, config=config)
model.save_model(&quot;path_to_save&quot;) 
</code></pre>
","1","Answer"
"78212527","78210393","<p><code>jax.linear_util</code> was <a href=""https://jax.readthedocs.io/en/latest/changelog.html#jax-0-4-16-sept-18-2023"" rel=""nofollow noreferrer"">deprecated in JAX v0.4.16</a> and <a href=""https://jax.readthedocs.io/en/latest/changelog.html#jax-0-4-24-feb-6-2024"" rel=""nofollow noreferrer"">removed in JAX v0.4.24</a>.</p>
<p>It appears that <code>flax</code> is the source of the <code>linear_util</code> import, meaning that you are using an older flax version with a newer jax version.</p>
<p>To fix your issue, you'll either need to install an older version of JAX which still has <code>jax.linear_util</code>, or update to a newer version of <code>flax</code> which is compatible with more recent JAX versions.</p>
","1","Answer"
"78213320","78119978","<p>Per <code>scikit-learn</code>'s docs for <code>OneVsRestClassifier</code> (<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier.fit"" rel=""nofollow noreferrer"">link</a>), as of v1.4.0 additional <code>**fit_params</code> are only passed through to estimators' <code>fit()</code> methods if you've enabled what <code>scikit-learn</code> calls &quot;metadata routing&quot;.</p>
<p>There are 2 required steps which are missing in your example:</p>
<ul>
<li>opting in by running <code>sklearn.set_config(enable_metadata_routing=True)</code></li>
<li>explicitly telling <code>scikit-learn</code> to pass through <code>eval_set</code> and <code>callbacks</code>, via <code>.set_fit_request()</code>.</li>
</ul>
<p>(<a href=""https://scikit-learn.org/stable/metadata_routing.html#metadata-routing"" rel=""nofollow noreferrer"">docs link</a>)</p>
<p>Consider this minimal, reproducible example using Python 3.11, <code>lightgbm==4.3.0</code>, and <code>scikit-learn==1.4.1</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import lightgbm as lgb
import sklearn
from sklearn.datasets import make_blobs
from sklearn.multiclass import OneVsRestClassifier

# enable metadata_routing
sklearn.set_config(enable_metadata_routing=True)

# create datasets
X, y = sklearn.datasets.make_blobs(
    n_samples=10_000,
    n_features=10,
    centers=2
)
eval_dataset = lgb.Dataset(X, label=y)
eval_results = {}

# construct estimator
params = {
    &quot;objective&quot;: &quot;binary&quot;,
    &quot;n_estimators&quot;: 10,
}
fit_params = {
    &quot;eval_set&quot;: (X, y),
    &quot;callbacks&quot;: [lgb.record_evaluation(eval_results)]
}

clf = OneVsRestClassifier(
    lgb.LGBMClassifier(**params)
    .set_fit_request(callbacks=True, eval_set=True)
)

# train
clf.fit(X, y,  **fit_params)

# check eval results, to prove that the callback was used
print(eval_results)

# {'valid_0': OrderedDict([('binary_logloss', [0.598138869381609, 0.5203293282602738, 0.45544446427154844, 0.40059849184355334, 0.3537472248673818, 0.31338812592304066, 0.2783839141567028, 0.24785302530927006, 0.22109850424011224, 0.19756016345789282])])}
</code></pre>
","1","Answer"
"78215804","78215783","<p>It appears that there are extra spaces in the column names 'Building Age', 'Floor', and 'Number of Floors' in your DataFrame. This discrepancy in column names with extra spaces is causing the KeyError.</p>
","0","Answer"
"78216207","78216115","<p>You can flatten your array like this:</p>
<pre><code>from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

a = np.array([0, 2, 5, 2])
b = np.array([0, 5, 0, 2])
c = np.array([2, 2, 0, 0])
d = np.array([5, 2, 5, 0])

age_a = np.array([5, 10, 7, 6])
age_b = np.array([3, 7, 11, 8])
age_c = np.array([15, 10, 17, 2])
age_d = np.array([2, 8, 12, 7])

color_a = np.array([0, 2, 1, 1])
color_b = np.array([1, 12, 0, 1])
color_c = np.array([0, 1, 1, 0])
color_d = np.array([1, 0, 0, 1])

data2 = {'name': [a, b, c, d], 'age': [age_a, age_b, age_c, age_d], 'color': [0, 1, 0, 1]}
new2 = pd.DataFrame.from_dict(data2)

print(new2)

x = new2.loc[:, new2.columns != 'color']
y = new2.loc[:, 'color']

x_flattened = np.array([np.concatenate((row['name'], row['age'])) for _, row in x.iterrows()])
y = np.array(y)

x_train, x_test, y_train, y_test = train_test_split(x_flattened, y, test_size=0.25, random_state=42)

from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(x_train, y_train)
sgd_clf.predict(x_test)
</code></pre>
<p>To make an output like this one, that seems correct:</p>
<pre><code>           name              age  color
0  [0, 2, 5, 2]    [5, 10, 7, 6]      0
1  [0, 5, 0, 2]    [3, 7, 11, 8]      1
2  [2, 2, 0, 0]  [15, 10, 17, 2]      0
3  [5, 2, 5, 0]    [2, 8, 12, 7]      1
</code></pre>
","0","Answer"
"78216756","78214495","<p>Every time you run this python script, you are supplying a persistent directory which will store the embeddings on disk at the specified directory. You are passing in the same chunked documents. And you are defining the embeddings model. So really with every script run, the vector database will perform that same action.</p>
<p>When you want to load the persisted database from disk, you instantiate the Chroma object, specifying the persisted directory and the embedding model as so:</p>
<pre><code># load from disk
db3 = Chroma(persist_directory=&quot;./vector/my_data&quot;, embedding_function= ollamaEmbeddings)
docs = db3.similarity_search(query)
print(docs[0].page_content)
</code></pre>
","1","Answer"
"78218478","78215347","<p>I think the link is incorrect, try to download the dataset with that :</p>
<pre><code>https://github.com/Tony-Y/pytorch_warmup/blob/master/examples/emnist/download.py
</code></pre>
<p>And then change your code with :</p>
<pre><code>import torchvision
from torchvision import transforms

# Update the path to where you've manually placed the EMNIST dataset
root_dir = &quot;./path/to/your/emnist&quot;  # Change this to the actual path

trainset = torchvision.datasets.EMNIST(root=root_dir,
                                   split=&quot;letters&quot;,
                                   train=True,
                                   download=False,  # Set to False since you already downloaded it 
                                   transform=transforms.ToTensor())
</code></pre>
","2","Answer"
"78218556","78196623","<p>From tensorflow 2.16, keras 3 is the default keras version. Legacy keras functions are not working anymore. A workaround is to set TF_USE_LEGACY_KERAS environment variable to 1 before importing tensorflow and install tf_keras package to be able to use the legacy keras.</p>
<p>That <a href=""https://github.com/serengil/deepface/blob/329606ffe83d5dd2b0ed97bcb1950cd010c93ea2/deepface/DeepFace.py#L8"" rel=""nofollow noreferrer"">environment variable</a> is set and tf_keras installation is <a href=""https://github.com/serengil/deepface/blob/329606ffe83d5dd2b0ed97bcb1950cd010c93ea2/deepface/commons/package_utils.py#L55"" rel=""nofollow noreferrer"">enforced</a> for users who are using tf 2.16 or later in the deepface's latest 0.0.89 version.</p>
<p>TLDR: if you upgrade your deepface package to latest, then you should not have this trouble anymore.</p>
","3","Answer"
"78218943","78218276","<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>pd.read_csv('dataset.csv')</code></a> returns a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame</code></a>, not a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.html"" rel=""nofollow noreferrer""><code>pandas.Series</code></a>.</p>
<p>You can read the data as a <code>pandas.DataFrame</code>, then select the columns you want in each axis. The selected column are <code>pandas.Series</code>. Column <code>Year</code> is converted to <code>str</code> so it doesn't show as <code>float</code>.</p>
<pre><code>df = pd.read_csv(&quot;dataset.csv&quot;)
y = difference(df[&quot;Obs&quot;])
x = df[&quot;Year&quot;].astype(str)[1:]
pyplot.plot(x, y)
pyplot.show()
</code></pre>
<p>This will plot:
<a href=""https://i.sstatic.net/2iQgl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2iQgl.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"78219159","78218890","<p>Not the exact same scenario, but I've solved a similar issue in the past by explicitly setting <code>interpolation=bilinear</code> in my <code>Upsampling2D</code> layers. Check any resize layers you have.
This would entail retraining the model though.</p>
","0","Answer"
"78219403","78219076","<p>Try these parameters to bypass the embedding layer :</p>
<pre><code>class GPT2WithoutWTE(GPT2Model):
def __init__(self, config):
    super().__init__(config)
    # Remove the word token embedding layer
    del self.wte

def forward(
    self,
    inputs_embeds,
    attention_mask=None,
    token_type_ids=None,
    position_ids=None,
    head_mask=None,
    inputs=None,
    encoder_hidden_states=None,
    encoder_attention_mask=None,
    past_key_values=None,
    use_cache=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
):
    # here you will bypass the embedding layer and use inputs_embeds directly
    return super().forward(
        inputs_embeds=inputs_embeds,
        attention_mask=attention_mask,
        token_type_ids=token_type_ids,
        position_ids=position_ids,
        head_mask=head_mask,
        encoder_hidden_states=encoder_hidden_states,
        encoder_attention_mask=encoder_attention_mask,
        past_key_values=past_key_values,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )
</code></pre>
<p>for the input embeddings you can use the following</p>
<pre><code>inputs_embeds = torch.rand(1, 10, config.n_embd)
</code></pre>
<p>Load the config of GPT2, send it to the class and then use the inputs_embeds for the new model.</p>
","1","Answer"
"78219443","78218276","<p>To difference, just use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.diff.html"" rel=""nofollow noreferrer""><code>Series.diff</code></a> or <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html"" rel=""nofollow noreferrer""><code>DataFrame.diff()</code></a>.</p>
<p>Also, <code>Year</code> should be the index:</p>
<pre><code>from matplotlib.ticker import MaxNLocator
import numpy as np
import pandas as pd

df = pd.DataFrame(
    {'Year': np.arange(1994, 2000),
     'Obs': [21, 62, 56, 29, 38, 201]})

stationary = df.set_index('Year').diff()
ax = stationary.plot(legend=False)
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/MxL6k.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MxL6k.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"78225618","78224205","<p>The error <code>The type or namespace name ‘Tensor’ could not be found (are you missing a using directive or an assembly reference?)</code> means that you are missing a nuget package or assembly reference.</p>
<p>Unfortunately I cannot determine from your question alone which nuget package is missing, but you might try adding a nuget package reference to:</p>
<p><a href=""https://www.nuget.org/packages/TensorFlowLite.iOS"" rel=""nofollow noreferrer"">https://www.nuget.org/packages/TensorFlowLite.iOS</a></p>
<p>However, I can only find iOS versions of this library on nuget.org and you may want to avoid this package unless you know for certain you will only need to support iOS (seems unusual.) This package also does not list any dependencies which I find suspect, but, I'm not familiar enough with it to know if that is true or not. If it has dependencies and they are not listed you will have the struggle of figuring out which dependencies need to be added yourself.</p>
<p>Not sure if <code>Tensorflow.NET</code> nuget package would be usable from Unity, or compatible with the model you are trying to use, I lack experience with these packages to know.</p>
<p>The error <code>‘Interpreter.GetOutputTensor(int)’ is inaccessible due to its protection level</code> may be a symptom of incorrect or missing packages being used. When I look at the current reference docs for tensorflow lite this particular method is decorated with a <code>public</code> accessor, and should not be giving this error.</p>
","0","Answer"
"78226619","78226139","<p>Not sure if it's what you are looking for :</p>
<pre><code>import numpy as np
from collections import Counter

def consistency_score(predictions, confidences):
    &quot;&quot;&quot;
    Calculate a consistency score for a sequence of predictions.
    

    &quot;&quot;&quot;
    # Calculate base consistency as the frequency of the most common class
    most_common_class, most_common_freq = Counter(predictions).most_common(1)[0]
    base_consistency = most_common_freq / len(predictions)
    
    # Adjust consistency based on confidences
    # Penalize deviations from the most common class, especially with high confidence
    penalty = sum(conf for pred, conf in zip(predictions, confidences) if pred != most_common_class) / len(predictions)
    adjusted_consistency = max(0, base_consistency - penalty)
    
    return adjusted_consistency
</code></pre>
<ul>
<li><p>Example :</p>
<pre><code>  predictions = [&quot;class_a&quot;, &quot;class_b&quot;, &quot;class_a&quot;, &quot;class_c&quot;]
  confidences = [0.9, 0.9, 0.9, 0.9]
  score_high_confidence = consistency_score(predictions, confidences)

  predictions_low_confidence = [&quot;class_a&quot;, &quot;class_b&quot;, &quot;class_a&quot;, &quot;class_c&quot;]
  confidences_low_confidence = [0.9, 0.2, 0.8, 0.3]
  score_low_confidence = consistency_score(predictions_low_confidence, confidences_low_confidence)

  print(f&quot;High confidence inconsistencies score: {score_high_confidence}&quot;)
  print(f&quot;Lower confidence inconsistencies score: {score_low_confidence}&quot;)
</code></pre>
</li>
</ul>
","1","Answer"
"78228818","78228806","<p>Try
<code>python3 -m virtualenv --python=/usr/bin/python2.7 python27</code></p>
<p>As a rule of thumb, if you installed it through <code>python3 -m pip</code>, you run it using <code>python3 -m ...</code></p>
","0","Answer"
"78229293","78229141","<p>I think it would be best to transform your dataset from long to wide. So a single row has the following columns: <code>&quot;Time&quot;</code>, <code>&quot;Red&quot;</code>, <code>&quot;Yellow&quot;</code>, <code>&quot;Green&quot;</code>, <code>&quot;Blue&quot;</code>, <code>&quot;Location&quot;</code>. Remember, an input into the model (or a row in your dataset) should contain all the features you want, so just imagine them as your columns.</p>
<p>I achieved this from your data in the following way</p>
<pre class=""lang-py prettyprint-override""><code># long to wide pivot
data = pd.pivot(df, index=[&quot;Time&quot;, &quot;Location&quot;], columns = &quot;Receiver&quot;, values=&quot;RSSI&quot;)

# drop &quot;Time&quot; into a column form the index
data.reset_index(drop=False, inplace=True)

# replace NaN with 0
data.fillna(0, inplace = True)

# order columns to have location last
data = data[[&quot;Time&quot;, *df.Receiver.unique(), &quot;Location&quot;]]
</code></pre>
<p>Where I had defined a pandas dataframe from your table you shared above as <code>df</code>.</p>
<p>This gives us</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Time</th>
<th>Location</th>
<th>Blue</th>
<th>Green</th>
<th>Red</th>
<th>Yellow</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024-03-21 20:37:58</td>
<td>Region A</td>
<td>254.0</td>
<td>208.0</td>
<td>182.0</td>
<td>0.0</td>
</tr>
<tr>
<td>2024-03-21 20:37:59</td>
<td>Region A</td>
<td>254.0</td>
<td>215.0</td>
<td>192.0</td>
<td>0.0</td>
</tr>
<tr>
<td>2024-03-21 20:38:00</td>
<td>Region A</td>
<td>254.0</td>
<td>207.0</td>
<td>202.0</td>
<td>17.0</td>
</tr>
<tr>
<td>2024-03-21 20:38:01</td>
<td>Region A</td>
<td>254.0</td>
<td>225.0</td>
<td>189.0</td>
<td>16.0</td>
</tr>
<tr>
<td>2024-03-21 20:38:02</td>
<td>Region A</td>
<td>255.0</td>
<td>213.0</td>
<td>204.0</td>
<td>18.0</td>
</tr>
</tbody>
</table></div>
<p>And to add just a few tweaks to your training code, typical practice is to have the final column of your dataset as the label or target column, so you can simply slice the dataframe as</p>
<pre class=""lang-py prettyprint-override""><code>x = data.iloc[:,:-1]
y = data.iloc[:,-1]
</code></pre>
<p>Also, <code>LabelEncoder</code> should only be used for the target column, you used it on <code>&quot;name&quot;</code>, which is not correct. You would typically use something like <code>OrdinalEncoder</code> for that instead. We don't need that here now anymore because the names are now just columns in their own right.  You also then re-defined your label encoder to decode the output, but you already used a label encoder to encode the targets so just reuse it.</p>
<p>You'll also need to convert your time column into a number, any method is fine really, so try just converting to int. Be wary that this number is very big, and ML models typically do better with values between 0 and 1, so maybe consider using a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">Scaler</a>.</p>
<p>Here is what I would do as a small update to your training</p>
<pre class=""lang-py prettyprint-override""><code>le = LabelEncoder()
y = le.fit_transform(y)

x['Time'] = pd.to_datetime(x['Time'])
x[&quot;Time&quot;] = x[&quot;Time&quot;].apply(lambda x: x.toordinal())
x = np.array(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Create the Random Forest model
model = RandomForestClassifier(n_estimators=100)

# Train the model
model.fit(x_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(x_test)

predicted_locations = le.inverse_transform(y_pred)
print(&quot;Predicted locations:&quot;, predicted_locations)
</code></pre>
","1","Answer"
"78232333","78219696","<p>I eventually got my answer to this question from a user named &quot;Tassle&quot; from computer science stack exchange; here is a link to his answer:</p>
<p><a href=""https://cs.stackexchange.com/questions/167260/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train?noredirect=1#comment346349_167260"">https://cs.stackexchange.com/questions/167260/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train?noredirect=1#comment346349_167260</a></p>
<p>In short, Tassle says that the computer will get &quot;stuck&quot; choosing one set of opening moves over and over even if it is not optimal — if it performs well against random moves, which it likely will. Tassle explains it much better than I could.</p>
<p>After getting my answer I tried training the AI against another copy of itself, and this produced slightly better results. Then I tried swapping between AI and random for a lot of games to see if perhaps the AI would find ways to exploit weaknesses developed by random training, and then random training could &quot;shake things up&quot; again. The difference was immediately apparent (it now only looses once in a blue moon).</p>
<p>Increasing the number of beads taken from the matchbox on a loss (and making sure the total wouldn't fall below zero) also helped. I am now happy with how it is working.</p>
","1","Answer"
"78233058","78196301","<p>I was able to do that creating a custom brute force class modifying the <code>Bruteforce</code> original class:</p>
<pre><code>@tf.keras.saving.register_keras_serializable(package=&quot;MyLayers&quot;)
class BruteForce2(tf.keras.layers.Layer):
  &quot;&quot;&quot;Brute force retrieval.&quot;&quot;&quot;

  def __init__(self, model, k=5, **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.k = k
        self._k = k
        
  def _compute_score(self, queries: tf.Tensor,
                     candidates: tf.Tensor) -&gt; tf.Tensor:
    
        return tf.matmul(queries, candidates, transpose_b=True)

  
  # @tf.function
  def call(self, queries: tf.Tensor, candidates_raw: tf.data.Dataset, k):
    
    candidates = tf.data.Dataset.zip(candidates_raw.map(lambda x: 
x['id']), 
                                           
candidates_raw.map(final_model.candidate_model)
                                          )
    
    spec = candidates.element_spec

    if isinstance(spec, tuple):
      identifiers_and_candidates = list(candidates)
      candidates = tf.concat(
          [embeddings for _, embeddings in identifiers_and_candidates],
          axis=0
      )
      identifiers = tf.concat(
          [identifiers for identifiers, _ in identifiers_and_candidates],
          axis=0
      )
    else:
      candidates = tf.concat(list(candidates), axis=0)
      identifiers = None
    
    self._candidates = candidates

    if identifiers is None:
      identifiers = tf.range(candidates.shape[0])
    if tf.rank(candidates) != 2:
      raise ValueError(
          f&quot;The candidates tensor must be 2D (got {candidates.shape}).&quot;)
    if candidates.shape[0] != identifiers.shape[0]:
      raise ValueError(
          &quot;The candidates and identifiers tensors must have the same 
number of rows &quot;
          f&quot;(got {candidates.shape[0]} candidates rows and 
{identifiers.shape[0]} &quot;
          &quot;identifier rows). &quot;
      )
    # We need any value that has the correct dtype.
    identifiers_initial_value = tf.zeros((), dtype=identifiers.dtype)
    self._identifiers = self.add_weight(
        name=&quot;identifiers&quot;,
        dtype=identifiers.dtype,
        shape=identifiers.shape,
        initializer=tf.keras.initializers.Constant(
            value=identifiers_initial_value),
        trainable=False)
    self._candidates = self.add_weight(
        name=&quot;candidates&quot;,
        dtype=candidates.dtype,
        shape=candidates.shape,
        initializer=tf.keras.initializers.Zeros(),
        trainable=False)
    self._identifiers.assign(identifiers)
    self._candidates.assign(candidates)
    # self._reset_tf_function_cache()
    

    k = k if k is not None else self._k

    if self._candidates is None:
      raise ValueError(&quot;The `index` method must be called first to &quot;
                       &quot;create the retrieval index.&quot;)

    if self.model.query_model is not None:
      queries = self.model.query_model(queries)

    scores = self._compute_score(queries, self._candidates)

    values, indices = tf.math.top_k(scores, k=k)

    return values, tf.gather(self._identifiers, indices)
</code></pre>
","0","Answer"
"78234373","78234289","<p><code>chunksize</code> <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#iterator"" rel=""nofollow noreferrer"">implies <code>iterable</code></a> and is what changes the return type of <code>read_csv</code> to be a <code>TextFileReader</code> object that you're iterating over.</p>
<blockquote>
<p>chunksize : int, optional<br />
Number of lines to read from the file per chunk. Passing a value will cause the function to return a <code>TextFileReader</code> object for iteration. See the <a href=""https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking"" rel=""nofollow noreferrer"">IO Tools docs</a> for more information on <code>iterator</code> and <code>chunksize</code>.</p>
</blockquote>
<p>When <code>chunksize</code> is not specified, it returns a <code>DataFrame</code> instead.</p>
<p>So, if you remove <code>chunksize</code>, <code>iterable</code> is no longer implied, and the object that you end up iterating over in your <code>for</code> loop will be no longer be a <code>TextFileReader</code> object, and the object types in your <code>df_list</code> will also be different as a consequence, ultimately causing the error when you call <code>pd.concat</code> on that list.</p>
<p>Chunking the file is useful in some cases where responsiveness or memory are concerns or when the full df is not needed, but if you don't need to chunk the file and want the full dataframe anyhow, you can skip the iteration and subsequent concatenation and just read the whole file into a dataframe in one step:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(file_path, names=['Size()sqft', 'No of bedrooms', 'No of floors', 'Age of home', 'Price(1000s dollar)'])
</code></pre>
","1","Answer"
"78234888","78234279","<p>The problem is that you use regressor models and not classifier models. The predict() function from the LinearRegression(), RandomForestRegressor() and SVR() will all give float numbers, not binary values, which will induce the reported error (&quot;<em>ValueError: Classification metrics can't handle a mix of binary and continuous targets</em>&quot;) when used with an accuracy metric.</p>
<p>For each regressor model you propose, there is a corresponding classifier model.</p>
<p>The following toy example will work:</p>
<pre><code>from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
X, y = load_breast_cancer(return_X_y=True)

def fit_and_accuracy(model, X, y):
    clf = model.fit(X, y)
    y_pred = clf.predict(X)
    print(accuracy_score(y, y_pred))

fit_and_accuracy(LogisticRegression(solver='newton-cg'), X, y)
fit_and_accuracy(DecisionTreeClassifier(), X, y)
fit_and_accuracy(SVC(), X, y)
fit_and_accuracy(RandomForestClassifier(), X, y)
fit_and_accuracy(MLPClassifier(), X, y)
</code></pre>
<p>In your code, you should thus use either LogisticRegression, DecisionTreeCLassifier, SVC, RandomForestClassifier, or MLPClassifier, or any other classifier, but not a regressor.</p>
","0","Answer"
"78235074","78234777","<p>Assuming there are no errors in the code such that y_test equals y_pred, I would analyse the dataset to try and understand if the result makes sense.</p>
<p>Remember that the K-Neighbors algorithm will pick the most common class among the K observations closest to the point. So, if the data is already well separated, it is very possible that the K closest observations are always from the correct class.</p>
<p>Imagine a XY plane with two clusters really far apart from each other, in this case the K-Neighbors will almost always return a 100% accuracy.</p>
","0","Answer"
"78235727","78234946","<p>Below is how you can do what you ask.  I converted your age categories to mean age because correlation requires two numeric values; a category will not work for correlation.  There are some other problems with your data.  It is unclear what the 65 and older class really is numerically.  I made it 65-100 but that may not be the case.  You also have your categories set at 25-34 for example.  It should be 25-35 because 25-35 does not contain 35 it contains 25, 26, 27, 28, 29, 30, 31, 32, 33 and 34 which is what I think you are trying to achieve.  I did not change this but you should change it if that is what you are trying to achieve.</p>
<pre><code>import pandas as pd
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings(&quot;ignore&quot;)

Agelst=['65 and older','55-64','up to 25','45-54','35-44','25-34']
Ratelst=[38.45,17.66,46.56,24.95,33.54,37.21]

df=pd.DataFrame()
df['Age_Group']=Agelst
df['Overal_Rating']=Ratelst

display(df)

#Change 'up to 25' to '0-25'
df.replace('up to 25', '0-25',inplace=True)
df.replace('65 and older', '65-100',inplace=True)

display(df)

#You will need a numeric age to use for correlation.  We can develop one from the strings in your 'Age_Group'
loweragelst=[]
upperagelst=[]
for i in range(len(df)):
    loweragelst.append(int(((df.iloc[i]['Age_Group']).split('-'))[0]))
    upperagelst.append(int(((df.iloc[i]['Age_Group']).split('-'))[1]))

df['Lower_Age']=loweragelst
df['Upper_Age']=upperagelst

#Sort the df
df.sort_values(by=['Lower_Age'], ascending=True,inplace=True)
display(df)

#Add a mean age column to use for correlation
df['Mean_Age']=(df['Lower_Age']+df['Upper_Age'])/2

display(df)

#Calculate Pearson's Correlation
X=df['Mean_Age']
Y=df['Overal_Rating']
PCor= pearsonr(X, Y)
print(PCor)
</code></pre>
<p>The resulting df and correlation are:</p>
<pre><code>Age_Group   Overal_Rating
0   65 and older    38.45
1   55-64   17.66
2   up to 25    46.56
3   45-54   24.95
4   35-44   33.54
5   25-34   37.21
    Age_Group   Overal_Rating
0   65-100  38.45
1   55-64   17.66
2   0-25    46.56
3   45-54   24.95
4   35-44   33.54
5   25-34   37.21
    Age_Group   Overal_Rating   Lower_Age   Upper_Age
2   0-25    46.56   0   25
5   25-34   37.21   25  34
4   35-44   33.54   35  44
3   45-54   24.95   45  54
1   55-64   17.66   55  64
0   65-100  38.45   65  100
    Age_Group   Overal_Rating   Lower_Age   Upper_Age   Mean_Age
2   0-25    46.56   0   25  12.5
5   25-34   37.21   25  34  29.5
4   35-44   33.54   35  44  39.5
3   45-54   24.95   45  54  49.5
1   55-64   17.66   55  64  59.5
0   65-100  38.45   65  100     82.5

PearsonRResult(statistic=-0.4489402583278369, pvalue=0.37183097344063043)
</code></pre>
","0","Answer"
"78235820","78228806","<blockquote>
<p>And here's a zsh-specific version:
<a href=""https://stackoverflow.com/a/11025993"">https://stackoverflow.com/a/11025993</a></p>
</blockquote>
<p>The <code>PATH</code> variable holds a list of directories separated by colons, so if you want to add more than one directory, just put a colon between them:</p>
<pre><code>export PATH=$PATH:/usr/local/git/bin:/usr/local/bin
</code></pre>
","0","Answer"
"78236244","78230023","<p>Based on the information provided, it looks like the instance is running out of memory during the conversion process.</p>
<p>You can monitor the current memory consumption via the JupyterLab UI (see <a href=""https://i.sstatic.net/MTZJK.png"" rel=""nofollow noreferrer"">example here</a>) or by running <code>free -m</code> in a terminal.</p>
<p>Apart from using an instance with more memory attached (e.g. ml.g4dn.2xlarge), you can also try to run the conversion without debug information enabled, or you can look into <a href=""https://www.tensorflow.org/lite/performance/post_training_quantization"" rel=""nofollow noreferrer"">post-training quantization</a> to reduce the model size.</p>
","0","Answer"
"78237363","78169647","<p>It's a bug of <code>Pycaret</code>.</p>
<p>Downgrade to <code>pip install pycaret==3.2.0</code> and <code>pip install scikit-learn==1.2.2</code></p>
<p>You can track the progress to solve this bug here: <a href=""https://github.com/pycaret/pycaret/pull/3935"" rel=""nofollow noreferrer"">https://github.com/pycaret/pycaret/pull/3935</a></p>
","1","Answer"
"78237650","78183834","<p>I am quite definitely not an expert either, but I have had the same issue. I resolved the issue by using a slightly older version of tf using:</p>
<pre><code>!pip install -U &quot;tensorflow-text==2.15.*&quot;
!pip install -U &quot;tf-models-official==2.15.*&quot;
</code></pre>
<p>For reference, I am running scripts on Google Colab (I know this sometimes has its own little ways..).</p>
","2","Answer"
"78240012","78239906","<p>I think there may be an issue with your formula for calculating the dequantized output.</p>
<pre><code>import numpy as np

# Original values
activation = np.array([1, 2, 3, 4])
weight = np.array([5, 6, 7, 8])

# Quantization parameters
bit = 16  # Desired bit precision
min_val = min(np.min(activation), np.min(weight))
max_val = max(np.max(activation), np.max(weight))

# Calculate scale factor
scale_factor = (2 ** (bit - 1) - 1) / max(abs(min_val), abs(max_val))

# Quantize activation and weight values
quantized_activation = np.round(activation * scale_factor).astype(np.int16)
quantized_weight = np.round(weight * scale_factor).astype(np.int16)

# Dequantize activation and weight values
dequantized_activation = quantized_activation / scale_factor
dequantized_weight = quantized_weight / scale_factor

# Print values
print(&quot;Original activation:&quot;, activation)
print(&quot;Original weight:&quot;, weight)
print(&quot;Minimum value:&quot;, min_val)
print(&quot;Maximum value:&quot;, max_val)
print(&quot;Scale factor:&quot;, scale_factor)
print(&quot;Quantized activation:&quot;, quantized_activation)
print(&quot;Quantized weight:&quot;, quantized_weight)
print(&quot;Dequantized activation:&quot;, dequantized_activation)
print(&quot;Dequantized weight:&quot;, dequantized_weight)

---------------------------------------------------------

Original activation: [1 2 3 4]
Original weight: [5 6 7 8]
Minimum value: 1
Maximum value: 8
Scale factor: 4095.875
Quantized activation: [ 4096  8192 12288 16384]
Quantized weight: [20479 24575 28671 32767]
Dequantized activation: [1.00003052 2.00006104 3.00009156 4.00012207]
Dequantized weight: [4.99990844 5.99993896 6.99996948 8.        ]

</code></pre>
<p>Calculate output:</p>
<pre><code>output = np.sum(dequantized_activation * dequantized_weight)
print(&quot;Dequantized output:&quot;, output) # 70.00183110125477
</code></pre>
","0","Answer"
"78242796","78242480","<p>Here's an approach to ensure that:</p>
<p>Every <code>&quot;Delivery Guy&quot;</code> is represented in both the training and test sets.
Each <code>&quot;Target&quot; class</code> is adequately represented in both sets.</p>
<ul>
<li>This can be achieved by manually splitting the dataset while considering both the &quot;Delivery Guy&quot; and the &quot;Target&quot; columns. Here's a step-by-step guide to doing this:</li>
</ul>
<h4>Step 1: Split Data by &quot;Delivery Guy&quot; :</h4>
<ul>
<li>First, split your dataset by &quot;Delivery Guy&quot; to ensure that data for each individual is grouped together.</li>
</ul>
<pre><code>for name, group in df.groupby('Delivery Guy'):
</code></pre>
<h4>Step 2: For Each Group, Further Split by &quot;Target&quot;</h4>
<ul>
<li>For each &quot;Delivery Guy&quot;, split their data based on the &quot;Target&quot; to ensure that you can separately handle the representation of each class. :</li>
</ul>
<pre><code>for target, target_group in group.groupby('Target'):
</code></pre>
<h4>Step 3 : Step 3: Allocate Train/Test Data</h4>
<ul>
<li>For each subgroup (i.e., each &quot;Delivery Guy&quot; with a specific &quot;Target&quot;), allocate a portion of the data to the training set and the rest to the test set. Given the imbalanced nature of your dataset, you might want to ensure that at least one instance of each &quot;Target&quot; for each &quot;Delivery Guy&quot; ends up in both the training and test sets, if possible.</li>
</ul>
<h4>Step 4: Combine Data Back</h4>
<p>After allocating both &quot;Target&quot; classes for each &quot;Delivery Guy&quot; to both sets, combine these allocations back into your final training and test sets.</p>
<p>Here's how you could implement this in Python:</p>
<pre><code>df = pd.DataFrame(data)

train_list = []
test_list = []

# Split the dataset by 'Delivery Guy' ensuring each one is represented in both sets
for name, group in df.groupby('Delivery Guy'):
    for target, target_group in group.groupby('Target'):
        if len(target_group) &gt; 1:
            target_train, target_test = train_test_split(target_group, test_size=0.5, random_state=42)
            train_list.append(target_train)
            test_list.append(target_test)
        else:
            # Decide to add the single sample group to the training set
            train_list.append(target_group)

# Concatenate all the lists into DataFrames at once
train = pd.concat(train_list, ignore_index=True)
test = pd.concat(test_list, ignore_index=True)

</code></pre>
","0","Answer"
"78243976","78243492","<p>First of all, you need to have 3 classes in your dataset, which means that you need to distinguish sneeze samples as you have done it for cough/not cough. Then, you need to convert your output to one hot encoded vectors, in which, all elements are zero except for the element corresponding to the class index. For example, if you consider not cough = 0, cough= 1, and sneeze =2, a sample with sneeze must be [0, 0, 1], a sample with cough must be [0, 1, 0] and a sample with no cough must be [1, 0, 0]
Finally, your output layer should have 3 neurons.</p>
<pre><code>model.add(Dense(3, activation='softmax'))
</code></pre>
","1","Answer"
"78244074","78244042","<p>Many things can cause this.</p>
<p>1- Small dataset: If your dataset is small, the number of testing samples will be low, and division can be done in a way that the model performs very good on those limited number of test data.</p>
<p>2- Your testing data is similar to training data.</p>
<p>3- Duplicate samples in the testing data.</p>
<p>Draw the cross-plots for your training and testing subset and asses their behavior. You can figure out the reason by analyzing those figures.</p>
","0","Answer"
"78245883","78245568","<p>Check the docs - <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer"" rel=""nofollow noreferrer"">transformer class</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer"" rel=""nofollow noreferrer"">transformer decoder</a></p>
<p>For an unbatched (2 dim) input where <code>src = (S, E)</code> and <code>tgt = (T, E)</code>, the output will be of shape <code>(T, E)</code>.</p>
<p>In the transformer decoder layer, the first argument is <code>tgt</code> which defines the output size.</p>
<p>Since you define your <code>tgt</code> param <code>L</code> as <code>torch.randn(1, d)</code>, your transformer decoder output will be of size <code>(1, d)</code>.</p>
<p>This has nothing to do with broadcasting, this is just the input/output mechanics of the transformer layer.</p>
","0","Answer"
"78247221","78246736","<p><strong>TL;DR</strong></p>
<pre class=""lang-py prettyprint-override""><code>X_train_permuted = (
    X_train_permuted.with_columns(
        pl.DataFrame(shuffle_arr, schema=features)
    )
)
</code></pre>
<hr />
<p>Let's work with a simple example.</p>
<p><strong>X_train_permuted</strong></p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
import numpy as np

np.random.seed(0)

data = {f'feature_{i}': np.random.rand(4) for i in range(0,3)}

X_train_permuted = pl.DataFrame(data)

X_train_permuted

shape: (4, 3)
┌───────────┬───────────┬───────────┐
│ feature_0 ┆ feature_1 ┆ feature_2 │
│ ---       ┆ ---       ┆ ---       │
│ f64       ┆ f64       ┆ f64       │
╞═══════════╪═══════════╪═══════════╡
│ 0.548814  ┆ 0.423655  ┆ 0.963663  │
│ 0.715189  ┆ 0.645894  ┆ 0.383442  │
│ 0.602763  ┆ 0.437587  ┆ 0.791725  │
│ 0.544883  ┆ 0.891773  ┆ 0.528895  │
└───────────┴───────────┴───────────┘
</code></pre>
<p><strong>Shuffle <code>feature_0</code> and <code>feature_1</code></strong></p>
<p>Use a list to keep track of the features you are shuffling: <code>features = [&quot;feature_0&quot;, &quot;feature_1&quot;]</code>.</p>
<pre class=""lang-py prettyprint-override""><code>features = [&quot;feature_0&quot;, &quot;feature_1&quot;]

shuffle_arr = np.array(X_train_permuted[:, features])

from sklearn.utils import check_random_state

random_state = check_random_state(42)
random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

random_state.shuffle(shuffle_arr)

shuffle_arr

array([[0.71518937, 0.64589411],
       [0.60276338, 0.43758721],
       [0.5488135 , 0.4236548 ],
       [0.54488318, 0.891773  ]])
</code></pre>
<p><strong>Replace associated columns in <code>X_train_permuted</code> with <code>shuffle_arr</code> values</strong></p>
<ul>
<li>Use <a href=""https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.with_columns.html"" rel=""nofollow noreferrer""><code>pl.DataFrame.with_columns</code></a> and pass a <a href=""https://docs.pola.rs/py-polars/html/reference/dataframe/index.html"" rel=""nofollow noreferrer""><code>pl.DataFrame</code></a> with <code>schema=features</code>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>X_train_permuted = (
    X_train_permuted.with_columns(
        pl.DataFrame(shuffle_arr, schema=features)
    )
)

X_train_permuted

shape: (4, 3)
┌───────────┬───────────┬───────────┐
│ feature_0 ┆ feature_1 ┆ feature_2 │
│ ---       ┆ ---       ┆ ---       │
│ f64       ┆ f64       ┆ f64       │
╞═══════════╪═══════════╪═══════════╡
│ 0.715189  ┆ 0.645894  ┆ 0.963663  │
│ 0.602763  ┆ 0.437587  ┆ 0.383442  │
│ 0.548814  ┆ 0.423655  ┆ 0.791725  │
│ 0.544883  ┆ 0.891773  ┆ 0.528895  │
└───────────┴───────────┴───────────┘
</code></pre>
","2","Answer"
"78250658","78250613","<p>I found the answer. You can solve this using</p>
<pre><code>from llama_index.core.node_parser import SimpleNodeParser
</code></pre>
","7","Answer"
"78250711","78250613","<p><code>from llama_index.core.node_parser import SimpleNodeParser</code> ?</p>
","2","Answer"
"78250829","78250613","<p>Check the version of your llama_index if you are using the current version, kindly use :</p>
<h2>from llama_index.core.node_parser import SimpleNodeParser</h2>
","2","Answer"
"78250904","78223936","<p>The error in your comment says <code>save_frequency</code> is an unknown keyword for <code>ModelCheckpoint</code>. <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint#args"" rel=""nofollow noreferrer"">In the docs</a> it shows that <code>ModelCheckpoint</code> expects <code>save_freq</code> as an argument. Also beware that if you set an integer for <code>save_freq</code>, it saves each x batches, not epochs. If you just want it to save every epoch, you can set</p>
<pre class=""lang-py prettyprint-override""><code>save_freq='epoch'
</code></pre>
<p>which is also the default.
Why this woked in 2.15, I can not say, the docs for 2.15 also have <code>save_freq</code> in it. Maybe it was compatible with <code>save_frequency</code> in the background up to that point. It is better to use <code>save_freq</code>, as it is officially supported.</p>
<p>If you want to change a package version in colab, use</p>
<pre><code>%pip install -U &quot;tensorflow~=2.15.0&quot;
</code></pre>
<p><code>%</code> enables the pip command outside the python environment, <code>-U</code> upgrades (or in your case downgrades) existing versions and you need <code>&quot;&quot;</code> and  either == or ~= for the version numbers. == for exact this version and ~= for, in this example &quot;2.15.x&quot;, where x is the newest version.</p>
","0","Answer"
"78253665","78253278","<p>The training is fine, but the comparison code is wrong though: You are sampling x uniformly in range(0,1). Try to correct it and train again.</p>
<pre><code>    for i in range(sample_num):
        train_feature = tensor([0, 0, 0, 0, 0, i / sample_num], dtype=float32)
</code></pre>
","0","Answer"
"78254690","78253997","<p>If you look into the source code of <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L160"" rel=""nofollow noreferrer""><code>VisionTransformer</code></a>, you will notice in <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L235-L243"" rel=""nofollow noreferrer""><strong>this section</strong></a> that <code>self.heads</code> is a sequential layer, not a linear layer. By default, it only contains a single layer <code>head</code> corresponding to the final classification layer. To overwrite this layer, you can do:</p>
<pre><code>heads = self.vit_b_16.heads
heads.head = nn.Linear(heads.head.in_features, num_classes)
</code></pre>
","2","Answer"
"78254750","78254412","<p>I preprocessed the image and used  <a href=""https://docs.opencv.org/3.4/db/d73/classcv_1_1LineSegmentDetector.html"" rel=""nofollow noreferrer"">LineSegmentDetector</a> to find the line segments. Later used some logical constraints to find the graph. Try below code, I have added comments for the explanation of each steps. Hope this help.</p>
<pre><code>import cv2
import numpy as np
import math

#Def to Calculate distance between 2 points
def distanceCalculate(p1, p2):
    &quot;&quot;&quot;p1 and p2 in format (x1,y1) and (x2,y2) tuples&quot;&quot;&quot;
    dis = ((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2) ** 0.5
    return dis


# Load the image
image_path = 'graphs.jpg'
image = cv2.imread(image_path)

# Convert the image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

#Threshold the image to obtain only the black parts
ret, thresholded_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#Invert the image
thresholded_image = 255 - thresholded_image

#Do an erosion operation using cross structure kernal to remove parts the are not horizontal or vertical
kernel =  cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))
thresholded_image = cv2.erode(thresholded_image,kernel,iterations = 1)

#Use LineSegmentDetector in CV2 to dinf the line segments in the image
lsd = cv2.createLineSegmentDetector(0)
dlines = lsd.detect(thresholded_image)

#Get the line segments in the image
#Find the distance and angle of the found line segments.
#I select the line segment if the distance is &gt; 25 and angle between 0 to 5 for horizontal aned 85 to 90 for vertical lines
line_image = np.zeros_like(thresholded_image)
for dline in dlines[0]:
    x0 = int(round(dline[0][0]))
    y0 = int(round(dline[0][1]))
    x1 = int(round(dline[0][2]))
    y1 = int(round(dline[0][3]))
    dis = distanceCalculate([x0, y0],[x1,y1])
    angle_between = math.degrees(math.atan2(abs(y1-y0), abs(x1-x0)))
    if dis&gt;25 and (0&lt;=angle_between&lt;5 or 85&lt;angle_between&lt;=90):
        cv2.line(line_image, (x0, y0), (x1,y1), 255, 5, cv2.LINE_AA)

#Dilate the the line image to fill gaps in between
kernel =  cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))
line_image = cv2.dilate(line_image,kernel,iterations = 1)

#Find fontour and the associated bounding box
#The area of graph bounding boxes will be large compared to other small line contours, this is used to find the graphs
graph_mask = np.zeros_like(thresholded_image)
contours, _ = cv2.findContours(line_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
selected_bounding_rect = []
for ctr in contours:
    bounding_rect = cv2.boundingRect(ctr)
    bounding_rect_area = bounding_rect[2] * bounding_rect[3]
    if bounding_rect_area&gt;15000:# This can be adjusted according to your need.
        contour_image = cv2.rectangle(graph_mask,bounding_rect,255,-1)

#Get the graph from original image from the mask
selected_image = cv2.bitwise_and(image, image, mask=graph_mask)
# Save the extracted graph image
cv2.imwrite('graph_result.jpg', selected_image)
</code></pre>
<p>Output image :</p>
<p><a href=""https://i.sstatic.net/AeAhz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AeAhz.jpg"" alt=""enter image description here"" /></a></p>
","1","Answer"
"78255547","78255277","<p>Your issue is your about the input features. You are using the predictor list as list of features for training.
For testing, you still need the same input list in your new_data_df. If the new_data_df has all of the predictors column, you can use the following to line of code for predictions</p>
<pre><code>predictions = model.predict(new_data_df[predictors])
</code></pre>
<p>Just remember, for prediction, you always need to pass all the parameters you passed during training apart from the target or result column.</p>
","1","Answer"
"78256222","78210297","<p>So, the solution was to save model and it's weights by using save_pretrained not by torch.save()</p>
","2","Answer"
"78258996","78257033","<p>Since you are using different libraries to achieve two different tasks, the error may be due to a mismatch in the Keras backend version in both libraries; I would suggest using the name of the loss as a string.</p>
<pre><code>model.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.MeanSquaredError(), metrics = ['mean_squared_error'])
</code></pre>
","0","Answer"
"78260163","78259907","<p>If you replace your classifier with the identity function, you will see what the problem is:</p>
<pre><code>model.classifier = nn.Identity()
model(torch.rand(2,3,512,512)).shape
torch.Size([2, 492032])
</code></pre>
<p>The <code>in_features</code> of your classifier linear layer should be <code>492032</code>, not <code>1280</code>. Beside, if you compare with the source code of <code>SqueezeNet</code>, you will see that <code>model.classfier</code> does not contain a linear layer but a convolutional layer followed by a pooling layer, <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py#L81-L83"" rel=""nofollow noreferrer"">line 81</a>:</p>
<pre><code>final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)
self.classifier = nn.Sequential(
   nn.Dropout(p=dropout), 
   final_conv, 
   nn.ReLU(inplace=True), 
   nn.AdaptiveAvgPool2d((1, 1)))
</code></pre>
<p>Considering the kernel size is of shape <code>1x1</code>, it acts as a linear layer. You could therefore replace your implementation with the following code:</p>
<pre><code>model.classifier = nn.Sequential(
    nn.Dropout(p=dropout), 
    nn.Conv2d(512, len(class_names), kernel_size=1),
    nn.ReLU(inplace=True), 
    nn.AdaptiveAvgPool2d((1, 1)))
</code></pre>
","0","Answer"
"78265382","78263004","<p>I think you also need to install <code>llama-index-llms-llama-cpp</code> and <code>llama-index-embeddings-huggingface</code> in addition to <code>llama-index</code> as suggested from the <a href=""https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/#installation"" rel=""nofollow noreferrer"">installation guide</a> using the command below</p>
<pre><code>pip install llama-index-llms-llama-cpp llama-index-embeddings-huggingface
</code></pre>
","3","Answer"
"78267414","78266973","<p>It seems like the trouble is with matplotlib/seaborn. (I can't reproduce it; maybe you need to give us a reproducible example with the exact code you wrote and your dataset.)</p>
<p>Instead of using the plot, you can display/print the confusion matrix as a DataFrame.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.metrics import confusion_matrix

def get_confusion_matrix_df(classifier, X, y):
    &quot;&quot;&quot;Return the confusion matrix as a DataFrame.&quot;&quot;&quot;
    labels = classifier.classes_
    columns_labels = pd.MultiIndex.from_product([[&quot;Predicted&quot;], labels])
    index_labels = pd.MultiIndex.from_product([[&quot;Actual&quot;], labels])
    prediction = classifier.predict(X)
    matrix = confusion_matrix(y, prediction, labels=labels)
    return pd.DataFrame(matrix, columns=columns_labels, index=index_labels)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>get_confusion_matrix_df(rf_cv, X_train, y_train)
</code></pre>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier

X, y = load_iris(return_X_y=True, as_frame=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

get_confusion_matrix_df(model, X_test, y_test)
</code></pre>
<p>Result:</p>
<img src=""https://i.sstatic.net/mrS10.png"" width=""200"" />
","1","Answer"
"78268385","78265338","<p>If I understood correctly, in the first method you define windows (short sequences) of data and then you shuffle the windows (<em>not</em> their contents, i.e. you preserve the sequential nature of the data). In the second method you simply shuffle all samples, which destroys the ordering information contained in the data.</p>
<p>Sequential models are designed to learn from patterns in the sequences where the order matters. When the data is randomly shuffled as in method 2, the ordering information is lost and the model doesn't perform as well. The first method preserves the ordering of samples within the windows, allowing the model to learn meaningful patterns and dependencies within each window, which I think is why it performs better.</p>
<blockquote>
<p>At least I am expecting a close result of 80-85% with the second step</p>
</blockquote>
<p>If your data is inherently sequential, I don't think method 2 is a suitable way of preprocessing the data as you lose the sequence information. If you proceed with method 2, then a recurrent model would not usually be a suitable choice. An MLP would be more appropriate, as MLPs treat each feature independently and they don't rely on sequential patterns. Though it still might not perform as well as method 1 + recurrent model.</p>
","0","Answer"
"78269418","78269213","<p>You're using <code>CrossEntropyLoss</code> incorrectly.</p>
<p>Read the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer"">pytorch docs</a> - <code>CrossEntropyLoss</code> combines a <code>LogSoftmax</code> and <code>NLLLoss</code> in a single operation.</p>
<p>You apply a log softmax at the end of your model, then send it to <code>CrossEntropyLoss</code>. This means you are softmaxing twice.</p>
","0","Answer"
"78270010","78251629","<pre><code>from llama_index.legacy.embeddings.langchain import LangchainEmbedding
</code></pre>
","0","Answer"
"78270056","78270055","<p>SARIMA (and Exponential Smoothing) have well-known issues with &quot;long&quot; seasonality, as noted in Rob Hyndman's blog <a href=""https://robjhyndman.com/hyndsight/longseasonality/"" rel=""noreferrer"">post</a>:</p>
<blockquote>
<p>[...] seasonal differencing of very high order does not make a lot of sense — for daily data it involves comparing what happened today with what happened exactly a year ago and there is no constraint that the seasonal pattern is smooth.</p>
</blockquote>
<p>The commonly accepted way of dealing with &quot;long&quot; seasonality is to either use a regression on Fourier terms with ARIMA errors, or to use specialized state space models like BATS/TBATS - refer to the above linked blog post.</p>
","9","Answer"
"78270057","78270055","<p>According to Rob Hyndman's blog post <a href=""https://robjhyndman.com/hyndsight/longseasonality/"" rel=""noreferrer"">&quot;Forecasting with long seasonal periods&quot;</a>, seasonal ARIMA(p,d,q)(P,D,Q) models with a long seasonal period are hard to estimate:</p>
<blockquote>
<p>The <code>arima()</code> function will allow a seasonal period up to m=350 but in practice will usually run out of memory whenever the seasonal period is more than about 200.</p>
</blockquote>
<p>This is likely the reason for why your code does not run.</p>
<p>Contrary to the answer by Stephan Kolassa, it is my understanding that this has nothing to do with the order of seasonal differencing, D, as the problem exists not only when D&gt;1 but also when D=0. Meanwhile, seasonal ARIMA models with short seasonal periods and D&gt;1 are not subject to this problem. Thus, I believe the focus on seasonal differencing is a red herring.</p>
<p>Regarding recommended alternatives, I concur with the ones suggested in the answer by Stephan Kolassa.</p>
","7","Answer"
"78271600","78271090","<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</a></p>
<p>It appears you're giving Model.fit([X], [y]) the wrong type.</p>
<p>What I almost always do before handing off data to train_test_split is converting my features and labels to np arrays.</p>
<p>So you can either convert them before handing them off to train_test_split or do it before the model.fit(...)</p>
<p>NOTE: Don't forget to add <code>import numpy as np</code></p>
<p>So in your case you'd do:</p>
<pre><code>X_training_np = np.array(X_training)
y_training_np = np.array(y_training)

model.fit(X_training_np, y_training_np, epochs=...)
</code></pre>
","12","Answer"
"78271695","78251629","<p>Try this.</p>
<pre><code>from llama_index.llms.huggingface.base import HuggingFaceInferenceAPI
</code></pre>
","0","Answer"
"78275382","78268793","<p>I think you can try adding this to register the custom function</p>
<pre><code>model = load_model(
  (path),
  custom_objects={
    'KerasLayer':hub.KerasLayer, 
    'NotEqual': NotEqual, 
  }
)
</code></pre>
<p>Reference: <a href=""https://www.tensorflow.org/guide/keras/serialization_and_saving#passing_custom_objects_to_load_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/serialization_and_saving#passing_custom_objects_to_load_model</a></p>
<p>p/s: the first line regarding <code>TF_ENABLE_ONEDNN_OPTS</code> is just a warning message, it's not related to the error</p>
","1","Answer"
"78275819","78275777","<p>The author treats the output as probabilities. <code>from_logits=True</code> represent that the model outputs its values as probabilities. The range has nothing to do with the logits status. The sigmoid is what limits the output between <code>0</code> and <code>1</code>. <code>Logits=True</code> in this case is means the author wants the outputs to be treated probabilities and not as as continuous values.</p>
","0","Answer"
"78276526","78276066","<p>The lines you've pointed out:</p>
<pre><code>M = 1 / n * np.sum(f(train_x) - train_y)
B = 1 / n * np.sum(np.sum(f(train_x) - train_y) * train_x)
</code></pre>
<p>are incorrect because they involve operations between arrays of incompatible shapes.</p>
<p><code>y_train</code> is a 1D array with shape of <code>(80,)</code> and <code>f(x_train)</code> is a 2D array with a shape of <code>(1, 80)</code>. So <code>f(train_x) - train_y</code> will be an <code>(80, 80)</code> array which is not intended in this use case.</p>
<p>In addition, the term <code>x_train</code> should be multiplied when calculating the gradient of <code>f</code> respecting <code>M</code> not <code>B</code>.</p>
<p>You can calculate gradients by either transposing one of them:</p>
<pre><code>M = 1 / n * np.sum((f(train_x) - train_y.reshape(-1, 1)) * train_x)
B = 1 / n * np.sum(f(train_x) - train_y.reshape(-1, 1))
</code></pre>
<p>or use For loops like this:</p>
<pre><code>M = 1 / n * np.sum(np.array([(f(x_i) - y_i) * x_i for x_i, y_i in zip(train_x, train_y)]))
B = 1 / n * np.sum(np.array([f(x_i) - y_i for x_i, y_i in zip(train_x, train_y)]))
</code></pre>
<p>Moreover, a better range for initialization should be used. For example, use <code>random.uniform</code> for <code>m</code> and <code>b</code> like this:</p>
<pre><code>m = random.uniform(-100, 100)
b = random.uniform(-100, 100)
</code></pre>
","0","Answer"
"78280466","78275238","<p>It could be that there are just a few samples with <code>ocean_proximity='ISLAND'</code>, such that on some splits it doesn't appear in the training set and only appears in the validation set. This would lead to an error condition, as the encoder is seeing categories in the validation set that it was not fit on at train time.</p>
<p>One way round this is to tell <code>OneHotEncoder</code> in advance what all the possible categories are using the <code>categories=</code> parameter. This is different to the default configuration (<code>categories='auto'</code>) where it figures out the categories based on the training data it sees. You might use something like:</p>
<pre><code>OneHotEncoder(
  ..., categories=['cat__ocean_proximity_&lt;1H OCEAN',
                   'cat__ocean_proximity_INLAND',
                   'cat__ocean_proximity_ISLAND',
                   'cat__ocean_proximity_NEAR BAY',
                   'cat__ocean_proximity_NEAR OCEAN']
)
</code></pre>
<p>This way, even if a category doesn't show up in the training split, the encoder can still handle it properly when it comes up in the validation split.</p>
","1","Answer"
"78283018","78282862","<p>As of version 2.2.0, torch does not yet support <code>compile</code> with Python 3.12.</p>
<p>Check <a href=""https://github.com/pytorch/pytorch/issues/120233"" rel=""noreferrer"">pytorch/pytorch#120233</a> issue for more details.</p>
","5","Answer"
"78284562","78284460","<p>I have faced that problem before and I fixed with called one by one package and it worked well. I hope it can help you to solve your problem.</p>
<pre><code>from tensorflow import keras
from keras import layers
from keras.layers import BatchNormalization
</code></pre>
","0","Answer"
"78286428","78286410","<p>There may be a way to do this with Laravel applications but I’m not sure what that would be. However, we have django in python, which may be a good way to resolve your problem, as Django is already a really easy module to install on python.</p>
<p>From what I was able to find on the web, a python module called VADER is also able to do stuff with PHP and all that.</p>
<p>I’d recommend looking into Django as it’s considered the default for using python with a web-framework.</p>
","-1","Answer"
"78286502","78286410","<p>I don't believe there's a good reason to use Laravel + Python here as you would be adding complexity for no clear reason.</p>
<p>I would agree with others about leveraging Django/Flask rather than added another Language + Stack.</p>
<p>I'm going to assume you need to deliver this on demand, such as viewing youtube's home page and seeing recommendations.</p>
<p>What you could do is execute shell commands from Laravel, but this is dangerous and you should avoid it all costs.</p>
<p>The only other option I can think of is to create a HTTP API in the python application that you only expose to the Laravel application, but it doesn't fully make sense to have two web applications in two different languages and frameworks as mentioned.</p>
<p>If you don't need it to be a realtime feature then you could run the python application on a set schedule that stores the results in the same database your Laravel application leverages.</p>
","0","Answer"
"78286506","78286410","<p>you can create a flask or fastapi application in python to serve as an api for your recommendation system. this api will receive input data from your laravel application and return the recommended results. so you'll have two servers: one for generate recomendation and the other is your main server with laravel.</p>
","1","Answer"
"78286511","78257853","<h2>Short Answer</h2>
<p>When the training data contains less than 200 rows, use the following parameters:</p>
<ul>
<li><code>min_data_in_leaf = 1</code></li>
<li><code>min_data_in_bin = 1</code></li>
</ul>
<h2>Details</h2>
<p>LightGBM has a few important parameters to prevent against overfitting, and the default values of these assume you have at least a few hundred samples.</p>
<ul>
<li><code>min_data_in_leaf</code>: minimum number of samples that must fall into a leaf node (default = 20)</li>
<li><code>min_data_in_bin</code>: minimum number of samples to group together into one histogram &quot;bin&quot; when LightGBM discretizes features (default = 3)</li>
</ul>
<p>For more details on that, see <a href=""https://stackoverflow.com/a/66728185/3986677"">&quot;Why R2 score is zero in LightGBM?&quot;</a> and <a href=""https://stackoverflow.com/a/66188319/3986677"">&quot;Why does this simple LightGBM classifier perform poorly?&quot;</a>.</p>
<p>For a very small dataset like the one in your example (41 rows, 3 columns), those default values might be very limiting, resulting in only a few splits being added per tree.</p>
<p>Consider the following example using exactly the data you provided, with Python 3.11, <code>lightgbm==4.3.0</code>, <code>pandas==2.2.1</code>, and <code>scikit-learn==1.4.1</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import lightgbm as lgb
import pandas as pd
from sklearn.model_selection import train_test_split

data_x = [[2021,5,368.92],[2023,11,356.82],[2022,10,352.49],[2023,5,343.63],[2023,10,324.91],[2022,12,352.02],[2021,6,370.79],[2022,5,386.59],[2019,2,301.56],[2021,4,353.7],[2021,1,303.93],[2021,9,371.94],[2019,4,310.77],[2021,3,345.3],[2020,5,249.63],[2022,4,381.16],[2023,4,363.14],[2019,7,304.19],[2020,7,258.43],[2022,2,412.47],[2022,8,353.43],[2019,6,302.34],[2020,1,319.88],[2022,7,361.66],[2020,9,265.39],[2022,3,408.72],[2022,1,417.47],[2022,6,351.92],[2022,9,344.06],[2022,11,373.75],[2019,9,314.97],[2019,11,324.14],[2023,2,377.23],[2021,11,380.83],[2021,12,403.12],[2023,7,368.73],[2023,1,379.76],[2019,5,295.02],[2023,9,343.78],[2020,4,248.54],[2019,10,314.79],[2019,8,295.92],[2023,3,354.09],[2023,6,357.35],[2021,2,324.31],[2020,3,246.26],[2019,3,295.36],[2020,12,306.27],[2021,8,376.54],[2020,6,258.21],[2023,8,352.35],[2021,7,370.21],[2020,10,259.13],[2020,8,275.66],[2019,12,315.47],[2020,11,301.27],[2021,10,389.23],[2019,1,291.94],[2020,2,302.38]]

df_x = pd.DataFrame(data_x, columns=['Year', 'Month', 'Close'])

data_y = [[1479.42],[1654.53],[1537.76],[1621.22],[1567.62],[1528.39],[1444.63],[1562.17],[1356.81],[1463.48],[1558.9],[1463.96],[1362.03],[1432.7],[1502.46],[1524.71],[1592.68],[1342.74],[1467.48],[1553.66],[1609.19],[1349.1],[1379.39],[1496.12],[1448.08],[1562.96],[1525.25],[1575.06],[1591.15],[1544.66],[1319.9],[1366.73],[1482.72],[1520.73],[1557.03],[1577.37],[1624.74],[1402.05],[1614.94],[1482.28],[1338.88],[1354.6],[1553.65],[1606.36],[1510.78],[1348.05],[1323.39],[1542.95],[1411.64],[1493.44],[1563.53],[1414.8],[1452.67],[1491.7],[1451.43],[1467.23],[1477.13],[1360.29],[1386.48]]

df_y = pd.DataFrame(data_y, columns=['Value'])

X_train, X_test, y_train, y_test = train_test_split(
    df_x,
    df_y,
    test_size=0.3,
    random_state=21
)

params = {
    &quot;num_iterations&quot;: 10,
    &quot;objective&quot;: &quot;regression&quot;,
    &quot;min_data_in_leaf&quot;: 1,
    &quot;min_data_in_bin&quot;: 1,
    &quot;verbose&quot;: 0,
}

# train
gbm = lgb.LGBMRegressor(**params)
gbm.fit(X_train, y_train,
        eval_set=[(X_test, y_test)],
        eval_metric='mape')

# predict
preds = gbm.predict(X_test)
print(preds)
</code></pre>
<p>That produces predictions with some variation.</p>
<pre><code>[1514.86588126 1557.1389268  1423.54076682 1514.86588126 1488.24836945
 1541.52116271 1555.63537413 1393.69927646 1404.48244093 1465.1569698
 1404.48244093 1404.48244093 1514.86588126 1440.95713788 1535.84165832
 1482.58308126 1471.96999117 1504.50006758]
</code></pre>
<p>And the following scores on the test set</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.metrics import mean_absolute_error, r2_score

mean_absolute_error(y_test, preds)
# 45.212

r2_score(y_test, preds)
# 0.47
</code></pre>
<p>Some other notes related to the original question:</p>
<ul>
<li><code>num_iterations</code> and <code>n_estimators</code> are aliases for each other... they mean exactly the same thing. Just use one of them. (<a href=""https://lightgbm.readthedocs.io/en/latest/Parameters.html#num_iterations"" rel=""nofollow noreferrer"">LightGBM docs</a>)</li>
<li><code>&quot;auc&quot;</code> is a classification metric... it isn't appropriate for regression problems (<a href=""https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective"" rel=""nofollow noreferrer"">LightGBM docs</a>)</li>
<li><code>tas&quot;</code> is only for the LightGBM CLI. It doesn't affect the Python package at all. Omit it. (<a href=""https://lightgbm.readthedocs.io/en/latest/Parameters.html#task"" rel=""nofollow noreferrer"">LightGBM docs</a>)</li>
<li>In the <code>scikit-learn</code> estimators for LightGBM, omit <code>metric</code> from <code>params</code> and just pass the <code>eval_metric</code> keyword arguments to <code>.fit()</code></li>
</ul>
","1","Answer"
"78286571","78286410","<p>As others have correctly pointed out, the preferred solution should be to not switch languages and directly implement your API with Python, that is use Flask or FastAPI to implement your recommendation system's API.</p>
<p>If you really want/have to use PHP for your endpoint, you may want to checkout <a href=""https://onnxruntime.ai/"" rel=""nofollow noreferrer"">ONNX</a>. ONNX is another type of ML model format that offers runtimes for other languages than Python, such as C++, Java or <a href=""https://github.com/ankane/onnxruntime-php"" rel=""nofollow noreferrer"">PHP</a>. You can export your model from PyTorch or TensorFlow to ONNX (refer to the respective documentation), and then use the resulting ONNX in the onnxruntime of your respective target language. This is only applicable if your model inference code is representable as computation graph, which probably is the case.</p>
","0","Answer"
"78286636","78178902","<p>From the <code>scikit-learn</code> docs (<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html"" rel=""nofollow noreferrer"">link</a>):</p>
<blockquote>
<p><em>The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.</em></p>
</blockquote>
<p>Those test sets are not passed to the <code>.fit()</code> method of estimators, so you cannot use them for early stopping with <code>xgboost</code>'s <code>scikit-learn</code> estimators.</p>
<p>Prior to running <code>cross_val_predict()</code>, split off a portion of your training data as a validation set, then use that validation set to trigger early stopping <em>across all <code>k</code> training runs performed by <code>cross_val_predict()</code></em>.</p>
<p>Consider this example using Python 3.11, <code>numpy==1.26.4</code>, <code>scikit-learn==1.4.1</code>, and <code>xgboost==2.0.3</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import xgboost as xgb

import sklearn
from sklearn.datasets import make_regression
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_predict, KFold, train_test_split

# generate synthetic regression training data
X, y = make_regression(n_samples=10_000, n_features=7)

# reserve 10% of the data as a validation set
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, train_size=0.9, random_state=708
)

# initialize 3-fold cross-validation splitter
cvx = KFold(n_splits=3, shuffle=True, random_state=239)


# define a custom metric that never improves
def custom_constant_metric(y_true, y_pred):
    return 0.123


# choose specific values for xgboost early stopping
early_stop = xgb.callback.EarlyStopping(
    rounds=2,
    min_delta=1e-3,
    save_best=True,
    maximize=False,
    data_name=&quot;validation_0&quot;,
    metric_name=&quot;custom_constant_metric&quot;,
)

# enable metadata_routing
sklearn.set_config(enable_metadata_routing=True)

# configure an XGBoost regressor, and tell scikit-learn's
# cross-validation machinery to forward parameter &quot;eval_set&quot;
# through to XGBRegressor.fit()
model = xgb.XGBRegressor(
    n_estimators=5,
    random_state=239,
    tree_method=&quot;approx&quot;,
    callbacks=[early_stop],
    eval_metric=custom_constant_metric,
).set_fit_request(eval_set=True)

# generate predictions from CV splits
cv_preds = cross_val_predict(
    estimator=model,
    X=np.asarray(X_train),
    y=np.asarray(y_train),
    cv=cvx,
    method=&quot;predict&quot;,
    params={&quot;eval_set&quot;: [(X_valid, y_valid)]},
    verbose=1,
)

# evaluate fit
r2_score(y_train, cv_preds)
# 0.406
</code></pre>
<p>Note that in this example, I used 3-fold cross validation, asked XGBoost to perform 5 rounds of boosting, and asked XGBoost to trigger early stopping after just 2 rounds without improvement.</p>
<p>The logs from <code>cross_val_predict()</code> confirm this is working as expected... they show 3 training runs (one for each fold produced by <code>KFold</code>), each stopping after just 2 boosting rounds.</p>
<pre><code>[0]     validation_0-rmse:106.90484     validation_0-custom_constant_metric:0.12300
[1]     validation_0-rmse:84.89786      validation_0-custom_constant_metric:0.12300
[0]     validation_0-rmse:107.70702     validation_0-custom_constant_metric:0.12300

[1]     validation_0-rmse:85.48965      validation_0-custom_constant_metric:0.12300

[0]     validation_0-rmse:108.08369     validation_0-custom_constant_metric:0.12300
[1]     validation_0-rmse:86.36046      validation_0-custom_constant_metric:0.12300
</code></pre>
<p>The use of a custom metric function is just to show how this works... replace with <code>&quot;rmse&quot;</code> or whatever evaluation metric you want in your actual application.</p>
<p>The necessity for <code>sklearn.set_config()</code> and <code>.set_fit_request()</code> come from changes that were introduced in <code>scikit-learn</code> 1.4. See &quot;Metadata Routing&quot; in the <code>scikit-learn</code> docs (<a href=""https://scikit-learn.org/stable/metadata_routing.html#metadata-routing"" rel=""nofollow noreferrer"">link</a>) for details.</p>
","0","Answer"
"78287143","78275121","<p>Instead of linear regression, you can use other models to test if you can do modelling on your data. And btw, R² is not the biggest problem in using linear regression. Use my answer to study the residual plot in both cases, as the residuals of assuming the linear regression clearly hint to a heteroscedasticity. Check the comparison here:</p>
<pre><code>fig, axs = plt.subplots(nrows = 1, ncols = 2) # define subplots
###################################################################################
lrModel = LinearRegression() # random forest
lrModel.fit(XTrain, yTrain) # fit
lryPred = lrModel.predict(XTest) # test
lrRMSE = mean_squared_error(yTest, lryPred, squared=False) # RMSE
lrR2 = r2_score(yTest, lryPred) # R2
axs[0].scatter(lryPred, yTest) # scatter
axs[0].set_title(&quot;Linear Regression\nR² = &quot;+str(round(lrR2,2))+&quot;; RMSE = &quot;+str(round(lrRMSE)))
###################################################################################
dtModel = DecisionTreeRegressor(random_state=42) # decision tree
dtModel.fit(XTrain, yTrain) # fit
dtyPred = dtModel.predict(XTest) # test
dtRMSE = mean_squared_error(yTest, dtyPred, squared=False) # RMSE
dtR2 = r2_score(yTest, dtyPred) # R2
axs[1].scatter(dtyPred, yTest) # scatter
axs[1].set_title(&quot;Decision Tree Regressor\nR² = &quot;+str(round(dtR2,2))+&quot;; RMSE = &quot;+str(round(dtRMSE)))
</code></pre>
<p>The results are like this:</p>
<p><a href=""https://i.sstatic.net/JF5Xm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JF5Xm.png"" alt=""Actual vs predicted"" /></a></p>
<p>The choice of linear regression was false from the beginning. Predictions are also going in the minus. Use decision tree or random forest, they should give similar fits.</p>
","2","Answer"
"78288393","78288109","<p>It seems that the runtime of your code is dominated by book-keeping and looping operations rather than actual gradient descent. I've modified your code to avoid the outer for loop and allocate a numpy array and write to it instead of relying on slower lists:</p>
<pre><code>import time
import numpy as np
import matplotlib.pyplot as plt

def h_theta(X1, theta1):
    # Implementation of hypothesis function
    return np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
    # Implementation of cost function
    return np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, theta):
    # Calculation of gradient
    h = h_theta(X1, theta)
    gradient = np.dot(X1.T, h - y1) / y1.shape[0]
    return gradient

def gradient_descent(X1, y1):
    learning_rates = [0.1, 0.01, 0.001]
    num_iterations = 1000

    num_learning_rates = len(learning_rates)
    learning_rates = np.array(learning_rates).reshape(1, -1)
    theta_initial = np.zeros((num_learning_rates, X1.shape[1])).T  # Initialize theta with zeros
    cost_iterations = np.zeros((num_learning_rates, num_iterations))
    theta_values = np.zeros((num_learning_rates, X1.shape[1], 1))
    theta = theta_initial.copy()
    start = time.time()

    #for idx, alpha in enumerate(learning_rates):

    for i in range(num_iterations):
        gradient = grad(X1, y1, theta)
        theta = theta - learning_rates * gradient
        cost = j_theta(X1, y1, theta)
        cost_iterations[:, i] = cost
    end = time.time()
    print(f&quot;Time taken: {end - start} seconds&quot;)
    # fig, axs = plt.subplots(len(learning_rates), figsize=(8, 15))
    # for i, alpha in enumerate(learning_rates):
    #     axs[i].plot(range(num_iterations), cost_iterations[i], label=f'alpha = {alpha}')
    #     axs[i].set_title(f'Learning Rate: {alpha}')
    #     axs[i].set_ylabel('Cost J')
    #     axs[i].set_xlabel('Number of Iterations')
    #     axs[i].legend()
    # plt.tight_layout()
    # plt.show()

X_normalized = np.random.randn(3072, 9)
y_normalized = np.random.randn(3072, 1).repeat(3, 1).reshape((3072, 3))
intercept_column = np.random.randn(3072, 1)


# code to reduce X to 3 features (columns) using SVD:
# Perform Singular Value decomposition on X and reduce it to 3 columns
U, S, Vt = np.linalg.svd(X_normalized)
# Reduce X to 3 columns
X_reduced = np.dot(X_normalized, Vt[:3].T)

# print the first 5 rows of X_reduced
print(&quot;First 5 rows of X_reduced:&quot;)
# Normalize X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;the means and stds of X after being reduced and normalized:\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# Print the shape of the reduced X to confirm it has only 3 features
print(&quot;Shape of X_reduced:&quot;, X_reduced.shape)

# Adding the intercept column to X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))
X_normalized_with_intercept = np.hstack((intercept_column, X_normalized))

# Example usage
# X_normalized_with_intercept and y_normalized represent the original dataset
# X_reduced_with_intercept and y_normalized represent the reduced dataset

# Performing gradient descent on the original dataset

gradient_descent(X_normalized_with_intercept, y_normalized)

# Performing gradient descent on the reduced dataset
gradient_descent(X_reduced_with_intercept, y_normalized)
</code></pre>
<p>Now, the code runs faster on the reduced dataset. I've filled in the blanks that you haven't specified so that I would be able to run your code. Hopefully I didn't butcher the gradient descent code too badly as I've relied on the dimensions making sense instead of working through the problem with a pen and paper.</p>
","1","Answer"
"78288532","78274904","<p>The latter is the display for a scipy sparse array. <code>OneHotEncoder</code> produces sparse arrays as output, and <code>ColumnTransformer</code> uses a sparse array when the overall density of values of its output is below the parameter <code>sparse_threshold</code>.  In the first dataset, the density is exactly 0.3, the default <code>sparse_threshold</code>; after adding a new column in your new dataset, it's below the threshold (3/11).</p>
<p>The display you have lists all the nonzero entries of the output, with the location in the matrix in parentheses and the value after that. So all those 307's and 5's are your passthrough values, the 1's are the &quot;one hot&quot;s, and the zeros that are &quot;not hot&quot; are omitted (saving memory space, which is the point of this sparse storage format).</p>
<p>Lots of algorithms can make use of the sparse format directly, so you can likely just leave it as is. If you want a dense output, you can cast the array directly using <code>&lt;transformed_out&gt;.toarray()</code> (remove the <code>np.array(...)</code> from your code), setting <code>sparse=False</code> in the encoder, <strong>or</strong> setting <code>sparse_threshold=0</code> in the column transformer.</p>
","0","Answer"
"78288655","78286577","<p>opencv_traincascade has been removed since version 4.x, so you have to use opencv version 3.4.x to have opencv_traincascade.</p>
<p>You can use this <a href=""https://github.com/bilardi/how-to-train-cascade"" rel=""nofollow noreferrer"">guide</a> to install opencv_createsamples and opencv_traincascade.</p>
<p>References:</p>
<ul>
<li><a href=""https://docs.opencv.org/3.4/dc/d88/tutorial_traincascade.html"" rel=""nofollow noreferrer"">opencv documentation</a></li>
<li><a href=""https://github.com/opencv/opencv/issues/13231"" rel=""nofollow noreferrer"">opencv issues 13231</a></li>
<li><a href=""https://github.com/opencv/opencv/issues/14322"" rel=""nofollow noreferrer"">opencv issues 14322</a></li>
</ul>
","0","Answer"
"78290081","78277279","<p>From the source code, tensorflow v2.2.3 is the last version that contains <code>tf.Summary</code> export</p>
<p><a href=""https://github.com/tensorflow/tensorflow/blob/v2.2.3/tensorflow/python/__init__.py#L199"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.2.3/tensorflow/python/__init__.py#L199</a></p>
<p>The paper is from 2019, therefore I would suggest you to limit package versions before 2019 to avoid possible issues.</p>
","0","Answer"
"78290676","78289901","<p>You can access its weights normally through <code>tm.parameters()</code> just like any <code>nn.Module()</code></p>
","0","Answer"
"78292976","78289756","<p>I have found a fix to it, the following code after installing gymnasium worked for me:</p>
<pre><code>!apt-get install build-essential
!apt-get install swig
!apt-get install python-box2d
!pip install gym[box2d]
</code></pre>
","0","Answer"
"78292999","78288786","<p>Answer from the support: &quot;The browsers need to stay on in order for notebooks to keep running. If you close the browser or Desktop application, then notebook will stop running&quot;</p>
","1","Answer"
"78293470","78293078","<p>You should watch the video that explains the correct size to use.  The optimal size is the one where the smallest objects you want to find will be resized in such a way that it can easily be found by the network you are using.  You can find that here:  <a href=""https://youtu.be/m3Trxxt9RzE"" rel=""nofollow noreferrer"">https://youtu.be/m3Trxxt9RzE</a></p>
<p>There is also an entry for this topic in the YOLO FAQ:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#optimal_network_size"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#optimal_network_size</a></p>
<p>Why limit yourself to 640x640 or 1280x1280?  Images and video frames are very rarely square.  You should size your network so the aspect ratio matches your images or video frames:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#square_network"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#square_network</a></p>
<p>This becomes even more important if the objects you are trying to find have a certain &quot;shape&quot;.  Such as differentiating between ovals and circles, or squares and rectangles.</p>
","1","Answer"
"78293555","78293441","<p>You should reset <code>validation_accuracy</code> to <code>0</code> at the start of every epoch.</p>
","0","Answer"
"78295852","78285377","<p>You need to specify the <code>n_select=&lt;no_of_models_return&gt;</code> parameter in the <code>compare_models()</code> method by default <code>n_select</code> is set to 1 thus you get only 1 model.</p>
<p>Example code (I set <code>verbose=False</code> to hide some information)</p>
<pre class=""lang-py prettyprint-override""><code>from pycaret.datasets import get_data
from pycaret.classification import *

data = get_data('diabetes', verbose=False)
s = setup(data, target = 'Class variable', session_id = 123, verbose=False)
top3_best_models = compare_models(n_select=3, verbose=False) # Get top 3 best models

for model in top3_best_models:
    display(model)
</code></pre>
<p><a href=""https://i.sstatic.net/1c87F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1c87F.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"78296149","78266928","<p><strong>With Yolo8:</strong></p>
<p>For the box in format xyxy</p>
<pre><code>from ultralytics import YOLO


model =YOLO('best.pt')

results = model(image, show=True)

result = results[0]
res = result.boxes[0] 
cords = res.xyxy[0].tolist()
cords = [round(x) for x in cords]
class_id = result.names[res.cls[0].item()]
conf = round(res.conf[0].item(), 2)

print(&quot;Coordinates:&quot;, cords)
print(&quot;Probability:&quot;, conf)
print(&quot;Object type:&quot;, class_name)
</code></pre>
<p>I added the confidence and the class!</p>
<p>If you want to <strong>crop</strong> and <strong>resize</strong> the bounding box image :</p>
<pre><code>import cv2
import numpy as np

size=128

img_cropped = cv2.resize(np.array(result.orig_img[cords[1]:cords[3],cords[0]:cords[2]]), (128, 128), interpolation=cv2.INTER_AREA)
</code></pre>
","0","Answer"
"78299494","78297772","<p>If you don't care about anything but the time it takes for something to boot, then simply record it.  Then you can off-line process that recording using whatever object detection framework you want to use, or is convenient to use.</p>
","0","Answer"
"78299496","78299375","<p>As suggested, use a SmoothingFunction (take a look at <a href=""https://github.com/nltk/nltk/issues/1554"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/issues/1554</a>)</p>
<pre><code>prediction = &quot;I am ABC. I have completed my bachelor's degree in computer application at XYZ University and I am currently pursuing my master's degree in computer application through distance education.&quot;


reference = &quot;I'm ABC. I have finished my four-year certification in PC application at XYZ and I'm currently pursuing my graduate degree in PC application through distance training.&quot;

from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
# Tokenize the sentences
prediction_tokens = prediction.split()
reference_tokens = reference.split()
   
# Calculate BLEU score
bleu_score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=SmoothingFunction().method4)

# Print the BLEU score
print(f&quot;BLEU score: {bleu_score:.4f}&quot;)
</code></pre>
<p>BLEU score: 0.1379</p>
","0","Answer"
"78301711","78298357","<p>I followed the example in the link attached to the comment here: <a href=""https://stackoverflow.com/a/66595678"">https://stackoverflow.com/a/66595678</a></p>
<p>and the thing that made the difference for me was adding 'epoch_' before my metric names which I assume is because that mimics how they are typically represented in tensorboard (for the benefit of the hparams callback):</p>
<p>Edit: According to the same source, you may also need to add the <strong>group</strong> argument to specify &quot;train&quot; or &quot;validation&quot;</p>
<pre><code>metrics=[hp.Metric('epoch_accuracy', group='validation', display_name='Accuracy')]
</code></pre>
","0","Answer"
"78301934","78288542","<p><code>Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [78,2], [batch]: [96,2]</code></p>
<p>The reason for this error is not related to your model architecture, but rather due to your data pipeline as the elements of your input data do not all have the <strong>exact same shape</strong> (this is required if you're batching your dataset - when you're using <code>.fit(train_dataset, epochs=200 , batch_size=128)</code>, or calling <code>train_dataset.batch(N)</code> explicitly).</p>
<p>Possible Solution</p>
<ol>
<li>Consider padding your data -
see <a href=""https://www.tensorflow.org/guide/data#batching_tensors_with_padding"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data#batching_tensors_with_padding</a></li>
<li>Resize your data (if it make sense for your usecase - e.g: image data)</li>
<li>Use batch size of 1 (not recommended, because this would mean your SGD algorithm would have very high variance + training will be very slow as it doesn't fully utilise parallelization)</li>
</ol>
","0","Answer"
"78302644","78302444","<p><strong>Yes</strong>, for two reasons:</p>
<ol>
<li><strong>Tokenization</strong>: &quot;I am&quot; vs. &quot;I'm&quot; lead to <strong>different tokenizations</strong>. BLEU scores are <strong>very sensitive</strong> to <strong>tokens</strong>. The different tokenization should lead to substantial impact.</li>
<li><strong>Short sentences</strong>: BLEU are notoriously <strong>unreliable</strong> for very <strong>short texts</strong>. The score will be disproportionately influenced by small differences if there isn't much text to begin with because there is a Brevity Penalty set in place to discourage the model from outputting fewer words and get a high score. Please take a look at the &quot;Brevity Penalty&quot; section in the following <a href=""https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b"" rel=""nofollow noreferrer"">article</a>.</li>
</ol>
<p>Hope you found the answer you are looking for 🤓.</p>
","0","Answer"
"78303929","78275777","<p>The <a href=""https://stackoverflow.com/a/78275819/15019223"">answer from David Demmers</a> is wrong, as xdurch0 pointed out in the comment. This answer ist just for the sake of having a full answer with the solution in one place. Credits go to xdurch0.</p>
<p>You are correct that <code>from_logits=True</code> woul be wrong to use if the output layer for the loss function would use a <code>sigmoid</code> activation itself. But in the <code>G_loss</code> function, the loss is calculted on the output of the <code>Discriminator</code>:</p>
<pre>
def G_loss(D, x_fake):
  return cross_entropy(tf.ones_like(D(x_fake)), <b>D(x_fake)</b>)
</pre>
<p>Mind the bold text. The output of the Generator is indeed a probability, but the output of the Discriminator is not, because it uses no activation at the last layer. And for the loss of both the GEnerator and the Discriminator, the output of the Discriminator is used.</p>
","0","Answer"
"78304082","78303940","<p>I recommend you looking at the <a href=""https://github.com/deeplearning4j/deeplearning4j-examples/blob/master/data-pipeline-examples"" rel=""nofollow noreferrer"">data pipeline examples</a> on the <a href=""https://github.com/deeplearning4j/deeplearning4j-examples"" rel=""nofollow noreferrer"">deeplearning4j examples</a> page, especially <a href=""https://github.com/deeplearning4j/deeplearning4j-examples/blob/master/data-pipeline-examples/src/main/java/org/deeplearning4j/datapipelineexamples/transform/basic/CSVMixedDataTypesLocal.java"" rel=""nofollow noreferrer"">CSVMixedDataTypesLocal.java</a> in your case.</p>
","0","Answer"
"78305825","78298624","<p>Distance calculation functions moved to modules/verification in the latest version of deepface. You can now consume distance functions as:</p>
<pre class=""lang-py prettyprint-override""><code>from deepface.modules import verification
distance = verification.find_euclidean_distance(x, y)
</code></pre>
","1","Answer"
"78307157","78306767","<p>We can better help you if you share a minimal reproducible example. Anyway, this code could be useful:</p>
<pre><code>library(dplyr)
data.frame(y, y_hat = myrf$predicted) %&gt;% 
  mutate(id = row_number()) %&gt;% 
  filter(y != y_hat)
</code></pre>
<p>where <code>y</code> is your observed response vector, <code>y_hat</code> is the predicted value based on your estimation, <code>id</code> inside <code>mutate</code> is a new column indicating the position of the i-th observation and <code>y != y_hat</code> <code>filter</code>s only observations misclassified. You can use <code>id</code> to locate those values.</p>
","0","Answer"
"78308332","78289901","<p>One potential solution I found later on using .children():</p>
<pre><code>for layer in tm.children():
    print(layer.state_dict())
</code></pre>
","0","Answer"
"78311283","78310990","<p>Is there any particular reason you want one big model for multiple outputs? If you just have multiple outputs you want to predict, why not just build a separate model for each output. This also maximises the use of your data if you have any NaN values.</p>
","-1","Answer"
"78313102","78285447","<p>While there are many slightly-different ways to think about it, it may help to consider the values in the (trained, frozen) neural network as associated with edges moreso than vertexes (nodes).</p>
<p>The network &quot;projection weights&quot; leading from an (abstract) one-hot encoding of each known word into the network are essentially the actual per-word word-vectors. Assuming a common case of 300d vectors, the 300 edges from the single-word node to the inner nodes are that words.</p>
<p>Then, there's another set of edge-weights from the internal activation to the &quot;output&quot; layer, which is offering the network's training goal of in-context word-projection.</p>
<p>In the (more common &amp; usual default) negative-sampling approach, each output node corresponds to a single predictable word. That's a very easy output-shape to visualize. And the value of negative-sampling is that you only check the activations of the desired word, and <code>n</code> more randomly chosen negative words, to perform your training updates. That's way less calculation than if you checked the output values at all <code>V</code> (size of vocabulary) output nodes, and still works pretty well, and doesn't get more expensive with larger vocabularies (unless you choose for other reasons to also increase your choice of <code>n</code> negative samples).</p>
<p>In hierarchical softmax, the interpretation of the output nodes is more complicated. Rather than the activation at a single node indicating the prediction of a single word, a (varying) set of nodes must have the right on/off activations to communicate the variable-length huffman-code of a word.</p>
<p>So in HS, to backprop the network more towards predicting your desired &quot;positive&quot; word (from one input context), the algorithm considers just those nodes involved in that words nique coding – a smaller set of nodes for the most-ommon words, but a larger set of nodes for rarer words – and nudges each of them more towards the pattern that predicts the desired word. Again, you get the sparse training efficiency of updating only a tiny subset, far smaller than all <code>V</code> nodes, each training-step. But, the cost will vary based on the target word, and grow with the log of <code>V</code> as vocabulary-size grows. Further, as the original/naive assignment of codes is based strictly on word-frequency, quite-dissimilar words may have very-similar codings, perhaps causing more word-to-word interference. (There were hints in the original <code>word2vec.c</code> release of refining the HS word-codings over time to ensure similar words share similar Huffman codings, but I've seen litle followup on that idea, perhaps because of the dominance of negative-sampling.)</p>
<p>So, in an HS network, the weights from the inner-activations, to the output nodes, are tuned to indicate, by Huffman code, which word is the preferred prediction from a context.</p>
<p>In the word2vec implementation I'm most familiar with, Python Gensim, these &quot;hidden to output&quot; weights are not even randomly initialized at the beginning, instead left as 0.0 – and I think this was directly copied from the initialization of Google's <code>word2vec.c</code> release. But, as soon as training begins, the explict random initialization of those &quot;input weights&quot; (initial random input word-vectors) means those weights are immediately perturbed in a way that at 1st is nearly all random but becomes more helpful over the SGD training.</p>
<p>So:</p>
<ul>
<li>those inner weights start 0.0 but quickly start reflecting the influence of the (initially-random) word vectors and training examples</li>
<li>they're usually <em>not</em> harvested from the network after training, like the final word-vectors are – but would be kept around if you wanted to continue training later, and perhaps provide a running start to future training runs</li>
</ul>
<p>(In the negative-sampling case, some research suggested those hidden-to-output weights could also be interpreted as same <code>embedding_size</code> per-word vectors, with some usefulness: see paper by Mitra et al at Microsoft about &quot;Dual Word Embeddings&quot;. But given the varying-length codings of output words in the HS case, extracting/interpretating those output-weights would be trickier.)</p>
<p>In the implementation I'm most familiar with, the It may help to think instead of the shallow neural network's &quot;projection layer&quot; (effectively the word-vectors themselves, as each virtual &quot;single node&quot; 1-hot word has its <code>embedding_size</code> out-weights) and &quot;hidden layer</p>
","2","Answer"
"78319547","78308446","<p>Further investigation reveals that <code>numpy.reshape()</code> operates in row-major (C-style) order by default, which means it fills the new array along the last axis first (i.e., from left to right, top to bottom)</p>
<p>So if I reshape this first then transpose:</p>
<pre><code>reshaped = t_new.reshape((1440, 721, 6))  # Reshape to (1440, 721, 4)
correct_order = reshaped.transpose((2, 1, 0))  # Swap axes to get (4, 721, 1440)
</code></pre>
<p>It seems to produce the desired output. I test this by looking at the slices to see that longitude/latitude are constant along each slice:</p>
<pre><code>correct_order[:,:,1]
correct_order[:,1,:]
</code></pre>
<p>Thanks to comments from hpaulj for hint to transpose this.</p>
","1","Answer"
"78321631","78320892","<p>The dummy estimators in <code>sklearn</code> are not intended for real problems (they are used to obtain baseline measures of performance using very simple rules). In your case, the dummy estimator is configured to <em>always</em> output &quot;C&quot; regardless of the input.</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"" rel=""nofollow noreferrer""><code>RandomForestClassifier</code></a> is usually a good 'off-the-shelf' estimator. I'd suggest viewing the train score after you do the training in order to verify that the model is learning something. Then you can assess its performance on data it hasn't seen (a validation set).</p>
<p>For the purposes of getting an accuracy score, you could use <code>my_classifier.score(X_data, y_data)</code>.</p>
","1","Answer"
"78323572","78322346","<p>Using <a href=""https://rsample.tidymodels.org"" rel=""nofollow noreferrer"">rsample from tidymodels</a> to group and stratify:</p>
<pre class=""lang-r prettyprint-override""><code>data &lt;- data.frame(
  IDs = c(
    001, 001, 001,
    002, 002, 002, 002,
    003, 003, 003, 003,
    004, 004, 004, 004, 004, 004,
    005, 005, 005, 005, 005,
    006, 006, 006, 006,
    007, 007, 007,
    008, 008,
    009, 009,
    010, 010, 010
  ),
  var1 = c(
    0102, 0210, 0405,
    0318, 0629, 1201, 0101,
    0923, 0702, 0710, 0801,
    0203, 0501, 1204, 0516, 0112, 1005,
    1101, 1125, 1020, 0112, 0310,
    0203, 0401, 0607, 0811,
    1010, 1212, 0707,
    0430, 0428,
    1030, 1008,
    0501, 0511, 0601
  ),
  var2 = c(
    &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;,
    &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;,
    &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;,
    &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;, &quot;warm&quot;,
    &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;,
    &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;,
    &quot;hot&quot;, &quot;hot&quot;, &quot;hot&quot;,
    &quot;warm&quot;, &quot;warm&quot;,
    &quot;hot&quot;, &quot;hot&quot;,
    &quot;cold&quot;, &quot;cold&quot;, &quot;cold&quot;
  )
)

library(rsample)

# Keep IDs together and stratify var2
data_split &lt;- data |&gt; group_initial_split(group = IDs, strata = var2)
train &lt;- training(data_split)
test &lt;- testing(data_split)

# Group v-fold cross-validation
train |&gt; group_vfold_cv(group = IDs, strata = var2, v = 2)
#&gt; # Group 2-fold cross-validation 
#&gt; # A tibble: 2 × 2
#&gt;   splits          id       
#&gt;   &lt;list&gt;          &lt;chr&gt;    
#&gt; 1 &lt;split [13/15]&gt; Resample1
#&gt; 2 &lt;split [15/13]&gt; Resample2
</code></pre>
<p><sup>Created on 2024-04-14 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.1.0</a></sup></p>
","0","Answer"
"78324733","78324647","<p>You were almost there:</p>
<pre><code>from sklearn.feature_extraction import FeatureHasher
import pandas as pd

data = {&quot;Item_Identifier&quot;: [&quot;ID1&quot;, &quot;ID2&quot;, &quot;ID3&quot;, &quot;ID4&quot;, &quot;ID5&quot;]}
X_train = pd.DataFrame(data)

hash_vector_size = 50
fh = FeatureHasher(n_features=hash_vector_size, input_type='string')
hashed_features = fh.transform([[item] for item in X_train[&quot;Item_Identifier&quot;]])

hashed_df = pd.DataFrame(hashed_features.toarray(),
                         columns=['H'+str(i) for i in range(hash_vector_size)])

print(hashed_df)
</code></pre>
<p>which gives</p>
<pre><code>H0   H1   H2   H3   H4   H5   H6   H7   H8   H9  ...  H40  H41  H42  H43  \
0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  ...  0.0  0.0  0.0  0.0   
1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   

   H44  H45  H46  H47  H48  H49  
0  0.0  0.0  0.0  0.0  0.0  0.0  
1  0.0  0.0  0.0  0.0  0.0  0.0  
2  0.0  0.0  0.0  0.0  0.0  0.0  
3  0.0  0.0  0.0  0.0  0.0  0.0  
4  0.0  0.0  0.0  0.0  0.0  0.0  

[5 rows x 50 columns]
</code></pre>
","1","Answer"
"78325401","78306767","<p>This is the line that ended up giving me a list of each samples classification within the RF model.</p>
<pre><code>rfmetrics$predicted 
</code></pre>
<p>Thanks for your help!</p>
","0","Answer"
"78325665","78284197","<p>I would recommend using PEFT-LORA method to prevent Catastrophic Forgetting for LLMs (to avoid crippling the model). You can use <code>SFTtrainer</code> or the <code>CustomTrainer</code> from the medium post for training an adapter that performs your task. And use <code>adapter.load_adapter(checkpointpath)</code> for your <code>PeftModel</code> for specific task. You can now train and switch between multiple adapters too for inference so you can train it for multiple domain specific tasks.</p>
<ol>
<li><p>Train a model using <code>AutoModelForSequenceClassification</code> + <code>Lora</code>. Select a model that does well.</p>
</li>
<li><p>You can re-load adapter from the saved checkpoint. set <code>is_trainable=True</code> and perform the retraining for text-completion task using <code>AutoModelForSequenceClassification</code> for text completion.</p>
</li>
</ol>
<p>What you have is Class imbalance in your dataset.~20k good and bad data should be enough to perform fine-tunin IMO. But I have too little context on you task so I cannot be confident on the data-size.</p>
<p>Using Peft adapters will help you switch between the base model and your peft adapters using something like s-LORA so you can use the base model for other forms of text completions using the base Mistral Model and switch to Peft model for your domain specific task.</p>
<p>Hope that helps. If you can share more details I am open to help further.</p>
","1","Answer"
"78327628","78327535","<p>I've tested and I solved problem by updating scikit-learn.</p>
<pre><code>pip install -U scikit-learn
</code></pre>
","2","Answer"
"78328042","78274833","<p>There are multiple ways to handle this scenario:</p>
<ol>
<li>Create data assets in Azure ML Studio from these six files.</li>
</ol>
<p><img src=""https://i.imgur.com/SNp2w00.png"" alt=""enter image description here"" /></p>
<p>Here, you can create a single data asset from the 6 files and use it in a component like below.</p>
<p><img src=""https://i.imgur.com/eIfGOsT.png"" alt=""enter image description here"" /></p>
<ol start=""2"">
<li>If you need to clean these files separately using different logic, create 6 separate data assets and access them in an R script like below. Then, clean and register them as data assets in Azure ML.</li>
</ol>
<pre class=""lang-r prettyprint-override""><code>azureml_main &lt;- function(dataframe1, dataframe2){
  print(&quot;R script run.&quot;)
  run = get_current_run()
  ws = run$experiment$workspace
  dataset1 = azureml$core$dataset$Dataset$get_by_name(ws, &quot;output_dataset_name&quot;)
  dataset2 = azureml$core$dataset$Dataset$get_by_name(ws, &quot;sql&quot;)
  #load all your data asset
  #clean your data
  dataframe1 &lt;- dataset$to_pandas_dataframe()
  # Return datasets as a Named List
  return(list(dataset1=dataframe1, dataset2=dataframe2))
}
</code></pre>
<p><img src=""https://i.imgur.com/HbXS24V.png"" alt=""enter image description here"" /></p>
<ol start=""3"">
<li>You can also use the 3-argument or port <strong>Script Bundle</strong> where you create a data asset using a <code>.zip</code> file containing all your files. These files are extracted and available in runtime under <code>./Script Bundle</code>. Use the code below to read your files.</li>
</ol>
<pre class=""lang-r prettyprint-override""><code>azureml_main &lt;- function(dataframe1, dataframe2){
  print(&quot;R script run.&quot;)
  mydataset&lt;-read.csv(&quot;./Script Bundle/mydatafile.csv&quot;,encoding=&quot;UTF-8&quot;);  
  # clean and return
  return(list(dataset1=mydataset, dataset2=dataframe2))
}
</code></pre>
<p>To create a data asset using a <code>zip</code> file, follow <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/execute-r-script?view=azureml-api-2#add-an-r-script-as-an-input"" rel=""nofollow noreferrer"">these steps</a>.</p>
<p>Refer to this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/execute-r-script?view=azureml-api-2"" rel=""nofollow noreferrer"">documentation</a> for more information about the R component and its usage.</p>
<p>You can also refer this github <a href=""https://github.com/Azure/azureml-sdk-for-r/blob/master/R/datasets.R"" rel=""nofollow noreferrer"">documentation</a> on how to load csv files as tabular data asset, cleaning them  and register it as data asset with out need of export data component.</p>
","0","Answer"
"78328631","78328539","<p>Every model with &quot;token-classification&quot; tag should be usable with pipeline : <a href=""https://huggingface.co/models?pipeline_tag=token-classification"" rel=""nofollow noreferrer"">https://huggingface.co/models?pipeline_tag=token-classification</a></p>
","0","Answer"
"78332133","78332079","<p>Your NLTK_Sentiment column is based on the sentiment of the Clean_Description column. The X column is also based off of the Clean_Description column.</p>
<p>You are essentially testing if there is a linear relationship between the count of the number of occurrences of each token, and the VADER categorization. Since VADER works by assigning each word a score between -4 and 4, and summing them up, this is a linear relationship. (There are some exceptions to this - VADER is capable of recognizing some idioms like 'bad ass,' or negations like 'not good,' but outside of those special cases, it's linear.)</p>
<p>For that reason, logistic regression is essentially just recovering the word-level weights in VADER. You're giving it a problem which is easy, and that's why you get such a high score.</p>
","2","Answer"
"78339303","78254344","<p>The exception was rised for is_accelerate_available() and is_bitsandbytes_available()</p>
<p>So,
I also had the same issue, uninstalling and reinstalling conda-forge and accelerate helped</p>
<pre><code>conda install -c conda-forge accelerate
</code></pre>
","1","Answer"
"78341222","78340023","<p>So, I've found the answer. The method that I'm looking for resides in <code>BinaryLogisticRegressionTrainingSummary</code> and not <code>LogisticRegressionTrainingSummary</code>. The former is the summary of a <strong>binomial logistic regression model</strong> (mine is multinomial). For the latter, the only way to get f-measure is by label using <code>fMeasureByLabel</code>.</p>
","0","Answer"
"78342075","78328211","<p>The example below shows forecasting and plotting for synthetic quarterly data. Is this the type of output you were after?</p>
<p><a href=""https://i.sstatic.net/xtD5d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xtD5d.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>trn window: 2000-03-31 to 2009-12-31 [ 40 samples] | abs % err:   47.23 | target: -0.493, forecast: -0.260
trn window: 2000-03-31 to 2010-03-31 [ 41 samples] | abs % err:    8.14 | target: -0.624, forecast: -0.573
trn window: 2000-03-31 to 2010-06-30 [ 42 samples] | abs % err:   51.47 | target: -0.515, forecast: -0.780
trn window: 2000-03-31 to 2010-09-30 [ 43 samples] | abs % err:   21.35 | target: -0.892, forecast: -0.702
trn window: 2000-03-31 to 2010-12-31 [ 44 samples] | abs % err:   28.81 | target: -0.717, forecast: -0.924
trn window: 2000-03-31 to 2011-03-31 [ 45 samples] | abs % err:   18.18 | target: -0.629, forecast: -0.743
...
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from matplotlib import dates as mdates
from matplotlib import pyplot as plt
import statsmodels.tsa.api as tsa

#
# Synthetic quarterly data
#
dates = pd.date_range('1/1/2000', '1/1/2025', freq='QE')
t = np.linspace(0, 2*np.pi * 4, len(dates))
sine = np.sin(t) * np.exp(-0.03 * t)
data = sine + np.random.normal(0, 0.1, dates.size)

df = pd.DataFrame({'signal': data}, index=dates) #index is the date

#
# Define SARIMA model
#

#Training start date
train_start = df.index[0] #&quot;31/03/2000&quot;
min_train_size = 40

#Forecasting will start after 40 samples, until end of the data
forecast_start = train_start + min_train_size * df.index.freq
forecast_end = df.index[-1] #&quot;31/12/2014&quot;

forecasts = []
for train_end in pd.date_range(forecast_start, forecast_end, freq=df.index.freq).shift(-1):
    #the &quot;shift(-1) is so we stop one sample before the forecast date
    train_data = df[train_start:train_end]
    
    #Fit the model and get the results object
    results = tsa.ARIMA(
        train_data,
        order=(3, 1, 0),
        seasonal_order=(1, 0, 0, 20)
    ).fit()
    
    #Use results object to forecast the next sample
    forecast = results.forecast()
    forecasts.append(forecast)

    #
    #Report results for each forecast
    #
    forecast_date = forecast.index.item().date()
    assert forecast_date == (train_end + df.index.freq).date()
    
    actual = df.loc[[forecast_date]].values.item()
    
    print(
        f'trn window: {train_start.date()} to {train_end.date()}',
        f'[{train_data.size:&gt;3d} samples] |',
        f'abs % err: {abs(1 - forecast.item() / actual) * 100:&gt;7.2f} |',
        f'target: {actual:&gt;+5.3f},',
        f'forecast: {forecast.item():&gt;+5.3f}',
    )

forecasts_df = pd.concat(forecasts, axis=0)

#
# Plot data and forecasts
#

#Plot the signal
ax = df.plot(
    y='signal', use_index=True, xlabel='date', ylabel='signal',
    marker='.', color='dodgerblue', linewidth=1, figsize=(8, 2)
)

#Plot the forecasts
forecasts_df.plot(
    use_index=True, label='forecast', color='crimson', ax=ax
)

#Add some visual lines marking various dates
[ax.axvline(date, ymax=0.05, linewidth=5, color='tab:green')
 for date in [train_start, forecast_start, forecast_end]
 ]

ax.legend(ncols=2, fontsize=8)
</code></pre>
","0","Answer"
"78343678","78337383","<p>It sounds like you've started by condensing the input size from 2000 down to 100, i.e. dimensionality reduction. Usually this affords benefits in terms of speed and memory, and <em>might</em> also improve performance.</p>
<p>However, before trying dimensionality reduction, I think it's worth seeing how the original features perform. You can feed your data straight into a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html"" rel=""nofollow noreferrer""><code>ExtraTreesRegressor</code></a>, <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"" rel=""nofollow noreferrer""><code>RandomForestRegresssor</code></a>, or <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html"" rel=""nofollow noreferrer""><code>Hist</code></a>/<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"" rel=""nofollow noreferrer""><code>GradientBoostingRegressor</code></a> and see how it does. No need to preprocess features for the aforementioned regressors.</p>
<blockquote>
<p>My idea was to split all books into 100 parts, take the average of these 100 parts for every book, and train a Support Vector Regression model (with poly kernel) on this data.</p>
</blockquote>
<p>Looking at graphs you've shown, I think this approach to dimensionality reduction is likely diluting some of the key discriminative features of the data. Comparing the red and blue classes, what they have in common is that they're somewhat periodic (cycling every ~250 steps); they trend in the same direction; and they swing up at the end (to different degrees). The blue graph is offset by about half a cycle.</p>
<p>When downsampling this data to 100 points, the periodic nature will be attenuated, and you'd be left with only the general trend, which is similar for both red and blue. Also, the upswing lasts only for a relatively short period, so averaging it could make the red and blue tails less distinct. In short, the new features seem likely to draw the classes closer together in feature space, rather than helping to separate them.</p>
<p>Some alternative ways of dimensionality reduction using <code>sklearn</code>:</p>
<ul>
<li><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html"" rel=""nofollow noreferrer""><code>PLSRegression</code></a> reduces the dimensionality of your data whilst taking the target into account.</li>
<li><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow noreferrer""><code>PCA</code></a> reduces the dimensionality of the data by discarding features that don't vary much. However, it doesn't look at the target, so it could miss useful information.</li>
</ul>
<blockquote>
<p>Is my regression interpreting my 100 number vector as 100 unrelated variables? Does that matter?</p>
</blockquote>
<p>They work collectively to define the decision boundary. The model doesn't take their ordering into account though, meaning that you can train the model on shuffled data and it will give you the same results.</p>
<p><em>Processing sequences</em></p>
<p>The data you have is sequential in nature, but the methods above ignore this. If the sequential characteristic is important, then it might be worth considering PyTorch or similar for training a sequence-based model. In your case, you'd set up a sequence-to-vector architecture, where the input is the sequence of sentiments and the output is a single score for the entire sequence.</p>
<p>There are different ways of processing sequences using neural nets. Given how long your sequences are (2000 in their original form), the first stage of the model could be a convolutional block that condenses the 2000 samples down to a smaller size. This intermediate result could either feed into another convolutional block that does the rest, or alternatively into a recurrent model. At the end, there will be a layer that maps things to a final predicted score.</p>
","0","Answer"
"78344010","78343980","<p>You need to remove the <code>pd.get_dummies</code> function call. Just directly assign</p>
<pre><code>x = df.drop(['Last Result'], axis=1)
</code></pre>
<p>Calling pd.get_dummies converts the samples into 0/1 data: <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html</a>.</p>
<p>Can you give more information about the nature of your dataset? This would help in understanding what you are trying to do.</p>
","0","Answer"
"78344578","78340927","<p>Apparently I found the solution for Tensorflow &lt;= 2.15.0 to use tke Keras-rl2, which is to add this code on your base code</p>
<pre><code>import tensorflow as tf
tf.keras.__version__ = __version__
</code></pre>
<p>so that it will not error if using keras-rl2, and for the newer tensorflow which is now currently at 2.16.0 I still dont know how to solve it</p>
","0","Answer"
"78345122","78291576","<p>replace &quot;//tensorflow/tools/pip_package:wheel&quot; to &quot;//tensorflow/tools/pip_package:build_pip_package&quot; and try again</p>
","3","Answer"
"78345443","78343327","<p>Because there doesn't have such method definition:</p>
<p><a href=""https://github.com/dotnet/machinelearning/blob/main/src/Microsoft.ML.Data/MLContext.cs"" rel=""nofollow noreferrer"">Microsoft.ML.MLContext Class</a></p>
","0","Answer"
"78348760","78345096","<p>The Oracle Machine Learning (OML) in-database algorithms check the data_length value in all_tab_columns, and if the data_length is &gt; 4K, the column is treated as text instead of varchar. To check the data length for a column:</p>
<pre><code>SQL&gt; SELECT column_name, data_type, data_length FROM all_tab_columns WHERE table_name = 'SALES_TRANS_CUST';
</code></pre>
<p>Check the maximum length for the PRODUCT_NAME column. If it is less than 4K, changing the PRODUCT_NAME data type to varchar(x), where x &lt; 4K will resolve the problem.</p>
","0","Answer"
"78349514","78325995","<blockquote>
<p>[...] in RBF kernal SVM I only get 0 and 1. Is there anyway i can get score of a detection?</p>
</blockquote>
<p>One way of getting a score is to use the <code>.decision_function(X)</code> method of the SVC. The larger it is, the further away the point is from the decision plane (the sign indicates the class). However, it is an unbounded value that can be arbitrarily large.</p>
<p>A second option is to set <code>SVC(probability=True, ...)</code>, which gives you access to a probability score (by default <code>probability</code> is <code>False</code>). It will take longer to run <code>.fit()</code>. You can then get the probabilities using the <code>.predict_proba(X)</code> method.</p>
<blockquote>
<p>How can i see coefficients of RBF kernal like linear SVM?</p>
</blockquote>
<p>You can't do this directly with <code>SVC(kernel='rbf')</code>. However, you can approximate the RBF feature space using <code>kernel_approximation.RBFSampler</code> or <code>kernel_approximation.Nystroem</code>, and then pipeline that into a <code>LinearSVC</code> (or any of <code>SGDClassifer(loss='hinge')</code>; <code>SVC(kernel='linear')</code>). That would allow you to access the coefficients for the approximated RBF space, as shown below.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.kernel_approximation import Nystroem, RBFSampler
from sklearn.svm import LinearSVC

from sklearn.pipeline import make_pipeline

#np.random.seed(0) #makes example reproducible

rbf_svc_approx = make_pipeline(
    Nystroem(kernel='rbf', n_components=100, n_jobs=-1),
    LinearSVC(),
)

#Access coefficients corresponding to the 100-dimensional RBF feature space
rbf_coeffs = rbf_svc_approx['linearsvc'].coef_
</code></pre>
<p>You might find some details <a href=""https://stackoverflow.com/a/78137312/21896093"">here</a> of interest.</p>
","0","Answer"
"78350237","78334661","<p>It seems like the issue might be related to how you're aggregating the predictions from individual trees. Instead of summing up the logits directly, you should consider averaging them. Since you're dealing with binary classification, averaging the logits makes more sense. The below code might help you:</p>
<pre><code>import numpy as np
import xgboost as xgb
from sklearn import datasets
from scipy.special import expit as sigmoid, logit as inverse_sigmoid

# Load data
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# Fit a model
model = xgb.XGBClassifier(
    n_estimators=10,
    max_depth=10,
    use_label_encoder=False,
    objective='binary:logistic'
)
model.fit(X, y)
booster_ = model.get_booster()

# Extract individual predictions
individual_preds = []
for tree_ in booster_:
    individual_preds.append(
        tree_.predict(xgb.DMatrix(X))
    )
individual_preds = np.vstack(individual_preds)

# Aggregated individual predictions to final predictions
individual_logits = inverse_sigmoid(individual_preds)
final_logits = np.mean(individual_logits, axis=0)  # Use mean instead of sum
final_preds = sigmoid(final_logits)

# Verify correctness
xgb_preds = booster_.predict(xgb.DMatrix(X))
np.testing.assert_almost_equal(final_preds, xgb_preds)
</code></pre>
<p>By taking the mean of the logits, you'll ensure that the predictions are more aligned with the model's predict function.</p>
","-1","Answer"
"78350337","78347872","<p>Consider the following group_count function:</p>
<pre><code>%python

def group_count(dat):
    import pandas as pd
    return pd.DataFrame([(dat[&quot;col1&quot;][0], dat.shape[0])],\
                        columns = [&quot;col1&quot;, &quot;COUNT&quot;])
</code></pre>
<p>To group by two grouping categories, specify both columns in the index parameter:</p>
<pre><code>index = DF[:,['col2', 'col3']] 
</code></pre>
<p>where DF is your oml proxy object and col2 and col3 are your grouping variables.</p>
<pre><code>res = oml.group_apply(oml_iris, index, 
                      func=group_count,
                      oml_input_type=&quot;pandas.DataFrame&quot;)
</code></pre>
","0","Answer"
"78350463","78348747","<p>OML4Py does not overload the apply method. The equivalent code can be achieved using the + and concat method.</p>
<pre><code>%python

new_col = oml_df['VAL_1'] + oml_df['VAL_2'] + 100
oml_df = oml_df.concat({&quot;VAL_NEW&quot;: new_col})
oml_df

 VAL_1  VAL_2  VAL_NEW
0     12      0      112
1     10      1      111
2     11     10      121
</code></pre>
","0","Answer"
"78350506","78343238","<p>You need to store the ordinal encoder from training and use it to transform the incoming data from streamlit, rather than creating a new encoder (which only ever sees one value, and so only ever assigns it value 0, regardless of what that value was).</p>
","0","Answer"
"78353336","78353335","<p>Use <code>https://some.url/</code> instead of <code>https://some.url</code> I don't know why but it works.</p>
","0","Answer"
"78355616","78349854","<p>You could try training the model to predict on a longer time-horizon, such the next 12 hours rather than just the next hour. This gives the model more context during learning. It's akin to changing the model into a multi-task or multi-forecast arrangement, rather than having it learn from just the next hour's forecast. It might also impart an averaging or stabilising effect on shorter-term trends, though I think it's worth trying. You could also modify the loss such that the model is penalised more for short-term errors, which will put more focus on the next hour's forecast.</p>
<p>The model's new output shape will be <code>(batch size, forecast_length)</code>. At inference, even though the model will be outputting a prediction for the next 12 hours, you'd only read off the first value. Changes to the code would include something like this:</p>
<pre class=""lang-py prettyprint-override""><code> ...
 #for each input sequence, the target is a &quot;forecast_length&quot; vector
 forecast_length = 14
 y.append(target[i + time_steps:i + timesteps + forecast_length])
</code></pre>
<p>At the moment the model is ingesting a sequence, and only rendering its prediction after stepping through all 100 frames (sequence-to-vector architecture). There can often be an improvement in changing to a sequence-to-sequence architecture, where you have a target forecast for each step in the input the sequence, rather than just a single target for the entire sequence. This can stabilise and speed up training as there are more error gradients and they don't have to flow through time as much. The input shape stays the same: <code>(batch_size, sequence_length, input_size)</code>. The target shape changes to <code>(batch_size, sequence_length, forecast_length)</code> - there is a target for each point in the sequence.</p>
<p>For the current sequence size of 100 frames, it sounds like the model would consider about 4 days' worth of context when learning to forecast. There could be an improvement to be had by using a wider window. However, rather than simply increasing the sequence length (LSTMs might have a hard time), you can prepend a causal convolutional stage that learns to condense a wide receptive field down to a shorter sequence, ready for the subsequent LSTM stage.</p>
<p>You could also forego LSTMs altogether and go for a fully-convolutional approach, similar to the WaveNet architecture.</p>
<p>I think it's difficult to judge in advance what would work best with the data as it requires experimentation and tuning, which is why I've outlined a couple of strategies.</p>
","1","Answer"
"78356346","78344537","<p>It turns out that I was passing different sample_df to the workers by accident. If your reference Datasets don't match, the thing freezes.</p>
","0","Answer"
"78356702","78356002","<p>404 error means the resource (endpoint URL you hit) was was not found. So the error is unrelated to the credential. Please make sure the endpoint is reachable from the environment you are usin Postman.</p>
","0","Answer"
"78357868","78333191","<p>One thing that helps me a lot and you might missing, is a Pearson correlation check. This will give you a report (I prefer matrix) where you can realize if there's a correlation between some of your features. Heavily related features, will greatly infect your model and make it prone to overfitting. Related/Correlated features, should be removed.
Although I have a lot of experience with Python, in regards to Machine/Deep Learning I feel more comfortable with R. I'll try to give both versions.</p>
<p>The code is something that :</p>
<p>In R:</p>
<pre><code>#Pearson correlation
# Cherry-pick only numeric data and do a correlation matrix
numeric_data &lt;- data[, sapply(data, is.numeric)]
correlation_matrix &lt;- cor(numeric_data, method = &quot;pearson&quot;)
print(correlation_matrix)
</code></pre>
<p>And Python:</p>
<pre><code>import pandas as pd
import numpy as np

# Assuming 'data' is your DataFrame containing the dataset
# Cherry-pick only numeric data and do a correlation matrix
numeric_data = data.select_dtypes(include=[np.number])  # Selects only numeric columns

# Calculate the Pearson correlation matrix
correlation_matrix = numeric_data.corr(method='pearson')

# Print the correlation matrix
print(correlation_matrix)
</code></pre>
<p>Another pitfall might be feature normalization. Depending on your data you might have to scale any numeric features. An example is the following. Here I am doing min-max. The type of scaling usually depends on your data:</p>
<p>In R:</p>
<pre><code># Doing Min-Max scaling
scaled_features &lt;- as.data.frame(lapply(numerical_features, rescale))
</code></pre>
<p>The code in Python (using pandas):</p>
<pre><code>scaled_features = pd.DataFrame(scaler.fit_transform(numerical_features), columns=numerical_features.columns)
</code></pre>
<p>Could you please try those and give me feedback? In general, I believe that everything in ML/DL is about Feature Engineering and Preprocessing. Let the Data speak and guide you.</p>
","1","Answer"
"78360453","78356992","<p>If you are using the Resample filter (supervised or unsupervised) as part of a <a href=""https://weka.sourceforge.io/doc.dev/weka/classifiers/meta/FilteredClassifier.html"" rel=""nofollow noreferrer"">FilteredClassifier</a> meta-classifier setup, then it is safe to use.</p>
<p>If you are using it from the <strong>Preprocess panel</strong>, then you are generating duplicates in the overall dataset. When performing cross-validation on this augmented dataset, you will end up with some instances appearing in train and test splits. That could explain the improvements that you have seen.</p>
<p>An alternative to Resample is the <a href=""https://weka.sourceforge.io/packageMetaData/SMOTE/index.html"" rel=""nofollow noreferrer"">SMOTE</a> filter (separate package).</p>
","0","Answer"
"78362448","78361038","<p>The main problem is that your second approach computes the AUC of each fold's model on the entire data set:</p>
<pre><code>    viz = RocCurveDisplay.from_estimator(estimator, X, y, ax=plt.gca(), name=f'ROC fold {i+1}')
</code></pre>
<p>Since this includes the data folds that the model was trained on, the scores are optimistically biased.</p>
","0","Answer"
"78363636","78361630","<p>Solve this issue. Its because <code>gymnasium</code> update step, reset function spec about return value. <code>env.reset()</code> returns initial state *and additional info dictionary. I don't know why at this stage, but there must be some reason. <code>env.step()</code> has same issue that it returns <code>terminated: bool</code> value *and <code>truncated:bool</code> which indicate the episode is stopped with other reason like time over, failure or something.</p>
<p>After all this, I simply encounter dead kernel issue with no traceable message lol</p>
","0","Answer"
"78364650","78360476","<p>The issue comes from your variables names.
You are calling knn, which hasn't been declared/trained. Instead, you trained knn_model.</p>
<p>Update to the following :</p>
<pre><code>from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=3, metric='euclidean')
knn_model.fit(train_val_process, merge_labels.label)
y_pred_knn = knn_model.predict(test_data_process)
</code></pre>
<p>Then, you can proceed to test your model comparing y_pred_knn with the test labels and compute accuracy, recall...</p>
<p>To the best of my knowledge, your standardization method has a mistake. Again, you seems to be calling the wrong variables.
While doing :</p>
<pre><code>train_val_process = pd.concat([train_data, val_data])
</code></pre>
<p>You are using the non standardized data, so your scaler seems useless.
I would recommend following a simple tutorial about standardization to correct this mistake.</p>
","1","Answer"
"78366939","78366460","<blockquote>
<p>[...] it doesnt give good results at all</p>
</blockquote>
<p>Is that with the validation data, or the training data?</p>
<p>If the train loss doesn't go down, it might point to an issue in the learning pipeline. I haven't noticed any issues with the code you posted, so the problem might be elsewhere. I would first confirm that the model is able to learn by reducing the training set size to just a few samples, and ensuring that your train loss decreases (and that the train <em>score</em> is very high). This would demonstrate that the model is able to learn something.</p>
<p>After confirming the model can learn, if you give the model more samples its train loss will be a bit higher and the train score will go down a bit, but you should start to see that the validation score improves.</p>
<p>Some quick checks:</p>
<ul>
<li>Start with <code>Adam</code>; it's a good default optimiser</li>
<li>The loss should be suitable for regression, such as MSE or similar</li>
<li>The input data should be scaled</li>
<li>Might be worth having no dropout initially. You can add some dropout back in later depending on the extent to which the model overfits.</li>
</ul>
<p>Your input image dimensions are around 680 x 680? Try increasing the kernel size of the input convolutional layer in order to capture more spatial information (I think 5x5 and 7x7 are typical at the input).</p>
<blockquote>
<p>i have applied a log scale to the target prediction to reduce this effect to attempt to improve learning but not sure how much it helped.</p>
</blockquote>
<p>To help discern whether things have improved over the previous run, set the <code>np</code> and <code>torch</code> random seeds and print out the metrics of at least the final epoch. That gives you a reference value to compare between two runs. When running such tests, I wouldn't go for too many epochs, otherwise you can't iterate as fast towards a good solution.</p>
","1","Answer"
"78373256","78373040","<p>I think that you are on the right track.
Based on the number of parameters that you have, I would suggest going with Random Forest of XGB.</p>
<p>(Logistic Regression assumes from my understanding linear relationship between your variables, so you might need to dig into data visualization first. Decision Tree is prone to overfitting, so with no more information on your dataset, I wouldn't suggest this).</p>
<p>Keep in mind that RF and XGB are computationally expensive, and require tuning (depth, regularization...). However, by following some tutorials and applying principles, you might be able to produce an accurate model using any of them.
They would allow you to grasp complex relationship within the data. However, they are also a bit less interpretable than LR and DT.</p>
<p>You might also consider another well used algorithm, Support Vector Machines (SVM), but I have no idea on how to use it in R</p>
","-1","Answer"
"78374541","78358516","<p>On approach I can think of is to modify your labels <code>y</code>. So you set all labels which are <strong>not</strong> class B to some new class label, let's say <strong>G</strong>. Then you train a binary classifier to predict either <strong>B</strong> or <strong>G</strong>. You collect all samples predicted as class <strong>B</strong> and remove them from your set.</p>
<p>Then taking this new set, repeat the previous approach, except keeping all class <strong>D</strong> labels as they were, and setting all others to the new class <strong>G</strong>.</p>
<p>Some basic pseudocode:</p>
<pre><code># Split data into train/test
# Optional I guess, but I added it for completeness
X_train, y_train, X_test, y_test = sklearn.train_test_split(X, y)

# Relabel your data
current_label = 'B'
y_train = [0 if i == curent_label else 1 for i in y_train]
y_test = [0 if i == current_label else 1 for i in y_test]

# Create and fit your classifier
clf = sklearn.SVC()
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_train)

# Index of samples predicted as class G
class_g_pred = np.array([i if v != 0 for i, v in enumerate(y_pred)])

# Remove the B samples from your train set
X_train_new = X_train[class_g_pred]
y_train_new = y_train[class_g_pred]

X_train = X_train_new
y_trian = y_train_new

# Repeat from the top
# Modify your labels now keeping D labels and setting others to G
# Create a new classifier
</code></pre>
<p>You can also do all the steps in a loop, so you don't need to hard code each step (good if you want to add more classes/layers later on, and for running your test data as well).
Splitting your data into train and test, isn't something you need to repeat at each step, rather just once at the start.</p>
<p>Something else to consider, is that you are turning each classification task into an imbalanced classification task. If each class has 100 samples, your <strong>B</strong> vs <strong>G</strong> classifier will now have 100 samples in class <strong>B</strong> and 400 in <strong>G</strong>, which can be harder to fit a model too in some cases.</p>
","-1","Answer"
"78377331","78374435","<p>I think the fit is poor due to rounding errors. If you lower the number of samples to 10 and the degree to 9, you get a perfect fit.</p>
<p>But the poor quality of the fit indeed surprised me. It is coming from <a href=""https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html"" rel=""nofollow noreferrer""><code>np.linalg.lstsq</code></a> which is used under the hood by <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"" rel=""nofollow noreferrer""><code>LinearRegression</code></a>. You can see the differences of various implementations using the following code:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

N = 50
deg = 49

X = X = np.linspace(0, 2 * np.pi, N).reshape(-1, 1)
X = np.sort(X, axis=0)
y = np.sin(X) + np.random.randn(N, 1) * 0.2

poly_features = PolynomialFeatures(degree=deg, include_bias=True)
X_poly = poly_features.fit_transform(X)

beta_blue = np.linalg.solve(X_poly.transpose().dot(X_poly), X_poly.transpose().dot(y))
y_blue = X_poly.dot(beta_blue)

beta_green = np.linalg.lstsq(X_poly, y, rcond=0)[0]  # rcond=None, rcond=-1
y_green = X_poly.dot(beta_green)

beta_magenta = np.linalg.pinv(X_poly.transpose().dot(X_poly)).dot(X_poly.transpose().dot(y))
y_magenta = X_poly.dot(beta_magenta)

reg = LinearRegression()
reg.fit(X_poly, y)
y_vals = reg.predict(X_poly)

plt.scatter(X, y)
plt.plot(X, y_vals, color='r')
plt.plot(X, y_blue, color='b')
plt.plot(X, y_green, color='g')
plt.plot(X, y_magenta, color='m')
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/btjeP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/btjeP.png"" alt=""polynomial fittings with noise"" /></a></p>
<p>The red curve does not start at zero because <code>LinearRegression</code> fits an extra intercept. So <code>np.linalg.solve</code> seems to be much less sensitive to ill conditioning than the other methods, at least in this case. It provides a perfect fit if you remove the noise (set 0.2 to 0.0) while the other methods still fail.</p>
<p>Only <code>np.linalg.solve</code> works even after turning off the noise by using this line in the above code:</p>
<pre><code>y = np.sin(X) + np.random.randn(N, 1) * 0.2
</code></pre>
<p><a href=""https://i.sstatic.net/mTgow.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mTgow.png"" alt=""polynomial fittings without noise"" /></a></p>
","3","Answer"
"78378399","78378193","<p>Check if there are some columns that all of it is <code>nan</code> values, this could be a reason why you get <code>inf</code> when you replace them with <code>mean</code> or <code>median</code> (if all the values are <code>nan</code> there is no meaning in <code>mean</code> or <code>meadian</code>)</p>
<p>Also if you can provide more information, it would be useful to check what are the potential problems in the task you're doing.</p>
","0","Answer"
"78383614","78383583","<p>you can prompt your llm with something like respond</p>
<pre><code>Respond with JSON of this format, when given any request
{
'action': &quot;intent / action to perform like turn-on-fan&quot;,
'device': &quot;id of device / any identifier&quot;
}
Human:
Example turn on Kitchen Fan
Response:
{
'action': 'fan-on',
'device': 'kitchen-fan'
}
-----
Human:
&lt;Your Command&gt;
</code></pre>
<p>If the LLM is powerful enough it should be able to perform this task.</p>
<blockquote>
<p>Fine-tuning the LLM won't be required for simple tasks like this.</p>
</blockquote>
<p>Regarding the API, using Lang Chain, and some pydantic you can extract the request json and perform the device control api calls.</p>
<p>As for the resources:
<a href=""https://platform.openai.com/docs/guides/function-calling"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/function-calling</a></p>
","1","Answer"
"78383685","78383583","<p>The first answer that you can describe output format for powerful LLM in system prompt, is correct.
However there are specific models/APIs for that, called function calling.
You can use commercial ones like <a href=""https://docs.mistral.ai/capabilities/function_calling/"" rel=""nofollow noreferrer"">Mistral</a> or pretrained models like <a href=""https://github.com/nexusflowai/NexusRaven-V2?tab=readme-ov-file"" rel=""nofollow noreferrer"">NexusRaven</a>, which you can use locally, e.g. with LM Studio.</p>
","-1","Answer"
"78384765","78354052","<p>The issue has been sorted out !</p>
<p>here is the updated script that fixed the issue</p>
<pre><code>from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer
from transformers.utils import PaddingStrategy
from sentence_transformers import SentenceTransformer


class TorchScriptWrapper(torch.nn.Module):
    def __init__(self, model):
        super(TorchScriptWrapper, self).__init__()
        self.model = model

    def forward(self, inputs: dict):
        inputs['token_type_ids'] = torch.zeros_like(inputs['attention_mask'], dtype=torch.int)
        with torch.no_grad():
            outputs = self.model(inputs)
        return {&quot;sentence_embedding&quot;: outputs['sentence_embedding']}


def export_to_torchscript(model_name: str, is_sentence_transformer: bool, output_path: str, max_seq_length: int = 128):
    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name)
    if is_sentence_transformer:
        model = SentenceTransformer(model_name, device=&quot;cpu&quot;)
    else:
        model = AutoModel.from_pretrained(model_name)
    model.eval()

    # Define example text
    text = &quot;This is a test string&quot;

    # Create inputs
    inputs = tokenizer(text, padding=PaddingStrategy.MAX_LENGTH, return_tensors=&quot;pt&quot;, max_length=max_seq_length)

    # Instantiate the wrapper class
    model_wrapper = TorchScriptWrapper(model)

    # Unpack HF batch encoding into a regular dict
    new_inputs = {}
    new_inputs[&quot;input_ids&quot;] = inputs[&quot;input_ids&quot;]
    new_inputs[&quot;attention_mask&quot;] = inputs[&quot;attention_mask&quot;]
    #if inputs.get(&quot;token_type_ids&quot;, None) is not None:
    #    new_inputs[&quot;token_type_ids&quot;] = inputs[&quot;token_type_ids&quot;]

    # Trace the wrapper class
    traced_model = torch.jit.trace(model_wrapper, new_inputs, strict=False)

    # Save traced model to file
    traced_model.save(output_path)


if __name__ == &quot;__main__&quot;:
    # Load pre-trained model and tokenizer
    model_name = &quot;sentence-transformers/LaBSE&quot;
    export_to_torchscript(model_name, True, output_path=&quot;torchscript_labse.pt&quot;)
</code></pre>
","0","Answer"
"78386809","78379552","<p>I have replicated your inquiry. It appears that code is missing &quot;S&quot; it should be <code>ML.NGRAMS</code> instead of <code>ML.NGRAM</code></p>
<pre><code>CREATE OR REPLACE MODEL
`singular-hub-291814.movie_sentiment.mymodel3`
TRANSFORM(ML.NGRAMS(string_field_0,[1,2])OVER() as ngram )
OPTIONS
  ( model_type='LOGISTIC_REG',
    auto_class_weights=TRUE,
    data_split_method='RANDOM',
    DATA_SPLIT_EVAL_FRACTION = .10,
    input_label_cols=['review']
  ) AS

SELECT 
  string_field_0 , review from table;
</code></pre>
<p>The error was replaced by missing table name after adding S to the text function.(since I do not have any table atm)</p>
","0","Answer"
"78387273","78334661","<p>The reason is because it is a boosting mechanism. Note the code in the for loop below, we need to update the base_margin each time. And the final result is NOT simply sum of all trees, but the results from the last tree.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import xgboost as xgb
from sklearn import datasets
from scipy.special import expit as sigmoid,logit

# Load data
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# Set basic model parameters
base_score = 0.5
n_rounds = 10
# Fit a model
model = xgb.XGBClassifier(
    n_estimators=n_rounds,
    base_score=0.5,
    max_depth=10,
    use_label_encoder=False,
    objective='binary:logistic'
)
model.fit(X, y)
booster_ = model.get_booster()

# Extract indivudual predictions
scores = np.full((X.shape[0],), logit(base_score))

individual_logits = []
for tree_ in booster_:

    X_margin = xgb.DMatrix(X, base_margin=scores)
    # get raw leaf value for accumulation
    scores = tree_.predict(
        X_margin, output_margin=True
    )

    individual_logits.append(
        scores
    )
individual_logits = np.vstack(individual_logits)

individual_preds = sigmoid(individual_logits)
final_preds = individual_preds[n_rounds-1,]
# Verify correctness
xgb_preds = booster_.predict(xgb.DMatrix(X))

np.testing.assert_almost_equal(final_preds, xgb_preds)
</code></pre>
","0","Answer"
"78389263","78370489","<p>As mentionned, <a href=""https://rasbt.github.io/mlxtend/user_guide/evaluate/GroupTimeSeriesSplit/"" rel=""nofollow noreferrer"">mlxtend</a> allows to do so.</p>
<pre><code>import pandas as pd, numpy as np
import seaborn as sns, matplotlib.pyplot as plt

from sklearn.datasets import make_regression
from sklearn.dummy import DummyRegressor
from sklearn.metrics import mean_squared_error, make_scorer
from sklearn.model_selection import cross_val_score

from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits

X_test, y_test = [], []

start_year = 2010
end_year = 2020

for year in np.arange(start_year, end_year+1):
    X_year, y_year = make_regression(n_samples=5+(year-start_year), n_features=2, bias=0, noise=1, random_state=year)
    X_year = pd.DataFrame(X_year).rename(columns={0:'X1', 1:'X2'})
    X_year['year'] = year
    y_year = pd.Series(y_year)
    X_test.append(X_year)
    y_test.append(y_year)

X, y = pd.concat(X_test), pd.concat(y_test)

# modelisation
model = DummyRegressor(strategy=&quot;mean&quot;)
metric = mean_squared_error
cv_args = {&quot;test_size&quot;: 1, 'n_splits': len(np.unique(X['year'])) - 1, 'window_type': 'expanding'}
cv = GroupTimeSeriesSplit(**cv_args)

scores = cross_val_score(model, X, y, cv=cv, groups=X['year'], scoring=make_scorer(metric))

plot_splits(X, y, X['year'], **cv_args)
</code></pre>
<p><a href=""https://i.sstatic.net/9yb86cKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9yb86cKN.png"" alt=""enter image description here"" /></a></p>
<p>It allows to use sklearn cross_val_score. It still has some drawbacks (adding a dependency, the plot can be improved), but it answers my needs.</p>
<p>Note: I have raised both of the following issues.</p>
<ul>
<li><a href=""https://github.com/scikit-learn/scikit-learn/issues/28873"" rel=""nofollow noreferrer"">git issue with sklearn</a> to improve ts split</li>
<li><a href=""https://github.com/rasbt/mlxtend/issues/1094"" rel=""nofollow noreferrer"">git issue with mlxtend</a> to improve the plot</li>
</ul>
","0","Answer"
"78389377","78387055","<p>If you convert to <a href=""https://en.wikipedia.org/wiki/CIELAB_color_space"" rel=""nofollow noreferrer"">Lab</a> colourspace, separate the 3 channels, contrast-stretch them and lay them out side-by-side across the page with <code>L</code> on the left, <code>a</code> in the centre and <code>b</code> on the right using <strong>ImageMagick</strong>:</p>
<pre><code>magick image.png -alpha off -colorspace lab -separate -normalize +append lab.png
</code></pre>
<p><a href=""https://i.sstatic.net/JOVbq72C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JOVbq72C.png"" alt=""enter image description here"" /></a></p>
<p>You can see in the centre (<code>a</code>) channel that your plastics are well differentiated from the crack at the left of the image. That suggests an OTSU threshold of the <code>a</code> channel will be a good discriminant for what you seek.</p>
<p>With OpenCV, that might look like this:</p>
<pre><code>import cv2 as cv

# Load image
im = cv.imread('image.png')

# Convert to Lab colourspace and take &quot;a&quot; channel
Lab = cv.cvtColor(im,cv.COLOR_BGR2LAB)
a = Lab[..., 1]

# discriminate plastics using &quot;a&quot; channel
plastics = cv.threshold(a, 0, 255, cv.THRESH_OTSU+cv.THRESH_BINARY)[1]
</code></pre>
<h2>First image</h2>
<p><a href=""https://i.sstatic.net/3KtMH2Tl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3KtMH2Tl.png"" alt=""enter image description here"" /></a></p>
<h2>Second image</h2>
<p><a href=""https://i.sstatic.net/bZ8x7eMU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZ8x7eMU.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>The Otsu threshold doesn't work so well for your third image because there is more <em>&quot;crack&quot;</em> on the left side than there is plastics, so the two categories you end up with are <em>&quot;background&quot;</em> and <em>&quot;crack&quot;</em>, rather than the desired <em>&quot;background&quot;</em> and <em>&quot;plastics&quot;</em>. However, the <code>a</code> channel does still discriminate if you find a (non-Otsu) threshold. For example, continuing from code above:</p>
<pre><code>norm = cv.normalize(a, None, alpha=0, beta=255, norm_type= cv2.NORM_MINMAX)
</code></pre>
<p><a href=""https://i.sstatic.net/6NFrzSBM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6NFrzSBM.png"" alt=""enter image description here"" /></a></p>
","3","Answer"
"78389941","78389839","<blockquote>
<p>Finds similarities between two images using SIFT. Retrieves well-matched points and uses these points to identify the cut region in the original image</p>
</blockquote>
<pre><code>import cv2
import numpy as np


image1 = cv2.imread('img1.jpg')
image2 = cv2.imread('img2.jpg')

sift = cv2.SIFT_create()
kp1, des1 = sift.detectAndCompute(image1, None)
kp2, des2 = sift.detectAndCompute(image2, None)
bf = cv2.BFMatcher()
matches = bf.knnMatch(des1, des2, k=2)
good_matches = [m for m, n in matches if m.distance &lt; 0.75 * n.distance]

if len(good_matches) &gt; 50:

    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    
    M, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)
    
    h, w = image2.shape[:2]
    pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)
    
    dst = cv2.perspectiveTransform(pts, M)
    image1 = cv2.polylines(image1, [np.int32(dst)], True, (0, 255, 0), 3)
    
    cv2.imshow('Result', cv2.resize(image1, (0, 0), fx=0.5, fy=0.5))
    cv2.waitKey(0)
    cv2.destroyAllWindows()
</code></pre>
<p>Result:</p>
<p><a href=""https://i.sstatic.net/BAait3zu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BAait3zu.png"" alt=""Result"" /></a></p>
<h2>Update</h2>
<p>In this code snippet, the expression 0.75 * n.distance represents a threshold value that is used to filter out good matches during the feature matching process. This value is obtained by multiplying the distance of the second nearest neighbor (n) by 0.75.</p>
<p>The threshold value 0.1 * n.distance determines how far away a match must be relative to the second closest match. The smaller this value, the stricter the match criterion is applied, leading to fewer matches. Hence, the number of matches in the good_matches list decreases.</p>
<p>On the other hand, if you use a larger threshold value, such as 0.75 * n.distance, the algorithm will accept more matches, because in this case the match criterion is less stringent. As a result, more matches are accepted and the number of matches in the good_matches list increases.</p>
<p>Therefore, as you increase the threshold value, you get more matches because the matching criteria is more relaxed. The smaller the threshold value, the fewer matches you get because the matching criteria become stricter. This is a parameter used to control the precision and accuracy of the match between two images.</p>
<p>0.1 = 90% similarity</p>
<p>total_matches:  17264</p>
<p>matches_with_thresh :  73</p>
<p>90% similarity:</p>
<p><a href=""https://i.sstatic.net/oJc2LJgA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oJc2LJgAm.png"" alt=""2m"" /></a></p>
<hr />
<p>0.75 = 25% similarity</p>
<p>total_matches:  17264</p>
<p>matches_with_thresh :  3789</p>
<p>25% similarity:</p>
<p><a href=""https://i.sstatic.net/ru7uvrkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ru7uvrkZm.png"" alt=""3m"" /></a></p>
<h2>Sample img</h2>
<p>0.1 = 90% similarity</p>
<p>total_matches:  17264</p>
<p>matches_with_thresh :  0</p>
<p>90% similarity Sample img:</p>
<p><a href=""https://i.sstatic.net/A0Uwu8JT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A0Uwu8JTm.png"" alt=""4m"" /></a></p>
<hr />
<p>0.75 = 25% similarity</p>
<p>total_matches:  17264</p>
<p>matches_with_thresh :  26</p>
<p>25% similarity Sample img:</p>
<p><a href=""https://i.sstatic.net/vTWObfXo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vTWObfXom.png"" alt=""5m"" /></a></p>
<pre><code>import cv2

image1 = cv2.imread('img1.jpg')
image2 = cv2.imread('img2.jpg')
# image2 = cv2.imread('sample_img.jpg')

sift = cv2.SIFT_create()
kp1, des1 = sift.detectAndCompute(image1, None)
kp2, des2 = sift.detectAndCompute(image2, None)

bf = cv2.BFMatcher()
matches = bf.knnMatch(des1, des2, k=2)

matches_with_thresh = [m for m, n in matches if m.distance &lt; 0.1 * n.distance]
matches_count = len(matches_with_thresh)
total_matches = len(matches)
print(&quot;total_matches: &quot;, total_matches)
print(&quot;matches_with_thresh : &quot;, matches_count)

img_matches = cv2.drawMatches(image1, kp1, image2, kp2, matches_with_thresh, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('Good Matches', cv2.resize(img_matches, (0, 0), fx=0.3, fy=0.3))
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>
","1","Answer"
"78390988","78389349","<p>Since splines are defined piecewise, a single mathematical equation must somehow select the piece you are looking at.</p>
<p>You can e.g. use something like <code>(SIGN(x-3)+1)*(SIGN(x-4)-1)/-4</code> which is zero for all values of x outside [3,4] and multiply this term by the spline in the range [3,4]. Then you simply sum up all the therms. however, one must be carefull at the knots.</p>
","0","Answer"
"78392753","78358516","<p>The <code>PerParentNodeLocalClassifier</code> estimator below works as follows.</p>
<ul>
<li>A linkage matrix <code>Z</code> is derived from the distance matrix, and represented as a tree</li>
<li>For each parent node in the tree, the classes belonging to the left and right branches are grouped and binarised into 0 (left group) and 1 (right group).</li>
<li>Irrelevant classes for that node are removed, and a classifier is fitted to learn left/right predictions at that node.</li>
<li>We recurse through the tree, fitting a classifier as above for each parent node.</li>
</ul>
<p>Credit to @NickODell with <a href=""https://stackoverflow.com/a/78355436/21896093"">this answer</a> for the graph recursion code, using <code>graphviz</code> to construct the graph visualisation. I modelled all the recursion routines on this same method.</p>
<p>At inference time, we take one sample at a time and traverse the tree. The classifier at each node is used to determine whether we branch left (0) or right (1).</p>
<p>Some details about the implementation below:</p>
<ul>
<li>By default the base estimator is a <code>DecisionTreeClassifier</code>. You can specify an alternative classifier using the <code>estimator=</code> parameter.</li>
<li>The estimator computes the class distance matrix during fitting when <code>dissimilarity=&quot;euclidean&quot;</code>. You can alternatively specify your own distance matrix by setting <code>dissimilarity=&quot;precomputed&quot;</code> and then <code>.fit(X, y, dissimilarity_matrix=&lt;precomputed_matrix&gt;)</code>.</li>
</ul>
<p>The implementation is not field-tested, so it's recommended to use the default <code>verbose=1</code> level and keep <code>plot_figures=True</code>. There's useful summary information there for sanity-checking the estimator's logic. The higher verbosity level <code>verbose=2</code> reports results on a per-sample basis.</p>
<p>Let's take two test cases; your example data and a more complex case.</p>
<h2>Test case 1</h2>
<p>Outputs:</p>
<p><a href=""https://i.sstatic.net/OlEKl6o1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OlEKl6o1.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>                Building leaf assignments dict (0: left, 1:right)                
classifier at parent node 8 
 comprises classes        [1, 3, 0, 2, 4] 
 with cluster assignments [0, 1, 1, 1, 1]
classifier at parent node 7 
 comprises classes        [3, 0, 2, 4] 
 with cluster assignments [0, 1, 1, 1]
...
</code></pre>
<pre class=""lang-py prettyprint-override""><code>                               Fitting classifiers                               
Fitting node8 classifier using 1000 samples (100% of the data) 
 left group:  [1] (201 samples) 
 right group: [3, 0, 2, 4] (799 samples) 
 imbalance (≥1) ratio: 3.98
Fitting node7 classifier using 799 samples (80% of the data) 
 left group:  [3] (199 samples) 
 right group: [0, 2, 4] (600 samples) 
 imbalance (≥1) ratio: 3.02
...
</code></pre>
<p>If we initialise the estimator with <code>verbose=2</code> we get sample-wise reports, useful for checking individual samples.</p>
<pre class=""lang-py prettyprint-override""><code>clf.verbose = 2
clf.predict(X[:1])
</code></pre>
<pre class=""lang-py prettyprint-override""><code>                             Predicting for sample                               
X_i:  [ 0.93065833 -1.10836254 -1.27437476  0.16353084 -0.9043927   0.114701...
 node8 is branching right (1)
 node7 is branching left (0)
 final prediction is encoded class=3 [y=3]
</code></pre>
<p>The test data and precomputed distance matrix:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.datasets import make_classification
from scipy.cluster import hierarchy

X, y = make_classification(
    n_samples=1000, n_features=10, n_classes=5, n_informative=4, random_state=0
)

distance_matrix = np.array(
    [[  0.,    201.537, 197.294, 200.823, 194.517],
     [201.537,   0.,    199.449, 202.941, 196.703],
     [197.294, 199.449,   0.,    198.728, 192.354],
     [200.823, 202.941, 198.728,   0.,    195.972],
     [194.517, 196.703, 192.354, 195.972,   0.   ]]
)
</code></pre>
<p>Fit and score:</p>
<pre class=""lang-py prettyprint-override""><code>clf = PerParentNodeLocalClassifier(
    dissimilarity='precomputed',
    linkage_method='complete',
    # estimator=LinearSVC() #optionally specify an alternative base classifier
)

clf.fit(X, y, dissimilarity_matrix=distance_matrix)

clf.predict(X_val)
clf.score(X_val, y_val)
</code></pre>
<h2>Test case 2</h2>
<p>A more complex tree structure.
<a href=""https://i.sstatic.net/82uDGbhT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82uDGbhT.png"" alt=""enter image description here"" /></a></p>
<p>Test data, this time without a precomputed distance matrix:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from sklearn.datasets import make_classification

n_features = 7
n_classes = 5
X_a, y_a = make_classification(
    n_samples=10_000,
    n_features=n_features, n_informative=5,
    n_redundant=0, n_repeated=0,
    n_classes=n_classes, random_state=0
)
</code></pre>
<p>Usage:</p>
<pre class=""lang-py prettyprint-override""><code>clf = PerParentNodeLocalClassifier(random_state=0).fit(X_a, y_a)
</code></pre>
<h2>Classifier implementation</h2>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from sklearn.base import BaseEstimator, clone, ClassifierMixin, MetaEstimatorMixin
from sklearn.utils.validation import check_array, check_X_y, check_is_fitted
from sklearn.preprocessing import LabelEncoder

from scipy.spatial.distance import squareform
from scipy.cluster import hierarchy

from sklearn.tree import DecisionTreeClassifier
from multiprocessing import cpu_count, Pool

class PerParentNodeLocalClassifier (BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
    &quot;&quot;&quot;
    A classifier implementing the &quot;local classifier per parent node&quot; (LCPN) algorithm.
    
    LPCN is a classification algorithm where local classification decisions are
    made over a pre-defined class hierarchy, resulting in a final prediction
    that reflects the hierarchical relationship between classes.
    
    This implementation is a meta-estimator that fits a classifier at each parent
    node. Parent nodes are derived from a class dissimilarity (distance) matrix
    using agglomerative clustering. The class dissimilarity matrix is either
    user-supplied or computed from the data during fitting.
    
    Parameters
    ----------
    estimator : object, default=None
        The base estimator used for each local classifier. If None, then the
        base estimator is DecisionTreeClassifier initialised with
        `max_depth=5` and `class_weight=&quot;balanced&quot;`.
    
    linkage_method : {&quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;weighted&quot;, &quot;centroid&quot;,
                      &quot;median&quot;, &quot;ward&quot;}, default=&quot;single&quot;
        The linkage method used for agglomerative clustering, supplied to
        scipy.hierarchy.linkage().
    
    dissimilarity : {&quot;precomputed&quot;, &quot;euclidean&quot;}, default=&quot;euclidean&quot;
        If &quot;precomputed&quot;, the class dissimilarity (distance) matrix must be
        supplied as `.fit(X, y, dissimilarity_matrix=...)`.
        
        If &quot;euclidean&quot;, a dissimilarity matrix will be computed from the data
        during `.fit(X, y)`. In this case, the dissimilarity between each pair
        of classes `(i, j)` is defined as the euclidean distance between their
        centroids in feature space.
    
    verbose : bool or int, default=1
        Controls the verbosity when fitting and predicting. Use 1/True for
        general information, and 2 for per-sample details. 0 or False not
        recommended as this estimator has not been field-tested.
    
    plot_figures : bool, default=True
        If True, two plots will be rendered showing the dendrograms and the
        decision tree. Useful for a visual check of the estimator's classification
        logic. False is not recommended as this estimator has not been field-tested.
    
    n_jobs : int, default=1
        Specifies how many workers to dispatch when predicting. -1 will use
        all CPUs. When set to 1, the samples will be evaluated strictly
        sequentially.
    
    random_state : int, RandomState instance or None, default=None
        Controls the randomness of `estimator`, if applicable.
    &quot;&quot;&quot;
    def __init__(
        self,
        estimator=None,
        linkage_method='single',
        dissimilarity='euclidean',
        verbose=1,
        plot_figures=True,
        n_jobs=1,
        random_state=None
    ):
        self.estimator = estimator
        self.linkage_method = linkage_method
        self.dissimilarity = dissimilarity
        self.verbose = verbose
        self.plot_figures = plot_figures
        self.n_jobs = n_jobs
        self.random_state = random_state
        
    def fit(self, X, y, **kwargs):
        &quot;&quot;&quot;
        Fit the estimator. Optionally takes `dissimilarity_matrix=` parameter.
        
        Parameters:
        X (pd.DataFrame, np.ndarray): Features shaped (n_samples, n_features).
        y (pd.Series, np.ndarray): Multi-class labels (n_samples,).
        
        **kwargs
        dissimilarity_matrix (np.ndarray): Required if `dissimilarity=&quot;precomputed&quot;`,
                                           shaped (n_classes, n_classes)
        &quot;&quot;&quot;
        #Input checks for sklearn estimators
        X, y = check_X_y(X, y)
                
        #sklearn checks
        if hasattr(X, 'columns'):
            self.feature_names_in_ = np.array(
                X.columns, dtype='object'
            )
        self.n_features_in_ = X.shape[1]
        
        #For simplicity always define a self.label_encoder_
        # It won't make a difference if supplied y is encoded.
        self.label_encoder_ = LabelEncoder().fit(y)
        self.classes_original_ = self.label_encoder_.classes_
        self.n_classes = len(self.classes_original_)
        
        #encoded values are used for all computations
        self.classes_ = self.label_encoder_.transform(self.classes_original_)
        
        if self.verbose:
            print(
                f'\nSupplied classes', self.classes_original_, 'encoded to',
                str(self.classes_),
                '\nFigures display encoded values. Predictions returned in original format.'
            )
        
        self.X = X
        self.y = self.label_encoder_.transform(y)
        
        #Handle dissimilarity= param
        # creates self.dissimilarity_matrix_
        self.handle_dissimilarity_arg(kwargs)
        if self.verbose:
            matrix_df = pd.DataFrame(
                self.dissimilarity_matrix_,
                index=[i for i in self.classes_],
                columns=[i for i in self.classes_]
            ).rename_axis(columns='class')
            
            display(
                matrix_df
                .style
                .set_caption(self.dissimilarity + ' dissimilarity matrix')
            )
        
        #Create linkage matrix
        # creates self.Z_ and self._df_
        self.create_linkage_matrix()
        if self.verbose:
            display(
                self.Z_df_
                .style
                .set_caption('linkage matrix Z')
                .background_gradient(subset='linkage_distance', cmap='plasma')
            )
        if self.plot_figures:
            self.plot_dendrogram()
        
        #Linkage matrix as tree
        self.root_node_ = hierarchy.to_tree(self.Z_)
        if self.plot_figures:
            self.render_tree_visualisation()
        
        #Leaf assignments
        self.leaf_assignments_dict = {}
        self.build_leaf_assignments_dict_recursive(
            self.root_node_,
            self.leaf_assignments_dict
        )
        
        #Fit classifiers
        self.classifiers_dict_ = {}
        
        if self.estimator is None:
            self.estimator = DecisionTreeClassifier(
                max_depth=5,
                class_weight='balanced',
            )
            
            if self.verbose:
                print('\nUsing default base estimator:\n', self.estimator)
        
        if hasattr(self.estimator, 'random_state'):
            self.estimator.set_params(**{'random_state': self.random_state})
            
        self.fit_classifier_tree_recursive(self.root_node_, self.classifiers_dict_)
        
        return self
    
    def predict(self, X):
        &quot;&quot;&quot;
        Render a prediction for each sample.
        &quot;&quot;&quot;
        check_is_fitted(self)
        
        X = check_array(X)
        
        if not self.n_jobs:
            raise ValueError('n_jobs must be a non-zero integer')
        
        #Strictly sequential evaluation is useful for debugging
        if self.n_jobs == 1:
            predictions = []
            print()
            for i, sample in enumerate(X):
                if self.verbose:
                    print(
                        f'Rendering prediction for sample',
                        f'{i+1:&gt;5d} of {len(X):&gt;5d} [{(i+1) / len(X):&gt;5.1%}]...',
                        end='\r' if i + 1 != len(X) else 'done\n',
                    )
                predictions.append( self.predict_sample(sample) )
            
        else:
            n_jobs = self.n_jobs if self.n_jobs &gt; 0 else self.n_jobs + cpu_count() + 1
            if self.verbose:
                print(f'\n Predicting using {n_jobs} CPUs')
            
            with Pool(n_jobs) as p:
                predictions = p.map(self.predict_sample, X)
        
        return self.label_encoder_.inverse_transform(predictions)
    
    #
    # Helper methods
    #
    
    def handle_dissimilarity_arg(self, fit_kwargs):
        &quot;&quot;&quot;
        Sets the dissimilarity matrix.
        
        Either compute it from Xy if dissimilariy=&quot;euclidean&quot;,
        or use dissimilarity_matrix supplied to
        .fit() when dissimilarity='precomputed'.
        
        &quot;&quot;&quot;
        if self.dissimilarity == 'euclidean':
            self.dissimilarity_matrix_ = self.compute_euclidean_class_dissimilarity_matrix()
        elif self.dissimilarity == 'precomputed':
            if 'dissimilarity_matrix' not in fit_kwargs:
                raise TypeError(
                    &quot;.fit() requires 'dissimilarity_matrix=' argument &quot;
                    &quot;when dissimilarity='precomputed'&quot;
                )
            
            dissimilarity_matrix = fit_kwargs['dissimilarity_matrix']
                
            self.dissimilarity_matrix_ = self.sanitise_dissimilarity_matrix(
                dissimilarity_matrix
            )
            
    def compute_euclidean_class_dissimilarity_matrix(self):
        &quot;&quot;&quot;
        Computes (n_classes, n_classes) dissimilarity (distance) matrix
        using the euclidean distance between pairs of classes in feature space.
        &quot;&quot;&quot;
        class_centroids = np.stack(
            [self.X[self.y==klass].mean(axis=0) for klass in self.classes_],
            axis=0
        )
        
        dissimilarity_matrix = np.array([
            [np.linalg.norm(class_centroids[i] - class_centroids[j]) for j in self.classes_]
            for i in self.classes_
        ]).reshape(self.n_classes, self.n_classes)
        
        return self.sanitise_dissimilarity_matrix(dissimilarity_matrix)


    def sanitise_dissimilarity_matrix(self, mtx):
        &quot;&quot;&quot;
        Checks if the supplied matrix is symmetrical and has a zero diagonal.
        If not, it forces those conditions on the matrix.
        &quot;&quot;&quot;
        if np.array_equal(mtx, mtx.T) and np.abs(mtx).trace()==0.:
            return mtx
        
        if self.verbose:
            print(
                '\nDissimilarity matrix is either asymmetrical or has '
                'non-zero diagonal. Coercing matrix for symmetry and diagonal=0.'
            )
        
        #Enforce symmetry, diag=0
        np.fill_diagonal(
            mtx_clean := (mtx + mtx.T) / 2,
            val=0
        )
        return mtx_clean

    
    def create_linkage_matrix(self):
        &quot;&quot;&quot;
        Uses scipy to create the linkage matrix Z, and a DataFrame version as well.
        &quot;&quot;&quot;
        self.Z_ = hierarchy.linkage(
            squareform(self.dissimilarity_matrix_),
            method=self.linkage_method,
        )
        
        self.Z_df_ = pd.DataFrame({
            'class_i': self.Z_[:, 0].astype(int),
            'class_j': self.Z_[:, 1].astype(int),
            'linkage_distance': self.Z_[:, 2],
            'total_members': self.Z_[:, 3].astype(int)
        }).rename_axis(index='step')
    
    
    def plot_dendrogram(self):
        &quot;&quot;&quot;
        Uses scipy to draw a dendrogram for each link point.
        &quot;&quot;&quot;
        f, axs = plt.subplots(
            nrows=1, ncols=len(self.Z_df_), figsize=(11, 3),
            sharey=True, layout='constrained'
        )
        
        for ax, cut_pt in zip(axs, self.Z_df_.linkage_distance):
            dendrogram = hierarchy.dendrogram(
                self.Z_,
                leaf_font_size=10,
                labels=self.classes_,
                color_threshold=cut_pt,
                above_threshold_color='darkslategray',
                ax=ax
            )
            ax.axhline(cut_pt, linewidth=1.5, color='deeppink', linestyle=':')
            ax.spines[['top', 'right', 'bottom']].set_visible(False)
            ax.set_title(f'linkage step {np.argwhere(ax==axs).item()}')
        #Formatting
        ax = axs[0]
        ax.set_xlabel('class')
        ax.spines.left.set_visible(True)
        ax.set_ylabel(self.linkage_method + ' linkage distance')
    
    def render_tree_visualisation(self):
        &quot;&quot;&quot;
        Uses graphviz package to display the linkage matrix as a tree
        &quot;&quot;&quot;
        try:
            import graphviz
        except ModuleNotFoundError as e:
            print(
                'Error importing graphiz package. '
                'graphviz package is required for tree visualisation.\n', e
            )
            return
        
        try:
            dot = graphviz.Digraph('cluster_heirarchy')
            self.render_dot_tree_recursive(self.root_node_, dot)
            display(dot)
        except Exception as e:
            print('Error rendering graph visualisation using graphviz\n', e)
            
            
    def render_dot_tree_recursive(self, node, dot, parent=None):
        if node.is_leaf():
            dot.node(f&quot;node{node.id}&quot;, f'class {node.id}')
        
        if parent:
            dot.edge(f&quot;node{parent.id}&quot;, f&quot;node{node.id}&quot;)
        if node.left:
            self.render_dot_tree_recursive(node.left, dot, node)
        if node.right:
            self.render_dot_tree_recursive(node.right, dot, node)
        
        
    #Build classifier tree
    def build_leaf_assignments_dict_recursive(self, node, res_dict, parent=None):
        &quot;&quot;&quot;
        Recursively populates a dictionary that identifies each parent node with
        its class members.
        &quot;&quot;&quot;
        if node.is_leaf():
            return
        
        # 0 2 4 1 3 relevant members, left-right
        # 0 1 0 1 1 assignments for [y0 y1 y2 y3 y4]
        #assignments[lr membs] = relevant assignments, l-r order
        leaves_lr = np.array(node.pre_order())
        assignments_lr = hierarchy.cut_tree(self.Z_, height=node.dist).ravel()[leaves_lr]
        
        #Convert left to 0, right  as 1
        assignments_lr = (assignments_lr - assignments_lr[0]).astype(bool).astype(int)
        
        res_dict[node.id] = [leaves_lr, assignments_lr]
        
        if self.verbose:
            if parent is None:
                print('\n', 'Building leaf assignments dict (0: left, 1:right)'.center(80, ' '))
            print(
                'classifier at parent node', node.id,
                '\n comprises classes       ', leaves_lr.tolist(),
                '\n with cluster assignments', assignments_lr.tolist(),
            )
        
        self.build_leaf_assignments_dict_recursive(node.left, res_dict, node)
        self.build_leaf_assignments_dict_recursive(node.right, res_dict, node)
    
    
    def fit_classifier_tree_recursive(self, node, clf_dict, parent=None):
        &quot;&quot;&quot;
        Fit a classifier at each parent node, and populate a dict with the
        classifiers.
        &quot;&quot;&quot;
        if node.is_leaf():
            return
        
        #leaves relevant to this node, and their assignments
        leaves, assignments = self.leaf_assignments_dict[node.id]
        
        #discard samples with irrelevant leaves
        subset_ixs_bool = np.isin(self.y, leaves)
        X_subset, y_subset = [arr[subset_ixs_bool] for arr in [self.X, self.y]]
        
        #group of left classes, and right classes
        # use to binarise y as 0:left group, 1:right group
        left_group, right_group = [leaves[assignments==i] for i in [0, 1]]
        y_bin = np.where(np.isin(y_subset, left_group), 0, 1)
        
        #Fit a classifer for this node
        clf = clone(self.estimator)

        if self.verbose:
            if not parent:
                print('\n', 'Fitting classifiers'.center(80, ' '))
                
            nsamples_left, nsamples_right = [sum(y_bin==i) for i in [0, 1]]
            imbalance_ratio = (
                max(nsamples_left, nsamples_right) / min(nsamples_left, nsamples_right)
            )
            print(
                f'Fitting node{node.id} classifier using {y_bin.size} samples',
                f'({y_bin.size / self.y.size:.0%} of the data)',
                
                f'\n left group:  {left_group.tolist()} ({nsamples_left} samples)',
                f'\n right group: {right_group.tolist()} ({nsamples_right} samples)',
                f'\n imbalance (≥1) ratio: {imbalance_ratio:&gt;4.2f}',
            )

        clf.fit(X_subset, y_bin)
        clf_dict[node.id] = clf
        
        #recurse into child nodes
        self.fit_classifier_tree_recursive(node.left, clf_dict, node)
        self.fit_classifier_tree_recursive(node.right, clf_dict, node)
    
    
    def predict_sample(self, sample):
        &quot;&quot;&quot;
        Prediction for a single sample.
        &quot;&quot;&quot;
        if self.verbose == 2:
            print(
                '\n', 'Predicting for sample'.center(80, ' '),
                '\nX_i: ', str(sample)[:70] + '...',
            )
            
        if sample.ndim == 1:
            sample = sample.reshape(1, -1)
        
        prediction = [None] #recursion will deposit value in prediction[0]
        self.predict_sample_recursive(sample, prediction, self.root_node_)
        return prediction[0]
        
    def predict_sample_recursive(self, sample, return_value_placeholder, node, parent=None):
        if sample.ndim == 1:
            sample = sample.reshape(1, -1)
        
        if self.verbose == 2:        
            if node.is_leaf():
                print(
                    f' final prediction is encoded class={node.id}',
                    f'[y={self.label_encoder_.inverse_transform([node.id]).item()}]'
                )

        #Reached leaf node/final prediction
        if node.is_leaf():
            return_value_placeholder[0] = node.id
            return
        
        #Get left/right prediction at this node
        prediction = self.classifiers_dict_[node.id].predict(sample).item()
        if self.verbose == 2:
            print(f' node{node.id}', 'is branching', ['left (0)', 'right (1)'][prediction])
        
        #Recurse into the left/right branch depending on &quot;prediction&quot;
        self.predict_sample_recursive(
            sample,
            return_value_placeholder,
            [node.left, node.right][prediction],
            parent=node
        )
</code></pre>
","4","Answer"
"78393334","78383883","<p>This seems to work as required. It checks out visually, at least. You'd need to explore the results further to confirm they're correct and as expected. I think the distributions of various parameters (e.g. <code>df.hist()</code>) ought to look similar between the sampled and remainder sets.</p>
<p>The process I followed sounds similar to the one you quoted, where I use the &quot;pixel values&quot; (KDE score for each data point) as &quot;weighting factors&quot;.</p>
<p>I fitted a <code>KernelDensity</code> estimator in order to derive a density score for each data point, and then supplied the density scores to the <code>weights=</code> parameter of <code>&lt;dataframe&gt;.resample(...)</code>.</p>
<p>Visual comparison below between unweighted and weighted sampling results.</p>
<p>Sampling without weights:
<a href=""https://i.sstatic.net/osvBdkA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/osvBdkA4.png"" alt=""enter image description here"" /></a></p>
<p>Sampling with weights:
<a href=""https://i.sstatic.net/AaaOvb8J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AaaOvb8J.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>Reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

#
#Data for testing
#
np.random.seed(0)
from sklearn.datasets import make_moons

X, _ = make_moons(n_samples=400, noise=0.1, random_state=11)
X = np.concatenate([X, np.random.uniform(-2, 2, size=[500, 2])], axis=0)

#Adjust scale and location
X[:, 0] = (X[:, 0] - X[:, 0].min()) / (X[:, 0] - X[:, 0].min()).max() * 4 + 10
X[:, 1] = (X[:, 1] - X[:, 1].min()) / (X[:, 0] - X[:, 0].min()).max() * 7.5 + 47

df = pd.DataFrame({'latitude': X[:, 1], 'longitude': X[:, 0]})

f, ax = plt.subplots(figsize=(8, 3.5))
ax.scatter(df.longitude, df.latitude, marker='.', color='black', label='monitoring station')
ax.set(xlabel='longitude', ylabel='latitude')
ax.spines[:].set_visible(False)

#
#Fit a density estimator &amp; get the density map
#
from sklearn.neighbors import KernelDensity

kd_estimator = KernelDensity(bandwidth=0.3, kernel='gaussian').fit(df[['longitude', 'latitude']])
density_scores = np.exp(kd_estimator.score_samples( df[['longitude', 'latitude']] ))
df['sample_weights'] = density_scores / density_scores.sum() #optionally norm to 1
 
#Overlay the density map
im = ax.tricontourf(
    df.longitude, df.latitude, density_scores,
    zorder=0, cmap='BuGn', levels=100
)
f.colorbar(im, label='density')

#Sample based on weight
df_test = df.sample(frac=0.2, weights=df.sample_weights, random_state=0)
# df_test = df.sample(frac=0.2, random_state=0) #sample without weights

df_train = df.drop(index=df_test.index)

#Visualise sampling distribution
ax.scatter(
    df_test.longitude, df_test.latitude,
    marker='o', s=50, facecolor='none', edgecolor='tab:red', linewidth=2,
    label='test sample'
)
f.legend(ncol=2)
f.tight_layout()
</code></pre>
","0","Answer"
"78393400","78367946","<p>The error trace shows that the error is coming from <code>cp1252.py</code>.  This indicates it's using the Windows code page 1252, not UTF-8 as you're expecting.  It also looks like you're using Python 3.11, which on Windows should be able to print any Unicode string to the console so I don't quite know why it's failing.</p>
","0","Answer"
"78393953","78393841","<p>Try using this repository.
<a href=""https://github.com/renjithsasidharan/seven-segment-ocr"" rel=""nofollow noreferrer"">https://github.com/renjithsasidharan/seven-segment-ocr</a></p>
<p>It seems to work according to the results they have provided. Since this is implemented in Tensorflow lite, I think it would be ideal for faster inference.</p>
<p>If you have time to look into this <a href=""https://msie4.ait.ac.th/wp-content/uploads/sites/5/2020/05/MSIE-03-L-M3S2_L01.pdf"" rel=""nofollow noreferrer"">paper</a> as well</p>
","-1","Answer"
"78394625","78378560","<p>Your input data contains invalid/missing values (NaNs or &quot;not a number&quot;). Three main options:</p>
<ul>
<li>Use an estimator that can handle invalid values, like <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor"" rel=""nofollow noreferrer""><code>HistGradientBoostingRegressor</code></a>. The one you're using (<code>SVM</code>) will error with NaNs, like most <code>sklearn</code> estimators. See <a href=""https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"" rel=""nofollow noreferrer"">here</a> for a list of estimators that handle NaN values.</li>
<li>If you want to use <code>SVR</code>, you could simply drop the NaNs from your dataset before giving it to the <code>SVR</code>. You should consider whether it's okay to simply discard any sample/row that has one or more missing values.</li>
<li>As an alternative to dropping the rows with NaNs, you could replace them using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer"" rel=""nofollow noreferrer""><code>SimpleImputer</code></a>. That replaces NaNs with a number, and you can then supply the cleaned dataset to <code>SVR</code>. <code>SimpleImputer</code> is a good place to start, and <a href=""https://scikit-learn.org/stable/modules/impute.html"" rel=""nofollow noreferrer"">here</a> is more about imputation.</li>
</ul>
","0","Answer"
"78400767","78358516","<pre><code>from sklearn.tree import DecisionTreeClassifier

# Assuming `X` and `y` are your data and labels respectively
# And `internal_nodes` is a list of internal nodes in the hierarchy

classifiers = {}  # To store classifiers for each internal node

for node in internal_nodes:
    # Implement logic to get samples for classes under the current node
    X_node, y_node = get_samples_for_node(node, X, y)
    
    # Train classifier
    clf = DecisionTreeClassifier()
    clf.fit(X_node, y_node)
    
    classifiers[node] = clf

# Implement `get_samples_for_node` to return samples for the given node
</code></pre>
","0","Answer"
"78401325","78400223","<p>You are getting error <strong>nodes didn't match Pod's node affinity/selector</strong> as labels are not matching, as you have mentioned you have checked the nodes and they do not include labels.</p>
<p>I have followed the same <a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/timesharing-gpus#overview"" rel=""nofollow noreferrer"">document</a> you have mentioned and I have replicated the issue and I got the pods scheduled on the node.</p>
<p><strong>Note:</strong> I have used standard cluster and autopilot deployment.</p>
<p>I have created a cluster as shown below:</p>
<p><a href=""https://i.sstatic.net/TMmfUhQJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMmfUhQJ.png"" alt=""image"" /></a></p>
<p>As mentioned in the doc I have created a nodepool but it was not scheduling the pods in it. So I have deleted the default node pool and created a new node pool with using command:</p>
<pre><code>gcloud container node-pools create gputimesharing \
    --region=us-west4-a \
    --cluster-version=latest \
    --machine-type=n1-standard-16 \
    --accelerator=type=nvidia-tesla-t4,count=1,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=12,gpu-driver-version=default
</code></pre>
<p>Then the nodes have used this node pool, then I have tried deploying the workload using below spec:</p>
<pre><code>  nodeSelector:
    cloud.google.com/gke-accelerator: &quot;nvidia-tesla-t4&quot;
    cloud.google.com/gke-gpu-sharing-strategy: &quot;time-sharing&quot;
    cloud.google.com/gke-max-shared-clients-per-gpu: &quot;12&quot;
</code></pre>
<p>Checked the nodes have the labels: <a href=""https://i.sstatic.net/MBIHQt4p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MBIHQt4p.png"" alt=""image"" /></a></p>
<p>Then the pods are scheduled without any error.</p>
<p>Can you try this process and let me know if you are getting any errors, and verify the number of GPUs visible in your nodes as mentioned in the doc.</p>
","1","Answer"
"78402034","78401636","<p>The error message you're seeing is because the <code>KerasLayer</code> object from TensorFlow Hub is not an instance of <code>keras.Layer</code>, and therefore cannot be added directly to a Keras Sequential model.</p>
<p>To work around this you need to convert the TensorFlow Hub layer into Keras layer by using <code>tf.keras.layers.Lambda</code> after creating the sequential model.</p>
","0","Answer"
"78402546","78402507","<p>you can use a <strong>stratified sampling</strong> approach during cross-validation or train/test splitting. This ensures that each fold or split contains a balanced representation of each class.</p>
<pre><code>from sklearn.model_selection import StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
</code></pre>
","1","Answer"
"78405890","78398017","<p>I'm not really sure if it's what you mean, but a lot (or maybe all?) of the classifiers in sklearn have the <code>feature_importance</code> method (see e.g. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow noreferrer"">Random Forest Classifier</a>. This is how much &quot;weight&quot; or &quot;importance&quot; the model gave to each feature. This is true also for regression models</p>
","0","Answer"
"78405996","78383840","<p>These competing patterns in <code>lightgbm.cv()</code> in the <code>lightgbm</code> package have been in the library since September 2017 (<a href=""https://github.com/microsoft/LightGBM/commit/ef77806934f6b020c6425372f705fd0240a68f5b#diff-9bd633ead0bdfe9540c42a618efd9e559cca16c522ad844a09fcf4ffc7d6e84c"" rel=""nofollow noreferrer"">this commit</a>). The ability to specify that in both interfaces was added mainly for convenience. It isn't functionally different from passing those arguments to <code>lightgbm.Dataset()</code>.</p>
<blockquote>
<p><em>If both are specified which one takes precedence?</em></p>
<p><em>Which one is the suggested place to specify the categorical_feature?</em></p>
<p><em>Are the two choices in any way different internally to the working of the lightgbm pipeline?</em></p>
</blockquote>
<p>Always prefer passing it to <code>lightgbm.Dataset</code>, and ignore the argument to <code>lightgbm.cv()</code> / <code>lightgbm.train()</code>.</p>
<p>The <code>categorical_feature</code> argument passed into <code>lightgbm.cv()</code> / <code>lightgbm.train()</code> is only used in one place, in a call to <code>Dataset.set_categorical_feature()</code> inside the <code>lightgbm.cv()</code> / <code>lightgbm.train()</code> function. At best, this will be useless and not update the <code>Dataset</code>.</p>
<p>At worst, it can cause an error if the raw data is no longer available.</p>
<pre class=""lang-py prettyprint-override""><code>import lightgbm as lgb
from sklearn.datasets import make_regression

X, y = make_regression(n_samples=1_000, n_features=10)

dtrain = lgb.Dataset(
    X,
    label=y,
    categorical_feature=[1, 4],
    free_raw_data=True
)
dtrain.construct()

bst = lgb.train(
  params={&quot;objective&quot;: &quot;regression&quot;},
  train_set=dtrain,
  categorical_feature=[1, 3]
)
# lightgbm.basic.LightGBMError: Cannot set categorical feature after freed raw data,
# set free_raw_data=False when construct Dataset to avoid this.
</code></pre>
","1","Answer"
"78410494","78143186","<p>This worked for me, although it took very long to generate an image.</p>
<pre><code>from diffusers import DiffusionPipeline
import numpy as np
pipeline = DiffusionPipeline.from_pretrained(
    &quot;RunDiffusion/Juggernaut-X-v10&quot;, cache_dir=cache_dir
)
prompt = &quot;Your Prompt&quot;

# Set num_inference_steps as per your memory or computation
image = pipeline(prompt, num_inference_steps=100, guidance_strength=0.7)
image_data = np.array(image.images[0])
image_pil = Image.fromarray(image_data.astype(np.uint8))
image_pil.save(f&quot;output_images/Example10.png&quot;)
image_pil
</code></pre>
","0","Answer"
"78411501","78400254","<p>Since you are learning a linear model, you might want to use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"" rel=""nofollow noreferrer"">sklearn.svm.LinearSVC</a> instead, which is based on liblinear and can learn way faster. In particular you can set the number of iterations of an underlying solver and thus trade speed for accuracy.</p>
","2","Answer"
"78413391","78348894","<p>I eventually found a trick that solved it for my case without too many customizations.</p>
<p>But if you do need to pass different kinds of data for training, I don't think there's an easy answer as for today.</p>
<p>It should be possible though to <a href=""https://keras.io/guides/writing_a_custom_training_loop_in_jax/"" rel=""nofollow noreferrer"">write your own training loop</a>, and use any structure that you want for the data and labels.
In this case you might also want to use the <a href=""https://keras.io/examples/keras_recipes/trainer_pattern/"" rel=""nofollow noreferrer"">trainer pattern</a>, implementing a custom version of <code>keras.src.backend.jax.trainer.JAXTrainer</code>.</p>
","0","Answer"
"78419620","78395647","<p><code>OneVsRestClassifier</code> has a <code>.estimators_</code> attribute which lists all of the individual estimators (one per class). You can access each estimator one by one, and get its <code>.n_support_</code> etc.</p>
<p>Regarding the ordering of <code>.estimators_</code>: My assumption is that <code>.estimators_</code> will match the <code>.classes_</code> attribute. So if you do <code>ovr = OneVsRest(SVC()).fit(X, y)</code>, then I'd expect <code>ovr.estimators_</code> to line up with <code>ovr.classes_</code>.</p>
","1","Answer"
"78420856","78392429","<p>Probably the easiest way to do this is to dump the param groups and state dict of the optimizer after every <code>backward</code> call. This will let you capture the parameters, gradients and optimizer state.</p>
<p>The code below shows an example with a simple MLP, but the <code>log_state</code> function should work for all models so long as the param groups/state dict don't have multiple levels of nesting.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# example model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h = self.fc1(x)
        pred = self.fc2(self.relu(h))
        return pred

def log_state(opt):
    output = {}

    # log state dict
    state_dict = {}
    for key, value in opt.state_dict().items():
        if key == 'state':
            state_dict[key] = {}
            for state_key, state_value in value.items():
                state_dict[key][state_key] = {}
                for k, v in state_value.items():
                    if torch.is_tensor(v):
                        state_dict[key][state_key][k] = v.cpu().clone() # move tensor to cpu
                    else:
                        state_dict[key][state_key][k] = v
        else:
            state_dict[key] = value

    output['state_dict'] = state_dict

    # log param groups
    param_groups = []
    for group in opt.param_groups:
        param_group = {key: value for key, value in group.items() if key != 'params'}
        param_group['params'] = []
        param_group['param_grads'] = []
        for param in group['params']:
            param_group['params'].append(param.data.cpu().clone()) # move tensor to cpu
            # log gradients
            if param.grad is not None:
                param_group['param_grads'].append(param.grad.data.cpu().clone()) # move tensor to cpu
            else:
                param_group['param_grads'].append(None)

        param_groups.append(param_group)

    output['param_groups'] = param_groups
    
    return output



net = MLP(64, 20, 10)

opt = torch.optim.Adam(net.parameters(), lr=1e-3)

state_log = {}
for i in range(5):
    x = torch.randn(8, 64)
    y = torch.randn(8,10)
    p = net(x)
    loss = nn.functional.mse_loss(p, y)
    opt.zero_grad()
    loss.backward()
    state_log[i] = log_state(opt) # log optimizer state every step
    opt.step()
</code></pre>
<p>The code logs after <code>backward</code> to grab gradients. You could add additional logging after <code>step</code>, but the values updated after <code>step</code> (params, optimizer averages, etc) will be captured on the next iteration. You can also use values from one iteration to recreate the next. For example, with Adam:</p>
<pre class=""lang-py prettyprint-override""><code>step = 2

old_state = state_log[step]
new_state = state_log[step+1]

# adam params
b1 = 0.9
b2 = 0.999
eps = 1e-8
lr = 1e-3

w = old_state['param_groups'][0]['params'][0]
g = old_state['param_groups'][0]['param_grads'][0]

m_old = old_state['state_dict']['state'][0]['exp_avg']
v_old = old_state['state_dict']['state'][0]['exp_avg_sq']

m = b1 * m_old + (1-b1)*g
v = b2 * v_old + (1-b2)*(g.pow(2))

m_hat = m.div(1-b1**(step+1))
v_hat = v.div(1-b2**(step+1))

w_new = w - lr * m_hat / (torch.sqrt(v_hat) + eps)

torch.allclose(w_new, new_state['param_groups'][0]['params'][0])
&gt; True
</code></pre>
","-1","Answer"
"78426747","78185218","<p>Google claims that tensorflow works with python 3.9 but does not actually verify this. The type annotation used in tensorflow is incompatible with python 3.9. I have verified that this error goes away when python is ugraded to 3.11 and tensorflow is installed in a fresh virtual environment</p>
","7","Answer"
"78436813","78249695","<p>I think what you are looking for is the Lazy Layers. There are Lazy implementations of Conv and Linear layers where you do not need to specify the in channels. Check this for an example and more details:
<a href=""https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin</a></p>
<p>Hope it helps!</p>
","1","Answer"
"78452087","78311513","<p>With 2 dense layers, you essentially have an input and an output layer. Going by your code, you only have two layer but no hidden layers in your model. That allows you to approximate functions of the 1st degree, e.g. lines for classification, regression, etc. The gradient is only calculated twice per iteration giving you essentially two anchor points - great for lines, not so great for parables.</p>
<p>If you add one more layer, you now have a hidden layer between input and output. That allows for more complex modelling because the gradient is calculated thrice now per iteration. Generally it used to approximate functions of the second order, e.g. x^2 and the like.</p>
<p>np.abs(x) is similar to a parable and needs a three layer model. If you are interested in how and why neural networks work, there is a very good course on Coursera from Andrew Ng &quot;Machine Learning&quot;. It's free if you don't want the certificate.</p>
","0","Answer"
"78465311","78241816","<p>For me updating the PyCharm IDE resolved the problem (from 2023.X to 2024.11).</p>
","1","Answer"
"78465339","78311513","<p><strong>It depends on the weight initialization.</strong></p>
<p>If both weights of are initilized with positive numbers, the network can only predict positive numbers. For negative numbers, it will always output zero.
This will also result no gradients - there is no small step to the weights that would make the output match a bit better.</p>
<p>So either switch to a different activation function, such as leaky Relu that also passes some signal for the negative values or change the init.</p>
<p>In the code below I demonstrate it with different custom inits.</p>
<p>good_init sets one weight to a positive, one to a negative values -&gt; The problem gets solved.
both bad_inits set the weights to the same sign, and only half of the domain will be learned.</p>
<pre><code>import tensorflow as tf
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

X = np.linspace(-1000,1000,400)
np.random.shuffle(X)
Y = np.abs(X)

# Reshape data to fit the model input
X = X.reshape(-1, 1)
Y = Y.reshape(-1, 1)

from keras import backend as K

def good_init(shape, dtype=None):
    # one positive, one negative weight
    val=np.linspace(-1,1,np.prod(shape)).reshape(shape)
    return K.variable(value=val, dtype=dtype)

def bad_init_right(shape, dtype=None):
    # both weights positive, only right side works
    val=np.linspace(-1,1,np.prod(shape)).reshape(shape)
    val=np.abs(val)
    return K.variable(value=val, dtype=dtype)


def bad_init_left(shape, dtype=None):
    # both weights negative, only right side works
    val=np.linspace(-1,1,np.prod(shape)).reshape(shape)
    val=-np.abs(val)
    return K.variable(value=val, dtype=dtype)

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, activation='relu', kernel_initializer=bad_init_left),
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mse',metrics=['mae'])
model.fit(X, Y, epochs=100)
# Predict using the model
Y_pred = model.predict(X)

# Plot the results
plt.scatter(X, Y, color='blue', label='Actual')
plt.scatter(X, Y_pred, color='red', label='Predicted')
plt.title('Actual vs Predicted')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
</code></pre>
","1","Answer"
"78482367","78323867","<p>Usually for object detection you should put two. Background and classes so always your number of classes + 1</p>
","0","Answer"
"78507300","78152636","<p>You should provide model_name such as:</p>
<pre><code>vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings(model_name='mistral-7b-openorca.gguf2.Q4_0.gguf'))
</code></pre>
","1","Answer"
"78507844","78379820","<p>I've experienced the same in a corporate network.</p>
<p>To get around I have been manually downloading through my browser (the url is to the left of the error message in your screenshot) and manually loading the models into the appropriate dir: <code>~/.cache/lm-studio/models</code> in the appropriate subpath.</p>
<p>ie <code>~/.cache/lm-studio/models/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q6_K.gguf</code></p>
<p>Not the most elegant solution but I have yet to find information on a better workaround.</p>
","4","Answer"
"78533145","78257038","<p>In gradient boosting, the optimization is done over the space of functions. So we use functional gradient descent as optimizer. To get a sense, see <a href=""https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm"" rel=""nofollow noreferrer"">this</a>. Gradient boosting in practice nearly always uses decision trees as the base learner.</p>
<p>The choice of a loss function depends on various factors, including the nature of the problem. Different loss functions emphasize different aspects of model performance and may be more suitable for specific applications. This is also true for function-level gradient descent. For example in <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"" rel=""nofollow noreferrer"">sklearn's GradientBoostingRegressor</a> ‘absolute_error’ refers to the absolute error of regression and is a robust loss function, as you mentioned.</p>
<p>To understand boosting, on <a href=""https://cs229.stanford.edu/cs229-notes-decision_trees.pdf"" rel=""nofollow noreferrer"">CS229 notes on decision trees</a> see <em>2.2 Boosting</em>, especially <em>2.2.2 Gradient boosting</em>.</p>
","0","Answer"
"78583999","78376338","<p>This usually happens you're trying to read the dataset and it is not using <code>.repeat()</code> and the iterator tries to read beyond the end of the dataset.</p>
<p>This can happen if the number of steps per epoch is set incorrectly.</p>
<p>Make sure to pay attention to your test and validation dataset iterator too!</p>
<p>Calculate it manually:</p>
<ul>
<li><strong>x</strong> = The total number of images in your database.</li>
<li><strong>y</strong> = The number of images you process in one batch.</li>
<li><strong>z</strong> = The number of batches needed to process the entire dataset once.</li>
</ul>
<p>Example:</p>
<ul>
<li><strong>x</strong> = 1000 images in the dataset.</li>
<li><strong>z</strong> = 32 images per batch.</li>
</ul>
<p><strong>Calculation:</strong></p>
<ul>
<li><strong>y</strong> = 1000 / 32</li>
<li><strong>y</strong> = 31.25</li>
</ul>
<p>As you don't have an integer as a result, you can use <code>math.ceil</code> for accurate calculation:</p>
<p>This function rounds up to the nearest integer, ensuring all batches are included in training and validation.</p>
<p><strong>Actual code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>history = model.fit(
       train_generator,
       steps_per_epoch=train_generator.samples // batch_size,
       epochs=10,
       validation_data=valid_generator,
       validation_steps=valid_generator.samples // batch_size
)
</code></pre>
<p><strong>New Version:</strong></p>
<pre class=""lang-py prettyprint-override""><code>history = model.fit(
    train_generator,
    steps_per_epoch=math.ceil(train_generator.samples / batch_size),
    epochs=10,
    validation_data=valid_generator,
    validation_steps=math.ceil(valid_generator.samples / batch_size)
)
</code></pre>
","5","Answer"
"78588372","78328539","<p>Each error might need to be dealt with differently. Let's talk about the authentication issue that you mentioned.</p>
<p>For available models that you can use, check out their Model Hub (more information see <a href=""https://huggingface.co/learn/nlp-course/chapter4/1?fw=pt"" rel=""nofollow noreferrer"">The Hugging Face Hub Guide</a>). You can use any tag to filter down to the specific task you are working on.</p>
<p>So most public models can be used by your <code>pipeline()</code> function, but some require extra step to access it (i.e. accept their community license for example).</p>
<p>To avoid authentication error in the future, your need:</p>
<ol>
<li><p>Create <a href=""https://huggingface.co/settings/tokens"" rel=""nofollow noreferrer"">access token</a> (with read access)</p>
</li>
<li><p>Accept model term of use:
To accept it, go to the model page, in your case, it would be <a href=""https://huggingface.co/google/gemma-2b-it"" rel=""nofollow noreferrer"">https://huggingface.co/google/gemma-2b-it</a> to accept the term.</p>
</li>
<li><p>Log in to huggingface in your code by:</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from huggingface_hub import login
login()

# or in notebook you would need
from huggingface_hub import notebook_login
notebook_login()
</code></pre>
<ol start=""4"">
<li>Add access token to your function</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import pipeline
print(torch.cuda.is_available())

generator = pipeline(&quot;text-generation&quot;, model=&quot;google/gemma-2b-it&quot;, device=&quot;cuda&quot;, token=&quot;your_token&quot;)
</code></pre>
","2","Answer"
"78592649","78396068","<p>The issue is that <code>explainer.shap_values(X_test)</code> will return a 3D DataFrame of shape (rows, features, classes) and to show a bar plot <code>summary_plot(shap_values)</code> requires shap_values to be a list of (rows, features) where the list is: length = number of classes.</p>
<p>For my own purposes, I used the following function which converts your shap_values into the format that you need:</p>
<pre><code>def shap_values_to_list(shap_values, model):
    shap_as_list=[]
    for i in range(len(model.classes_)):
        shap_as_list.append(shap_values[:,:,i])
    return shap_as_list
</code></pre>
<p>Then you can do:</p>
<pre><code>shap_as_list = shap_values_to_list(shap_values, model)
shap.summary_plot(shap_as_list, X_test, plot_type=&quot;bar&quot;)
</code></pre>
<p>You can always add <em>feature_names</em> and <em>class_names</em> to the summary_plot if you need. With my own example I went from having the same kind of interaction plot that you did to the following:</p>
<p><a href=""https://i.sstatic.net/pztgjKYf.png"" rel=""noreferrer"">Example of shap.summary_plot output using shap_values converted to a list of shap_values</a></p>
","5","Answer"
"78635123","78239906","<p>Fundamentally you are looking at converting the matrix multiplication :</p>
<p>$$Y = WX+b$$</p>
<p>De quantization is carried out according to this formula :
Where $S_i, Z_i$ are the scale and zero point of the $X$ (activations in your case) and bias, $b$.</p>
<p>$$Y = S_xS_w(X_q-Z_x)(W_q-Z_w) + S_b(b_q-Z_b)$$</p>
<p>I am not sure your quantization formula looks correct since it does not involve a scale and zero point. But if you give me the formula used to quantize, then I can probably revise it for you. I go into painful detail about this in my blog and it would be worth your while to try to recreate it with your quantization formula :</p>
<p><a href=""https://franciscormendes.github.io/2024/05/16/quantization-layer-details/"" rel=""nofollow noreferrer"">https://franciscormendes.github.io/2024/05/16/quantization-layer-details/</a></p>
","1","Answer"
"78693720","78218890","<p>I was able to do a similar conversion with ai-edge-torch: <a href=""https://github.com/google-ai-edge/ai-edge-torch"" rel=""nofollow noreferrer"">https://github.com/google-ai-edge/ai-edge-torch</a></p>
<pre class=""lang-py prettyprint-override""><code>import ai_edge_torch
import torch
import torchvision


enet = torchvision.models.efficientnet_b0()
sample_input = (torch.randn(1, 3, 224, 224),)

edge_model = ai_edge_torch.convert(enet.eval(), sample_input)
edge_model.export(&quot;enet.tflite&quot;)
</code></pre>
<p>maybe that will help</p>
","0","Answer"
"78744838","78252584","<p>For anyone finding this later, turns out the LAYOUT blocks does not contain any text but it does link to other block children through the Relationships-&gt;Ids . You need to iterate and concanate the LINE blocks to piece together the LAYOUT blocks.</p>
<p>Hope this helps people in the future</p>
","2","Answer"
"78745047","78271090","<pre><code>import tensorflow as tf
from sklearn.model_selection import train_test_split
import csv
import numpy as np


# Read data in from file
with open(&quot;banknotes.csv&quot;) as f:
    reader = csv.reader(f)
    next(reader)

    data = []
    for row in reader:
        data.append({
            &quot;evidence&quot;: [float(cell) for cell in row[:4]],
            &quot;label&quot;: 1 if row[4] == &quot;0&quot; else 0
        })

# Separate data into training and testing groups
evidence = [row[&quot;evidence&quot;] for row in data]
labels = [row[&quot;label&quot;] for row in data]
# convert to numpy arrays
evidence = np.array(evidence)
labels = np.array(labels)
X_training, X_testing, y_training, y_testing = train_test_split(
    evidence, labels, test_size=0.4
)

# Create a sequential neural network
model = tf.keras.models.Sequential()

# Add a hidden layer with 8 units, with ReLU activation
unit in the previous layer
model.add(tf.keras.Input(shape=(4,)))
model.add(tf.keras.layers.Dense(8, activation=&quot;relu&quot;))

# Add output layer with 1 unit, with sigmoid activation
model.add(tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;))

# Train neural network
model.compile(
    optimizer=&quot;adam&quot;,
    loss=&quot;binary_crossentropy&quot;,
    metrics=[&quot;accuracy&quot;]
)
model.fit(X_training, y_training, epochs=20)

# Evaluate how well model performs
model.evaluate(X_testing, y_testing, verbose=2)
</code></pre>
","0","Answer"
"78766172","78151170","<p>I think the problem occurs because an extra index column is created when you reset your DataFrame index. When this happens, <code>Dataset.from_pandas()</code> cannot map the features correctly.</p>
<p>To fix the problem, add <code>preserve_index=False</code> in <code>Dataset.from_pandas()</code> This prevents an index column from being created, which resolves the issue.</p>
","1","Answer"
"78804899","78360982","<p>I got a similar issue while using <strong>Tensorflow version 2.16.2</strong> and
<strong>Keras version 3.4.1</strong> while sub-classing the Keras' Layer class. I resolved it by looking at the error: <strong>&quot;ValueError: Only input tensors may be passed as positional arguments.&quot;</strong></p>
<p>i.e. arguments to call function of sub-class of Layer class should be tensors if they are positional arguments, if they are not tensors they should be keywords arguments.</p>
<p>I believe the following lines in you code should be changed.</p>
<pre><code># Define the custom layer using Lambda (without name argument)
class NotEqual(tf.keras.layers.Layer):
    def __init__(self, name=None):
        super(NotEqual, self).__init__(name=name)

    def call(self, x=some_default_value, y=some_default_value):  # Use keyword arguments 'x' and 'y'
        return tf.math.not_equal(x, y)
</code></pre>
<p>And change the calling to this layer appropriately e.g.</p>
<pre><code>NotEqual()(x=10, y=10)
</code></pre>
<p>You can also try to convert x and y to tensors before applying NotEqual class to them using tf.constant, that also works. e.g.</p>
<pre><code>x = tf.constant(10)
y = tf.constant(10)
NotEqual()(x, y)
</code></pre>
<p>In this method, you don't need to change call method of your NotEqual class and it will be same as you have used.</p>
","1","Answer"
"78823292","78183834","<pre><code>!pip install tensorflow_hub==0.16.1
!pip install tf-keras
</code></pre>
<p>Just include these lines in your notebook before the imports.
I had the same problem, this solved my issue.
Let me know if it worked for you.</p>
","0","Answer"
"78846675","78144820","<p>To extract the <code>train_loss</code> and <code>val_loss</code> from the <code>fit</code> method of an <code>RNNModel</code> in Darts, you can use a custom PyTorch Lightning callback.</p>
<pre><code>from darts import TimeSeries
from darts.models import RNNModel
from pytorch_lightning.callbacks import Callback
import matplotlib.pyplot as plt
</code></pre>
<p>Create a custom callback to record the training and validation loss at the end of each epoch.</p>
<pre><code>class LossRecorder(Callback):
    def __init__(self):
        self.train_loss_history = []
        self.val_loss_history = []

    def on_train_epoch_end(self, trainer, pl_module):
        self.train_loss_history.append(trainer.callback_metrics[&quot;train_loss&quot;].item())
        self.val_loss_history.append(trainer.callback_metrics[&quot;val_loss&quot;].item())
</code></pre>
<p>Initialise the <code>LossRecorder</code> callback.</p>
<pre><code>loss_recorder = LossRecorder()
</code></pre>
<p>Initialise the <code>RNNModel</code> and pass the <code>LossRecorder</code> callback to the <code>pl_trainer_kwargs</code> parameter.</p>
<pre><code>model = RNNModel(
    model=&quot;LSTM&quot;,
    pl_trainer_kwargs={&quot;callbacks&quot;: [loss_recorder]}
)
</code></pre>
<p>Train the model with your training and validation data.</p>
<pre><code>model.fit(train_series, val_series=val_series)
</code></pre>
<p>After training, plot the recorded training and validation losses.</p>
<pre><code>plt.plot(loss_recorder.train_loss_history, label='Train Loss')
plt.plot(loss_recorder.val_loss_history, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
</code></pre>
","1","Answer"
"78893622","78370048","<p>You should use <code>tff.learning.algorithms.build_weighted_fed_avg()</code> instead.</p>
<p>This function takes in input a <code>FunctionalModel</code>: for this reason, you have to call <a href=""https://www.tensorflow.org/federated/api_docs/python/tff/learning/models/functional_model_from_keras"" rel=""nofollow noreferrer""><code>tff.learning.models.functional_model_from_keras()</code></a> to convert your Keras model in a Functional one before creating the trainer</p>
<pre class=""lang-py prettyprint-override""><code>model = tff.learning.models.functional_model_from_keras(
        model_fn(),
        loss_fn=tf.keras.losses.CategoricalCrossentropy(),
        input_spec=(
              tf.TensorSpec(shape=[None, 4], dtype=tf.float32),
              tf.TensorSpec(shape=[None, 3], dtype=tf.float32)
        ),
        metrics_constructor=collections.OrderedDict(
              accuracy=tf.keras.metrics.BinaryAccuracy,
              precision=tf.keras.metrics.Precision,
              recall=tf.keras.metrics.Recall,
            )
  )

fed_learning_model = tff.learning.algorithms.build_weighted_fed_avg(
                      model_fn= model,
                      client_optimizer_fn=client_optimizer,
                      server_optimizer_fn=server_optimizer
                    )
</code></pre>
","2","Answer"
"78938074","78367946","<p>Add the following lines at the beginning of your Python script.</p>
<pre><code>import sys

sys.stdout.reconfigure(encoding='utf-8')

sys.stderr.reconfigure(encoding='utf-8')
</code></pre>
<p>Description:- In Python, you can explicitly set the default encoding to UTF-8, which supports a wide range of characters.</p>
<p>It solved my problem.</p>
","0","Answer"
"78953360","78380376","<p>PCA and t-SNE often perform better when data is normalised. Before applying PCA, make sure the pixel values are scaled from 0 to 1. This helps to centre the data around zero and standardise the scale.</p>
<p>Choosing 60 components for PCA may be suitable, but you might try different numbers to see how they affect the t-SNE results. Depending on the amount of variance captured, fewer components may be better. For example, you can start with 4 and gradually increase it to 8, 16, 24,... and see which one works best.</p>
<p>Also I need to mention that t-SNE has several parameters you can tweak:
Perplexity: effects how t-SNE balances attention between local and global features of the data. (Typical values range between 5 and 50.)
learning_rate: I think the default is 200, but you may need to adjust it as well.
Try these, I hope you'll like the result more.</p>
","0","Answer"
"78994507","78340927","<p>I had the same issue. After trying several tensorflow versions, version = 2.11 finally worked. It seems to be compatible with keras-rl2.</p>
<p>To install the correct version: <code>pip install tensorflow==2.11</code></p>
","1","Answer"
"79009301","78251318","<p>You are using the default value of the parameter <code>n_trials</code> in the <code>study.optimize</code> function, which is <code>None</code>. According to the <a href=""https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize"" rel=""nofollow noreferrer"">documentation</a>, that means that it will stop evaluating configurations when it &quot;times out&quot;.</p>
<p>Optuna's Hyperband implementation is not identical to what was described in the original article. It has <a href=""https://tech.preferred.jp/en/blog/how-we-implement-hyperband-in-optuna/"" rel=""nofollow noreferrer"">some tweaks to make the algorithm compatible with Optuna's inner workings</a>.</p>
<p>You can check the number of successive halving brackets like this: <code>study.pruner._n_brackets</code>. And you can check the allocated budget to each bracket like this: <code>study.pruner._trial_allocation_budgets</code>.</p>
<p>What I am still trying to figure out is how the <code>n_trials</code> plays into defining the number of configurations that will be examined at each bracket.</p>
","1","Answer"
"79035330","78401636","<p>You can use this right after declaring the <code>hub_layer</code>:</p>
<pre><code>hub_layer = tf.keras.layers.Lambda(hub_layer)
</code></pre>
<p>So in this particular case you're left with:</p>
<pre><code>hub_layer = hub.KerasLayer(model_url, input_shape=(224, 224, 3))
hub_layer = tf.keras.layers.Lambda(hub_layer)
</code></pre>
","0","Answer"
"79058198","78266509","<p>you need to export this variable</p>
<pre><code>export MLFLOW_TRACKING_URI=http://127.0.0.1:&lt;PORT of the mlflow server&gt;
</code></pre>
","0","Answer"
"79096079","78337397","<p>You can use GPU acceleration. If you have an NVIDIA GPU, use GPU acceleration python libraries. This is how I train all of my AI models, whether chess related or just fun projects.</p>
","0","Answer"
"79099268","78379820","<p>Ya I see this a lot with Docker &amp; AppImage projects that do not allow custom CA/Certificate integration.  The application is expecting a direct connection to their https endpoint but this is not the case when behind a proxy.</p>
<p>Most likely you have a Firewall or MiTM type Proxy in place.</p>
<p>If you know how to add URL or HTTP Certificate Validation exceptions.
You could try adding these domains:</p>
<pre><code>^([A-Za-z0-9.-]*\.)?lmstudio\.ai\.?/
^([A-Za-z0-9.-]*\.)?huggingface\.co\.?/
^([A-Za-z0-9.-]*\.)?hf\.co\.?/
</code></pre>
<p>Exception url's should get you past most LM Studio guff downloads...</p>
","0","Answer"
"79298530","78251318","<p>To expand on the answer provided by <a href=""https://stackoverflow.com/users/5629527/sole-galli"">Sole Galli</a>, I, too, was initially puzzled by Optuna's implementation of Hyperband.</p>
<p>It is important to note that <code>study.pruner._trial_allocation_budgets</code> does not represent actual budgets but rather <em>relative budgets</em>. These relative budgets are used to determine the probability of selecting a particular bracket for a trial in the <code>optuna.pruners._hyperband.HyperbandPruner._get_bracket_id</code> method.</p>
<p>The algorithm assigns a trial to a bracket randomly by calculating a hash value based on the combination of the study name and trial number. It then takes the modulus of this hash value with respect to <code>sum(_trial_allocation_budgets)</code>. Subsequently, it iterates through the brackets and selects the one whose cumulative relative budget first exceeds the calculated hash value. This approach ensures that brackets are chosen proportionally to their relative budgets divided by the total budget.</p>
<p>The core logic for this resides in the <code>optuna.pruners._hyperband.HyperbandPruner._get_bracket_id</code> method, in particular the following snippet:</p>
<pre class=""lang-py prettyprint-override""><code>n = (  
    binascii.crc32(&quot;{}_{}&quot;.format(study.study_name, trial.number).encode())  
    % self._total_trial_allocation_budget  
)  
for bracket_id in range(self._n_brackets):  
    n -= self._trial_allocation_budgets[bracket_id]  
    if n &lt; 0:  
        return bracket_id
</code></pre>
","1","Answer"
"79348095","78215347","<p>It works on Google Colab (you can try on <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer"">https://colab.research.google.com/</a>):</p>
<pre><code>import torchvision
import torchvision.transforms as transforms
trainset = torchvision.datasets.EMNIST(root=&quot;emnist&quot;,
                                   split=&quot;mnist&quot;,#letters #digits 
                                   train=True,
                                   download=True,
                                   transform=transforms.ToTensor())
</code></pre>
<p>Produces:</p>
<pre><code>Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to emnist/EMNIST/raw/gzip.zip
100%|██████████| 562M/562M [00:10&lt;00:00, 52.9MB/s]
Extracting emnist/EMNIST/raw/gzip.zip to emnist/EMNIST/raw
</code></pre>
","0","Answer"
"79364326","78240714","<p>I'm MAPIE current lead tech.</p>
<p>Sorry for the response delay, we're focused on GitHub issues, but happy to see that our community is on SO as well :)</p>
<p>May I ask which version of MAPIE you are using?</p>
<p>To answer your question:</p>
<ul>
<li>We implement several conformal classification in MAPIE: LAC, Top-k, APS, and RAPS</li>
<li>Among those, only LAC may produce empty sets. The 3 other methods won't, not because we decided so, but by theoretical design</li>
</ul>
<p>Let me know if you need more details.</p>
","0","Answer"
"79472435","78379820","<p>@gooeylewie is right but there's an additional step. In Windows you have to import your model using</p>
<pre><code>lms import &lt;path/to/model.gguf&gt;
</code></pre>
<p>using the absolute path of the downloaded model in a terminal window. Since this procedure can put the model in the appropriate directory, it's not necessary to previously copy it in the LM models folder. The command above will start a short interactive session.</p>
","0","Answer"
"79477719","78171263","<p>this is the Mean Square Error loss function formula</p>
<p><a href=""https://i.sstatic.net/v8JN8K8o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v8JN8K8o.png"" alt=""Mean Square Error"" /></a></p>
<p>Y with hat represents the guess. In here <code>wx = this.yArr[i] - (this.weight * this.xArr[i] + this.bias)</code> you are calculating the error part.</p>
<p>The goal of gradient descent is to iteratively update the slope (m) and bias (b) (also called weights and intercepts) to minimize the loss function. To update the slope (m) and bias (b), we take the partial derivatives of the loss function with respect to <code>m</code> and <code>b</code>. so we calculate their partial derivatives:</p>
<p><a href=""https://i.sstatic.net/GPggR4kQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GPggR4kQ.png"" alt=""partial derivatives of Mean Square error"" /></a></p>
<p>so this part <code>-2 * wx * this.xArr[i]</code> you are asking is the partial derivative Mean Square Error with respect to w (weight)</p>
","0","Answer"
"79502860","78241816","<p>Updating Pycharm to 2024.3 resolved the problem.</p>
","-1","Answer"
"79513852","78161067","<p>Add this line to your metro.config.js</p>
<pre><code>config.resolver.assetExts.push(&quot;bin&quot;);
</code></pre>
<p>The config should look like:</p>
<pre class=""lang-none prettyprint-override""><code>const config = getDefaultConfig(__dirname);

config.resolver.assetExts.push(&quot;bin&quot;);

module.exports = config;
</code></pre>
<p>if you are using expo regenerate the metro.config.js :</p>
<pre><code>npx expo customize metro.config.js
</code></pre>
","0","Answer"
"79590838","78251629","<p>For me that worked:</p>
<pre><code>from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI
</code></pre>
","0","Answer"
"79657732","78379820","<p>I got it to work on MacOS with Zscaler by setting a node.js-specific environment variable:</p>
<pre class=""lang-bash prettyprint-override""><code>export NODE_EXTRA_CA_CERTS=/path/to/ZscalerRootCA.pem
/Applications/LM\ Studio.app/Contents/MacOS/LM\ Studio
</code></pre>
<p>using LM Studio Version 0.3.16</p>
","0","Answer"
"78402799","78402507","<p>I assume you're dealing using <strong>random forest for binary classification</strong>. If not, you may ignore this:</p>
<p>To tackle a similar situation, I once used ROC to find the threshold where Youden's J statistic (<a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2515362/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2515362/</a>) maximises.</p>
","0","Answer"
"78405420","78405026","<p>Any answer will be short, there is a lot of things to do and that's what I think the role of one data scientist is about. Some questions that you can do to yourself to start:</p>
<ul>
<li>How bad is the RMSE in your model? how much do you think is better for you? what is your point of comparison to choose that metric and no other one?</li>
</ul>
<p>I think there are some exploratory data analysis on the data that is missing, for instance you may see that your scatter plot is telling you that may be the linear regression is not the best approach. I have just tried myself with <code>sklearn.ensemble.RandomForestRegressor</code> with the by default hyperparameters, and the behavior of the resulting plot is better and the RMSE has decreased to around 45K, and R2 has moved from 0.88 to 0.94 (see <code>sklearn.metrics.r2_score</code>)</p>
<p>You can, for instance, get the correlation between your vector of features versus your target to see what is happening with the variables before to executing any model and you will see that is a high correlation with &quot;year&quot;:</p>
<pre><code>df_encoded.corr(method=&quot;spearman&quot;)[&quot;price_with_vat_czk&quot;].sort_values()
</code></pre>
<p>You can try exploratory data analysis to tune better your features (statistics is your friend), also, you can try several algorithms, and after that, you can play with different hyperparameters (see <code>sklearn.model_selection.GridSearchCV</code>), then put your models in pipelines and try to get feature importance to reduce the dimensionality on the amount of features you have (see SHAP or GINI to do that)... To sum up, still a lot of things to do...</p>
","1","Answer"
"78405608","78402507","<p>Have you thought about oversampling the positive class? Then you can use any of the &quot;common&quot; models.</p>
<p>Here you have a lot of examples to see and investigate: <a href=""https://github.com/scikit-learn-contrib/imbalanced-learn"" rel=""nofollow noreferrer"">https://github.com/scikit-learn-contrib/imbalanced-learn</a></p>
<p>Regarding the metric, any should do as long as you know how analyse them properly. For example, if you are using accuracy your baseline should be 80%, since is what you would get if you predict always negative (instead of 50% that is the minimum accuracy when you have balanced classes). Personally, for this cases I like to use a mix of precision/recall and the confusion matrix.</p>
<p>Good luck</p>
","0","Answer"
"78405828","78405026","<p>The idea itself is looking pretty good, but now you need to iterate over what you did, it's the funny part :).
Some things you could try out:</p>
<ol>
<li><p>Improve preprocessing:
In general, I would try to understand better the data, plot it, analyse the distributions, see the correlation. Data is friend not food.
You could try normalizing the data (depending on the model you are going to use it matters a lot, a little or nothing).
Try removing outliers if you have too many of them.
If the distributions are skewed (like a normal distribution but &quot;moved&quot; you could try applying a logarithmic transformation</p>
</li>
<li><p>Try out different models:
Here you can try out a lot of stuff. I would recommend starting with a tree-based model, in my personal experience XGboost usually gives really good results, but I invite you to try different variations and research different methods!
If you really want to get a score you could even make your own assembly of models. That is, training for example 5 different models, each with it's strengths and weaknesses, and average (or something) the result of them all. But for this you first need 5 good models and think if combining them will improve or just decrease the RMSE of the best model.</p>
</li>
</ol>
<p>Then you can try all of the boring stuff: use a grid search to change hyperparameters like the learning rate, the regulations terms, and all that stuff, but if you are trying to learn I would say that is secondary.</p>
","1","Answer"
"78406016","78405164","<p>As written, your code tunes the hyperparameters of the final model, but not those of the gbm inside the feature-selection step.  A couple of options:</p>
<ol>
<li><p>Expand the search space to include hyperparameters of the selection-gbm, e.g. <code>feature_selection__estimator__max_depth</code>.</p>
</li>
<li><p>Drop the model step. <code>RFE</code> gives access to a final model on the selected feature set (<code>estimator_</code>) and the methods you're likely to need from it are made available directly from the <code>RFE</code> object (e.g. <code>rfe.predict</code>). So then just modify the names of the hyperparameters as above.</p>
</li>
</ol>
<p>The difference between these approaches is that the first allows a selection-gbm with different hyperparameters from the model-gbm. That'll tend to be more computationally expensive, but more flexible. I personally would be surprised if it provided significant improvement, so I'd suggest the second approach unless you have some time and an inclination to experiment.</p>
","0","Answer"
"78406339","78406265","<p>&quot;The best model is no model&quot; - If your problem can be solved without using a model then do this in this way, if you can't - then use the simplest model which serves your goal - because this will ensure your model is learning (or at most able to learn) that information which matters.</p>
<ol>
<li><p>Interpretability: Yes, you should favor the model with 17 variables.</p>
</li>
<li><p>Overfitting: Yes, larger model is more prone to overfit. The unnecessary variables don't serve any real benefit.</p>
</li>
<li><p>Computational Efficiency: The smaller the model, the more computationally efficient it is.</p>
</li>
<li><p>Feature Importance: See here <a href=""https://stackoverflow.com/q/37627923/6907424"">How to get feature importance in xgboost?</a></p>
</li>
<li><p>Data Quality: You should consider the quality and dimensionality of the dataset in making this decision. Take decision which favors the validation performance.</p>
</li>
<li><p>Information: In general, more variable means more variability is learnt. Variance doesn't necessarily mean information. It can be noise as well. Noise is what doesn't help your model in making decision. For example, in an eye detection model iris color can be treated as noise, but for an iris recognition biometric system it is not. Making your model to learn noise will hurt the performance. Because if you do so your model will also try to find features which are not really necessary, and even worse the model may become confused if it doesn't find those (unnecessary) features in the test data in its expected way.</p>
</li>
<li><p>Consideration: Yes, evaluation is considered more.</p>
</li>
</ol>
","0","Answer"
"78407784","78407668","<p>Well, as always, depends on your data. I would say that if you have enough data, every model (including logistic regression) should be able to &quot;learn&quot; which are the important features and which are not. For example, if you are using sklearn you can try using the <code>feature_imoortance</code> method after training the model, you'll see that you get a result similar to that one that you got in the correlations.</p>
<p>If you have little data I guess it could be a good regularisation technique, since it's a way to remove noise.</p>
<p>If possible I would try to always use all features without taking it to the extreme. That is, if I'm trying to predict, for example, the prices of houses and I have a feature that is how fast did each marathoner ran the last marathon I would definitely drop that column.</p>
","1","Answer"
"78408243","78403537","<p>In Azure ML Designer v1, which is <strong>Classic prebuilt</strong>, loading the model to build a pipeline is not supported.</p>
<p><img src=""https://i.imgur.com/WiLdbcI.png"" alt=""enter image description here"" /></p>
<p>Only in Designer v2, which is <strong>Custom</strong>, is possible. Below is Designer v2, where you can add your model.</p>
<p><img src=""https://i.imgur.com/mZy7aXv.png"" alt=""enter image description here"" /></p>
<p>First, register the best model from the Auto ML training job.</p>
<p>Click on <strong>Register model</strong> after the training job is completed.</p>
<p><img src=""https://i.imgur.com/L4WZ9Nu.png"" alt=""enter image description here"" /></p>
<p>In the next window, the best model is automatically selected.</p>
<p><img src=""https://i.imgur.com/zIAc7lt.png"" alt=""enter image description here"" /></p>
<p>Then, click next, give a name to the model, and click on register.</p>
<p><img src=""https://i.imgur.com/rg9Y185.png"" alt=""enter image description here"" /></p>
<p>Next, when creating a Designer v2 pipeline, your registered model appears in the left <strong>model</strong> tab as shown below.</p>
<p><img src=""https://i.imgur.com/3F5babJ.png"" alt=""enter image description here"" /></p>
<p>Use it further in your pipeline.</p>
","0","Answer"
"78408621","78408313","<p>It is very difficult to point to a single thing as an issue with the code without knowing more about the training, validation and testing data sets.</p>
<p>I would say, though, that it looks like you have <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow noreferrer"">overfitting</a> in your model by epoch 8. This can be seen by looking at the <code>val_loss</code> and <code>val_accuracy</code> parameters. You should expect that validation loss <em>decreases</em> over the complete training loop while validation accuracy <em>increases</em>. Once you see that either is moving in the opposite direction you might have overfitting. This would also explain some (if not all) of your poor performance on the test set.</p>
<p>One possible solution to this is to just stop fitting the model once you hit overfitting, for example try setting the epoch parameter to something lower than 15, maybe: <code>epochs=8</code>.</p>
<p>Another solution could be to set a different learning rate with a specified optimizer, see <a href=""https://stackoverflow.com/questions/59737875/keras-change-learning-rate"">this answer</a> for a more thourough explanation on this topic.</p>
","-1","Answer"
"78408664","78408313","<p>You are getting overfitted. You could think of overfitting as if the model is so expressive that is learning by heart all the training dataset, and not really learning the patterns of the images.</p>
<p>An example I like is when you have little time to study for a test. What do you do here? Well, you learn by heart all the previous tests. You will do good if they ask you something similar to that, but if they ask you something different you will do horrible.</p>
<p>So, how can you tell when you are overfitted? You get really good results in the training dataset but catastrophic results in the validation.</p>
<p>Now the important part, how to not overfit? Well, a lot of options here. For starter, you could try changing the model to a smaller one, to one with fewer parameters. You could change data augmentation on your data (although it seems okay, but in some cases one data augmentation might kill the model. I would add them one by one and check see if they improve the result or not. For this you could train on only a sample dataset so it doesn't take that long). You could increase the dropout or use less fewer neurones in the penultimate layer, try 128 for example. Lastly, you could try early stopping</p>
<p>With the 128 neurones I mean instead of this right here that you have:</p>
<pre class=""lang-py prettyprint-override""><code>    model = keras.Sequential([
    hub_layer,
    layers.Dropout(0.2),  # Lower dropout rate
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),  # Batch normalization
    layers.Dropout(0.5),  # Dropout
    layers.Dense(9, activation='softmax')
])
</code></pre>
<p>Try out something like this:</p>
<pre class=""lang-py prettyprint-override""><code>    model = keras.Sequential([
    hub_layer,
    layers.Dropout(0.2),  
    layers.Dense(128, activation='relu'), # see the decrease in parameters
    layers.BatchNormalization(),  
    layers.Dropout(0.5),  # Dropout
    layers.Dense(9, activation='softmax')
])
</code></pre>
<p>I believe with something like this and early stopping it should do. I really recommend you checking out the <a href=""https://keras.io/api/callbacks/"" rel=""nofollow noreferrer"">Keras Callbacks</a>, they have a lot of useful stuff like the EarlyStopping or ReduceLROnPlateau</p>
","-1","Answer"
"78409153","78404768","<p>I spend to solve this issue around 4 hours. I solved this problem to <strong>upgrading gpu</strong>. And I think problem in gpu, <strong>like T4 can't compute for this case</strong>. So I changed gpu <strong>T4 --&gt; L4</strong>.</p>
","0","Answer"
"78409411","78407603","<p>Found a walkaround this. Got to create a wrapper around <code>XGBClassifier</code>, like so:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.base import BaseEstimator

class XGBHierarchicalClassifier(BaseEstimator):
    def __init__(self, **kwargs):
        self.clf = XGBClassifier(**kwargs)
        self.label_map = {}
        self.inverse_label_map = {}
        self.label_counter = 0

    def fit(self, X, y):
        # Map string labels to numeric values
        unique_labels = np.unique(y)
        for label in unique_labels:
            if label not in self.label_map:
                self.label_map[label] = self.label_counter
                self.inverse_label_map[self.label_counter] = label
                self.label_counter += 1
        
        y_numeric = np.array([self.label_map[label] for label in y])

        self.clf.fit(X, y_numeric)
        self.classes_ = np.unique(y)
        return self

    def predict(self, X):
        return np.array([self.inverse_label_map[label] for label in np.argmax(self.predict_proba(X), axis=1)])

    def predict_proba(self, X):
        return self.clf.predict_proba(X)
</code></pre>
<p>Usage:</p>
<pre class=""lang-py prettyprint-override""><code>base3 = XGBHierarchicalClassifier()

clf = HierarchicalClassifier(
    base_estimator=base3,
    class_hierarchy=class_hierarchy,
    )

with multi_labeled(y_test, y_pred, clf.graph_) as (y_test_, y_pred_, graph_):
  h_fbeta = h_fbeta_score(
      y_test_, y_pred_, graph_, )

print(&quot;h_fbeta_score: &quot;, h_fbeta)
h_fbeta_score:  0.8501118568232662
</code></pre>
","1","Answer"
"78411105","78411015","<p>In the picture you can see a <a href=""https://i.sstatic.net/bZcT5z2U.png"" rel=""nofollow noreferrer"">General overfitting graph</a>. So, what would you say?</p>
<p>Usually the train and validation loss seem to converge to the same place until they don't. Once that they start diverging the train loss continue decreasing and the validation loss increases or keeps the same. So, answering your question, yes, I would say you are overfitting from the iteration ~70</p>
","1","Answer"
"78411493","78409561","<p>From a purely computer science perspective you might want to look into <a href=""https://docs.scipy.org/doc/scipy/reference/sparse.html"" rel=""nofollow noreferrer""><strong>sparse matrices</strong></a>. While indeed encoding something as one-hot in a naive way will explode your memory (as it requires 4 bytes * num_rows * num_values to store) if you instead store it in a sparse format, you only need to remember index of the &quot;1&quot;, and all the extra 0s are not stored, so if num_values is large, this will save you (num_values - 1)/num_values of the memory.</p>
","1","Answer"
"78416173","78415660","<p>I found an answer. An issue was that I was logging to torch-lightning the tensor object containing the accuracy. and as it is multiclass accuracy computed per batch, when I had batch of unshufled data for example 64 cats, it gave 0% acc for dogs for this batch, and then averaged the accuracy. therefore i got 40% accuracy. To fix this now I pass an actual object of accuracy so lightning will accumulate per class acuraccy across the whole run and give the accuracy correctly.</p>
<p>Correct code:</p>
<pre><code>self.accuracy(predictions_softmax, labels)
self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
self.log('train_acc', self.accuracy, on_step=True, on_epoch=True, prog_bar=True)
</code></pre>
","0","Answer"
"78416936","78416214","<blockquote>
<p><em>How do I actually set the number of trees, and related to that, what does n_estimators do?</em></p>
</blockquote>
<p>Pass <code>n_estimators</code> or one of its aliases (<a href=""https://lightgbm.readthedocs.io/en/latest/Parameters.html#num_iterations"" rel=""nofollow noreferrer"">LightGBM docs</a>).</p>
<p><code>n_estimators</code> in LightGBM's <code>scikit-learn</code> interface (classes like <code>LGBMClassifier</code>) controls the number of <strong>boosting rounds</strong>.</p>
<p>For all tasks other than multiclass classification, LightGBM will produce 1 tree per boosting round.</p>
<p>For multiclass classification, LightGBM will train 1 tree <strong>per class</strong> in each boosting round.</p>
<p>So, for example, if your target has 5 classes, then training with <code>n_estimators=3</code> and no early stopping will produce 15 trees.</p>
<blockquote>
<p><em>how do I interpret the separate trees, with their ordering, 0, 1, 2, etc... how can I &quot;simulate&quot; the LightGBM inference process?</em></p>
</blockquote>
<p>Each consecutive grouping of <code>{num_classes}</code> trees corresponds to one <em>boosting round</em>. They are ordered by target class.</p>
<p>Given an input <code>X</code>, LightGBM's prediction that <code>X</code> belongs to class <code>i</code> will be given by:</p>
<pre><code>tree_{i}(X) +
tree_{i+num_classes}(X) +
tree_{i+num_classes*2}(X)
... etc/, etc.
</code></pre>
<p>So, for example, consider 5-class multiclass classification and 3 boosting rounds, using LightGBM's built-in multiclass objective.</p>
<p>LightGBM's score for a sample <code>x</code> belonging to the first class will be the sum of the corresponding leaf values from the 1st, 6th, and 11th trees.</p>
<p>Here's a minimal, reproducible example with <code>lightgbm==4.3.0</code> and Python 3.11.</p>
<pre class=""lang-py prettyprint-override""><code>import lightgbm as lgb
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# generate multiclass dataset with 5 classes
X, y = make_blobs(n_samples=1_000, centers=5, random_state=773)

# fit a small multiclass classification model
clf = lgb.LGBMClassifier(n_estimators=3, num_leaves=4, seed=708)
clf.fit(X, y)

# underlying model has 15 trees
clf.booster_.num_trees()
# 15

# but just 3 iterations (boosting rounds)
clf.n_iter_
# 3

# just plot the trees for the first class
lgb.plot_tree(clf, tree_index=0)
lgb.plot_tree(clf, tree_index=5)
lgb.plot_tree(clf, tree_index=10)
plt.show()
</code></pre>
<p>This will produce tree diagrams that look like this:</p>
<p><a href=""https://i.sstatic.net/FZ2HqkVo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FZ2HqkVo.png"" alt=""enter image description here"" /></a></p>
<p>Try generating the raw predictions for the first row of the training data.</p>
<pre class=""lang-py prettyprint-override""><code>clf.predict(X, raw_score=True)[0,]
# array([-1.24209749, -1.90204682, -1.9020346 , -1.89711144, -1.23250193])
</code></pre>
<p>You could manually calculate which leaf node that sample belongs to in each tree and add those leaf values. That number should match the first item in the raw score above (in this example, <code>-1.24209749</code>).</p>
<p>If you have <code>pandas</code> available, you might find it easier to dump the tree structure to a dataframe and work with it there.</p>
<pre class=""lang-py prettyprint-override""><code>model_df = clf.booster_.trees_to_dataframe()

# trees relevant to class 0
relevant_trees = [0, 5, 10]

# figure out which leaf index each sample falls into
leaf_preds = clf.predict(X, pred_leaf=True)[0,]

# subset that to only the trees relevant to class 0
relevant_leaf_ids = [
    f&quot;0-L{leaf_preds[0]}&quot;,
    f&quot;5-L{leaf_preds[5]}&quot;,
    f&quot;10-L{leaf_preds[10]}&quot;
]

# show the values LightGBM would predict from each tree
model_df[
  model_df[&quot;tree_index&quot;].isin(relevant_trees) &amp;
  model_df[&quot;node_index&quot;].isin(relevant_leaf_ids)
][[&quot;tree_index&quot;, &quot;node_index&quot;, &quot;value&quot;]]
</code></pre>
<pre><code>    tree_index node_index     value
5            0       0-L3 -1.460720
38           5       5-L3  0.119902
73          10      10-L3  0.098720       
</code></pre>
<p>Those 3 values add to <code>-1.242098</code>, almost identical to the score predicted by <code>clf.predict(X, raw_score=True)</code> <em>(just different by numerical precision lost from printing)</em>.</p>
","1","Answer"
"78418778","78418741","<p>It seems like the variable/object model has been redefined as a numpy.ndarray after being trained.
Might be better to pass it to this method and ensure there has been no re-assignment for that variable.</p>
<p>Edit: Also, using type hints for the method arguments will help you find the error even before running the code. The type hints are VERY useful in such cases as well.</p>
","0","Answer"
"78419266","78419199","<p>The issue is that you're using a <strong>logistic regression model</strong>, a probabilistic model. The threshold you're getting is <strong>not</strong> on your <strong>input feature</strong>, but the probability output by the <strong>logistic regression model</strong>.</p>
<p>Do it <strong>without</strong> machine learning models:</p>
<ol>
<li>Sort the data by feature values.</li>
<li>For each unique value, calculate the precision of classifying all points with a feature value <code>&lt;=</code> to the current value as <strong>class 0</strong> and all points with a feature value <code>&gt;</code> current value as <strong>class 1</strong>.</li>
<li>The feature value that gives the highest precision is your threshold.</li>
</ol>
<p>Implementation:</p>
<pre><code>import numpy as np
import pandas as pd
data = pd.read_csv('data.txt', header=None, names=['feature', 'label'])
data = data.sort_values('feature')
precisions= []
threshold = data['feature'].unique()
for threshold in thresholds:
    predicted_labels = np.where(data['feature'] &lt;= threshold, 0, 1)
    tp = np.sum((predicted_labels == 1) &amp; (data['label'] == 1))
    fp = np.sum((predicted_labels == 1) &amp; (data['label'] == 0))
    precision = tp / (tp + fp) 
    precisions.append(precision)
max_precision_index = np.argmax(precisions)
best_threshold = thresholds[max_precision_index]
print(&quot;Best threshold: {}&quot;.format(best_threshold))
</code></pre>
","1","Answer"
"78420626","78420559","<p>RFECV eliminates features one by one solely based on their importance. In theory, each model using <em>k &lt; K</em> features can be scored by multiple metrics. However, once all possible number of features have been tested, <code>RFECV</code> picks the model with the greatest <code>mean_test_score</code>, which means that it relies on a single figure to pick the best number of features.</p>
<p>How RFECV works, short version:</p>
<p>RFE removes features based on &quot;feature_importance&quot;. However you still need to select <code>k</code> parameter - number of features to use. You can do this by evaluating a scoring function on a test dataset for all possible <code>k</code> values. And then select <code>k</code> that gives best performance on Test dataset. And this is how you left with final <code>k</code> features. However to make this process more reliable you can repeat this process several times on CV-folds. Then average scores from test datasets for all folds.</p>
<p>How RFECV works, long version:</p>
<ol>
<li>First it breaks the data using provided CV split and thus returning pairs (X_train, X_test) for each fold.</li>
<li>For each pair of (X_train, X_test) it applies RFE (obviously for each cv-pair RFE results might be different). So on each cv-pair on each step RFE selects <code>k</code> best features according to <code>feature_importance</code> if fitting given <code>X_train</code>. Then on each step for selected features it evaluates model on <code>X_test</code> using scorer and returns a score for the selected <code>k</code> features.</li>
<li>Now for each cv-fold (X_train, X_test) and each <code>k</code> value you have a score on the test fold. This scores gets averaged across all cv-folds. So you finally get pairs: (k_features, cv_test_score)</li>
<li>Now for each <code>k</code> - features selected by RFE - we have an averaged single score value. This scores are sorted and used to select <code>k</code> with highest score. You get <code>best-k</code></li>
<li>Finally we run RFE again but this time on the whole X dataset targeting RFE to return exactly <code>best-k</code> features.</li>
</ol>
<p>To conclude, RFECV needs a single number to decide which <code>k</code> value is the best. So it can't use several scoring functions unless they are combined somehow via custom scorer.</p>
<p>Answering your question: you need to score rfe outputs separately if you want to get 3 separate scoring functions outputs.</p>
<p>Also to note, because <code>f1</code> already is a mixture of <code>precision</code> and <code>recall</code> it might be worth to run RFECV with <code>scoring='f1'</code>.</p>
","1","Answer"
"78420674","78420048","<p>the <code>model</code> variable is not defined anywhere in the code and is certainly not in the <code>predict()</code> function where it is called.</p>
<p>in fact, there is a comment:</p>
<pre><code># Assume `model` is already defined and trained
</code></pre>
<p>presumably it is in the global scope (poor practice) but omitted in the question.</p>
<p>so it looks like this should either:</p>
<ol>
<li>be created inside the <code>predict()</code> function.
or</li>
<li>passed as an argument into the same function.</li>
</ol>
<p>As a side note, the code is also missing <code>cv</code> which one might assume comes from the <code>opencv</code> module. In any regard, the code will not work without this being defined too...</p>
","0","Answer"
"78421095","78420651","<p>This is just a quick implementation for the above task, and many optimizations are possible, which can be explored later, but at first glace here is the function:</p>
<pre><code>def BIO_converter(r, entities):
    to_replace = {} # needed to maintain all the NER to be replaced
    for i in entities:
        sub = r[i['start']+1:i['end']+2].split(' ') # 1 indexed values in entities
        if len(sub) &gt; 1:
            vals = [f&quot;B-{i['category']}&quot;] + ([f&quot;I-{i['category']}&quot;] * (len(sub)-1))
        else:
            vals = [f&quot;B-{i['category']}&quot;]

        to_replace = to_replace | dict(zip(sub,vals))

    r = r.split(' ')
    r = [to_replace[i] if i in to_replace else 'O' for i in r ]
    return ' '.join(r)

js = {
        &quot;request&quot;: &quot;I want to fly to New York on the 13.3&quot;,
        &quot;entities&quot;: [
          {&quot;start&quot;: 16, &quot;end&quot;: 23, &quot;text&quot;: &quot;New York&quot;, &quot;category&quot;: &quot;DESTINATION&quot;},
          {&quot;start&quot;: 32, &quot;end&quot;: 35, &quot;text&quot;: &quot;13.3&quot;, &quot;category&quot;: &quot;DATE&quot;}
        ]
      }
BIO_converter(js['request'], js['entities'])
</code></pre>
<p>Should output:</p>
<pre><code>O O O O O B-DESTINATION I-DESTINATION O O B-DATE
</code></pre>
","1","Answer"
"78423446","78423364","<p>It sounds like the categories you'd like to use (<code>&quot;A&quot;</code>, <code>&quot;B&quot;</code>, <code>&quot;C&quot;</code>, and <code>&quot;Unknown&quot;</code>) are known up front. In this case, <a href=""https://docs.pola.rs/py-polars/html/reference/api/polars.datatypes.Enum.html"" rel=""nofollow noreferrer""><code>pl.Enum</code></a> should be used.</p>
<p>Let us consider the following sample data.</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl

df_train = pl.DataFrame({
    &quot;cat&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;],
})

df_test = pl.DataFrame({
    &quot;cat&quot;: [&quot;A&quot;, &quot;X&quot;, &quot;C&quot;],
})
</code></pre>
<p>Based on the training data, a <code>pl.Enum</code> datatype including an additional category <code>&quot;Unknown&quot;</code> can be created as follows.</p>
<pre class=""lang-py prettyprint-override""><code>enum_dtype = pl.Enum(
    df_train.get_column(&quot;cat&quot;).unique().drop_nulls().append(pl.Series([&quot;Unknown&quot;]))
)
</code></pre>
<p><em>Note.</em> <code>.drop_nulls()</code> is used (although the data in this example doesn't contain nulls) as precautionary measure since Enum categories cannot contain null values.</p>
<p>Now, the categorical column in the training dataframe can be casted directly to the <code>pl.Enum</code> datatype.</p>
<pre class=""lang-py prettyprint-override""><code>df_train.with_columns(pl.col(&quot;cat&quot;).cast(enum_dtype))
</code></pre>
<pre><code>shape: (3, 1)
┌──────┐
│ cat  │
│ ---  │
│ enum │
╞══════╡
│ A    │
│ B    │
│ C    │
└──────┘
</code></pre>
<p>For the test dataframe, this would fail as <code>&quot;D&quot;</code> is not a known category in the <code>pl.Enum</code> datatype. Here, using <code>strict=False</code> can be used to cast unknown categories to <code>None</code>. Subsequently, missing values can be replaced with the category <code>&quot;Unknown&quot;</code>.</p>
<pre class=""lang-py prettyprint-override""><code>df_test.with_columns(
    pl.col(&quot;cat&quot;).cast(enum_dtype, strict=False).fill_null(&quot;Unknown&quot;)
)
</code></pre>
<pre><code>shape: (3, 1)
┌─────────┐
│ cat     │
│ ---     │
│ enum    │
╞═════════╡
│ A       │
│ Unknown │
│ C       │
└─────────┘
</code></pre>
<p><strong>Edit.</strong> If the test data already contains missing values that should be kept as such, unknown values can be explicitly replaced with <code>&quot;Unknown&quot;</code> before (strict) casting.</p>
<pre><code>df_test.with_columns(
    pl.when(
        pl.col(&quot;cat&quot;).is_not_null() &amp; ~pl.col(&quot;cat&quot;).is_in(enum_dtype.categories)
    ).then(
        pl.lit(&quot;Unknown&quot;)
    ).otherwise(
        pl.col(&quot;cat&quot;)
    ).cast(enum_dtype)
)
</code></pre>
","6","Answer"
"78423584","78419755","<p>In the case of <em>Python 3.12/tensorflow 2.16.1</em>, the code added to <code>tensorflow/__init__.py</code> in <a href=""https://stackoverflow.com/questions/73496946/vscode-autocomplete-and-suggestion-intellisense-doesnt-work-for-tensorflow-an"">this question</a> needs to be changed to</p>
<pre><code>if _typing.TYPE_CHECKING:
 from tensorflow_estimator.python.estimator.api._v2 import estimator as estimator
 from keras._tf_keras import keras
 from keras._tf_keras.keras import losses
 from keras._tf_keras.keras import metrics
 from keras._tf_keras.keras import optimizers
 from keras._tf_keras.keras import initializers
</code></pre>
<p><a href=""https://i.sstatic.net/HaGQ3aOy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HaGQ3aOy.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"78426060","78425666","<p>The nuber of parameters can be computed with this formula:</p>
<p>LSTM parameter number = 4 × ((x + h) × h + h)</p>
<p>where x is the dimension of the input vector
and h is the size of the output space.</p>
<p>See this link for an explanation: <a href=""https://www.kaggle.com/code/kmkarakaya/lstm-understanding-the-number-of-parameters"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/kmkarakaya/lstm-understanding-the-number-of-parameters</a></p>
<p>N_u=1, N_x=1, means the ouput space size is 1, the input space size is 1, so P=12</p>
<p>N_u=1, N_x=2, means you have changed the input space size (x) to 2,
so using the formula you get 16.</p>
<p>N_u=2, Nx_1, means you have doubled the output space. Again using the formula you get 32</p>
<p>The formula holds also for N_u=2, N_x=2.</p>
<p>I did not find a paper used as a reference for LSTM Keras implementation, but maybe this source code explanation could be of help: <a href=""https://blog.softmaxdata.com/keras-lstm/"" rel=""nofollow noreferrer"">https://blog.softmaxdata.com/keras-lstm/</a></p>
<p>Please note the source code link is not working. Use this instead: <a href=""https://github.com/keras-team/keras/blob/master/keras/src/layers/rnn/lstm.py"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/master/keras/src/layers/rnn/lstm.py</a></p>
","1","Answer"
"78427628","78423313","<p>Yes, the dataset sometimes doesn't suit the algorithm you use to build the model.
For eg. if You are using continuous data and implementing classification techniques then there is a very high possibility that the evaluation metrics might not suit well in any order.
For that, you need to plot some graphs and heat maps to understand what type of dataset you have.
Make sure to change your approach if single one doesn't work.
To answer your question further, please add more details like what was your approach</p>
","0","Answer"
"78428201","78418098","<p>Although readily available datasets on this exact topic might be missing, you can try to scrape existing digital postcard websites. As an example, you can try to develop a scrapper to search for &quot;birthday postcard text&quot; on Google and extract the text from the top 20 websites. You can refer to <a href=""https://nanonets.com/blog/web-scraping-with-python-tutorial/"" rel=""nofollow noreferrer"">this guide</a> to get started with scraping in Python.</p>
<p>If you are still interested in curating your own dataset, keep in mind the following steps and make sure you have sufficient expertise in your team to get the best out of this data since dataset creation is very extensive:</p>
<ul>
<li><p>Clearly define the purpose of your datase that fuels your drive and enables you to reach your goals</p>
</li>
<li><p>Identify the key variables or characteristics related to your purpose that you will be collecting data on. For example, if your case can be about the relationship between occasion (e.g. birthday, wedding) and postcard text, your variables would be occasion and postcard text.</p>
</li>
<li><p>Make a plan for how you will gather the data, including the method (surveys, observations, etc.), number of participants needed, how you will select participants, and the tool you'll use.</p>
</li>
<li><p>Gather the first batch of data following your plan in iterations.</p>
</li>
<li><p>Review the first collected data carefully, fixing any errors and removing duplicates. Organize it neatly. Then iterate over this gathering and reviewing steps until a sufficient amount of data is collected.</p>
</li>
</ul>
<p>You can also refer to the following guidelines:
<a href=""https://www.data-mania.com/blog/creating-datasets/"" rel=""nofollow noreferrer"">https://www.data-mania.com/blog/creating-datasets/</a>
<a href=""https://towardsdatascience.com/the-definite-guide-for-creating-an-academic-level-dataset-with-industry-requirements-and-6db446a26cb2"" rel=""nofollow noreferrer"">https://towardsdatascience.com/the-definite-guide-for-creating-an-academic-level-dataset-with-industry-requirements-and-6db446a26cb2</a>
<a href=""https://pcsocial.medium.com/how-to-create-a-dataset-a-comprehensive-step-by-step-interactive-guide-c1271a5f18e5"" rel=""nofollow noreferrer"">https://pcsocial.medium.com/how-to-create-a-dataset-a-comprehensive-step-by-step-interactive-guide-c1271a5f18e5</a></p>
","0","Answer"
"78432218","78432197","<ul>
<li>Retrieval-Augmented Generation (RAG) models are slower as compared to Large Language Models (LLMs) due to an extra retrieval step.</li>
<li>Since RAG models search a database for relevant information, which can be time-consuming, especially with large databases, it is tend to be slower. Versus LLMs respond faster as they rely on pre-trained information and skip the said database retrieval step.</li>
<li>You must also note that LLMs may lack the most current or specific information compared to RAG models, which usually access external data sources and can provide more detailed responses using the latest information.</li>
<li>Thus, Despite being slower, RAG models have the advantage in response quality and relevance for complex, information-rich queries. Hope I am able to help.</li>
</ul>
<p>Please mark this as a answer, if you think this sets a perfect context for your case.</p>
","1","Answer"
"78433457","78433029","<p>I resolved the issue doing this :</p>
<ol>
<li>Create a custom score function (It is important to know that the API should be <code>func(estimator, X, y)</code> where X will be the data matrix passed by gridsearchcv and y the corresponding output)</li>
</ol>
<pre><code>    def scorer(estimator, X, y):
        predicted_class = estimator.predict(X)
        return accuracy_score(predicted_class, train_new_y[y.index])
</code></pre>
<ol start=""2"">
<li>Passing this function as a parameter of GridSearchCV</li>
</ol>
<pre><code>    olm = RegressionClassifier()
    params = {
        &quot;alpha&quot;:np.linspace(0,1,10),
        &quot;n_components&quot;:np.linspace(50, 200, 10)
    }
    poly_cv = GridSearchCV(olm, params, scoring=scorer, verbose=3)
</code></pre>
","0","Answer"
"78434439","78427741","<p>I managed to find a solution with the help of GitHub copilot. If anybody is interested, here is a code that</p>
<ol>
<li>Loops through all images in a coco data set</li>
<li>Select pixels that belong to Category 1 but not to Category 2 or 3 (if Categories 2 and 3 exist for the image)</li>
<li>Plots the filtered annotation layer</li>
<li>And returns a list with the pixel coordinates, RGB values, and filename</li>
</ol>
<p>The <code>category_ids</code> are taken from the name of the category (input to function).</p>
<pre><code>import matplotlib.pyplot as plt

def get_rgb_values(coco, img_folder, cat1_name, cat2_name, cat3_name):
    # Initialize list to store data
    data = []

# Get all categories
categories = coco.loadCats(coco.getCatIds())

# Get category IDs for the given names
cat1_id = next((cat['id'] for cat in categories if cat['name'] == cat1_name), None)
cat2_id = next((cat['id'] for cat in categories if cat['name'] == cat2_name), None)
cat3_id = next((cat['id'] for cat in categories if cat['name'] == cat3_name), None)

if cat1_id is None or cat2_id is None or cat3_id is None:
    raise ValueError('Category not found')

# Get all images
img_ids = coco.getImgIds()

for img_id in img_ids:
    # Get image info
    img_info = coco.loadImgs(img_id)[0]
    filename = img_info['file_name']

    # Load image
    img = np.array(Image.open(img_folder + '/' + filename))

    # Get annotations of category 1
    ann_ids_cat1 = coco.getAnnIds(imgIds=img_id, catIds=cat1_id)
    anns_cat1 = coco.loadAnns(ann_ids_cat1)

    # Get mask for category 1
    mask_cat1 = np.zeros((img_info['height'], img_info['width']))
    for ann in anns_cat1:
        mask_cat1 = np.logical_or(mask_cat1, coco.annToMask(ann))

    # Get annotations of category 2
    ann_ids_cat2 = coco.getAnnIds(imgIds=img_id, catIds=cat2_id)
    anns_cat2 = coco.loadAnns(ann_ids_cat2)

    # Get annotations of category 3
    ann_ids_cat3 = coco.getAnnIds(imgIds=img_id, catIds=cat3_id)
    anns_cat3 = coco.loadAnns(ann_ids_cat3)

    # If category 2 or 3 exists, modify the mask for category 1
    if anns_cat2 or anns_cat3:
        # Get mask for category 2
        mask_cat2 = np.zeros((img_info['height'], img_info['width']))
        for ann in anns_cat2:
            mask_cat2 = np.logical_or(mask_cat2, coco.annToMask(ann))

        # Get mask for category 3
        mask_cat3 = np.zeros((img_info['height'], img_info['width']))
        for ann in anns_cat3:
            mask_cat3 = np.logical_or(mask_cat3, coco.annToMask(ann))

        # Modify mask for pixels in category 1 and not in category 2 or 3
        mask_cat1 = np.logical_and(mask_cat1, np.logical_not(mask_cat2))
        mask_cat1 = np.logical_and(mask_cat1, np.logical_not(mask_cat3))

    # Get RGB values and positions
    positions = np.argwhere(mask_cat1)
    red = img[:,:,0][mask_cat1]
    green = img[:,:,1][mask_cat1]
    blue = img[:,:,2][mask_cat1]

    # Add to data list
    for (y, x), r, g, b in zip(positions, red, green, blue):
        data.append({'filename': filename, 'Red': r, 'Green': g, 'Blue': b, 'X': x, 'Y': y})

    # Display the image with the mask overlay
    plt.imshow(img)
    plt.imshow(mask_cat1, alpha=0.5, cmap='jet')
    plt.show()

# Create DataFrame from list of dictionaries
df = pd.DataFrame(data)

return df
</code></pre>
","0","Answer"
"78434544","78434421","<p>According to <a href=""https://stackoverflow.com/questions/40207422/binary-numbers-instead-of-one-hot-vectors"">this</a> answer, with the bit-based encoding, now the 2nd city and the 4th city share the feature x0, also the 3rd city and the 4th city share the feature x1, and it affects our predictions.</p>
","2","Answer"
"78434644","78434627","<p>You usually want to save a machine learning model as a Pickle (. pkl) file.</p>
<pre><code># Saving the model
import pickle
with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)
</code></pre>
<p>This can then be loaded again later.</p>
<pre><code>with open('model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)
</code></pre>
<p>There are other methods depending on how you trained your model.</p>
","0","Answer"
"78436177","78404705","<p>I think, want you need is masked tensor. Please have a look to <a href=""https://pytorch.org/docs/stable/masked.html"" rel=""nofollow noreferrer"">this documentation</a>. Unfortunately, it does not support <code>matmul</code> for now, but I believe it will be added in future, you may try to open GitHub issue to speed up the process. However, it supports <code>mul</code> and <code>sum</code> operations, you may try to use those to solve your problem.</p>
<p>To create masked tensors you can do following (adapting the code from Rehan Ahmed):</p>
<pre class=""lang-py prettyprint-override""><code>mask_row_ids = torch.arange(mask_indices.shape[0]).unsqueeze(1).repeat(1, mask_indices.shape[1])

mask_tensor = torch.ones(inp_tensor.shape).bool()
mask_tensor[mask_row_ids, mask_indices] = False

inp_tensor_masked = torch.masked.masked_tensor(inp_tensor, mask_tensor, requires_grad=True)

mask_tensor = torch.ones(my_tensor.shape).bool()
my_tensor_masked = torch.masked.masked_tensor(my_tensor, mask_tensor, requires_grad=True)
</code></pre>
<p>This will give you following tensors:</p>
<pre class=""lang-py prettyprint-override""><code>MaskedTensor(
  [
    [  0.7860,   0.1115,       --,   0.6524,   0.6057,   0.3725,   0.7980,       --],
    [  1.0000,   0.1115,       --,   0.6524,   0.6057,   0.3725,       --,   1.0000]
  ]
)

MaskedTensor(
  [
    [  0.8823,   0.9150,   0.3829],
    [  0.9593,   0.3904,   0.6009],
    [  0.2566,   0.7936,   0.9408],
    [  0.1332,   0.9346,   0.5936],
    [  0.8694,   0.5677,   0.7411],
    [  0.4294,   0.8854,   0.5739],
    [  0.2666,   0.6274,   0.2696],
    [  0.4414,   0.2969,   0.8317]
  ]
)
</code></pre>
<p>Then you may try to make them to be the same size, by transposing and repeating, and use <code>mul</code> and <code>sum</code> functions.</p>
<p><em>Note:</em> <code>numpy</code> also has masked array idea, please have a look <a href=""https://numpy.org/doc/stable/reference/generated/numpy.ma.dot.html"" rel=""nofollow noreferrer"">here</a>. But the way it performs dot product, is again by replacing with 0s, please see the implementation <a href=""https://github.com/numpy/numpy/blob/ca58cde7de282a485ca12e5f2582a64fb2e52113/numpy/ma/core.py#L8047"" rel=""nofollow noreferrer"">here</a>. So, you need to make sure whether in future, torch implements it the way you want.
I guess switching back and forth from torch tensor to numpy is not applicable solution for you either.</p>
","0","Answer"
"78436775","78436672","<p>it seem to me that you need to map your alphanumeric characters to uniques floats, the easiest way to do that seem to be converting the alphanumeric values to binary using either ascii of utf-8 encoding, then into base 10 numbers.
If you have duplicates, just stay in base 2, 0 and 1's can make a float and you're sure it's unique
<a href=""https://www.geeksforgeeks.org/python-convert-string-to-binary/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/python-convert-string-to-binary/</a> for ways of turning a string to binary</p>
<p>another way, would be to simply generate a dict with the alphanumeric entries
as keys and arbitrarily distributed numbers as value, like so :</p>
<pre><code>numbers = {}
for i in range(len(x)):
    numbers[x[i]] = i
</code></pre>
<p>or in a shorter way:</p>
<pre><code>numbers = {x[i]:i for i in range(len(x))}
</code></pre>
","0","Answer"
"78439803","78404705","<p>Would this work for you:</p>
<pre class=""lang-py prettyprint-override""><code>mask_row_ids = torch.arange(mask_indices.shape[0]).unsqueeze(1).repeat(1, mask_indices.shape[1])

mask_tensor = torch.ones(inp_tensor.shape)
mask_tensor[mask_row_ids, mask_indices] = 0

output = torch.mm(inp_tensor * mask_tensor, my_tensor)
print(output)
</code></pre>
","0","Answer"
"78441397","78435504","<p>Your <code>fc_input_size</code> is wrong. First, you're putting in <code>data.shape[0]</code>, which is the number of data, and not the size of each data item. Second, you are forgetting about the 2 MaxPool operations you are doing, each of which reduce the spatial dimensions by 2x.</p>
<p>Calculate it like following:</p>
<pre><code>fc_input_size = (data.shape[1] // 4) * (data.shape[2] // 4)
</code></pre>
","0","Answer"
"78441934","78420289","<p>The error refers to the shape mismatch while calculating the cross-entropy loss.</p>
<p>The last layer defined by you have 5 output units, while looking at the error, it feels like you have 3 distinct classes. Verify using <code>len(set(Y))</code></p>
<pre class=""lang-bash prettyprint-override""><code>classifier.add(Dense(units = 5, activation = 'softmax'))
</code></pre>
<p>Change the last softmax layer to</p>
<pre class=""lang-bash prettyprint-override""><code>classifier.add(Dense(units = 3, activation = 'softmax'))
</code></pre>
<p>or maybe a little dynamic</p>
<pre class=""lang-bash prettyprint-override""><code>classifier.add(Dense(units = len(set(Y)), activation = 'softmax'))
</code></pre>
","0","Answer"
"78442382","78442079","<p>Try penalizing PD and T1 separately as well. Account for their relative scales to each other and the combined metric. Or only use their loss, when the difference is over a threshold or the ratio between them is wrong.</p>
","0","Answer"
"78443136","78442377","<p>One way is to load the Python file with the heavy imports once and keep it open. Then, every time the Bash script runs it can send a message to the already running script to start processing.</p>
<p>To communicate with the running Python script from Bash there are various methods. One easy and flexible way might be to use a named pipe. This is a special type of file that causes Python to wait for input when you read it.</p>
<p>The following is based on <a href=""https://stackoverflow.com/a/55734712/"">https://stackoverflow.com/a/55734712/</a>:</p>
<pre><code>import pathlib
import os
# heavy imports here

FIFO = 'myfifo'
# remove pipe in case it exists
pathlib.Path(FIFO).unlink(missing_ok=True)
# create new pipe
os.mkfifo(FIFO)

def do_something(msg):
    print(f&quot;open {msg} and process it&quot;)

while True:
    with open(FIFO) as fifo:
        for line in fifo:
            msg = line.strip()
            do_something(msg)
</code></pre>
<p>Now you can write to the pipe from Bash. Make sure the Bash script has the same working directory as the Python script, or provide the full path to the pipe. Note that you need to end the line explicitly with <code>\n</code>.</p>
<pre class=""lang-bash prettyprint-override""><code>printf &quot;somefile.jpg\n&quot; &gt; myfifo
</code></pre>
<p>This causes the Python script to print</p>
<pre class=""lang-none prettyprint-override""><code>open somefile.jpg and process it
</code></pre>
<p>and then the <code>while True</code> loop causes the script to wait for further input.</p>
","1","Answer"
"78446303","78443975","<p><code>tf.data.experimental.make_csv_dataset</code> returns orderedDict with key as feature names and value as the actual features.</p>
<pre class=""lang-bash prettyprint-override""><code>dataset = tf.data.experimental.make_csv_dataset(
                'test.csv',label_name='target',
                batch_size=1,num_epochs=1)
</code></pre>
<p>If you look closely, the features and labels given by the <code>dataset</code></p>
<pre class=""lang-bash prettyprint-override""><code>$ dataset.__iter__().next()
&gt;&gt; (OrderedDict([('sepal length (cm)',
               &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)&gt;),
              ('sepal width (cm)',
               &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.3], dtype=float32)&gt;),
              ('petal length (cm)',
               &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.3], dtype=float32)&gt;),
              ('petal width (cm)',
               &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)&gt;)]),
 &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])&gt;)
</code></pre>
<p>So, you cannot simply pass in this ordered dictionary as input to the model. you can convert it into an interpretable format by writing a pre-processing mapping function</p>
<pre class=""lang-bash prettyprint-override""><code>def pre_process(features, labels):
    features = tf.stack([value for key, value in features.items()], axis=-1)
    return features, labels

dataset = dataset.map(pre_process)
</code></pre>
<p>Now if you have a look at the <code>dataset</code>, it will have features which can be passed into the model</p>
<pre class=""lang-bash prettyprint-override""><code>$ dataset.__iter__().next()
&gt; (&lt;tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[5.1, 3.8, 1.6, 0.2]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])&gt;)
</code></pre>
<p>Now this dataset can be directly passed into the model directly for training.</p>
","0","Answer"
"78447342","78439701","<p>You could use wandb.alert(...) inside a try/except block.</p>
<p>Check the docs here: <a href=""https://docs.wandb.ai/guides/runs/alert"" rel=""nofollow noreferrer"">https://docs.wandb.ai/guides/runs/alert</a></p>
","0","Answer"
"78448498","78448470","<p>For adding new classes to an already trained model, you might consider the concept of Transfer Learning. Instead of retraining the entire model from scratch with both old and new classes combined, you can:</p>
<ol>
<li>Freeze the layers up to the last one or few (which have learned
feature representations from your initial training).</li>
<li>Then, only train the final layers (or add new ones) to learn the
additional classes.</li>
</ol>
<p>Here's a simplified code snippet to give you an idea:</p>
<pre><code>from ultralytics import YOLO

model = YOLO('runs/detect/train/weights/best.pt') # load a pretrained model (recommended for training)
 
# unfreeze the last 4 layers of model,
named_parameters = list(model.named_parameters())
for i, (name, param) in enumerate(named_parameters):
    if i &gt;= len(named_parameters) - 4:  # Unfreeze the last 4 layers
        param.requires_grad = True
    else:
        param.requires_grad = False


# # Proceed with training on the new dataset (with 11 classes now)
model.train(data=&quot;dataset/data.yaml&quot;, epochs=200, batch=16, workers=1)  # train the model
</code></pre>
<p>This code illustrates the general approach. You'll need to adjust the specifics based on your actual model structure and how your data is organized.</p>
<p>Remember to update your dataset configuration file (<em>new_data_with_11_classes.yaml</em> in the example) to reflect all 11 classes.</p>
<p>This approach can significantly reduce both training time and costs.</p>
<p>@glenn-jocher</p>
","0","Answer"
"78449461","78444040","<p>The warning says your column 'bush_name' was detected as bad or constant and is not used during training. So, the result predictions are not based on this column.</p>
<p>If the column is not bad or constant (the bush_name data looks good in your example), it is used to train a model, and the result predictions are based on its values (levels).</p>
<p>If you put some unknown level in your test data, the model will create the prediction, but the prediction could not be as good as if the value were in the training dataset. That is the reason why we are raising the warning. However, the model will create some predictions for data with unknown levels.</p>
","1","Answer"
"78450603","78450279","<p>This hack might work.</p>
<p>Random forests sample columns, right? Then I would just extend the dataset by adding tens (or hundreds or thousands) of copies of price column.</p>
","1","Answer"
"78451592","78451297","<p>It doesn't look like there are any obvious errors in your code.</p>
<p>It looks like you're trying to predict housing prices, though, based on the filepaths for the data you're working with. For this task, those are potentially reasonable MSE values.</p>
<p>MSE is mean <em>squared</em> error, so if you take the square root of it, you get <em>root</em> mean squared error. RMSE can be interpreted as more or less the average absolute value of the error in your predictions, while MSE is a little trickier to interpret, since MSE is measured in units that are the square of the target variable, while RMSE is measured in units equal to those of the target variable. If you take the square root of the <code>MSE</code> values you've provided, you get values between 27,000 and 39,000, which are not crazy - in other words, it means your models are off by about that much, on average. If you are predicting US house prices in dollars, that is a relatively reasonable range for your models to be off by.</p>
<p>More on RMSE vs MSE <a href=""https://www.oreilly.com/library/view/machine-learning-with/9781785889936/669125cc-ce5c-4507-a28e-065ebfda8f86.xhtml#:%7E:text=RMSE%20is%20the%20square%20root,units%20as%20the%20target%20variable."" rel=""nofollow noreferrer"">here</a></p>
","1","Answer"
"78453131","78444040","<p>The problem might be with <code>bush_name</code> being a character vector and not a factor vector. Since <a href=""https://blog.r-project.org/2020/02/16/stringsasfactors/"" rel=""nofollow noreferrer"">R 4.0</a>, <code>stringsAsFactors</code> is set to FALSE so the character vector is not automatically converted to factors and H2O does not consider string vector as a valid input for most of its supervised algorithms.</p>
<p>You can use <code>str()</code> to inspect the structure of a data structure in R, this can be very useful for <code>data.frame</code>s to find out what data types individual columns have. For example, when adding <code>stringsAsFactors=TRUE</code> the types of <code>date</code> and <code>bush_name</code> change types from <code>character</code> to <code>factor</code>:</p>
<pre><code>&gt; train_data &lt;- data.frame(date = c(&quot;2022-01-01&quot;, &quot;2022-01-07&quot;, &quot;2022-02-09&quot;, &quot;2022-05-01&quot;, &quot;2022-11-01&quot;, &quot;2022-11-02&quot;),
                   bush_name = c(&quot;bush001&quot;, &quot;bush001&quot;, &quot;bush001&quot;, &quot;bush043&quot;, &quot;bush043&quot;, &quot;bush043&quot;),
                   bugs = c(2, 0, 1, 0, 3, 1),
                   has_rotten_berry = c(1, 0, 0, 1, 1, 0),
                   berry_count = c(12, 1, 7, 100, 14, 4),
                   weather = c(1, 0, 2, 0, 1, 1))
&gt; str(train_data)
'data.frame':   6 obs. of  6 variables:
 $ date            : chr  &quot;2022-01-01&quot; &quot;2022-01-07&quot; &quot;2022-02-09&quot; &quot;2022-05-01&quot; ...
 $ bush_name       : chr  &quot;bush001&quot; &quot;bush001&quot; &quot;bush001&quot; &quot;bush043&quot; ...
 $ bugs            : num  2 0 1 0 3 1
 $ has_rotten_berry: num  1 0 0 1 1 0
 $ berry_count     : num  12 1 7 100 14 4
 $ weather         : num  1 0 2 0 1 1
&gt;
&gt; # Now let's add stringsAsFactors=TRUE to the data.frame function
&gt; train_data &lt;- data.frame(date = c(&quot;2022-01-01&quot;, &quot;2022-01-07&quot;, &quot;2022-02-09&quot;, &quot;2022-05-01&quot;, &quot;2022-11-01&quot;, &quot;2022-11-02&quot;),
                   bush_name = c(&quot;bush001&quot;, &quot;bush001&quot;, &quot;bush001&quot;, &quot;bush043&quot;, &quot;bush043&quot;, &quot;bush043&quot;),
                   bugs = c(2, 0, 1, 0, 3, 1),
                   has_rotten_berry = c(1, 0, 0, 1, 1, 0),
                   berry_count = c(12, 1, 7, 100, 14, 4),
                   weather = c(1, 0, 2, 0, 1, 1), stringsAsFactors=TRUE)
&gt; str(train_data)
'data.frame':   6 obs. of  6 variables:
 $ date            : Factor w/ 6 levels &quot;2022-01-01&quot;,&quot;2022-01-07&quot;,..: 1 2 3 4 5 6
 $ bush_name       : Factor w/ 2 levels &quot;bush001&quot;,&quot;bush043&quot;: 1 1 1 2 2 2
 $ bugs            : num  2 0 1 0 3 1
 $ has_rotten_berry: num  1 0 0 1 1 0
 $ berry_count     : num  12 1 7 100 14 4
 $ weather         : num  1 0 2 0 1 1
</code></pre>
<p>However, I'd recommend to use the default and change the type afterwards (so the <code>date</code> is not converted to a factor vector) by using:</p>
<pre><code>&gt; train_data$bush_name &lt;- factor(train_data$bush_name, levels=c(&quot;bush001&quot;, &quot;bush043&quot;))
</code></pre>
<p>Note the explicit enumeration of levels - you should use the same argument for <code>test_data</code> so that you ensure the same encoding of the factors.</p>
<p>You'll probably also want to encode the date column. H2O doesn't do any clever date preprocessing - at best it will represent date as a number of seconds since 1st January 1970 which is not ideal for rotten berry prediction as seasonality will likely have a bigger effect than number of seconds since some date. You can use <a href=""https://recipes.tidymodels.org/reference/step_date.html"" rel=""nofollow noreferrer"">step_date</a> for date preprocessing.</p>
","1","Answer"
"78453453","78441794","<p>In synapse spark 3.2 will be retiring soon.</p>
<p><img src=""https://i.imgur.com/fT7PDUb.png"" alt=""enter image description here"" /></p>
<p>So, i would suggest you to use either spark 3.3 or 3.4.</p>
<p>In any version out of this 2 you will get built in <code>synapseml</code> packages even  <strong>LightGBM</strong> as shown below.</p>
<p><img src=""https://i.imgur.com/xMkvkVD.png"" alt=""enter image description here"" /></p>
<p>Here, you can see the <code>synapseml</code> version is <code>1.0.2</code> in spark 3.4.</p>
<p>If you wish to configure different version execute below command as mentioned this <a href=""https://microsoft.github.io/SynapseML/docs/Get%20Started/Install%20SynapseML/#synapse"" rel=""nofollow noreferrer"">documentation</a>.</p>
<pre><code>%%configure -f
{
  &quot;name&quot;: &quot;synapseml&quot;,
  &quot;conf&quot;: {
      &quot;spark.jars.packages&quot;: &quot;com.microsoft.azure:synapseml_2.12:&lt;version&gt;&quot;,
      &quot;spark.jars.repositories&quot;: &quot;https://mmlspark.azureedge.net/maven&quot;,
      &quot;spark.jars.excludes&quot;: &quot;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind&quot;,
      &quot;spark.yarn.user.classpath.first&quot;: &quot;true&quot;,
      &quot;spark.sql.parquet.enableVectorizedReader&quot;: &quot;false&quot;
  }
}
</code></pre>
","0","Answer"
"78455882","78454026","<p>It appears that the parameter only has an effect when one of the <code>colsample_by*</code> parameters is less than 1 (and by default they are all 1).</p>
","1","Answer"
"78458236","78457370","<p>First, this sound just like a classification problem, so in another word: yes, it does make sense.</p>
<p>Second, yes. Pytorch provides you everythings numpy offers (well, almost). For generating random uniform data, you can use the following:</p>
<pre><code>x = torch.rand(1000, 2).sub(.5).mul(200) # &lt;- torch.rand() return range [0,1]
y = (x.sum(1, keepdim = True) &gt; 10).float().sub(.5).mul(20) # &lt;- (x.sum(1) &gt; 10).float() return {0,1}
</code></pre>
","0","Answer"
"78461888","78460997","<p>It seems indeed that the web server is misconfigured: <a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow noreferrer"">http://yann.lecun.com/exdb/mnist/</a>. As this dataset is built-in in many standard libraries like keras (see <a href=""https://www.kaggle.com/code/amyjang/tensorflow-mnist-cnn-tutorial"" rel=""nofollow noreferrer"">this tutorial</a>), it is not so frequently downloaded from the &quot;lecun url&quot; I think.</p>
<p>In the source (<a href=""https://github.com/datapythonista/mnist/blob/master/mnist/__init__.py"" rel=""nofollow noreferrer""><code>mnist/__init__.py</code></a>) there is a comment:</p>
<pre class=""lang-py prettyprint-override""><code># `datasets_url` and `temporary_dir` can be set by the user using:
# &gt;&gt;&gt; mnist.datasets_url = 'http://my.mnist.url'
# &gt;&gt;&gt; mnist.temporary_dir = lambda: '/tmp/mnist'
datasets_url = 'http://yann.lecun.com/exdb/mnist/'
temporary_dir = tempfile.gettempdir
</code></pre>
<p>So theoretically, you could set the <code>mnist.datasets_url</code> variable for a mirror and it should work. The only mirror I found with the original format is this: <a href=""https://github.com/mkolod/MNIST"" rel=""nofollow noreferrer"">https://github.com/mkolod/MNIST</a>. But this is <code>https</code>, and it did not work for me.</p>
<p>So instead you can manually download the data from the GitHub mirror into the temp directory shown by this code:</p>
<pre class=""lang-py prettyprint-override""><code>import temp file
tempfile.gettempdir()
</code></pre>
<p>And then <code>mnist.train_images()</code> should work.</p>
","1","Answer"
"78462133","78461828","<p>You need to use AutoEncoder for dimensionality reduction. You could take a look <a href=""https://www.analyticsvidhya.com/blog/2021/06/dimensionality-reduction-using-autoencoders-in-python/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The steps in simplified form would be the following:</p>
<ol>
<li>Train an AutoEncoder</li>
<li>Take the encoder part</li>
<li>Encode your data with the encoder</li>
<li>Train another model of your choice with encoded data</li>
</ol>
<p>If you are uncomfortable with <code>keras</code> or <code>pytorch</code>, you can even use <code>scikit-learn</code> <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"" rel=""nofollow noreferrer"">MLPClassifier</a>.</p>
","0","Answer"
"78464551","78432197","<p>RAG is slower than pure LLM, or fine tuned LLM as explained by @nbaua.</p>
<p>If you want faster responses in you custom data you need to fine tune the LLM, which makes it faster in response for custom use cases, but you need more machine power and beware of <a href=""https://en.wikipedia.org/wiki/Catastrophic_interference"" rel=""nofollow noreferrer"">catastrophic forgetting</a>.</p>
<p>Or check out other methods like PEFT(Parameter efficient fine tuning).</p>
<p>You can also try LLM-agentic approach which calls RAG based data only when necessary, this could be more adaptive solution.</p>
<p>Experiment with methods and choose wisely</p>
","1","Answer"
"78465937","78461070","<p>You suggested the &quot;linear sum of coef and new data&quot;, which is correct, but since you only have one feature it's just the coefficient times the new data value:</p>
<pre><code>66.3902 * 4.3
&gt;&gt;&gt; 285.47786
</code></pre>
<p>But the other entries of the <code>coef</code> column aren't really coefficients in the traditional sense (and there is no softmax); instead they represent cutoffs for the discrete targets.</p>
<p>The prediction from the linear model (285.48 above) is taken as the mean of a normal distribution <code>y</code> with standard deviation of 1 (by default, see parameter <code>distr</code>), and the probability of each target is the probability that <code>y</code> is between the associated cutoffs.</p>
<p>It's not documented so well what those cutoffs are, but I assume that the first non-coefficient <code>coef</code> is the first cutoff, and the rest indicate the difference between consecutive cutoffs. So</p>
<pre><code>p_1 = P(y &lt; 285.5835) ~= 0.5264086
p_2 = P(285.5835 &lt; y &lt; 285.5835 + 4.2698) ~= 0.4735914
p_3 = P( 285.5835 + 4.2698 &lt; y &lt;  285.5835 + 4.2698 + 4.1879) ~= 0
etc.
</code></pre>
","0","Answer"
"78466413","78466376","<p>It seems to me that there are just a few minor issues in your code.</p>
<ul>
<li>It's missing the import of torch.</li>
<li>There are some vars, depreciated that I changed.</li>
<li>Still, I get a few warnings, but I guess, it may not be important.</li>
</ul>
<pre><code>import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline
from typing import List, Dict


def process_prompts(prompts: List[str], model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, num_completions) -&gt; List[List[str]]:
    device = 0 if model.device.type == 'cuda' else -1
    text_generator = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer, device=device)
    outputs = []

    for prompt in prompts:
        try:
            results = text_generator(prompt, truncation=True,
                                     num_return_sequences=num_completions, num_beams=num_completions)
            completions = [result['generated_text'] for result in results]
            outputs.append(completions)
        except Exception as e:
            print(f&quot;Error processing prompt {prompt}: {str(e)}&quot;)

    return outputs


if __name__ == &quot;__main__&quot;:
    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
    model.to(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    example_prompts = [&quot;Hello, how are you?&quot;]
    num_completions = 2
    processed_outputs = process_prompts(example_prompts, model, tokenizer, num_completions)

    res = ' '.join(processed_outputs[0])
    print(res)

</code></pre>
<h3>Prints</h3>
<pre><code>Hello, how are you? How are you?&quot;

&quot;I'm fine,&quot; I said.

&quot;Well, I guess I'm just fine,&quot; he said, turning to me.

&quot;Well, I guess I'm just fine Hello, how are you? How are you?&quot;

&quot;I'm fine,&quot; I said.

&quot;Well, I guess I'm just fine,&quot; he said, turning to me.

&quot;Well, I guess I'm just not
</code></pre>
","0","Answer"
"78466524","78466376","<p>I think this works:</p>
<pre><code># -- test

def pipeline_tests_():
    print(f'\n--&gt; pipeline_tests_()')
    import torch
    from transformers import pipeline

    # pipe = pipeline(model=&quot;gpt2&quot;, device_map=&quot;auto&quot;, model_kwargs={&quot;load_in_8bit&quot;: True})
    pipe = pipeline(model=&quot;gpt2&quot;, device_map=&quot;auto&quot;, model_kwargs={&quot;load_in_4bit&quot;: True})

    output = pipe(&quot;This is a cool example!&quot;, do_sample=True, top_p=0.95, temperature=0.8, max_length=50)
    print(f'\n{output=}')
    print(f'{len(output)=}')

    output = pipe(&quot;This is a cool example!&quot;, do_sample=True, top_p=0.95, temperature=0.8, max_length=50, num_return_sequences=4)
    print(f'\n{output=}')
    print(f'{len(output)=}')

    output = pipe(&quot;This is a cool example!&quot;, do_sample=False, top_p=0.95, temperature=0.8, max_length=50, num_return_sequences=4, num_beams=5)
    print(f'\n{output=}')
    print(f'{len(output)=}')

    print()

# -- main 

def main(
        # path_2_eval_dataset: str = '~/gold-ai-olympiad/data/MATH/test',
        path_2_eval_dataset: str = '~/putnam-math/data/Putnam_MATH_original_static2/test',
        model: str = 'gpt-4-turbo',  # e.g., gpt-4-turbo, gpt-3.5-turbo
        start: int = 0, 
        end: int = sys.maxsize, 
        ):
    from evals.data_eval_utils import get_iter_for_eval_data_set
    from evals.prompts_evals import HELM_MATH_PROMPT_8SHOT_COT2_TEMPLATE, get_math_problem_prompt_ala_helm_8shot_cot2 
    # - Get eval data
    path_2_eval_dataset: Path = Path(path_2_eval_dataset).expanduser()
    math_gold_probs_solns: list[dict] = list(get_iter_for_eval_data_set(path_2_eval_dataset))
    math_gold_probs_solns: list[dict] = math_gold_probs_solns[start:end]
    print(f'{path_2_eval_dataset=} \n {len(math_gold_probs_solns)=}')
    assert len(math_gold_probs_solns) &gt; 0, f'No math problems found in {path_2_eval_dataset=}'

    # - Get vllm generator
    prompt_template: str = HELM_MATH_PROMPT_8SHOT_COT2_TEMPLATE
    prompt_gen_func: Callable = get_math_problem_prompt_ala_helm_8shot_cot2
    math_prompts_problems: list[str] = [prompt_gen_func(gold_data_prob_soln, prompt_template) for gold_data_prob_soln in math_gold_probs_solns]
    math_guessed_outputs: list[str] = [f&quot;Solution: Let's think step by step. &quot; + gold_data_prob_soln['solution'] for gold_data_prob_soln in math_gold_probs_solns]

    # - Estimate cost of inference
    result = estimate_openai_api_inference_cost(prompts=math_prompts_problems, outputs=math_guessed_outputs, model=model, verbose=True)
    print(f'--&gt; Inference cost: {result=}')

if __name__ == '__main__':
    import fire
    import time
    start = time.time()
    # main()
    # fire.Fire(main)
    fire.Fire(pipeline_tests_)
    # pyton boxed_acc_eval.py --model meta-llama/Meta-Llama-3-8B-Instruct
    print(f&quot;Done!\a Time: {time.time()-start:.2f} sec, {(time.time()-start)/60:.2f} min, {(time.time()-start)/3600:.2f} hr\a&quot;)
</code></pre>
<p>output:</p>
<pre><code>--&gt; pipeline_tests_()
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')

output=[{'generated_text': &quot;This is a cool example! I got my first two kids in my living room to play with, and I've played with them for years before that. It was so beautiful. They were so nervous.\n\nAdvertisement\n\nI don't think&quot;}]
len(output)=1
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.

output=[{'generated_text': 'This is a cool example!\n\nThe next thing I\'m going to do is create a class called &quot;Misc.Libraries.Libraries.Libraries&quot; and set it up with a simple definition:\n\n// This is a class'}, {'generated_text': &quot;This is a cool example! In the video below, you can see how you can add a second time to your first time!\n\nIt's all about balancing the energy density of the atmosphere and the way you keep it going at night.\n&quot;}, {'generated_text': &quot;This is a cool example! I've tried to build a simple tool for tracking the change over time. My friends and I will try to improve it for everyone.\n\nFor now, I've started with a simple simple tool that takes just a&quot;}, {'generated_text': 'This is a cool example!\n\nWe need to see how the system works.\n\nWe want to see how the &quot;network&quot; works.\n\nAnd what about &quot;tasking&quot; for an existing service.\n\nThis is just one'}]
len(output)=4
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.

output=[{'generated_text': &quot;This is a cool example!\n\nI've been working on this for a few months now, and I'm really excited to share it with you.\n\nI've been working on this for a few months now, and I'm really excited&quot;}, {'generated_text': &quot;This is a cool example!\n\nI've been working on this for a few months now, and I'm really excited to share it with you guys.\n\nI've been working on this for a few months now, and I'm really&quot;}, {'generated_text': &quot;This is a cool example!\n\nI've been working on this for a few months now, and I'm really excited to share it with you.\n\nI've been working on this for a while now, and I'm really excited to&quot;}, {'generated_text': &quot;This is a cool example!\n\nI've been working on this for a few months now, and I'm really excited to share it with you guys.\n\nI've been working on this for a couple months now, and I'm really&quot;}]
len(output)=4

Done! Time: 33.57 sec, 0.56 min, 0.01 hr
</code></pre>
<p>ultimately this helped me <a href=""https://huggingface.co/docs/transformers/pipeline_tutorial#using-pipeline-on-large-models-with--accelerate-"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pipeline_tutorial#using-pipeline-on-large-models-with--accelerate-</a>:</p>
<pre><code># pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model=&quot;facebook/opt-1.3b&quot;, device_map=&quot;auto&quot;, model_kwargs={&quot;load_in_8bit&quot;: True})
output = pipe(&quot;This is a cool example!&quot;, do_sample=True, top_p=0.95)
</code></pre>
<p>and guessed the inputs names based on other attempts, errors.</p>
<hr />
<pre><code>error: 
</code></pre>
<p>error: <code>Truncation was not explicitly activated but</code>max_length<code>is provided a specific value, please use</code>truncation=True<code>to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to</code>truncation<code>.</code></p>
<pre><code>
----

fix:
</code></pre>
<p>def pipeline_tests_():
print(f'\n--&gt; pipeline_tests_()')
import torch
from transformers import pipeline</p>
<pre><code># pipe = pipeline(model=&quot;gpt2&quot;, device_map=&quot;auto&quot;, model_kwargs={&quot;load_in_8bit&quot;: True})
pipe = pipeline(model=&quot;gpt2&quot;, device_map=&quot;auto&quot;, model_kwargs={&quot;load_in_4bit&quot;: True})

# output = pipe(&quot;This is a cool example!&quot;, do_sample=True, top_p=0.95, temperature=0.8, max_length=50)
output = pipe(&quot;This is a cool example!&quot;, do_sample=True, top_p=0.95, temperature=0.8, max_length=50, truncation=True)
print(f'\n{output=}')
print(f'{len(output)=}')

output = pipe(&quot;This is a cool example!&quot;, do_sample=True, top_p=0.95, temperature=0.8, max_length=50, num_return_sequences=4, truncation=True)
print(f'\n{output=}')
print(f'{len(output)=}')

output = pipe(&quot;This is a cool example!&quot;, do_sample=False, top_p=0.95, temperature=0.8, max_length=50, num_return_sequences=4, num_beams=5, truncation=True)
print(f'\n{output=}')
print(f'{len(output)=}')

print()
</code></pre>
<pre><code></code></pre>
","0","Answer"
"78466926","78466923","<p>Found the <a href=""https://docs.quantum.ibm.com/api/migration-guides/qiskit-1.0-features#quantumcircuit-gates"" rel=""nofollow noreferrer"">documentation</a> that talks about these deprecated methods for quantum gates.
for this specific method you can use the QuantumCircuit.cx instead of cnot.
Here's some more:
<a href=""https://i.sstatic.net/fzqigQ46.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","0","Answer"
"78466938","78463519","<h1>Change</h1>
<pre class=""lang-py prettyprint-override""><code># beta += kernel_matrix[i] * alpha * (t - sigmoid_value)
beta[i] +=  alpha * (t - sigmoid_value)
</code></pre>
<h1>Full Code</h1>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import warnings

warnings.filterwarnings('ignore')

def monomial_kernel(d):
    def k(x, y, d=d):
        phi_x_y = 0
        prod_xy = np.dot(x.T,y)

        for n in range(d+1):
            phi_x_y += (prod_xy ** n)

        return phi_x_y
        
    return k

def rbf_kernel(sigma):
    def k(x, y, sigma=sigma):
        numerator = np.linalg.norm(x - y) **2
        denominator = 2 * (sigma ** 2)
        
        return np.exp(-numerator / denominator)

    return k

def sigmoid(z):
    if type(z) == np.ndarray:
        z = z[0]
    try:
        return 1 / (1 + np.exp(-z)) 
    except:
        print(z)

def logistic_regression_with_kernel(X, y, k, alpha, iterations):

    n_samples, _ = X.shape
    bias = 0
    kernel_matrix = np.zeros((n_samples, n_samples))
    beta = np.zeros(n_samples)

    #create kernel matrix
    for i in range(n_samples):
        for j in range(n_samples):
            kernel_matrix[i][j] = k(X[i], X[j])
    
    for _ in range(iterations):
        for i in range(n_samples):
            total = 0
            for j in range(n_samples):
                total += beta[j] * kernel_matrix[i][j]
            total += bias
            sigmoid_value = sigmoid(total)
            t = y[i]

            # beta += kernel_matrix[i] * alpha * (t - sigmoid_value)
            beta[i] +=  alpha * (t - sigmoid_value)
                    
            bias += (alpha * (t - (sigmoid_value))) 

    def model(x, beta=beta, bias=bias, k=k, ref=X):
        z = sum([k(ref[i], x) * beta[i] for i in range(ref.shape[0])]) + bias 

        sig = sigmoid(z)
        # print(sig)
        return round(sig)
    return model
</code></pre>
","1","Answer"
"78467534","78412765","<p>you should use <code>tf.keras.applications.MobileNetV2</code> instead of installing from <code>tensorflow_hub</code> and also freeze the model weight using <code>mobile_net_v2_model.trainable=False</code></p>
","0","Answer"
"78469304","78469183","<p>It's not clear how your data looks like.</p>
<p>I guess you have a large text. If the text is too large, KMP is a choice to look into, it is fast, but you could maybe solve the problem much easier. Because the number of companies out there are limited. Right?</p>
<hr />
<h3>KMP</h3>
<p>KMP algorithm is difficult to explain. If you are interested to know how it works, please check <a href=""https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm"" rel=""nofollow noreferrer"">the wikipedia</a>.</p>
<h3>Approach 1</h3>
<pre><code>def get_lps(p):
    lps, j = [0] * len(p), 0
    for i in range(1, len(p)):
        while j and p[i] != p[j]:
            j = lps[j - 1]
        if p[i] == p[j]:
            j += 1
        lps[i] = j
    return lps


def kmp(s, p):
    lps, indices, j = get_lps(p), [], 0
    for i in range(len(s)):
        while j and s[i] != p[j]:
            j = lps[j - 1]
        if s[i] == p[j]:
            j += 1
        if j == len(p):
            indices.append(i - len(p) + 1)
            j = lps[j - 1]
    return indices


s = &quot;&quot;&quot;&quot;
some text Apple Stock common us inc
some text Apple INC Stock common us inc
some text Apple Inc. Stock common us inc
some text APPL Stock common us inc Apple APPLE
&quot;&quot;&quot;

print(kmp(s, 'Apple'))
print(kmp(s, 'Apple INC'))
print(kmp(s, 'Apple inc'))
print(kmp(s.lower(), 'apple inc'))
print(kmp(s.upper(), 'APPLE'))
print(kmp(s.upper(), 'NOTAPPLE'))

</code></pre>
<h3>Prints</h3>
<p>It prints the first index where the match is found:</p>
<pre><code>[12, 48, 88, 154]
[48]
[]
[48, 88]
[12, 48, 88, 154, 160]
[]
</code></pre>
<h3>Note</h3>
<p>I'm assuming that the name of companies are known to you in advance, thus the variable <code>pattern</code>.</p>
<hr />
<h3>Approach 2 (Accuracy)</h3>
<p>If you need 100% accuracy, then you have to find other ways to solve this problem, and fuzzy and &quot;closeness&quot; methods, will fail.</p>
<p>But, the number of companies are pretty limited. Right?</p>
<h3>More accurate</h3>
<p>It is not as simple as it looks. Because if you have 10,000 companies (let's just guess), then this would be difficult to write and maintain.</p>
<pre><code>import re
import pprint
from collections import defaultdict

pattern = r&quot;(?i)(\d+\s*metals?\s+(?:limited|ltd|common.*)?)|(\bmol\b.*hungar.*)|(\bmol\b.*\bmag.*)|(\bmolina\b.*\bhealth.*)|(.*)&quot;

s = &quot;&quot;&quot;
29METALS LIMITED
29METALS LIMITED
29Metals Limited
29METALS LTD
29METALS LTD.

29METENTY LTD

APPLE 

GOOGLE



29 METALS COMMON STOCK
29 METAL PTD

29METALS
29METENTY LTD
29METALS Ltd
29METALS LIMITED
29METALS LIMITED
29Metals Limited
29METALS LTD
29METALS LTD.
29METENTY LTD
29 METALS COMMON STOCK
29 METAL PTD
29METALS
29METENTY LTD
29METALS Ltd

MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125
MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125
MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125
MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125
MOL HUNGARIAN OIL AND GAS PL
MOL HUNGARIAN OIL AND GAS PL
MOL HUNGARIAN OIL AND GAS PL
MOL MAGYAR OLAJ GAZIPARI
Mol Magyar Olajes Gazipari Nyrt
MOLINA HEALTHCARE
MOLINA HEALTHCARE INC
MOLINA HEALTHCARE INC
MOLINA HEALTHCARE INC
MOLINA HEALTHCARE INC
MOLINA HEALTHCARE INC
MOLINA HEALTHCARE INC
Molina Healthcare Inc
Molina Healthcare Inc
MOLINA HEALTHCARE INC
Molina Healthcare Inc

&quot;&quot;&quot;

memo = defaultdict(list)
companies = re.findall(pattern, s)

for i, (a, b, c, d, e) in enumerate(companies):
    if a:
        memo['G 1'].append((i, a))
    if b:
        memo['G 2'].append((i, b))
    if c:
        memo['G 3'].append((i, c))
    if d:
        memo['G 4'].append((i, d))
    if e:
        memo['G 5'].append((i, e))


pprint.pprint(memo)


</code></pre>
<h3>Prints</h3>
<pre><code>defaultdict(&lt;class 'list'&gt;,
            {'G 1': [(1, '29METALS LIMITED'),
                     (3, '29METALS LIMITED'),
                     (5, '29Metals Limited'),
                     (7, '29METALS LTD'),
                     (9, '29METALS LTD'),
                     (24, '29 METALS COMMON STOCK'),
                     (26, '29 METAL '),
                     (30, '29METALS\n'),
                     (33, '29METALS Ltd'),
                     (35, '29METALS LIMITED'),
                     (37, '29METALS LIMITED'),
                     (39, '29Metals Limited'),
                     (41, '29METALS LTD'),
                     (43, '29METALS LTD'),
                     (48, '29 METALS COMMON STOCK'),
                     (50, '29 METAL '),
                     (53, '29METALS\n'),
                     (56, '29METALS Ltd')],
             'G 2': [(59, 'MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125'),
                     (61, 'MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125'),
                     (63, 'MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125'),
                     (65, 'MOL HUNGARIAN OIL &amp; GAS PLC COMMON STOCK HUF 125'),
                     (67, 'MOL HUNGARIAN OIL AND GAS PL'),
                     (69, 'MOL HUNGARIAN OIL AND GAS PL'),
                     (71, 'MOL HUNGARIAN OIL AND GAS PL')],
             'G 3': [(73, 'MOL MAGYAR OLAJ GAZIPARI'),
                     (75, 'Mol Magyar Olajes Gazipari Nyrt')],
             'G 4': [(77, 'MOLINA HEALTHCARE'),
                     (79, 'MOLINA HEALTHCARE INC'),
                     (81, 'MOLINA HEALTHCARE INC'),
                     (83, 'MOLINA HEALTHCARE INC'),
                     (85, 'MOLINA HEALTHCARE INC'),
                     (87, 'MOLINA HEALTHCARE INC'),
                     (89, 'MOLINA HEALTHCARE INC'),
                     (91, 'Molina Healthcare Inc'),
                     (93, 'Molina Healthcare Inc'),
                     (95, 'MOLINA HEALTHCARE INC'),
                     (97, 'Molina Healthcare Inc')],
             'G 5': [(10, '.'),
                     (13, '29METENTY LTD'),
                     (16, 'APPLE '),
                     (19, 'GOOGLE'),
                     (27, 'PTD'),
                     (31, '29METENTY LTD'),
                     (44, '.'),
                     (46, '29METENTY LTD'),
                     (51, 'PTD'),
                     (54, '29METENTY LTD')]})

</code></pre>
<hr />
<h3>Edit</h3>
<p>Since you have 30,000 companies:</p>
<ul>
<li>You can first sort them.</li>
<li>Separate them by prefix.</li>
<li>Preprocess them. For instance, remove all the common words and suffixes (e.g., common, stock, preferred, LTD, Inc, INC., corporation, etc.)</li>
<li>Divide them roughly to 300 groups by prefix (e.g., companies with suffix <code>1</code> in group one, companies with suffix <code>2</code> in group 2 such as 29 metals... , and so on).</li>
<li>Then, you have roughly 100 companies per each category. You can write much simpler regular expression pattern for each category/group.</li>
<li>Every time you see a new company or a mismatch in your outputs, then you have to only modify the related pattern to that category/group.</li>
</ul>
<hr />
","2","Answer"
"78470197","78468674","<p>The <code>plt.scatter</code> function does not accept multiple datasets to plot, you can only pass an x and y. So, as suggested in the comments, you'll need to loop through what you want to plot.</p>
<pre class=""lang-py prettyprint-override""><code>y_cols = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;]
for y_col in y_cols:
    plt.scatter(x_train[&quot;date&quot;], x_train[y_col], c=kmeans.labels_)
</code></pre>
<p>You didn't provide enough code for us to run, so I cannot test this to see if I made any mistakes.</p>
","0","Answer"
"78470418","78470290","<p>You have forgotten to drop the 'weekly_Sales' from the X dataset, which causes overfitting problems. This occurs because the target value is included in your features, allowing your model to make perfect predictions, hence a MAE and RMSE of 0.</p>
<p>Secondly :
It would also be better to split the dataset first, then apply standard scaling. Scaling the entire dataset before splitting into train and test sets can lead to data leakage. For more information on data leakage, see  <a href=""https://medium.com/@megha.natarajan/scaling-data-before-or-after-train-test-split-35e9a9a7453f#:%7E:text=The%20Correct%20Sequence%3A%20Split%2C%20Then,used%20to%20create%20the%20model."" rel=""nofollow noreferrer"">this</a></p>
","1","Answer"
"78471408","78469835","<p>Your understanding is correct. You should follow these steps for setting up your neural network for binary classification:</p>
<p><em>Input Layer: 5 neurons (corresponding to your 5 inputs). First Hidden Layer: 32 neurons with ReLU activation. Second Hidden Layer: 32 neurons with ReLU activation. Output Layer: 1 neuron with Sigmoid activation (since this is a binary classification task).</em></p>
<p>The process of adjusting the weights of the neural network is generally handled through a method known as <a href=""https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd"" rel=""nofollow noreferrer"">backpropagation</a>, which is coupled with an optimization algorithm like Stochastic Gradient Descent (SGD), Adam, or others.</p>
<p>Below is an example of how you can implement this using TensorFlow:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Define the model
model = Sequential([
    Dense(32, input_shape=(5,), activation='relu'),
    Dense(32, activation='relu'),                    
    Dense(1, activation='sigmoid')             
])


model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Assuming X_train and y_train are your data matrices
# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
</code></pre>
","1","Answer"
"78474243","78471070","<p>If in your environment the seams may be different depending on the area of the chair it is recommended to classify the different seams (different bounding boxes for the same line).</p>
<p>To give an answer to the issue of instance segmentation.
You can use yolov5 without too many problems, you can learn to use <a href=""https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I don't know if you need particular applications, with yolov5 it is often necessary to work on the code to customize the problem</p>
","0","Answer"
"78474490","78474448","<p>It seems this model is an OpenCLIP only model right now. You can not load it the usual way.</p>
<p>You should first install <code>open_clip</code> with <code>pip install open_clip_torch</code> then use this code:</p>
<pre><code>import open_clip

model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')
tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')
</code></pre>
","1","Answer"
"78475864","78475573","<p>This post addresses potential causes and fixes for issues that might arise in the provided cosine similarity function:</p>
<ol>
<li>Non-numerical Data Types:</li>
</ol>
<p>Problem: The user_pref or restaurant_vec might contain strings or non-numerical data types.
Fix: Convert these values to floats using a list comprehension:
Python
user_pref = [float(x) for x in user_pref]</p>
<ol start=""2"">
<li>Empty Vectors:</li>
</ol>
<p>Problem: The np.linalg.norm function can raise errors with empty vectors (all zeros).
Fix: The code includes a check for this, but consider improving clarity.
Improved Code Snippet:</p>
<p>Python
def cosine_similarity(user_pref, restaurant_vec):</p>
<h1>Ensure vectors have the same dimensions (padding with zeros if necessary)</h1>
<h1>... (unchanged padding logic)</h1>
<h1>Check for empty vectors</h1>
<p>if not any(user_pref) or not any(restaurant_vec):
return 0  # Avoid division by zero</p>
<h1>Convert to NumPy arrays for efficiency (consider alternative for small datasets)</h1>
<p>user_pref = np.array(user_pref, dtype=float)
restaurant_vec = np.array(restaurant_vec, dtype=float)</p>
<p>return np.dot(user_pref, restaurant_vec) / (np.linalg.norm(user_pref)</p>
","0","Answer"
"78477588","78473057","<p>If <code>labels</code> is a <code>pandas.Series</code> object then <code>labels[train_index]</code> will be using the dataframe's original indices rather than the linear numpy indices. The two won't necessarily be aligned.</p>
<p>Either ensure <code>labels</code> is converted to a numpy array, or alternatively keep it as a Series but index into it using <code>labels.iloc[train_index]</code>.</p>
<hr />
<p>The <code>labels</code> supplied to <code>def train_and_validate(self, data, labels)</code> are getting overwritten with the subsequent lines <code>for inputs, labels in train_loader:</code> and <code>for inputs, labels in val_loader:</code>.</p>
<p>Try renaming the latter two to <code>labels_train</code> and <code>labels_val</code>.</p>
","0","Answer"
"78478815","78460776","<p>I'm not that familiar with the algorithm, but the problem is in <code>update_C(...)</code> as it alters the dimensions of C from (5,3) to (3,5) on this row:</p>
<pre><code>C = update_C(X, X_tilde, L, C, gamma, alpha, lam, J)
</code></pre>
<p>To be more precise, when doing <code>C_new = pinv(C_old - t * gradient_f)</code>, the inverse, by definition, will have switched dimensions.</p>
<p>One guess is that you might miss a transpose in <code>update_C(...)</code>. I have not studied it in detail, but assuming that you are trying to update C with eq.13 from <a href=""https://proceedings.mlr.press/v202/kumar23a/kumar23a.pdf"" rel=""nofollow noreferrer"">this paper</a>, it seems like the transpose of C should be used both in the inverse and in the gradient calculation.</p>
<p><img src=""https://i.sstatic.net/AJvEGqa8.png"" alt=""Equation 13 in ][2]"" /></p>
","0","Answer"
"78480561","78475975","<p>I can't tell you where you're going wrong, but I can tell you that this is a vocab size issue.</p>
<p>Your target value in your loss contains the value <code>50001</code>.</p>
<p>Your target values are attempting to index into your model output tensor, which is of size <code>(bs, C, ...)</code> where <code>C</code> is the number of items in your vocab. For your model, <code>C &lt; 50001</code>, so you get an indexing error.</p>
<p>You need to set your model up so that the size of the output matches the size of your vocabulary. You should inspect the size of the model output, verify what size it actually is, then update the model config to use the correct size matching your vocab size.</p>
","-1","Answer"
"78481888","78481612","<p>You need to build a partial function with <code>delta</code> set. There are several ways to do this. One would be the following:</p>
<pre><code>model.compile(loss=lambda x, y: huber(x, y, delta=delta), optimizer='adam')
</code></pre>
<p>or just use the capital H <code>Huber</code>:</p>
<pre><code>model.compile(loss=lambda x, y: tf.keras.losses.Huber(delta=delta), optimizer='adam')
</code></pre>
","0","Answer"
"78482086","78439767","<p>Well, actually using <code>neuralnetwork</code> backend and knowing the fact that depth shape is <code>1xHxW</code> the following modifications made for <code>shape</code> and <code>scale</code> values did the trick:</p>
<pre class=""lang-py prettyprint-override""><code>import coremltools as ct
import torch

x = torch.rand(1, 3, 518, 518)
traced_model = torch.jit.trace(depth_anything, x, strict=False)

mlmodel = ct.convert(traced_model,inputs=[ct.ImageType(shape=x.shape,bias=[-0.485/0.229,-0.456/0.224,-0.406/0.225],scale=1.0/255.0/0.226)], convert_to='neuralnetwork')

mlmodel.save('/content/drive/MyDrive/trained_models/depth_anything.mlmodel')
</code></pre>
<p>I'm not sure it's a good solution, but as a workaround I've achieved it had covered all my needs</p>
","1","Answer"
"78483366","78439640","<p>Switching to Linux certainly works, but seems to not be necessary.
If you choose this route, I recommend you start with <a href=""https://ubuntu.com/download"" rel=""nofollow noreferrer"">Ubuntu</a> as there is a lot of information available.</p>
<p>CUDA Toolkit however is only available if you have a NVIDIA GPU and will not work for you.</p>
<p>To get Pytorch to work on Windows, check out this stack-overflow question as it is quite detailed: <a href=""https://stackoverflow.com/questions/63008040/how-to-use-amd-gpu-for-fastai-pytorch"">How to use AMD GPU for fastai/pytorch?</a></p>
","1","Answer"
"78485774","78456621","<pre><code>drug.spending &lt;- drug.spending %&gt;%
  mutate(CAGR_Direction = ifelse(CAGR_Avg_Spnd_Per_Dsg_Unt_18_22 &gt; 0, 'Up', 'Down'))
drug.spending$CAGR_Direction &lt;- factor(drug.spending$CAGR_Direction, levels = c('Down', 'Up'))
</code></pre>
<h3>Factor the CAGR directions</h3>
","0","Answer"
"78488232","78486992","<p>There are several topics to address here. First of all, you don't need to create a container just to include additional dependencies.</p>
<h2>Using additional dependencies with an Estimator</h2>
<p>You can add dependencies to an Estimator by providing <code>source_dir</code> and including a requirements.txt file in the referenced source directory.</p>
<p>From the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator"" rel=""nofollow noreferrer"">Estimator API documentation</a>:</p>
<blockquote>
<p><strong>source_dir</strong>
The absolute, relative, or S3 URI Path to a directory with any other training source code dependencies aside from the entry
point file. If source_dir is an S3 URI, it must point to a tar.gz
file. Structure within this directory is preserved when training on
Amazon SageMaker.</p>
</blockquote>
<p>The most straightforward way to include a <code>source_dir</code> is to have it locally next to your notebook.</p>
<pre><code>|----- example-notebook.ipynb
|----- src
        |----- train.py
        |----- requirements.txt
</code></pre>
<p>You can then configure your estimator to use the source directory with the following configuration:</p>
<pre><code>estimator = Estimator(
    [...]
    entry_point=&quot;train.py&quot;,
    source_dir=&quot;src&quot;,
    [...]
)
</code></pre>
<p>If <code>source_dir</code> is specified, then <code>entry_point</code> must point to a file located at the root of <code>source_dir</code>. The training job will automatically install dependencies from the provided <code>requirements.txt</code>.</p>
<p>Since you're using Scikit-Learn, you could also use the <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html"" rel=""nofollow noreferrer"">SKLearn Estimator</a>, which already bundles several dependencies and provides a simplified interface compared to the general Estimator.</p>
<h2>Fixing Hyperparameter Parsing</h2>
<p>If you'd like to use your code as is, then you could adapt your code as follows:</p>
<pre><code>import json

# JSON encode hyperparameters
def json_encode_hyperparameters(hyperparameters):
    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}


hyperparameters = json_encode_hyperparameters({
    &quot;vertical&quot;: vertical,
    &quot;s3_bucket&quot;: s3_bucket,
    &quot;target_column&quot;: target_column, 
    &quot;test_size&quot;: 0.2
})

estimator = Estimator(
    image_uri=container,
    role=role,
    instance_count=1,
    instance_type=instance_type,
    volume_size=10,
    output_path=s3_output_location,
    sagemaker_session=sagemaker.Session(),
    hyperparameters=hyperparameters
)
</code></pre>
<p><code>set_hyperparameters</code> expects the input as <code>kwargs</code>, while the <code>hyperparameters</code> property accepts the input in different formats. Therefore, you can't use the JSON-encoded dict with <code>set_hyperparameters</code>; instead, use it with the <code>hyperparameters</code> property.</p>
","1","Answer"
"78488771","78433332","<p>I made some edits to the CustomDataset code you provided, which might help you:</p>
<pre><code>class CustomDataset(InMemoryDataset):
    def __init__(self, listOfDataObjects):
        super().__init__()
        self.data, self.slices = self.collate(listOfDataObjects)
    
    def __len__(self):
        return len(self.slices)
    
    def __getitem__(self, idx):
        sample = self.get(idx)
        return sample
</code></pre>
<p>A list of Data objects is inefficient to store. That's why collation is done most of the time at the end of the initialization. To retrieve items from the data, you can't index the collated Data object, this will result in a KeyIndexError like you experienced. However, Pytorch Geometric <a href=""https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/in_memory_dataset.html#InMemoryDataset.get"" rel=""nofollow noreferrer"">InMemoryDataset</a> implemented a <code>get</code> method that does the indexing for you! They use a <code>separate</code> method that can handle the collated objects and uses the slices provided to separate the indexed object from the collated object.</p>
","0","Answer"
"78490278","78490240","<p>I am sorry, but it is absolutely not possible to use numeric and array inputs simultaneously in a machine learning model.</p>
","1","Answer"
"78490871","78490506","<blockquote>
<p>Apply mode imputation on training and test set independently</p>
</blockquote>
<p>In general, when making predictions (including on the test set), you can have as little as one single sample. In such a case, which is perfectly permissible for the test set (unlike the training set), the model will fail to impute missing values as there's nothing to go on. So I think the more robust approach would be to fit an imputer on the training set, and use that fitted imputer to handle NaNs in the test set.</p>
<p><code>sklearn</code> pipelines handle this naturally, as once you fit the pipeline on the training data, the fitted estimators can be seamlessly applied to any other set.</p>
<blockquote>
<p>is it okay to perform the mode imputation on the whole training set and then perform RandomSearchCV? [...] Or should I perform the imputation in each fold of RandomSearchCV to avoid data leakage?</p>
</blockquote>
<p>Learning the imputation <em>after</em> the split means that the training data can't take the validation samples into account. This keeps the validation set &quot;unseen&quot;, so you get a more realistic measure of generalisation performance.</p>
<blockquote>
<p>I cannot figure out how to apply the mode imputation only to the specific feature I need</p>
</blockquote>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer"" rel=""nofollow noreferrer""><code>ColumnTransformer</code></a> and <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer"" rel=""nofollow noreferrer""><code>make_column_transformer</code></a> are used to selectively apply transformations to different columns. Suppose you wanted to impute for a column named <code>&quot;feature_12&quot;</code>, leaving the rest of the columns intact:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer

preprocessor = make_column_transformer(
  (SimpleImputer(strategy='mode'), ['feature_12']),
  remainder='passthrough',
)
</code></pre>
<p>You can chain on an estimator, and give the resulting pipeline to <code>sklearn</code>-compatible CV methods.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.pipeline import make_pipeline
from sklearn.ensemble import HistGradientBoostingRegressor

pipeline = make_pipeline(preprocessor, HistGradientBoostingRegressor())

search_results = RandomizedSearchCV(
  pipeline, ...
).fit(X, y)
</code></pre>
<p>The resulting pipeline:</p>
<p><a href=""https://i.sstatic.net/LhcKVcgd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhcKVcgd.png"" alt=""enter image description here"" /></a></p>
<p>The inner box is the column transformer, which has a left branch that imputes for the specified column, and a right branch that passes the other columns on. The regressor comes after the column transformer, operating on the imputed data.</p>
","0","Answer"
"78492388","78444040","<p>Hi:  It looks like you are just trying to predict the rotten blueberries on a bush based on the predictors bugs, has_rotten_berry, berry_count and weather.  I will simply build a model with bugs, has_rotten_berry, berry_count and weather and ignore all the other columns.</p>
<p>The name of the berry bush should not matter to rotten blueberries.  The date is not important because this should be reflected by the weather column.</p>
","0","Answer"
"78494023","78493918","<p>You're predicting &quot;Malignant&quot;, not &quot;Benign&quot;, since the outcome is a factor with 2 levels and the R glm function predicts the <em>second</em> level when running logistic regression.</p>
<pre><code>levels(training_set$Class)
[1] &quot;Benign&quot;    &quot;Malignant&quot;
</code></pre>
<p>Just switch &quot;Benign&quot; with &quot;Malignant&quot; for your two <code>ifelse</code> commands and you'll get a much better accuracy.</p>
<pre><code>predicted_y_training &lt;- ifelse(prob_y_train &gt;= 0.5, &quot;Malignant&quot;, &quot;Benign&quot;)
predicted_y_test &lt;- ifelse(prob_y_test &gt;= 0.5, &quot;Malignant&quot;, &quot;Benign&quot;)
...

print(cm_test)
           predicted_y_test
            Benign Malignant
  Benign        86         3
  Malignant      2        46
</code></pre>
","1","Answer"
"78496583","78491587","<p>Change to <code>softmax = torch.nn.Softmax(dim=1)</code> in order to softmax over the channels dimension <code>dim=1</code></p>
<p>In the training loop that starts <code>for batch_idx, (data, targets) in train_loop:</code>, check the following:</p>
<ul>
<li><code>targets.shape</code> should be <code>(batch, 512, 512)</code>
<ul>
<li>At each pixel, <code>targets</code> should be an integer ranging <code>[0, n_classes - 1]</code>, denoting the class</li>
</ul>
</li>
<li><code>data.shape</code> should be <code>(batch, channels, 512, 512)</code></li>
<li><code>model(data).shape</code> should be <code>(batch, channels, 512, 512)</code></li>
</ul>
<hr />
<p>Original reply:</p>
<p>Does the train loss go down at all? Useful to print it out at each epoch.</p>
<p>Try fitting just a single image or one small batch - keep running it until the loss goes down further and further. A sensible output should start emerging, like blobs roughly in the right place. If not, it suggests the pipeline is broken somewhere since it's failing to learn at all</p>
<p>It might also be worth working with down-sampled images initially and limiting the mask to a single channel. They will help the net converge more quickly and highlight convergence issues.</p>
","1","Answer"
"78497035","78496983","<p>Your are hardcoding the weights and bias calculations. In this case, the optimizer cannot have access to that.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim


def train_step(w, b, optimizer, scheduler, loss_function):
    optimizer.zero_grad()
    loss = loss_function()
    loss.backward()
    optimizer.step()
    scheduler.step()
    return loss.item()


def loss_function():
    rand_input = torch.randn(64, input_size)
    target = torch.randn(64, output_size)
    output = rand_input.mm(w) + b
    loss = nn.MSELoss()(output, target)
    return loss


input_size, output_size = 100, 3
learning_rate = 0.01
w = torch.randn(input_size, output_size, requires_grad=True)
b = torch.randn(output_size, requires_grad=True)
trainable_variables = [w, b]
optimizer = optim.SGD(trainable_variables, lr=learning_rate, momentum=0.9)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)
steps = 10000
display_step = 100


graph1 = []

for i in range(steps):
    loss = train_step(w, b, optimizer, scheduler, loss_function)
    if i % display_step == 0:
        graph1.append(loss)
        print(f'Epoch {i} \t Training Loss: {loss}')

graph = torch.tensor(graph1)


</code></pre>
<h3>Prints</h3>
<pre><code>Epoch 0      Training Loss: 94.3976058959961
Epoch 100    Training Loss: 1.1436320543289185
Epoch 200    Training Loss: 1.1590440273284912
Epoch 300    Training Loss: 0.9118895530700684
Epoch 400    Training Loss: 0.8449723720550537
Epoch 500    Training Loss: 0.9973302483558655
Epoch 600    Training Loss: 0.9681441783905029
Epoch 700    Training Loss: 1.1692309379577637
Epoch 800    Training Loss: 1.0565043687820435
Epoch 900    Training Loss: 1.0424968004226685
Epoch 1000   Training Loss: 0.9199855923652649
Epoch 1100   Training Loss: 1.1443506479263306
Epoch 1200   Training Loss: 0.9741299748420715
Epoch 1300   Training Loss: 1.1515040397644043
Epoch 1400   Training Loss: 1.2819862365722656
Epoch 1500   Training Loss: 0.9993045926094055
Epoch 1600   Training Loss: 1.066098928451538
Epoch 1700   Training Loss: 1.0772987604141235
... 

</code></pre>
","0","Answer"
"78499906","78497836","<p>To use a custom model for batch transforms, you need to first to invoke <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.create"" rel=""nofollow noreferrer"">Model.create()</a> to register the model with SageMaker.</p>
<blockquote>
<p>In a CreateModel request, container definitions include the
ModelDataUrl parameter, which identifies the location in Amazon S3
where model artifacts are stored. When you use SageMaker to run
inferences, it uses this information to determine from where to copy
the model artifacts. It copies the artifacts to the /opt/ml/model
directory in the Docker container for use by your inference code.</p>
</blockquote>
<p>You can find further details in <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.create"" rel=""nofollow noreferrer"">How SageMaker Loads Your Model Artifacts</a>.</p>
","0","Answer"
"78500768","78497891","<p>Yes, you need to preprocess/prepare your validation/test data as well. Sklearn makes this very simple for you with <a href=""https://scikit-learn.org/stable/modules/compose.html"" rel=""nofollow noreferrer"">pipelines</a>.
Basically, every <a href=""https://scikit-learn.org/stable/data_transforms.html"" rel=""nofollow noreferrer"">data transformer</a> and model can be part of a pipeline.
If there sklearn has not the right data transformer for your use case, it is also very simple to write your own one.</p>
<p>To give you a little demonstration (this works as is):</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import load_diabetes
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import QuantileTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import pandas as pd

RANDOM_STATE = 0

# define all your preprocessing steps
pipeline = make_pipeline(
    # fill nan with median
    SimpleImputer(missing_values=pd.NA, strategy=&quot;median&quot;), 
    QuantileTransformer(output_distribution=&quot;normal&quot;, random_state=RANDOM_STATE,),
    # regression model
    LinearRegression(),
)

# load some toy dataset for demonstration
X, y = load_diabetes(return_X_y=True, as_frame=True)

# split into training and test dataset
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=RANDOM_STATE,
)

# fit pipeline on train data
pipeline.fit(X_train, y_train)

# now only use .score, .predict etc.
train_score = pipeline.score(X_train, y_train)
test_score = pipeline.score(X_test, y_test)

print(f'Train: {train_score:.2f}, Test: {test_score:.2f}')
# Train: 0.51, Test: 0.38
</code></pre>
<p>The important part about pipelines lies in the methods you call. The same method will be sequentially called on all steps in the pipeline. So you <code>fit</code> your model with the training data. Afterward, you only call the <code>score</code> or <code>predict</code> method. If you use <code>fit</code> again, you would overwrite the parameters learned from the first-pass.</p>
","0","Answer"
"78501753","78501325","<p>The issue lies in how you create the <code>train</code> and <code>test</code> datasets. Since your dataset is already split into JSON files, you need to specify the <code>split</code> when reading the data using the <code>split</code> parameter. If you don't specify the split every time you run <code>ds.load_dataset</code>, it automatically creates train and test splits for you. According to the function documentation:</p>
<pre><code>split (`Split` or `str`):
Which split of the data to load.

**If `None`, it will return a `dict` with all splits** (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).

If a split is provided, it will return a single Dataset.

Splits can be combined and specified like in tensorflow-datasets.
</code></pre>
<p>To address this issue, use the following code:</p>
<pre><code>trainDS = ds.load_dataset(&quot;json&quot;, data_files=fileName, split='train')
evalDS = ds.load_dataset(&quot;json&quot;, data_files=fileName, split='test')
</code></pre>
","0","Answer"
"78508798","78506114","<p>Transformers are touchy when it comes to exactness of settings.
I ended up using command to generate config.cfg for me.
That way I know where exactly I deviated from working version if something goes wrong, as long as I change params one by one.</p>
<pre><code>!python -m spacy init config      \
    &quot;/path/to/project/config.cfg&quot; \
    --lang en                     \
    --pipeline transformer,ner    \
    --optimize accuracy           \
    --gpu                         \
    --force
</code></pre>
<p>This worked right away:</p>
<p><a href=""https://i.sstatic.net/Fyrg0SrV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fyrg0SrV.jpg"" alt=""enter image description here"" /></a></p>
","0","Answer"
"78508957","78507382","<p>No, you should turn off either the bias term of the polynomial transformer or the intercept term of the linear regression.</p>
","0","Answer"
"78513943","78513688","<p>Here is a simple, but real example to illustrate the concepts.</p>
<p>Brown eyes are dominant over blue eyes. My grandmother had blue eyes. Her husband came from a family with only brown eyes as far back as you go. My father, likewise. I had two children with a blue-eyed woman. Let's let <code>x</code> and <code>y</code> be the eye color of those children, with the eldest child being <code>y</code> and the younger being <code>x</code>. And to discuss the underlying genetics, let's use <code>B</code> for the gene for brown eyes, and <code>b</code> for blue.</p>
<p>First let's figure out <code>p(x, y)</code>. My mother got one gene from her mother and father, and so must be <code>bB</code>. She had brown eyes. My father's were <code>BB</code>. He also had brown eyes. Depending on what my mother gave me, I have even odds of <code>bB</code> and <code>BB</code>. Whichever one I actually have, I have brown eyes.</p>
<p>I then had children with a blue-eyed woman with genes <code>bb</code>.</p>
<p>IF my genes are <code>BB</code>, then my children are both <code>bB</code> and will have brown eyes.</p>
<p>IF my genes are <code>bB</code>, then each child has even odds of <code>bb</code> and <code>bB</code>, independently of each other. And therefore my children could come out <code>(blue, blue)</code>, <code>(blue, brown)</code>, <code>(brown, blue)</code> and <code>(brown, brown)</code> with equal likelihood.</p>
<p>When you add it up, here are the odds we get:</p>
<pre><code>(blue, blue): 1/8
(brown, blue): 1/8
(blue, brown): 1/8
(brown, brown): 5/8
</code></pre>
<p>That is <code>p(x, y)</code>. Let's show how to generate a pair.</p>
<p>First work out the cumulative probabilities. In that order it is <code>1/8, 1/4, 3/8, 1</code>. Now just roll a random number. I just got <code>0.7284333516674881</code>. Comparing to the cumulative probabilities, I got <code>(brown, brown)</code>, so that's what I generate. (Funnily enough, that's also the real eye colors of my children! Not a giant coincidence, but still...)</p>
<p>What random number corresponds to what output will change if I changed the order in which I list the probabilities. But the outputs will come up with the right frequencies no matter how I do it.</p>
<p>Now let's work out <code>p(x | y)</code>. If you use Bayes' formula you can verify:</p>
<pre><code>(blue|blue): 1/2
(brown|blue): 1/2
(blue|brown): 1/6
(brown|brown): 5/6
</code></pre>
<p>From this, we can figure out what the color of the second child's eyes are likely to be, given the color of the first child's eyes. This is exactly what we need for a discriminative algorithm.</p>
<p>But we have absolutely no idea how to tell from these numbers whether, a priori, the odds of the first child having <code>brown</code> eyes are <code>1/2</code> (naive guess) or <code>3/4</code> (the real answer). If they were <code>1/2</code> then our first table would have been:</p>
<pre><code>(blue, blue): 1/4
(brown, blue): 1/4
(blue, brown): 1/12
(brown, brown): 5/12
</code></pre>
<p>Obviously this is going to generate a very different distribution than the real one. It just happens to give the same <code>p(x|y)</code>. And so we need more information for the generative algorithm than the discriminative. Specifically, we need to know <code>p(y)</code>.</p>
<p>Does that clarify things for you?</p>
","3","Answer"
"78514087","78497575","<p>Several things to note:</p>
<ol>
<li><p>I suspect you are using the old abandoned repo, just based on the paths you give in the example commands.  Note the AlexeyAB repo was abandoned in July 2021.  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#which_repos_to_use_and_avoid"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#which_repos_to_use_and_avoid</a></p>
</li>
<li><p>The new Darknet/YOLO repo people should be using is the one sponsored by Hank.ai.  All new development is being done there.  See here:  <a href=""https://github.com/hank-ai/darknet#table-of-contents"" rel=""nofollow noreferrer"">https://github.com/hank-ai/darknet#table-of-contents</a></p>
</li>
<li><p>Looks like your training images contains cropped images of fruit, is that right?  That first image of an apple that takes up almost 100% of the image...if you train with that, you're telling Darknet/YOLO that the objects you want to detect must take up ~100% of the image.  So if you then pass in images that have apples, it will fail to detect anything.  See here for details:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#crop_training_images"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#crop_training_images</a></p>
</li>
<li><p>Your training command is not the recommended one.  Please see the Darknet/YOLO FAQ where this is explained:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#training_command"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#training_command</a></p>
</li>
<li><p>You should also post a copy of your chart.png file.  Maybe have a peek at your other chart_*.png files as well to see if all 3 classes are training the same way.  But at the very least, please edit this question and add your chart.png file before trying to figure out why something cannot be detected.</p>
</li>
<li><p>I <strong>strongly</strong> recommend you download and run DarkMark to test your annotations <em>prior</em> to training.  I cannot begin to count the number of issues this has solved over the years.  It was written specifically to find and prevent problems.  (Disclaimer:  I'm the author.)  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#what_software_is_used"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#what_software_is_used</a></p>
</li>
<li><p>Join the Darknet/YOLO discord server.  Lots of folks there to help people get started.  <a href=""https://discord.gg/zSq8rtW"" rel=""nofollow noreferrer"">https://discord.gg/zSq8rtW</a></p>
</li>
</ol>
","0","Answer"
"78521330","78514097","<p>You seem to mixing some things up here. The model contains an exponential at the end, so the targets should be <code>y</code>, not <code>log(y)</code>, OR you need to remove the exponential in the model. Also, if you have an exponential in the model, it is incorrect to use <code>np.exp</code> <em>again</em> after <code>predict</code>. This version works fine:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Generate data
x1 = np.random.randint(1, 21, size=(1000, 1))
x2 = np.random.randint(1, 21, size=(1000, 1))
y = 3 * (x1 ** 2) * (x2 ** 3)

# Preprocess data
log_x1 = np.log(x1)
log_x2 = np.log(x2)
log_inputs = np.hstack((log_x1, log_x2))

# Define model
model = Sequential()
model.add(Dense(1, input_dim=2, kernel_initializer='ones', bias_initializer='zeros'))

# Compile model
model.compile(optimizer=Adam(learning_rate=0.01), loss='mae')

# Train model
model.fit(log_inputs, np.log(y), epochs=100, batch_size=32)

# Evaluate model
test_x1 = np.array([[2], [4], [5]])
test_x2 = np.array([[3], [7], [19]])
test_inputs = np.hstack((np.log(test_x1), np.log(test_x2)))
predicted = model.predict(test_inputs)
print(np.exp(predicted))
</code></pre>
","0","Answer"
"78522286","78522177","<p>One thing to bear in mind is that for many popular choices of activation function (EG Relu), in any neuron that doesn't have a bias, an input value of zero will map to an output value of zero. Likewise, if your whole network uses such activation functions (without normalisation), the same applies: zero inputs get mapped to zero outputs, so dark pixels (with value zero) will map to zero, and effectively behave linearly. If you want all pixels to behave non linearly (which is generally true for neural networks), one solution is to use biases.</p>
<p>The situation is slightly different for transformers: they often <em>don't</em> use biases, partly because they use frequent Layer Normalisation layers, which effectively add their own biases.</p>
<p>But in some cases, EG SWIN transformers,  the size of the attention map is always known (equal to window size), and they add a learned positional bias directly to the attention maps.</p>
","1","Answer"
"78524980","78524575","<p>You can try following specific versions, as they work</p>
<pre><code>
pip install scikit-learn==0.24.2 imbalanced-learn==0.8.0

</code></pre>
","3","Answer"
"78525476","78518695","<p>The calculation of 'accuracy' for a classification model, against some set of inputs with known classes, is always the same:</p>
<ul>
<li>run the model against each item</li>
<li>if its prediction matches the target class, count it as correct</li>
<li>accuracy is the count-of-correct divided by the total-number-of-tests</li>
</ul>
<p>If you have trained a FastText model in its <code>-supervised</code> mode, and can ask it to predict labels for new texts, and you have additional texts with known correct labels, you have everything you need to do the above calculation.</p>
<p>Testing the accuracy against the same datapoints used to train the model will give an artificially high accuracy that may be somewhat useful as a relative measure against other design choices, but will not likely be an accurate estimate of its accuracy on other yet-to-be-seen items with other idiosyncracies the model had no chance to learn.</p>
<p>Testing the accuracy against a held-out set of known-class items – a 'test' or 'validation' set – generates an accuracy more likely to estimate future accuracy againt similar not-yet-seen items.</p>
","0","Answer"
"78527523","78527509","<p>This is because you are returning a history and nn_model, which would return a type tuple. if you just return nn_model and then do the same it would work. Else try this:</p>
<pre><code>predicted = least_loss_model[0].predict(x_test)
</code></pre>
<p>This should work.</p>
","1","Answer"
"78530754","78530486","<p>In the function of <code>state_action</code>, instead of getting the probability itself, it is better to get the logarithm of the probability. So, your function should be:</p>
<pre><code>def select_action(state):
    state = torch.from_numpy(state).float().to(device)
    mean = F.softmax(actor(state), dim=-1)       # This is the mean value of the output
    m = Categorical(mean)            # This is our distribution based on mean
    action = m.sample()
    probs = m.log_prob(action)       # This is the log_prob (I didn't change the name)
    state_value = critic(state)
    actor.saved_actions.append((probs.detach(), state_value, state, action.detach()))   # changed to probs.detach()
    return action.item()
</code></pre>
<p>Also, in the <code>finish_episode</code> function, you need to change the equation of the ratio:</p>
<pre><code>ratios = torch.exp(new_probs - old_probs)
</code></pre>
<p>Also, for the problem of <code>num_epochs</code>, you are using the old state values for the policy update. You need to use the critic for update. Your code is modified as follow (check the comments):</p>
<pre><code>def finish_episode():
    # Calculating losses and performing backprop
    R = 0
    saved_actions = actor.saved_actions
    returns = []
    epsilon = 0.2
    num_epochs = 3

    for r in actor.rewards[::-1]:
        R = r + 0.99 * R # Gamma is 0.99
        returns.insert(0, R)
    returns = torch.tensor(returns, device=device)
    returns = (returns - returns.mean()) / (returns.std() + eps)

    old_probs, state_values, states, actions = zip(*saved_actions)

    old_probs = torch.stack(old_probs).to(device)
    state_values = torch.stack(state_values).to(device)
    states = torch.stack(states).to(device)
    actions = torch.stack(actions).to(device)
    values = critic(states).to(device).detach()    # value function is added and detached
    
    advantages = returns - values.squeeze()      # value function from critic added to advantage

    for epoch in range(num_epochs):

        values = critic(states).to(device)   # also, you need to pudate the critic in the loop
        new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()

        # ratios = new_probs / old_probs
        ratios = torch.exp(new_probs - old_probs)

        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages

        actor_loss = -torch.min(surr1, surr2).mean()
        # actor_loss = -surr1.mean()
        
        actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)
        actor_optimizer.step()

        # if epoch == num_epochs - 1:
        critic_loss = F.smooth_l1_loss(values.squeeze(), returns)       # it is changed from state_values to values
        
        critic_optimizer.zero_grad()
        critic_loss.backward(retain_graph=True)
        critic_optimizer.step()

    del actor.rewards[:]
    del actor.saved_actions[:]
</code></pre>
<p>I changed the learning rates:</p>
<pre><code>actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)
critic_optimizer = optim.Adam(critic.parameters(), lr=1e-5)
</code></pre>
<p>I ran the code and got following results (just give it some time to run):</p>
<pre><code>Episode 1330 Reward: 430.00 Average reward: 420.69
Episode 1340 Reward: 500.00 Average reward: 428.80
Episode 1350 Reward: 500.00 Average reward: 436.10
Episode 1360 Reward: 458.00 Average reward: 453.50
Episode 1370 Reward: 500.00 Average reward: 465.66
Episode 1380 Reward: 498.00 Average reward: 462.26
Episode 1390 Reward: 500.00 Average reward: 475.03
Solved, running reward is now 475.02882655892296 and the last episode runs to 500 timesteps
</code></pre>
<p>I am pretty sure, you can get better results after some hyperparameter tunning</p>
","0","Answer"
"78530846","78524575","<p>You have 1.5 version, try this
pip uninstall scikit-learn
pip install scikit-learn==1.4.0</p>
","1","Answer"
"78530861","78530486","<p>It turned out that I was directly using my logits in place of the new probability when calculating the probability ratio.</p>
<p>So instead of:</p>
<pre><code>new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()
</code></pre>
<p>It should have been:</p>
<pre><code>new_probs = F.softmax(actor(states)).gather(1, actions.unsqueeze(-1)).squeeze()
</code></pre>
","0","Answer"
"78531882","78524575","<p>This worked for me <code>pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1</code></p>
","2","Answer"
"78533674","78533567","<ol>
<li><p>Make sure you're using the most up-to-date repo for Darknet.  The new repo is this one:  <a href=""https://github.com/hank-ai/darknet#table-of-contents"" rel=""nofollow noreferrer"">https://github.com/hank-ai/darknet#table-of-contents</a></p>
</li>
<li><p>The question you ask is addressed in the FAQ.  See the question &quot;How to run against multiple images&quot; here:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#json_output"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#json_output</a>  The command that used to be recommended was:</p>
</li>
</ol>
<blockquote>
<p>To process a list of images data/train.txt and save results of detection to result.json file use: <code>darknet detector test coco.data yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json &lt; test.txt</code></p>
</blockquote>
<ol start=""3"">
<li>But personally, I prefer using the DarkHelp CLI.  It is also open source, and provides a lot of functionality that doesn't exist in Darknet, as well as being much more robust.  For example, this is how you'd do the equivalent in DarkHelp:</li>
</ol>
<p><code>DarkHelp --json --threshold 0.9 yolo-obj.cfg yolo-obj_best.weights yolo-obj.names --list test.txt</code></p>
<ol start=""4"">
<li><p>Make sure you read the whole FAQ:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/</a></p>
</li>
<li><p>Join the Darknet/YOLO discord server if you have more questions:  <a href=""https://discord.gg/zSq8rtW"" rel=""nofollow noreferrer"">https://discord.gg/zSq8rtW</a></p>
</li>
</ol>
","1","Answer"
"78534788","78524575","<p>This is a known issue (<a href=""https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1081#issuecomment-2127245933"" rel=""nofollow noreferrer"">https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1081#issuecomment-2127245933</a>).
You can either</p>
<pre class=""lang-bash prettyprint-override""><code>pip install git+https://github.com/scikit-learn-contrib/imbalanced-learn.git@master
</code></pre>
<p>or downgrade scikit-learn to 1.5.</p>
","0","Answer"
"78537747","78530745","<p>RetrievalQA has been deprecated. Try this instead:</p>
<pre><code>from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

system_prompt = (
               &quot;Use the given context to answer the question. &quot;
               &quot;Context: {context}&quot;
           )
prompt = ChatPromptTemplate.from_messages(
    [
        (&quot;system&quot;, system_prompt),
        (&quot;human&quot;, &quot;{input}&quot;),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, QA_CHAIN_PROMPT)
chain = create_retrieval_chain(vectordb.as_retriever(), question_answer_chain)
   
result = chain.invoke({&quot;input&quot;: question}) 
</code></pre>
","0","Answer"
"78538856","78538485","<p><code>skater</code> needs module <code>ds-lime</code> but it seems <code>ds-lime</code> doesn't exists any more.</p>
<p>I found <a href=""https://www.kaggle.com/code/smitisinghal/model-interpretability-using-skater"" rel=""nofollow noreferrer"">tutorial on Kaggle</a> which installs <code>skater</code> (and <code>ds-lime</code>) using <code>conda</code>. I found <a href=""https://anaconda.org/conda-forge/ds-lime"" rel=""nofollow noreferrer"">ds-lime in conda</a> and it seems last version was created 5 years ago. It seems original module was created by <code>DataScience.com</code> but its page doesn't exist, and its GitHub also doesn't exist. It seems it was acquired by <code>oracle.com</code> and probably they removed all data. It seems you may have to use <code>conda</code> to work with it.</p>
<hr />
<p>Other idea: <code>conda</code> has <code>ds-lime</code> as <a href=""https://anaconda.org/conda-forge/ds-lime/files"" rel=""nofollow noreferrer"">files .tar.bz2</a> and you may download it, extract itm and you may try to put manually files from <code>site-packages</code> in folder with python modules.</p>
<p>I downloaded <code>noarch/ds-lime-0.1.1.27-py_1.tar.bz2</code>, extracted it, and I moved subfolders <code>ds_lime-0.1.1.27-py2.7.egg-info</code> and <code>lime</code> to <code>/usr/lib/python3.10</code> and later <code>install skater</code> starts working. But now it has problem with other modules - <code>scikit-image</code> and <code>wordcloud</code> - so I can't check if <code>skater</code> can really work.</p>
<pre class=""lang-none prettyprint-override""><code>!wget https://anaconda.org/conda-forge/ds-lime/0.1.1.27/download/noarch/ds-lime-0.1.1.27-py_1.tar.bz2

!tar -xvjf 'ds-lime-0.1.1.27-py_1.tar.bz2'

!cp -r site-packages/ds_lime-0.1.1.27-py2.7.egg-info /usr/lib/python3.10/
!cp -r site-packages/lime /usr/lib/python3.10/
</code></pre>
<p>Maybe problem is because it is very old code, and files for <code>ds-lime</code> have <code>py36</code> in name which may suggest that it was created for Python 3.6. So it may need to install <code>Python 3.6</code> to use it.</p>
","0","Answer"
"78538907","78535919","<p>I fixed the problem, turns out I was right, the problem is in the bidirectionnal layer, what fixed my problem for me was removing it, here is the new model with the same achieved accuracy, structure:</p>
<pre><code>input_layer = Input(shape=(1,),dtype=tf.string)
x = int_vectorize_layer(input_layer)
x = Embedding(vocab_size, embedding_dim)(x)
x = SpatialDropout1D(drop_lstm)(x)

#Changed Bidirectionnal to this
forward_lstm = LSTM(units=32, return_sequences=False, name='forward_lstm')(x)
backward_lstm = LSTM(units=32, return_sequences=False, go_backwards=True, name='backward_lstm')(x)
concat_lstm = Concatenate()([forward_lstm, backward_lstm])

x = Dropout(drop_lstm)(concat_lstm)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dense(32, activation='relu')(x)
output_layer = Dense(1, activation='sigmoid')(x)
model1 = Model(inputs=input_layer, outputs=output_layer)
model1.compile(loss='binary_crossentropy',
               optimizer='adam',
               metrics=['accuracy'])
</code></pre>
<p>now I can save and load the model without a problem.</p>
","0","Answer"
"78540482","78521104","<p>When working with Weights and Biases (W&amp;B/wandb) for hyperparameter (hp) optimization, you can use sweeps to systematically explore different combinations of hyperparameters to find the best performing set.</p>
<h1>What are Sweeps?</h1>
<p>Sweeps in W&amp;B allow you to define a set of hyperparameters to search over. When you create a sweep in the cli or in python, this creates a set (e.g., grid) of hyperparameters to search over. This (usually afaik) create a sweep with all the possible hyperparameters in your wandb account/server and later when you run an agent it fetches one and tries that hp and logs it to wandb.</p>
<h1>Understanding count and Agents</h1>
<ul>
<li>Agents: These are the workers that run the trials/hp attempts/runs -- basically try each hp. Each agent pulls a set of hyperparameters (hps) from the W&amp;B/wandb server, runs the (usually) training script (but in our example a search for optimal chinchilla scaling laws) with these hps, logs the results, and repeats.</li>
<li>Count: This is the number of trials/runs the agent will run. If you set count to 100, the agent will run 100 trials, each with a different combination of hyperparameters.</li>
</ul>
<p>Afaik, if you use grid search and count is higher than the total number of combinations, the sweep stops after running all combinations. If count is lower, it runs only the specified number of trials.</p>
<p>Afaik as long as an agent is running and the sweep has hps to try, it will keep fetching them from your wandb sweep server from your wandb account. You Can see the agents running (and kill them, pause them) etc for a run in your wandb's account.</p>
<blockquote>
<p><strong>I think the crux is that an agent continually fetches hps until the sweep is finished from your wandb's sweep run on the wandb website (or you kill it). You can run multiple agents until the sweep's hps are exhausted on your wanbd's website.</strong></p>
</blockquote>
<p>I will provide an example without multiprocessing then make it multiprocessing (mp):</p>
<h1>1 Example without Multiprocessing</h1>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import scipy.optimize as opt
import scipy.special
import wandb

# Define the synthetic scaling law function
def scaling_law(c, e, a, b, alpha, beta):
    return np.exp(e) + np.exp(a) * c[:, 0] ** (-alpha) + np.exp(b) * c[:, 1] ** (-beta)

# Generate synthetic data
np.random.seed(0)
C = np.array([[7e9, 2e12], [13e9, 2e12], [34e9, 2e12], [70e9, 2e12]])  # [m, Din] = [m, 2]
e_true, a_true, b_true = np.log(1.8172), np.log(482.01), np.log(2085.43)
alpha_true, beta_true = 0.3478, 0.3658
L_target = scaling_law(C, e_true, a_true, b_true, alpha_true, beta_true).reshape(-1, 1)  # [m, K]
L_target = np.repeat(L_target, 1, axis=1)  # [m , K]

# Define the cost function using the Huber loss
def aggregate_huber_loss(theta_sl, c, l_target, delta=1e-3):
    e, a, b, alpha, beta = theta_sl
    E, A, B = np.exp(e), np.exp(a), np.exp(b)
    l_pred = E + A * c[:, 0] ** (-alpha) + B * c[:, 1] ** (-beta)
    log_l_target = np.log(l_target)
    x1 = a - alpha * np.log(c[:, 0]).reshape(-1, 1)
    x2 = b - beta * np.log(c[:, 1]).reshape(-1, 1)
    x3 = e * np.ones((c.shape[0], 1))
    lse = scipy.special.logsumexp([x1, x2, x3], axis=0)
    h = scipy.special.huber(delta, lse - log_l_target)
    return h.sum()

# Training function to run each trial
def train():
    wandb.init()
    config = wandb.config

    initial_params = [config.e, config.a, config.b, config.alpha, config.beta]

    # Perform the optimization
    result = opt.minimize(aggregate_huber_loss, initial_params, args=(C, L_target), method='BFGS')
    optimized_params = result.x

    e_opt, a_opt, b_opt, alpha_opt, beta_opt = optimized_params
    loss = aggregate_huber_loss(optimized_params, C, L_target)

    wandb.log({
        &quot;e&quot;: e_opt,
        &quot;a&quot;: a_opt,
        &quot;b&quot;: b_opt,
        &quot;alpha&quot;: alpha_opt,
        &quot;beta&quot;: beta_opt,
        &quot;loss&quot;: loss
    })

# Sweep configuration for grid search
sweep_config = {
    &quot;method&quot;: &quot;grid&quot;,
    &quot;metric&quot;: {
        &quot;name&quot;: &quot;loss&quot;,
        &quot;goal&quot;: &quot;minimize&quot;
    },
    &quot;parameters&quot;: {
        &quot;e&quot;: {
            &quot;values&quot;: [-1, 0, 1]
        },
        &quot;a&quot;: {
            &quot;values&quot;: [0, 5, 10]
        },
        &quot;b&quot;: {
            &quot;values&quot;: [0, 5, 10]
        },
        &quot;alpha&quot;: {
            &quot;values&quot;: [0, 1, 2]
        },
        &quot;beta&quot;: {
            &quot;values&quot;: [0, 1, 2]
        }
    }
}

# Initialize the sweep
sweep_id = wandb.sweep(sweep_config, project=&quot;scaling-law-optimization&quot;)

# Print the sweep URL and ID
print(f&quot;Sweep URL: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/sweeps/{sweep_id}&quot;)
print(f&quot;Sweep ID: {sweep_id}&quot;)

# Run the sweep
# wandb.agent(sweep_id, function=train, count=10)  # only tries 10 out og G^5 sweeps
wandb.agent(sweep_id, function=train)  # tries all G^5 hps! Sweeps them all!
</code></pre>
<p>my understanding is as long as the agents are running it keeps fetching hps fromt he server sweep until the sever (your wandb site/account) for this sweep is exhausted. Some sweeps like random and bayesian afaik can run forever! So count is important here (or manually killing it).</p>
<h1>2 Example with Multiprocessing</h1>
<p>The main idea I think is realizing that when you create a sweep (in python or in the cli), the process fetching hps/trials to try is the agent. So my suggestions to parallelize over the agent e.g.,</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import scipy.optimize as opt
import scipy.special
import wandb
from multiprocessing import Process, cpu_count

# Define the synthetic scaling law function
def scaling_law(c, e, a, b, alpha, beta):
    return np.exp(e) + np.exp(a) * c[:, 0] ** (-alpha) + np.exp(b) * c[:, 1] ** (-beta)

# Generate synthetic data
np.random.seed(0)
C = np.array([[7e9, 2e12], [13e9, 2e12], [34e9, 2e12], [70e9, 2e12]])  # [m, Din] = [m, 2]
e_true, a_true, b_true = np.log(1.8172), np.log(482.01), np.log(2085.43)
alpha_true, beta_true = 0.3478, 0.3658
L_target = scaling_law(C, e_true, a_true, b_true, alpha_true, beta_true).reshape(-1, 1)  # [m, K]
L_target = np.repeat(L_target, 1, axis=1)  # [m , K]

# Define the cost function using the Huber loss
def aggregate_huber_loss(theta_sl, c, l_target, delta=1e-3):
    e, a, b, alpha, beta = theta_sl
    E, A, B = np.exp(e), np.exp(a), np.exp(b)
    l_pred = E + A * c[:, 0] ** (-alpha) + B * c[:, 1] ** (-beta)
    log_l_target = np.log(l_target)
    x1 = a - alpha * np.log(c[:, 0]).reshape(-1, 1)
    x2 = b - beta * np.log(c[:, 1]).reshape(-1, 1)
    x3 = e * np.ones((c.shape[0], 1))
    lse = scipy.special.logsumexp([x1, x2, x3], axis=0)
    h = scipy.special.huber(delta, lse - log_l_target)
    return h.sum()

# Training function to run each trial
def train():
    wandb.init()
    config = wandb.config

    initial_params = [config.e, config.a, config.b, config.alpha, config.beta]

    # Perform the optimization
    result = opt.minimize(aggregate_huber_loss, initial_params, args=(C, L_target), method='BFGS')
    optimized_params = result.x

    e_opt, a_opt, b_opt, alpha_opt, beta_opt = optimized_params
    loss = aggregate_huber_loss(optimized_params, C, L_target)

    wandb.log({
        &quot;e&quot;: e_opt,
        &quot;a&quot;: a_opt,
        &quot;b&quot;: b_opt,
        &quot;alpha&quot;: alpha_opt,
        &quot;beta&quot;: beta_opt,
        &quot;loss&quot;: loss
    })

# Sweep configuration for grid search
sweep_config = {
    &quot;method&quot;: &quot;grid&quot;,
    &quot;metric&quot;: {
        &quot;name&quot;: &quot;loss&quot;,
        &quot;goal&quot;: &quot;minimize&quot;
    },
    &quot;parameters&quot;: {
        &quot;e&quot;: {
            &quot;values&quot;: [-1, 0, 1]
        },
        &quot;a&quot;: {
            &quot;values&quot;: [0, 5, 10]
        },
        &quot;b&quot;: {
            &quot;values&quot;: [0, 5, 10]
        },
        &quot;alpha&quot;: {
            &quot;values&quot;: [0, 1, 2]
        },
        &quot;beta&quot;: {
            &quot;values&quot;: [0, 1, 2]
        }
    }
}

# Initialize the sweep
sweep_id = wandb.sweep(sweep_config, project=&quot;scaling-law-optimization&quot;)

# Print the sweep URL and ID
print(f&quot;Sweep URL: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/sweeps/{sweep_id}&quot;)
print(f&quot;Sweep ID: {sweep_id}&quot;)

# Function to run an agent
def run_agent():
    # wandb.agent(sweep_id, function=train, count=10)  # runs subset 10 &lt;= G^5 sweeps
    wandb.agent(sweep_id, function=train)  # keeps fetching hps until all hps in sweep are done. All G^5

# Number of agents to run in parallel
num_agents = min(cpu_count(), 72)  # Adjust this number based on your system

if __name__ == &quot;__main__&quot;:
    processes = []
    for _ in range(num_agents):
        p = Process(target=run_agent)
        p.start()
        processes.append(p)
    
    for p in processes:
        p.join()
    print('Done!\a')

</code></pre>
<h1>In CLI Bash</h1>
<h1>1 Without multiprocressing</h1>
<p>Creteate an sweep from the config .yaml file and then pass that sweep id to the multiple agents you create (e.g., with a slurm array or tmux sessions).</p>
<p>In detail:</p>
<h2>1 create yaml file for sweeps</h2>
<pre class=""lang-yaml prettyprint-override""><code>program: ~/github_repo_proj_folder/scaling_laws.py
method: grid
metric:
  name: loss
  goal: minimize
parameters:
  e:
    values: [-1, 0, 1]
  a:
    values: [0, 5, 10]
  b:
    values: [0, 5, 10]
  alpha:
    values: [0, 1, 2]
  beta:
    values: [0, 1, 2]
</code></pre>
<h2>2: Initialize the Sweep in cli terminal</h2>
<pre class=""lang-bash prettyprint-override""><code>wandb sweep sweep_config.yaml
</code></pre>
<p>This command will output a sweep ID in the format entity/project/sweep_ID. Note this sweep ID for the next steps.</p>
<h2>3: Run a Single Agent</h2>
<p>Running an agent will continually fetch hps from your wandb server sweep run until it's done:</p>
<pre><code>wandb agent &lt;sweep_id&gt;
</code></pre>
<p>replacing &lt;sweep_id&gt; with your actual sweep ID, e.g., from the output of the previous command.</p>
<h1>2 With multiprocessing parallelization in the cli</h1>
<p>One way is to run each agent once the wandb sweep is initialize in lots of tmux sessions with the <code>&amp;</code> commands or with slurm arrays or even with nohop:</p>
<pre><code>#!/bin/bash

# start sweep
## wandb sweep sweep_config.yaml

# Number of agents to run
NUM_AGENTS=4
SWEEP_ID=&lt;sweep_id&gt;  # Replace with your actual sweep ID

# Run agents in parallel
for i in $(seq 1 $NUM_AGENTS); do
  nohup wandb agent $SWEEP_ID &gt; agent_$i.log 2&gt;&amp;1 &amp;
done

# Wait for all agents to finish (optional)
wait

</code></pre>
","2","Answer"
"78541955","78528640","<p>Found the solution we have to invoke the method from reflection for it to work</p>
<pre><code>        var typedInputDataList = typeof(Enumerable).GetMethod(nameof(Enumerable.Cast))
        .MakeGenericMethod(type)
        .Invoke(null, new object[] { inputDataList });
    var typedEnumerable = typeof(Enumerable).GetMethod(nameof(Enumerable.ToList))
        .MakeGenericMethod(type)
        .Invoke(null, new object[] { typedInputDataList });

    var loadFromEnumerableMethod = typeof(DataOperationsCatalog)
                        .GetMethods()
                        .Where(m =&gt; m.Name == &quot;LoadFromEnumerable&quot;)
                        .First()
                        .MakeGenericMethod(type);
    MLContext mlContext = new();

    IDataView dataView = (IDataView)loadFromEnumerableMethod.Invoke(mlContext.Data, new object[] { typedEnumerable, schemaDefination });
</code></pre>
","0","Answer"
"78543885","78543787","<p>They are a few ways to handle missing data, the simplest being replacing the NaN values with the mean (average value) or a constant value (like 0 or 1).</p>
<p>However, if you think the data points closest in time are more similar (which is common with time series data like with your market trading data), it's a good idea to use the surrounding data points to predict the missing data using techniques like:</p>
<ul>
<li>Rolling Statistics Imputation: imagine a rolling window over your data and the mean/median is used to replace missing data.</li>
<li>K-Nearest Neighbors (KNN) Imputation: fill the values based on 'k' nearest values.</li>
<li>etc.</li>
</ul>
<p>These techniques and more are summarized in <a href=""https://medium.com/@aaabulkhair/data-imputation-demystified-time-series-data-69bc9c798cb7"" rel=""nofollow noreferrer"">Data Imputation Demystified | Time Series Data</a>.</p>
","0","Answer"
"78547515","78531747","<p><strong>The Problem :-</strong>
Actually I have too much of Data And the code I wrote Is not compatible with that data due to which model was not saving because in Training session I was seeing NaN value which means model is not training correctly . To solve This issue I used For loop to train The model ...................................</p>
","0","Answer"
"78547795","78537136","<p>I've found the answers by myself, and I think this detailed answer will be useful to others (see the questions above):</p>
<ol>
<li>The <code>ε_threshold</code> value was too high. I'd never written SGD before and I tried smaller values of it, but went through this inattentively. Although even <code>0.03</code> value leads to <code>&gt;80%</code> accuracy and the problem was the combination of some mistakes and misunderstandings (as always be).</li>
<li>Yes, the error for individual output must affect appropriate row of weights <code>w_j</code>. The gradient vector is <code>δ = -(d-y)*f'(net)</code> (in the code two negatives make a positive when adjusting weights).</li>
<li>If you want to use another activation function, you should correct the initial desired vector <code>d</code>. For example, <code>tanh(x)</code> has range of values = <code>(-1, 1)</code>, and <code>d</code> vector should have maximum (<code>1</code>) for desired value and minimum (<code>-1</code>) for other values. Such way the range of values of error vector always stays <code>(1-, 1)</code>. Don't confuse it.</li>
<li>Unit step function is a threshold function, not activation function. At first, it is rarely used in neural networks, and, at second, it can't be used in SGD as the derivative of simple unit step function is <code>0</code>.</li>
<li>Sure, we can use <code>np.outer(x, y)</code>.</li>
</ol>
<p>I chose the value of <code>ε_threshold = 3e-6</code> as the most accuracy/learning speed optimized with <code>89-91%</code> accuracy and <code>68,000</code> epochs. This is average accuracy for the MNIST dataset.</p>
<p>Here is the working code and the list of changes:</p>
<ol>
<li>Data flattens with one <code>reshape()</code> operation instead of <code>np.append()</code> for each vector.</li>
<li>Learning rate set to <code>0.01</code>, <code>ε_threshold</code> set to <code>3e-6</code>.</li>
<li>Weights adjust using <code>np.outer()</code>.</li>
<li>Typos fix and some changes that does not affect learning process.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import random
import numpy as np
from keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Flatten and normalize data
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)
X_train = X_train / 255.0
X_test = X_test / 255.0

# Constants from data
X_vector_len = 28*28
y_vector_len = 10

# Learning parameters
learning_rate = 0.01
offset = 0.03  # Random initial weight offset
e_threshold = 3e-6


def activation_function(x):
    return 1 / (1 + np.exp(-x))


def activation_derivative(x):
    return activation_function(x) * (1 - activation_function(x))


def train(X_train, y_train):
    # Weight initialization
    weights = np.random.rand(X_vector_len, y_vector_len) * offset * 2 - offset
    bias = np.random.rand(y_vector_len) * offset * 2 - offset

    epochs = 0
    while True:
        # Getting X and D vectors
        s = random.randrange(len(X_train))
        sample = X_train[s]
        desired = np.zeros(y_vector_len, dtype=np.float64)
        desired[y_train[s]] = 1

        net = np.dot(sample, weights) + bias  # Weighted sum
        y_pred = activation_function(net)
        e_vec = desired - y_pred
        e = np.sum(e_vec ** 2) / 2  # Error ε for current vector

        if e &lt; e_threshold:
            break

        delta = e_vec * activation_derivative(net)
        weights += learning_rate * np.outer(sample, delta)
        bias += learning_rate * delta

        epochs += 1
    return weights, bias, epochs


def test(X_test, y_test, weights, bias):
    correct_predictions = 0
    for j, text in enumerate(y_test):
        sample = X_test[j]
        net = np.dot(sample, weights) + bias
        print(f&quot;Test {j+1}: Desired {text}, prediction: {net.argmax()} ({net})&quot;)
        if text == net.argmax():
            correct_predictions += 1
    return correct_predictions / len(X_test)


if __name__ == &quot;__main__&quot;:
    weights, bias, epochs = train(X_train, y_train)
    accuracy = test(X_test, y_test, weights, bias)

    print(f&quot;Model trained for {epochs} epochs&quot;)
    print(f&quot;Accuracy for test selection: {accuracy * 100:.2f}%&quot;)

</code></pre>
","0","Answer"
"78549145","78547320","<p>As it comes from the comments, you are using an old version of <strong>Ultralytics==8.0.0</strong>. It in fact returns the result as a list of <code>torch.Tensor</code> object instead of <code>ultralytics.engine.results.Results</code> object, and exactly the last one has such parameters like <em>boxes, masks, keypoints, probs, obb</em>. The documentation complies with the latest framework version, <strong>8.2.24</strong> for now, and 8.0.0 is from January 2023.</p>
<p>The easiest way to solve your problem is to <strong>upgrade the Ultralytics version</strong> to the latest one, so you will get all the results parameters described in the documentation.</p>
<p>If circumstances prevent you from this update, you will need some data postprocessing with the <strong>understanding of the returned results format</strong> from the 8.0.0 version.</p>
<p>OBJECT DETECTION task results, version==8.0.0</p>
<pre class=""lang-py prettyprint-override""><code># for 3 detected objects

[tensor([[2.89000e+02, 7.10000e+01, 1.44000e+03, 5.07000e+02, 8.91113e-01, 2.00000e+00],
         [1.26700e+03, 6.00000e+01, 1.68200e+03, 3.19000e+02, 8.31055e-01, 2.00000e+00],
         [6.96000e+02, 0.00000e+00, 1.32200e+03, 1.31000e+02, 2.56836e-01, 7.00000e+00]], device='cuda:0')]

# where every array stores 6 values, first 4 are in pixels:
[x_centre, y_centre, box_width, box_height, confidence, class_id]

# for easy manipulation you can run results[0].tolist() and will get the following format:
[[289.0, 71.0, 1440.0, 507.0, 0.89111328125, 2.0],
 [1267.0, 60.0, 1682.0, 319.0, 0.8310546875, 2.0],
 [696.0, 0.0, 1322.0, 131.0, 0.2568359375, 7.0]]
</code></pre>
<p>OBJECT SEGMENTATION task results, version==8.0.0. Will be the same as for detection, but adds the second torch.Tensor with the segmentation masks for every object.</p>
<pre class=""lang-py prettyprint-override""><code># for 3 detected objects

[[tensor([[1.23000e+02, 8.90000e+01, 4.21000e+02, 2.21000e+02, 2.55216e-01, 7.00000e+00],
          [1.26700e+03, 5.80000e+01, 1.68100e+03, 3.17000e+02, 8.04158e-01, 2.00000e+00],
          [2.70000e+02, 7.70000e+01, 1.46000e+03, 4.98000e+02, 8.19106e-01, 2.00000e+00]], device='cuda:0'),
  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]],
  
          [[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]],
  
          [[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]]

</code></pre>
<p>Knowing the data format you receive from this old Ultralytics version, you can easily reach them and translate them to the format you need. But upgrading Ultralytics to the latest version is still the best way to get the most effective usage of this framework.</p>
<p>Install Ultralytics: <a href=""https://docs.ultralytics.com/quickstart/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/quickstart/</a></p>
","1","Answer"
"78551926","78550784","<p>For multi-label classification problems with sparse outputs, a more suitable loss function is the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"" rel=""nofollow noreferrer"">BCEWithLogitsLoss</a>. This loss function treats each class as an independent binary classification problem and computes the binary cross-entropy loss for each class, summing the losses across all classes.</p>
<p>You could implement something like this:</p>
<pre><code>import torch.nn.functional as F

# these are raw logit outputs of the model, not after Sigmoid
logits = model(input_data)  # Shape: (batch_size, num_classes)

loss = F.binary_cross_entropy_with_logits(logits, target)
</code></pre>
","0","Answer"
"78552283","78550892","<p>Yes, it seems to be a general issue with the extension <code>ml</code> in version 2.26.0. I solved it by running these commands one by one in a local bash:</p>
<pre><code>az extension remove -n azure-cli-ml
az extension remove -n ml
az extension add -n ml --version=2.25.0
</code></pre>
<p>After running these commands, you can run the commands to provision the resource group (don't use the same group name you used previously, use a new one) and the ml workspace:</p>
<pre><code>az group create --name &quot;new-group-name&quot; --location &quot;eastus&quot;
az ml workspace create --name &quot;mlw-dp100-labs&quot; -g &quot;new-group-name&quot;
</code></pre>
<p>Remember to do it in a local bash, because the Cloud Shell in Azure does not allow modifying the version of ml. Here a screenshot that it works
<a href=""https://i.sstatic.net/BHUps5Xz.png"" rel=""nofollow noreferrer"">Creating a ML Workspace</a></p>
","2","Answer"
"78554235","78554157","<p>You can grab the data set here:</p>
<pre><code>import pandas as pd

url= &quot;https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv&quot;
dataset = pd.read_csv(url)
</code></pre>
","-1","Answer"
"78554373","78554157","<p>Officially, the <strong>iris</strong> dataset belongs to scikit-learn package.
In case you want to do it in the most <em>clever</em> way without using URLs and extracting the information directly from the package, use this code:</p>
<pre><code>from sklearn import datasets
import pandas as pd
import numpy as np

iris = datasets.load_iris()
df = pd.DataFrame(iris.data)
df.columns = ['sepal-length','sepal-width','petal-length','petal-width']
df[&quot;class&quot;] = np.arange(0,150,1)
print(df)
</code></pre>
<ul>
<li>You call the <strong>scikit-learn</strong> package subdivison called dataset which contain the dataset you are looking for, iris.</li>
<li>You use the function <strong>load_iris()</strong> to load it.</li>
<li>You read the DataFrame with <strong>pandas</strong>.</li>
<li>You assign the column´s names with df.columns.</li>
<li>In the next line you add the <em><strong>class</strong></em> column that it is not implemented yet, and with <em>np.arange</em> you create a list of numbers from 0 to 149.</li>
</ul>
<p>Hope it helps!</p>
","-1","Answer"
"78554571","78554214","<p>Actually your question is not clearly. But if you wondering selecting parameter K, you can search <a href=""https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/"" rel=""nofollow noreferrer"">Elbow</a> or <a href=""https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/#:%7E:text=The%20silhouette%20score%20ranges%20from,wrong%20cluster%20(poor%20clustering)."" rel=""nofollow noreferrer"">Silhouette</a> method. These methods calculate error or distances variance to each K parameter which between (2, n) , so you can select best K parameter which your dataset' best represented.</p>
<p>Also, How did you evaluate your prediction by percentage without y?</p>
<p>If you wondering about metrics, this is complicated. Each metrics have some advantages and disadvantage. In some case, cosine better than other metrics. This situation depend on how data' feature separated each other.</p>
<p>If you want to improve your model, you can try normalization feature (softmax, L1, L2 etc.) or dimension reduction (PCA etc.) or feature selection (drop some feature).</p>
","0","Answer"
"78557767","78477344","<p>Albumentations indeed, treats images of the type float32 as having min value of 0 and max value of 1.</p>
<p>If your images are in <code>[-2, 2]</code> range, you may rescale your image to <code>[0, 1]</code> with:</p>
<pre class=""lang-py prettyprint-override""><code>def convert_to_01(image):
    return (image + 2) / 4    
</code></pre>
<p>and later rescale back with</p>
<pre class=""lang-py prettyprint-override""><code>def convert_from_01(image):
   return (image * 4) - 2
</code></pre>
<p>If possible, you may even convert images to <code>[0, 255]</code> uint8 type, as transforms in <code>uint8</code> happen much faster.</p>
<p>P.S. Thank you for the question, I will make it clear in the documentation of <a href=""https://albumentations.ai/"" rel=""nofollow noreferrer"">Albumentations</a> that float32 images are expected to be in <code>[0, 1]</code> range.</p>
","0","Answer"
"78559090","78559061","<p>You need to call <code>compute_class_weights</code> with keyword arguments rather than positional, just like they do in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)
</code></pre>
<p>The way to know this is by looking at the function signature when they define it (also seen in the documentation I linked above):</p>
<pre><code>def compute_class_weight(class_weight, *, classes, y)
</code></pre>
<p>The asterisk <code>*</code> means that positional arguments after the first one (<code>class_weight</code>) are disregarded. It's a common method to force users to use keyword/named arguments like <code>classes=</code> or <code>y=</code>. See <a href=""https://stackoverflow.com/a/14302007/7662085"">this</a> Stackoverflow answer.</p>
","0","Answer"
"78560638","78546693","<p>it is not the AutoTokenizer issue. BertTokenizerFast shows same memory usage.</p>
<pre><code>Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     5    406.1 MiB    406.1 MiB           1   @profile
     6                                         def load_tokenizer():
     7    406.1 MiB      0.0 MiB           1       path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
     8    708.3 MiB    302.2 MiB           1       tokenizer = transformers.BertTokenizerFast.from_pretrained(path)
</code></pre>
<p>Most of the RAM use increment comes from the loading lines:</p>
<p><code>tokenizer_file_handle = json.load(tokenizer_file_handle)</code></p>
<p>and</p>
<p><code>fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)</code></p>
<p>Well, this is Python, there is quite much written about why objects take much space in RAM. Here are a few inks at SO:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/48085752/python-size-of-object-in-memory-vs-on-disk"">Python - size of object in memory vs. on disk</a></li>
<li><a href=""https://stackoverflow.com/questions/49034997/why-does-a-file-stored-as-a-dictionary-take-up-much-more-space-than-file"">Why does a file stored as a dictionary take up much more space than file</a></li>
<li><a href=""https://stackoverflow.com/questions/33978/find-out-how-much-memory-is-being-used-by-an-object-in-python"">Find out how much memory is being used by an object in Python</a></li>
</ul>
","0","Answer"
"78561550","78558728","<p>I cannot help much with YOLOv5.  I maintain and use Darknet/YOLO, which is both faster and has better precision than what you'll get from Ultralytics/Roboflow and other python-based YOLO frameworks.</p>
<p>But I do have YouTube videos where I discuss using YOLO to detect playing cards.  Some of this will apply to you regardless of which version of YOLO or framework you are using.</p>
<p>Things to note:</p>
<ul>
<li>Sizing the network is very important:  <a href=""https://www.youtube.com/watch?v=m3Trxxt9RzE"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=m3Trxxt9RzE</a></li>
<li>What happens if you don't size your network correctly (this might explain the results you are seeing):  <a href=""https://www.youtube.com/watch?v=wcW7OrKaYiA"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=wcW7OrKaYiA</a></li>
<li>context is important -- why things outside of your bounding boxes is also important:  <a href=""https://www.youtube.com/watch?v=auEvX0nO-kw"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=auEvX0nO-kw</a></li>
</ul>
<p>Also see the YOLO FAQ:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/</a></p>
<p>You don't mention how you've setup your classes.  Do you have classes for all 52 cards?  Or do you have Ace-to-King and then the 4 suites, which you then plan on combining to make a card?  Either way, only having 138 training images doesn't seem like much, but especially if you used the first option with 52 classes, then that would mean only 2 examples of each card, which isn't enough.  See this:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#how_many_images"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#how_many_images</a></p>
","0","Answer"
"78562644","78561885","<blockquote>
<p>I find this too imprecise, as certain tree nodes, may have separated my (non-deterministic) samples, into fairly precise leaves (100 class a , 0 class b), and imprecise leaves (5 class a, 3 class b).</p>
</blockquote>
<p>As with Decision Trees in a Bagging ensemble, the individual Decision Tree learners in a Random Forest are typically fully expanded and not pruned. This is the preferred methodology of Hastie et al. in ESL. The default settings for an <code>sklearn.ensemble.RandomForestClassifier</code> result in fully expanded trees. What this means is that each tree perfectly fits its training data, and the entropy in the leaf nodes will be zero (assuming that there are no noisy samples with different labels for the same sample points). Therefore with the <em>default</em> settings, your example simply can not occur.</p>
<p>Otherwise, in the case that you constrain your individual trees by specifying an appropriate <code>max_depth</code> or a <code>min_samples_leaf</code> parameter, what you are asking for is simply a different algorithm than the Random Forest algorithm described by Breiman and Cutler and implemented by <code>sklearn</code>. I am not aware of any library that offers such an interface. However it would be relatively trivial to achieve what you are looking for, especially if you have a good working knowledge of the algorithm. You just need to modify the return type of the Decision Tree's <code>predict</code> method, and then refactor the logic for taking the majority vote in the Random Forest. Minor modifications, assuming you have working implementations of the Decision Tree and Random Forest modules.</p>
<blockquote>
<p>Or maybe just crank up the number of classifiers (not optimal)?</p>
</blockquote>
<p>I'm not sure how this would help you, in general, and in particular with what you seem to be looking to do. With more classifiers, you still will be taking a majority vote in Random Forest, only the vote will be over a larger ensemble. Furthermore, in the limit, you may be worse off: although it is rare that people have issues with a RandomForest ensemble being too big and overfitting, it can happen, and too many trees in the forest can result in a final Random Forest model with excess variance.</p>
<p>If you would like to work with the <code>sklearn</code> implementation, then I think you should find most of the information you need to get started at <a href=""https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"" rel=""nofollow noreferrer"">Understanding the Decision Tree Structure</a>. For example, the <code>DecisionTreeClassifier</code> has an attribute <code>tree_</code> that points to an instance of the <code>Tree</code> class, an array-based representation of a binary decision tree. This member allows access to a number of low-level attributes such as <code>max_depth</code>, or of interest to you, <code>tree_.value</code>. As mentioned in the documentation:</p>
<blockquote>
<p>The <code>tree_.value array</code> is a 3D array of shape <code>[n_nodes, n_classes, n_outputs]</code> which provides the count of samples reaching a node for each class and for each output. Each node has a value array which is the number of weighted samples reaching this node for each output and class.</p>
</blockquote>
<p>I would also suggest taking a look at the <code>decision_path</code> method of the <code>RandomForestClassifier</code> and the <code>DecisionTreeClassifier</code>. Lastly, you should ask yourself if you really need this functionality? Do you have a statistical justification for why this modification would improve the classifiers performance? Is there a specific problem you are trying to solve with this approach?</p>
","1","Answer"
"78562796","78558974","<p>I figured out the answer. There is no bug, but I was severely underestimating the amount of training time I needed. I figured because the cost function was flatlining, it had reached the point of diminishing returns. By training for several hours, I was able to push through the plateau, and it began rapidly decreasing the cost again. I also needed to bump up the training rate significantly, and running in batches increased the performance dramatically.</p>
","0","Answer"
"78565795","78565774","<pre><code>bool_val = True
print(bool_val)       # returns True
print(int(bool_val))  # returns 1
</code></pre>
","0","Answer"
"78566052","78565978","<p>You can use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.unique.html#numpy-unique"" rel=""nofollow noreferrer"">numpy.unique</a>. It can help you getting the uniqe array with no duplicates and also give you;</p>
<ul>
<li><p>the indices of the input array that give the unique values</p>
</li>
<li><p>the indices of the unique array that reconstruct the input array</p>
</li>
<li><p>the number of times each unique value comes up in the input array</p>
</li>
</ul>
<p>Example usage is available in the link.</p>
","0","Answer"
"78566060","78565125","<p>After installing any libraries make sure to restart the kernel.</p>
","0","Answer"
"78566669","78566413","<p>Using the Google AI SDK for JavaScript directly from a client-side app is recommended for prototyping only. For non-prototyping use cases, we strongly recommend that you call the Google AI Gemini API only server-side to keep your API key safe. If you embed your API key directly in your web app or fetch it remotely at runtime, you risk potentially exposing your API key to malicious actors.</p>
<p>That being said:</p>
<pre><code>&lt;html&gt;
&lt;body&gt;
&lt;!-- ... Your HTML and CSS --&gt;

&lt;script type=&quot;importmap&quot;&gt;
    {
        &quot;imports&quot;: {    &quot;@google/generative-ai&quot;: &quot;https://esm.run/@google/generative-ai&quot;
    }
    }
&lt;/script&gt;
&lt;script type=&quot;module&quot;&gt;
    import { GoogleGenerativeAI } from &quot;@google/generative-ai&quot;;

    // Fetch your API_KEY
    const API_KEY = &quot;REDACTED&quot;;

    // Access your API key (see &quot;Set up your API key&quot; above)
    const genAI = new GoogleGenerativeAI(API_KEY);

    // ...

    // The Gemini 1.5 models are versatile and work with most use cases
    const model = genAI.getGenerativeModel({ model: &quot;gemini-1.5-flash&quot;});

    const imagePicker = document.getElementById(&quot;image-picker&quot;);
    const selectedImagesContainer = document.getElementById(&quot;selected-images&quot;);
    const compareButton = document.getElementById(&quot;compare-button&quot;);

    // Converts a File object to a GoogleGenerativeAI.Part object.
    async function fileToGenerativePart(file) {
        const base64EncodedDataPromise = new Promise((resolve) =&gt; {
            const reader = new FileReader();
            reader.onloadend = () =&gt; resolve(reader.result.split(',')[1]);
            reader.readAsDataURL(file);
        });
        return {
            inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },
        };
    }

    imagePicker.addEventListener(&quot;change&quot;, async (event) =&gt; {
        const selectedFiles = event.target.files;

        // Clear previous selections
        selectedImagesContainer.innerHTML = &quot;&quot;;
        compareButton.disabled = true; // Disable button initially

        if (selectedFiles.length !== 2) {
            alert(&quot;Please select exactly two images.&quot;);
            return;
        }

        // Display selected images
        for (const file of selectedFiles) {
            const reader = new FileReader();
            reader.onload = (event) =&gt; {
                const img = document.createElement(&quot;img&quot;);
                img.src = event.target.result;
                selectedImagesContainer.appendChild(img);
            };
            reader.readAsDataURL(file);
        }

        // Enable compare button only when two images are selected
        compareButton.disabled = false;
    });

    compareButton.addEventListener(&quot;click&quot;, async () =&gt; {
        const selectedFiles = imagePicker.files;

        if (selectedFiles.length !== 2) {
            alert(&quot;Please select exactly two images.&quot;);
            return;
        }

        const descriptions = [];
        for (const file of selectedFiles) {
            const description = await DescribeImage(file);
            descriptions.push(description);
        }

        console.log(&quot;Description 1:&quot;, descriptions[0]);
        console.log(&quot;Description 2:&quot;, descriptions[1]);
        // Implement your logic to compare descriptions and identify differences (optional)

        // You can display the descriptions to the user here (optional)
    });

    async function DescribeImage(file) {
        const imagePart = await fileToGenerativePart(file);
        const prompt = &quot;Describe this image&quot;;
        const result = await model.generateContent([prompt, imagePart]);
        const response = await result.response;
        const text = response.text();
        document.write(text);
        return text;
    }
    // ...
&lt;/script&gt;

&lt;input type=&quot;file&quot; id=&quot;image-picker&quot; accept=&quot;image/*&quot; multiple&gt;
&lt;div id=&quot;selected-images&quot;&gt;&lt;/div&gt;
&lt;button id=&quot;compare-button&quot; disabled&gt;Compare Images&lt;/button&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
","1","Answer"
"78568805","78544969","<p>It isn't necessary to store the output of the Spark jobs separately unless your use case explicitly requires it because it would otherwise just be a waste of processing and storage.</p>
<p>As a side note in case you weren't already aware, <a href=""https://cassio.org/"" rel=""nofollow noreferrer"">the CassIO library</a> provides a simple integration <a href=""https://cassio.org/frameworks/langchain/prompt-templates-feast-setup/"" rel=""nofollow noreferrer"">to use Cassandra as a Feast feature store</a> with minimal boilerplate code. Feel free to try it out on your own Cassandra cluster. If you don't already have a cluster running, you can launch one in less than 2 minutes on the free-tier (no credit card required) of <a href=""https://dtsx.io/3jdZVbU"" rel=""nofollow noreferrer"">Astra DB</a> -- a cloud-based Cassandra-as-a-service. Cheers!</p>
","0","Answer"
"78572471","78572370","<p>You are generating an entirely different random dataset for the manual validation data.</p>
<p>The first time you generate a validation set is in the lines</p>
<pre><code># Set up model, data, and trainer
model = ResNet()
train_loader = get_dataloader()
val_loader = get_dataloader()
</code></pre>
<p>And the second time you generate a validation set is the lines</p>
<pre><code># Predict on validation data
val_loader = get_dataloader()
</code></pre>
<p>Since the <code>get_dataloader()</code> function generates a random synthetic dataset each time you run it, you are essentially running the same accuracy tests on two different sets of data.</p>
<p>To fix this, I would remove the second initialization of <code>val_loader</code>, so that the same Dataloader instance is used in the manual and automatic tests.</p>
","0","Answer"
"78573176","78561885","<blockquote>
<p>An idea I had was to iterate through every tree, retrieving their probabilities, and averaging them. However [...]</p>
</blockquote>
<p>This is already how sklearn random forests work, so-called &quot;soft voting&quot;. E.g. from <a href=""https://scikit-learn.org/stable/modules/ensemble.html#random-forests"" rel=""nofollow noreferrer"">the User Guide</a>:</p>
<blockquote>
<p>In contrast to the original publication [B2001], the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</p>
</blockquote>
<p>That said, if you want the sample sizes in the leaves to have an effect, you'd need to do some manual work. You can use the forest's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.apply"" rel=""nofollow noreferrer""><code>apply</code> method</a> to get the leaf indices reached, then use the tree structure that you and Apanasov referred to, specifically the <code>n_node_samples</code> and <code>values</code> attributes.</p>
","1","Answer"
"78575919","78575816","<p>Are you working in the notebook? How do you use <code>Session</code> in your code? It seems that <code>Session</code> is deprecated and it is recommended to rewrite the code without it.</p>
<p>If you are using gpus in the notebooks I think one of the options could be to use <code>InteractiveSession</code>:</p>
<pre><code>from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession
from tensorflow.python.keras.backend import set_session

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.2
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

set_session(session)
</code></pre>
","1","Answer"
"78577900","78576495","<p>I'm not deeply familiar with the package or it's source code, but from the documentation I assume:</p>
<p>It doesn't (need to) compute gradients of your function. It's fitting gbdt's to (increasing) sets of inputs <code>x</code> for which the output <code>y</code> of your function has been computed (some initial set together with earlier iterations of the optimizer). And <em>that</em> involves gradients, but it's of the standard MSE loss of the gbdt fitting procedure.</p>
","0","Answer"
"78579487","78577415","<p>I have tried just now and with a recent version of MLJ (v0.20.5) it works with the trick below:</p>
<p>Type this on a script <code>Foo.jl</code> in an empty directory:</p>
<pre><code>using Pkg
Pkg.activate(@__DIR__) # Activate the environment on the directory of this script. The first time the environment is &quot;created&quot; by making in this directory 2 files, Project.toml and Manifest.toml
Pkg.add(&quot;MLJ&quot;)
Pkg.add(&quot;BetaML&quot;)

using MLJ
import BetaML  # &lt;--- trick here
X = rand(100,5)
y = [r[2]+r[3]^2-r[5] for r in eachrow(X)]
model_name = &quot;rf_reg&quot;
function predict_y(model_name,X,y)
    models = Dict(
        &quot;xgb_reg&quot;=&gt; [&quot;XGBoost&quot; =&gt; &quot;XGBoostRegressor&quot;],
        &quot;ridge_reg&quot;=&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;RidgeRegressor&quot;],
        &quot;lasso_reg&quot;=&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;LassoRegressor&quot;],
        &quot;rf_reg&quot; =&gt; [&quot;BetaML&quot; =&gt; &quot;RandomForestRegressor&quot;],
        &quot;lin_reg&quot; =&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;LinearRegressor&quot;],
        &quot;log_class&quot; =&gt; [&quot;MLJLinearModels&quot; =&gt; &quot;LogisticClassifier&quot;],
        &quot;rf_class&quot; =&gt; [&quot;DecisionTree&quot; =&gt; &quot;RandomForestClassifier&quot;],
        &quot;xgb_class&quot; =&gt; [&quot;XGBoost&quot; =&gt; &quot;XGBoostClassifier&quot;]
    )
    mod = models[model_name][1]
    p = mod[1]
    m = mod[2]
    Model = @eval @load $(m) pkg=$(p) verbosity=0
    model = Model()

    # train machine and get parameters
    m1 = machine(model, X, y) |&gt; fit!
    ŷ  = predict_mode(m1, X)
    return ŷ
end
ŷ = predict_y(model_name,X,y)
hcat(y,ŷ)
</code></pre>
<p>As I wrote in the comment, it is always best to start a project on a dedicated environment.
Also BetaML works on standard arrays, not dataframes.</p>
<p>[EDIT]
Indeed, if @evel @load is done within a function, you end up with the issue you discovered.
I don't know exactly what MLJ.@load does and the reason behind it, but the trick is just to import the package providing the model, here <a href=""https://github.com/sylvaticus/BetaML.jl"" rel=""nofollow noreferrer"">BetaML</a>, before calling that function.</p>
","0","Answer"
"78579610","78564460","<p>You have to use the <code>seed</code> argument on the first reset of the episode (<code>env.reset(seed=...)</code>) for deterministic behavior of Gymnasium environments</p>
<p>more information on: <a href=""https://gymnasium.farama.org/api/env/#gymnasium.Env.reset"" rel=""nofollow noreferrer"">https://gymnasium.farama.org/api/env/#gymnasium.Env.reset</a></p>
","0","Answer"
"78581578","78575816","<p>I fixed my issue, not by changing any code. I realized I just had to adopt an older version in order to do anything with my model so I used:</p>
<ul>
<li>python 3.7.16</li>
<li>tensorflow==2.5.0</li>
<li>keras==2.4</li>
</ul>
<hr />
<p><strong>Although, I think I know a solution for those whose models can handle the change to the new tensorflow version.</strong></p>
<p>I'd highly reccommend trying the following:</p>
<pre><code>tensorflow.python.keras.backend as K
set_session = K.get_session
</code></pre>
<p>While using the latest tensorflow (which auto installs keras)</p>
","0","Answer"
"78582247","78452655","<p>The main problem here is that you use <code>reduction='batchmean'</code> in <code>F.kl_div</code>. That would be right for a common classification problem, in which output of a model has shape <code>[B, C]</code>, where B is batch size and C is number of classes. Note that the sum over the second dimension will give 1, as the model gives the probas of classes for each sample in batch.  In your case you may consider segmentation as pixel-wise classification, so that the value for each pixel is predicted independently. The number of classes <code>C</code> is 2, but <code>B</code> is not batch size any more,<code>B=224x224x8</code> as each pixel is a separate &quot;sample&quot;. So, there are two options to solve this problem:</p>
<ol>
<li>Reshape tensors to <code>[224x224x8, 2]</code> before <code>kl_div</code>, for example, by using <code>.view(-1,2)</code> and continue using <code>reduction='batchmean'</code></li>
<li>Calculate <code>kl_div</code> in this way (pay attention to <code>mean</code> and <code>* 2</code>):</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>kd_loss = F.kl_div(stu_prob, 
                       preds_teacher, 
                       reduction='mean',
                  ) * T * T * 2
</code></pre>
","1","Answer"
"78583483","78581041","<p>You are not setting the embedding model, so I think Llama Index is defaulting to OpenAI.<br />
You must specify an embedding model that does not require an API key.</p>
<p>You can use Ollama:</p>
<pre><code>from llama_index.embeddings.ollama import OllamaEmbedding

# Using Nomic
Settings.embed_model = OllamaEmbedding(model_name=&quot;nomic-embed-text&quot;)

# Using Llama
Settings.embed_model = OllamaEmbedding(model_name=&quot;llama2&quot;)
</code></pre>
<p>But there are many options in the documentation like <a href=""https://docs.llamaindex.ai/en/stable/examples/embeddings/ollama_embedding/"" rel=""nofollow noreferrer"">this</a>, <a href=""https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface/"" rel=""nofollow noreferrer"">this</a>, <a href=""https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook/#setup-embedding-model"" rel=""nofollow noreferrer"">this</a></p>
","1","Answer"
"78584188","78542429","<p>The sound inputs were too long, after resampling the audio into chunks, the problem was resolved.</p>
","0","Answer"
"78584375","78579401","<p>If you increase the number of iterations, as suggested by the warning message, you should get the desired results.</p>
<pre><code>train_proc &lt;- data.table::fread(&quot;train_proc.csv&quot;)
names(train_proc)[25] &lt;- &quot;group&quot;  # missing from your code

set.seed(123)
firth.logist.model &lt;- train(train_proc[, .SD, .SDcols = !&quot;group&quot;],
                            train_proc$group,
                            method = firth_model,
                            trControl = train_control,
                            plcontrol=logistpl.control(maxit=1000))

print(firth.logist.model)
</code></pre>
<hr />
<pre><code>53 samples
24 predictors
 2 classes: 'AD', 'control' 

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 3 times) 
Summary of sample sizes: 35, 36, 35, 36, 35, 35, ... 
Resampling results:

  Accuracy   Kappa     
  0.4270153  -0.1482292

Tuning parameter 'none' was held constant at a value of none
</code></pre>
","0","Answer"
"78585059","78551615","<p>The specific POSIX permissions are depending on the user which is used by the container image of your processing job.</p>
<p>In most cases, this is <code>root</code> but there also images which are using non-privileged users. If you're using the SageMaker Distribution, it is also a non-root user (<code>uid</code>: 1000, <code>gid</code>: 100).</p>
<p>The paths for the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingS3Output.html"" rel=""nofollow noreferrer"">ProcessingOutputs</a> are generated when the entrypoint of the container image is invoked and are generated with root permissions.</p>
<blockquote>
<p>LocalPath:
The local path of a directory where you want Amazon SageMaker to upload its contents to Amazon S3. LocalPath is an absolute path to a directory containing output files. This directory will be created by the platform and exist when your container's entrypoint is invoked.</p>
</blockquote>
<p>You can add the following to your processing code to update the permissions of the created output folder:</p>
<pre><code>import subprocess

output_path = &quot;/opt/ml/processing/output/&quot;

# Option A: if you're using the SageMaker Distribution
subprocess.check_call([&quot;sudo&quot;,&quot;chown&quot;,&quot;-R&quot;,&quot;sagemaker-user&quot;, output_path])
# Option B: for other container images
subprocess.check_call([&quot;sudo&quot;,&quot;chmod&quot;,&quot;-R&quot;,&quot;777&quot;, output_path])
</code></pre>
<p>Alternatively, you could also use the boto3 library within your processing job and write to S3 directly.</p>
","1","Answer"
"78588381","78587419","<p>Coefficients are in the same order of columns of X_train.
I would not recommend performing &quot;np.abs&quot; on the coefficients. You are losing precious information on whether they are +ve or -ve. You can keep the sign as it is and visualize it richly. See below.</p>
<p>I would create a pandas data frame like this:</p>
<pre><code>import pandas as pd
pd.options.plotting.backend='plotly' #or use matplotlib
pdf = pd.DataFrame(data=coefficients, columns=&lt;List of your column names&gt;)
fig = pdf.T.plot(kind='bar') # T stands for transpose
fig.plot()
</code></pre>
<p>I usually use plotly as the backend as I like its API which is more intuitive and the charts are &quot;interactive&quot;</p>
<p>Once you get the &quot;fig&quot; options, you can update the traces with color information and others as well. I usually take &quot;percentiles&quot; and then assign colors from Crimson Red to Olive green with a gentle color progression.</p>
<p>An e.g. Use the qcut function like this:</p>
<pre><code>pdf['color'] = pd.qcut(
    pdf[&lt;col of interest&gt;], 
    4, 
    labels=['limegreen', 'seagreen', 'green', 'darkgreen']
).to_list()
</code></pre>
<p>You can use colors of your choice. And pass it to the &quot;marker&quot; property of the Plotly trace.</p>
<pre><code>marker = {'color' : pdf['color']}, # While setting marker property while invoking add_trace (or) update_trace in a plotly figure
</code></pre>
","0","Answer"
"78590763","78584772","<p>As per this <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-register"" rel=""nofollow noreferrer"">documentation </a> there is no <code>container_registry</code> and <code>model_url</code> arguments to
<code>Model.register</code>.</p>
<p>Also, it is mentioned <code>model_path</code> is the path on the local file system where the model assets are located so you cannot give the blob url.</p>
<p>All you can do is using python sdk v2 which supports the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models?view=azureml-api-2&amp;tabs=python%2Cuse-local#supported-paths"" rel=""nofollow noreferrer"">datastore path</a>.</p>
<p>Create a data store in you ml workspace with the storage account your model is.</p>
<p><img src=""https://i.imgur.com/gKJQEjC.png"" alt=""enter image description here"" /></p>
<p>After creating, go to datastore and browse for your model file.</p>
<p><img src=""https://i.imgur.com/d6FiCsG.png"" alt=""enter image description here"" /></p>
<p>Then copy datastore uri.</p>
<p><img src=""https://i.imgur.com/PIl7vlG.png"" alt=""enter image description here"" /></p>
<pre class=""lang-py prettyprint-override""><code>from  azure.ai.ml  import  MLClient
from  azure.ai.ml.entities  import  Model
from  azure.ai.ml.constants  import  AssetTypes
from  azure.identity  import  DefaultAzureCredential

subscription_id = &quot;&quot;
resource_group = &quot;&quot;
workspace = &quot;mljgs&quot;

ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)


cloud_model = Model(
    name=&quot;model-1&quot;,
    path=&quot;azureml://subscriptions/&lt;&gt;/resourcegroups/&lt;&gt;/workspaces/mljgs/datastores/jgsblob/paths/ml_models/model.pkl&quot;,
    type=AssetTypes.CUSTOM_MODEL,
    description=&quot;Model created from cloud path.&quot;)

ml_client.models.create_or_update(cloud_model)
</code></pre>
<p>Output:</p>
<p><img src=""https://i.imgur.com/5rgBbhP.png"" alt=""enter image description here"" /></p>
<p>Check this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models?view=azureml-api-2&amp;tabs=python%2Cuse-local#register-your-model-as-an-asset-in-machine-learning-by-using-the-sdk"" rel=""nofollow noreferrer"">document for more information</a></p>
","0","Answer"
"78590915","78585509","<p>This error occurs with certain library methods when using a High Concurrency cluster with credential passthrough enabled.
If this is your situation, one potential workaround is to use a different cluster mode.</p>
<p>As per the <a href=""https://learn.microsoft.com/en-us/azure/databricks/archive/credential-passthrough/adls-passthrough#py4jsecuritypy4jsecurityexception--is-not-whitelisted"" rel=""nofollow noreferrer"">py4j.security.Py4JSecurityException: … is not whitelisted</a></p>
<blockquote>
<p>This exception is thrown when you have accessed a method that Azure Databricks has not explicitly marked as safe for Azure Data Lake Storage credential passthrough clusters. In most cases, this means that the method could allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials.</p>
</blockquote>
<p>Refernce: <a href=""https://stackoverflow.com/questions/55427770/error-running-spark-on-databricks-constructor-public-xxx-is-not-whitelisted"">SO</a> link.</p>
","0","Answer"
"78593514","78558863","<blockquote>
<p><strong>Hypothesis</strong> : <em>&quot;regression or classification here it is does not matter at all&quot;</em></p>
</blockquote>
<p>Well, it actually does.<br><sub>( Prove me to be wrong, <em>if you can</em>, yet some 20+ years of HFT &amp; Quant strategy 24/7/365 massive-parallel backtesting factory design &amp; technology operations present here. I remain open to evidence of a working counter-example to disprove this claim by evidence, not by  opinions. 20+ years of SDAAT-based dMM-tools can re-test any such presented counter-example candidate-claim. )</sub></p>
<blockquote>
<p><strong>Q1</strong> : <em>&quot;using (...) would it be possible to train the model on the performance metric used to backtest the strategy?&quot;</em></p>
</blockquote>
<blockquote>
<p><strong>Q2</strong> : <em>&quot;Is it possible to compute the gradient and the hessian with respect to the overall return and so, training the model using this custom loss function?&quot;</em></p>
</blockquote>
<blockquote>
<p><strong>Q3</strong> : <em>&quot;How can the function <code>maximize_performance_metric()</code> have access to the variable that contains <code>real_prices</code> ( needed for the overall return computation )?&quot;</em></p>
</blockquote>
<p>Let's start with some reality-checks, ok?
<a href=""https://www.desmos.com/calculator/rtl3mbwwqp"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMkjhR3J.png"" alt=""enter image description here"" /></a>
How many &quot;training&quot;-examples can your current predictor get its &quot;training&quot;-phase on? That matters. See Wassily HOEFFDING inequality below for more details.</p>
<pre><code>import numpy as np

MASK = &quot;We need at least {0:9d} training examples to drive a PREDICTOR-instance under Proba( {1: &gt;5.3f} ) to deliver a prediction beyond +/- Epsilon-( {2: &gt;5.3f} )-distance from y_GROUND_TRUTH&quot;

for     EPS_NORMALISED_ERROR      in ( 1., 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001 ):
    for ALPHA_PROBA_OF_BEYOND_EPS in ( 1., 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001 ):
        print( MASK.format( int( np.log( 2. / ALPHA_PROBA_OF_BEYOND_EPS )
                                 / 2.
                                 / EPS_NORMALISED_ERROR**2
                                 ),                    # 0: N per Hoeffding
                            ALPHA_PROBA_OF_BEYOND_EPS, # 1: ALPHA
                            EPS_NORMALISED_ERROR       # 2: EPS
                            )
               )
</code></pre>
<p>If and only if you have at least a few orders of magnitude more labeled data, than HOEFFDING pre-scribed as a must for reaching a certainty better than <strong><code>( 1 - ALPHA )</code></strong> not to hear &quot;answers&quot; worse than <strong><code>EPSILON</code></strong>-away from <code>y_GROUND_TRUTH</code> for any and all your used, already known, past examples, it makes sense <a href=""https://www.desmos.com/calculator/rtl3mbwwqp"" rel=""nofollow noreferrer"">to drill further</a>.</p>
<p>Re-using famous Stephen LEACOCK's <a href=""https://www.gutenberg.org/files/23449/23449-h/23449-h.htm#Page_159"" rel=""nofollow noreferrer"">Juggins</a> strategy, first published 100+ years ago, the lowest hanging fruit is the <strong>Q3</strong>.</p>
<p>A dirty anti-Pythonista's move might be to use such data &quot;declared&quot; as <strong><code>global</code></strong>, for cases, when you need to assign new value(s) thereinto, otherwise a plain reference to such variable shall get resolved in actual namespaces and fetch the needed values into your <code>maximize_performance_metric()</code>-function.</p>
<p>No matter how simple and promising this might sound, the things tend to get complicated - namely when a single Python-interpreter process gets turned into some form of multi-process ( not threads ) multi-processing and/or even distributed, over a pool of computing nodes, for increased overall performance. There this simplest way will not work. In all cases, there are chances to use data-on-demand pipelines ( ZeroMQ, nanomsg and other low-overhead, ultra-performing FinTech Quant tools for doing this no matter how distributed ).</p>
<p>Your <strong>Q2</strong> gets us into another corner. ML-tools, XGB-toys included, are nothing but trivial HyperParametrised-toys. Classical toys get <code>.train()</code>-method &quot;loaded&quot; with a task to &quot;minimise&quot; some penalty-(or-reward-inversed)-function ( for a given set of HyperParameter values, be theirs actual values explicitly set or implicitly derived from (hopefully documented) defaults ).</p>
<p>Nothing else.</p>
<p>It is not fair to expect these &quot;minimiser&quot;-engines do anything else than very this.</p>
<p>Given a set of &quot;supervised&quot; <code>( X_[M_features,N_examples], y_GROUND_TRUTH_answers[N_examples] )</code>, the <code>.train()</code>-methods govern the ML-toy to adapt its internal state, right according the &quot;minimiser&quot; penalty-function ( thus you get tasked to deliver also the <code>grad, hess</code> for cases, you opt to not re-use the built-in penalty functions, for which these are known and implemented, outsourcing such duty and implementation thereof onto you, to do so and deliver this via local <code>grad, hess</code> values per call ). Having done so, the ML-toy has reached no other state, but the such one, that will get the lowest possible amount of penalty ( ... for the PAST market data ... Here no one ought be surprised, as we did not say the ML-toy to anything else but this very goal - set yourself ( internally, within your implemented, so hardwired, internal numerical logic ) so as to receive a minimum amount of accumulated penalties for those examples, we gave you.  In other words, any other internal re-configuration of the hardwired logic will deliver higher penalty for the same data, than the internal &quot;minimiser&quot; self-adapted the ML-toy into. That is great, no doubts, massive amounts of computation powers was consumed to reach such &quot;minimum&quot;-penalty goal, yet such state is &quot;minimal&quot; only ... for the PAST market data ... which we all know beforehand, will repeat with a probability close to zero. )</p>
<p>This is why the ML-practitioners carefully distinguish DATA-driven <code>.train()</code>-phase, for which a fixed, pre-programmed logic is hardwired, from HyperParameters-driven &quot;minimiser&quot;-tuning and from actual ways, how ML-toys get improved results for <em><em>&quot;ability to</em> (somehow) <em>generalise&quot;</em></em> ( to escape from a cage of &quot;training&quot;-DATA dictated dangerously local, never re-appearing minimum ) by means of Quant Feature Engineering etc.</p>
<p><strong>Q1</strong> is the most complicated, if ought be answered honestly and seriously. First, as said above, it is nonsense not to force, by the use of the <code>.train()</code>-method, the ML-toy to internally self-tune on the very Quant-fair performance, is it?</p>
<p>Non-Quants often start to understand this way too late, while academic papers carry the same kind of nonsense so often, as they do not bear the adverse costs of losses, do they?</p>
<p>Why is this claimed to be most complicated? Because of heteroskedasticity.</p>
<p><em>PROOF:</em><br>
Imagine for a moment, we have already reached a point we have operational a Dream-Machine system, that can trade our well-prepared, always winning strategy ( as was already thoroughly Quant-proven in all our backtesting scenarios under indeed all possible market conditions ).</p>
<p>What will happen with such a cool Dream-Machine system on the real Markets?</p>
<p>Theory, nose-dived into temporal-episode-data minimised, local-only, non-heteroskedastic ML-toys, will answer &quot;It will apply our best-ever trading strategy, that will always be better than any other and we will beat the Market.&quot; - it cannot answer anything else, as the ML-&quot;minimiser&quot;-toys indeed do nothing else but this.</p>
<p>That would also mean, that such Dream-Machine strategy will progress forever and gain and accumulate profits growing in principle infinitely large, over time.</p>
<p>We know, that this is not possible to practically materialise, if for nothing else, then for the overall global limit of all money ( funds ) operating in Financial Markets.</p>
<p>This proof by contradiction ( Q.E.D. ) suffices to refute the hypothesis a such tool, knowingly trained on a static set of no matter how large or small local-context of past data, having no internal adaptations so as to at least ex-post somehow reflect any actual external exosystem evolutions, the less having any kind of robustness to operate inside a heteroskedastic, heavily-constrained, fast self-evolving &amp; self-modifying exosystem, can ever meet the initial hypothesis in real world of Financial Markets.</p>
","0","Answer"
"78593677","78587301","<p>Parameter <code>scale_pos_weight</code> scales only the weights of the positive samples, hence the documentation suggest setting it to <code>sum(negatives)/sum(positives)</code>. It leaves the weights of the negative samples equal to one, so the sum of the scaled weights of the positive samples will be equal to the sum of the (unit) weights of the negative samples.</p>
<p>The <code>scale_pos_weight</code> parameter has to be positive, but it can be larger than one, so if you have more negative than positive samples, you can still use it and set it to <code>sum(negatives)/sum(positives)</code>.</p>
<p>The point of this parameter to let XGBoost see the data as balanced, hence using it I think will affect the metrics which depend on class imbalance like precision and F1 score. The <a href=""https://en.wikipedia.org/wiki/F-score#Dependence_of_the_F-score_on_class_imbalance"" rel=""nofollow noreferrer"">wikipedia page of F1 score</a> seems to have some interesting references (<a href=""https://arxiv.org/abs/2001.05571"" rel=""nofollow noreferrer"">12</a> and <a href=""https://arxiv.org/abs/1909.02827"" rel=""nofollow noreferrer"">13</a>) for this topic, which you might find useful.</p>
<hr />
<p>You can see this in the source code. Let's take <a href=""https://github.com/dmlc/xgboost/tree/release_2.1.0"" rel=""nofollow noreferrer"">xgboost code version 2.1.0</a>. First visit <a href=""https://github.com/dmlc/xgboost/blob/release_2.1.0/src/objective/regression_param.h"" rel=""nofollow noreferrer"">src/objective/regression_param.h</a>, where you can find briefly what I described above:</p>
<pre><code>struct RegLossParam : public XGBoostParameter&lt;RegLossParam&gt; {
  float scale_pos_weight;
  // declare parameters
  DMLC_DECLARE_PARAMETER(RegLossParam) {
    DMLC_DECLARE_FIELD(scale_pos_weight).set_default(1.0f).set_lower_bound(0.0f)
      .describe(&quot;Scale the weight of positive examples by this factor&quot;);
  }
};
</code></pre>
<p>Parameter <code>scale_pos_weight</code> defaults to 1 and lower bounded by zero, it is however not upper bounded and the description says it scales the &quot;positive examples&quot;.</p>
<p>Visiting file <a href=""https://github.com/dmlc/xgboost/blob/release_2.1.0/src/objective/regression_obj.cu"" rel=""nofollow noreferrer"">src/objective/regression_obj.cu</a> can confirm the latter. There you can find</p>
<pre><code>for (size_t idx = begin; idx &lt; end; ++idx) {
  bst_float p = Loss::PredTransform(preds_ptr[idx]);
  bst_float w = _is_null_weight ? 1.0f : weights_ptr[idx / n_targets];
  bst_float label = labels_ptr[idx];
  if (label == 1.0f) {
    w *= _scale_pos_weight;
  }
  out_gpair_ptr[idx] = GradientPair(Loss::FirstOrderGradient(p, label) * w,
                                    Loss::SecondOrderGradient(p, label) * w);
}
</code></pre>
<p>which is the only nontrivial use of <code>scale_pos_weight</code> (as I see), and it indeed scales only the positive samples (having label 1).</p>
","0","Answer"
"78594286","78594171","<p>In your <code>cost_function</code>, the average cost is inside the loop, which results in a cost value that is m times smaller than it should be, preventing the data set from converging. You should sum up all data before averaging.</p>
<pre><code>def cost_function(x, y, w, b):

    # 1) Number of training examples
    m = x.size
    cost = 0

    # 2) Index the training examples and account for cost per instance
    for i in range(m):
        y_hat = w * x[i] + b      
        cost += (y_hat-y[i])**2
    cost /= 2 * m

    # 3) Return total cost
    return cost
</code></pre>
","2","Answer"
"78594968","78563364","<p>I found same question in the huggingface forum (<a href=""https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/37"" rel=""nofollow noreferrer"">https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/37</a>), and the answer is to use &quot;AutoTokenizer&quot;. Please try it.</p>
","-1","Answer"
"78596192","78596157","<p>Edit:</p>
<p>It was the Dropout (facepalm).</p>
<p>With dropout, the model can't rely on the one feature I am giving it that is useful.</p>
<p>Wow.</p>
","0","Answer"
"78596193","78596157","<p>You can use <code>StandardScaler()</code> and increase the number of epochs:</p>
<pre><code>import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler

test_length = 0.15
features = np.zeros((1000, 40, 40))
labels = np.random.rand(1000)

features[:, 0, 0] = labels.copy()

features_train = features[0:int(len(features) * (1 - test_length))]
labels_train = labels[0:int(len(labels) * (1 - test_length))]
features_test = features[int(len(features) * (1 - test_length)):]
labels_test = labels[int(len(labels) * (1 - test_length)):]

SS = StandardScaler()

TR = features_train.reshape(-1, features_train.shape[-1])
TS = features_test.reshape(-1, features_test.shape[-1])

TRN = SS.fit_transform(TR).reshape(features_train.shape)
TSN = SS.transform(TS).reshape(features_test.shape)

model = Sequential([
    LSTM(100, return_sequences=True, input_shape=(features_train.shape[1], features_train.shape[2])),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(50, activation='relu'),
    Dropout(0.2),
    Dense(30, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
history = model.fit(
    TRN, labels_train,
    epochs=100,
    batch_size=64,
    validation_data=(TSN, labels_test),
    callbacks=[early_stopping],
    verbose=1
)

</code></pre>
<h3>Note:</h3>
<p>You can also pass <code>EarlyStopping()</code> callbacks.</p>
","0","Answer"
"78597070","78597010","<ul>
<li>We can create a data structure for the nodes, weights, and edges of the graph.</li>
<li>We initialize the distance matrix with infinite values and zeros on the diagonal and fill the matrix with the weights.</li>
<li>We compute the shortest paths among all nodes using Floyd-Warshall.</li>
<li>We calculate the score based on the equation.</li>
<li>We use a heuristic method to find a good coloring of the nodes that minimizes the score.</li>
</ul>
<h3>Code</h3>
<pre><code>import numpy as np
import random
from collections import defaultdict


def _graph(edges):
    weights, nodes = defaultdict(int), set()
    for a, b, w in edges:
        nodes.update({a, b})
        weights[(a, b)] = w
        weights[(b, a)] = w
    return list(nodes), weights


def _dist(nodes, weights):
    n = len(nodes)
    dist = np.full((n, n), np.inf)
    for i in range(n):
        dist[i, i] = 0
    for (a, b), w in weights.items():
        i, j = nodes.index(a), nodes.index(b)
        dist[i, j] = w
    return dist


def floyd_warshall(n, dist):
    for k in range(n):
        for i in range(n):
            for j in range(n):
                if dist[i, j] &gt; dist[i, k] + dist[k, j]:
                    dist[i, j] = dist[i, k] + dist[k, j]
    return dist


def get_score(coloring, dist, nodes):
    I = {'R': 0, 'B': 0}
    E = 0
    i_colors = {
        'R': [i for i, c in enumerate(coloring) if c == 'R'],
        'B': [i for i, c in enumerate(coloring) if c == 'B']
    }
    for c in i_colors:
        inds = i_colors[c]
        for i in range(len(inds)):
            for j in range(i + 1, len(inds)):
                I[c] += dist[inds[i], inds[j]]
    for i in i_colors['R']:
        for j in i_colors['B']:
            E += dist[i, j]
    equation = 0.8 * (I['R'] + I['B']) + 0.2 * E
    return equation


def get_heuristic(nodes, colors, dist):
    res_coloring = None
    res_score = float('inf')
    colors = ['R'] * colors['R'] + ['B'] * colors['B']
    random.seed(17)
    random.shuffle(colors)
    for _ in range(100):
        random.shuffle(colors)
        curr = get_score(colors, dist, nodes)
        if curr &lt; res_score:
            res_score = curr
            res_coloring = colors[:]
    return res_coloring, res_score


def main():
    edges = (('A', 'B', 10), ('A', 'C', 9), ('A', 'D', 2), ('B', 'C', 1))
    colors = {'R': 2, 'B': 2}
    nodes, weights = _graph(edges)
    dist = _dist(nodes, weights)
    dist = floyd_warshall(len(nodes), dist)
    coloring, score = get_heuristic(nodes, colors, dist)
    res = dict(zip(nodes, coloring))
    return res, score


if __name__ == &quot;__main__&quot;:
    print(main())

</code></pre>
<h3>Prints</h3>
<p>({'A': 'B', 'D': 'B', 'C': 'R', 'B': 'R'}, 10.8)</p>
<hr />
<p>We can also use Dijkstra or other similar graph algorithms:</p>
<pre><code>import numpy as np
import random
import heapq
from collections import defaultdict, deque


def _graph(edges):
    weights, nodes = defaultdict(dict), set()
    for a, b, w in edges:
        nodes.update({a, b})
        weights[a][b] = w
        weights[b][a] = w
    return list(nodes), weights


def dijkstra(n, nodes, weights):
    dist = np.full((n, n), np.inf)
    for i in range(n):
        dist[i, i] = 0
        source = nodes[i]
        Q = [(0, source)]
        seen = set()
        while Q:
            curr_dist, u = heapq.heappop(Q)
            if u in seen:
                continue
            seen.add(u)
            u_idx = nodes.index(u)
            for v, w in weights[u].items():
                v_idx = nodes.index(v)
                if dist[i, v_idx] &gt; curr_dist + w:
                    dist[i, v_idx] = curr_dist + w
                    heapq.heappush(Q, (dist[i, v_idx], v))
    return dist


def get_score(coloring, dist, nodes):
    I = {'R': 0, 'B': 0}
    E = 0
    i_colors = {
        'R': [i for i, c in enumerate(coloring) if c == 'R'],
        'B': [i for i, c in enumerate(coloring) if c == 'B']
    }
    for c in i_colors:
        inds = i_colors[c]
        for i in range(len(inds)):
            for j in range(i + 1, len(inds)):
                I[c] += dist[inds[i], inds[j]]
    for i in i_colors['R']:
        for j in i_colors['B']:
            E += dist[i, j]
    equation = 0.8 * (I['R'] + I['B']) + 0.2 * E
    return equation


def get_heuristic(nodes, colors, dist):
    res_coloring = None
    res_score = float('inf')
    colors = ['R'] * colors['R'] + ['B'] * colors['B']
    random.seed(17)
    random.shuffle(colors)
    for _ in range(100):
        random.shuffle(colors)
        curr = get_score(colors, dist, nodes)
        if curr &lt; res_score:
            res_score = curr
            res_coloring = colors[:]
    return res_coloring, res_score


def main():
    edges = (('A', 'B', 10), ('A', 'C', 9), ('A', 'D', 2), ('B', 'C', 1))
    colors = {'R': 2, 'B': 2}
    nodes, weights = _graph(edges)
    dist = dijkstra(len(nodes), nodes, weights)
    coloring, score = get_heuristic(nodes, colors, dist)
    res = dict(zip(nodes, coloring))
    return res, score


if __name__ == &quot;__main__&quot;:
    print(main())

</code></pre>
","0","Answer"
"78598796","78597865","<p>the model definition is correct (however I would consider using <code>dict</code> in model <code>outputs</code> attribute). My guess is the shape of the inputs is where the problem lays. Check this mock example that is based on your code, but uses random data and correct shape of your inputs to be aligned with this example:</p>
<pre class=""lang-py prettyprint-override""><code>def custom_generator(n_samples = 20):
    &quot;&quot;&quot; Iterator that returns random data. 
    
        Arguments:
          n_samples - same as `batch_size`
    &quot;&quot;&quot;

    while True:
        x = tf.random.normal([n_samples, 224, 244,3])

        n_classes = 2
        # shape of [n_samples, 1]
        indices = tf.random.categorical(tf.math.log([[0.5] * n_classes]), n_samples)[0]
        # shape of [n_samples, 2]
        edible_labels = tf.one_hot(indices, depth = n_classes)

        n_classes = 20
        # shape of [n_samples, 1]
        indices = tf.random.categorical(tf.math.log([[0.5] * n_classes]), n_samples)[0]
        # shape of [n_samples, 20]
        mushroom_onehot = tf.one_hot(indices, depth = n_classes)


        yield x, {'edibility_output': edible_labels, 'mushroom_class': mushroom_onehot}



n_steps = 60
train_generator_custom = custom_generator()
validation_generator_custom = custom_generator()

# Create the full model with two outputs
model = Model(inputs=base_model.input, 
              outputs={'edibility_output':edibility_output, 
                       'mushroom_class':mushroom_class})


# Compile the model
model.compile(optimizer=Adam(),
              loss={'edibility_output': 'binary_crossentropy', 
                    'mushroom_class': 'categorical_crossentropy'},
              metrics={'mushroom_class': 'accuracy', 'edibility_output': 'accuracy'})


history = model.fit(
    train_generator_custom,
    steps_per_epoch=n_steps,
    epochs=2,
    validation_data=validation_generator_custom,
    validation_steps=n_steps
)

</code></pre>
","0","Answer"
"78599078","78599045","<p>Dunno what just happened, doing this solution can solved my problem:</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == '__main__':
  sys.stdin.reconfigure(encoding='utf-8')
  sys.stdout.reconfigure(encoding='utf-8')

  # ...
</code></pre>
<p>Output:</p>
<pre><code>int(0)
string(407) &quot;Init model
Model already exists in keras-model/model-transventricular-v3.keras, skipping model init.
Infer image
(1, 256, 256, 1)
&quot; keras-model\model-transventricular-v3.keras &quot;

[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2s/step
[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 2s/step
&quot;
string(1601) &quot;2024-06-09 23:19:26.963207: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may
see slightly different numerical results due to floating-point round-off errors from different computation orders. To
turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-09 23:19:27.937896: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly
different numerical results due to floating-point round-off errors from different computation orders. To turn them off,
set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-09 23:19:29.885933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to
use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler
flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1717949982.644210 22100 service.cc:145] XLA service 0x2097abd9cb0 initialized for platform Host (this does
not guarantee that XLA will be used). Devices:
I0000 00:00:1717949982.644533 22100 service.cc:153] StreamExecutor device (0): Host, Default Version
2024-06-09 23:19:42.697126: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash
reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1717949983.623732 22100 device_compiler.h:188] Compiled cluster using XLA! This line is logged at most once
for the lifetime of the process.
&quot;
</code></pre>
<p>Reference: <a href=""https://stackoverflow.com/a/63573649/11748924"">https://stackoverflow.com/a/63573649/11748924</a></p>
","0","Answer"
"78606008","78592454","<blockquote>
<p>My question is, how do you decide what parameter values to split at, if your parameters are real?</p>
</blockquote>
<p>It depends on what <code>criterion</code> the tree is using to judge the quality of a split.</p>
<blockquote>
<p>I was expecting to find some sort of parameter to determine how many &quot;equally spaced&quot; splits to make</p>
</blockquote>
<p>When the tree is deciding what splits to make, it is only considering splits which split the dataset into two parts. It never considers splits which split up the dataset into three parts, or four parts, etc.</p>
<p>There are two reasons for this.</p>
<p>First, it would increase the search space. If you have N datapoints of one variable, you have a linear number of ways to split the dataset into two subsets. But there are a quadratic number of ways to split it into three subsets.</p>
<p>Second, it is redundant to search these. The tree-fitting algorithm will be run recursively on each subset of the dataset, so it is possible the algorithm will split variable #1, then split it again, and end up with a three way split. It all depends on how informative each variable is relative to the others.</p>
","0","Answer"
"78606845","78603383","<p>You can use <code>fsspec</code> and <code>adlfs</code> package to write data to storage account with proper authentication.</p>
<p>Install packages
<code>pip install azureml-fsspec adlfs</code></p>
<p>Refer this <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/tutorial-use-pandas-spark-pool#readwrite-data-using-secondary-adls-account"" rel=""nofollow noreferrer"">documentation </a></p>
<pre class=""lang-py prettyprint-override""><code>st = {'account_key' : '&lt;account_key&gt;'}
## or st = {'sas_token' : 'sas_token_value'}
## or st = {'connection_string' : 'connection_string_value'}
## or st = {'tenant_id': 'tenant_id_value', 'client_id' : 'client_id_value',    'client_secret': 'client_secret_value'}

df.to_csv('abfs://&lt;container_name&gt;@&lt;storage_account_name&gt;.dfs.core.windows.net/local/tmp.csv', storage_options  =  st)
</code></pre>
<p>Here, you need to give storage option for authentication, I would recommend using <code>sas_token</code> or service principal.</p>
<p>output:</p>
<p><img src=""https://i.imgur.com/Xk7Zpig.png"" alt=""enter image description here"" /></p>
","2","Answer"
"78608273","78605622","<p><code>LabelEncoder</code> can not only <code>transorm</code> your original target into numerical values but also it can <code>inverse_transform</code> numerical values into your original target values.</p>
<p>So your code should look like this:</p>
<pre><code>from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit_transform(y_train)


# fit
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=200,
    max_depth=6,  
    learning_rate=0.1,  
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric='mlogloss'  
)

xgb.fit(X_train, y_train)



# prediction
pred = xgb.predict(X_test)
original_pred = le.inverse_transform(pred)
</code></pre>
<p>Official documentation <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">here</a>.</p>
","1","Answer"
"78612038","78610497","<p>When dealing with data containing anonymous features and records, employing grid search to determine optimal parameters through hyperparameter tuning is advisable. Sklearn offers various hyperparameter tuning systems, among which GridSearchCV stands out for its efficacy. It often yields superior results in a shorter timeframe compared to randomized search. However, the choice between these methods ultimately depends on the specific dataset being utilized.</p>
<p>To utilize grid search, one should define potential parameters in a dictionary format and supply them along with the model to GridSearchCV</p>
<pre><code>from sklearn.model_selection import GridSearchCV

# define hypertuning parameters
parameters = {'n_neighbors':[3, 5, 10, 15]}

knn = KNeighborsClassifier()
clf = GridSearchCV(knn, parameters)

# Fit the GridSearchCV object to the data
clf.fit(X_train, y_train)

# Get the best parameters
best_params = clf.best_params_

print(&quot;Best parameters found:&quot;, best_params)
</code></pre>
<p>Feel free to add more tuning parameters for the model. For further information about the GridSearchCV please refer the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"" rel=""nofollow noreferrer"">page</a>.</p>
","-1","Answer"
"78612253","78610947","<p>If you simply wish to utilize a pre-trained PyTorch model, please follow the instructions provided below.</p>
<pre><code>import torch

# Load the model
model = torch.load('model.bin')
input_data = torch.tensor([1, 2, 3, 4, 5])  # Example input data

with torch.no_grad():
    output = model(input_data)
print(output)
</code></pre>
","0","Answer"
"78616753","78615860","<p>I am not fully sure if I unsterdood what you are trying to do.</p>
<p>However, if you want to do sampling from a discrete probability in a differentiable way, you probably need to use the gumbel-softmax trick. It allows for sampling from a categorical distribution during the forward pass through a neural network  :</p>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html</a></p>
<p><a href=""https://sassafras13.github.io/GumbelSoftmax/"" rel=""nofollow noreferrer"">https://sassafras13.github.io/GumbelSoftmax/</a></p>
","1","Answer"
"78616871","78616686","<p>Take a look at the traceback you are getting. To have a reproducible example, I reduced your code to this:</p>
<pre><code>

import os

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader




# CNN Model
class CNNModel(nn.Module):
    def __init__(self, num_classes):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64*30*115*115, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool3d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool3d(x, 2)
        x = x.view(-1, 64*30*115*115)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNModel(num_classes=20).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()
</code></pre>
<p>I get the following traceback:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[1], line 33
     31 # Training Loop
     32 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
---&gt; 33 model = CNNModel(num_classes=20).to(device)
     34 optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
     35 loss_function = nn.CrossEntropyLoss()

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:1145, in Module.to(self, *args, **kwargs)
   1141         return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   1142                     non_blocking, memory_format=convert_to_format)
   1143     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
-&gt; 1145 return self._apply(convert)

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:797, in Module._apply(self, fn)
    795 def _apply(self, fn):
    796     for module in self.children():
--&gt; 797         module._apply(fn)
    799     def compute_should_use_set_data(tensor, tensor_applied):
    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
    801             # If the new tensor has compatible tensor type as the existing tensor,
    802             # the current behavior is to change the tensor in-place using `.data =`,
   (...)
    807             # global flag to let the user control whether they want the future
    808             # behavior of overwriting the existing tensor or not.

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:820, in Module._apply(self, fn)
    816 # Tensors stored in modules are graph leaves, and we don't want to
    817 # track autograd history of `param_applied`, so we have to use
    818 # `with torch.no_grad():`
    819 with torch.no_grad():
--&gt; 820     param_applied = fn(param)
    821 should_use_set_data = compute_should_use_set_data(param, param_applied)
    822 if should_use_set_data:

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:1143, in Module.to.&lt;locals&gt;.convert(t)
   1140 if convert_to_format is not None and t.dim() in (4, 5):
   1141     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   1142                 non_blocking, memory_format=convert_to_format)
-&gt; 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)

RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>which states that the issue is in <code>model = CNNModel(num_classes=20).to(device)</code>, so it is already your model that is too large for your GPU, it has nothing to do with training.</p>
<pre><code>nn.Linear(64*30*115*115, 512)
</code></pre>
<p>will take up a huge amount of memory, as the weight matrix will be <code>64*30*115*115*512</code> entries large. At 4 bytes per entry, this gives roughly 48 GB of GPU memory required.</p>
<p>So you will need to rethink your model.</p>
","1","Answer"
"78618268","78617059","<p>Steps to Run YOLO Model on Mobile using Kotlin</p>
<ol>
<li>nConvert YOLOv8 to ONNX format.</li>
<li>Use ONNX Runtime in your Android application to run the model.</li>
<li>Capture image using Camera and process it through the model.</li>
<li>Draw rectangles on detected objects on the captured image.</li>
</ol>
<pre><code>dependencies {
    implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.8.1'
}

</code></pre>
<p>Load the ONNX model in your Kotlin code:</p>
<pre class=""lang-kotlin prettyprint-override""><code>
import ai.onnxruntime.*

class YoloV8Model(context: Context) {

    private val ortEnvironment: OrtEnvironment = OrtEnvironment.getEnvironment()
    private val sessionOptions = OrtSession.SessionOptions()
    private val ortSession: OrtSession

    init {
        val modelBytes = context.assets.open(&quot;yolov8.onnx&quot;).readBytes()
        ortSession = ortEnvironment.createSession(modelBytes, sessionOptions)
    }

    fun detectObjects(bitmap: Bitmap): List&lt;Detection&gt; {
        val resizedBitmap = Bitmap.createScaledBitmap(bitmap, 640, 640, true)
        val floatBuffer = convertBitmapToFloatBuffer(resizedBitmap)

        val inputName = ortSession.inputNames.iterator().next()
        val inputTensor = OnnxTensor.createTensor(ortEnvironment, floatBuffer, longArrayOf(1, 3, 640, 640))

        val results = ortSession.run(mapOf(inputName to inputTensor))
        val outputTensor = results[0].value as Array&lt;FloatArray&gt;

        return parseDetections(outputTensor)
    }

    private fun convertBitmapToFloatBuffer(bitmap: Bitmap): FloatBuffer {
        val floatBuffer = FloatBuffer.allocate(3 * 640 * 640)
        val intValues = IntArray(640 * 640)
        bitmap.getPixels(intValues, 0, 640, 0, 0, 640, 640)

        for (pixelValue in intValues) {
            floatBuffer.put(((pixelValue shr 16) and 0xFF) / 255.0f)
            floatBuffer.put(((pixelValue shr 8) and 0xFF) / 255.0f)
            floatBuffer.put((pixelValue and 0xFF) / 255.0f)
        }

        floatBuffer.rewind()
        return floatBuffer
    }

    private fun parseDetections(outputTensor: Array&lt;FloatArray&gt;): List&lt;Detection&gt; {
        val detections = mutableListOf&lt;Detection&gt;()
        for (detection in outputTensor) {
            val confidence = detection[4]
            if (confidence &gt; 0.5) {
                val x1 = detection[0]
                val y1 = detection[1]
                val x2 = detection[2]
                val y2 = detection[3]
                val classId = detection[5].toInt()
                detections.add(Detection(x1, y1, x2, y2, confidence, classId))
            }
        }
        return detections
    }
}

data class Detection(val x1: Float, val y1: Float, val x2: Float, val y2: Float, val confidence: Float, val classId: Int)

</code></pre>
<p>Use the YoloV8Model class to run the model and draw rectangles on the detected objects:</p>
<pre class=""lang-kotlin prettyprint-override""><code>
import android.graphics.Bitmap
import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint

fun drawDetections(bitmap: Bitmap, detections: List&lt;Detection&gt;): Bitmap {
    val mutableBitmap = bitmap.copy(Bitmap.Config.ARGB_8888, true)
    val canvas = Canvas(mutableBitmap)
    val paint = Paint().apply {
        color = Color.RED
        strokeWidth = 3f
        style = Paint.Style.STROKE
    }

    for (detection in detections) {
        canvas.drawRect(detection.x1, detection.y1, detection.x2, detection.y2, paint)
    }

    return mutableBitmap
}

</code></pre>
<p>Capture the image using Camera and process it through the model:</p>
<pre class=""lang-kotlin prettyprint-override""><code>import android.graphics.BitmapFactory
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import kotlinx.android.synthetic.main.activity_main.*

class MainActivity : AppCompatActivity() {

    private lateinit var yoloV8Model: YoloV8Model

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        yoloV8Model = YoloV8Model(this)

        // Capture image (assuming you have an image in assets for demo purposes)
        val inputStream = assets.open(&quot;test_image.jpg&quot;)
        val bitmap = BitmapFactory.decodeStream(inputStream)

        val detections = yoloV8Model.detectObjects(bitmap)
        val resultBitmap = drawDetections(bitmap, detections)

        imageView.setImageBitmap(resultBitmap)
    }
}



</code></pre>
","0","Answer"
"78626438","78626417","<p><code>included</code> is a list and <code>worst_feature</code> is not part of that list, hence removing it fails and you get the error you have mentioned in the question. You can check whether the element is in the list, like</p>
<pre><code>if pvalues[worst_feature] in included:
    included.remove(pvalues[worst_feature])
</code></pre>
<p>Note that I'm actually getting the value located at the index of the worst feature.</p>
<p>This would only technically solve your issue though, because you basically have a logic issue as the culprit. You want to remove the worst included feature, but you search for the worst feature among all features and if the very worst feature happens not to be included, then you will get the error you received. So, a better way to solve this would be to search for the worst feature among the included features rather than among all features.</p>
","0","Answer"
"78626445","78626417","<p>When calling list.remove(x) you need to pass the value you want to remove, while here you pass the index returned by np.argmax().</p>
<p>See <a href=""https://docs.python.org/3/tutorial/datastructures.html"" rel=""nofollow noreferrer"">https://docs.python.org/3/tutorial/datastructures.html</a></p>
","0","Answer"
"78627831","78622030","<p>For anyone who gets stuck on this after me:</p>
<p>ML.NET (as of 3.0) doesn't seem to support multidimensional input/output arrays, so models that use it from tensorflow need to be flattened!</p>
<p>Use the VectorType attribute to tell ML.NET what the flattened float arrays look like:</p>
<pre><code>public class ModelInput
{
    [VectorType(64, 128, 96)]
    [ColumnName(&quot;melspectrogram&quot;)]
    public float[] Melspectrogram { get; set; }
}

// Define output schema
public class ModelOutput
{
    [VectorType(64, 512)]
    [ColumnName(&quot;embeddings&quot;)]
    public float[] Embeddings { get; set; }
}
</code></pre>
<p>Then Flatten your arrays! These are specifically fit for my usecase, yours may be different:</p>
<pre><code>public static float[] FlattenArray(float[,,] inputArray)
{
    int dim1 = inputArray.GetLength(0);
    int dim2 = inputArray.GetLength(1);
    int dim3 = inputArray.GetLength(2);

    float[] flatArray = new float[dim1 * dim2 * dim3];

    int index = 0;
    for (int i = 0; i &lt; dim1; i++)
    {
        for (int j = 0; j &lt; dim2; j++)
        {
            for (int k = 0; k &lt; dim3; k++)
            {
                flatArray[index++] = inputArray[i, j, k];
            }
        }
    }

    return flatArray;
}
public static float[,] UnflattenArray(float[] flatArray)
{
    int dim1 = 64;
    int dim2 = 512;

    float[,] outputArray = new float[dim1, dim2];

    int index = 0;
    for (int i = 0; i &lt; dim1; i++)
    {
        for (int j = 0; j &lt; dim2; j++)
        {
            outputArray[i, j] = flatArray[index++];
        }
    }

    return outputArray;
}
</code></pre>
","0","Answer"
"78632101","78599128","<p>There is one primary unknown here, what is the approximate or average number of tokens in the &quot;text&quot; part of your input.</p>
<p>Scenario 1: You do not have a very long input (say, somewhere around 512 tokens)</p>
<p>In this case, to get better results, you can train your own &quot;embedding-model&quot;, please look at <a href=""https://stackoverflow.com/a/71574306/5123629"">my answer</a> here which has some info around it.</p>
<p>Once you get right embedding model, you index corresponding text vectors in you RAG pipeline. There are a couple of other steps as well which are applicable to all the scenarios, so, I will add them at the end.</p>
<p>Scenario 2: You have a very long input per document, say, every &quot;text&quot; input is huge (say, ~8000 tokens, this number can be anything though). In this case you can leverage symbolic search instead of vector search. Symbolic search because, in any language, to describe something which really means the same or has similar context, there will surely be a lot of words overlap in source and target text. It will be very rare to find 10 pages text on a same topic that does not have a lot of work overlap.</p>
<p>So, you can leverage symbolic search here, ensemble it with vector based validators and use an LLM service that allows long context prompts. So, you find some good candidates via symbolic searches, then, pass it on the long context LLM to for remaining parts.</p>
<p>Steps Applicable to all the scenarios:</p>
<ol>
<li><p>You json object should also contain &quot;tag&quot;, &quot;location&quot; along with &quot;text&quot; and &quot;vector&quot;</p>
<blockquote>
<pre><code>{&quot;text&quot;:&quot;some text&quot;,
&quot;text_embedding&quot;:[...], #not applicable in symbolic search

&quot;location&quot;:&quot;loc&quot;,
&quot;tags&quot;:[]
}
</code></pre>
</blockquote>
</li>
<li><p>This way, when you get matches from either vector search or symbolic search; you will further able to filter or sort based on other properties like tags and location</p>
</li>
</ol>
<p>Please comment if you have more doubts!</p>
","0","Answer"
"78632241","78630853","<p>If you want to use element wise multiplication use <code>*</code> . Typically for gradients or activation functions</p>
<p>If you want Matrix Multiplication use <code>np.dot</code> or <code>@</code>. This is use for combining the gradients across several layers/ nodes.</p>
","0","Answer"
"78633678","78565463","<p>It looks like Ryo Wakabayashi implemented a way to do it:</p>
<p><a href=""https://qiita.com/RyoWakabayashi/items/ff68bb9313c2c30f3b10"" rel=""nofollow noreferrer"">https://qiita.com/RyoWakabayashi/items/ff68bb9313c2c30f3b10</a></p>
<p><img src=""https://media.cleanshot.cloud/media/43831/nz0uZiFVs0g0LlFhjCItzGTmxT3U19hCGLTuPEhQ.gif?Expires=1718662368&amp;Signature=hzV52OwAQEuplngX%7EHO5gYwUPpNNXpBDsTy3NEv8tLudfKMVixO2V9IkDvam96FBNi6y432kogazXw-q-aBBkMHyDAMQ6FjSisXl9JM7-GZpKz6L1SGFeyyYIFt8BA9q2PZRWOCHOlkZ49Y8SNy6Hf2c0vt5E3Jb%7EF4qwbZCZ%7Eyu34jiKXLo%7EGIXsJ0BvQZ8duAdZYYvdvpwk1CtvFTIHUGP4ofCn3YqSN4XbkuQkmdVIe-RoE-qtpPeFakKfdA2hNyegmA80TOtUTszo3HD8q%7EXEt0poGNhPdeDYhAszfjWgfNx0-xfF6FZl5aAHUPc6e-SnXqN8aY5qUqupCE-5Q__&amp;Key-Pair-Id=K269JMAT9ZF4GZ"" alt="""" /></p>
","2","Answer"
"78634295","78619753","<p>The short answer is that one should not use S3 to access training data at the time of training. S3 is not designed for that, there are ways around it, like s3fs project, but this was not the designer's intent.
Among AWS services EFS or EBS are meant for this purpose, and with them things are straightforward - they provide a filesystem.</p>
","0","Answer"
"78635975","78631415","<p>You can try <a href=""https://www.e-iceblue.com/Introduce/pdf-for-python.html"" rel=""nofollow noreferrer"">Spire.PDF for Python</a>.</p>
<pre><code>pip install Spire.Pdf
</code></pre>
<p>Here is the code for extracting text from a PDF:</p>
<pre><code>from spire.pdf.common import *
from spire.pdf import *

def extract_text_from_pdf(file_path, output_file):
    # Load a PDF document
    doc = PdfDocument()
    doc.LoadFromFile(file_path)

    extracted_text = []

    # Iterate over the pages of the document
    for i in range(doc.Pages.Count):
        page = doc.Pages.get_Item(i)
        # Extract the text from the page
        textExtractor = PdfTextExtractor(page)
        option = PdfTextExtractOptions()        
        text = textExtractor.ExtractText(option)
        extracted_text.append(text)

    # Save the extracted text to a text file
    with open(output_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
        text_file.write(&quot;\n&quot;.join(extracted_text))

    doc.Close()

file_path = &quot;Arabic.pdf&quot;
output_file = &quot;ExtractedText.txt&quot;
extract_text_from_pdf(file_path, output_file)
</code></pre>
<p>Here is the input pdf:
<a href=""https://i.sstatic.net/4akTqG4L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4akTqG4L.png"" alt=""enter image description here"" /></a></p>
<p>Here is the result:
<a href=""https://i.sstatic.net/8LX1yjTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8LX1yjTK.png"" alt=""enter image description here"" /></a></p>
<p>It looks like the result is ok. But I am not familiar with Arabic, you can check the accuracy by yourself.
Note: I work for the company that developed this module.</p>
","0","Answer"
"78636354","78636296","<p>Predictions are not wrong per se, you train a model and based on the quality of your data you will have an error when making predictions which will deviate from the test value by some amount.</p>
","0","Answer"
"78636587","78636029","<p>In order to visualize trees generated by HistGradientboostingClassifier this function worked for me:</p>
<pre><code>def visualize_tree(tree, feature_names, class_names): 
dot = graphviz.Digraph() 
def add_nodes_edges(dot, nodes, node_id):
    node = nodes[node_id]
    if node['is_leaf']: 
        value = node['value'] 
        dot.node(str(node_id), f&quot;Predict: {value}&quot;) 
    else: 
        feature = feature_names[node['feature_idx']] 
        threshold = node['bin_threshold'] 
        dot.node(str(node_id), f&quot;{feature} &lt;= {threshold:.2f}&quot;) 
        left_child = node['left'] 
        right_child = node['right']
        dot.edge(str(node_id), str(left_child), &quot;True&quot;) 
        dot.edge(str(node_id), str(right_child), &quot;False&quot;) 
        add_nodes_edges(dot, nodes, left_child) 
        add_nodes_edges(dot, nodes, right_child) 
nodes = tree.__getstate__()['nodes'] 
add_nodes_edges(dot, nodes, 0) 
return dot 

# Create and visualize the tree 
dot = visualize_tree(single_tree, RF_90.feature_names_in_, 1) 
dot.render(&quot;hist_gb_tree&quot;) # Save to file 
dot #view from jupyter
</code></pre>
<p>where RF_90 is the fitted model and single_tree is:</p>
<pre><code>single_tree = trees_per_iteration[iteration][class_index]
</code></pre>
<p>where iteration = 0 and class_index = 0</p>
","0","Answer"
"78639538","78625589","<p>If you're just learning clustering you should start with some of the built in datasets like those found <a href=""https://scikit-learn.org/stable/api/sklearn.datasets.html#module-sklearn.datasets"" rel=""nofollow noreferrer"">here</a>. They are already labeled for you. Without knowing what your dataset, code, or dendrogram look like, I'm not sure how I can help, but I would definitely recommend you start with the documentation on <a href=""https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering"" rel=""nofollow noreferrer"">hierarchical clustering</a>. It sounds like you could potentially benefit through working through a few examples before diving into an unlabeled dataset. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration"" rel=""nofollow noreferrer"">Here</a> is an example of hierarchical clustering with unsupervised dimensionality reduction. <a href=""https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py"" rel=""nofollow noreferrer"">Here</a> is an example of generating a dendrogram, and <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow noreferrer"">here</a> is the Wikipedia page on hierarchical clustering that goes into more detail on the theory behind how dendrograms work. It should help you read dendrograms more easily.</p>
<p>Once you have a firm understanding of the modeling techniques and visualization, you might choose to have some dataset you're interested in labeled if you find it still presents a compelling research question.</p>
<p>Best of luck!</p>
","0","Answer"
"78640294","78640262","<p>Training a machine learning (ML) model repeatedly could lead to model overfit. That's why you need to estimate the best epochs (number of iterations to train the model with same data).</p>
<p>One of the way that I use to analyze and determine how many epochs I need to train a model is by using graphs to compare epochs, model accuracy and model loss. Look at the graphs below.</p>
<ul>
<li><a href=""https://i.sstatic.net/MhnAvHpB.png"" rel=""nofollow noreferrer"">n of epochs and model accuracy</a></li>
<li><a href=""https://i.sstatic.net/8MlOSvjT.png"" rel=""nofollow noreferrer"">n of epochs and model loss</a></li>
</ul>
<p>You can tell if the model is already overfit or still in underfit stage if the model accuracy is not significanly increasing or the model loss is not significantly decreasing.</p>
","1","Answer"
"78641549","78565463","<p>With the advice of <a href=""https://qiita.com/RyoWakabayashi"" rel=""nofollow noreferrer"">Mr. Wakabayashi</a>, I was able to solve this problem using Nx.</p>
<p>The function <code>FPL.update/4</code>, which is the base of machine learning, is shown below:</p>
<pre><code>defmodule FPL do
  import Nx.Defn

  defnp loss_fn({a, b, c, d}, x, y) do
    y_pred = log4pl(x, a, b, c, d)
    Nx.mean(Nx.pow(y_pred - y, 2))
  end

  defnp log4pl(x, a, b, c, d) do
    ((a - d) / (1.0 + Nx.pow(x / c, b)) + d)
  end

  defn update({a, b, c, d} = params, x, y, lr) do
    {grad_a, grad_b, grad_c, grad_d} = grad(params, &amp;loss_fn(&amp;1, x, y))
    {a - grad_a * lr,  b - grad_b * lr, c - grad_c * lr * 1000, d - grad_d * lr}
  end
end
</code></pre>
<p>Using this function, we can obtain the parameters of the approximate curve as follows:</p>
<pre><code>x = Nx.tensor([1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125.0, 250.0, 500.0, 1000.0])
y = Nx.tensor([0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372])

epochs = 2500
lr = 0.05

{a, b, c, d} = {Nx.tensor(1.0), Nx.tensor(1.0), Nx.tensor(1.0), Nx.tensor(1.0)}

acc =
  Enum.reduce(1..epochs, {a, b, c, d}, fn _epoch, acc -&gt;
    FPL.update(acc, x, y, lr)
  end)

{a, b, c, d} = acc
a = Nx.to_number(a)
b = Nx.to_number(b)
c = Nx.to_number(c)
d = Nx.to_number(d)
</code></pre>
<p>The actual values of the parameters obtained are as follows:</p>
<pre><code>a = 0.31874406337738037
b = 1.787353754043579
c = 85.46488952636719
d = 2.420581579208374
</code></pre>
<p>This result is quite close to the original article.</p>
<p>Note that Mr. Wakabayashi said that normalization of the data would make the learning process more efficient.</p>
","1","Answer"
"78642177","78625589","<p>You can use validation techniques such as Silhouette Score, Calinski-Harabasz Index, or Davies-Bouldin Index to know the correctness of model and change the parameters according.</p>
<p>Hope this work.</p>
","0","Answer"
"78643874","78594832","<p>As for the first question, there are <strong>no such objects within the current library code</strong>.
You may want to <a href=""https://github.com/search?q=repo%3Afacebookresearch%2Fnougat+cached&amp;type=code"" rel=""nofollow noreferrer"">check yourself</a>.</p>
<p>The maintainers received such alerts two weeks ago, but haven't responsed yet
<a href=""https://github.com/facebookresearch/nougat/issues/226"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/nougat/issues/226</a></p>
<p>As for the second question, the mismatch is between <code>nougat</code> and <code>transformers</code> as <a href=""https://github.com/huggingface/transformers/issues/30029"" rel=""nofollow noreferrer"">pointed out here</a>. The reported solution is to <strong>downgrade transformers to the moment before the mismatch occurred</strong>, e.g. <code>pip install transformers==4.38.2</code>.</p>
<p>Overall, be prepared for lots of improvisation when working with unmature/ experimental / rapidly developing code. Breaking changes are unavoidable, and users are expected to handle many issues themselves.</p>
","0","Answer"
"78645188","78639577","<p>What data are you using in your replication? As far as I can tell this paper does not mention explicitly the parameters of the data used for the particular result you are trying to replicate. Indeed, it tests a variety of alpha values for the distributions used in figure 6. It is feasible for the loss to be low even after one step of GD if the alpha value is low. If you find the same trends in relative behavior of GD and transformer layers, I don't think it's important to match the exact loss values.</p>
","2","Answer"
"78646427","78643575","<p>You can not control where the code should upload but what you can do is directly giving the path of your storage account where you code is saved.</p>
<p>For the parameter <code>code</code> in <code>command</code> job function you can give</p>
<blockquote>
<p>local path or &quot;http:&quot;, &quot;https:&quot;, or &quot;azureml:&quot; url pointing to a remote location.</p>
</blockquote>
<p>When you give local path it tries to create container and upload the code,
so, i would recommend using <code>https</code>.</p>
<p>If the you python code in <code>src</code> folder than you upload it to any folder in your storage account and copy <code>https</code>  path.</p>
<p><img src=""https://i.imgur.com/zweT2qP.png"" alt=""enter image description here"" /></p>
<p>Sample code</p>
<pre class=""lang-py prettyprint-override""><code>command_job = command(
    code=&quot;https://jgsml28890137600.blob.core.windows.net/data/src/&quot;,
    command=&quot;echo hi from command&quot;,
    environment=&quot;azureml:docker-context-example-10:1&quot;,
    timeout=10800
)
</code></pre>
<p><strong>Note</strong>: Storage account should be associated with ml workspace, if not create a datastore linking you storage account and the http path.</p>
","1","Answer"
"78646748","78581619","<p>I solved the problem. First, I installed matplotlib, and added two functions to my code:</p>
<pre><code>def visualize_mask(image, mask):
    plt.figure(figsize=(10, 10))
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title(&quot;Image Originale&quot;)
    plt.subplot(1, 2, 2)
    plt.imshow(mask, cmap='gray')
    plt.title(&quot;Masque de Segmentation&quot;)
    plt.show()

def visualize_cropped_image(image, cropped_image):
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title(&quot;Image Originale&quot;)
    plt.subplot(1, 2, 2)
    plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))
    plt.title(&quot;Image Recadrée&quot;)
    plt.show()
</code></pre>
<p>With that I can see if the mask is correct and if the crop is OK too. This showed that the mask was reversed, the background was selected instead of the subject.</p>
<p>I solved that by reversing the mask before cropping the image. However, when the base image is a .png with an alpha background, the transparent background was translated to black when converting and cropping the image. That is because OpenCV does not support alpha (transparency) channels by default.</p>
<p>To maintain transparency, you need RGBA images (where channel A represents transparency), and ensure that the background remains transparent.</p>
<pre><code>def load_image(image_path):
    return cv2.imread(image_path, cv2.IMREAD_UNCHANGED)

def save_image(image, path):
    if image.shape[2] == 4:
        b, g, r, a = cv2.split(image)
        white_background = np.ones_like(a, dtype=np.uint8) * 255
        alpha_inv = cv2.bitwise_not(a)
        white_background = cv2.bitwise_and(white_background, white_background, mask=alpha_inv)
        b = cv2.bitwise_or(b, white_background)
        g = cv2.bitwise_or(g, white_background)
        r = cv2.bitwise_or(r, white_background)
        image_no_alpha = cv2.merge([b, g, r])
        image_pil = Image.fromarray(cv2.cvtColor(image_no_alpha, cv2.COLOR_BGR2RGB))
        image_pil.save(path, 'JPEG', quality=95)
    else:
        image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        image_pil.save(path, 'JPEG', quality=95)
</code></pre>
<p>and the final code:</p>
<pre><code>import os
import cv2
import numpy as np
from PIL import Image
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

def load_image(image_path):
    return cv2.imread(image_path, cv2.IMREAD_UNCHANGED)

def save_image(image, path):
    if image.shape[2] == 4:
        b, g, r, a = cv2.split(image)
        white_background = np.ones_like(a, dtype=np.uint8) * 255
        alpha_inv = cv2.bitwise_not(a)
        white_background = cv2.bitwise_and(white_background, white_background, mask=alpha_inv)
        b = cv2.bitwise_or(b, white_background)
        g = cv2.bitwise_or(g, white_background)
        r = cv2.bitwise_or(r, white_background)
        image_no_alpha = cv2.merge([b, g, r])
        image_pil = Image.fromarray(cv2.cvtColor(image_no_alpha, cv2.COLOR_BGR2RGB))
        image_pil.save(path, 'JPEG', quality=95)
    else:
        image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        image_pil.save(path, 'JPEG', quality=95)

def select_object(image):
    long_side = 1024
    height, width = image.shape[:2]
    if max(height, width) &gt; long_side:
        scale = long_side / max(height, width)
        new_size = (int(width * scale), int(height * scale))
        resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)
    else:
        resized_image = image

    if resized_image.shape[2] == 4:
        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGRA2BGR)
    
    sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;env\\Checkpoint\\sam_vit_b_01ec64.pth&quot;)
    mask_generator = SamAutomaticMaskGenerator(sam)
    masks = mask_generator.generate(resized_image)
    largest_mask = max(masks, key=lambda x: x['area'])
    
    mask = largest_mask['segmentation']
    mask = np.logical_not(mask).astype(np.uint8)
    
    original_size_mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)
    
    return original_size_mask

def crop_to_object(image, mask):
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    x, y, w, h = cv2.boundingRect(contours[0])
    cropped_image = image[y:y+h, x:x+w]
    return cropped_image

def resize_with_padding(image, size):
    height, width = image.shape[:2]
    scale = size / max(height, width)
    new_size = (int(width * scale), int(height * scale))
    resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)
    
    delta_w = size - new_size[0]
    delta_h = size - new_size[1]
    top, bottom = delta_h // 2, delta_h - (delta_h // 2)
    left, right = delta_w // 2, delta_w - (delta_w // 2)
    
    color = [255, 255, 255]
    new_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)
    
    return new_image

def process_image(image_path, output_path):
    image = load_image(image_path)
    mask = select_object(image)
    cropped_image = crop_to_object(image, mask)
    
    final_image = resize_with_padding(cropped_image, 800)
    save_image(final_image, output_path)

def process_folder(input_folder, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    for root, _, files in os.walk(input_folder):
        for filename in files:
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                input_path = os.path.join(root, filename)
                relative_path = os.path.relpath(input_path, input_folder)
                output_path = os.path.join(output_folder, relative_path)
                output_path = os.path.splitext(output_path)[0] + '.jpg'
                output_dir = os.path.dirname(output_path)
                if not os.path.exists(output_dir):
                    os.makedirs(output_dir)
                try:
                    process_image(input_path, output_path)
                    print(f&quot;Processed {input_path}&quot;)
                except Exception as e:
                    print(f&quot;Failed to process {input_path}: {e}&quot;)

if __name__ == &quot;__main__&quot;:
    input_folder = &quot;&quot;
    output_folder = &quot;&quot;
    process_folder(input_folder, output_folder)
</code></pre>
","0","Answer"
"78649928","78649700","<p>The problem is inconsistency between your <code>__init__</code> parameters and the dictionary returned by <code>get_config</code>, this dictionary should contain keys corresponding to the parameters in your <code>__init__</code>.</p>
<p>In your case the <code>encoder_embedding</code> variable is saved in <code>get_config</code> but this is not a parameter in <code>__init__</code>, so you get an error. Make sure to save only the variables and keys that are required and not more.</p>
","-1","Answer"
"78650389","78649554","<ol>
<li>Decision Trees work best when the dataset is large and there are multiple features. If the dataset is small and the features are limited, Decision Tree Overfits the data. Regularization, Cross-Validation, Data Augmentation, Feature Engineering, pruning, Hyperparameter tuning,  Normalization can help generate good results using Decision Trees.</li>
<li>Random Forest reduces variance and generates generalizable predictions, as a result of averaging multiple trees.</li>
<li>The piecewise constant predictions in DT can be precise in each region but overall, while generalizing it might not perform good in addition to overfitting.</li>
</ol>
","0","Answer"
"78652083","78643538","<p>Here is what ended up working for me</p>
<pre><code># source venv/bin/activate

from diffusers import StableDiffusion3Pipeline
import torch

from dotenv import load_dotenv
import os

load_dotenv()

os.environ[&quot;PYTORCH_MPS_HIGH_WATERMARK_RATIO&quot;] = &quot;0.0&quot;

print(&quot;Starting Process&quot;)

pipe = StableDiffusion3Pipeline.from_pretrained(
        &quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;, 
        low_cpu_mem_usage=False,
        torch_dtype=torch.float16,
        variant=&quot;fp16&quot;,
        use_safttensors=True
        ).to(&quot;mps&quot;)


pipe.enable_attention_slicing()

print(&quot;Starting Process&quot;)

steps = 40
query = &quot;Rain Weather in New York City, New York&quot;

image = pipe(query, num_inference_steps=steps).images[0]

image.save(&quot;oneOffImage.jpg&quot;)

print(&quot;Successfully Created Image as oneOffImage.jpg&quot;)
</code></pre>
","0","Answer"
"78655543","78437477","<p>I just tried a test case with Claude 3.5 (sonnet) and it isn't too bad, although it does place some polytomies in places where it's visually clear that they should be bifurcations, for example, for the tree below</p>
<p><a href=""https://i.sstatic.net/J9VgCH2Cm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/J9VgCH2Cm.jpg"" alt=""Phylogenetic tree of trichoplax"" /></a></p>
<p>With the prompt &quot;Please can you extract a newick-format string from the attached image of a phylogenetic tree&quot;, I get</p>
<pre><code>(Capsaspora,((Monosiga,Salpingoeca),(Amphimedon,(((Acropora,Nematostella),(Clytia,Hydra)),(Drosophila,(Ciona,(Branchiostoma,Capitella))),(Polyplacotoma_mediterranea_H0,((Trichoplax_H17,(Trichoplax_adhaerens_H1,Trichoplax_H2)100)100,(Cladtertia_collaboinventa_H23,(H19,H24)100,(Hoilungia_hongkongensis_H13,Hoilungia_H4)100,(Hoilungia_H15,Hoilungia_H25)100)100)100)100)30)100)100);
</code></pre>
<p>Which corresponds to</p>
<p><a href=""https://i.sstatic.net/xf5gUciIm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xf5gUciIm.png"" alt=""AI generated phylogenetic tree"" /></a></p>
","0","Answer"
"78660555","78657727","<p>I thought that i need to mask the images previously but has many errors.
i know my problem by help of dears that i need to a CNN model and machine learning. thanks them. i assume that the shape in my project is a circle instead of conjuctival pulpabral. so base of my codes is like this even has many distance from it.</p>
<pre><code>import os
from mrcnn.config import Config
from mrcnn.model import MaskRCNNModel
# prepare training dataset take circle instead of con_pal
dataset_dir = 'path/to/dataset'
circle_dir = os.path.join(dataset_dir, 'circle')
no_circle_dir = os.path.join(dataset_dir, 'no_circle')

#prepare the training enviroment
class CircleConfig(Config):
    NAME = &quot;circle&quot;
    NUM_CLASSES = 2  # Background + circle
    IMAGE_MIN_DIM = 512
    IMAGE_MAX_DIM = 512
    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)
    TRAIN_ROIS_PER_IMAGE = 32
    STEPS_PER_EPOCH = 100
    VALIDATION_STEPS = 50
    DETECTION_MIN_CONFIDENCE = 0.7

#train the MASK RCNN model
model = MaskRCNNModel(mode=&quot;training&quot;, config=CircleConfig(), model_dir=&quot;./&quot;)
model.load_dataset(dataset_dir)
model.train()

#save the trained wieghts
model.save_weights(&quot;mask_rcnn_circle.h5&quot;)
</code></pre>
","0","Answer"
"78660744","78645720","<p>Looks like the problem is in this line</p>
<pre><code>y_pred_prob = torch.softmax(y_pred, dim=1).argmax(dim=1)
</code></pre>
<p>It should be without the argmax function. Argmax returns the indices of the maximum value of all elements in the input tensor whereas softmax provides the probability. Try it without argmax as following:</p>
<pre><code>y_pred_prob = torch.softmax(y_pred, dim=1)
</code></pre>
<p>For accuracy you can add the argmax function</p>
<pre><code>train_acc += accuracy_fn(y_true=y,
                             y_pred=y_pred_prob.argmax(dim=1))
</code></pre>
<p>You might have to change the loss function to accommodate this change.</p>
","0","Answer"
"78661866","78649611","<p>you can't use predict from cli to save bounding boxes because yolo segmentation save_txt argument only save the x and y for points of each segment. you should a separate write a python script for this job.</p>
","0","Answer"
"78666959","78666757","<p>Currently, TensorFlow Addons does not support Python 3.12, which is why you're unable to find a compatible version for installation.</p>
<p>Using Python 3.11 with a virtual environment is the simplest and most reliable solution.</p>
","1","Answer"
"78667095","78667039","<p>nltk library, TF-IDF vectorizer etc. are statistical models and have limited capabilities. You are looking for something that can analyses a text and output a list of relevant tags. This task is more suited for LLMs.</p>
<p>Unfortunately, because page content can be quite long, you will need an LLM with a large context window, hence an heavy one. Make sure that GPT-2 is suited for this task. Could you provide further information as to why its output is not what you are expecting? You could try to change your prompt, or add more context to it.</p>
","0","Answer"
"78667194","78666961","<p>Managed to find a workaround.</p>
<pre class=""lang-py prettyprint-override""><code># helper functions
def split_dataset(X, y, n_splits):
    # data splits
    split_X = np.array_split(X, n_splits)
    split_y = np.array_split(y, n_splits)
    return split_X, split_y

def compute_meta_features(X, y):
    # meta-features for a partition
    extractor = MFE(features=[&quot;t1&quot;], groups=[&quot;complexity&quot;], 
        summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
    extractor.fit(X, y)
    return extractor.extract()

def average_results(results):
    # summary of results
    features = results[0][0]
    summary_values = np.mean([result[1] for result in results], axis=0)
    return features, summary_values

# Split dataset
n_splits = 10  # ten splits
split_X, split_y = split_dataset(X, y, n_splits)

#  meta-features 
results = [compute_meta_features(X_part, y_part) for X_part, y_part in zip(split_X, split_y)]

# Combined results
final_features, final_summary = average_results(results)

</code></pre>
","0","Answer"
"78667269","78666961","<p><code>StratifiedKFold</code> in the <code>sklearn.model_selection</code> is ideal for your case to create multiple balanced partitions while preserving the class distribution in each split.</p>
<p>Here is the code</p>
<pre><code>import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedKFold
from pymfe.mfe import MFE

# Generate the large dataset
X, y = make_classification(n_samples=20000, n_features=80, n_informative=60, n_classes=5, random_state=42)

# Function to process each partition and extract features
def process_partition(X, y):
    extractor = MFE(features=[&quot;t1&quot;], groups=[&quot;complexity&quot;], summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
    extractor.fit(X, y)
    return extractor.extract()

# Function to combine results from all partitions
def combine_results(partition_results):
    combined = {}
    for features, values in partition_results:
        for feature, value in zip(features, values):
            combined.setdefault(feature, []).append(value)
    return {feature: np.mean(values) for feature, values in combined.items()}

# Create stratified partitions using StratifiedKFold
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
partition_results = [process_partition(X[train_idx], y[train_idx]) for train_idx, _ in skf.split(X, y)]

# Combine the results from all partitions
final_result = combine_results(partition_results)
print(final_result)
</code></pre>
","1","Answer"
"78667545","78626157","<p>When visualizing your image, just add the following:</p>
<pre><code>instance_mode=ColorMode.IMAGE_BW 
</code></pre>
<p>Here is my full visualization block, in case it helps:</p>
<pre><code>output_dir = &quot;output_images&quot;
os.makedirs(output_dir, exist_ok=True)

for i, d in enumerate(random.sample(dataset_dicts, 5)):    
    im = cv2.imread(d[&quot;file_name&quot;])
    outputs = mask_rcnn_predictor(im)
    v = Visualizer(im[:, :, ::-1],
                   metadata=val_metadata, 
                   scale=1.5, 
                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
    )
    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))
    cv2.imshow(&quot;Window&quot;,v.get_image()[:, :, ::-1])

    # Save the image
    cv2.imwrite(os.path.join(output_dir, f'image_{i}.jpg'), v.get_image()[:, :, ::-1])

    key = cv2.waitKey(0)
    if key == 27:  # if ESC is pressed, exit loop
        cv2.destroyAllWindows()
        break
</code></pre>
","0","Answer"
"78668486","78666998","<p>Solved using the solution in this GitHub issue for keras_cv (adapting input size to my own).
<a href=""https://github.com/keras-team/keras-cv/issues/2455"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras-cv/issues/2455</a></p>
","0","Answer"
"78669692","78664794","<p>The issue can be related to the learning rate scheduler, the ´<strong>lr_scheduler_type</strong>´ is set to ´<strong>linear</strong>´ (wich depends on the total number of epochs), causing variations in the results.</p>
<p>You need to ensure that the scheduler's behavior is consistent. Since I'm not familiar with the Sentence Transformers library, I'm not sure how you should modify your code, but I hope my logic is correct.</p>
","0","Answer"
"78669721","78606543","<p>I have a similar problem. I am stuck at creating the Custom Dataset. I have labeled 6 delivery receipts with 3 labels in Label Studio (only testing purposes to set up the pipeline). Exported the JSON from it, created programmatically <em>one</em> .JSON file <em>for each data entry</em> from the export. Now I followed all the way up until the end of this <a href=""https://www.youtube.com/live/KHQDnWGpcLE?feature=shared"" rel=""nofollow noreferrer"">youtube video</a>, but unfortunately I am also stuck at the &quot;out of index&quot; problem :/
If I print my datasets then they are not empty, so my problem is to understand where the out of index error actually comes from!</p>
<p>This is the stack trace after running trainer.train()</p>
<pre><code>Currently training with a batch size of: 1
The following columns in the training set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: image_path, ground_truth_string, annotation_path. If image_path, ground_truth_string, annotation_path are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 5
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed &amp; accumulation) = 1
  Gradient Accumulation steps = 1
  Total optimization steps = 5
  Number of trainable parameters = 201,121,912
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-160-3435b262f1ae&gt; in &lt;cell line: 1&gt;()
----&gt; 1 trainer.train()

13 frames
/usr/local/lib/python3.10/dist-packages/datasets/table.py in &lt;listcomp&gt;(.0)
    122         return pa.Table.from_batches(
    123             [
--&gt; 124                 self._batches[batch_idx].slice(i - self._offsets[batch_idx], 1)
    125                 for batch_idx, i in zip(batch_indices, indices)
    126             ],

IndexError: list index out of range
</code></pre>
<p>Here is my code for the dataset, adapted to CORD-V2:</p>
<pre><code>import json

KEY_START = &quot;&lt;key_start&gt;&quot;
KEY_END = &quot;&lt;key_end&gt;&quot;
VALUE_START = &quot;&lt;value_start&gt;&quot;
VALUE_END = &quot;&lt;value_end&gt;&quot;

def parse_json(batch):

    batch_data = []
    image_path = []

    for filename in batch[&quot;annotation_path&quot;]:
        with open(filename) as fp:
            data = json.load(fp)
        
        x_values, y_values, width_values, height_values, labels, texts = [], [], [], [], [], []

        # Extract the values from the JSON structure
        for annotation in data['annotations']:
            for result in annotation['result']:
                value = result['value']
                x_values.append(value['x'])
                y_values.append(value['y'])
                width_values.append(value['width'])
                height_values.append(value['height'])
                labels.append(value['rectanglelabels'][0])  # Assuming each label list contains one item
                texts.append(value.get('text', ''))  # Extract the associated text if available

        x_values = list(map(str, x_values))
        y_values = list(map(str, y_values))
        width_values = list(map(str, width_values))
        height_values = list(map(str, height_values))
        labels = list(map(str, labels))
        texts = list(map(str, texts))

        ground_truth_string = &quot;&quot;
        for label, text in zip(labels, texts):
            ground_truth_string += f&quot;{KEY_START}{label}{KEY_END}{VALUE_START}{text}{VALUE_END}&quot;

        batch_data.append(ground_truth_string)
        print(f&quot;batch_data: {batch_data}&quot;)
        file_id = Path(filename).stem
        print(Path(filename))
        print(file_id)
        image_path.append(str(images_dir / f&quot;{file_id}.png&quot;))
        print(f&quot;image_path len: {len(image_path)}&quot;)

    return {
        &quot;ground_truth_string&quot;:batch_data,
        &quot;image_path&quot;:image_path}
</code></pre>
","0","Answer"
"78671599","78665213","<p>Now I can fix this problem, which comes from the date being formatted incorrectly. I change my code as below</p>
<pre><code>Tencent2 = Tencent[['Date','Close']]
Tencent2['Date'] = pd.to_datetime(Tencent['Date'], infer_datetime_format=True, errors='coerce')
Tencent2['Close'] = Tencent2['Close'].astype(float)
#Resetting the index
Tencent2 = Tencent2.set_index('Date')    #Setting the Date as Index
Tencent2.sort_index(inplace=True)


ax = Tencent2.plot(figsize=(15, 9))
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))
ax.xaxis.labelpad = 10
ax.tick_params(axis='x',labelrotation=40)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Close', fontsize=12)
plt.title(&quot;Closing price distribution of Tencent&quot;, fontsize=15)
plt.plot(Tencent2, color='green', linewidth = 2)
plt.show()
</code></pre>
","1","Answer"
"78672844","78672059","<p>Optuna says it requires an Optuna Distribution object for each hyperparameter's &quot;range&quot;.  But there is <code>CategoricalDistribution</code> for providing a discrete list of options.  There is the Note in the documentation though which might indicate an issue in some settings:</p>
<blockquote>
<p>Not all types are guaranteed to be compatible with all storages. It is recommended to restrict the types of the choices to <code>None</code>, <code>bool</code>, <code>int</code>, <code>float</code> and <code>str</code>.</p>
</blockquote>
<hr />
<p>I first interpreted your question to be about setting the hyperparameters of steps in a pipeline, so the convention like <code>preprocessor__StandardScaler</code>.  I no longer thing that's what you meant, but in case others come here with that confusion:</p>
<p>In the documentation of <code>OptunaSearchCV</code>, the <code>estimator</code> parameter is explained as &quot;Object to use to fit the data. This is assumed to implement the scikit-learn estimator interface. [...]&quot;, and I would assume that means they use <code>clone</code> and <code>set_params</code> to create an estimator for each hyperparameter setting; then it should be fine to use a pipeline with the usual naming convention for steps' hyperparameters.</p>
","0","Answer"
"78672916","78671295","<p>It looks to me like you've got everything right up to defining the dataframe of hyperparameters and their fold-scores.</p>
<p>But the &quot;best&quot; set of hyperparameters is the one that minimizes the MSE <strong>averaged</strong> over folds (not minimized over folds).</p>
","0","Answer"
"78673244","78668432","<p>For each row of the df you're pulling the data out of polars and into python objects and then you're reinitiating a polars df. That is very expensive both in time and memory. Instead you should keep everything in polars memory unless for some reason you absolutely can't. You mention that most of the time is taken in steps 1 and step 2 but step 2 includes your ML stuff so that might still be the bottleneck.</p>
<p>Using <code>map_elements</code> here isn't ideal because you're not returning anything back to the df, just use a regular for loop. Also I combined your two functions into this one:</p>
<pre><code>def build_and_process_candidate_output(i, df_items_sm_ex, df_products, ml, ml_comp):
    output_df = (
        df_items_sm_ex[i]
        .explode(&quot;SEARCH_SIMILARITY_SCORE&quot;, &quot;SEARCH_POSITION&quot;)
        .rename(
            {
                &quot;PRODUCT_INFO&quot;: &quot;QUERY&quot;,
                &quot;SEARCH_SIMILARITY_SCORE&quot;: &quot;SIMILARITY_SCORE&quot;,
                &quot;SEARCH_POSITION&quot;: &quot;POSITION&quot;,
                &quot;SKU&quot;: &quot;QUERY_SKU&quot;,
                &quot;YELLOW_CAT&quot;: &quot;QUERY_LEAF&quot;,
                &quot;CATL3&quot;: &quot;QUERY_CAT&quot;,
                &quot;EMBEDDINGS&quot;: &quot;QUERY_EMBEDDINGS&quot;,
            }
        )
        .join(
            df_products.select(
                &quot;SKU&quot;, &quot;EMBEDDINGS&quot;, &quot;INDEX&quot;, &quot;DESCRIPTION&quot;, &quot;CATL3&quot;, &quot;YELLOW_CAT&quot;
            ),
            left_on=[&quot;POSITION&quot;],
            right_on=[&quot;INDEX&quot;],
            how=&quot;left&quot;,
        )
        .rename(
            {
                &quot;DESCRIPTION&quot;: &quot;SIMILAR_PRODUCT_INFO&quot;,
                &quot;CATL3&quot;: &quot;SIMILAR_PRODUCT_CAT&quot;,
                &quot;YELLOW_CAT&quot;: &quot;SIMILAR_PRODUCT_LEAF&quot;,
            }
        )
        .select(
            &quot;QUERY&quot;,
            &quot;QUERY_SKU&quot;,
            &quot;QUERY_CAT&quot;,
            &quot;QUERY_LEAF&quot;,
            &quot;SIMILAR_PRODUCT_INFO&quot;,
            &quot;SIMILAR_PRODUCT_CAT&quot;,
            &quot;SIMILAR_PRODUCT_LEAF&quot;,
            &quot;SIMILARITY_SCORE&quot;,
            
            # This assumes these are the same length
            (pl.col(&quot;QUERY_EMBEDDINGS&quot;).explode() + pl.col(&quot;EMBEDDINGS&quot;).explode())
            .implode()
            .over(&quot;POSITION&quot;)
            .alias(&quot;COMBINED_EMBEDDINGS&quot;),

            &quot;SKU&quot;,
            &quot;POSITION&quot;,
        )
         # Given the sample data, this filter makes everything go away
         # which is why supplying good sample data is important
        .filter(pl.col(&quot;SKU&quot;) != pl.col(&quot;QUERY_SKU&quot;).first())
    )

    # ML predictions
    output_df = predict_complements(output_df, ml)
    output_df = output_df.filter(pl.col(&quot;COMPLEMENTARY_PREDICTIONS&quot;) == 1)
    # Other ML predictions
    output_df = predict_required_accessories(output_df, ml_comp)
    output_df = output_df.sort(by=&quot;LABEL_PROBABILITY&quot;, descending=True)
    return output_df
</code></pre>
","1","Answer"
"78675068","78606543","<p><em>I found a solution, you should create a label map for the data you want to extract, then Scale Bounding Boxes after that, and Detect Currency in Text because the problem is the dataset has a lot of different currencies and languages also so this is what I did</em></p>
<p>label_map = {
&quot;total.total_price&quot;: 1,
&quot;other&quot;: 0
}</p>
<pre><code>def scale_bbox(box, original_size, target_size=(1000, 1000)):
    x_scale = target_size[0] / original_size[0]
    y_scale = target_size[1] / original_size[1]
    return [int(box[0] * x_scale), int(box[1] * y_scale), int(box[2] * x_scale), int(box[3] * y_scale)]

def detect_currency(text):
    currency_symbols = {
        '$': 'USD',
        '€': 'EUR',
        '£': 'GBP',
        '¥': 'JPY',
        '₹': 'INR',
        '₩': 'KRW',
    }
    for symbol, currency in currency_symbols.items():
        if symbol in text:
            return currency
    return None

def preprocess_data(examples):
    images = []
    words = []
    boxes = []
    labels = []
    original_size = (224, 224)
    currency_converter = CurrencyRates()

    for image, gt in zip(examples['image'], examples['ground_truth']):
        img = image.convert(&quot;RGB&quot;).resize(original_size)
        images.append(img)
        gt = json.loads(gt)
        batch_words = []
        batch_boxes = []
        batch_labels = []

        for item in gt['valid_line']:
            for w in item['words']:
                text = w['text']
                quad = w['quad']
                bbox = scale_bbox([quad['x1'], quad['y1'], quad['x3'], quad['y3']], original_size)
                bbox = [min(max(0, coord), 1000) for coord in bbox]
                batch_words.append(text)
                batch_boxes.append(bbox)
                if item['category'] == 'total.total_price':
                    try:
                        total_amount_match = re.findall(r&quot;\d+\.\d{2}&quot;, text)
                        if total_amount_match:
                            total_amount = float(total_amount_match[0])
                            detected_currency = detect_currency(text)
                            if detected_currency and detected_currency != 'USD':
                                total_amount = currency_converter.convert(detected_currency, 'USD', total_amount)
                            text = f&quot;{total_amount:.2f} USD&quot;
                    except Exception as e:
                        print(f&quot;Error processing text: {e}&quot;)
                    batch_labels.append(label_map[&quot;total.total_price&quot;])
                else:
                    batch_labels.append(label_map[&quot;other&quot;])

        words.append(batch_words)
        boxes.append(batch_boxes)
        labels.append(batch_labels)

    encoding = processor(images, words, boxes=boxes, word_labels=labels, truncation=True, padding=&quot;max_length&quot;, max_length=512, return_tensors=&quot;pt&quot;)
    return encoding`enter code here`
</code></pre>
","0","Answer"
"78676087","78671295","<p>@Ben Reiniger 's post solved the issue. However, one more slight change to the code in the original code had to be made: <code>alpha_path.flatten(&quot;C&quot;)</code> instead of <code>alpha_path.flatten()</code> to get the alphas in the correct order.</p>
<p>So the following code should do it:</p>
<pre><code>import numpy as np
from sklearn.linear_model import LinearRegression

#Sample data:
num_samples = 100  # Number of samples
num_features = 1000  # Number of features 
X = np.random.rand(num_samples, num_features)
Y = np.random.rand(num_samples)

#Model
l1_ratios = np.arange(0.1, 1.1, 0.1)
tscv=TimeSeriesSplit(max_train_size=None, n_splits=3)
regr = ElasticNetCV(cv=tscv.split(X), random_state=42,l1_ratio=l1_ratios)
regr.fit(X,Y)

se = regr.mse_path_
regr.alphas_
regr.alpha_
regr.l1_ratio
regr.l1_ratio_

mse_path =  regr.mse_path_
alpha_path = regr.alphas_

# Reshape mse_path to have l1_ratios, n_alphas, cross_validation_step as separate columns
mse_values = mse_path.flatten()
alpha_values = alpha_path.flatten(&quot;C&quot;)
l1_values=np.tile(l1_ratios ,int(alpha_values.shape[0]/l1_ratios.shape[0]))
repeated_l1_ratios = np.repeat(l1_ratios, 100)

# mse has dimensions (11, 100, 3)
array_3d = mse

# Flatten the 3D array into a 2D array
# Each sub-array of shape (100, 3) becomes a row in the new 2D array
array_2d = array_3d.reshape(-1, 3)

# Create a DataFrame from the 2D array
df = pd.DataFrame(array_2d, columns=['MSE Split1', 'MSE Split2', 'MSE Split3'])

df['alpha_values'] = alpha_values
df['l1_values'] = repeated_l1_ratios


# Calculate the minimum MSE for each row across the three splits
df['Min MSE'] = df[['MSE Split1', 'MSE Split2', 'MSE Split3']].mean(axis=1)

# Identify the row with the overall minimum MSE
min_mse_row_index = df['Min MSE'].idxmin()

# Retrieve the row with the minimum MSE
min_mse_row = df.loc[min_mse_row_index]

print(&quot;Row with the minimum MSE across all splits:&quot;)
print(min_mse_row)
</code></pre>
","1","Answer"
"78678329","78677115","<blockquote>
<p>The objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion.</p>
</blockquote>
<p>This goal is not feasible using JAX. You could register your class as a custom Pytree to make it work with JAX functions (see <a href=""https://jax.readthedocs.io/en/latest/pytrees.html#extending-pytrees"" rel=""nofollow noreferrer"">Extending pytrees</a>), but this won't mean you can vectorize an operation over a list of such objects.</p>
<p>JAX transformations like <code>vmap</code> and <code>jit</code> work for data stored with a struct-of-arrays pattern (e.g. a single <code>LinkedList</code> object containing arrays that represent multiple batched linked lists) not an array-of-structs pattern (e.g. a list of multiple <code>LinkedList</code> objects).</p>
<p>Further, the algorithm you're using, based on a <code>while</code> loop, is not compatible with JAX transformations (See <a href=""https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow"" rel=""nofollow noreferrer"">JAX sharp bits: control flow</a>), and the dynamically sized tree of nodes will not fit into the static shape constraints of JAX programs.</p>
<p>I'd love to point you in the right direction, but I think you either need to give up on using JAX, or give up on using dynamic linked lists. You won't be able to do both.</p>
","1","Answer"
"78679419","78678402","<p>Removing the quotes around oml.Datetime will resolve the problem:</p>
<pre><code>df_datetime = df.select_types(include=[oml.Datetime])
</code></pre>
","0","Answer"
"78684269","78684235","<p>I think you would convert it into a pandas dataframe and work with the dataframe? Something like</p>
<pre><code>import pandas as pd

kobj = q('{select name,price,volume,vwap from tab where date&gt;2024.01.01}')
mydf = pd.DataFrame(dict(kobj.flip))
</code></pre>
","-1","Answer"
"78684529","78669554","<p>Images and masks consist of 3 classes. In the above code, plotting of predicted mask was done using greyscale colormap (2-valued). Since, these masks are also 3-valued, we need colored settings. Simply replace this line of code</p>
<pre><code>plt.imshow(predictions[i][:, :, 0], cmap='gray')
</code></pre>
<p>with this</p>
<pre><code>plt.imshow(predicted_masks[i], cmap='viridis', vmin=0, vmax=2)
</code></pre>
<p>here is the complete code:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np

def plot_predictions(model, images, masks, num_samples=5):
    predictions = model.predict(images[:num_samples])
    predicted_masks = np.argmax(predictions, axis=-1)

    for i in range(num_samples):
        plt.figure(figsize=(15, 5))

        # Real Image
        plt.subplot(1, 3, 1)
        plt.title('Real Image')
        plt.imshow(images[i])

        # Ground Truth Mask
        plt.subplot(1, 3, 2)
        plt.title('Ground Truth Mask')
        plt.imshow(masks[i], cmap='viridis', vmin=0, vmax=2)

        # Predicted Mask
        plt.subplot(1, 3, 3)
        plt.title('Predicted Mask')
        plt.imshow(predicted_masks[i], cmap='viridis', vmin=0, vmax=2)

        plt.show()

plot_predictions(model, test_images.numpy(), test_masks_cat, num_samples=5)
</code></pre>
","2","Answer"
"78684562","78684235","<p>With PyKX you should be able to convert to a pandas dataframe using the <code>.pd</code> method off the PyKX objects (tables in this case), for your example this would be</p>
<pre class=""lang-py prettyprint-override""><code>q(‘select name,price,volume,vwap from tab where date&gt;2024.01.01’).pd()
</code></pre>
<p>Assuming in the above that the table <code>tab</code> is accessible from q memory and/or you are using a remote q process containing your data</p>
","2","Answer"
"78687119","78663805","<p>You are using a library-specific <code>autolog</code> for <code>tensorflow</code>/<code>keras</code>. So, according to <a href=""https://mlflow.org/docs/latest/tracking/autolog.html#id12"" rel=""nofollow noreferrer"">this part of the mlflow documentation</a>:</p>
<blockquote>
<p>MLflow will then <strong>automatically end the run</strong> once training ends via
calls to <code>tf.keras.fit()</code>.</p>
</blockquote>
<p>In this regard, when you call the manual log <code>mlflow.log_figure</code> and mix <code>Manual Logging</code> and <code>Auto Logging</code> tracking APIs, a new run automatically starts and the last run will end, and that is why you only see the figure is logged. You may probably find the artifact and metrics in a previous run.</p>
","1","Answer"
"78687282","78686328","<p>You're incorrectly unpacking the results in the loop. With <code>average=None</code>, your <code>recall</code> is a pair:</p>
<ol start=""0"">
<li><p>the first one is recall of the &quot;negative class&quot; of this loop-iteration's one-vs-rest metric; this doesn't correspond to any single class metric.</p>
</li>
<li><p>The second one is recall of the positive class, which really is the recall for this class in the multiclass sense.</p>
</li>
</ol>
<p>But then you've labeled them wrong in your dataframe: what you called specificity is really the sensitivity/recall (and indeed these match the classification report), and what you called sensitivity isn't any single-class metric.</p>
","1","Answer"
"78690519","78677544","<p>You can use <a href=""https://pub.dev/packages/tflite_flutter"" rel=""nofollow noreferrer"">flutter_tflite</a> package to infer tflite models in flutter. Since you are using this for object detection, You need to figure out more than that.</p>
<ol>
<li>Load the model and the labels. This can be done with the flutter_tflite</li>
<li>get the image and prepare the image for your model (crop the image for the model input size and make a byte array as required by the model)</li>
<li>run the model with the image.</li>
<li>Clean the outputs (if it need nms) and get the bounding boxes.</li>
</ol>
<p>There are some examples in the flutter_tflite package. But it all depends on your model type.</p>
","0","Answer"
"78691364","78679016","<p>I've found the solution in the microsoft documentation, posting the answer here in case anyone else is struggling to find the solution. Turns out this is quite straightforward to achieve using the python sdk v2, it can be done in a single line of code, see an extract from the docs <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2#optional-register-components-to-workspace"" rel=""nofollow noreferrer"">here</a> below:</p>
<pre><code>comp = ml_client.components.get(name=&quot;&lt;component_name&gt;&quot;, version=&quot;&lt;component_version&gt;&quot;)
</code></pre>
","1","Answer"
"78691726","78691574","<p>The build and installation steps for darknet are very well documented.  Make sure you are using the most recent version of Darknet/YOLO:  <a href=""https://github.com/hank-ai/darknet#table-of-contents"" rel=""nofollow noreferrer"">https://github.com/hank-ai/darknet#table-of-contents</a></p>
<p>If you find the steps difficult or lacking in any way, please let me know on the Darknet/YOLO discord server so I can fix or better document whatever you didn't understand:  <a href=""https://discord.gg/zSq8rtW"" rel=""nofollow noreferrer"">https://discord.gg/zSq8rtW</a></p>
<p>Note the recommended training command on the repo and in the Darknet/YOLO FAQ (<a href=""https://www.ccoderun.ca/programming/yolo_faq/#training_command"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#training_command</a>) is not exactly the command you state.</p>
<ol>
<li>You left out the very critical parameter <code>-map</code></li>
<li>You wouldn't normally specify the weights file <code>yolo-obj_2000.weights</code> unless you are resuming training from the 2000th iteration.</li>
</ol>
<p>Make sure you understand the license difference between v8-9-10, and the completely free and open-source license of Darknet/YOLO.</p>
<p>Lastly, you may want to see this recent video which compares some of the more recent versions of YOLO against the original Darknet/YOLO:  <a href=""https://www.youtube.com/watch?v=2Mq23LFv1aM"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=2Mq23LFv1aM</a></p>
<p>Disclaimer:  I maintain the Darknet/YOLO codebase.  I'm the author of tools such as DarkHelp and DarkMark.</p>
","-1","Answer"
"78694947","78691616","<p>As explained in the <a href=""https://huggingface.co/docs/timm/v1.0.7/en/feature_extraction#feature-extraction"" rel=""nofollow noreferrer"">timm documentation</a>, you can get the final hidden state of the model with the <code>forward_features</code> method.</p>
<pre class=""lang-py prettyprint-override""><code>model = timm.create_model(...)
x = ...
features = model.forward_features(x)
</code></pre>
","1","Answer"
"78696769","78696449","<p>There a 2 points you should consider.</p>
<ol>
<li><strong>Arrange rewards and penalties.</strong> When you are creating your policy, don't just give rewards to shooting asteroids, experiment with different states. Give high penalties for shooting when there isn't an asteroid on the same x axis, and give a high reward to shooting where there is an asteroid on that axis. Give penalties for present asteroids, and give rewards when you get closer to an asteroid on x-axis, etc. These are examples ofcourse, you need to emperiment and think on your problem to come up with smart rewards/penalties.</li>
<li><strong>Exploration vs Exploitation</strong> The action that has the biggest reward on the current state may not be the best action in the long-term. You need to experiment with your epsilon value to push the model into exploration.</li>
</ol>
","-1","Answer"
"78698753","78698399","<ol>
<li>Using &quot;random_state&quot; means something random is happening. Unlike your custom implementation, sklearn shuffles your training data by default.</li>
<li>Convergence: You set &quot;max_iter&quot; to 1000 so the weights get adjusted 1000 times. But your custom implementation will stop adjusting the weights as soon as there are no misclassifications.</li>
<li>Your actual data. Does it really have a linear relationship? This would matter as well.</li>
<li>Did you use exactly the same train and test data ?</li>
</ol>
","0","Answer"
"78699010","78688976","<p>Yes, you've chosen the right approach.  My advice is to use Elasticsearch (built-in) capabilities to store huge data as well as to extremely fast compare long vectors.
I'm normally using <a href=""https://github.com/deepinsight/insightface"" rel=""nofollow noreferrer"">Insightface</a> for all face recognition tasks, so Elasticsearch docs have the following mapping:</p>
<pre><code>mapping = {
    &quot;properties&quot;: {
        &quot;face_id&quot;: {
            &quot;type&quot;: &quot;keyword&quot;
            },
        &quot;face_vector&quot;:{
            &quot;type&quot;: &quot;dense_vector&quot;,
            &quot;dims&quot;: 512,
            &quot;index&quot;: &quot;true&quot;,
            &quot;similarity&quot;: &quot;l2_norm&quot;
            },
        &quot;pic_file_path&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;index&quot;: &quot;false&quot;
            },
        &quot;face_location&quot;: {
            &quot;type&quot;: &quot;integer&quot;,
            &quot;index&quot;: &quot;false&quot;
            },
        &quot;kps&quot;: {
            &quot;type&quot;: &quot;integer&quot;,
            &quot;index&quot;: &quot;false&quot;
            },
        &quot;gender&quot;: {
            &quot;type&quot;: &quot;byte&quot;
            },
        &quot;description&quot;: {
            &quot;type&quot;: &quot;text&quot;
            },
        &quot;descr_length&quot;: {
            &quot;type&quot;: &quot;integer&quot;
            }
        }
    }

}
</code></pre>
<p>The mapping will also allow to use extra fast KNN-search among millions of the students faces :)</p>
","0","Answer"
"78699239","78694076","<p>After some testing, it seems it is possible to run a Core ML model on macOS 10.12. I am not aware of any workaround.</p>
","0","Answer"
"78700276","78700237","<p>You said that <em>sometimes</em> the code doesn't work, but not always. The most obvious reason is that <code>cross_join_df</code> somehow doesn't have the key <code>'cos_sim'</code> <em>and</em> does not allow a new key as a mean to create a new entry. I'm not so sure what kind of object <code>cross_join_df</code> is, but generically you can use the following function to determine if there's a <code>cos_sim</code> entry:</p>
<pre><code>def check_have_entry(object,key):
    if issubclass(object, dict):
        return key in object.keys()  # .keys() not really needed here, just for clarity
    elif isinstance(object, pandas.DataFrame):
        return key in object.index
    return False
</code></pre>
<p>However, if your dataframe is some other object, the above function will not work.</p>
","0","Answer"
"78703324","78698316","<p>There is no ideal option, each choice has its pros and cons. Therefore, I recommend trying all the valid options and compare the performance. You can start with these options:</p>
<ol>
<li>You can resize images to a slightly larger size e.g., 520x520.</li>
<li>You can randomly crop images to the desired fixed size e.g., 512x512.</li>
<li>You can use both options by starting with resizing and then cropping.</li>
</ol>
<p>There is an advanced option that you can try which is computing the widths and heights of all images and then calculate the average. Then, use the average width and height to resize all images to.</p>
","0","Answer"
"78706831","78704861","<p>The linear layer implements an <a href=""https://en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix"" rel=""nofollow noreferrer"">affine transformation</a>, that is a matrix multiplication and translation by a bias vector, which is applied to all matrices in any tensor viewed as some layout of matrices.</p>
<hr />
<p>It is better to consider your second example for demonstration.
Your (slightly modified) second example:</p>
<pre><code>inp = torch.rand(1,2,3,4) # B,C,W,H
print(f'inp ({inp.shape}):\n{inp}')
linear = nn.Linear(4,5)
print(f'weight ({linear.weight.shape}):\n{linear.weight}')
print(f'bias {linear.bias.shape}:\n{linear.bias}')
out = linear(inp)
print(f'out ({out.shape}):\n{out}')
</code></pre>
<p>prints (might be different for you due to randomness)</p>
<pre><code>inp (torch.Size([1, 2, 3, 4])):
tensor([[[[0.3340, 0.6843, 0.6702, 0.9667],
          [0.9990, 0.3094, 0.7772, 0.7851],
          [0.3004, 0.6993, 0.3088, 0.5238]],

         [[0.5257, 0.9793, 0.2408, 0.4065],
          [0.7183, 0.8921, 0.8280, 0.1272],
          [0.7826, 0.2930, 0.1266, 0.8724]]]])
weight (torch.Size([5, 4])):
Parameter containing:
tensor([[ 0.2177, -0.0575, -0.4756, -0.1297],
        [ 0.3632, -0.2986, -0.0157, -0.2817],
        [ 0.4323,  0.3205,  0.2895, -0.1527],
        [ 0.2368,  0.4018, -0.2126,  0.4732],
        [-0.0158, -0.4908,  0.3854, -0.4685]], requires_grad=True)
bias torch.Size([5]):
Parameter containing:
tensor([-0.0749, -0.1002, -0.3814,  0.2213, -0.4468], requires_grad=True)
out (torch.Size([1, 2, 3, 5])):
tensor([[[[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
          [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
          [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]],

         [[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
          [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
          [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]]]],
       grad_fn=&lt;ViewBackward0&gt;)
</code></pre>
<p>So here <code>inp</code> contains two 3x4 matrices embedded into a 4-dimensional tensor according to the first two dimensions 1x2 layout. The linear layer multiplies all matrices by the weights, adds the bias vector to all of them, and finally stacks the resulted two 3x5 matrices according to the 1x2 layout. You can perform the <code>linear(inp)</code> operation equivalently as</p>
<pre><code>torch.tensordot(inp, linear.weight.T, dims=1) + linear.bias
</code></pre>
<p>which will print the same result as <code>out</code>.
The <code>dim=1</code> argument of <a href=""https://pytorch.org/docs/stable/generated/torch.tensordot.html"" rel=""nofollow noreferrer""><code>torch.tensordot</code></a> specifies how many dimensions the operation &quot;consumes&quot; which is 1 for the matrix product.</p>
<hr />
<p>To make it even clearer, use can use only matrix products by <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html"" rel=""nofollow noreferrer""><code>torch.matmul</code></a> and perform the unstacking and stacking manually:</p>
<pre><code>mat0 = torch.matmul(inp[0][0], linear.weight.T) + linear.bias
mat1 = torch.matmul(inp[0][1], linear.weight.T) + linear.bias
print(f'mat0 ({mat0.shape}):\n{mat0}')
print(f'mat1 ({mat1.shape}):\n{mat1}')
# =&gt;
# mat0 (torch.Size([3, 5])):
# tensor([[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
#         [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
#         [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]], grad_fn=# &lt;AddBackward0&gt;)
# mat1 (torch.Size([3, 5])):
# tensor([[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
#         [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
#         [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]], grad_fn=# &lt;AddBackward0&gt;)
</code></pre>
<p>which can be stacked by <a href=""https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack"" rel=""nofollow noreferrer""><code>torch.stack</code></a> into a 2x3x5 tensor or a 1x2x3x5 tensor (as above):</p>
<pre><code>t1 = torch.stack([mat0, mat1])
print(f't1 ({t1.shape}):\n{t1}')
# =&gt;
# t1 (torch.Size([2, 3, 5])):
# tensor([[[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
#          [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
#          [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]],
#
#         [[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
#          [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
#          [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]]],
#        grad_fn=&lt;StackBackward0&gt;)

t2 = torch.stack([t1])
print(f't2 ({t2.shape}):\n{t2}')
# =&gt;
# t2 (torch.Size([1, 2, 3, 5])):
# tensor([[[[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
#           [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
#           [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]],
#
#          [[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
#           [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
#           [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]]]],
#        grad_fn=&lt;StackBackward0&gt;)
</code></pre>
","1","Answer"
"78710952","78710808","<p>One way to approach this would be to look for a table that has the string &quot;Key Rates&quot; associated with a th element.</p>
<p>You may want to use column names other than those derived from the HTML</p>
<p>Something like this:</p>
<pre><code>import requests
from bs4 import BeautifulSoup as BS
from bs4.element import Tag
import pandas as pd
import io

TH = &quot;Key Rates&quot;

def is_table(table: Tag, text: str=TH) -&gt; bool:
    for th in table.select(&quot;tr th&quot;):
        if th.get_text() == text:
            return True
    return False

with requests.get(&quot;https://www.centralbank.go.ke&quot;) as response:
    response.raise_for_status()
    for table in BS(response.text, &quot;lxml&quot;).select(&quot;table.tg&quot;):
        if is_table(table):
            html = io.StringIO(str(table))
            df = pd.read_html(html)[0].drop(columns=[TH])
            mapper = {k: v for k, v in zip(df.columns, [&quot;Rate type&quot;, &quot;Percentage&quot;, &quot;Date&quot;])}
            print(df.rename(columns=mapper).to_string(index=False))
            break
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>          Rate type Percentage       Date
  Central Bank Rate     13.00% 05/06/2024
    Inter-Bank Rate     13.26% 04/07/2024
CBK Discount Window     16.00% 05/06/2024
      91-Day T-Bill    15.977% 01/07/2024
               REPO      0.00% 08/05/2024
     Inflation Rate      4.64%  June,2024
       Lending Rate     16.45% April,2024
       Savings Rate      4.14% April,2024
       Deposit Rate     10.77% April,2024
               KBRR       8.9% 27/06/2016
</code></pre>
","0","Answer"
"78713132","78713048","<p>Try passing the <code>early_stopping_rounds</code> parameter to the <code>XGBClassifier</code> rather than to the <code>fit</code> method which does not have a <code>early_stopping_rounds</code> parameter.</p>
<p>So using your code:</p>
<pre><code>xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE, early_stopping_rounds = 10)
xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)])
</code></pre>
<p>Reference: <a href=""https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier"" rel=""noreferrer"">https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier</a></p>
","9","Answer"
"78713569","78713551","<p>Use <a href=""https://huggingface.co/transformers/v4.10.1/main_classes/model.html#:%7E:text=disable%20fast%20initialization.-,torch_dtype:%7E:text=Override%20the%20default%20torch.dtype%20and%20load%20the"" rel=""nofollow noreferrer""><code>torch_dtype='auto'</code></a> in <code>from_pretrained()</code>. Example:</p>
<pre><code>model2 = AutoModelForTokenClassification.from_pretrained(save_directory, 
                                                         local_files_only=True,
                                                         torch_dtype='auto')
</code></pre>
<p>Full example:</p>
<pre><code># pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f&quot;Parameter: {name}, Data type: {param.dtype}&quot;)

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True, torch_dtype='auto')
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)
</code></pre>
<p>It'll load model2 as <code>torch.float16</code>.</p>
","1","Answer"
"78714267","78598596","<p>I found a way to do it: When starting the training with e.g. <code>--num-envs=4</code>, each environment will be launched on its own <code>mlagents-port</code>, starting with port 5005. The port will be passed as a commandline argument to the Unity process. Printing the processes with <code>ps aux</code> will show this:
<a href=""https://i.sstatic.net/wiiVKx2Y.png"" rel=""nofollow noreferrer"">ML Processes</a></p>
<p>In the Unity code, the port can then be fetched via the <code>System.Environment.GetCommandLineArgs()</code> and used to determine the player name.</p>
","1","Answer"
"78714320","78576306","<p>You should indeed <strong>use the in-sample mean and standard deviation to rescale the forecasts back to the original scale</strong> because of the following reasons:</p>
<ul>
<li><strong>Consistency</strong>: Your model was trained on data scaled with these
parameters, so using the same parameters for rescaling maintains
consistency.</li>
<li><strong>Avoiding data leakage</strong>: Using out-of-sample statistics for rescaling
would introduce information that wasn't available during model
training, which could lead to biased results.</li>
</ul>
<p>Rescale the predictions:</p>
<pre><code>X_test_scaled = scaler_X.transform(X_test)
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)
</code></pre>
<p>Manual rescaling:</p>
<pre><code>y_pred_manual = y_pred_scaled * scaler_y.scale_ + scaler_y.mean_
</code></pre>
","2","Answer"
"78717596","78704234","<h1>No, but ...</h1>
<p><code>torch.where(...)</code> does not detach anything from the computational graph.</p>
<p><code>torch.where(cond, a, b)</code> has the same gradient as <code>a</code> where <code>cond</code> is <code>True</code> and the same as <code>b</code> where <code>cond</code> is <code>False</code></p>
<p>(so in essence, if <code>c = torch.where(cond, a, b)</code>, <code>c.grad</code> is <code>torch.where(cond, a.grad, b.grad)</code>)</p>
<p>In your case though, <code>a</code> and <code>b</code> are constants so all those gradients are 0, which is effectively cutting the results from the graph.</p>
<p>You say your operation is &quot;thresholding&quot;, but <strong>that is not what you are doing!</strong></p>
<p>Thresholding would be keeping the value unless it is above (or below) some threshold. What you are doing is setting the values below the threshold to <code>0</code> and the values above to 1, which is a <a href=""https://en.wikipedia.org/wiki/Heaviside_step_function"" rel=""nofollow noreferrer"">Heaviside step function</a>. It is differentiable almost everywhere, but <strong>its gradient is always 0 when defined</strong> (so unusable for optimization purposes)</p>
<h1>Fix</h1>
<p>You may want to replace that heaviside function with its differentiable approximation, the <a href=""https://en.wikipedia.org/wiki/Sigmoid_function"" rel=""nofollow noreferrer"">sigmoid</a></p>
<p>The code would be something like this</p>
<pre class=""lang-py prettyprint-override""><code>channels[tracker_index]= torch.sigmoid(channel_tensor - threshold)
</code></pre>
<p><strong>Note</strong> also that if you are looking for a loss for this data, you may want to look towards binary cross-entropy, in which case there is a (more stable) <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html"" rel=""nofollow noreferrer"">version</a> in pytorch that takes the raw logits instead of the output of the sigmoid</p>
","0","Answer"
"78717951","78717924","<p>The output order of <code>train_test_split</code> is not correct in your code. So your <code>y_train</code> is actualy the <code>X_test</code>, which has a different sample size than train set. Here is the correct order:</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
</code></pre>
","0","Answer"
"78719731","78719712","<p>Use Windows Command Prompt (cmd.exe) instead of PowerShell:
Sometimes, running the same command in Command Prompt instead of PowerShell can resolve issues with Unix-like commands.</p>
<p>Open Command Prompt and run:</p>
<pre><code>./darknet detector train DATASET/voc.data cfg/yolov3-voc.cfg darknet53.conv.74
</code></pre>
","0","Answer"
"78720115","78719068","<p>It isn't really clear how you plot the confusion matrix, but you could iterate over <code>preds</code> or <code>test_gen</code> and plot the sample and output when their labels differ.</p>
<p>As you don't show the type of inputs you use for your model, the displaying method to use is up to you.</p>
<p>The code would look something like this :</p>
<pre class=""lang-py prettyprint-override""><code># You should have before :
#   x_test : test dataset 
#   y_true : labels of x_test

y_pred=model.predict(x_test)

# First loop to sort falty predictions from correct ones
wrong_labels = []
for label, i in enumerate(y_pred) : # The array we iterate on doesn't matter, as they all are the same length
    if label != y_true[i] :
        wrong_labels.append([x_test, y_true, y_pred])

# Second loop to display errors, the limit to 4 samples is arbitrary

for x, y_true, y_pred, i in enumerate(wrong_labels) : 
    if i&gt;=4 :
        break
    plt.subplot(1,4,i+1)
    display_sample(x)   # This function depends on the type of sample you have
    plt.title(&quot;Label &quot; + str(y_true) + &quot; expected, but predicted &quot;+str(y_pred))

plt.show()
</code></pre>
","0","Answer"
"78721390","78719585","<p>Your data's shape is not correct, it is missing the batch dimension, for a single data point, the shape should be <code>(1, 40000, 1)</code>.</p>
","1","Answer"
"78721843","78719712","<p>The command you mention running:</p>
<pre><code>./darknet detector train DATASET/voc.data cfg/yolov3-voc.cfg  darknet53.conv.74
</code></pre>
<p>...implies you are running the old abandoned AlexeyAB repo.  Note that repo was abandoned in July 2021, just over 3 years ago.  The new repo is this one:</p>
<ul>
<li><a href=""https://github.com/hank-ai/darknet#table-of-contents"" rel=""nofollow noreferrer"">https://github.com/hank-ai/darknet#table-of-contents</a></li>
</ul>
<p>Part of the installation step for that repo will install darknet.exe to a proper location, such as <code>C:\Program Files\Darknet\bin\darknet.exe</code>, and the NSIS package installation also attempts to put that location in your PATH.  This way, you can run <code>darknet.exe</code> from any location without running into problems.</p>
<p>I strongly suggest you upgrade to a newer and supported version of Darknet/YOLO.</p>
<p>If you have further question, note the following:</p>
<ol>
<li>The Darknet/YOLO FAQ:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/</a></li>
<li>The Darknet/YOLO discord server:  <a href=""https://discord.gg/zSq8rtW"" rel=""nofollow noreferrer"">https://discord.gg/zSq8rtW</a></li>
</ol>
","0","Answer"
"78722863","78722250","<p>Scikit regression scores are calculated using R2 and this can be negative if you have a poor fit (see details <a href=""https://stackoverflow.com/questions/63757258/negative-accuracy-in-linear-regression"">here</a>).</p>
<p>In your code, you initially held out 20% of the data and <code>model.score(x_test, y_test)</code> comes up with the score of about <code>0.59</code> when I run it.</p>
<p>The function <code>cross_val_score</code> does this for you.  For example, the default <code>cv</code> uses 5-fold cross validation. The idea of cross validation is to use all the data to train, but also independently check the results.  This randomly splits the data into 5 sections, then trains the model holding out one of those splits (or 20% of the data), checks the model on that 20%, then repeats for each section). The average across each of these should give a reasonable assessment of score. Indeed it gives <code>[0.54866323, 0.46820691, 0.55078434, 0.53698703, 0.66051406]</code> which has a mean/std of <code>0.553 +/- 0.062</code>, which is in-line with the single case mentioned above.</p>
<p>Upon increased number of <code>cv</code> sections, there is a risk of a non-representative sample dominating the score for that section. This leads to more variance in the score result. For example with <code>cv=100</code>, the result is <code>-0.095 +/- 0.918</code>, which is still statistically in-line with the above scores but too wide a variance to be helpful in analyzing the data. I suggest keeping the k-fold low enough to have a statistically meaningful score.</p>
<p>P.S. To reproduce this, I needed to add the following lines at the top of your code:</p>
<pre><code>from sklearn.datasets import fetch_california_housing
import pandas
import matplotlib.pyplot as plt
import numpy as np

california_housing = fetch_california_housing(as_frame=True)
df = california_housing.frame
</code></pre>
","0","Answer"
"78723107","78721703","<p>For this specific case, you can define a custom accuracy function as a metric and define a <code>Callback</code> for your <code>Keras</code> model.</p>
<p><strong>Custom Accuracy Metric</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>import keras.backend as K

def custom_accuracy(y_true, y_pred, tolerance=0.05):
    absolute_difference = K.abs(y_true - y_pred)
    correct_predictions = K.cast(absolute_difference &lt;= tolerance, dtype='float32')
    return K.mean(correct_predictions)

model.compile(optimizer='adam', loss='mse', metrics=[custom_accuracy])
</code></pre>
<p><strong>Custom Callback</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>from keras.callbacks import Callback
import numpy as np

class CustomAccuracyCallback(Callback):
    def __init__(self, validation_data, tolerance=0.05):
        super(CustomAccuracyCallback, self).__init__()
        self.validation_data = validation_data
        self.tolerance = tolerance

    def on_epoch_end(self, epoch, logs={}):
        x_val, y_val = self.validation_data
        y_pred = self.model.predict(x_val)
        accuracy = np.mean(np.abs(y_val - y_pred) &lt;= self.tolerance)
        print(f&quot;\nEpoch {epoch + 1}: Custom Accuracy: {accuracy:.4f}&quot;)
        logs['custom_accuracy'] = accuracy

custom_callback = CustomAccuracyCallback((x_val, y_val))
model.fit(x_train, y_train, validation_data=(x_val, y_val), callbacks=[custom_callback])
</code></pre>
","1","Answer"
"78726140","78725972","<p>It should work if you transform the MNIST dataset labels/targets into tensors as well, e.g.,</p>
<pre class=""lang-py prettyprint-override""><code>transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

def target_transform(t):
    return torch.tensor(t)

dataset1 = datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    target_transform=target_transform,
)

...

</code></pre>
","0","Answer"
"78726564","78722413","<p>Figured it out. After loading the model, specifically define the input and output shapes again like</p>
<pre><code>inputs = tf.keras.Input(shape=(240, 146, 1))
outputs = model(inputs)
</code></pre>
<p>and replace <code>shape</code> with whatever dimensions your model takes.</p>
","-2","Answer"
"78726921","78726907","<p>The problem is that you're using the same relative path from two pages in different directories. On <code>localhost:5174/practice</code>, <code>model/model.json</code> means <code>localhost:5174/model/model.json</code>, but on <code>localhost:5174/practice/1</code>, it means <code>localhost:5174/practice/model/model.json</code>.</p>
","1","Answer"
"78729775","78729235","<p>You can achieve that by doing incremental training:</p>
<pre><code># To run at each new increment:
model = xgboost.train(
      params, 
      df_train = f(df, model), # f to be implemented with the logic you described
      num_boost_round=1, xgb_model=model,  # Will add 1 new tree to model
    )
</code></pre>
","1","Answer"
"78729789","78728307","<p>I think you can try to use another activation function. Embedding Vector may have negative values. But your model outputs is between 0 and 1.</p>
","0","Answer"
"78730145","78728869","<p>The model expects an image in the form of a NumPy array or a tensor, but you're passing a file path as a string directly. You need to read the image from the file first and then pass it to the model.</p>
<p>Something like this:</p>
<pre><code> # Image path
img_path = 'WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg'

# Read the image using OpenCV
img = cv2.imread(img_path) # height, width, channel
img = img.swapaxes(0,2)    # channel, height, width
img = np.expand_dims(img,0) # batch, channel, height, width
</code></pre>
<p>and then pass the <code>img</code> to your <code>model</code></p>
<pre><code># Inference
res = model(img)
</code></pre>
","2","Answer"
"78731219","78688976","<p>Firstly, find vector embeddings of your facial database.</p>
<pre class=""lang-py prettyprint-override""><code>from deepface import DeepFace

students = [&quot;alice.jpg&quot;, &quot;bob.jpg&quot;, &quot;charlie.jpg&quot;]

for student in students:
   embedding = DeepFace.represent(img_path = student)[0][&quot;embedding&quot;]
   # store student and embedding in your database
</code></pre>
<p>Herein, the querying approach will be different according to the vector database you adopted. If you prefer to use postgres with pgvector extension.</p>
<pre class=""lang-py prettyprint-override""><code># real-time taken photo
target_img = &quot;target.jpg&quot;
target_embedding = DeepFace.represent(img_path = target_img)[0][&quot;embedding&quot;]

query = f&quot;&quot;&quot;
     SELECT *
     FROM (
         SELECT i.img_name, embedding &lt;-&gt; '{str(target_embedding)}' as distance
         FROM embeddings_datastore i
     ) a
     WHERE distance &lt; {threshold}
     ORDER BY distance asc
&quot;&quot;&quot;
</code></pre>
<p>the trick is to determine the correct threshold. the default facial recognition model in deepface is vgg-face. its pre-tuned threshold is 1.17 for euclidean distance as mentione <a href=""https://github.com/serengil/deepface/blob/master/deepface/modules/verification.py#L369"" rel=""nofollow noreferrer"">here</a>. so, the query will discard items with higher distance.</p>
<p>you can also adopt vector indexes such as annoy, voyager, faiss, nmslib or elasticsearch instead of a vector database. querying will be different according to your design but the idea will be same.</p>
<p>also, if the number of items in your database is not too big (~hundreds), then you may not need to use either vector index or vector database. postgres without pgvector extension, mongo, redis, cassandra or any relational database even sqlite would be fine.</p>
","1","Answer"
"78734257","78731177","<p>As the working sample data, it doesn't contain the unit:</p>
<pre><code>var sampleData = new MLModel1.ModelInput()
{
    Device = 1,
    Temperature = 20,
    Weather = 1,
    Time = 2,
};
```
</code></pre>
","0","Answer"
"78735229","78735084","<p>The error you are encountering , is due to how you are trying to access the provided row with <code>RMSE</code> in your <code>rainfall_data</code> DataFrame using <code>argmin</code> the correct way to retrieve that row with minimum value of <code>RMSE </code> like this :</p>
<pre><code>optimized_params = rainfall_data.loc[rainfall_data['RMSE'}.idxmin()]
</code></pre>
<p>there is why :</p>
<p>the thing is that <code>argmin</code> is an older method that return index of the minumum value .
while <code>idxmin</code> is a modern pandas and it return directly the index label with the minumum value .
<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.argmin.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.Series.argmin.html</a></p>
<p>the only explantation i can give about the error message <code>ValueError: The truth value of a DataFrame is ambiguous</code> specifically indicates that pandas cannot determine how to interpret the entire DataFrame <code>(rainfall_data)</code> in a boolean context. This confusion stems from trying to use <code>.argmin</code>, which is not a valid method for finding the index of the minimum value in a pandas Series <code>(like rainfall_data['RMSE'])</code>.</p>
","0","Answer"
"78737555","78703313","<p>For someone else that runs into the same issue, the cause seems to be an issue with JetBrains PyCharm's Jupyter Notebook support. I am filing a bug report with them as well. Running jupyter-notebook externally showed the correct version of numpy being used and the code works as expected.</p>
","0","Answer"
"78740080","78736775","<p>According to this <a href=""https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.load_model"" rel=""nofollow noreferrer"">documentation</a> any one of the paths should be given for loading the model.</p>
<ul>
<li><p><code>/Users/me/path/to/local/model</code></p>
</li>
<li><p><code>relative/path/to/local/model</code></p>
</li>
<li><p><code>s3://my_bucket/path/to/model</code></p>
</li>
<li><p><code>runs:/&lt;mlflow_run_id&gt;/run-relative/path/to/model</code></p>
</li>
<li><p><code>models:/&lt;model_name&gt;/&lt;model_version&gt;</code></p>
</li>
<li><p><code>models:/&lt;model_name&gt;/&lt;stage&gt;</code></p>
</li>
</ul>
<p>But you are giving model id and datastore path which is not supported.</p>
<p>So, try this code.</p>
<pre class=""lang-py prettyprint-override""><code>loaded_model = mlflow.sklearn.load_model(&quot;models:/local-mlflow-example/1&quot;)
loaded_model.predict(sample_data[&quot;data&quot;])
</code></pre>
<p>Output:</p>
<p><img src=""https://i.imgur.com/5RfSEvo.png"" alt=""enter image description here"" /></p>
<p>Here the path should be  <code>models:/&lt;model_name&gt;/&lt;model_version&gt;</code></p>
<p><code>model.id</code> or <code>model.path</code> or datastore path is supported when using azure ml jobs context.</p>
<p>So, to use <code>model.id</code> or <code>model.path</code> you submit the command job like below.</p>
<pre class=""lang-py prettyprint-override""><code>from azure.ai.ml import command
from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import Input, Output

inputs = {
    &quot;input_data&quot;: Input(
        type=AssetTypes.URI_FILE, path=&quot;./mlflow-model/input_example.json&quot;
    ),
    &quot;input_model&quot;: Input(type=AssetTypes.MLFLOW_MODEL, path=model.path),
}

outputs = {
    &quot;output_folder&quot;: Output(
        type=AssetTypes.URI_FOLDER,
        path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/workspaceblobstore/paths/predictions&quot;,
    )
}

job = command(
    code=&quot;./src&quot;,  # local path where the code is stored
    command=&quot;python load_score.py --input_model ${{inputs.input_model}} --input_data ${{inputs.input_data}} --output_folder ${{outputs.output_folder}}&quot;,
    inputs=inputs,
    outputs=outputs,
    environment=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1&quot;,
    compute=&quot;cpu-cluster&quot;,
)

# submit the command
returned_job = ml_client.jobs.create_or_update(job)
# get a URL for the status of the job
returned_job.studio_url
</code></pre>
<p><code>load_score.py</code> script which loads the model and prints the result.</p>
<pre class=""lang-py prettyprint-override""><code>import argparse
import pandas as pd
import mlflow.sklearn
import pandas as pd
import json
import os

parser = argparse.ArgumentParser()
parser.add_argument(&quot;--input_data&quot;, type=str)
parser.add_argument(&quot;--input_model&quot;, type=str)
parser.add_argument(&quot;--output_folder&quot;, type=str)
args = parser.parse_args()

with open(args.input_data) as f:
    sample_data = json.load(f)

f.close()

print(sample_data)

sk_model = mlflow.sklearn.load_model(args.input_model)
predictions = sk_model.predict(sample_data[&quot;data&quot;])

# Writing to stdout
print(predictions)

with open(os.path.join(args.output_folder, &quot;predictions.txt&quot;), &quot;x&quot;) as output:
    # Writing data to a file
    output.write(str(predictions))
output.close()
</code></pre>
<p>Job output:
<img src=""https://i.imgur.com/YX6Hs2X.png"" alt=""enter image description here"" /></p>
<p>Refer this <a href=""https://github.com/Azure/azureml-examples/blob/main/sdk/python/assets/model/model.ipynb"" rel=""nofollow noreferrer"">notebook</a> for more information.</p>
<p>If you face no such file error that is because of the dependencies required is not present like <code>MLmodel</code> or <code>model.pkl</code> etc is deleted in storage account or moved to other folders.</p>
","1","Answer"
"78741346","78731508","<p>the problem was an error in tensoflow zipping and reformating dataset helped</p>
<pre><code>def post_zip_process(example1, example2):
    reshaped_input = tf.transpose(example1[0], [0, 1, 2 ,-1])
    reshaped_input = reshaped_input[0, :, :, :]
    print(reshaped_input.shape)
    return (reshaped_input, example2[0]), example1[1]
</code></pre>
","0","Answer"
"78749810","78740880","<p>Yep you are absolutely right, the more positive value the more probability to be a 1, and the more negative value the higher probability to have a 0.
As for your ideai regarding comparing ith the SHAP absolute values you are absolutelly correct  here.</p>
<p>Check this article for more details <a href=""https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137"" rel=""nofollow noreferrer"">Using SHAP Values to Explain How Your Machine Learning Model Works</a></p>
","1","Answer"
"78750747","78736804","<p>Solution:</p>
<pre><code>image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
lower_limit = 89  # 35% of 255, although a slightly higher percentage my increase visibility of text if that is desired
upper_limit = 255
mask = cv2.inRange(gray, lower_limit, upper_limit)

mask_inv = cv2.bitwise_not(mask)

# Invert the mask_inv to get black text on white background
inverted_mask_inv = ~mask_inv

plt.subplot(1,2,1); plt.imshow(image); plt.title(&quot;Original&quot;)
plt.subplot(1,2,2); plt.imshow(inverted_mask_inv, cmap=&quot;gray&quot;); plt.title(&quot;Just the text&quot;)
</code></pre>
","1","Answer"
"78752304","78467998","<p>You have to fill in missing gaps in data, using either forward fill or imputation, then use forward fill to make them all the same resolution, for example make quarterly data into hourly. Then you need to scale all the stationary data using differencing, and you can tell if a dataset is stationary using the A.D. fuller test. Then you standardize using z-score scaling. Then you are finally ready to train. You also need to think of any feature engineering you want to do on the data too.</p>
","0","Answer"
"78752390","78751670","<p>Assuming <code>exact_*</code> is what you're attempting to compute, you're going about it in the wrong way. Your indexing within the differentiated functions (i.e. <code>...[0]</code>) is removing some of the elements that you're trying to compute.</p>
<p>What <code>exact_dF1</code> and <code>exact_ddF1</code> are computing is element-wise first and second derivatives for 2D inputs. You can compute this using either <code>grad</code> or <code>jacobian</code> by applying <code>vmap</code> twice (once for each input dimension). For example:</p>
<pre class=""lang-py prettyprint-override""><code>exact_dF1  = (-1/array**2)
grad_dF1 = jax.vmap(jax.vmap(jax.grad(F1)))(array)
jac_dF1 = jax.vmap(jax.vmap(jax.jacobian(F1)))(array)
print(jnp.allclose(exact_dF1, grad_dF1))  # True
print(jnp.allclose(exact_dF1, jac_dF1))  # True

exact_ddF1 = (2/array**3)
grad_ddF1 = jax.vmap(jax.vmap(jax.grad(jax.grad(F1))))(array)
jac_ddF1 = jax.vmap(jax.vmap(jax.jacobian(jax.jacobian(F1))))(array)
print(jnp.allclose(exact_ddF1, grad_ddF1))  # True
print(jnp.allclose(exact_ddF1, jac_ddF1))  # True
</code></pre>
<p>What <code>exact_dF2</code> and <code>exact_ddF2</code> are computing is a row-wise jacobian and hessian of a 2D-&gt;1D mapping. By its nature, this is difficult to compute using <code>jax.grad</code>, which is meant for functions with scalar output, but you can compute it using the jacobian this way:</p>
<pre class=""lang-py prettyprint-override""><code>exact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))
exact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))

jac_dF2 = jax.vmap(jax.jacobian(lambda a: F2(a)[0]))(array)
jac_ddF2_full = jax.vmap(jax.jacobian(jax.jacobian(lambda a: F2(a)[0])))(array)
jac_ddF2 = jax.vmap(jnp.diagonal)(jac_ddF2_full)
print(jnp.allclose(exact_dF2, jac_dF2))  # True
print(jnp.allclose(exact_ddF2, jac_ddF2))  # True
</code></pre>
","0","Answer"
"78753780","78724837","<p>I ran into similar issues yesterday evening and, therefore, tried to set up a playground environment to test Ollama with in a docker-compose environment. I managed to make it work on my MacBook with Apple Silicon and uploaded the code to GitHub in a public repo in case you are still blocked on the issue.</p>
<p><a href=""https://github.com/dcleres/ollama-docker-macOS"" rel=""nofollow noreferrer"">https://github.com/dcleres/ollama-docker-macOS</a></p>
<p>I hope this helps!</p>
","0","Answer"
"78755869","78680846","<p>Try this</p>
<pre><code>!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0 do_pr_metric True --plot_confusion_matrix --plot_curve True
</code></pre>
<p>Adding those 3 arguments gave me a confusion matrix, F1 curve, P curve, PR curve, and R curve graphs.</p>
<p>These are the explanations of those arguments that i found in the eval.py script:</p>
<ul>
<li>'<strong>--do_pr_metric</strong>',
default=False,
type=boolean_string, help='whether to calculate precision, recall and F1, n, set False to close'</li>
<li>'<strong>--plot_curve</strong>', default=True, type=boolean_string, help='whether to save plots in savedir when do pr metric, set False to close'</li>
<li>'<strong>--plot_confusion_matrix</strong>', default=False, action='store_true', help='whether to save confusion matrix plots when do pr metric, might cause no harm warning print'</li>
</ul>
<p>And I suggest you to also play around with the <strong>--task</strong> flag. It has 3 options <strong>('val, test, or speed')</strong>. I haven't tried test and speed, so I don't know what the output. Play around and see which one you really need.</p>
","0","Answer"
"78759829","78752899","<p>As already hinted at in <a href=""https://stackoverflow.com/questions/78752899#comment138849375_78752899"">@xdurch0's comment</a>, I think you have fallen trap to a misconception here. Weight decay and decaying the learning rate are two different things – at least conceptually¹:</p>
<ul>
<li><em>Weight decay</em> is used for regularizing the weights in your network by penalizing large weight values. It is used as a means to prevent overfitting and thus to help generalization.</li>
<li>By <em>decaying the learning rate</em>, one hopes to converge to a good solution: it follows the idea that a larger initial learning rate helps &quot;exploring the loss landscape&quot;, while decreasing it in later stages of training helps &quot;homing in&quot; on the final solution.</li>
</ul>
<p>Crucially, altering the weight decay value does <em>not</em> alter the learning rate value!</p>
<h2>Use a learning rate scheduler</h2>
<p>To achieve a  decaying learning rate in PyTorch, one usually makes use of a <em>learning rate scheduler</em>, typically in the following way:</p>
<ol>
<li>One creates a scheduler instance that receives, as one of its inputs, the optimizer instance.</li>
<li>At the end of each training epoch, one calls <code>step()</code> on the scheduler (just like calling <code>step()</code> on the optimizer after processing a training batch). This will trigger the scheduler to calculate the new learning rate value for the next epoch, which it will then use for adjusting the optimizer accordingly.</li>
</ol>
<p>The following is an excerpt of the <a href=""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"" rel=""nofollow noreferrer"">PyTorch documentation</a>, demonstrating this approach (comments added by me):</p>
<pre class=""lang-py prettyprint-override""><code>optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 1. Create scheduler, provide it with optimizer
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input_, target in dataset:
        optimizer.zero_grad()
        output = model(input_)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    # 2. Trigger calculation and setting of next learning rate value
    scheduler.step()
</code></pre>
<p>Here, an <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html"" rel=""nofollow noreferrer""><code>ExponentialLR</code></a> scheduler is used, which decays the learning rate exponentially, according to a fixed schedule (i.e. only depending on its initial parameters; in particular, the decay factor, but independent of the current network performance). There are also adaptive schedulers, which adjust the learning rate depending on the actual network performance; for example, <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html"" rel=""nofollow noreferrer""><code>ReduceLROnPlateau</code></a> reduces the learning rate by a given factor as soon as a loss plateau of a given length has been detected.</p>
<p><sub>¹) Mathematically, for some optimizers, learning rate and weight decay are implicitly coupled, which is one of the reasons why <em>AdamW</em> was derived from the <em>Adam</em> optimizer in the first place. For more information, one might want to read the <a href=""https://arxiv.org/abs/1711.05101"" rel=""nofollow noreferrer"">AdamW paper</a>. But this is not the problem described in the question.</sub></p>
","1","Answer"
"78761440","78757193","<p>To apply convolutions to sequence data, you want to use a 1D convolution (ie <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"" rel=""nofollow noreferrer"">Conv1D</a> instead of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer"">Conv2D</a>).</p>
<p>The 1D conv operation expects an input of shape <code>(batch_size, channels, d)</code> which matches your tensor of shape <code>(batch_size, sequence_length, d)</code>.</p>
<p>HOWEVER convolutions (both 1D and 2D) expect a fixed size input. This is problematic because sequence data is typically variable length in the <code>sequence_length</code> dimension.</p>
<p>You can get around this by defining a maximum sequence length and padding all inputs to that sequence length. You could do something like this:</p>
<pre class=""lang-py prettyprint-override""><code>def pad_tensor(x, max_size):
    batch_size, sequence_length, d = x.size()
    
    pad_size = max_size - sequence_length
    
    if pad_size &lt;= 0:
        return x
    
    padding = (0, 0,
               0, pad_size,
               0, 0)
    
    padded_x = torch.nn.functional.pad(x, padding, mode='constant', value=0)
    
    return padded_x

class PaddedConv1D(nn.Module):
    def __init__(self, max_size, **conv_kwargs):
        super().__init__()
        self.max_size = max_size
        self.conv = nn.Conv1d(in_channels=max_size, out_channels=max_size, **conv_kwargs)
        
    def forward(self, x):
        sequence_length = x.shape[1]
        x = pad_tensor(x, self.max_size)
        x = self.conv(x)
        return x[:, :sequence_length]
</code></pre>
<p>However but this is computationally wasteful. You're better off sticking with an operation that works with variable sequence lengths.</p>
<p>That said, it seems like your intent is to replace the feed forward layer of the transformer. In this case using a convolution doesn't really make sense. The convolution operates across the sequence length, while the FF layer operates on each input time step individually. You could (technically speaking) do a 1D conv along the embedding dimension but this doesn't really make sense. Convolving along the embedding dimension actually uses more flops to produce the same output size compared to a FF layer. You can benchmark the following code to verify.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(8, 17, 64)

ff = nn.Linear(64, 64)

conv = nn.Conv1d(64, 64, 3, padding=1)

y1 = ff(x)
y2 = conv(x.permute(0, 2, 1)).permute(0,2,1)
</code></pre>
","0","Answer"
"78761535","78761234","<p>As the error is saying as an <code>covert_to</code> argument you should use one of the following <code>&quot;mlprogram&quot;, &quot;neuralnetwork&quot;, &quot;milinternal&quot;, &quot;milpython&quot;</code>. You need to read about each of this formats and select one that will work for your case.</p>
<p>Here is reference from the manual:</p>
<p><code>mlprogram</code> - is the Core ML program format. The model saved from this returned object is executable on iOS15, macOS12, watchOS8, and tvOS15.</p>
<p><code>neuralnetwork</code> - is the original Core ML format. The model saved from this returned object is executable either on iOS13/macOS10.15/watchOS6/tvOS13 and newer, or on iOS14/macOS11/watchOS7/tvOS14 and newer, depending on the layers used in the model.</p>
<p><code>milinternal</code> - Returns an MIL program object. An MIL program is primarily used for debugging and inspection.</p>
<p><code>milpython</code> - barely documented somewhere. But comments in the code says that it is one of the internal data structure, thus shouldn't be used by users.</p>
","0","Answer"
"78764285","78762914","<p>bm3d package you are using depends on a specific version of scipy causing the import error. To resolve this issue, you can try installing a compatible version of scipy along with the bm3d package.</p>
<p>Please use the following commands for this.</p>
<pre><code>!pip uninstall scipy -y 
!pip install scipy==1.8.0 
!pip install bm3d 

import bm3d 
</code></pre>
","0","Answer"
"78767036","78623717","<p>For MUSAE, if you read the <a href=""https://arxiv.org/pdf/1909.13021"" rel=""nofollow noreferrer"">paper</a>, you'll see that they derive the concept directly from Doc2Vec, which is a skipgram based embedding method widely used in NLP. The window_size which is a parameter in the function MUSAE, is the reason you see the behavior in question. The dimension of the embeddings returned is <code>d*(t+1)</code>, where d is the dimension you specify and t is the window_size.
By default, the value is 3, and hence the resulting dimension is 128 (32*4). In theory, window_size corresponds to context size, which means how far you travel from a root node to create the vocabulary. So if you want 32 dimensional embeddings, you would want to pass 8 instead of 32 as the parameter.</p>
<p>Note that changing the window_size parameter value also changes the number of dimensions, but in this case it would be impossible to get 32 dimensional embeddings while d is passed as 32, since you would have to pass 0 as the window_size and that is not allowed. Additionally, sometimes it will not allow you to decrease the window_size below a certain value, since you need some context to build vocabulary and capture the structure.</p>
<p>I am not sure what happens in case of GLEE, but it seems like it is generating embeddings that are 1 plus the specified dimension. Here you would have to pass 31 to get 32 dimensional embeddings. I do not know the inner workings of this algorithm.</p>
","1","Answer"
"78767301","78767192","<p>I think in your data there could be some columns where there are only <code>np.nan</code> or empty features for all rows that can cause KNNImputer to drop that column in the output</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from sklearn.impute import KNNImputer
&gt;&gt;&gt; 
&gt;&gt;&gt; imputer = KNNImputer(n_neighbors=1, 
...                      weights=&quot;uniform&quot;,
...                      add_indicator=False)
&gt;&gt;&gt; 
&gt;&gt;&gt; df = pd.DataFrame([[1.69, 2.69, np.nan], [3.69, 4.69, 3.69, np.nan], [np.nan, 6.69, 5.69, np.nan], [8.69, 8.69, 7.69, np.nan]])
&gt;&gt;&gt; print(df)
      0     1     2   3
0  1.69  2.69   NaN NaN
1  3.69  4.69  3.69 NaN
2   NaN  6.69  5.69 NaN
3  8.69  8.69  7.69 NaN
&gt;&gt;&gt; print(df.shape)
(4, 4)
&gt;&gt;&gt; print(df.dtypes.value_counts())
float64    4
Name: count, dtype: int64
&gt;&gt;&gt; 
&gt;&gt;&gt; output_df = imputer.fit_transform(df)
&gt;&gt;&gt; print(output_df.shape)
(4, 3)
</code></pre>
<p>I think you can avoid this by setting <code>keep_empty_features</code>  param to <code>True</code> instead of default <code>False</code> to avoid removing columns</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from sklearn.impute import KNNImputer
&gt;&gt;&gt; 
&gt;&gt;&gt; imputer = KNNImputer(n_neighbors=1, 
...                      weights=&quot;uniform&quot;,
...                      keep_empty_features=True,
...                      add_indicator=False)
&gt;&gt;&gt; 
&gt;&gt;&gt; df = pd.DataFrame([[1.69, 2.69, np.nan], [3.69, 4.69, 3.69, np.nan], [np.nan, 6.69, 5.69, np.nan], [8.69, 8.69, 7.69, np.nan]])
&gt;&gt;&gt; print(df)
      0     1     2   3
0  1.69  2.69   NaN NaN
1  3.69  4.69  3.69 NaN
2   NaN  6.69  5.69 NaN
3  8.69  8.69  7.69 NaN
&gt;&gt;&gt; print(df.shape)
(4, 4)
&gt;&gt;&gt; print(df.dtypes.value_counts())
float64    4
Name: count, dtype: int64
&gt;&gt;&gt; 
&gt;&gt;&gt; output_df = imputer.fit_transform(df)
&gt;&gt;&gt; print(output_df.shape)
(4, 4)
</code></pre>
","0","Answer"
"78770165","78769858","<p>Categorical cross entropy is for classification. The loss function is the negative of the sum of y_i*log(f(y_i)) where f(y_i) is your model's probability prediction. It looks like you have an array of probabilities for y=0 and y=1. You need to choose one of these, typically y=1, as your target variable and convert to probabilities to labels:</p>
<pre><code>y[y&gt;=0.5]=1
y[y&lt;0.5]=0
</code></pre>
<p>or</p>
<pre><code>y = (y &gt;= 0.5).astype(int)
</code></pre>
","0","Answer"
"78771272","78765868","<p>you can consider using a combination of the Cloud Speech-to-Text and Cloud Translation API. You can use speech to text api to Transcribe audio from <a href=""https://cloud.google.com/speech-to-text/docs/transcribe-streaming-audio#speech-streaming-recognize-python:%7E:text=Send%20feedback-,Transcribe%20audio%20from%20streaming%20input,-bookmark_border"" rel=""nofollow noreferrer"">streaming input</a> into text and then we can use <a href=""https://cloud.google.com/translate/docs"" rel=""nofollow noreferrer"">translation API</a>  to convert text into the targeted language.Follow this two blog to deploy and get an idea <a href=""https://medium.com/@mimichen123/step-by-step-guide-to-live-translation-using-google-cloud-ai-dc21fc508540"" rel=""nofollow noreferrer"">Live Translation Using GCP</a></p>
<p>Another way it can be possible using Pub/Sub ,first using text to speech client library you can transcribe the text and publish on Pub/Sub and then those published messages through topic id , you can translate  it using translation API .  To deploy you can take a look at this  <a href=""https://medium.com/analytics-vidhya/build-a-serverless-transcriber-translator-on-google-cloud-platform-in-10-min-54763568bf34#:%7E:text=Build%20a%20Serverless%20Transcriber%2DTranslator%20on%20Google%20Cloud%20Platform%20in%2010%20min"" rel=""nofollow noreferrer"">Build a Serverless Transcriber-Translator on GCP</a> .</p>
","1","Answer"
"78773302","78751682","<p>CLIP is trained such that the text and image embeddings are projected on to a shared latent space. In fact, image-text similarity is what the model is trained to optimise.</p>
<p>So a very typical use case of CLIP is to compare and match images and text based on similarity. In your case, you don't seem to be interested in any measure of similarity. You already have an image and the text and want some joint embedding representation. So concatenation of the two embeddings the way you described it is fine. An alternative would be take their mean (since they are in the same embedding space, it's fine to do this).</p>
<p>As for the size of the embedding, I don't think there is a way to change it as it's hardwired into the architecture of the model when it's trained. You can perhaps employ a dimensionality reduction technique, or fine tune the model after stacking another fully connected layer with the dimensionality of your choice.</p>
","0","Answer"
"78773857","78773489","<h1>Load trained binary classification models</h1>
<pre><code>m0 = init_binary_classification_model(input_shape)
m0.load_weights('model_0_weights.h5')
m1 = init_binary_classification_model(input_shape)
m1.load_weights('model_1_weights.h5')
m2 = init_binary_classification_model(input_shape)
m2.load_weights('model_2_weights.h5')
</code></pre>
<h1>Construct New Multiclassification Models</h1>
<pre><code>from keras.layers import Softmax, Concatenate

input_layer = Input(shape=(28,28))

output_0 = m0(input_layer)
output_1 = m1(input_layer)
output_2 = m2(input_layer)

concat_output = Concatenate()([output_0, output_1, output_2])

final_output = Softmax()(concat_output)

final_model = Model(inputs=input_layer, outputs=final_output)
</code></pre>
","0","Answer"
"78773995","78773942","<p>Your target (y_train) is of type <code>object</code></p>
<p>Casting y_train before doing CV to int32 fixes the error.</p>
<pre><code>y_train = y_train.astype(&quot;int32&quot;)
</code></pre>
","0","Answer"
"78774757","78739816","<p>I have read some papers where researchers have tried to combine positional and character information with CNNs, but they focused more on a grid-based approach. Grid transformers use such types of architectures.</p>
<p>In the case of word-grid, the approach works by appending the input image with word embeddings, such that the image also contains additional information about the text present in that part of the image. For character-grid, this involves appending a one-hot encoded vector representing each character.</p>
<p>Here are some papers on grid-based methods that might be helpful:</p>
<ul>
<li><a href=""https://%5Bhttps://arxiv.org/abs/1706.02337%5D"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1706.02337</a></li>
<li><a href=""https://arxiv.org/abs/2002.06144"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2002.06144</a></li>
<li><a href=""https://arxiv.org/abs/2105.06220"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2105.06220</a></li>
</ul>
<p>These methods cast text with layout information into 2D semantic representations: char-grid and sentence-grid.</p>
","0","Answer"
"78779699","78776364","<p>Hello everyone so I have had found the solution of my question. There is a Math.py script in library folder. There is a function create_corners in Math.py, we can use this function in our inference.py script in order to get labels of 3d bounding boxes. And the format of our 3d bounding boxes is x,y,z,x,y,z... , it is mentioned in the Math.py script only.</p>
","1","Answer"
"78780031","78778576","<p>I believe that I found a solution.</p>
<p>The Walker2D object has an attribute called jdict which is a dictionary with Joint objects. Each Joint object has its own id as an attribute called jointIndex.</p>
<p>By having the ids of the joints the state of each joint can be gained by using the <code>getJointState()</code> from the pybullet package. The first element of the returned array is the angle of the joint (in radians?). The required inputs for the <code>getJointState()</code> function are the robot id and the joint id that its state is wanted.</p>
<p>Example</p>
<pre><code>env = gym.make('Walker2BulletEnv-v0')
env.reset()

robot_id = env.env.robot.objects[0]
angle = pybullet.getJointState(robot_id, joint_id)[0]
</code></pre>
<p><em>Note:</em> I think that if you are using more than one physics client the client id is also required and if there are more than one robots there should be some changes in order to get the correct robot id.</p>
","0","Answer"
"78785061","78419755","<p>A simple workaround is to create a symbolic link of Keras package to the TensorFlow package directory.</p>
<p>Run this command: <code>ln -s ../keras/_tf_keras/keras/ keras</code></p>
<p>at the <code>site-packages/tensorflow</code> or <code>dict-packages/tensorflow</code> directory.</p>
<p>You may find the installed tensorflow package directory at</p>
<ol>
<li>venv: <code>.venv/lib/python3.*/site-packages/tensorflow</code></li>
<li>docker (linux): <code>/usr/local/lib/python3.*/dist-packages/tensorflow</code></li>
<li>homebrew (macOS): <code>/opt/homebrew/lib/python3.*/site-packages/tensorflow</code></li>
</ol>
","0","Answer"
"78785554","78784723","<p>I solved it just by taking the loss from only the last output rather than taking all the losses and sum them up. It fixed my issue but I still don't understand why my first approach doesn't work!</p>
<pre><code>for epoch in range(EPOCHS):
    dataset = CustomDataset(10000)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)
    model.train()
    total_loss = 0

    for length, sequence, next_number in dataloader:
        optimizer.zero_grad()
        h = torch.zeros(BATCH_SIZE)

        for i in range(length):
            x = torch.cat([h, sequence[0, i].unsqueeze(0)])
            h = model(x)[0].unsqueeze(0)
            
            if i == length - 1: loss = criterion(model(x)[1], next_number[0])
            
        loss.backward()
        optimizer.step()
        total_loss += loss.item() 
        
    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')
</code></pre>
","0","Answer"
"78787391","78787112","<p>This could happen, when tensorflow averages over the batches and gives you an &quot;aggregated&quot; version of this metric. To test that, try using a 1-sample-batchsize.</p>
","0","Answer"
"78788106","78658886","<p>In tfx, a pipeline uses Channels to shuttle information from/to ml metadata and feed it to the executor.  The pipeline, when it is running, is using those channels to substitute in the right values for <code>example_gen.outputs['examples']</code>.  When it is no longer running, it doesn't have those Channels active anymore.  Although the video is about custom components, it does a really good job of explaining how tfx works: <a href=""https://youtu.be/_c-znj_cZvU?si=v3qIp1iC-gPthnfn"" rel=""nofollow noreferrer"">https://youtu.be/_c-znj_cZvU?si=v3qIp1iC-gPthnfn</a></p>
<p>Conceptually, I think of the components producing inputs and outputs, and the pipeline facilitating the execution of these components.  For example, if there were three components that ran simultaneously at the end, which one of them should be the output?  As such, it is helpful to think of the pipeline in a few different ways: The pipeline definition (contains components which refer to inputs, outputs, execution parameters), the pipeline while it is executing (is using Channels to shuttle information back and forth from ml metadata, components are executing), and the pipeline after it has finished executing (ml metadata has information about pipeline runs).</p>
<p>You might've heard the term 'hermetically sealed' in the context of pipelines.  TFX does a really good job of ensuring there are no remnants of state lying around, and that you can reproduce these runs using information in ml metadata as long as you have the associated data still accessible.</p>
<p>Another good place to understand some of these concepts is: <a href=""https://www.tensorflow.org/tfx/guide"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/guide</a></p>
","0","Answer"
"78788303","78786800","<h2>Update :</h2>
<p>kaggle released <code>kaggle 1.6.17</code> on <strong>Jul 25, 2024.</strong></p>
<p>Now,</p>
<pre><code>pip install tf-models-official
</code></pre>
<p>is working with <code>tf-models-official 2.17.0</code></p>
<hr />
<h2>Old answer</h2>
<p><code>tf-models-official 2.17.0</code> was released on <code>Jul 17, 2024</code></p>
<p><a href=""https://i.sstatic.net/3Ky6Waxl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3Ky6Waxl.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>It is using <code>kaggle-1.6.15</code></strong>.</p>
<p>There is a issue going on <code>kaggle-1.6.15</code> as discussed here :</p>
<p><strong><a href=""https://github.com/Kaggle/kaggle-api/issues/611"" rel=""nofollow noreferrer"">https://github.com/Kaggle/kaggle-api/issues/611</a></strong></p>
<p>For now you can use:</p>
<pre><code>pip install tf-models-official==2.16.0
</code></pre>
<p>Until <code>kaggle-1.6.15</code> is fixed.</p>
","0","Answer"
"78790734","78761783","<p>You can set verbosity to your desired value like below. This solved it for me.</p>
<pre><code>import xgboost as xgb
xgb.set_config(verbosity=1)
</code></pre>
<p>Check <a href=""https://xgboost.readthedocs.io/en/stable/python/python_api.html"" rel=""nofollow noreferrer"">here</a> for documentation.</p>
","0","Answer"
"78792599","78668638","<p>if you are working on MacOS, add below code blcok on top of your porgram. It will sove the issue</p>
<p>import ssl
ssl._create_default_https_context = ssl._create_unverified_context</p>
","1","Answer"
"78794175","78781313","<p>I don't follow what you are trying to do on a conceptual level, but I can help you diagnose why torch is complaining.</p>
<h3><strong>Issue #1</strong> - Incorrect Matrix Shapes</h3>
<p>For any given observation and prediction (n=1), cross entropy loss will need the real class value y <code>(1,1)</code> and the prediction probabilities for all classes (in this case:- <code>(1,2)</code>.</p>
<p>In each epoch you are feeding the model a tensor of shape <code>(100, 10, 15)</code> and outputting logits of shape <code>(100, 1)</code>. If 100 refers to the number of individual examples - then I found this output shape odd. If for each observation you would like to a binary prediction, an output shape of <code>(100, 2)</code> would make more sense.</p>
<p>That is because internally <code>nn.CrossEntropyLoss</code> will generate the probabilities for each class. For example, the following would work</p>
<pre><code># note the dtypes
outputs = torch.randn(100, 2, requires_grad=True)  # logits
y_train_tensor = torch.randint(low=0, high=1, size=(100,))  # labels

output = loss(outputs, y_train_tensor)
</code></pre>
<h4>Issue #2 - labels dtype</h4>
<p>You've also set the dtype of <code>y_train_tensor</code> to <code>bool</code>. I don't think that torch converts this internally, so best to use a numeric dtype such as <code>torch.int</code>.</p>
<p>Now, to actually solve the problem (which of the tensors to reshape) will depend upon your task and what the model output shape represents.</p>
<p>Hope this helps.</p>
","0","Answer"
"78794433","78793796","<p>You shouldn't need the target variable of the test set while applying leave-one-out (or any other) encoding. Even if you somehow managed to pass it when you do your offline evaluations on the test set, how will you actually apply
it during inference time? During inference time when your model is serving traffic from real users, obviously the true label wouldn't be available. And you should always compute your test metrics such that they are representative of what happens in the real world. So conceptually, it seems wrong to use the test labels to do feature encoding.</p>
<p>I looked up the <a href=""https://github.com/scikit-learn-contrib/category_encoders/blob/11fbba6520341e9b960d35dafd44704a67b5bafe/category_encoders/leave_one_out.py#L157-L158"" rel=""nofollow noreferrer"">source code</a> of leave-one-out encoding in the <code>category_encoders</code> package and it's apparent that they find the mean target without leaving the current example out when the target variable is not supplied</p>
<pre><code># Replace level with its mean target; if level occurs only once, use global mean
level_means = (colmap['sum'] / colmap['count']).where(level_notunique, self._mean)
</code></pre>
<p>So if I would just use the encoder like this</p>
<pre><code>import category_encoders as ce
from sklearn.model_selection import train_test_split
import pandas as pd

dataframe = pd.DataFrame({
    'f1': ['P', 'Q', 'P', 'Q', 'P', 'P', 'Q', 'Q'],
    'f2': ['M', 'N', 'M', 'N', 'M', 'N', 'M', 'N'],
    'f3': ['A', 'B', 'C', 'C', 'C', 'C', 'A', 'C'],
    'y': [1, 0, 1, 0, 1, 1, 0, 0]
})

train_data, test_data = train_test_split(dataframe, test_size=0.2)

encoder = ce.LeaveOneOutEncoder(cols=['f1', 'f2', 'f3'])

encoded_train = encoder.fit_transform(train_data, train_data['y'])
encoded_test = encoder.transform(test_data)
</code></pre>
","1","Answer"
"78796369","78796342","<p><code>x_train</code> is defined as an empty list <code>[]</code>.  As stated in the error, lists don't have any attribute called <code>shape</code>.</p>
<p>I'm assuming you're trying to use it as if it were a numpy array.<br />
If you want a more in-depth tutorial on how to use it, see <a href=""https://www.w3schools.com/python/numpy/numpy_array_shape.asp"" rel=""nofollow noreferrer"">https://www.w3schools.com/python/numpy/numpy_array_shape.asp</a></p>
<p>Basically, the <code>shape</code> attribute exists on numpy arrays, not regular python arrays.<br />
You can create numpy arrays with <code>np.array</code> - for example, <code>np.array([1, 2, 3, 4, 5])</code>.</p>
","0","Answer"
"78796374","78796342","<p>You are trying to use a numpy arrays attribute for a normal list.
When you write <code>x_train = []</code> that creates an empty list and data is added to it with the loop. Then with <code>x_train.shape[1]</code> you treat it as numpy array. If you want the length of your list, or how many elements it contains, you can use <code>len(x_train)</code></p>
","1","Answer"
"78797121","78786001","<p>I could not solve my problem to mask accurate with JESON data. I try to use open CV and creat polygon to mask my originals images so I decide to use this script as my time and accuracy project requirements:</p>
<pre><code>
import cv2
import os
import time

# Path to the folder containing red images
red_folder = os.path.expanduser('~/Desktop/red')

# Path to the folder for saving masked images
mask_folder = os.path.expanduser('~/Desktop/q')

# Ensure the folder for saving masks exists
os.makedirs(mask_folder, exist_ok=True)

# List all image files in the folder
image_files = [f for f in os.listdir(red_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]

# Iterate over each image
for i, image_file in enumerate(image_files):
    # Read the image
    image_path = os.path.join(red_folder, image_file)
    image = cv2.imread(image_path)
    
    # Display the original image for debugging
    cv2.imshow(&quot;Original Image&quot;, image)
    cv2.waitKey(0)  # Wait for a key press
    cv2.destroyAllWindows()  # Close the window

    # Create binary mask for red regions
    mask1 = cv2.inRange(image, (0, 0, 100), (100, 100, 255))  # Bright red
    mask2 = cv2.inRange(image, (0, 100, 100), (100, 255, 255))  # Dark red
    mask = cv2.bitwise_or(mask1, mask2)  # Combine both masks

    # Display the mask for debugging
    cv2.imshow(&quot;Mask&quot;, mask)
    cv2.waitKey(0)  # Wait for a key press
    cv2.destroyAllWindows()  # Close the window

    # Save the masked image
    mask_filename = f&quot;mask_{image_file}&quot;
    mask_path = os.path.join(mask_folder, mask_filename)
    cv2.imwrite(mask_path, mask)
    
    print(f&quot;Processed image {i+1}/{len(image_files)}: {mask_filename}&quot;)
    
    # Wait for 2 seconds
    time.sleep(2)


</code></pre>
<p>however this way has challenges in masking when there is more red color things. this code add these more red thing to mask and lowing the quality. and is not accurate when I want train a conjuctival pulpebral model in u net in the future as is my final medical goal.</p>
","0","Answer"
"78797617","78796974","<p>TL;DR:<br />
The definition of training set matters.</p>
<p>Longer answer:</p>
<p>Your understanding is right.</p>
<p>However, what's not quite right is SHAP's hidden data transformations silently applied behind the scenes, which can be traced like this:</p>
<pre><code>import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import shap

X, y = fetch_california_housing(return_X_y=True, as_frame=True)
X = X.drop(columns = [&quot;Latitude&quot;, &quot;Longitude&quot;, &quot;AveBedrms&quot;])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0,
)

scaler = MinMaxScaler().set_output(transform=&quot;pandas&quot;).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

linreg = LinearRegression().fit(X_train, y_train)
coeffs = pd.Series(linreg.coef_, index=linreg.feature_names_in_)

X_test.reset_index(inplace=True, drop=True)
obs = 6188

explainer = shap.LinearExplainer(linreg, X_test)
shap_values = explainer(X_test)
shap_values[obs]
</code></pre>
<hr />
<pre><code>.values =
array([ 0.15757575, -0.45065211, -0.12948118,  0.03568408, -0.00211654])

.base_values =
2.023180048641746

.data =
array([0.25094137, 0.01960784, 0.06056066, 0.07912217, 0.00437137])
</code></pre>
<p>vs:</p>
<pre><code>y_train.mean()
</code></pre>
<hr />
<pre><code>2.0682462451550387
</code></pre>
<p>which is already alarming.</p>
<p>To shed some light on what's going on:</p>
<pre><code>X_train.mean(0).values
</code></pre>
<hr />
<pre><code>array([0.23218765, 0.54154317, 0.03475851, 0.03985979, 0.00382413])
</code></pre>
<p>but</p>
<pre><code>explainer.masker.data.mean(0)
</code></pre>
<hr />
<pre><code>array([0.22695687, 0.53117647, 0.03449285, 0.03624149, 0.00379029])
</code></pre>
<p>which should hint you they applied masker and the masker data mean is surprisingly similar to what is actually used in SHAP values calculations (<a href=""https://github.com/shap/shap/blob/master/shap/explainers/_linear.py#L438"" rel=""nofollow noreferrer"">source code</a>):</p>
<pre><code>explainer.mean
</code></pre>
<hr />
<pre><code>array([0.22695687, 0.53117647, 0.03449285, 0.03624149, 0.00379029])
</code></pre>
<p>So, to reconcile what you see as outcome to SHAP value calculations you should account for the use of masker:</p>
<pre><code>#expected result
(X_test.loc[obs] - explainer.mean) * coeffs
</code></pre>
<hr />
<pre><code>MedInc        0.157576
HouseAge     -0.450652
AveRooms     -0.129481
Population    0.035684
AveOccup     -0.002117
dtype: float64
</code></pre>
<p>or simply use less than 100 datapoints from the very beginning to avoid use of masker altogether.</p>
","2","Answer"
"78798869","78780530","<p>The solution is to use a more modern Python-native alternative:</p>
<ul>
<li>Pathway (business source license, Python + Rust)</li>
<li>Bytewax (open source, Python + Rust)</li>
<li>Quix (available as open source, Python + C#)</li>
</ul>
","2","Answer"
"78798988","78780530","<p>The pattern most often seen is to deploy the <a href=""https://github.com/apache/flink-kubernetes-operator"" rel=""nofollow noreferrer"">flink operator</a> in kubernetes and then you run a task manager for every additional workload. The problem is that this is very resource intensive.</p>
","0","Answer"
"78800178","78796671","<blockquote>
<p>I expected [...] no splits would be made.</p>
<p>[...]</p>
<p>the subtrees all have no depth and no splits are made</p>
</blockquote>
<p>So you've already confirmed one of your suspicions. Is the only issue left for you why <code>score</code> looks so good? That's probably because <code>score</code> is the accuracy metric, and if your dataset is imbalanced and always predicting the majority class, then the accuracy is the majority's prevalence. (See stats.SE's <a href=""https://stats.stackexchange.com/q/312780/232706"">opinion of accuracy</a>.)</p>
","0","Answer"
"78801396","78800789","<p>You could switch to TensorFlow v2 and use the function <code>tf.compat.v1.placeholder</code>, however, as said in the <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder"" rel=""nofollow noreferrer"">documentation</a> try to use <code>tf.Keras.Input</code> instead migrating all code to v2.</p>
","0","Answer"
"78804631","78804130","<p>For some reason, despite the inputs being stored as a key-value, the order in which they are given to <code>model = models.Model</code> matters. If you reverse the order, and make it <code>model = models.Model(inputs=[Tperiod, AMP_WK, bathyZ], outputs=skew)</code>, it works.</p>
","0","Answer"
"78805247","78804884","<p>You need to tinker with the <code>cp</code> argument (complexity parameter), which controls the process of splitting each variable. The default is 0.01. If you set this to -1, and set the <code>maxdepth</code> argument to 3, then you get something more interesting, at least for a start.</p>
<pre><code>loans_model &lt;- rpart(default ~ loan_amount + credit_score + debt_to_income, 
                     data = loans, 
                     method = 'class',
                     cp=-1,
                     maxdepth = 3)

rpart.plot(loans_model, cex=0.7)
</code></pre>
<hr />
<p><a href=""https://i.sstatic.net/MBfbGRdp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MBfbGRdp.png"" alt=""enter image description here"" /></a></p>
<p>On page 21 of the <a href=""https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf"" rel=""nofollow noreferrer"">longintro.pdf</a>, &quot;<em>The default value (for cp) of .01 has been reasonably successful at ‘pre-pruning’ trees so that the cross-validation step need only remove 1 or 2 layers, but it sometimes over prunes, <strong>particularly for large data sets</strong></em>.&quot;</p>
","1","Answer"
"78808187","78804107","<p>Your main problem is that both alpha and the number of iterations are too small to get to the regression line.</p>
<p>You didn't provide your data, so it's difficult to test; however, when I made up some vaguely comparable data it did what you have shown with your existing code, but converged OK when both alpha and the number of iterations were multiplied by 10.</p>
<p>Ideally, you need a better criterion for convergence than just a fixed number of iterations. For example, you could stop when the relative change over one iteration becomes less than tolerance.</p>
<p>For this problem, of course, standard regression formulae would do the job admirably.</p>
","1","Answer"
"78808418","78807931","<p>Just read that the feature importance values are normalised to sum up to 100:
<a href=""https://catboost.ai/en/docs/concepts/fstr#regular-feature-importance"" rel=""nofollow noreferrer"">https://catboost.ai/en/docs/concepts/fstr#regular-feature-importance</a> (Specifics section).</p>
","0","Answer"
"78811554","78802566","<ul>
<li>Consider fine-tuning a model specific to the shelf images. Refine the method to detect gaps more accurately, taking into account overlapping objects and smaller gaps that might be missed.</li>
<li>Fine tune the model with a larger number of dataset labelled on empty places using azure ml labelling tool. Refer this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-label-data?view=azureml-api-2"" rel=""nofollow noreferrer"">doc1</a>, <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-image-labeling-projects?view=azureml-api-2"" rel=""nofollow noreferrer"">doc2</a></li>
</ul>
<p>Here I highlighted the image and increased contrast to get brighter and it help to detect the object effectively.</p>
<p><img src=""https://i.imgur.com/AD8zMQB.png"" alt=""enter image description here"" /></p>
<p><strong>Project structure:</strong></p>
<pre><code>shelf-recognition/
│
├── app.py
├── requirements.txt
├── templates/
│   ├── index.html
│   └── result.html
├── static/
│   ├── uploads/
│   └── css/
│       └── style.css
└── .env
</code></pre>
<p><strong>requirements.txt:</strong></p>
<pre><code>flask
azure-cognitiveservices-vision-computervision
msrest
opencv-python-headless
matplotlib
python-dotenv
</code></pre>
<p><strong>templates/index.html:</strong></p>
<pre class=""lang-html prettyprint-override""><code>&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;title&gt;Upload Image&lt;/title&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;{{ url_for('static', filename='css/style.css') }}&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;Upload Image for Shelf Product Recognition&lt;/h1&gt;
    &lt;form method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;
        &lt;input type=&quot;file&quot; name=&quot;file&quot;&gt;
        &lt;button type=&quot;submit&quot;&gt;Upload&lt;/button&gt;
    &lt;/form&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><strong>templates/result.html:</strong></p>
<pre class=""lang-html prettyprint-override""><code>&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;title&gt;Analysis Result&lt;/title&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;{{ url_for('static', filename='css/style.css') }}&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;Analysis Result&lt;/h1&gt;
    &lt;div&gt;
        &lt;img src=&quot;{{ url_for('static', filename='uploads/' + results['image_url']) }}&quot; alt=&quot;Result Image&quot;&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;h2&gt;Detected Objects:&lt;/h2&gt;
        &lt;ul&gt;
            {% for box in results['bounding_boxes'] %}
            &lt;li&gt;{{ box }}&lt;/li&gt;
            {% endfor %}
        &lt;/ul&gt;
        &lt;h2&gt;Empty Areas:&lt;/h2&gt;
        &lt;ul&gt;
            {% for area in results['empty_areas'] %}
            &lt;li&gt;{{ area }}&lt;/li&gt;
            {% endfor %}
        &lt;/ul&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><strong>Application running successfully:</strong></p>
<p><img src=""https://i.imgur.com/RtDDHEu.png"" alt=""enter image description here"" /></p>
<p><strong>Uploading image:</strong></p>
<p><img src=""https://i.imgur.com/GF8Un7l.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/mJW5vfb.png"" alt=""enter image description here"" /></p>
","0","Answer"
"78811718","78790401","<p>As suggested by @Alex Alex, <a href=""https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html"" rel=""nofollow noreferrer"">Optical Flow</a> is certainly the way to go.</p>
<p>The idea of the code bellow is to compute Optical Flow between the two images, then to take a few points of the contours of each cell and average the flow of these point to obtain a &quot;cell flow&quot;. This flow is then displayed.</p>
<p>The result for your two images:
<a href=""https://i.sstatic.net/t80plAyf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t80plAyf.png"" alt=""enter image description here"" /></a></p>
<pre><code>def get_contour_center(contour):
    m = cv.moments(contour)
    if m['m00'] != 0: 
        cx = int(m['m10']/m['m00'])
        cy = int(m['m01']/m['m00'])

        return cx, cy
    return None

def draw_arrow(img, x, y, fx, fy, arrow_length_coef=2):
    lines = np.int32(np.array([x, y, x + fx*arrow_length_coef, y + fy*arrow_length_coef]) ).reshape(-1, 2, 2)

    cv.line(img, (int(x),int(y)),(int(x + fx*arrow_length_coef), int(y + fy*arrow_length_coef)), (0, 255, 0),1) 
    # cv.polylines(img, lines, 0, (0, 255, 0))
    cv.circle(img, (x, y), 1, (0, 255, 0), -1)
    return img

def draw_cell_movement(img1, img2, filtered_contour_list, result_image):

    img1_gray = cv.cvtColor(img1, cv.COLOR_BGR2GRAY) 
    img2_gray = cv.cvtColor(img2, cv.COLOR_BGR2GRAY) 

    # Calculates dense optical flow by Farneback method 
    flow = cv.calcOpticalFlowFarneback(img1_gray, img2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0) 

    for cnt in filtered_contour_list:
        
        # Take a few point of cell contour (here six at most)
        ids = np.random.randint(0,cnt.shape[0],min(cnt.shape[0],6))
        cell_flow = np.zeros(2)
        for id in ids:
            x, y = cnt[id,:,:][0]
            fx, fy = flow[y, x].T

            cell_flow += flow[y, x]
        cell_flow /= len(ids)

        # find cell center and draw arrow direction
        center = get_contour_center(cnt)
        if center:
            cx, cy = center
            result_image = draw_arrow(result_image, x, y, fx, fy)
    return result_image

img1 = cv.imread(&quot;/mnt/d/wsl/experiments/trasj/bac1.jpg&quot;)
img2 = cv.imread(&quot;/mnt/d/wsl/experiments/trasj/bac2.jpg&quot;)
result_image, filtered_contour_list = process_image(img1, size_threshold)
result_image = draw_cell_movement(img1, img2, filtered_contour_list, result_image)

plt.imshow(result_image)
plt.show()
</code></pre>
<p>Note that given that your contours are actually ellipses there is a smarter way to sample the contours points and to find the center.
Also I have modified your <code>process_image()</code> so that it also return the contours you used to draw your ellipses.</p>
","0","Answer"
"78813347","78804107","<p>The code you wrote seems generally correct. I think the compute_cost, compute_gradient and gradient_descent functions could be written more optimized. Also, you have kept the Alpha (learning rate) and iteration rate very low, so it may not be updating itself sufficiently. My advice to you is to try again with alpha = 0.001 and num_iterations = 10000. After you have optimized the necessary functions and updated the variables as I said, you will get an output like this.<a href=""https://i.sstatic.net/xPfusLiI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xPfusLiI.png"" alt=""Output"" /></a>.</p>
<p>By the way, the functions are correct, they just need a little improvement.</p>
","0","Answer"
"78816369","78752483","<p>If you are using Linux, I recommend trying FireDucks, an accelerator that optimizes any pandas workload without code changes. <a href=""https://fireducks-dev.github.io/"" rel=""nofollow noreferrer"">https://fireducks-dev.github.io/</a></p>
<p>FireDucks can efficiently address existing bottlenecks in your code and memory issues during execution. I am one of the developers of this library. Feel free to contact me with any queries you may have.</p>
","0","Answer"
"78816670","78815544","<p>Once you fuse the model you don't want to specify the adapter path otherwise it will try to add adapters to an already fused model (which is a bug).</p>
<p>Try using:</p>
<pre><code>mlx_lm.generate --model new_model --prompt &quot;Tell sth about New York&quot; --temp 0.01
</code></pre>
<p>Also fusing can cause some degradation. The adapted weights are: <code>W = W + scale * b^T a</code>. When you fuse <code>b^T a</code> into <code>W</code> it can be destructive if the adapter (<code>b^T a</code>) has very different magnitude than the base weights (<code>W</code>), particularly when using quantized or low precision base weights.</p>
<p>Tuning the <code>scale</code> parameter can improve the model performance after fusion.</p>
","2","Answer"
"78816764","78816668","<p>I think you could solve this using the <a href=""https://scikit-image.org/docs/stable/api/skimage.draw.html#skimage.draw.line"" rel=""nofollow noreferrer"">skimage.draw.line</a> method where you input the starting (i,j) and ending (0,0) coordinates and it calculates the indices belonging to the line which you can use for indexing.</p>
<p>Worked example:</p>
<pre><code>from skimage.draw import line
import numpy as np
import matplotlib.pyplot as plt
arr = np.zeros((100, 100))
i, j = 10, 80
origin = 50, 50
rr, cc = line(*origin, i, j)
arr[rr, cc] = 1
plt.imshow(arr, cmap='gray', interpolation='nearest')
</code></pre>
<p><a href=""https://i.sstatic.net/bZlfG53U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZlfG53U.png"" alt=""Gray plot showing the line"" /></a></p>
<p>Or if you want the line to continue:</p>
<pre><code>arr = np.zeros((100, 100))
rr, cc = line(i, j, 2*origin[0]-i, 2*origin[1]-j)
arr[rr, cc] = 2
plt.imshow(arr, cmap='gray', interpolation='nearest')
</code></pre>
<p><a href=""https://i.sstatic.net/bZqttOKU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZqttOKU.png"" alt=""Longer line"" /></a></p>
","1","Answer"
"78817613","78816835","<p>You can use <code>ImageDataGenerator</code>:</p>
<pre><code>from keras.preprocessing.image import ImageDataGenerator
</code></pre>
","0","Answer"
"78817614","78802177","<p>Hi huge thanks to @ChristophRackwitz that suggested to disable this piece of the code</p>
<pre class=""lang-cs prettyprint-override""><code>image.Draw(results);

    image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
        SKEncodedImageFormat.Jpeg);
</code></pre>
<p>After disabling this part the timings per detection started to be on point.
I'm not really sure why but my suspect is that the Skiasharp for all the I/O
operations generated a huge use of memory slowing down the image passing from the CPU to the GPU.</p>
<p>This is the code that I have written to have as much things ready and to not do nothing else than detections in the loop.</p>
<p>I have also forced the disposal of the image after the loop to be sure to have freed the memory for the next model inference.</p>
<pre class=""lang-cs prettyprint-override""><code>// Load all images into memory
for (var i = 1; i &lt; 500; i++)
{
    string file = $@&quot;C:\Users\Eduard\Documents\assets\images\input\frame_{i}.jpg&quot;;
    using var stream = new FileStream(file, FileMode.Open, FileAccess.Read);
    var image = SKImage.FromEncodedData(stream);
    images.Add(i, image);
}

// Run inference on in-memory images
foreach (var kvp in images)
{
    int i = kvp.Key;

    sw.Restart();
    var results = yolo.RunObjectDetection(kvp.Value, confidence: 0.25, iou: 0.7);
    sw.Stop();
    Console.WriteLine($&quot;Time taken for image {i}: {sw.Elapsed.TotalMilliseconds:F2} ms&quot;);
    rawResults.Add((i, results, sw.Elapsed.TotalMilliseconds));
}

// Dispose of images to free memory
foreach (var kvp in images)
{
    kvp.Value.Dispose();
}

// Process results outside the loop
foreach (var rawResult in rawResults)
{
    int imageIndex = rawResult.imageIndex;
    var results = rawResult.results;
    double elapsedMilliseconds = rawResult.elapsedMilliseconds;

    times.Add(elapsedMilliseconds);

    if (results.Count &gt; 0)
    {
        var labels = new List&lt;string&gt;();
        var confidences = new List&lt;double&gt;();
        foreach (var result in results)
        {
            labels.Add(result.Label.Name.ToString());
            confidences.Add(result.Confidence);
        }
        detections.Add((
           imageIndex,
           labels,
           confidences
         ));
    }
}
</code></pre>
<p>Now the times for the YOLOv8 size m are</p>
<ul>
<li>Total time of execution <code>11825.34</code>ms</li>
<li>And an average time per image of <code>23.70</code>ms</li>
</ul>
","0","Answer"
"78820550","78816835","<p>You can use layers, e.g. the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation"" rel=""nofollow noreferrer"">RandomRotation</a> layer. I think every operation you listed expect for random shear is available as a TensorFlow layer now. Random shear is available as a layer in the <a href=""https://keras.io/api/keras_cv/layers/augmentation/random_shear/"" rel=""nofollow noreferrer"">keras-cv</a> package.</p>
<p>You could add these layers at the beginning of your model directly, or create a separate model with these preprocessing layers, which you can add as a sub-model. By default, these augmentations are only applied in training, so your test set (or training set in <code>model.evaluate(train)</code>) will not be affected.</p>
","0","Answer"
"78821012","78820748","<p>I was facing same problem with yolov8n.pt model. I tried with downgrading ultralytics version from 8.2.70 to 8.2.60. The issue is resolved now.</p>
","7","Answer"
"78821115","78821038","<p>Given that it’s an 8-billion-parameter model, requiring approximately 16GB of space, free Colab notebooks lack the capacity to load it.</p>
","0","Answer"
"78824727","78820971","<p>It works now. I went through the instructions again from the beginning and downloaded everything again. The model file on my computer was somehow corrupt.</p>
","0","Answer"
"78828011","78827762","<p>I remember when I had a 12 GB Image Dataset for Object detection in an Autonomous car, I had to split it into <code>Train</code> and <code>Test</code> Dataset</p>
<p>I used the below code
Just add your path in Pycharm.</p>

<pre><code>import os
import random

def split_large_file(input_file_path, train_file_path, test_file_path, test_ratio=0.2, buffer_size=100000):
    train_file = open(train_file_path, 'w')
    test_file = open(test_file_path, 'w')

    with open(input_file_path, 'r') as input_file:
        buffer = []
        for i, line in enumerate(input_file):
            buffer.append(line)
            if len(buffer) &gt;= buffer_size:
                for buf_line in buffer:
                    if random.random() &lt; test_ratio:
                        test_file.write(buf_line)
                    else:
                        train_file.write(buf_line)
                buffer = []
        
        for buf_line in buffer:
            if random.random() &lt; test_ratio:
                test_file.write(buf_line)
            else:
                train_file.write(buf_line)

    train_file.close()
    test_file.close()

# you can see the below 
input_file_path = 'large_input_file.txt'
train_file_path = 'train_set.txt'
test_file_path = 'test_set.txt'
split_large_file(input_file_path, train_file_path, test_file_path)
</code></pre>
<p>Let me know if you face any error</p>
","0","Answer"
"78828112","78827762","<p>Here is a quick solution. There are probably some improvements that can be made. The solution assumes that each line in your <code>.txt</code> file constitutes one training/test sample. It will create a <code>train.txt</code> file and <code>test.txt</code> file by random selection with the desired percentage of training samples and the rest test.</p>
<pre class=""lang-py prettyprint-override""><code>import random

# Set seed for reproducibility if desired
random.seed(0)

# Get line count of file
line_count = 0
with open(&quot;large.txt&quot;, &quot;rt&quot;) as file:
    for line in file:
        line_count += 1

# Determine number for train and test examples based on line count
train_percentage = 0.8
train_count = int(train_percentage * line_count)
test_count = line_count - train_count

# Create selector to determine which lines will be written to train or test files
selector = [&quot;train&quot;] * train_count + [&quot;test&quot;] * test_count
# Randomly shuffle
random.shuffle(selector)

# Create file names for train and test
train_fname = &quot;train.txt&quot;
test_fname = &quot;test.txt&quot;

# Create train and test files
with (
    open(&quot;large.txt&quot;, &quot;rt&quot;) as file,
    open(train_fname, &quot;wt&quot;) as train_file,
    open(test_fname, &quot;wt&quot;) as test_file,
):
    for idx, line in enumerate(file):
        if selector[idx] == &quot;train&quot;:
            train_file.write(line)
        elif selector[idx] == &quot;test&quot;:
            test_file.write(line)
        else:
            raise Exception(&quot;Option not recognised. Check options and spelling&quot;)
</code></pre>
","0","Answer"
"78828140","78752483","<h2>Memory management in Pandas</h2>
<p><em>The Kaggle book</em> suggests modifying data types based on the machine limits for floating and integer types to reduce memory usage in Pandas.</p>
<pre class=""lang-py prettyprint-override""><code>def reduce_mem_usage(df, verbose=True):
    numerics = [&quot;int16&quot;, &quot;int32&quot;, &quot;int64&quot;, &quot;float16&quot;, &quot;float32&quot;, &quot;float64&quot;]
    start_mem = df.memory_usage().sum() / 1024**2
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == &quot;int&quot;:
                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if (
                    c_min &gt; np.finfo(np.float32).min
                    and c_max &lt; np.finfo(np.float32).max
                ):
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose:
        print(
            &quot;Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)&quot;.format(
                end_mem, 100 * (start_mem - end_mem) / start_mem
            )
        )
    return df
</code></pre>
<h2>Alternative to Pandas for handling large datasets</h2>
<p><strong>1. Polars</strong>: A DataFrame library written in Rust that aims to provide fast performance and a low memory footprint. <a href=""https://pola.rs/posts/benchmarks/"" rel=""nofollow noreferrer"">According to the benchmarks</a>, Polars achieve more than <strong>30x performance gains</strong> compared to Pandas.</p>
<p><strong>2. RAPIDS</strong>: An open-source suite of data science and analytics libraries that leverages <strong>NVIDIA GPUs</strong> for accelerating data processing workflows. Within RAPIDS, <code>cuDF</code> is a GPU DataFrame library that offers a pandas-like API but with the computational advantages of GPU acceleration. It is not limited to data processing and you can also speed up the training of ML models. For instance, you can train a <code>RandomForest</code> on GPU using RAPIDS: <a href=""https://www.kaggle.com/code/titericz/randomforest-on-gpu-in-3-minutes"" rel=""nofollow noreferrer"">Kaggle Notebook</a>.</p>
","0","Answer"
"78829601","78823685","<p>One possible solution is to call CrossEntropyLoss once for each label/target and add up the losses. This also works if the labels have different numbers of classes.</p>
<p>Here is an example using your data:</p>
<pre><code>inputs = torch.tensor([
    [5.65, 3.56, 0.94, 9.23, 6.43],
    [7.43, 3.95, 1.24, 7.22, 2.66],
    [9.31, 2.42, 2.91, 2.64, 6.28],
    [8.19, 5.12, 1.32, 3.12, 8.41],
    [9.35, 1.92, 3.12, 4.13, 3.14],
    [8.43, 9.72, 7.23, 8.29, 9.18],
    [4.32, 2.12, 3.84, 9.42, 8.19],
    [3.92, 3.91, 2.90, 8.19, 8.41],
    [7.89, 1.92, 4.12, 8.19, 7.28],
    [5.21, 2.42, 3.10, 0.31, 1.31]]) #samples,features
targets = torch.tensor([
    [0, 2, 1],
    [0, 0, 0],
    [2, 0, 2],
    [0, 2, 0],
    [0, 1, 1],
    [1, 0, 2],
    [0, 1, 0],
    [2, 0, 2],
    [0, 1, 2],
    [2, 0, 0]]) #samples,labels
manual_class_weight = torch.tensor([0.425,0.15,0.425]) #classes
class_counts = torch.stack([label.unique(return_counts=True)[1] \
                           for label in targets.T]) #labels,classes
print(class_counts)
class_weights = manual_class_weight.unsqueeze(0)/class_counts #labels,classes

torch.manual_seed(0)
model = nn.Linear(5,3*3) #features -&gt; labels*classes
optimizer = torch.optim.Adam(model.parameters(),lr=0.1)
for epoch in range(100):
    predicted_logits = model(inputs).view(-1,3,3) #samples,labels,classes
    loss = sum(F.cross_entropy(predicted_logits[:,i_label],
                            targets[:,i_label],
                            class_weights[i_label]) \
            for i_label in range(3))
    if epoch%10==0:
        accuracy = (predicted_logits.argmax(dim=2)==targets).float().mean()
        print(f&quot;epoch: {epoch}  loss: {loss:.2f}  accuracy: {accuracy:.2f}&quot;)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
<p>This code results in:</p>
<pre><code>tensor([[6, 1, 3],
        [5, 3, 2],
        [4, 2, 4]])
epoch: 0  loss: 13.68  accuracy: 0.20
epoch: 10  loss: 3.59  accuracy: 0.57
epoch: 20  loss: 2.00  accuracy: 0.63
epoch: 30  loss: 1.51  accuracy: 0.73
epoch: 40  loss: 1.22  accuracy: 0.83
epoch: 50  loss: 1.10  accuracy: 0.83
epoch: 60  loss: 1.02  accuracy: 0.87
epoch: 70  loss: 0.97  accuracy: 0.83
epoch: 80  loss: 0.93  accuracy: 0.90
epoch: 90  loss: 0.90  accuracy: 0.87
</code></pre>
<p>If all labels have the same number of classes and no (or equal) class weights, then this can be done in parallel with only one call (<a href=""https://discuss.pytorch.org/t/crossentropyloss-for-multiple-output-classification/111660"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/crossentropyloss-for-multiple-output-classification/111660</a>). I wrote some code that achieves parallelism in your case and gives the exact same output as before, but I am not sure whether it is actually faster:</p>
<pre><code>unreduced_loss = F.cross_entropy(
    predicted_logits.permute(0,2,1), #bc. class-dim expected as second!
    targets,reduction=&quot;none&quot;) #samples,labels
weight = class_weights[torch.arange(3),targets] #samples,labels
loss = (unreduced_loss*weight/weight.sum(0)).sum()
</code></pre>
<p>Often it is also suggested to use Binary Cross Entropy for multi-label classification, but I think they aren't mathematically equivalent.</p>
","0","Answer"
"78832584","78832537","<p>You should be splitting a vector, typically an index vector to use as the split, not a data frame.</p>
<pre><code>splits &lt;- createDataPartition(1:nrow(data.full), p = 0.8, list = FALSE)

training.sample &lt;- data.full[splits[,1],]
test.sample &lt;- data.full[setdiff(1:nrow(data.full), splits[,1]),]

Or

test.sample &lt;- dplyr::anti_join(data.full, training.sample)
</code></pre>
<hr />
<pre><code>dim(training.sample)
#[1] 64  4

dim(test.sample)
#[1] 13  4
</code></pre>
","2","Answer"
"78833299","78831225","<p>The generator starts with <code>layers.Dense(8*8*128)</code>, so 8 pixel × 8 pixel images. Each application of <code>Conv2DTranspose</code> with <code>strides=2</code> and <code>padding='same'</code> doubles the size of the image. There are three layers, so the output is 8 pixels × 2<sup>3</sup> = 64 pixels by 64 pixels.</p>
<p>So if you add another layer, you will double again and get 128 × 128 pixels. Or, if you use a stride of 3 then each layer will inflate by a factor of 3. Or, if you changing the padding strategy, you can add pixels at the edges.</p>
<p>Remember that changes you make to the generated images may need corresponding changes to the discriminator.</p>
<p>To build some intuition, I suggest reading about convolution and &quot;deconvolution&quot; (really transposed convolution, not the same as deconvolution in signal processing) and playing a bit with different arguments. To get you started, <a href=""https://arxiv.org/pdf/1603.07285v1"" rel=""nofollow noreferrer"">here is an awesome tutorial (PDF)</a> and <a href=""https://stackoverflow.com/a/68980531/20623798"">here is one SO answer</a> that might help; there may be others.</p>
","0","Answer"
"78833339","78831225","<p>In short, to make the generator generates images with larger dimension, you can add more <code>Conv2DTranspose</code> layer or increase the strides and/or kernel size.</p>
<p>Conv2D reduces the dimension of the input, e.g. the feature of shape <code>(64, 64, 3)</code> after being processed by Conv2D with 64 kernels, <code>kernel_size=4</code>, and <code>strides=2</code> will become of shape <code>(30, 30, 64)</code>. <a href=""https://stackoverflow.com/questions/53580088/calculate-the-output-size-in-convolution-layer"">Here's</a> a realted question regarding how to calculate the output size of a convolution.</p>
<p>Meanwhile, <code>Conv2DTranspose</code> does regular convolution but &quot;reverses&quot; the dimensional reduction so if you have an input of shape <code>(30, 30, 64)</code> and <code>Conv2DTranpose</code> with the same configuration as the Conv2D before, you would get an output of the original shape <code>(64, 64, 3)</code>. I found <a href=""https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d"" rel=""nofollow noreferrer"">this</a> helpful article to learn about the concepts of convolution in machine learning.</p>
<p>Your generator starts with an input of shape <code>(128,)</code> and you expect it to have an output of shape <code>(512, 512, 3)</code>. You can reverse your thought process and start backward. How do you arrange the <code>Conv2D</code> layers in a model so that it eventually outputs an array of size <code>(128,)</code>. Then you can try reverse the process by changing the <code>Conv2D</code> layers into <code>Conv2DTranspose</code> and reverting the sequence.</p>
","0","Answer"
"78834005","78687394","<p>The only layers that change the output shape in your <code>Generator</code> network are the <code>nn.ConvTranspose2d</code> layers. The other layers, including <code>BatchNorm2d</code>, <code>nn.ReLU</code>, <code>nn.Tanh</code> have no impact on the output shapes. Thus, your question essentially boils down to how to calculate the output shape of a <code>nn.ConvTranspose2d</code> layer?</p>
<p>Here is how: given input tensors with shape (N, C_in, H_in, W_in), where N, C_in, H_in and W_in are the input batch size, channel, height, and width respectively, you can calculate the output tensor shape (N, C_out, H_out, W_out) using the following formula:</p>
<pre><code>H_out = (H_in − 1) × stride[0] − 2 × padding[0] + dilation[0] × (kernel_size[0] − 1) + output_padding[0] + 1
W_out = (W_out − 1) × stride[1] − 2 × padding[1] + dilation[1] × (kernel_size[1] − 1) + output_padding[1] + 1
</code></pre>
<p>Where the parameters including C_in, C_out, kernel_size, stride, dilation, padding,... are set when defining the <code>nn.ConvTranspose2d</code> layer. You can read more about each of them on <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html"" rel=""nofollow noreferrer"">PyTorch documentations</a>.</p>
<p>For example your test input is (num_test, 10, 1, 1), and your <code>test_hidden_block</code> layer has <code>C_in=10</code>, <code>C_out=20</code>, <code>kernel_size=(4, 4)</code>, <code>stride=(1, 1)</code>, <code>padding=(0, 0)</code>, so:</p>
<pre><code>H_out = (1 - 1) × 1 - 2 × 0 + 1 × (4 - 1) + 0 + 1 = 4
W_out = (1 - 1) × 1 - 2 × 0 + 1 × (4 - 1) + 0 + 1 = 4
</code></pre>
<p>Hence the output shape of this layer is (num_test, 20, 4, 4).</p>
<p>I find the visual representations of this <a href=""https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11"" rel=""nofollow noreferrer"">blog</a> on transposed convolutions very helpful.</p>
","0","Answer"
"78834118","78833739","<p>I'm a beginner too. My first instinct was to suggest a pipeline, but if you <strong>need</strong> to run two functions concurrently, maybe you need a parallel processing tool like joblib?</p>
<pre><code>from joblib import Parallel, delayed
res = Parallel(n_jobs=2)(delayed(f)() for f in [game, browser])
</code></pre>
<p><a href=""https://docs.python.org/3/library/concurrent.futures.html"" rel=""nofollow noreferrer"">You could also try your luck with the concurrent module.</a></p>
","1","Answer"
"78834424","78833727","<p>If you're using Google Colab, files stored in the runtime can get lost when the runtime gets disconnected. You can instead store your model on Google Drive and load it from there. Here is an example <a href=""https://colab.research.google.com/notebooks/io.ipynb#scrollTo=c2W5A2px3doP"" rel=""nofollow noreferrer"">notebook</a> which demonstrates this integration with Google Drive.</p>
","1","Answer"
"78835127","78828998","<p>Have you tried using <a href=""https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.ZZFeatureMap"" rel=""nofollow noreferrer"">ZZFeatureMap</a>? The issue here is how you map your data onto the circuit. You want to map [x1,x2] onto some 1D label, which means that our featuremap should just have two tunable variables, while your feature_map circuit has more than two parameters, thus there is a mismatch between expected data shape and what you are giving it.</p>
<p>To put what I'm saying in code, replace your featuremap to be</p>
<pre><code>feature_map = ZZFeatureMap(X_train.shape[1], entanglement='linear')
</code></pre>
<p>(I'm avoiding using <code>num_qubits</code> here as its more modelling data points/parameters of gate rotations and not individual qubits).</p>
<p>It should still be possible to use two local as a feature map, but you have to cautious on how you map the data onto the gates such that there exists only two varaibles. Here is an example I modified from the <a href=""https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.TwoLocal"" rel=""nofollow noreferrer"">documentation</a>.</p>
<pre><code>entangler_map = [[0, 1], [1,0]] #size should be num of params
feature_map = TwoLocal(2, 'x', 'crx', entangler_map, reps=1)
</code></pre>
<p>This featuremap might not be of the best design, but it shouldn't throw an error. You can increase the qubits and try messing around with what they have in the documentation linked above.</p>
","0","Answer"
"78835757","78835539","<p>If you could define a threshold, there's a potential for you to find the peaks and then test for an anomaly given a set of peaks:</p>
<pre><code>from scipy.signal import find_peaks
peaks, _ = find_peaks(df[&quot;Ref Wt. Diff&quot;], height=threshold)  
df[&quot;Peak Indicator&quot;] = 0  # init the Peak Indicator column
df.loc[peaks, &quot;Peak Indicator&quot;] = 1  # Mark the peaks
peak_data = df[df[&quot;Peak Indicator&quot;] == 1]
if not peak_data.empty:
    df[&quot;WOB Anomaly&quot;] = np.nan  # init anomaly column
    df.loc[peak_data.index, &quot;WOB Anomaly&quot;] = detect_wob.predict(peak_data[&quot;Ref Wt. Diff&quot;].values.reshape(-1, 1))
</code></pre>
","0","Answer"
"78836647","78758312","<pre><code>I solved the error with this, but still not retrieving.



from dspy.retrieve.qdrant_rm import QdrantRM
from dsp.modules.sentence_vectorizer import FastEmbedVectorizer

# Vectorizer Setup
vectorizer = FastEmbedVectorizer('BAAI/bge-base-en-v1.5') # 768

# Qdrant Retriever Model Setup
qdrant_retriever_model = QdrantRM(
    COLLECTION_NAME,
    client,
    k=5,  # Increase k to fetch more passages
    vectorizer=vectorizer,
    vector_name='default' # change for your vector name.
)
</code></pre>
","0","Answer"
"78837159","78837133","<p>It looks like your model <code>output</code> parameter has 2 entries in its list</p>
<pre><code>model=Model(inputs=[inputs],outputs=[output_1,output_2])
</code></pre>
<p>while in your <code>model.compile</code>, <code>metrics</code> parameter only has 1 entry in the list</p>
<pre><code>model.compile(loss=[&quot;binary_crossentropy&quot;,&quot;mae&quot;],optimizer=&quot;Adam&quot;,metrics=[&quot;accuracy&quot;])
</code></pre>
<p>You need to keep length of two <code>outputs</code> and <code>metrics</code> consistent with each other, either by remove one of the entry in <code>outputs</code> list or add another entry to the list in <code>metrics</code> parameter. You can learn more about the <code>model.compile</code> method metrics <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile"" rel=""nofollow noreferrer"">here</a></p>
","0","Answer"
"78839646","78839421","<p>Two points here:</p>
<pre><code>model = MyTreeClf(max_depth=3, min_samples_split=2, max_leafs=1)
</code></pre>
<p>will trigger a return in the fit due to the <code>max_leafs &lt;2</code> condition
putting it to two will allow the tree to be build</p>
<p>i do not have your data set, so we'll test with a random one</p>
<pre><code>df = pd.DataFrame(columns=['variance', 'skewness', 'curtosis', 'entropy', 'target'],data=np.random.random(size=(500, 5)))
df['target'] =df['target'].apply(lambda x: 0 if x&lt;0.5 else 1)
X, y = df.iloc[:, :4], df['target']
</code></pre>
<p>and then, look at the tree of you model to see if it is built</p>
<pre><code>model = MyTreeClf(max_depth=3, min_samples_split=2, max_leafs=2)
model.fit(X, y)
print(model.tree)
</code></pre>
<p>it gives us a tree and as such the node structure will work
you can now run the <code>model.predict_proba(X)</code> which gives an array, hence the p is a value and not a list, you need to modify the function predict:</p>
<pre><code>def predict(self, X: pd.DataFrame):
    return [1 if p &gt; 0.5 else 0 for p in self.predict_proba(X)]
</code></pre>
","0","Answer"
"78841779","78841736","<p>Normally, if you want to host a web application that can store large binary file, you would need a <a href=""https://en.wikipedia.org/wiki/Object_storage"" rel=""nofollow noreferrer"">blob storage</a> database. There are various enterprise solutions like <a href=""https://aws.amazon.com/s3/"" rel=""nofollow noreferrer"">AWS S3</a>, <a href=""https://azure.microsoft.com/en-us/products/storage/blobs"" rel=""nofollow noreferrer"">Azure Blob Storage</a>, Google Cloud that provides infrastructure to store your data that you can just load  to use when you needed. However, this might be a bit costly and also not too efficient when the model size is too large.</p>
<p>Another alternative that I can think of is having a separate running backend server that has readily access to the model and can accept incoming requests from the frontend server containing user input parameters and then use the input parameters to generate desired output as a response to frontend server request. That way, you don't need to load the model everytime users visit your web application route and instead forward the request to the server that already has the model loaded to handle the request. In docker terms, It would mean you need to create a separate image to load the model and handle frontend incoming requests and have its running container exposed to the image that is running the flask web application server so that it can forward requests and receive response from that container</p>
","0","Answer"
"78843418","78842299","<p>Here's what I was alluding to in the comments. Select the black pixels, dilate them a little, then merge them back into original image:</p>
<pre><code>#!/usr/bin/env python3

import cv2 as cv
import numpy as np

# Load image
im = cv.imread('scene.png', cv.IMREAD_COLOR)

# Select black pixels
blacks = ( np.all(im == (0, 0, 0), axis=-1) * 255 ).astype(np.uint8)
cv.imwrite('DEBUG-blacks-before.png', blacks)

# Dilate blacks with a circular structuring element
SE = cv.getStructuringElement(cv.MORPH_ELLIPSE, (4,4))
blacks = cv.morphologyEx(blacks, cv.MORPH_DILATE, SE)
cv.imwrite('DEBUG-blacks-after.png', blacks)

# Select darker pixel at each location - the tilde (~) means the inverted image
res = np.minimum(im, ~blacks[..., np.newaxis])
cv.imwrite('result.png', res)
</code></pre>
<p><a href=""https://i.sstatic.net/Kn1OLPdG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kn1OLPdG.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>Here is an animation, alternating between the two debug images, showing how the dilation affects the blacks:</p>
<p><a href=""https://i.sstatic.net/wjMjDS4Y.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wjMjDS4Y.gif"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Notes:</strong></p>
<ol>
<li><p>Now you can see how dilation works, you can adapt it to Christoph's other suggestions.</p>
</li>
<li><p>The <code>np.newaxis</code> thing makes a [M,N] single colour channel array compatible with an [M,N,3] 3-colour channel array.</p>
</li>
<li><p>Make the structuring element larger to fatten the black lines even more, e.g. <code>cv.getStructuringElement(cv.MORPH_ELLIPSE, (5,5))</code></p>
</li>
</ol>
","1","Answer"
"78843773","78687946","<p>Tracking from &quot;sort.py&quot; file is CPU based. Instead, you can use &quot;model.track&quot; from Ultralytics, which will significantly reduce CPU usage.</p>
","0","Answer"
"78844000","78843004","<p>There are some things you could check to see where exactly is the problem. What environment manager do you use? If you are using Conda for example make sure you have your env active when trying to run code. Using virtual env with Conda worked fine for me so this should solve the issues.</p>
<p>Here are some long shots that you might try:</p>
<ol>
<li><p>Check the structure of you project. It does seem like the whole 'datachain' is not being found as library but this might be useful to see if structure is like you have in your imports.</p>
</li>
<li><p>I had this problem some years ago on one of my old machines when I wasn't using any env manager that some libraries would be a problem to run after some time. Meaning that everything would be just saved with ´pip install x´ to the same env. Again pointing to use of virtual envs to solve the issue</p>
</li>
</ol>
","0","Answer"
"78844464","78841275","<p>You can find those details at the <a href=""https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters"" rel=""nofollow noreferrer"">model parameters</a> documentation.</p>
<p>But in a short:</p>
<ul>
<li><code>max output tokens</code> limits the response max length. You literally limit how short (or long) you want your answer in tokens. Roughly speaking, just as a reference, 100 tokens is around 60-80 words.</li>
</ul>
<p>Gemini is a generative model which means that, in a high level explanation, it &quot;composes&quot; (or generates) an answer given its semantic knowledge in a given language (being a spoken language, a programming language, etc). So basically you can imagine a bag of possible &quot;next tokens&quot; when writing a sentence and top-k and top-p will customize the possible vocabulary to be considered.</p>
<ul>
<li><p>with <code>top-k</code> basically you limit the possible tokens universe. If the next tokens can be 200 possible different ones, you limit in the top first k ones. So <code>top-k = 30</code> means that the model you consider the first 30 tokens in the possible list. but the next tokens is not picked yet at this step.</p>
</li>
<li><p>with <code>top-p</code> you will work on a limit based on the cumulative probability. meaning: each token will have a probability related to how often the model saw the previous token followed by this token. So if you define <code>top-p = 20</code> each means that from the 30 token you limited with <code>top-k</code>, it will generate a new list with the tokens that sum a max probability of 20%. ie. if the first token has a 10% probability, the second has 5%, the third has 4% and the fifth has 2% - the list after the top-p analysis will contain the first, the second and the third (10% + 5% + 4%). Yet the next token is not picked in the step too.</p>
</li>
<li><p>finally comes the <code>temperature</code> parameter which defines how deterministic the next token will be picked. A temperature equals to 0 drives the more deterministic choice where the token with higher priority will be chosen; temperature at maximum will be the more random choice of next token, which means that even the less probable token may be chosen too.</p>
</li>
</ul>
<p>hope that helps.</p>
","0","Answer"
"78844884","78827482","<p>Before loading from pretrained model set transformers logger level to error as shown below. It sure is really frustrating not being able to leverage the <code>warnings</code> library filter</p>
<pre><code>    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
    for logger in loggers:
        if &quot;transformers&quot; in logger.name.lower():
            logger.setLevel(logging.ERROR)

    # now you can load state dict from pretrained
    model = transformers.BertModel.from_pretrained(
        &quot;bert-base-uncased&quot;,
        use_safetensors=True,
        return_dict=False,
        attn_implementation=&quot;sdpa&quot;,
    )
</code></pre>
","2","Answer"
"78845492","78844901","<p>I have seen that the mathematical formula used is to implement a linear regression model? For clarification, if the data is not suited for linear regression, then the model will definitely struggle since the parameters you've set are relatively  rigid.</p>
<p>Gradient Calculation: The calculation for the gradients appears to be correct, but it’s always good to double-check the formulas. My point is that maybe you can clarify a bit more? Here are the code changes I made:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import time
import random

# Veri seti
data = {'study_time': [1, 2, 3, 4, 5], 'score': [2, 4, 9, 16, 25]}
points = pd.DataFrame(data)

print(points)

# Gradient Descent Fonksiyonu
def gradient_descent(m_next, b_next, points, L):
    m_gradient = 0
    b_gradient = 0
    n = len(points)

    for i in range(n):
        x = points.iloc[i].study_time
        y = points.iloc[i].score
    
    m_gradient += -(2/n) * x * (y - (m_next * x + b_next))
    b_gradient += -(2/n) * (y - (m_next * x + b_next))

    m = m_next - (L * m_gradient * 0.0001) #(L = 0.0001)
    b = b_next - (L * b_gradient * 0.1 )   #(L = 0.1)

    return m, b

# Grafik Gösterim Fonksiyonu
def show_graph(m, b):
    plt.scatter(points.study_time, points.score, color=&quot;red&quot;)
    x_range = range(int(points.study_time.min()), int(points.study_time.max()) + 1)
    plt.plot(x_range, [m * x + b for x in x_range], color=&quot;blue&quot;)
    plt.xlabel('Study Time')
    plt.ylabel('Score')
    plt.title('Study Time vs Score')
    plt.show()
    time.sleep(0.001)
    print(&quot;=&gt;  F(X):&quot;, round(m, 1), &quot;X +&quot;, round(b, 3))

# Initial values for m and b
m_next = 0
b_next = 0

# Single learning rate
L = 0.01


# Ana Fonksiyon
def main(m, b, L, epochs):
    print(&quot;=&gt;  F(X):&quot;, m, &quot;X&quot;, b)

    for i in range(epochs):
        m, b = gradient_descent(m, b, points, L)
        show_graph(m, b)
    
 # Başlangıç değerleri
 main(random.uniform(-1, 110), random.uniform(-10, 10), 0.1, 250)
</code></pre>
<p>For context, f(x)**2 is the synthetic data I used.</p>
","0","Answer"
"78848646","78839246","<p>It's quite obvious from the error that <code>PrivacyEngine</code> doesn't take <code>batch_size</code> as a parameter. Looking at the <a href=""https://opacus.ai/api/privacy_engine.html#opacus.privacy_engine.PrivacyEngine"" rel=""nofollow noreferrer"">docs</a>, you should do something like</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.fc1 = nn.Linear(32*26*26, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(-1, 32*26*26)
        x = self.fc1(x)
        return torch.log_softmax(x, dim=1)

transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)

privacy_engine = PrivacyEngine()

model, optimizer, train_loader = privacy_engine.make_private(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    max_grad_norm=1.0,
    noise_multiplier=1.1,
)

criterion = nn.NLLLoss()
model.train()
for epoch in range(1):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

epsilon = privacy_engine.get_epsilon(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)

</code></pre>
","1","Answer"
"78849793","78843331","<p>The problem is the line <code>return torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2])</code></p>
<p>When you pass those values to <code>torch.tensor</code>, you are constructing an entirely new tensor object that has no relationship to the input values. This means you can't backprop through the new tensor to your <code>params</code>.</p>
<p>You need to compute the output with torch operations.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

params = nn.Parameter(torch.tensor([1.1, .4, .1, .4], dtype = torch.float64))

y0 = torch.tensor([1., 2.], dtype = torch.float64)

y1, y2 = y0
a, b, c, d = params

torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2]).requires_grad
&gt; False

torch.stack([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2]).requires_grad
&gt; True
</code></pre>
","0","Answer"
"78850538","78848428","<p>In keras, <code>categorical_crossentropy</code> and <code>sparse_categorical_crossentropy</code> functions use the same loss functions. The only difference is the format of the target array you use, which is described in detail <a href=""https://stats.stackexchange.com/a/420730/281499"">here</a>. Hence, you should one-hot encode the target array in order to use <code>categorical_crossentropy</code>.</p>
<p>The problem of low accuracy on the other hand could be a result of many things, such as too high/low learning rate, shallowness of the neural network model etc. you need to give more details on your code with model definition and training.</p>
","0","Answer"
"78850944","78850531","<p>Your precision-recall curve actually showed a relatively great result!
The sudden drop in precision value as recall approaches 1 should not be confused with poor model performance. This <a href=""https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"" rel=""nofollow noreferrer"">link</a> from sklearn explained how to interpret the curve.</p>
<p>The curve shows the performance of <code>my_model</code> after the last epoch of your training as the return value of <code>train_model</code>, so the curve has nothing to do with loss and accuracy values in each epoch in the training history.</p>
<p>The precision-recall curve plots different precision-recall values of your model on a dataset (in this case <code>X_test</code> and <code>y_test</code>) across various threshold values.</p>
<p>Suppose your test dataset consists of 50 positive and 50 negative. For example, if the threshold is -infinity, all <code>y</code> values produced by the model on <code>X_test</code> are greater than the threshold and would therefore be classified as positive, i.e. <code>TP = 50, FP = 50, TN = 0, FN = 0</code>. This corresponds to the <code>(1, 0.5)</code> point in the curve since <code>precision = TP/(TP+FP) = 0.5</code> and <code>recall = TP/(TP+FN) = 1</code>. Do it with other threshold values and you get the precision-recall curve. The model is said to have better performance if the area under the curve (AUC) is bigger (approaches 1), which your curve is showing.</p>
","1","Answer"
"78851495","78851057","<p>Let's start from the only constraints the NetLogo placed on how the actual extension-module gets visible and usable inside NetLogo ABM Simulated Worlds :</p>
<blockquote>
<p><sub>To use an extension in a model, add the <code>extensions</code> keyword at the beginning of the Code tab, before declaring any breeds or variables.
<br>
<code>extensions</code> is followed by a pair of square brackets containing a list of extension names. For example:
<br><br>
<code>extensions [sound speech]</code>
<br><br>
Using <code>extensions</code> instructs NetLogo to make the specified extensions’ commands and reporters available in the current model, just as if they were built-in NetLogo primitives. An extension must be installed for the <code>extensions</code> keyword to take effect.</sub></p>
</blockquote>
<p>So, only the syntax-related and location-related constraints apply here. Installation details are similarly clear.</p>
<p>Given you try to develop something new, the concept of separation-of-problems ought address your false dilemma. Your new Java code, if you opt to develop an extension, shall never violate the boundaries of other extensions' source code ( if it were for nothing else for the sake of principles of modularity &amp; maintenance concerns ).</p>
<p>That means all your &quot;cross&quot;-module operations and interactions should happen either via NetLogo-level of passing arguments to and from extension modules, serving a procedural- or a reporter-role; or design either side of &quot;cross&quot;-extension interaction, maintaining in mind &amp; design practice the modularity principles, incl. neutrality of (by)passing data-objects ( perhaps using ZeroMQ messaging &amp; signalling metaplane(s) for glue-logic, that protects either side from any attempt of permitting a kind of wild crossing the extension-module boundaries of separation-of-concerns ).</p>
","0","Answer"
"78852632","78851708","<p>The error message you get is a bit misleading, <code>prune_low_magnitude</code> can indeed optimize <code>Sequential</code> models. This error happens most likely due to version incompabilities between tensorflow and tfmot. Also, installing keras on its own could resolve your issue (see <a href=""https://github.com/tensorflow/tensorflow/issues/62929#issuecomment-1936988409"" rel=""nofollow noreferrer"">here</a>).</p>
<p>Furthermore, it looks a bit confusing that you are using <code>SparseCategoricalCrossentropy</code> loss on the pruned model. This is usually used for multi-class classification, not for regression.</p>
<p>Before you start &quot;real&quot; pruning, it might also be worth a try to reduce the number of <code>Dense</code> layers from 5 to 4, or reduce the number of filters in each layer.</p>
","2","Answer"
"78852949","78852192","<p>First thing is that you can use <code>device_map=&quot;auto&quot;</code>, it means that the model uses the available resources in the system.</p>
<p>But if you want to use only 2 specific GPU's in the system, so now <code>device_map={&quot;0&quot;: &quot;cuda:&lt;GPU_id&gt;&quot;, &quot;1&quot;: &quot;cuda:&lt;GPU_id&gt;&quot;, .... so on (if any) }</code>.</p>
<p>Or add other parameter named <code>device_ids=[&lt;GPU_id&gt;, &lt;GPU_id&gt;, ... so on (if any)]</code> instead of <code>device_map</code> parameter.</p>
<p>So now if you don't know the GPU ids so then perform several ways:</p>
<ol>
<li>
<pre class=""lang-py prettyprint-override""><code>import torch
print(torch.cuda.device_ids()) #it shows all GPU's ids.
</code></pre>
</li>
<li>
<pre class=""lang-py prettyprint-override""><code>import torch
print(torch.cuda.device_count())
for i in range(torch.cuda.device_count()):
    print(f&quot;GPU {i}: {torch.cuda.get_device_name(i)}&quot;)
</code></pre>
</li>
</ol>
<p>See GPU ids using this and then perform above operation.</p>
","0","Answer"
"78853527","78851057","<p>There are probably many reasons to not have an extension depend on another extension, but the most important is that, in the NetLogo line <code>extensions [table llm]</code>, where <code>llm</code> depends on <code>table</code>, <code>llm</code> cannot control/predict/enforce which version of <code>table</code> the user's NetLogo installation will use.  So, if <code>llm</code> outputs <code>table</code> objects from extension version v1.0, but the user has <code>table</code> v2.0 installed, <code>table</code> might not actually understand the objects produced by <code>llm</code>.</p>
<p>Instead, I would have the LLM extension output a NetLogo string containing the JSON, and then feed that into the <code>table:from-json</code> primitive, in order to get your table.  From there, you can process the table in NetLogo code.</p>
<p>If you want to process it in Java code in your extension, I guess you could run <code>table:to-list</code> on the output of <code>table:from-json</code>, and then pass the NetLogo list back into your extension.</p>
<p>Dealing with JSON data in Java isn't going to be fun, no matter how you do it.  But there are definitely ways of doing it that are more fun than using the Table extension as a JSON parser.  There are many other tutorials online for parsing JSON in Java, like <a href=""https://stackoverflow.com/questions/2591098/how-to-parse-json-in-java"">this one</a>, which you might find helpful.</p>
","2","Answer"
"78854593","78842184","<p>I am not sure, which architecture you use, but there are some potential solutions, which might help you:</p>
<ol>
<li>Check what activation functions you have. For fast training, try to use Relu activations</li>
<li>Experiment with batch size of your dataloader - larger batch size leads to faster training, but can take more RAM</li>
<li>Check how many threads are used, when training your model. Sometimes you could speed up your training, if you have parallelism, when loading the data</li>
<li>Check the types of your numpy arrays (because your images are in the numpy arrays). Float16 could be a bit faster than Float32</li>
</ol>
<p>I am sure, there are more ways to speed up training, but these are the most basic ones.</p>
","0","Answer"
"78855433","78855135","<blockquote>
<p>Are there more efficient alternatives?</p>
</blockquote>
<p>Yes.</p>
<p>If we speak about efficiency-of-work completion in time, use more resources and more performant resources to shorten the time-to-complete the work. A 1.5 GHz clocked herd of SIMD-SM engines, no matter how many are present on the GPU fabric, will perform the same amount of a pure-<code>[SERIAL]</code> work no better than 3 times slower, than a 4.5 GHz clocked CPU ( if using a single CPU-core only ).</p>
<p>If we speak about efficiency-of-resources' use, use better organisation-of-work, not to let a single resource instance ( RAM, CPU, GPU, storage ) unused on CP/M critical-chain-of-process-flow. Then the end-to-end processing time could not get, given the resources pools, shorter, unless processing strategy gets refactored into a faster organisation-of-work. On the contrary, any resource, not used during the work performed on the CP/M critical-chain-of-process-flow, reduces the efficiency-of-resources' use. This reads into avoiding any mutual-coordination ( GPU-kernel-code &quot;barriers&quot; for synchronisations are example of such, <code>.await()</code>-s, any kind of locking, Python GIL-lock included, shared-queues and any similar high-level code constructors are examples, that in effect block the flow of work on CP/M, thus decrease both the efficiency-of-work and efficiency-of-resources' use )</p>
<p>Latency ( a time your next processing step, be it on CP/M critical-chain-of-process-flow or not, has to wait for &quot;products&quot; from the slowest of the preceding processing steps ) can in well organised flow-of-work get masked ( called latency-masking ).</p>
<p>Example might be, if your input data is big enough to payback the add-on costs of instantiations of any process-based concurrent work ( <a href=""https://stackoverflow.com/a/78845402""><strong>details</strong> matter, <strong>a lot</strong></a> as costs are additive and sequential to the intent-of-work ), split your input text processing into 2 parts, if you have 2 GPU engines, each one processed by one of such process-flow, the first starting from beginning <code>sentence[0:half]</code>, the other one starting ( at the same time ) processing the 2nd half <code>sentence[half:]</code>.</p>
<p>Similar scheme applies for any number of resources you can populate the work to harness them ( increasing both the efficiency-of-resources' usage and the efficiency-of-work completion in time ), for which the add-on costs of using additional &quot;on-demand-mobilised&quot; resources does not diminish or devastate benefits they ought bring  ( becoming more expensive in add-on costs, then the effect they help bring for the process-flow efficiency-of-work completion in time -- &quot;negative&quot; speedup ( technically speaking S &lt; 1.0, not negative ) ).</p>
<p>The last, but not least available strategy is the &quot;shift-in-paradigm&quot;, if you can invent a straight different technology, that can do the same, with perhaps other types of resources and processing-steps, but in shorter time.</p>
","0","Answer"
"78855521","78765868","<p>Yes, combine STT, TTT and TTS together. That's all you need.</p>
<p>In this blog I shared how I did that with FreeSWITCH and GCP:
<a href=""https://dataandsignal.com/blog/static/real_time_voice_ai_speech_translation"" rel=""nofollow noreferrer"">https://dataandsignal.com/blog/static/real_time_voice_ai_speech_translation</a></p>
","1","Answer"
"78860304","78860233","<p>first install timm, torch python packages via pip</p>
<p>create model and load pre-trained weights</p>
<pre class=""lang-py prettyprint-override""><code>import timm
import torch
model = timm.create_model('resnet50', pretrained=True, features_only=True)

# convert image torch tensor as ( nimages, channels, height, width ) ex- (1,3,224, 224)  
features = model( image ) 
print( features.shape )

</code></pre>
<p>(1, 2048, 224, 224)</p>
","0","Answer"
"78860840","78858321","<p>Print the contents of <strong>train_metrics</strong>, just before line 50;</p>
<pre><code>train_metrics_data = pd.DataFrame(train_metrics)
</code></pre>
<p>Then you will see what the dict looked like just before it crashes. I ran a part of the faulty code, and it seems to work just fine.</p>
<p>Using python 3.10.14 :</p>
<pre><code>import pandas as pd

train_metrics = {'model': ['Linear Regression',
   'Random Forest Regressor',
   'Gradient Boost Regression',
   'Extra Tree Regressor'],
  'MAE': [829.1023412412194,
   288.33455697065233,
   712.9637267872279,
   0.0010629575741748962],
  'MAPE': [1.0302372135902111,
   0.20937541440883897,
   0.538244903316323,
   6.306697580961048e-07],
  'RMSE': [1120.5542708017374,
   416.48933196590013,
   1012.399201767692,
   0.05804079289490426],
  'RSquared': [0.5598288286601083,
   0.9391916010838417,
   0.6406981997919169,
   0.9999999988190745]}

train_metrics_data = pd.DataFrame(train_metrics)

print(train_metrics_data)
</code></pre>
<p>prints:</p>
<pre><code>                       model         MAE          MAPE         RMSE  RSquared
0          Linear Regression  829.102341  1.030237e+00  1120.554271  0.559829
1    Random Forest Regressor  288.334557  2.093754e-01   416.489332  0.939192
2  Gradient Boost Regression  712.963727  5.382449e-01  1012.399202  0.640698
3       Extra Tree Regressor    0.001063  6.306698e-07     0.058041  1.000000
</code></pre>
","0","Answer"
"78861552","78860233","<p>I didn't read the paper in detail, but when they say <code>[w, h, f]</code> I don't think the <code>w</code> and <code>h</code> have to match the width and height of the original image. They likely just mean that if the output of your ResNet after the last Conv + Pooling layer is <code>[w, h, f]</code>, you reshape it into 2d (making it it <code>[fxh, w]</code>) and then pass it through a fully-connected layer to make it <code>f</code> dimensional.</p>
<p>Something like this</p>
<pre><code>import torch
import torch.nn as nn
import torchvision.models as models

resnet = models.resnet50(pretrained=True)

# Remove the last fully connected layer and adaptive pooling layers
resnet = torch.nn.Sequential(*list(resnet.children())[:-2])

# Dummy image of shape [1, 3, 224, 224]
image = torch.randn(1, 3, 224, 224)

intermediate_features = resnet(image)  # This will be [1, 2048, 7, 7]

batch_size, channels, h, w = intermediate_features.size()

# [1, 14336, 7] where f=14336 and w=7
reshaped_features = intermediate_features.view(batch_size, channels * h, w)

fc_layer = nn.Linear(w, 1)  # This layer reduces the w dimension to 1

final_output = fc_layer(reshaped_features)  # [1, 14336, 1]

final_output = final_output.squeeze(-1)  # [1, 14336]

print(final_output.shape)

</code></pre>
<p>(My example also has batch size as a dimension because in the real world you work with batches of examples)</p>
","1","Answer"
"78861910","78860154","<p>I managed to resolve this by using torch, torchvision GPU version instead.</p>
<pre><code>pip uninstall torch torchvision
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124
</code></pre>
<p>Also check your CUDA version to match with the wheel version, mine is 12.4</p>
","0","Answer"
"78862820","78859343","<p>The main thing as <a href=""https://x.com/haileysch__/status/1822758486632997102"" rel=""nofollow noreferrer"">Hailey S</a> Pointed out is that the HF reinits defaults are huge. I assume that with signal propagation theory/techniques one can predict the output would be to large for learning or something (see greg's work), but for now I'm going with this:</p>
<pre class=""lang-py prettyprint-override""><code>#%%
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Config

from pdb import set_trace as st

def reinit_gpt_neox_20B_inspired_use_case_llama2_mutates(model, 
                                                L: int,  # for beyond scale we filled the data to block size which is 4096 for max seq length llama2
                                                ):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):  # all linear layers including MLP and attention, let's try this first given it's smaller
            D = module.in_features  # I think this is right size it's xW []
            std = 3 / (L * (D)**0.5)
            nn.init.normal_(module.weight, mean=0, std=std)
            if module.bias is not None:  # don't think biases matter cuz bias=False in all layers
                nn.init.constant_(module.bias, 0)
        elif str(module) == 'LlamaRMSNorm()':
            if hasattr(module, 'weight'):
                if module.weight is not None:  # todo: idk if needed for layer norm
                    nn.init.constant_(module.weight, 1.0)
            if hasattr(module, 'bias'):  # I don't think RMSNorm has bias, the whole point it doesn't think centering matters so bias is similar to centering
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
        else:  
            if hasattr(module, 'weight'):
                if module.weight is not None: 
                    D = module.weight.shape[0]
                    std = (2 / (D + 4*D))**0.5  # e.g., small init attention layers
                    nn.init.normal_(module.weight, mean=0, std=std)
            if hasattr(module, 'bias'):
                if module.bias is not None:  # don't think biases matter cuz bias=False in all layers
                    nn.init.constant_(module.bias, 0)
    return

def reinit_gpt2_weights_mutates(
        model, 
        # weight_std: float = 0.00000002,  # 0.02 ref: Hailey S doesn't recommend this huge value! ref: https://x.com/haileysch__/status/1822758486632997102 I'm choosing a really small value due to my previous research with Tommy Poggio suggested to us that larger inits give worse generalization error
        weight_std: float = 2e-6,  # 0.02 ref: Hailey S doesn't recommend this huge value! ref: https://x.com/haileysch__/status/1822758486632997102 I'm choosing a really small value due to my previous research with Tommy Poggio suggested to us that larger inits give worse generalization error
        # weight_std: float = 0.0,
        bias_std: float = 0.0, 
        verbose: bool = False,
        ) -&gt; None:
    &quot;&quot;&quot; 
    Why we chose &lt; 0.02 for standard deviation: https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18
    Reinit for gpt2 only test for xl: https://huggingface.co/openai-community/gpt2-xl
    &quot;&quot;&quot;
    model_weight_norm = sum([torch.norm(param, p=2).item() for param in model.parameters()]) if verbose else None
    print(f'{model_weight_norm=}') if verbose else None
    for module_name, module in model.named_modules():
        print(f'{module_name=} {isinstance(module, nn.Linear)=} {type(module)=}') if verbose else None
        if isinstance(module, nn.Linear):
            # nn.init.normal_(module.weight, mean=0, std=0.02) # original, evil!
            print(f'{module.weight.norm(2)=}') if verbose else None
            nn.init.normal_(module.weight, mean=0, std=weight_std)
            print(f'{module.weight.norm(2)=}') if verbose else None
            if module.bias is not None:
                # gpt suggestion: https://chatgpt.com/c/b9d34414-a123-48d6-bbae-334dedb580f3
                nn.init.constant_(module.bias, bias_std)
        elif isinstance(module, nn.Embedding):
            print(f'{module.weight.norm(2)=}') if verbose else None
            nn.init.normal_(module.weight, mean=0, std=weight_std)
            print(f'{module.weight.norm(2)=}') if verbose else None
        elif isinstance(module, nn.Dropout):
            pass # has no params
        elif isinstance(module, nn.LayerNorm):
            # gpt suggestion: https://chatgpt.com/c/b9d34414-a123-48d6-bbae-334dedb580f3
            print(f'{module.weight.norm(2)=}') if verbose else None
            nn.init.constant_(module.weight, 0.0)
            print(f'{module.weight.norm(2)=}') if verbose else None
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
        elif isinstance(module, nn.Conv1d):
            print(f'{module.weight.norm(2)=}') if verbose else None
            nn.init.normal_(module.weight, mean=0, std=weight_std)
            print(f'{module.weight.norm(2)=}') if verbose else None
            if module.bias is not None:
                nn.init.constant_(module.bias, bias_std)
        # elif isinstance(module, nn.NewGELUActivation):
        #     pass
        else:  
            if hasattr(module, 'weight'):
                if module.weight is not None: 
                    D = module.weight.shape[0]
                    # std = (2 / (D + 4*D))**0.5  # e.g., small init attention layers
                    std = weight_std
                    nn.init.normal_(module.weight, mean=0, std=std)
            if hasattr(module, 'bias'):
                if module.bias is not None:  # don't think biases matter cuz bias=False in all layers
                    nn.init.constant_(module.bias, bias_std)
    model_weight_norm = sum([torch.norm(param, p=2).item() for param in model.parameters()]) if verbose else None
    print(f'{model_weight_norm=}') if verbose else None
    return

# Step 1: Load the pre-trained GPT-2 XL model
torch.cuda.empty_cache() # Clear CUDA cache to free up memory
torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32 
model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;, torch_dtype=torch_dtype, trust_remote_code=True)
print(f'{model=}')
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)
pretrained_tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2-xl&quot;, padding_side=&quot;right&quot;, trust_remote_code=True)
pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token if pretrained_tokenizer.pad_token_id is None else pretrained_tokenizer.pad_token
print(f'{pretrained_tokenizer=}\n{pretrained_tokenizer.bos_token_id=} {pretrained_tokenizer.eos_token_id=} {pretrained_tokenizer.pad_token_id=} {pretrained_tokenizer.vocab_size=}')
# Step 2: Calculate the L2 norm of the weights for the pre-trained model
pretrained_weight_norm = sum([torch.norm(param, p=2).item() for param in model.parameters()])
print(f&quot;Total L2 norm of pre-trained model weights: {pretrained_weight_norm:.2f}&quot;)

# Step 1: Initialize a new GPT-2 model from scratch with custom configuration
model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;, torch_dtype=torch_dtype, trust_remote_code=True)
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)
config = GPT2Config(
    vocab_size=pretrained_tokenizer.vocab_size,  # Ensure this matches the tokenizer's vocabulary size
    n_ctx=1024,  # Context window size (number of tokens the model can see at once)
    bos_token_id=pretrained_tokenizer.bos_token_id,  # Begin-of-sequence token
    eos_token_id=pretrained_tokenizer.eos_token_id,  # End-of-sequence token
    pad_token_id=pretrained_tokenizer.eos_token_id,  # pad-sequence token
)
model = AutoModelForCausalLM.from_config(config)
# Step 2: Calculate the L2 norm of the weights for the freshly initialized model
scratch_weight_norm = sum([torch.norm(param, p=2).item() for param in model.parameters()])
print(f&quot;Total L2 norm of model initialized from scratch: {scratch_weight_norm:.2f}&quot;)

# Step 1: Reinit GPT2 with really small init
model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;, torch_dtype=torch_dtype, trust_remote_code=True)
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)
reinit_gpt2_weights_mutates(model)
scratch_weight_norm_small_reinit = sum([torch.norm(param, p=2).item() for param in model.parameters()])
print(f&quot;Total L2 norm of model initialized from scratch with small reinit (not default HF config): {scratch_weight_norm_small_reinit:.2f}&quot;)

# Step 1: Reinit GPT2 with really small init
model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;, torch_dtype=torch_dtype, trust_remote_code=True)
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)
reinit_gpt_neox_20B_inspired_use_case_llama2_mutates(model, 1024)
scratch_weight_norm_small_reinit = sum([torch.norm(param, p=2).item() for param in model.parameters()])
print(f&quot;Total L2 norm of model initialized from scratch with gpt_neox_20B reinit (not default HF config): {scratch_weight_norm_small_reinit:.2f}&quot;)

# Justification:
# If the model is truly being initialized from scratch, the weight norm should be much smaller compared to the pre-trained model. 
# This confirms that the training process is starting from a random initialization and not from any pre-existing pre-trained weights.
</code></pre>
<p>It did give a smaller reinit total l2 norm of weights but until I run things/train I don't know if that is ok, but look weights look ok (but larger than I expected):</p>
<pre class=""lang-py prettyprint-override""><code>~/beyond-scale-language-data-diversity# python playground/test_gpt2_pt_vs_reinit_scratch.py

model=GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 1600)
    (wpe): Embedding(1024, 1600)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-47): 48 x GPT2Block(
        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)
)
pretrained_tokenizer=GPT2TokenizerFast(name_or_path='gpt2-xl', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;', 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
        50256: AddedToken(&quot;&lt;|endoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
}
pretrained_tokenizer.bos_token_id=50256 pretrained_tokenizer.eos_token_id=50256 pretrained_tokenizer.pad_token_id=50256 pretrained_tokenizer.vocab_size=50257
Total L2 norm of pre-trained model weights: 24551.96
Total L2 norm of model initialized from scratch: 1635.31
Total L2 norm of model initialized from scratch with small reinit (not default HF config): 1.06
Total L2 norm of model initialized from scratch with gpt_neox_20B reinit (not default HF config): 7035.41
</code></pre>
<p>I will document things here: <a href=""https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18"" rel=""nofollow noreferrer"">https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18</a> but links can die so copied the main code.</p>
","0","Answer"
"78863745","78863529","<p>Your code works without issues in my environment; I believe your <code>huggingface_hub</code> package is old. Try</p>
<pre><code>python -m pip install --upgrade huggingface_hub
</code></pre>
","0","Answer"
"78865002","78752483","<p>You can try to use <a href=""https://pypi.org/project/parallel-pandas/"" rel=""nofollow noreferrer"">parallel-pandas</a> library.
It's very easy to use:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from parallel_pandas import ParallelPandas

#initialize parallel-pandas
ParallelPandas.initialize(n_cpu=16)

# create big DataFrame
df = pd.DataFrame(np.random.random((1_000_000, 100)))

#your CPU-intensive function
def foo(x):
    pass

res = df.p_apply(foo)


</code></pre>
","0","Answer"
"78866026","78863903","<p>One of the most common reasons for getting garbage predictions, especially if the predictions are all the same, is forgetting to scale the input for inference. That is, apply the fitted <code>StandardScaler</code> from your preprocessing.</p>
<p>The best way to avoid this pitfall, and a few others, is to use pipelines. It's a bit more complex for you because you only want to scale some columns. So, for example:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.pipelines import make_pipeline

# Must split dataset before scaling.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Use pipelines!
ctx = ColumnTransformer(
        remainder='passthrough',
        transformers=[
            ('scaler', StandardScaler(), features_to_scale),
        ])
model = RandomForestClassifier(min_samples_split=10, random_state=78)
pipe = make_pipeline(ctx, model)

# Fit model etc etc.
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
</code></pre>
<p>This has the side benefit of encouraging you not to scale the data before splitting it (another common pitfall).</p>
<p>There are a few other things you should do:</p>
<ul>
<li>Consider splitting <code>test</code> into <code>val</code> and <code>test</code> sets. The purpose of <code>val</code> is exploring algorithms, hyperparameters, etc. And the purpose of <code>test</code> is finally checking the performance of your selected model. <a href=""https://arxiv.org/abs/1811.12808"" rel=""nofollow noreferrer"">Read this carefully.</a></li>
<li>Consider not only looking at accuracy, but also at recall and precision, and at the confusion matrix. Consider other accuracy measures like Matthews correlation coefficient, or looking at the ROC-AUC curve.</li>
<li>If there is any class imbalance, you should stratify the train-test split.</li>
<li>Once you know your optimal hyperparameters, don't forget to fit a model using all of your data! (The model trained on <code>X_train</code> was only to help optimize the hyperparameters and estimate performance.)</li>
</ul>
","1","Answer"
"78866520","78866202","<p>here is how you should pass datasets to the <code>super_gradients</code>. I have tested this folder structure on MacOS. On windows you need to rewrite all &quot;/&quot; symbols with the &quot;\&quot;</p>
<pre><code># this is an example for macOS/Linux
# tested with coco8 dataset
# https://github.com/ultralytics/assets/releases/download/v0.0.0/coco8.zip

dataset_params = {
    'data_dir': &quot;dataset&quot;, 
    'train_images_dir': &quot;images/val&quot;,
    'train_labels_dir': &quot;labels/val&quot;,
    'val_images_dir': &quot;images/val&quot;,
    'val_labels_dir': &quot;labels/val&quot;,
    'classes': ['cabecalho', 'assinatura', 'rodape']
}
</code></pre>
<p>Folder structure is the following:</p>
<pre><code>- dataset
- dataset/images
- dataset/images/val
- dataset/images/val/000000000049.jpg
- ...
- dataset/images/train
- dataset/images/train/000000000034.jpg
- ...
- dataset/labels/val
- dataset/labels/val/000000000049.txt 
- ...
- dataset/labels/train
- dataset/labels/train/000000000034.txt 
- ...




</code></pre>
","0","Answer"
"78868012","78820748","<p>This is a known bug with inference using CPU on Windows for <code>PyTorch 2.4.0</code> : <a href=""https://github.com/ultralytics/ultralytics/issues/15049"" rel=""noreferrer"">Bug Ticket</a></p>
<p>Common issues are:</p>
<ul>
<li>Too many detections at the top of the image</li>
<li>Reporting missing <code>fbgemm.dll</code></li>
</ul>
<hr />
<p>Here are the recommended fixes:</p>
<ul>
<li>When only CPU available: Downgrade PyTorch (CPU) to a previous version</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>pip uninstall torch torchvision torchaudio -y
pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu
</code></pre>
<ul>
<li>When NVIDIA GPU with CUDA support available: Install PyTorch with CUDA support (will still fail in case of <code>PyTorch 2.4.0</code> when using CPU)</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>
<ul>
<li>Install nightly build of <code>PyTorch</code> (<strong>WARNING</strong>: might be unstable)</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>pip install torch  --index-url https://download.pytorch.org/whl/nightly/cu121 --force-reinstall
</code></pre>
","6","Answer"
"78869024","78862511","<p>this is because of tokenizer error change the tokenizer and the issue will be resolved</p>
","0","Answer"
"78869828","78868439","<p>Have your script in a folder in google drive. Connect your Colab workspace with your Google Drive. You don't even have to remember the code for it when you press button it automatically gets added at the top. Now run all of your commands. Now your Yolov5 folder should be saved in the folder you created and your runs are saved there as well, make sure that you are in the selected folder by running <code>path</code> and <code>cd</code> to navigate</p>
","0","Answer"
"78870114","78869863","<p>In the example using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit"" rel=""nofollow noreferrer""><code>sklearn.svm.SVC.fit()</code></a>, the input is expected to be of shape <code>(n_samples, n_features)</code> (thus being 2-dimensional).</p>
<p>In your case, each <em>sample</em> would be an image. To make your code <em>technically</em> work, you would thus need to flatten your <code>X_train</code> input and make each &quot;raw&quot; pixel value a <em>feature</em>,</p>
<pre class=""lang-py prettyprint-override""><code>X_train_flat = X_train.reshape(X_train.shape[0], -1)
</code></pre>
<p>which, in your example, would produce a <code>(20624, 49152)</code>-shaped array (as 128·128·3=49152), where each row is a flattened version of the corresponding image.</p>
<p>What is often done instead of using the &quot;raw&quot; pixels as an input to SVM and similar classifiers, however, is using a set of &quot;features&quot; extracted from the images, to reduce the dimensionality of the data (i.e., in your example, using a <code>(20624, d)</code>-shaped array instead, where <em>d</em>&lt;49152). This could be <a href=""https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients"" rel=""nofollow noreferrer"">HOG</a> features, for example, or the result of any other dimensionality reduction technique (it could even be the output of a neural network) – you might want to also have a look at <a href=""https://stackoverflow.com/questions/50131667/"">this related question</a> and its answers.</p>
","1","Answer"
"78870571","78870012","<p>The implementation <em>behind the scenes</em> calculates the bilinear product, as per documentation.<br />
That's not to say that multiplying the tensors as they are is equivalent to the bilinear layer.<br />
If you follow the trail of the actual calculation, you end up with a python-interface function, while the actual implementation is in the cpp, where you can see that there are reshapes and flattening of the tensors so as to ensure the output is the expected expression:</p>
<p>Here's an excerpt from the <a href=""https://github.com/pytorch/pytorch/blob/a8dc9d8e353ddcf7db0247349a3acd0dd37fcc6f/aten/src/ATen/native/Linear.cpp#L702"" rel=""nofollow noreferrer"">relevant code</a>:</p>
<pre><code>  auto size1 = input1.sym_sizes();
  output_size.insert(output_size.end(), size1.begin(), size1.end() - 1);
  output_size.push_back(weight.sym_size(0));
  auto input1_flattened = input1.reshape_symint({-1, input1.sym_size(-1)});
  auto input2_flattened = input2.reshape_symint({-1, input2.sym_size(-1)});
  Tensor output = at::_trilinear(input1_flattened, weight, input2_flattened, {1,3}, {0}, {1,2}, {2,3}).reshape_symint(output_size);
  if (bias.defined()) {
    output = output + bias;
  }
</code></pre>
<p>Hope that helps</p>
","1","Answer"
"78871119","78867452","<p>You probably just need to apply more sophisticated filtering to the position data, to suppress the noise and make the real motion stand out.</p>
<p>The noise is probably evenly spread across frequencies, but the &quot;difference between frames&quot; operation is a high-pass operation that will accentuate the high frequency noise.  Fish swimming should be pretty low frequency, though so a good low-pass filter with an appropriate cut-off frequency should isolate it pretty well.</p>
<p>The other thing you can do that would really help is to increase your frame rate.  This will spread the noise over a wider frequency range, so that less of it will overlap with real data, and it will be easier to filter out.</p>
<p>Do an FFT or spectrogram of your position data to see what the frequency distribution currently looks like so you know what to filter out, and then you can use a tool like <a href=""http://jaggedplanet.com/iir/iir-explorer.asp"" rel=""nofollow noreferrer"">this one</a> to design an IIR filter.</p>
<p>You should apply something like a 2-degree Butterworth low-pass to your data in both the forward and backward directions so there's no time shift.  Do the &quot;difference between frames&quot; operation either before or after (it doesn't matter which) to produce a nicely filtered &quot;fish velocity&quot; graph.</p>
","0","Answer"
"78871538","78862511","<p>TLDR - this has been raised with sumy <a href=""https://github.com/miso-belica/sumy/issues/216"" rel=""nofollow noreferrer"">via GitHub</a></p>
<p>nltk's latest release (3.8.2) now restricts the use of pickled files.</p>
<ul>
<li>sumy's <code>Tokenizer</code> initialisation calls nltk with a pickle file <a href=""https://github.com/miso-belica/sumy/blob/d31ead3/sumy/nlp/tokenizers.py#L202"" rel=""nofollow noreferrer"">here</a></li>
<li>nltk's latest changes means a pickle file will raise the error you are receiving as it is restricted <a href=""https://github.com/nltk/nltk/blob/3.8.2/nltk/data.py#L763"" rel=""nofollow noreferrer"">see here</a></li>
</ul>
<p>For your code, as a temporary solution whilst sumy addresses this change on their end, you can create a temp Tokenizer class</p>
<pre><code>from nltk.tokenize import PunktTokenizer
from sumy.nlp.tokenizers import Tokenizer

class MyTokenizer(Tokenizer):

    def _get_sentence_tokenizer(self, language):
        # Use the linked GitHub Issue's code where they have overridden this function
</code></pre>
<p>Also your step</p>
<blockquote>
<p>python -m nltk.downloader punkt</p>
</blockquote>
<p>should now be</p>
<pre><code>python -m nltk.downloader punkt_tab
</code></pre>
","0","Answer"
"78872372","78870012","<p>The notation here is definitely confusing. The transpose notation is not clarifying.</p>
<p>I think the operation makes more sense in einsum notation - <code>bn,anm,bm-&gt;ba</code>. <code>x1</code> has a multiplication/reduction along the second axis of <code>A</code>. <code>x2</code> has a multiplication/reduction along the second axis of <code>A</code>.</p>
<pre class=""lang-py prettyprint-override""><code>x1 = torch.randn(2, 8)
x2 = torch.randn(2, 4)
A = torch.randn(16, 8, 4)

b1 = torch.bilinear(x1, x2, A)
b2 = torch.einsum('bn,anm,bm-&gt;ba', x1, A, x2)
assert torch.allclose(b1, b2)
</code></pre>
<p>You can also compute the multiplications/reductions explicitly:</p>
<pre class=""lang-py prettyprint-override""><code>b3 = (x1[:,None,:,None] * A[None] * x2[:,None,None,:]).sum(-1).sum(-1)
assert torch.allclose(b1, b3)
</code></pre>
","1","Answer"
"78874086","78873685","<p>It is transforming the columns you specified: 0 and 1.</p>
<p>Your log transformer specifies: <code>('function', FunctionTransformer(n.log1p), [0,1])</code> and those two columns are being transformed.</p>
<p>For example, the first row was:</p>
<pre><code>[     0,      1,     19,  15000,     39]
</code></pre>
<p>And the result was:</p>
<pre><code>[0.00000000e+00, 6.93147181e-01, 1.90000000e+01, 1.50000000e+04, 3.90000000e+01]
</code></pre>
<p>If you only want to transform the first column, then change the transformer: ``('function', FunctionTransformer(n.log1p), [0])`.</p>
<p>The number <code>1.9e+01</code> is the same as <code>19</code> — don't be fooled by the <a href=""https://en.wikipedia.org/wiki/Scientific_notation"" rel=""nofollow noreferrer"">scientific notation</a>.</p>
<p>You can suppress this default behaviour with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html"" rel=""nofollow noreferrer""><code>numpy.set_printoptions(suppress=True)</code></a>.</p>
","1","Answer"
"78876118","78872766","<p>Two mistakes in your code</p>
<ol>
<li>You must flatten the input before the first dense layer. I'm not sure why you've placed it in between dense layers.</li>
<li>The model expects a batch of inputs. If it's just one example, you just pass it in a batch of size 1.</li>
</ol>
<pre><code>import tensorflow as tf
import numpy as np

def GenerateModel():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.InputLayer(input_shape=(16, 3)))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(256, activation='relu'))
    model.add(tf.keras.layers.Dense(16, activation='relu'))
    model.add(tf.keras.layers.Dense(1))
    return model

input_matrix = np.array([[15, 81, -25.169450961198],
                         [53, 108, -36.4360540112101],
                         [73, 84, -40.6695085658194],
                         [0, 69, -20.7084454809044],
                         [35, 97, -31.3954623571617],
                         [117, 102, -44.2065629437328],
                         [48, 68, -35.7085340456925],
                         [17, 59, -23.1078535464318],
                         [64, 24, -24.2111029108488],
                         [101, 2, -25.97821118996],
                         [57, 35, -23.7173409519286],
                         [117, 101, -44.1413580763786],
                         [88, 10, -25.4001185503816],
                         [11, 14, -22.0778253042297],
                         [46, 50, -24.7021623105999],
                         [0, 0, -1]])

model = GenerateModel()

output = model.predict(np.expand_dims(input_matrix, axis=0))
print()
print(output)

</code></pre>
","0","Answer"
"78877814","78677993","<p>First, your statement regarding DataLoader is not exactly correct. The rules of where the data is stored and where it is fetched from will be controlled by the <code>__getitem__</code> or the <code>__getitems__</code> in the Dataset object tied to the DataLoader. The easiest solution to your problem would implementing a CustomDataset which would be a subclass of the <code>torch.utils.data.Dataset</code> class. Probably something like this</p>
<pre><code>class CustomDataset(torch.utils.data.Dataset):
  def __init__(self, ndatapoints):
    self.len = ndatapoints #the number of points you want to generate
    '''
    declare any other parameters related to your simulation here, just remember to add self. before the, to make them persistent in the class
    '''
  def __len__(self):
    return self.len

  def __getitem(self, id):
    '''
    simulate one datapoint here, maybe use id as a random seed, put the data in a torch.Tensor using something like dataTensor = torch.as_sensor(...)
    '''
    return dataTensor
</code></pre>
<p>You might also wanna look into <code>torch.utils.data.IterableDataset</code>. Then declare this and put it into a DataLoader</p>
<pre><code>dataset = CustomDataset(10000)
dataLoader = torch.utils.data.DataLoader(dataset, batch_size=124)
</code></pre>
<p>You can use the dataLoader like any others you have seen and it will basically generate <code>batch_size</code> number of dataPoints at a time and you can use them as batches for your training/alidation. If your simulation happens on CPU, then you might wanna set the <code>num_workers</code> kwarg in the DataLoader constructor to concurrently generate more than one batch at once.</p>
","0","Answer"
"78883118","78687946","<p>Try exporting your yolo model to .engine format, it should boost your detection speed significantly. also for the tracking part you could utilize the built in Bytracker of supervision library
<a href=""https://docs.ultralytics.com/modes/export/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/export/</a>
<a href=""https://supervision.roboflow.com/latest/trackers/"" rel=""nofollow noreferrer"">https://supervision.roboflow.com/latest/trackers/</a></p>
","0","Answer"
"78883380","78881480","<p>This might not speed up the training time significantly but you can try to optimize your data, for example use float32 instead of float64. You can also use early stopping for random forest since it doesn’t natively use GPU. You can also profile the code and see the bottlenecks that's making the training slow.</p>
","1","Answer"
"78885582","78862449","<p>What I have done now to get my setup working with DQN is flattening both spaces:</p>
<pre><code>self.observation_space = spaces.MultiBinary(n * n)
self.action_space = spaces.Discrete(n*n)
</code></pre>
<p><strong>observation space:</strong> While my state / observation space is actually matrix with 0s and 1s, I am <strong>flattening</strong> it to a MultiBinary, where true represents 1 and false represents 0. Using MultiBinary has greatly improved performance.</p>
<p><strong>action_space:</strong> Similarly, while it represents a vector that point a location in the matrix location, I use a Discrete action space, which I then <strong>unflatten</strong> for backing out the entry in the matrix</p>
<pre><code>    row = action // (self.n)
    col = action % (self.n)
</code></pre>
<p>In summary, while it was unintuitive for me, the DQN model can handle matrices in observation spaces, simply by flattening the matrix to one MultiDiscrete or MultiBinary. Also, the action of selecting an entry in a matrix can be represented by Discrete(n*n).</p>
","0","Answer"
"78887207","78884108","<p>This doesn't seem to be a possibility. I was looking for this a few years ago and nothing seems to have changed since then.</p>
<p>In my case I used a workaround of transforming my label to reduce the skew (like applying the log transform), fitting the model and un-transforming it back during inference to get the actual prediction.</p>
<p>Another option would be to write your class for a custom regression decision tree that directly makes use of lower-level Spark APIs and uses a custom impurity function.</p>
","0","Answer"
"78890454","78890391","<pre><code>treino_x, teste_x, treino_y, teste_y = train_test_split(x,y,test_size=0.33)
modelo.fit(x,np.ravel(y,order=&quot;c&quot;))
</code></pre>
<p>You are training on the whole data set here, so no wonder you are getting 100% correct results. You should be using just the training set:</p>
<pre><code>treino_x, teste_x, treino_y, teste_y = train_test_split(x,y,test_size=0.33)
modelo.fit(treino_x,np.ravel(treino_y,order=&quot;c&quot;))
</code></pre>
<p>Your prediction code also looks wonky, but that's another problem...</p>
<p>See <a href=""https://www.datacamp.com/tutorial/decision-tree-classification-python#splitting-data-tound"" rel=""nofollow noreferrer"">this page</a> for a detailed example.</p>
","0","Answer"
"78891929","78885044","<p>You need to download the WordNet dataset using the NLTK downloader.</p>
<p>Try running this command:</p>
<pre class=""lang-py prettyprint-override""><code>import nltk

nltk.download('wordnet')
</code></pre>
","0","Answer"
"78891979","78885395","<p>I think you can checkout few points :</p>
<ol>
<li>if you have splitted data from 'train_test_split' then consider random_state=42</li>
<li>draw graphs of loss and acc to check for overfitting and underfitting , i think that data is overfitted and may try restarting the kernel</li>
</ol>
","0","Answer"
"78892176","78891901","<p>When loading a custom model which may not be avaialale by default in PyTorch or for any reason has a custom implementation, the first step is to initialise an object of that model, which would contain a supposed &quot;skeleton&quot; of the model, with all the trainable constants, layers, and depending on the implementation, default weights of the edges as well.</p>
<p>The second step then would be to load the state dictionary of the model, which is nothing but a mapping of the values of each and every weight to an actual value. Hence, in such a case you need to specifically intialise the layers since they would be intialised (be assigned weights, independent of pre-trained or actually trained). And then one may shift the model to the device they desire.</p>
<p>In case a model is already available in the Pytorch library (references as a built-in model), such as the <code>resnet50</code>, its weights can be either initialised via values provided by PyTorch:</p>
<pre><code>from torchvision.models import resnet50, ResNet50_Weights

# Old weights with accuracy 76.130%
resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
</code></pre>
<p>Or a custom dictionary can be fed into the model however that has to be of the same format/layout as required by, in this case, the <code>resnet50</code> model builder.</p>
<p>Using <code>torch.load()</code> is generally <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model"" rel=""nofollow noreferrer"">not an accepted way</a> to load models, and rather the dictionary of the model is saved and loaded for the most efficiency and flexibility.</p>
<p>Citing the Pytorch documentation's example here. This is also how I personally save and load my own implementations and till now this approach has been pretty reliable:</p>
<pre><code># To save the mdoel
torch.save(model.state_dict(), PATH)
# To load the model. As I mentioned above, instantiating the model class
# and then loading the state_dict
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.eval()
</code></pre>
<p>If I have to shift the model to the GPU, then this is how I do it:</p>
<pre><code>model.load_state_dict(BEST_MODEL_STATE_DICT)
model = model.to(pytorch_dev) if next(model.parameters()).device != pytorch_dev else model
</code></pre>
<p>wherein <code>pytorch_dev</code> is selected as the GPU if CUDA is available on the device.</p>
<p>Some good reads may be:</p>
<ol>
<li><a href=""https://pytorch.org/vision/stable/models.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/stable/models.html</a></li>
<li><a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html</a></li>
</ol>
<p>How the model builder works internally:
<a href=""https://pytorch.org/vision/master/_modules/torchvision/models/_api.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/master/_modules/torchvision/models/_api.html</a></p>
","0","Answer"
"78894269","78893376","<p>I think I have this figured out. After following the source definitions of TFIDFVectorizer which inherits (is that the way to say it in python?) from CountVectorizer, I was able to drop some print statements into the transform and _count_vocab methods in TFIDF and CV respectively.</p>
<p>I saw that while my dataframe column was successfully passing into the TFIDF transform method and then to the CV transform method and then to the CV _count_vocab method, when the _count_vocab method went to iterate through that dataframe column, it could not. The code from that implementation is below:</p>
<pre><code>#print(&quot;REMOVE****** RawDocs in _count_vocab-&gt;&quot;, raw_documents)
values = _make_int_array()
indptr.append(0)
for doc in raw_documents:
    #MY ERROR Showed up Here, this for loop only iterated once
    #print(&quot;REMOVE****** Individual Doc in _count_vocab-&gt;&quot;, doc)
    feature_counter = {}
    for feature in analyze(doc):
        try:
            feature_idx = vocabulary[feature]
            if feature_idx not in feature_counter:
                feature_counter[feature_idx] = 1
            else:
                feature_counter[feature_idx] += 1
        except KeyError:
            # Ignore out-of-vocabulary items for fixed_vocab=True
            continue
</code></pre>
<p>The solution was to modify my FeatureSelector to include an option to return an iterable list rather than a data frame column.</p>
<p>I dont know how I was able to pass a data frame column when not using a pipeline but, this appears to work inside of a pipeline.</p>
","2","Answer"
"78896837","78896800","<p>To me there seems like there are a bunch of errors.</p>
<p>First try <code>import pathlib from Path</code>.</p>
<p>Then try these to see where the actual error is:</p>
<pre><code>try:
if snapshot.exists():
    print(f&quot;resuming: {snapshot}&quot;)
    workspace.load_snapshot()
except Exception as e:
    print(f&quot;Error loading snapshot: {e}&quot;)

try:
    workspace.train()
except Exception as e:
    print(f&quot;Error during training: {e}&quot;)
</code></pre>
<p>Hope it helps</p>
","0","Answer"
"78897132","78885395","<p>Have you tried CNN (convolutional neural networks), in most cases CNN converges neatly with image data, i have worked on a similar project before with the following model Architecture :</p>
<pre><code># Building the model
model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))
model.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.15))
model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))
model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.20))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(rate=0.25))
model.add(Dense(43, activation='softmax'))

# Compilation of the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Model display
model.summary() 
</code></pre>
<p>I have a public notebook on Kaggle on it if you want to check it: <a href=""https://www.kaggle.com/code/yacharki/traffic-signs-image-classification-97-cnn"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/yacharki/traffic-signs-image-classification-97-cnn</a></p>
<p>It's also worth mentioning that preprocessing in very important.</p>
","1","Answer"
"78897258","78896943","<p>Pipelines, ColumnTransformers, GridSearch, and others all have attributes (and sometimes a custom <code>__getitem__</code> to access these like dictionaries) exposing their component parts, and similarly each of the transformers has fitted statistics as attributes, so it's just a matter of chaining these all together, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>(
    train_mod  # is a grid search, has the next attribute
    .best_estimator_ # is a pipeline, has steps accessible by getitem
    ['preprocess'] # is a columntransformer
    .named_transformers_ # fitted transformers, accessed by getitem
    ['cats']  # pipeline
    ['mode']  # simpleimputer
    .statistics_  # the computed modes, per column seen by this simpleimputer.
)
</code></pre>
","1","Answer"
"78898579","78429387","<p>I don't know your model structure, but if you use <code>tf.Variable</code> in model, this problem may arise.</p>
<pre class=""lang-py prettyprint-override""><code>class Re(layers.Layer):
    def __init__(self, **kwargs):
        super(self.__class__, self).__init__(**kwargs)
        self.gamma = tf.Variable(1.0, trainable=True, dtype=tf.float32)
        print(self.gamma.name)

    def call(self, x, training=None, **kwargs):
        x = self.gamma * x
        return x

class Generator(Model):
    def __init__(self, **kwargs):
        super(self.__class__, self).__init__(**kwargs)
        self.layer = Sequential([Re() for _ in range(2)])

    def call(self, x, training=None, **kwargs):
        x = self.layer(x, training=training)
        return x
</code></pre>
<p>The result of the print is</p>
<pre><code>Variable:0
Variable:0
</code></pre>
<p>They have the same name, resulting in a naming conflict.
So define variables under namescope in the current layer</p>
<pre class=""lang-py prettyprint-override""><code>class Re(layers.Layer):
    def __init__(self, **kwargs):
        super(self.__class__, self).__init__(**kwargs)
        with tf.name_scope(self.name_scope()):
            self.gamma = tf.Variable(1.0, trainable=True, dtype=tf.float32)
        print(self.gamma.name)

    def call(self, x, training=None, **kwargs):
        x = self.gamma * x
        return x
</code></pre>
<pre><code>re/Variable:0
re_1/Variable:0
</code></pre>
<p>Tip: The naming rule for the layer is <code>{class_name.lower()}_{order}</code> like <code>re_1</code></p>
<p>However if you use:</p>
<pre class=""lang-py prettyprint-override""><code>class Re(layers.Layer):
    def __init__(self, **kwargs):
        super(self.__class__, self).__init__(**kwargs)
        self.gamma = tf.Variable(1.0, trainable=True, dtype=tf.float32)
        print(self.gamma.name)

    def call(self, x, training=None, **kwargs):
        x = self.gamma * x
        return x


class Generator(Model):
    def __init__(self, **kwargs):
        super(self.__class__, self).__init__(**kwargs)
        self.layer1 = Re()
        self.layer2 = Re()

    def call(self, x, training=None, **kwargs):
        x = self.layer1(x, training=training)
        x = self.layer2(x, training=training)
        return x
</code></pre>
<pre><code>Variable:0
Variable:0
</code></pre>
<p>this problem can also be solved. idk why.</p>
","0","Answer"
"78900773","78898892","<p>You can freeze a layer with</p>
<pre class=""lang-py prettyprint-override""><code>layer = tf.keras.layers.Dense(12)
layer.trainable = False
</code></pre>
<p>You can also do that for a model, and then all layers of that model are frozen.</p>
<pre class=""lang-py prettyprint-override""><code>model.trainable = False
model.layers[0].trainable  # -&gt; False
</code></pre>
<hr />
<p>With the method in your question you can force a layer to behave like it is always in training or always in inference <strong>for the forward pass</strong>. For most layers this does not change a thing, e.g. a <code>Dense</code> layer behaves the same in training and validation for inference. But e.g. a <code>Dropout</code> layer behaves different in training or outside. Normally, a dropout layer is disabled outside of training.</p>
<pre class=""lang-py prettyprint-override""><code>inp = tf.keras.layers.Input((12,))
drop = tf.keras.layers.Dropout()(inp, training=True)
</code></pre>
<p>In this example, you get a <code>Monte Carlo Dropout</code> layer. This is a <code>Dropout</code> layer that is always active, regardless of training or test time.</p>
","0","Answer"
"78900907","78900691","<p>In pinecone V2:</p>
<pre><code>from langchain_pinecone import PineconeVectorStore

vector_store = PineconeVectorStore(index=index, embedding=embeddings)
</code></pre>
<p>But we need to pass index_name as well:</p>
<pre><code>from langchain_pinecone import PineconeVectorStore
vector_store = PineconeVectorStore(index=index, embedding=embeddings,index_name=index_name)
vector_store
</code></pre>
<p>also i case of from_documents() we have to pass index_name.</p>
<pre><code>db = vector_store.from_documents(documents,embeddings,index_name=index_name)
</code></pre>
<p>And last while creating index pass <code>dimension=2048</code>.</p>
<p>Worked for me, but dont have any idea why copied code from document itself not worked earlier.</p>
","0","Answer"
"78902181","78844904","<p>When you increase the <code>k</code> to 5 or any value greater than 1, the output <code>result</code> will be a vector containing the indices of the <code>k</code> nearest neighbours rather than a single index. This is why your code doesn't work as expected when you set <code>k = 5</code>.</p>
<p>I would highly recommend utilizing the debugger, here is a <a href=""https://au.mathworks.com/help/matlab/matlab_prog/debugging-process-and-features.html"" rel=""nofollow noreferrer"">help center page</a> and a <a href=""https://au.mathworks.com/videos/debugging-a-matlab-code-section-1680711990405.html"" rel=""nofollow noreferrer"">MATLAB video</a> to get you started.</p>
","0","Answer"
"78903805","78903794","<p>To ensure that the trained Random Forest model predicts the status_value for all 180 employees in the July dataset without splitting it, you need to make sure that the input data (X_pred) has the same format and columns as the data used for training (X_resampled).
Also, check the missing values and handle its and If you have a normalize data  during training, apply the same normalization.</p>
","0","Answer"
"78905685","78905671","<p>I think that you somewhere did not delete lists, numpy arrays. Because I have similar problem, and it was caused by usual list that I forgot to make empty after each iteration</p>
","-1","Answer"
"78907108","78903793","<p>Consider where your gradients are coming from. You want to backprop from your post-transformation result into your input image.</p>
<p>This requires a direct computational link between your result and your input.</p>
<p>Looking at the code, the input only participates in the computation at the final step:</p>
<pre class=""lang-py prettyprint-override""><code>    img_flat = tensor.reshape(-1, channels)
   
    pixel = (wa[:, None] * img_flat[y0 * tensor.shape[1] + x0] +
             wb[:, None] * img_flat[y0 * tensor.shape[1] + x1] +
             wc[:, None] * img_flat[y1 * tensor.shape[1] + x0] +
             wd[:, None] * img_flat[y1 * tensor.shape[1] + x1])
   
    result = pixel.reshape(height, width, channels)
    return result
</code></pre>
<p>That snippet is the only place where the input image is used. The input image does not participate in the computations of <code>wa, ... wd</code> or <code>y0, y1, x0, x1</code>. It only participates in the indexing operation.</p>
<p>This means that the only gradient signal sent back to the input image will be at the index values selected in the above snippet. The specific pixels selected by <code>img_flat[y0 * tensor.shape[1] + x0]</code> and the other similar operations are the only pixel values that will have gradient. There is no way around this.</p>
<p>Additionally, the vectors <code>wa, ... wd</code> have zero values. These will zero out additional gradients. Take for example <code>wa[:, None] * img_flat[y0 * tensor.shape[1] + x0]</code>. From that operation, we can only get gradient signal through the pixels defined by <code>y0 * tensor.shape[1] + x0</code> where <code>wa != 0</code>.</p>
<p>We can see this empirically:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import numpy as np

torch.manual_seed(42)
tensor = torch.rand(64, 64, 3, requires_grad=True)
M = [[1, 0, 10], [0, 1, 20]]
output_shape = (64, 64)

# remove function to work with intermediates 
height, width = output_shape
channels = tensor.shape[2]
device = tensor.device
dtype = tensor.dtype


# Convert M to tensor and compute inverse
M_tensor = torch.tensor(M, dtype=torch.float32, device=device)
M_padded = torch.cat([M_tensor, torch.tensor([[0, 0, 1]], dtype=torch.float32, device=device)], dim=0)
M_inv = torch.inverse(M_padded)[:2]
# Create coordinate grids
y_coords, x_coords = torch.meshgrid(torch.arange(height, device=device), torch.arange(width, device=device), indexing='ij')
coords = torch.stack([x_coords.flatten(), y_coords.flatten(), torch.ones_like(x_coords.flatten())], dim=0).float()
# Transform coordinates
src_coords = M_inv @ coords
src_x, src_y = src_coords[0], src_coords[1]
# Compute interpolation weights
x0 = src_x.long()
y0 = src_y.long()
x1 = x0 + 1
y1 = y0 + 1
x0 = torch.clamp(x0, 0, tensor.shape[1] - 1)
x1 = torch.clamp(x1, 0, tensor.shape[1] - 1)
y0 = torch.clamp(y0, 0, tensor.shape[0] - 1)
y1 = torch.clamp(y1, 0, tensor.shape[0] - 1)
dx = src_x - x0.float()
dy = src_y - y0.float()
# Perform bilinear interpolation
wa = (1 - dx) * (1 - dy)
wb = dx * (1 - dy)
wc = (1 - dx) * dy
wd = dx * dy
img_flat = tensor.reshape(-1, channels)

pixel = (wa[:, None] * img_flat[y0 * tensor.shape[1] + x0] +
         wb[:, None] * img_flat[y0 * tensor.shape[1] + x1] +
         wc[:, None] * img_flat[y1 * tensor.shape[1] + x0] +
         wd[:, None] * img_flat[y1 * tensor.shape[1] + x1])

result = pixel.reshape(height, width, channels)

# example loss calculation to get gradient
loss = result.mean()
loss.backward()

# get gradient
grad = tensor.grad
grad_nonzero = (grad != 0).sum()
fraction_nonzero = grad_nonzero / grad.numel()

print(f&quot;Gradient has {grad_nonzero} nonzero values, {fraction_nonzero:.4f} percent of total values&quot;)

# grab the index values used, ignore values where w_{i} == 0
valid_indices = torch.cat([
                    (y0 * tensor.shape[1] + x0)[wa != 0],
                    (y0 * tensor.shape[1] + x1)[wb != 0],
                    (y1 * tensor.shape[1] + x0)[wc != 0],
                    (y1 * tensor.shape[1] + x1)[wd != 0],
                ]).unique()

total_valid_indices = valid_indices.shape[0]
total_valid_grad_elements = total_valid_indices * 3 # pixel structure has 3 grad values per index
print(f&quot;Valid indices: {total_valid_indices}&quot;)
print(f&quot;Valid grad elements: {total_valid_grad_elements}&quot;)
</code></pre>
<p>When I run this code it prints the following:</p>
<pre><code>Gradient has 7128 nonzero values, 0.5801 percent of total values
Valid indices: 2376
Valid grad elements: 7128
</code></pre>
<p>We can see the gradient has 7128 nonzero values, which matches exactly the number computed from the unique indices used.</p>
<p>With the <code>M</code> and <code>output_shape</code> parameters used at the start, there are only 2376 specific pixel indices from the input image that are propagated to the output image. This results in 7128 (2376*3) non-zero gradient values in the input tensor.</p>
<p>This can be improved slightly by using some <code>detach</code> hacking to keep the gradients zeroed by the <code>w_{i}</code> arrays:</p>
<pre class=""lang-py prettyprint-override""><code>w_list = [wa, wb, wc, wd]
pixel_list = [
    img_flat[y0 * tensor.shape[1] + x0],
    img_flat[y0 * tensor.shape[1] + x1],
    img_flat[y1 * tensor.shape[1] + x0],
    img_flat[y1 * tensor.shape[1] + x1]
]

pixel = []

for i in range(len(w_list)):
    pixel.append((w_list[i][:,None] * pixel_list[i]).detach() + pixel_list[i] - pixel_list[i].detach())
    
pixel = torch.stack(pixel).sum(0)
</code></pre>
<p>This results in keeping all the gradients associated with <code>pixel_list[i]</code>.</p>
<p>For the parameters I used, this increases the nonzero gradient values from 7128 to 7425.</p>
<p>I don't think that can be improved on. The other pixel values have zero gradient because they simply do not participate in the computation. You cannot compute a gradient from the result to <code>pixel_{i}</code> if <code>pixel_{i}</code> does not participate in the computation of the output.</p>
<p>If you want there to be a gradient at all pixels, then all pixels need to participate in the computation of the output.</p>
","1","Answer"
"78907679","78907434","<p>So I made some minor adjustments to my script based on the feedback from the AI chat bot provided by google. See below:</p>
<pre><code>import pandas as pd
import numpy as np

def main():
    data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
    initial_data_df = pd.DataFrame(data,columns=['pounds','mpg'])

    number_of_iterations = 6
    weight = 0 # initialize weights
    bias = 0 # initialize weights
    weight_slope = 0
    bias_slope = 0
    final_results_df = pd.DataFrame()
    learning_rate = 0.01

    for i in range(number_of_iterations):
        loss = round(calculate_loss(initial_data_df,weight,bias),2)
        final_results_df = update_results(final_results_df,weight,bias,loss)
        weight_slope = find_weight_slope(initial_data_df,weight,bias)
        bias_slope = find_bias_slope(initial_data_df,weight,bias)
        weight = round(new_weight_update(weight,learning_rate,weight_slope),2)
        bias = round(new_bias_update(bias,learning_rate,bias_slope),2)
    print(final_results_df)

def calculate_loss(df,weight,bias):
    loss_summation = []
    for i in range(0,len(df)):
        loss_summation.append((df['mpg'][i]-((weight*df['pounds'][i])+bias))**2)
    return (sum(loss_summation)/len(df))

def update_results(df,weight,bias,loss):
    if df.empty:
        df = pd.DataFrame([[weight,bias,loss]],columns=['weight','bias','loss'])
    else:
        df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
    return df

def find_weight_slope(df,weight,bias):
    slope = []
    for i in range(0,len(df)):
        predicted_mpg = (weight*df['pounds'][i])+bias   #   predicted_mpg = (weight * car_weight) + bias
        error = predicted_mpg-df['mpg'][i]
        slope.append(error*2*df['pounds'][i])
    return sum(slope)/len(df)

def find_bias_slope(df,weight,bias):
    slope = []
    for i in range(0,len(df)):
        predicted_mpg = (weight*df['pounds'][i])+bias   #   predicted_mpg = (weight * car_weight) + bias
        error = predicted_mpg-df['mpg'][i]
        slope.append(2*error)
    return sum(slope)/len(df)

def new_weight_update(old_weight,lr,slope):
    return old_weight-(lr*slope)

def new_bias_update(old_bias,lr,slope):
    return old_bias-(lr*slope)

if __name__=='__main__':
    main()
</code></pre>
<p>With these changes I get the following results:</p>
<pre><code>   weight  bias    loss
0    0.00  0.00  303.71
0    1.20  0.34  170.67
0    2.05  0.59  103.22
0    2.66  0.77   68.66
0    3.09  0.91   51.13
0    3.40  1.01   42.11
</code></pre>
<p>I provided the calculated weighted slope to get the next updated weight which should be the third iteration weight value.</p>
<p><a href=""https://i.sstatic.net/f7Mpbd6t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f7Mpbd6t.png"" alt=""enter image description here"" /></a></p>
<p>With these results and documentation I would conclude that google's example results are incorrect for gradient descent.</p>
","0","Answer"
"78907713","78907434","<p>Maybe you are not implementing the MSE cost function correctly, or you're initializing the hyperparameters (like learning rate) with other values, but look the MSE results on both: your answer and the result provided by google website, they're decreasing in each iteration, that doesn't mean that the algorithm is learning??</p>
","0","Answer"
"78908422","78907557","<p>As stated in the up-to-date <a href=""https://docs.ultralytics.com/models/yolov10/?h=yolov10#usage-examples"" rel=""nofollow noreferrer"">Ultralytics documentation</a> it will be correct to use <code>from ultralytics import YOLO</code>. Here are some usage examples:</p>
<pre><code>from ultralytics import YOLO

# Load YOLOv10n model from scratch
model = YOLO(&quot;yolov10n.yaml&quot;)

# Load a pre-trained YOLOv10n model
model = YOLO(&quot;yolov10n.pt&quot;)

# Train the model
model.train(data=&quot;coco8.yaml&quot;, epochs=100, imgsz=640)

# Perform object detection on an image
results = model(&quot;image.jpg&quot;)

# Export the model to NCNN format
model.export(format=&quot;ncnn&quot;)  # creates 'yolov10n_ncnn_model'
</code></pre>
<p>As for the recommendation to use the YOLOv10 module, it comes from the earlier times when this model wasn't fully integrated into Ultralytics (approximately May 2024). For example, in <a href=""https://blog.roboflow.com/yolov10-how-to-train/"" rel=""nofollow noreferrer"">this Roboflow article</a> from May 24, it is proposed to install the <a href=""https://github.com/THU-MIG/yolov10"" rel=""nofollow noreferrer"">Official PyTorch implementation of YOLOv10</a> first and then use it with Ultralytics, which is excessive for now and may lead to the unwanted behavior like export issues. If you are using Ultralytics, follow the official tutorials and operate with v10 as you do with the v8 model.</p>
","1","Answer"
"78911818","78911068","<p>This is caused by batchnorm. Batchnorm behaves differently in train mode vs eval mode.</p>
<p>Batchnorm tracks the mean and variance of each batch run through the model and uses those values to compute a running mean and running variance of all batches.</p>
<p>In train mode, batchnorm normalizes with the current batch stats.</p>
<p>In eval mode, batchnorm normalizes with the running mean and running variance.</p>
<p>Your model is based off a pre-trained imagenet model. This means that when the model is in eval mode, the batchnorm layers use statistics they computed from training on imagenet.</p>
<p>When the model is in train mode, the batchnorm layers use in-batch statistics computed on the random input you pass to the model.</p>
<p>The random input has very different mean/var stats compared to imagenet, so you see a large difference.</p>
<p>If you fine-tune this model on whatever dataset you plan to use, then do a train/eval comparison on a real image from that dataset, you will see a smaller deviation between the outputs.</p>
","1","Answer"
"78911853","78911757","<p>ok i got your issue as per our discussion in comment and your updated information , you are trying to create a project using already provided template &quot;Model building, training, and deployment&quot; in AWS Sagemaker studio and it  requires <code>codecommit</code> as one of the resource to be created to deploy this project.
But the bad news is <code>Beginning on 06 June 2024, AWS CodeCommit ceased onboarding new customers. Going forward, only customers who have an existing repository in AWS CodeCommit will be able to create additional repositories.</code>
<a href=""https://simonwillison.net/2024/Jul/30/aws-codecommit-quietly-deprecated/#:%7E:text=Beginning%20on%2006%20June%202024,able%20to%20create%20additional%20repositories."" rel=""nofollow noreferrer"">link</a></p>
<p>Therfore i am sure your account is new after june 6, 2024 you are not able to create the project and facing the issue due to deprcation of <code>codecommit</code> service for new customers.</p>
","0","Answer"
"78912672","78912616","<p>C is Inverse of regularization strength; higher C means weaker regularization, lower C means stronger regularization.
Given Formula: As train samples increases, C decreases, leading to stronger regularization.
Reasoning: This is intentional, designed to prevent overfitting by applying more regularization when the dataset size increases.</p>
<p>Coefficient Matrix (coef): Represents the importance of each pixel (feature) in predicting the class.
Visualization: Reshaping coef into the image’s dimensions shows which pixels the model considers important, effectively &quot;drawing&quot; the digit as the model sees it.</p>
","0","Answer"
"78913606","78911839","<p>If you are using GPU(s) for training, some calculations can be non-deterministic on them. This usually only results in very small differences in the final scores, so if you just see some &quot;noise&quot; on it, then it could be a problem.</p>
<p>You can force TF to be as deterministic as possible using: <a href=""https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism"" rel=""nofollow noreferrer"">tf.config.experimental.enable_op_determinism()</a>
This might make your training a bit slower, but hopefully more deterministic.</p>
<p>You could also try a few very short trainings using CPU (which should be fully deterministic, if you seeded everything correctly), just to check if your seeding is OK.</p>
","1","Answer"
"78918592","78865593","<p>Based on my reading on your problem, very likely you don't have to train or tune your own model. Most of the foundation models should do a good job on it. All you need to do is probably just working on a good prompt. Describe your problem clearly to llm and give some good/bad examples. The only question is that if you want to fully automate the test case generation based on your input or not. If that's the case you will need to configure and make the api calls instead of doing it from UI (such as ChatGPT).</p>
","0","Answer"
"78918719","78753201","<p>There may be different reasons for the model not learning properly...</p>
<ol>
<li>Guarantee yourself the number of datapoints you have is enough for the parameters of your model, its complexity and the complexity of the signal you are trying to extract</li>
<li>Try to provide the data in a convolutional format (see <a href=""https://github.com/SebastienMc/series_to_supervised/blob/master/series_to_supervised.py"" rel=""nofollow noreferrer"">here</a>) and with both standardised input and output (you can do this via min max normalisation).</li>
<li>Play with different batch sizes to allow your stateless model to better interpret your data patterns globally for all batches. In some cases i saw better results with batch sizes that span enough to consider a wide enough sample that has recurrent charactersitics (given mine is a timeseries prediction problem).</li>
<li>Play with different units and model configurations backed by a reasonable idea on extracting, refining and predicting potentials</li>
</ol>
<p>Hope it helps. Lot of times is a backwards feeding loop for improval on your problem understanding, framing and codebase layout.</p>
","0","Answer"
"78922321","78917847","<p>The trainer expects a <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer"">DataLoader</a> wrapped around a <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"" rel=""nofollow noreferrer"">Dataset</a>.</p>
<p>If you only want to train a subset there are multiple possibly ways, from writing a custom Dataset class or using a custom <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler"" rel=""nofollow noreferrer"">Sampler</a> that only returns indices within a selected dataset range.</p>
<p>The quickest method will be to just create a DataLoader of the dataset you want to use:</p>
<pre class=""lang-py prettyprint-override""><code>trainer.train(
    model=model,
    training_params=train_params,
    train_loader=DataLoader(train_datasets[0], batch_size=16, shuffle=True, num_workers=4)
    valid_loader=DataLoader(train_datasets[0], batch_size=16, shuffle=False, num_workers=4)
)
</code></pre>
","1","Answer"
"78922613","78916012","<p>The problem is that tensorflow needs one value for the input, one for the output, currently your dataset will return three. You need to combine the first two:</p>
<p>You canset up your dataset like this by zipping the values.</p>
<pre class=""lang-py prettyprint-override""><code>X1 = tf.data.Dataset.from_tensors(vectorized_text)
X2 = tf.data.Dataset.from_tensors(content)
X = tf.data.Dataset.zip((X1,X2))
Y = tf.data.Dataset.from_tensors(likes)
dataset = tf.data.Dataset.zip((X, Y))

model = tf.keras.models.Model(inputs=[text_input, content_input], outputs=output)
model.compile(loss='mse',optimizer='Adam')
model.fit(dataset, epochs=10)
</code></pre>
","0","Answer"
"78925651","78885552","<ul>
<li><p>Your error does look like a syntax error in one of your yaml files.</p>
</li>
<li><p>Make you sure they are correct using your IDE linter/formatter or
using tnis website: <a href=""https://www.yamllint.com/"" rel=""nofollow noreferrer"">https://www.yamllint.com/</a> .</p>
</li>
</ul>
","0","Answer"
"78927927","78925914","<p>You need to send the data in a list, the content is on what column you trained and also how your scoring script accepts the data.</p>
<p>Example:
For below scoring script.</p>
<pre><code>def init():
    global model
    global input_schema
    # &quot;model&quot; is the path of the mlflow artifacts when the model was registered. For automl
    # models, this is generally &quot;mlflow-model&quot;.
    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model&quot;)
    model = mlflow.pyfunc.load_model(model_path)
    input_schema = model.metadata.get_input_schema()


def run(raw_data):
    json_data = json.loads(raw_data)
    if &quot;input_data&quot; not in json_data.keys():
        raise Exception(&quot;Request must contain a top level key named 'input_data'&quot;)

    serving_input = json.dumps(json_data[&quot;input_data&quot;])
    data = infer_and_parse_json_input(serving_input, input_schema)
    predictions = model.predict(data)

    result = StringIO()
    predictions_to_json(predictions, result)
    return result.getvalue()
</code></pre>
<p>and for below trained dataset  columns.</p>
<pre><code>&quot;columns&quot;: [
      &quot;age&quot;,
      &quot;sex&quot;,
      &quot;bmi&quot;,
      &quot;bp&quot;,
      &quot;s1&quot;,
      &quot;s2&quot;,
      &quot;s3&quot;,
      &quot;s4&quot;,
      &quot;s5&quot;,
      &quot;s6&quot;
    ]
</code></pre>
<p>the sample json input is.</p>
<pre><code>{&quot;input_data&quot;: {
    &quot;columns&quot;: [
      &quot;age&quot;,
      &quot;sex&quot;,
      &quot;bmi&quot;,
      &quot;bp&quot;,
      &quot;s1&quot;,
      &quot;s2&quot;,
      &quot;s3&quot;,
      &quot;s4&quot;,
      &quot;s5&quot;,
      &quot;s6&quot;
    ],
    &quot;data&quot;: [
      [ 1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0 ],
      [ 10.0,2.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0]
    ],
    &quot;index&quot;: [0,1]
  }}
</code></pre>
<p>You can find scoring script in the output files of training component.</p>
<p><img src=""https://i.imgur.com/jejP7gr.png"" alt=""enter image description here"" /></p>
<p>Not only scoring script you can find sample json file which should be given as input, but you also need to check scoring script what and all it accepts.</p>
<p>If you don't find any of the files scoring script or sample.json or schema.json.</p>
<p>You check you dataset for column trained and create an input like below.</p>
<pre><code>{&quot;data&quot;: [
 [1,2,3,4,5,6,7,8,9,10], 
 [10,9,8,7,6,5,4,3,2,1]
]}
</code></pre>
<p>Go through the samples and scoring script given in this <a href=""https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online"" rel=""nofollow noreferrer"">GitHub repository</a> for more information</p>
","0","Answer"
"78932316","78932302","<p>To find the most &quot;standard&quot; profile without a target variable, clustering is a good idea, but KMeans with a single cluster might oversimplify things. Instead, try using KMeans with multiple clusters (e.g., 3-5) and then analyze the centroids to find a representative profile. Each centroid will give you an average profile for that cluster.</p>
<p>Alternatively, you could use Principal Component Analysis (PCA) to identify the main characteristics that vary the least, giving you a sense of the &quot;standard&quot; features across the dataset.</p>
<p>RandomForest is more about classification or regression with a target variable, so it's less useful here. For an NLP approach, if you have a lot of text data, you could try Topic Modeling (like LDA) to find patterns in descriptions or job titles.</p>
<p>So, stick with KMeans clustering or PCA for now!</p>
","0","Answer"
"78932389","78932302","<p>Maybe <a href=""https://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow noreferrer"">Regression</a> or <a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow noreferrer"">Principle Component Analysis</a>.
Regression lets you find which variables or combination of variables are most significant and which don't matter.</p>
<p>(Note: &quot;Regression&quot; is because the original invention of the math was to show how species 'regressed' back to their perfect form. This was before evolution was accepted and it was thought that individuals born with deviations from the perfect form of the species were culled out by natural selection. So the 'perfect' leg length vs. overall height would be found from regression of many examples of the species.  Darwin's genius was to see natural selection as a creative force, not a stabilizing force.)</p>
","0","Answer"
"78933201","78625475","<p>In VITS, the input to the HiFi-GAN module is not a traditional mel-spectrogram, but rather latent variables produced by the Encoder, which is conditioned by the text and an alignment model. In this way, one can say that VITS is an end-to-end model.</p>
<p>The HiFi-GAN in VITS acts as a decoder that takes these latent variables and generates the final audio waveform directly. Therefore, what you are trying to plot is not a mel-spectrogram, but rather these latent variations.</p>
<p>This approach allows VITS to maintain flexibility and high-quality synthesis without relying on traditional interactive representations such as mel-spectrograms.</p>
","2","Answer"
"78937830","78937759","<h4>The iterator protocol when you don't define <code>__iter__</code></h4>
<p>What you are seeing here is the standard iteration protocol in Python for a class that does not implement <code>__iter__</code>.</p>
<p>When you write <code>for x, y in ds:</code>, and <code>ds</code> does not have an <code>__iter__</code> method, what Python runs is equivalent to something like this:</p>
<pre><code>i = 0
while True:
    try:
        x, y = ds[i]
    except IndexError:
        break
    [the body of your loop]
    i = i + 1
</code></pre>
<h4>What that means in your code</h4>
<p>In your code, that <code>IndexError</code> happens at <code>index==4141</code> because of the <code>window_size</code> parameter:</p>
<p>With <code>window_size=200</code>, <code>self.x[index: index + self.window]</code> is <code>self.x[4141:4341]</code> when <code>index==4141</code> but your <code>self.x</code> has len 4340. Hence it's that sub-expression that raises the <code>IndexError</code>. This is consistent with your assignment of <code>self.len</code> in <code>__init__</code> except Python does not rely on that to infer it's at the end, it waits for the <code>IndexError</code> exception.</p>
<h4>A minimal example illustrating the iterator protocol</h4>
<p>Here's a much shorter example that illustrates all this:</p>
<pre><code>class MySeq:
    data = list(range(10,15))

    def __len__(self):
        print(&quot;Called __len__&quot;)
        return len(self.data)

    def __getitem__(self, i):
        print(f&quot;Called __getitem__({i=})&quot;)
        return self.data[i]

myseq = MySeq()
for x in myseq:
    print(f&quot;Got {x=}&quot;)
</code></pre>
<p>When you run this, you get this output:</p>
<pre><code>Called __getitem__(i=0)
Got x=10
Called __getitem__(i=1)
Got x=11
Called __getitem__(i=2)
Got x=12
Called __getitem__(i=3)
Got x=13
Called __getitem__(i=4)
Got x=14
Called __getitem__(i=5)
</code></pre>
<p>That last line, calling <code>__getitem__(i=5)</code>, is going to raise <code>IndexError</code> since <code>len=5</code> the way I created my example, and that's what stops the iteration.</p>
<p>By contrast, if you add an <code>__iter__</code> function to this class:</p>
<pre><code>    def __iter__(self):
        print(&quot;Called __iter__&quot;)
        return iter(self.data)
</code></pre>
<p>the output becomes:</p>
<pre><code>Called __iter__
Got x=10
Got x=11
Got x=12
Got x=13
Got x=14
</code></pre>
<p>Notice that in either case, <code>__len__</code> does not get called, that's just not how iteration works in Python. Iteration just continues until an <code>IndexError</code> is raised if you don't implement <code>__iter__</code>. And if you do implement it, then the protocol relies on the <code>StopIteration</code> exception being raised to end iteration.</p>
","0","Answer"
"78939136","78938961","<p>It's probably overfitting, it could mean you don't have enough data, have a model that is too complex for the task (probably not the case this time), or a lot of other things.
by the way the model you provided is not a CNN model, if you use one you'll see much better results.</p>
","1","Answer"
"78939138","78938961","<p>It seems like the model is getting overfitted on the training dataset. To fix this you can do a few things:</p>
<ul>
<li><p><strong>Add more variety in the training data</strong>. All the training images might be too similar, so the model fails at different images. Mix it up. One way to do this is <a href=""https://keras.io/api/layers/preprocessing_layers/image_augmentation/"" rel=""nofollow noreferrer"">Data Augmentation</a>.</p>
</li>
<li><p><strong>Try simplifying the model.</strong> Simplifying the model can also help reduce overfitting.</p>
</li>
</ul>
<p>Also check out this <a href=""https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d"" rel=""nofollow noreferrer"">article</a> for more tips on how to prevent overfitting</p>
","1","Answer"
"78941280","78939808","<p>Setting nrounds = 0 doesn’t mean the model will use the base score for predictions. Instead, it means no boosting iterations will be performed, but the model still uses the initial guess based on the data provided.</p>
<p>Below are 2 points you need to complete your understanding</p>
<ol>
<li><p>Initial Guess: The base_score is used as an initial guess, but when nrounds = 0, the model doesn’t perform any boosting iterations. The predictions you see are likely influenced by the initial state of the model and the data provided.</p>
</li>
<li><p>Data Influence: Even without boosting iterations, the model might still be influenced by the data structure and the initial state of the model parameters.</p>
</li>
</ol>
<p>for addtional infor refer :<a href=""https://xgboost.readthedocs.io/en/stable/parameter.html"" rel=""nofollow noreferrer"">https://xgboost.readthedocs.io/en/stable/parameter.html</a></p>
","0","Answer"
"78941321","78940523","<p>You can use <a href=""https://pytorch.org/docs/stable/generated/torch.vmap.html"" rel=""nofollow noreferrer"">torch.vmap</a> for this exact purpose</p>
<pre class=""lang-py prettyprint-override""><code>class LinearMultidimModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearMultidimModel, self).__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, hidden_dim, output_dim))
        self.bias = nn.Parameter(torch.randn(output_dim))

    def forward(self, x):
        # Using torch.tensordot to perform the linear transformation
        out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
        return out
    
input_dim = 3
hidden_dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# create input with batch dimension
batch_size = 8
x = torch.randn(batch_size, input_dim, hidden_dim)

# example unbatched inference
y1 = torch.stack([model(i) for i in x])

# vmap model to make it batched
model_batched = torch.func.vmap(model)

# batch inference
y2 = model_batched(x)

# assert outputs are the same
assert torch.allclose(y1, y2)
</code></pre>
","1","Answer"
"78942388","78842184","<p>I used the template_match function of OpenCV to compare whether the &quot;Front&quot; image saved as a sample matches the &quot;Back&quot; image.</p>
<p>I succeeded in reducing the time to about 1 minute.</p>
<p><a href=""https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html"" rel=""nofollow noreferrer"">https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html</a></p>
","0","Answer"
"78943043","78942595","<p>Solved! I am sorry, the problem was outside of the aforementioned code. Correctness of best bandit choice was coded in a wrong way!</p>
<p>An interesting thing:
<code>np.choice(a_list)</code> returns <code>numpy.some_type</code> <code>variable</code>! And when you compare this <code>variable</code> to <code>another_list</code> then numpy broadcasts this <code>variable</code> and compares both as <em>array-likes</em>!</p>
<p>That was something I didn't know about / pay attention to, which made the actual error in code unbeknown to me.</p>
","0","Answer"
"78946715","78946669","<p>In Google Colab try:</p>
<pre><code>!pip install scikeras
from scikeras.wrappers import KerasRegressor
</code></pre>
<p>This should get you up and running with the current libraries on Google Colab (as of September 2024).</p>
<p>It looks like the <code>wrappers</code> module may no longer be availabele in <code>tensorflow</code>.</p>
","0","Answer"
"78947350","78946820","<p>I recommend first generating embeddings using the pre-trained model 'paraphrase-MiniLM-L6-v2' from the &quot;sentence_transformers&quot; library. These embeddings should then be stored in a Faiss index, which is a similarity search library developed by Facebook. Once this is set up, you can input new blog titles or other metadata into the Faiss index, and it will return the most similar blogs based on the embeddings.</p>
<p>Additionally, you can control the number of similar blogs returned by the Faiss index, specifying whether you want 5, 7, 10, or any other number of results. You can also set a similarity threshold so that only blogs meeting a certain level of similarity are returned.</p>
","0","Answer"
"78947507","78836514","<p>Try to build the layer before set weights -
<code>encoder.layers[0].build(w.shape)</code> and then
<code>encoder.layers[0].set_weights([w])</code></p>
","0","Answer"
"78951943","78949615","<p><strong>Short answer:</strong> Transposed convolutions and (regular) convolutions are not the same thing, so equations to determine the output shape for the latter do not apply to the former.</p>
<h2>Long answer</h2>
<p>You are asking for the shape of a <code>Conv2d</code> result, and you are using the equation of the <code>torch.nn.Conv2d</code> documentation. However, your code is using <em>transposed convolutions</em> (sometimes also misleadingly called &quot;deconvolutions&quot;), namely <code>torch.nn.ConvTranspose2d</code> layers – which is a whole different thing than (regular) convolutions. See, for example, <a href=""https://github.com/vdumoulin/conv_arithmetic"" rel=""nofollow noreferrer"">here</a> for a visual demonstration of various kinds of convolutions, including regular and transposed convolutions.</p>
<p>Using the equations for the output shapes provided in the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html"" rel=""nofollow noreferrer""><code>ConvTranspose2d</code> documentation</a>, you have</p>
<pre><code>H_out = (H_in - 1) · stride[0] - 2 · padding[0] +
        dilation[0] · (kernel_size[0] - 1) +
        output_padding[0] + 1
</code></pre>
<p>Thus, for the first block, with <code>H_in=1</code>, <code>stride=2</code>, <code>padding=0</code>¹,
<code>dilation=1</code>, <code>kernel_size=3</code>, <code>output_padding=0</code>, you will have</p>
<pre><code>H_out = (1 - 1) · 2 - 2 · 0 + 1 · (3 - 1) + 0 + 1
      = 0 - 0 + 2 + 0 + 1
      = 3
</code></pre>
<p>which is exactly what you saw. (The same equation applies to the width accordingly.)</p>
<p>Methodically, the layer-by-layer increasing size through the use of transposed convolutions that you see makes perfect sense to me, given that it is the role of the generator in a GAN to produce samples from lower-dimensional input noise.</p>
<p><sub>¹) Note here, that the <code>padding=1</code> argument of <code>make_gen_block()</code>'s signature is <em>not</em> passed on to the initialization of <code>ConvTranspose2d</code> in the method's body (in fact, it is not used, at all), so <code>ConvTranspose2d</code>'s default, <code>padding=0</code>, is actually applied instead.</sub></p>
","1","Answer"
"78952768","78902994","<p>The error specified occurs when the loss function receives tensors whose type differs from those specified when declaring the generator.
In your case, the output specification of your generator must contain int64 and float64 types.</p>
<pre><code>data_gen = DataGenerator(a, 3)
dataset = tf.data.Dataset.from_generator(
        data_gen,
        output_signature=(
            tf.TensorSpec(shape=(None,10), dtype=tf.int64), # &lt;- here
            tf.TensorSpec(shape=(None,), dtype=tf.float64))) # &lt;- and here
</code></pre>
<p>After you change types, your Colab example will start working:
<a href=""https://i.sstatic.net/19Q7nVH3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/19Q7nVH3.png"" alt=""Colab example"" /></a></p>
","0","Answer"
"78953574","78953273","<p>GCP is trying to load the model, but it can't find the <code>preprocess_text</code> function because it's not included in the serialized model.</p>
<p>Save the scikit-learn pipeline, functions like <strong>preprocess_text</strong> are not automatically saved with the model. To ensure that GCP knows where to find this function, you can either:</p>
<p>Define <code>preprocess_text</code> inside the same script where you're loading the model, or
Package utils as part of your deployment (including it in your GCP deployment files) so that the preprocess_text function is available in the same environment.</p>
<pre><code>import pickle
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

class CustomTextClassifier:
    def __init__(self):
        self.pipeline = Pipeline(
            [
                (&quot;preprocess&quot;, TfidfVectorizer(preprocessor=self.preprocess_text, ngram_range=(1, 2))),
                (&quot;clf&quot;, MultinomialNB(alpha=0.3)),
            ]
        )

    def preprocess_text(self, text):
        
        return text.lower() 

    def train(self, X, y):
        self.pipeline.fit(X, y)

    def predict(self, X):
        return self.pipeline.predict(X)


model = CustomTextClassifier()
# train model with your data...
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)
</code></pre>
","0","Answer"
"78954332","78950394","<p>It doesn't matter, the parameters are tracked both ways. If you use <code>shared_block = ...</code>, the parameters in <code>shared_block</code> will be referenced in your state dict (<code>model.state_dict()</code>) twice, once for <code>self.nested1</code> and again for <code>self.nested2</code>.</p>
<p>If you use the <code>self.shared_block = ...</code> approach, the state dict will reference the parameters a third time in <code>MyModule</code> itself.</p>
<p>Either way, the parameters are tracked and <code>model.parameters()</code> will return a non-duplicated set of parameters.</p>
<p>You can run this code to look at a simplified version</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

class SharedBlock(nn.Module):
    def __init__(self):
        super().__init__()

        self.block = nn.Linear(8, 8)

    def forward(self, x):
        return self.block(x)

class MyNestedModule(nn.Module):
    def __init__(self, shared_block):
        super().__init__()

        self.shared_block = shared_block

    def forward(self, x):
        return self.shared_block(x)
    
class MyModule1(nn.Module):
    def __init__(self):
        super().__init__()
        shared_block = SharedBlock()

        self.nested1 = MyNestedModule(shared_block)
        self.nested2 = MyNestedModule(shared_block)

    def forward(self, x):
        x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
        y_1 = self.nested1(x_1)
        y_2 = self.nested2(x_2)
        return y_1, y_2
    
class MyModule2(nn.Module):
    def __init__(self):
        super().__init__()
        self.shared_block = SharedBlock()

        self.nested1 = MyNestedModule(self.shared_block)
        self.nested2 = MyNestedModule(self.shared_block)

    def forward(self, x):
        x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
        y_1 = self.nested1(x_1)
        y_2 = self.nested2(x_2)
        return y_1, y_2
    
    
model1 = MyModule1()
print(model1.state_dict())
print(list(model1.parameters()))

model2 = MyModule2()
print(model2.state_dict())
print(list(model2.parameters()))
</code></pre>
","1","Answer"
"78959091","78885552","<p>@naruto007 Check line 229 in your yaml to see if syntax is correct. Use the link YassineLbk put in above to validate the yaml file(s).</p>
","0","Answer"
"78960974","78917847","<p>As said by Karl and Daraan, I was using ConcatDataset with an array of DataLoader instead of Dataset, I discovered it by printing the object type:</p>
<pre><code>print(f&quot;Type: {type(train_datasets[0])}&quot;)
</code></pre>
<p>Then I used the train_datasets[0].dataset of the dalaloader and it works.</p>
","0","Answer"
"78961289","78959447","<p>To perform efficient matrix multiplication with a tridiagonal matrix on the GPU using PyTorch, you can try those leads :</p>
<ul>
<li>Use batched tridiagonal matrix multiplication: PyTorch provides some support for batched operations on banded matrices using torch.bmm (batched matrix-matrix multiplication), but it requires you to handle the band structure yourself. For a tridiagonal matrix, we only need to store the main, upper, and lower diagonals.</li>
<li>Custom implementation with CUDA kernels or use specialized libraries: For more advanced optimization, consider writing custom CUDA kernels for tridiagonal matrix multiplication or use specialized libraries like cuSPARSE or SciPy.</li>
</ul>
<p>I try an approach of it :</p>
<pre><code>import torch

def band_matrix_mult(A: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Multiplies a tridiagonal matrix A with matrix B, assuming A is tridiagonal.
    
    Args:
    - A (torch.Tensor): Tridiagonal matrix of shape (N, N).
    - B (torch.Tensor): Matrix to multiply with A, of shape (N, M).

    Returns:
    - torch.Tensor: Resulting matrix of shape (N, M).
    &quot;&quot;&quot;
    N, M = B.shape

    # Extract diagonals from A
    main_diag = torch.diagonal(A)  # Shape: [N]
    upper_diag = torch.diagonal(A, offset=1)  # Shape: [N-1]
    lower_diag = torch.diagonal(A, offset=-1)  # Shape: [N-1]

    # Initialize result matrix
    result = torch.zeros_like(B)

    # Multiply main diagonal
    result += main_diag.unsqueeze(-1) * B

    # Multiply upper diagonal
    result[:-1] += upper_diag.unsqueeze(-1) * B[1:]

    # Multiply lower diagonal
    result[1:] += lower_diag.unsqueeze(-1) * B[:-1]

    return result

# Example usage
N = 5
A = torch.tensor([[2., 3., 0., 0., 0.],
                  [1., 2., 3., 0., 0.],
                  [0., 1., 2., 3., 0.],
                  [0., 0., 1., 2., 3.],
                  [0., 0., 0., 1., 2.]])
B = torch.randn((5, 4))

# Efficient tridiagonal multiplication
result = band_matrix_mult(A, B)
print(result)

</code></pre>
<p>if we talk about performance :
This custom function leverages the tridiagonal structure for efficiency and should have a time complexity of
𝑂(𝑁^2⋅𝑀), which is significantly better than the 𝑂(𝑁^3) complexity of dense matrix multiplication.</p>
<p>If you're working with extremely large matrices or need even more performance optimization, consider using GPU-based libraries or writing a custom CUDA kernel to take full advantage of GPU parallelism for sparse matrix operations.</p>
","1","Answer"
"78961308","78961133","<p>Here is my implementation :</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.model_selection import train_test_split

# Sample data provided
data = np.array([
    [0.77, -1.14, 0], [-0.33, 1.44, 0], [0.91, -3.07, 0], [-0.37, -1.91, 0],
    [-1.84, -1.13, 0], [-1.50, 0.34, 0], [-0.63, -1.53, 0], [-1.08, -1.23, 0],
    [0.39, -1.99, 0], [-1.26, -2.90, 0], [-5.27, -0.78, 0], [-0.49, -2.74, 0],
    [1.48, -3.74, 0], [-1.64, -1.96, 0], [0.45, 0.36, 0], [-1.48, -1.17, 0],
    [-2.94, -4.47, 0], [-2.19, -1.48, 0], [0.02, -0.02, 0], [-2.24, -2.12, 0],
    [-3.17, -3.69, 0], [-4.09, 1.03, 0], [-2.41, -2.31, 0], [-3.45, -0.61, 0],
    [-3.96, -2.00, 0], [-2.95, -1.16, 0], [-2.42, -3.35, 0], [-1.74, -1.10, 0],
    [-1.61, -1.28, 0], [-2.59, -2.21, 0], [-2.64, -2.20, 0], [-2.84, -4.12, 0],
    [-1.45, -2.26, 0], [-3.98, -1.05, 0], [-2.97, -1.63, 0], [-0.68, -1.52, 0],
    [-0.10, -3.43, 0], [-1.14, -2.66, 0], [-2.92, -2.51, 0], [-2.14, -1.62, 0],
    [-3.33, -0.44, 0], [-1.05, -3.85, 0], [0.38, 0.95, 0], [-0.05, -1.95, 0],
    [-3.20, -0.22, 0], [-2.26, 0.01, 0], [-1.41, -0.33, 0], [-1.20, -0.71, 0],
    [-1.69, 0.80, 0], [-1.52, -1.14, 0], [3.88, 0.65, 1], [0.73, 2.97, 1],
    [0.83, 3.94, 1], [1.59, 1.25, 1], [3.92, 3.48, 1], [3.87, 2.91, 1],
    [1.14, 3.91, 1], [1.73, 2.80, 1], [2.95, 1.84, 1], [2.61, 2.92, 1],
    [2.38, 0.90, 1], [2.30, 3.33, 1], [1.31, 1.85, 1], [1.56, 3.85, 1],
    [2.67, 2.41, 1], [1.23, 2.54, 1], [1.33, 2.03, 1], [1.36, 2.68, 1],
    [2.58, 1.79, 1], [2.40, 0.91, 1], [0.51, 2.44, 1], [2.17, 2.64, 1],
    [4.38, 2.94, 1], [1.09, 3.12, 1], [0.68, 1.54, 1], [1.93, 3.71, 1],
    [1.26, 1.17, 1], [1.90, 1.34, 1], [3.13, 0.92, 1], [0.85, 1.56, 1],
    [1.50, 3.93, 1], [2.95, 2.09, 1], [0.77, 2.84, 1], [1.00, 0.46, 1],
    [3.19, 2.32, 1], [2.92, 2.32, 1], [2.86, 1.35, 1], [0.97, 2.68, 1],
    [1.20, 1.31, 1], [1.54, 2.02, 1], [1.65, 0.63, 1], [1.36, -0.22, 1],
    [2.63, 0.40, 1], [0.90, 2.05, 1], [1.26, 3.54, 1], [0.71, 2.27, 1],
    [1.96, 0.83, 1], [2.52, 1.83, 1], [2.77, 2.82, 1], [4.16, 3.34, 1]
])

# Split data into features (X) and labels (y)
X = data[:, :2]
y = data[:, 2]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)


# Perceptron class definition
class Perceptron():
    def __init__(self, num_features):
        self.num_features = num_features
        self.weights = np.random.rand(num_features) * 0.1  # Small random weights initialization
        self.bias = 0.0
        
    def forward(self, x):
        # Compute the linear output and apply step function
        linear = np.dot(x, self.weights) + self.bias
        predictions = np.where(linear &gt; 0, 1, 0)
        return predictions
        
    def backward(self, x, y, predictions):
        # Calculate the error
        errors = y - predictions
        # Update weights and bias using the perceptron rule
        self.weights += self.learning_rate * np.dot(x.T, errors)
        self.bias += self.learning_rate * np.sum(errors)
        return errors
        
    def train(self, x, y, epochs, learning_rate=0.01):
        self.learning_rate = learning_rate
        for e in range(epochs):
            # Calculate predictions for entire batch
            predictions = self.forward(x)
            # Update weights and bias after processing the entire batch
            self.backward(x, y, predictions)
                
    def evaluate(self, x, y):
        # Predict on the test data
        predictions = self.forward(x)
        # Calculate accuracy
        accuracy = np.mean(predictions == y)
        return accuracy

# Using the Perceptron class
perceptron = Perceptron(num_features=2)
perceptron.train(X_train, y_train, epochs=100, learning_rate=0.01)
accuracy = perceptron.evaluate(X_test, y_test)
accuracy_train = perceptron.evaluate(X_train, y_train)

# final result
accuracy, accuracy_train

</code></pre>
<p>I use, train_test_split before calling the function to test it quickly.
Here are some errors I spot in your code :</p>
<ul>
<li>Batch Training: The training is now using the entire batch for weight updates.</li>
<li>Predictions Shape: The forward function has been adapted for batch input.</li>
<li>Learning Rate and Epochs: Adjust the number of epochs and learning rate for convergence.</li>
</ul>
","0","Answer"
"78965003","78963755","<p>Transpose is not required, because you can multiply by the matrix either from right or left. In particular, multiply from the left <code>(out_features, in_features) * (in_features)</code>, to get  <code>(out_features)</code> vector.</p>
<p>The benefit is deep in computer architecture. Matrix <code>(out_features, in_features)</code> is stored row by row in memory, e.g. for 3 <code>in_features</code>, it looks like that in flat memory</p>
<pre><code>[in0, in1, in2], [in0, in1, in2], ..., [in0, in1, in2]
</code></pre>
<p>This allows to access memory in <a href=""https://stackoverflow.com/questions/16699247/what-is-a-cache-friendly-code""><em>cache-friendly</em> way</a>, because you read 3 consecutive numbers to compute the first output feature, then read 3 consecutive numbers to compute the second output feature, etc</p>
","2","Answer"
"78966233","78963755","<p>Transposing the weight matrix makes the backward pass more efficient while adding almost no overhead to the forward pass. You can read more about this <a href=""https://discuss.pytorch.org/t/why-does-the-linear-module-seems-to-do-unnecessary-transposing/6277"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/pytorch/pytorch/issues/2159"" rel=""nofollow noreferrer"">here</a></p>
","0","Answer"
"78970464","78959131","<p>I ran into this issue before and just clearing the cache with <code>torch.cuda.empty_cache()</code> did not help. You need to explicitly clear the allocated memory on cuda via <a href=""https://pytorch.org/docs/stable/generated/torch.cuda.reset_peak_memory_stats.html"" rel=""nofollow noreferrer""><code>torch.cuda.reset_peak_memory_stats()</code></a>. Include these lines into your <code>run_vllm_eval()</code> function:</p>
<pre><code>torch.cuda.reset_peak_memory_stats()
torch.cuda.synchronize()
</code></pre>
<p>Here's a complete code snippet with some borrowed code from <code>vllm</code>'s <a href=""https://docs.vllm.ai/en/latest/getting_started/quickstart.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>import torch
import gc
from vllm import LLM, SamplingParams

def run_vllm_eval(model_name, sampling_params, path_2_eval_dataset):
    # Instantiate LLM in a function
    llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

    # Run some VLLM inference or evaluation here (simplified)
    result = llm.generate(path_2_eval_dataset, sampling_params)
    print(result)

    # Clean up after inference
    del llm
    gc.collect()
    torch.cuda.empty_cache()
    
    # Reset CUDA device to fully clear memory
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.synchronize()  # Wait for all streams on the current device

prompts = [
    &quot;Hello, my name is&quot;,
    &quot;The president of the United States is&quot;,
    &quot;The capital of France is&quot;,
    &quot;The future of AI is&quot;,
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

run_vllm_eval(model_name=&quot;facebook/opt-125m&quot;, sampling_params=sampling_params, path_2_eval_dataset=prompts)
run_vllm_eval(model_name=&quot;facebook/opt-125m&quot;, sampling_params=sampling_params, path_2_eval_dataset=prompts)
</code></pre>
","3","Answer"
"78972465","78969962","<p>The reason is simple, the provided model from PyTorch is not made to segment body parts but cities elements.</p>
<p>The confusion came from the name of the provided model by Keras-io.</p>
<p>As I need to avoid installing PyTorch and TensorFlow on the same project, but I can use <code>onnxruntime</code>, I only had to convert the (old Keras version) model to ONNX format.</p>
<p>So, my snippet (with normalization) was OK, that was only the bad model that was used.</p>
<p>PS: I share the model here <a href=""https://huggingface.co/Metal3d/deeplabv3p-resnet50-human"" rel=""nofollow noreferrer"">https://huggingface.co/Metal3d/deeplabv3p-resnet50-human</a></p>
","0","Answer"
"78973894","78958361","<p>I don't think this is possible: sklearn pipelines don't support transforming the target variable. See <a href=""https://stackoverflow.com/a/62826301/10495893"">https://stackoverflow.com/a/62826301/10495893</a> for some notes about that.</p>
<p>(There is <code>TransformedTargetRegressor</code>, but that's for e.g. log-transforming the target before fitting a regressor. I don't think there's a way to hack it to working with a classifier.)</p>
<p>IMO, since throughout much of sklearn <code>y</code> is taken to be 1D, that should happen outside pipelines. You probably don't need <code>to_numpy</code>, just slicing to a pandas Series should be enough, and could be done sooner in your workflow, e.g. <code>y = df['clinical_course']</code>.</p>
","1","Answer"
"78974674","78974474","<p>If you are already using LLMs this means you need a lot of compute power, so it does not seem like a good idea to me to then circle back to a simple binary vector and use that for the actual clustering, since you might have a lot of information loss from that step compared to how well the LLM actually encoded the semantics.</p>
<p>It would probably be much more efficient to either use something like <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/kmeans.py"" rel=""nofollow noreferrer"">SentenceTransformers for embedding + k-Means Clustering</a> if you just want clusters/groups or use something like <a href=""https://huggingface.co/learn/nlp-course/en/chapter5/6?fw=pt"" rel=""nofollow noreferrer"">FAISS</a> to efficiently create and perform similarity search in a vector database (a database of all embedded documents). If the latter is too much of a hassle you can also just use any library that allows you to calculate similarity metrics between vectors and apply this to the (normalized) embedded documents.</p>
","0","Answer"
"78975490","78975293","<p>Predicting the sum of two variables is a linear task and very easy for neural networks. <strong>At the core of a neuron it calculates a weighted sum of its input values.</strong></p>
<hr />
<p>This &quot;network&quot; below is fully sufficient to solve the task in just one epoch.
The math behind the network is: <code>x*weight1 + y*weight</code> and it just needs to learn to set both its weights to 1.0.</p>
<pre class=""lang-py prettyprint-override""><code>  class myModel(nn.Module):
    def __init__(self):
      super().__init__()

      self.input = nn.Linear(2,1)

    def forward(self,x):
      return self.input(x)
</code></pre>
<p>By using <code>relu</code> you make it actually harder for the network as negative values cannot go trough. Still the network below with two units is still enough for this easy task.</p>
<pre class=""lang-py prettyprint-override""><code>def createModel():
  class myModel(nn.Module):
    def __init__(self):
      super().__init__()

      self.input = nn.Linear(2,2)
      self.output = nn.Linear(2,1)

    def forward(self,x):
      x = F.relu(self.input(x))
      return self.output(x)
</code></pre>
<p>The math behind the network is <code>relu(x@weights11 + y@weights12)@weight2</code> now just with weights of length two.
Ideally will the network now return in layer1 <code>[-(x+y), x+y]</code>, which after the relu it will contain <code>0</code> and <code>abs(x+y)</code> the second layer now needs to only fix the sign and multiply by 1 or -1 to yield (x+y).</p>
<hr />
<p>In the first variant form the network is basically a <a href=""https://en.wikipedia.org/wiki/Linear_regression"" rel=""nofollow noreferrer"">linear regression</a> and you can think of the second one as two chained regression. I recommend that you familiarize yourself with this little bit of math, because in the end some basic good to knows are:</p>
<ul>
<li><strong>linear neurons are just a weighted sum of its inputs followed optionally by an activation.</strong></li>
<li>A layer of neurons is just a collection of different weights for the inputs</li>
<li>Other neuron types change only the order how inputs and weights are combined, e.g. convolutional neurons apply identical weights to inputs selected from a grid.</li>
</ul>
<p>So yes your model learns very quickly to do this simply task and it could even be much faster ;)</p>
","1","Answer"
"78976871","78976058","<p>When the dataset type is <code>MLTable</code>, you also need to give the <code>MLTable</code> file defining the files to be included and there transformations.</p>
<p>Next, <code>.xlsx</code> is not supported.</p>
<p>Refer <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&amp;tabs=cli#supported-file-types"" rel=""nofollow noreferrer"">here</a> for supported files in <code>MLTable</code>.</p>
<p>Below is the sample file you should be having.</p>
<pre><code>$schema: https://azuremlschemas.azureedge.net/latest/MLTable.schema.json 

paths:
    - pattern: ./*.csv

transformations:
  - read_delimited:
      delimiter: &quot;,&quot;
      header: all_files_same_headers
      encoding: utf8
</code></pre>
<p>Or</p>
<p>you can directly give the filename.</p>
<pre><code>paths: 
  - file: ./titanic.csv
transformations: 
  - read_delimited: 
      delimiter: ',' 
      encoding: 'ascii' 
      empty_as_string: false
      header: from_first_file
</code></pre>
<p>So, in your case the <code>MLTable</code> file is</p>
<pre><code>paths: 
  - file: ./CarPrice_Assignment.csv
transformations: 
  - read_delimited: 
      delimiter: ','
</code></pre>
<p>The folder should contain files like below.</p>
<p><img src=""https://i.imgur.com/7CVzp4F.png"" alt=""enter image description here"" /></p>
<p>And in mltable file you add the content according to your files.</p>
<p>Refer more about working with mltable files <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&amp;tabs=cli"" rel=""nofollow noreferrer"">here</a>.</p>
","1","Answer"
"78983681","78981288","<p>Yes this seems to be how they have written this.</p>
<p>If you see the full stack trace, you will see that <a href=""https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/validation.py#L129-L168"" rel=""nofollow noreferrer"">this</a> code gets called eventually where the check for <code>Nan</code> does happen.</p>
<p>One workaround is to impute it but also create a feature called <code>is_nan</code> so that model knows when it's actually missing and if your model is complex enough it could learn to ignore the imputed value when the <code>is_nan</code> feature is true.</p>
<p>I agree that the class should have supported <code>Nan</code>s, so it might be worth filing a bug request.</p>
","1","Answer"
"78983927","78835380","<p>To do cross-device reductions in TFF, we must use TFF's special <code>intrinsic</code> symbols--essentially, these 'register' certain reductions (e.g., the <code>reduce_sum</code> above) as special, so that they can be identified later as the ones that the user intended to use to express 'this is a reduction that should go cross-device now'.</p>
<p>In TFF, pure tensorflow logic is <em>always</em> 'running locally', rather than extracted to run cross-device. This means that the <code>tff.tensorflow.computation</code> you have above (<code>custom_weighted_aggregate</code>) is really expressing a 'per-client reduction', rather than a cross-client reduction.</p>
<p>One way you might express such a thing, if your <code>values</code> and <code>weights</code> are placed at clients, could be that captured in <a href=""https://github.com/google-parfait/tensorflow-federated/blob/main/tensorflow_federated/python/aggregators/mean.py#L124"" rel=""nofollow noreferrer"">this implementation</a>. Or, alternatively, I believe that implementation should be directly usable from <a href=""https://www.tensorflow.org/federated/api_docs/python/tff/aggregators/MeanFactory"" rel=""nofollow noreferrer"">this symbol</a>, whose <code>create</code> symbol should return you an aggregation process to whom you can pass custom weights.</p>
","1","Answer"
"78989530","78981288","<p>Yes, there seems to be a discrepancy between the behavior of <code>RandomForestClassifier</code> and <code>ClassifierChain</code>.</p>
<p>I think you should use <code>SimpleImputer</code> with <code>strategy=&quot;constant&quot;</code> or <code>strategy=&quot;most_frequent&quot;</code> to fill the missing values.</p>
<p>In case the above solution does't work, you might want to use <code>MultiOutputClassifier</code> with the <code>base_estimator</code> set to <code>RandomForestClassifier</code>.</p>
","0","Answer"
"78992759","78992679","<p>Your input needs three channels. That is, the image has to be in <code>RGB/BGR</code> format, no matter if the actual content is grayscale. Grayscale content is one-channel only, so you need to replicate the grayscale channel two extra times, and use that image array as input to the network.</p>
<p>There's more than a way to achieve this, I prefer to use <code>OpenCV</code> for this operation, but keep in mind this conversion won't be part of the tensorflow graph, since it uses an external library and runs on the CPU. It could be inside a generator function, though. The conversion would be something like this, assuming your input is a <code>BGR</code> image:</p>
<pre><code># To Grayscale (1-channel):
inputImage = cv2.cvtColor(inputImage, cv2.COLOR_BGR2GRAY) 
# Back to BGR (3-channels):
inputImage  = cv2.cvtColor(inputImage , cv2.COLOR_GRAY2BGR)
</code></pre>
<p>What follows is the usual scaling to <code>[0.0, 1.0]</code> with <code>rescale</code>.</p>
<p>Another option, using the <a href=""https://www.tensorflow.org/api_docs/python/tf/image"" rel=""nofollow noreferrer"">tf.image</a> functions to convert to <code>grayscale</code> then back to <code>RGB</code>. The preprocessing function can be then, called from an <code>ImageDataGenerator</code>:</p>
<pre><code>def convertGrayscale(image):
    image = tf.image.rgb_to_grayscale(image)
    image = tf.image.grayscale_to_rgb(image)
    return image

tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1/255,
    preprocessing_function=convertGrayscale
)
</code></pre>
<p>Be aware, however, that <code>ImageDataGenerator</code> is now deprecated.</p>
","0","Answer"
"78993969","78991472","<p>A solution using scikit-learn <code>MultiLabelBinarizer</code>, which is precisely designed for what you want to do:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.preprocessing import MultiLabelBinarizer
import pandas as pd

df = pd.DataFrame(
    {
        &quot;food&quot;: [
            &quot;pepper steak&quot;, 
            &quot;baked salmon&quot;
        ],
        &quot;ingredients&quot;:[
            &quot;black pepper,steak&quot;, 
            &quot;salmon,salt,black pepper&quot;
        ]
    }
)

mlb = MultiLabelBinarizer()
ingredients_lists = df[&quot;ingredients&quot;].str.split(&quot;,&quot;)
ingredients_dummies_df = pd.DataFrame(
    mlb.fit_transform(ingredients_lists),
    columns = mlb.classes_,
    index=df.index
)

df = pd.concat([df, ingredients_dummies_df], axis= 1)
</code></pre>
<p>Beware that in your precise case, you may have to remove blanks before ingredient names to avoid creating a column &quot; pepper&quot; (with a preceding white space) in addition to the column &quot;pepper&quot;. For instance,</p>
<pre><code>ingredients_lists = df[&quot;ingredients&quot;].str.replace(&quot;, &quot;, &quot;,&quot;).str.split(&quot;,&quot;)
</code></pre>
","1","Answer"
"78995441","78995310","<p>Yes, there is.</p>
<p>We can use base. Convert <code>mst.mod</code> to <code>matrix</code>, apply <code>which()</code> to find indices where <code>1</code> occurs, and, for instance, convert to a list.</p>
<pre class=""lang-r prettyprint-override""><code>mst.mod = ape::mst(dist(iris))
unstack(as.data.frame(which(as.matrix(mst.mod)==1L, arr.ind=TRUE)))

</code></pre>
<p>giving</p>
<pre class=""lang-r prettyprint-override""><code>&gt; |&gt; head()
$`1`
[1]  5 18 28 40

$`2`
[1] 13 35 46

$`3`
[1] 48

$`4`
[1] 30 48

$`5`
[1]  1 38

$`6`
[1] 11 19
</code></pre>
<p>For <code>1</code>, besides <code>18</code> and <code>40</code> there are <code>5</code> and <code>28</code>.
Depending on the desired output, <code>which(as.matrix(mst.mod )==1L, arr.ind=TRUE)</code> might be enough. Haven't checked the documentation/help files if there is a more direct way using <code>{ape}</code>.</p>
","2","Answer"
"78996069","78996028","<p>Unless you're using Unsupervised Learning (or regression/forecasting where observations are available after the fact), traditionally you need lots of (expensive to acquire) labelled data to train a model. More recently &quot;few-shot&quot; or &quot;one-shot&quot; learning has emerged which (using a pre-trained model) can learn based on a handful of labelled examples - but you still need labelled data, just not nearly as much.</p>
<p>So your statement &quot;does it mean that it is supposed to be labeled manually&quot; .. &quot;what's the point of training and serving a model then?&quot; doesn't really make sense as you cannot train a model without labelled (at least some) data manually.</p>
<p>Secondly - there's the issue of &quot;data drift&quot;. A model trained on data which has characteristics that change over time (fraud detection in particular because bad actors are always looking for new methods) will degrade in performance, so you need to monitor and retrain using new (labelled) data. Using the fraud detection example - if you notice that the model is missing some new fraud technique you need to find examples and label them, and then retrain the model. Also note, it's highly unlikely that your original dataset will result in 100% accuracy anyway - the expert may get it wrong/miss some examples so the model will always have some uncertainty - especially with something that's rare and hard to define.</p>
","1","Answer"
"78996306","78995759","<p>We don't yet have an interface to do that (as we do with regression or classification models).</p>
<p>I wrote some <em>unsupported</em> code on tidymodels.org for an article there to create <a href=""https://www.tidymodels.org/learn/statistics/survival-metrics/figs/cal-both-1.svg"" rel=""nofollow noreferrer"">this figure</a>. <a href=""https://github.com/tidymodels/tidymodels.org/blob/main/learn/statistics/survival-metrics/index.qmd#L420"" rel=""nofollow noreferrer"">Here it is</a>, look at the code chunk labeled <code>prep-cal-for-shiny</code>.</p>
<p>It is not production-ized or tested but you might be able to use it for some analysis.</p>
","0","Answer"
"78998355","78763327","<p><a href=""https://medium.com/@manish.thota1999/an-experiment-to-unlock-ollamas-potential-video-question-answering-e2b4d1bfb5ba"" rel=""nofollow noreferrer"">https://medium.com/@manish.thota1999/an-experiment-to-unlock-ollamas-potential-video-question-answering-e2b4d1bfb5ba</a></p>
<pre><code>python3 /home/myles/llama.cpp/examples/llava/llava_surgery_v2.py -C -m /home/myles/llama.cpp/checkpoint-300



import argparse
import os
import json
import re

import torch
import numpy as np
from gguf import *

from transformers import SiglipModel, SiglipProcessor, SiglipVisionModel


TEXT = &quot;clip.text&quot;
VISION = &quot;clip.vision&quot;


def k(raw_key: str, arch: str) -&gt; str:
    return raw_key.format(arch=arch)


def should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -&gt; bool:
    if name in (
        &quot;logit_scale&quot;,
        &quot;text_model.embeddings.position_ids&quot;,
        &quot;vision_model.embeddings.position_ids&quot;,
    ):
        return True

    if has_llava and name in [&quot;visual_projection.weight&quot;, &quot;vision_model.post_layernorm.weight&quot;, &quot;vision_model.post_layernorm.bias&quot;]:
        return True

    if name.startswith(&quot;v&quot;) and not has_vision:
        return True

    if name.startswith(&quot;t&quot;) and not has_text:
        return True

    return False


def get_tensor_name(name: str) -&gt; str:
    if &quot;projection&quot; in name:
        return name
    if &quot;mm_projector&quot; in name:
        name = name.replace(&quot;model.mm_projector&quot;, &quot;mm&quot;)
        name = re.sub(r'mm\.mlp\.mlp', 'mm.model.mlp', name, count=1)
        name = re.sub(r'mm\.peg\.peg', 'mm.model.peg', name, count=1)
        return name

    return name.replace(&quot;text_model&quot;, &quot;t&quot;).replace(&quot;vision_model&quot;, &quot;v&quot;).replace(&quot;encoder.layers&quot;, &quot;blk&quot;).replace(&quot;embeddings.&quot;, &quot;&quot;).replace(&quot;_proj&quot;, &quot;&quot;).replace(&quot;self_attn.&quot;, &quot;attn_&quot;).replace(&quot;layer_norm&quot;, &quot;ln&quot;).replace(&quot;layernorm&quot;, &quot;ln&quot;).replace(&quot;mlp.fc1&quot;, &quot;ffn_down&quot;).replace(&quot;mlp.fc2&quot;, &quot;ffn_up&quot;).replace(&quot;embedding&quot;, &quot;embd&quot;).replace(&quot;final&quot;, &quot;post&quot;).replace(&quot;layrnorm&quot;, &quot;ln&quot;)


def bytes_to_unicode():
    &quot;&quot;&quot;
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a significant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    &quot;&quot;&quot;
    bs = (
        list(range(ord(&quot;!&quot;), ord(&quot;~&quot;) + 1))
        + list(range(ord(&quot;¡&quot;), ord(&quot;¬&quot;) + 1))
        + list(range(ord(&quot;®&quot;), ord(&quot;ÿ&quot;) + 1))
    )
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


ap = argparse.ArgumentParser()
ap.add_argument(&quot;-m&quot;, &quot;--model-dir&quot;, help=&quot;Path to model directory cloned from HF Hub&quot;, required=True)
ap.add_argument(&quot;--use-f32&quot;, action=&quot;store_true&quot;, default=False, help=&quot;Use f32 instead of f16&quot;)
ap.add_argument(&quot;--text-only&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;Save a text-only model. It can't be used to encode images&quot;)
ap.add_argument(&quot;--vision-only&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;Save a vision-only model. It can't be used to encode texts&quot;)
ap.add_argument(&quot;--clip-model-is-vision&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;The clip model is a pure vision model (ShareGPT4V vision extract for example)&quot;)
ap.add_argument(&quot;--clip-model-is-openclip&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;The clip model is from openclip (for ViT-SO400M type))&quot;)
ap.add_argument(&quot;--llava-projector&quot;, help=&quot;Path to llava.projector file. If specified, save an image encoder for LLaVA models.&quot;)
ap.add_argument(&quot;--projector-type&quot;, help=&quot;Type of projector. Possible values: mlp, ldp, ldpv2&quot;, choices=[&quot;mlp&quot;, &quot;ldp&quot;, &quot;ldpv2&quot;], default=&quot;mlp&quot;)
ap.add_argument(&quot;-o&quot;, &quot;--output-dir&quot;, help=&quot;Directory to save GGUF files. Default is the original model directory&quot;, default=None)
# Example --image_mean 0.48145466 0.4578275 0.40821073 --image_std 0.26862954 0.26130258 0.27577711
# Example --image_mean 0.5 0.5 0.5 --image_std 0.5 0.5 0.5
default_image_mean = [0.48145466, 0.4578275, 0.40821073]
default_image_std = [0.26862954, 0.26130258, 0.27577711]
ap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)
ap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)

# with proper
args = ap.parse_args()


if args.text_only and args.vision_only:
    print(&quot;--text-only and --image-only arguments cannot be specified at the same time.&quot;)
    exit(1)

if args.use_f32:
    print(&quot;WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.&quot;)

# output in the same directory as the model if output_dir is None
dir_model = args.model_dir

if args.clip_model_is_vision or not os.path.exists(dir_model + &quot;/vocab.json&quot;) or args.clip_model_is_openclip:
    vocab = None
    tokens = None
else:
    with open(dir_model + &quot;/vocab.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        vocab = json.load(f)
        tokens = [key for key in vocab]

with open(dir_model + &quot;/config.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
    config = json.load(f)
    if args.clip_model_is_vision:
        v_hparams = config
        t_hparams = None
    else:
        v_hparams = config[&quot;vision_config&quot;]
        t_hparams = config[&quot;text_config&quot;]

# possible data types
#   ftype == 0 -&gt; float32
#   ftype == 1 -&gt; float16
#
# map from ftype to string
ftype_str = [&quot;f32&quot;, &quot;f16&quot;]

ftype = 1
if args.use_f32:
    ftype = 0

if args.clip_model_is_vision or args.clip_model_is_openclip:
    model = SiglipVisionModel.from_pretrained(dir_model)
    processor = None
else:
    model = SiglipModel.from_pretrained(dir_model)
    processor = SiglipProcessor.from_pretrained(dir_model)

fname_middle = None
has_text_encoder = True
has_vision_encoder = True
has_llava_projector = False
if args.text_only:
    fname_middle = &quot;text-&quot;
    has_vision_encoder = False
elif args.llava_projector is not None:
    fname_middle = &quot;mmproj-&quot;
    has_text_encoder = False
    has_llava_projector = True
elif args.vision_only:
    fname_middle = &quot;vision-&quot;
    has_text_encoder = False
else:
    fname_middle = &quot;&quot;

output_dir = args.output_dir if args.output_dir is not None else dir_model
os.makedirs(output_dir, exist_ok=True)
output_prefix = os.path.basename(output_dir).replace(&quot;ggml_&quot;, &quot;&quot;)
fname_out = os.path.join(output_dir, f&quot;{fname_middle}model-{ftype_str[ftype]}.gguf&quot;)
fout = GGUFWriter(path=fname_out, arch=&quot;clip&quot;)

fout.add_bool(&quot;clip.has_text_encoder&quot;, has_text_encoder)
fout.add_bool(&quot;clip.has_vision_encoder&quot;, has_vision_encoder)
fout.add_bool(&quot;clip.has_llava_projector&quot;, has_llava_projector)
fout.add_file_type(ftype)
model_name = config[&quot;_name_or_path&quot;] if &quot;_name_or_path&quot; in config else os.path.basename(dir_model)
fout.add_name(model_name)
if args.text_only:
    fout.add_description(&quot;text-only CLIP model&quot;)
elif args.vision_only and not has_llava_projector:
    fout.add_description(&quot;vision-only CLIP model&quot;)
elif has_llava_projector:
    fout.add_description(&quot;image encoder for LLaVA&quot;)
    # add projector type
    fout.add_string(&quot;clip.projector_type&quot;, args.projector_type)
else:
    fout.add_description(&quot;two-tower CLIP model&quot;)

if has_text_encoder:
    assert t_hparams is not None
    assert tokens is not None
    # text_model hparams
    fout.add_uint32(k(KEY_CONTEXT_LENGTH, TEXT), t_hparams[&quot;max_position_embeddings&quot;])
    fout.add_uint32(k(KEY_EMBEDDING_LENGTH, TEXT), t_hparams[&quot;hidden_size&quot;])
    fout.add_uint32(k(KEY_FEED_FORWARD_LENGTH, TEXT), t_hparams[&quot;intermediate_size&quot;])
    fout.add_uint32(&quot;clip.text.projection_dim&quot;, t_hparams.get(&quot;projection_dim&quot;, config[&quot;projection_dim&quot;]))
    fout.add_uint32(k(KEY_ATTENTION_HEAD_COUNT, TEXT), t_hparams[&quot;num_attention_heads&quot;])
    fout.add_float32(k(KEY_ATTENTION_LAYERNORM_EPS, TEXT), t_hparams[&quot;layer_norm_eps&quot;])
    fout.add_uint32(k(KEY_BLOCK_COUNT, TEXT), t_hparams[&quot;num_hidden_layers&quot;])
    fout.add_token_list(tokens)

if has_vision_encoder:
    # vision_model hparams
    fout.add_uint32(&quot;clip.vision.image_size&quot;, v_hparams[&quot;image_size&quot;])
    fout.add_uint32(&quot;clip.vision.patch_size&quot;, v_hparams[&quot;patch_size&quot;])
    fout.add_uint32(k(KEY_EMBEDDING_LENGTH, VISION), v_hparams[&quot;hidden_size&quot;])
    fout.add_uint32(k(KEY_FEED_FORWARD_LENGTH, VISION), v_hparams[&quot;intermediate_size&quot;])
    fout.add_uint32(&quot;clip.vision.projection_dim&quot;, v_hparams.get(&quot;projection_dim&quot;, config[&quot;projection_dim&quot;]))
    fout.add_uint32(k(KEY_ATTENTION_HEAD_COUNT, VISION), v_hparams[&quot;num_attention_heads&quot;])
    fout.add_float32(k(KEY_ATTENTION_LAYERNORM_EPS, VISION), v_hparams[&quot;layer_norm_eps&quot;])
    block_count = v_hparams[&quot;num_hidden_layers&quot;] - 1 if has_llava_projector else v_hparams[&quot;num_hidden_layers&quot;]
    fout.add_uint32(k(KEY_BLOCK_COUNT, VISION), block_count)
                            #     /**
                            #      &quot;image_grid_pinpoints&quot;: [
                            #         [
                            #         336,
                            #         672
                            #         ],
                            #         [
                            #         672,
                            #         336
                            #         ],
                            #         [
                            #         672,
                            #         672
                            #         ],
                            #         [
                            #         1008,
                            #         336
                            #         ],
                            #         [
                            #         336,
                            #         1008
                            #         ]
                            #     ],
                            #     Flattened:
                            #     [
                            #         336, 672,
                            #         672, 336,
                            #         672, 672,
                            #         1008, 336,
                            #         336, 1008
                            #     ]
                            #  *
                            #  */
    if &quot;image_grid_pinpoints&quot; in v_hparams:
        # flatten it
        image_grid_pinpoints = []
        for pinpoint in v_hparams[&quot;image_grid_pinpoints&quot;]:
            for p in pinpoint:
                image_grid_pinpoints.append(p)
        fout.add_array(&quot;clip.vision.image_grid_pinpoints&quot;, image_grid_pinpoints)
    if &quot;image_crop_resolution&quot; in v_hparams:
        fout.add_uint32(&quot;clip.vision.image_crop_resolution&quot;, v_hparams[&quot;image_crop_resolution&quot;])
    if &quot;image_aspect_ratio&quot; in v_hparams:
        fout.add_string(&quot;clip.vision.image_aspect_ratio&quot;, v_hparams[&quot;image_aspect_ratio&quot;])
    if &quot;image_split_resolution&quot; in v_hparams:
        fout.add_uint32(&quot;clip.vision.image_split_resolution&quot;, v_hparams[&quot;image_split_resolution&quot;])
    if &quot;mm_patch_merge_type&quot; in v_hparams:
        fout.add_string(&quot;clip.vision.mm_patch_merge_type&quot;, v_hparams[&quot;mm_patch_merge_type&quot;])
    if &quot;mm_projector_type&quot; in v_hparams:
        fout.add_string(&quot;clip.vision.mm_projector_type&quot;, v_hparams[&quot;mm_projector_type&quot;])


    if processor is not None:
        image_mean = processor.image_processor.image_mean if args.image_mean is None or args.image_mean == default_image_mean else args.image_mean  # pyright: ignore[reportAttributeAccessIssue]
        image_std = processor.image_processor.image_std if args.image_std is None or args.image_std == default_image_std else args.image_std  # pyright: ignore[reportAttributeAccessIssue]
    else:
        image_mean = args.image_mean if args.image_mean is not None else default_image_mean
        image_std = args.image_std if args.image_std is not None else default_image_std
    fout.add_array(&quot;clip.vision.image_mean&quot;, image_mean)
    fout.add_array(&quot;clip.vision.image_std&quot;, image_std)

use_gelu = v_hparams[&quot;hidden_act&quot;] == &quot;gelu&quot;
fout.add_bool(&quot;clip.use_gelu&quot;, use_gelu)


if has_llava_projector:
    model.vision_model.encoder.layers.pop(-1)  # pyright: ignore[reportAttributeAccessIssue]
    projector = torch.load(args.llava_projector)
    for name, data in projector.items():
        name = get_tensor_name(name)
        # pw and dw conv ndim==4
        if data.ndim == 2 or data.ndim == 4:
            data = data.squeeze().numpy().astype(np.float16)
        else:
            data = data.squeeze().numpy().astype(np.float32)

        fout.add_tensor(name, data)

    print(&quot;Projector tensors added\n&quot;)

state_dict = model.state_dict()  # pyright: ignore[reportAttributeAccessIssue]
for name, data in state_dict.items():
    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_llava_projector):
        # we don't need this
        print(f&quot;skipping parameter: {name}&quot;)
        continue

    name = get_tensor_name(name)
    data = data.squeeze().numpy()

    n_dims = len(data.shape)

    # ftype == 0 -&gt; float32, ftype == 1 -&gt; float16
    ftype_cur = 0
    if n_dims == 4:
        print(f&quot;tensor {name} is always saved in f16&quot;)
        data = data.astype(np.float16)
        ftype_cur = 1
    elif ftype == 1:
        if name[-7:] == &quot;.weight&quot; and n_dims == 2:
            print(&quot;  Converting to float16&quot;)
            data = data.astype(np.float16)
            ftype_cur = 1
        else:
            print(&quot;  Converting to float32&quot;)
            data = data.astype(np.float32)
            ftype_cur = 0
    else:
        if data.dtype != np.float32:
            print(&quot;  Converting to float32&quot;)
            data = data.astype(np.float32)
            ftype_cur = 0

    print(f&quot;{name} - {ftype_str[ftype_cur]} - shape = {data.shape}&quot;)
    fout.add_tensor(name, data)


fout.write_header_to_file()
fout.write_kv_data_to_file()
fout.write_tensors_to_file()
fout.close()

print(&quot;Done. Output file: &quot; + fname_out)
</code></pre>
<pre><code> ./llama-llava-cli -m /home/myles/llama.cpp/Meta-Llama-3.1-8B-Instruct/meta-llama-3.1-8B-instruction_f32.gguf --mmproj /home/myles/llama.cpp/vit/mmproj-model-f16.gguf  --image /home/myles/Desktop/extreme_ironing.jpg -c 4096 --n-gpu-layers 33 -p &quot;how many cars are shown in the image&quot; 
</code></pre>
<p>crate a config file json like so</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;google/siglip-so400m-patch14-384&quot;,
  &quot;architectures&quot;: [
    &quot;siglip_vision_model&quot;
  ],
  &quot;attention_dropout&quot;: 0.0,
  &quot;dropout&quot;: 0.0,
  &quot;hidden_act&quot;: &quot;silu&quot;,
  &quot;hidden_size&quot;: 1152,
  &quot;image_size&quot;: 384,
  &quot;initializer_factor&quot;: 1.0,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 4304,
  &quot;layer_norm_eps&quot;: 1e-06,
  &quot;model_type&quot;: &quot;siglip_vision_model&quot;,
  &quot;num_attention_heads&quot;: 16,
  &quot;num_channels&quot;: 3,
  &quot;num_hidden_layers&quot;: 27,
  &quot;patch_size&quot;: 14,
  &quot;projection_dim&quot;: 768,
  &quot;torch_dtype&quot;: &quot;bfloat16&quot;,
  &quot;transformers_version&quot;: &quot;4.45.0.dev0&quot;,
  &quot;image_aspect_ratio&quot;: &quot;anyres_max_9&quot;,
  &quot;image_grid_pinpoints&quot;: [
    [
      384,
      384
    ],
    [
      384,
      768
    ],
    [
      384,
      1152
    ],
    [
      384,
      1536
    ],
    [
      384,
      1920
    ],
    [
      384,
      2304
    ],
    [
      768,
      384
    ],
    [
      768,
      768
    ],
    [
      768,
      1152
    ],
    [
      768,
      1536
    ],
    [
      768,
      1920
    ],
    [
      768,
      2304
    ],
    [
      1152,
      384
    ],
    [
      1152,
      768
    ],
    [
      1152,
      1152
    ],
    [
      1152,
      1536
    ],
    [
      1152,
      1920
    ],
    [
      1152,
      2304
    ],
    [
      1536,
      384
    ],
    [
      1536,
      768
    ],
    [
      1536,
      1152
    ],
    [
      1536,
      1536
    ],
    [
      1536,
      1920
    ],
    [
      1536,
      2304
    ],
    [
      1920,
      384
    ],
    [
      1920,
      768
    ],
    [
      1920,
      1152
    ],
    [
      1920,
      1536
    ],
    [
      1920,
      1920
    ],
    [
      1920,
      2304
    ],
    [
      2304,
      384
    ],
    [
      2304,
      768
    ],
    [
      2304,
      1152
    ],
    [
      2304,
      1536
    ],
    [
      2304,
      1920
    ],
    [
      2304,
      2304
    ]
  ],
  &quot;initializer_range&quot;: 0.02,
  &quot;max_position_embeddings&quot;: 131072,
  &quot;mlp_bias&quot;: false,
  &quot;mm_hidden_size&quot;: 1152,
  &quot;mm_newline_position&quot;: &quot;grid&quot;,
  &quot;mm_patch_merge_type&quot;: &quot;spatial_unpad&quot;,
  &quot;mm_projector_type&quot;: &quot;mlp2x_gelu&quot;,
  &quot;mm_spatial_pool_mode&quot;: &quot;bilinear&quot;,
  &quot;mm_tunable_parts&quot;: &quot;mm_vision_tower,mm_mlp_adapter,mm_language_model&quot;,
  &quot;mm_use_im_patch_token&quot;: false,
  &quot;mm_use_im_start_end&quot;: false,
  &quot;mm_vision_select_feature&quot;: &quot;patch&quot;,
  &quot;mm_vision_select_layer&quot;: -2,
  &quot;mm_vision_tower&quot;: &quot;google/siglip-so400m-patch14-384&quot;,
  &quot;mm_vision_tower_lr&quot;: 2e-06
}
</code></pre>
","0","Answer"
"79005180","79005084","<p>I figured out the issue. The evaluation json format was incorrect
We simply need to use:</p>
<pre><code> {
    &quot;metrics&quot;: {
        &quot;mse&quot;: {
            &quot;value&quot;: 6107087691.964753
        },
        &quot;mae&quot;: {
            &quot;value&quot;: 46717.104932016475
        },
        &quot;rmse&quot;: {
            &quot;value&quot;: 78147.85788468391
        },
        &quot;r2&quot;: {
            &quot;value&quot;: 0.9062238811893062
        }
    }
}
</code></pre>
<p>Earlier I got confused because when I tried to add an evaluation job manually to a model registry I was getting an error that the JSON requires metric_groups and metric_data</p>
","0","Answer"
"79006664","78976316","<p>Sometimes it will work but most of the it wont work</p>
","-2","Answer"
"79007343","78985137","<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>
<p><a href=""https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>
<p>The second one is used to match your model with the devices:</p>
<p><a href=""https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>
<p>So basically, in your case, you can use the following code:</p>
<pre><code>from accelerate import dispatch_model, infer_auto_device_map

model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)

***
...
new_model = CustomModel(model)
...
***

device_map_dict = infer_auto_device_map(new_model)
dispatch_model(new_model, device_map_dict)
</code></pre>
<p>P.S. This code still needs to be tested on fine-tuning.</p>
","2","Answer"
"79008175","79000230","<p>Use <code>torch.Generator(device='cpu')</code>.</p>
<p>You shouldn't do anything cuda related inside your dataloader, especially if it's running multiple workers. Pull a batch from your dataloader and move the outputs to cuda after they are collated.</p>
<p>The generator for a dataloader sets the RNG state for sampling. It expects a CPU tensor. This is why you get the error</p>
<p><code>RuntimeError: Expected a 'cpu' device type for generator but found 'cuda'</code></p>
<p>Cuda generators are used for generating random numbers on-GPU inside a cuda process - they shouldn't be used for dataloaders.</p>
<p>The <code>Cannot re-initialize CUDA in forked subprocess</code> is caused by trying to do cuda stuff inside a forked process. Hard to say without the original code but it was probably caused by returning a cuda tensor from getitem.</p>
","0","Answer"
"79009807","79009698","<p>Your training data has <code>2924</code> columns. When you trained your model using <code>model.fit</code>, presumably, it made a random forest that takes in <code>2924</code> features. Hence, for inference, you need to give the same amount of features. Whereas you are giving <code>8</code> features only: <code>3</code> from <code>one-hot encoding of location</code>, <code>3</code> from <code>one-hot encoding of type</code>, <code>2</code> from <code>cost</code> and <code>votes</code>.</p>
<p>Please be mindful of any preprocessing you apply to training data. You must apply the same preprocessing to test data—otherwise they belong to different distributions. I believe it will be helpful, perhaps crucial, to understand the difference between <code>fit</code> and <code>fit_transform</code> <a href=""https://datascience.stackexchange.com/q/12321"">here</a> and <a href=""https://stackoverflow.com/a/43296172/4498050"">here</a>.</p>
","0","Answer"
"79010016","79009687","<p>to fix your problem you need to change the fit function. Your fit function should pass inputs as list instead of dictionary:</p>
<pre><code>history = complex_model_2_1.fit([X_train_A, X_train_B],
                                y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))
</code></pre>
<p><strong>Explanation</strong><br>
you either pass inputs as dictionary or as list. To pass inputs as lists you initiate keras model like this:</p>
<pre><code>complex_model_2_1 = keras.Model(inputs=[input_A,input_B], outputs=[output])
</code></pre>
<p>If you want to use inputs as dictionary you should pass dict argument as inputs to the model. But in this case during training and inference all datasets must be passed as lists.</p>
<pre><code>complex_model_2_1 = keras.Model(inputs={'wide_input':input_A,                                        
                                        'deep_input':input_B}, outputs=[output])
</code></pre>
","0","Answer"
"79010023","79010018","<p>You are not specifying the number of possible labels for your sequence classification model, which means you only allow the model to predict the same class for every single data point.</p>
<p>Just modify the part where you load the model to:</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
</code></pre>
","0","Answer"
"79016947","79016443","<p>The issue can be solved by replacing the code with <code>X = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])</code>.</p>
","0","Answer"
"79017406","79016929","<p>The problem is that you read the test data into <code>test_data</code>, but then use the original DataFrame, <code>df</code>, containing the training data, to make the test set.</p>
<p>Change this line:</p>
<pre><code>test_data_array = np.asarray(df[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
</code></pre>
<p>To:</p>
<pre><code>test_data_array = np.asarray(test_data[[&quot;message&quot;, &quot;fingers&quot;, &quot;tail&quot;]])
</code></pre>
<p>And you should have the correct number of predictions.</p>
<p>Remember to also compare <code>y_prediction</code> to <code>test_data['species']</code>.</p>
","2","Answer"
"79024244","79021064","<p>I changed the training argument's value for batch_sampler from</p>
<p><code> batch_sampler=BatchSamplers.GROUP_BY_LABEL</code></p>
<p>to</p>
<p><code>batch_sampler=BatchSamplers.NO_DUPLICATES</code></p>
<p>and the problem was solved.  Originally <code>GROUP_BY_LABEL</code> was selected as the <a href=""https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss"" rel=""nofollow noreferrer"">documentation for this loss calculation</a> recommended it but switching it seems to have cleared this up.</p>
","0","Answer"
"79024649","79024541","<p>In Your code You use line:</p>
<pre><code>model.trainable = False
</code></pre>
<p>This line means that your model can't be trained. If you want do transfer learning You have to unfreeze some layers.</p>
<p>Some example how to unfreeze last 2 layers, try to experiment with different values:</p>
<pre><code># Unfreeze the top layers of the base model
for layer in base_model.layers[-2:]:  
    if not isinstance(layer, layers.BatchNormalization):
        layer.trainable = True
    else:
        layer.trainable = False  
</code></pre>
<p>BatchNormalization  layers are frozen because for small dataset it may cause problems with convergence (values in this layer will be unstable).</p>
","0","Answer"
"79025778","78996950","<p>import the class of onxx implementation where onnx runtime is used before any other imports</p>
<pre><code>import ArcFace
import ...

ImportError: DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.
</code></pre>
<p>this will prevent complect between dependencies</p>
","-1","Answer"
"79026429","79026315","<p>I believe the parameter you're looking for is <code>base_url</code>:</p>
<pre class=""lang-py prettyprint-override""><code>llm = ChatOpenAI(
    model=model,
    temperature = temperature,
    base_url='localhost:3005' # might need to try with 127.0.0.1:3005
)
</code></pre>
<p>More info <a href=""https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"" rel=""nofollow noreferrer"">here</a></p>
","0","Answer"
"79027342","79027142","<p>Changing the scaling to use <code>/ (math.sqrt(self.head_dim))</code> solves this issue. I'm not sure why that happens, because <code>self.head_dim ** -0.5</code> should be equivalent to <code>/ (math.sqrt(self.head_dim))</code>. Maybe someone else can explain?</p>
","0","Answer"
"79028395","79028217","<p>When two variables in a regression are correlated, it complicates the interpretation of the coefficients. Normally, a coefficient tells you how much the dependent variable is expected to change for a one-unit increase in the independent variable, holding everything else constant. But when two variables are highly correlated, this &quot;holding everything else constant&quot; part gets tricky.</p>
<p>Here’s what happens:</p>
<ol>
<li>The standard errors of the coefficients get larger, making them less precise.</li>
<li>The coefficients themselves can become unstable, meaning they can change a lot with even slight changes in the data.</li>
<li>The signs or values of the coefficients can be misleading, suggesting a relationship that isn’t really there.</li>
</ol>
<p>This doesn’t mean the regression is useless, but you should be careful when interpreting the results if your variables are correlated. You might need to drop one of the correlated variables or combine them to avoid these issues.</p>
","1","Answer"
"79028416","79028217","<p>Seems you are asking about multicollinearity - when independent features in a linear regression model are highly correlated with each other.</p>
<p>Let's imagine that we want to buy a used car. Most likely, the mileage (<strong>x1</strong>) will strongly correlate with the age of the car (<strong>x2</strong>): the older the car, the more km it has probably traveled, and vice versa.<br />
The linear regression model looks like this:
<code>F(x) = w0 + w1x1 + w2x2</code>, where <strong>w</strong> are the weights, the &quot;contribution&quot; of each feature <strong>x</strong> to the target variable.</p>
<p>If the mileage coef (<strong>w1</strong>) is big, it may mean that used cars with lower mileage have higher prices, and the age of the car coef (<strong>w2</strong>) may be also high. As a result, it is difficult to understand what unique information each feature brings. It may be difficult to explain their individual effects (uncertainty), and it's leading to problems such as <strong>multicollinearity</strong>. With no math - multicollinearity is very bad :-)</p>
<p>Usually such features are removed or transformed (for example, one feature can be made - <code>milage per year</code> for example)</p>
","1","Answer"
"79030288","79029841","<p>Your <code>.pkl</code> file is likely a python pickle. You should be able to use <a href=""https://docs.rs/serde-pickle/latest/serde_pickle/"" rel=""nofollow noreferrer"">serde_pickle</a> to deserialize it.</p>
","1","Answer"
"79030306","79030256","<p>You could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>fillna</code></a> with empty strings (<code>''</code>) and define those as the missing values, also slice the output to make it 1D:</p>
<pre><code>imputer = SimpleImputer(strategy='most_frequent', missing_values='')
df['Fuel_Type'] = imputer.fit_transform(df[['Fuel_Type']].fillna(''))[:, 0]
</code></pre>
<p>Output:</p>
<pre><code>      Car      Fuel_Type Transmission
0  Toyota       Gasoline    Automatic
1   Honda  E85 Flex Fuel         None
2   Tesla  E85 Flex Fuel    Automatic
3    None         Hybrid       Manual
4    Ford  E85 Flex Fuel       Manual
</code></pre>
<p>If you want to handle all columns:</p>
<pre><code>imputer = SimpleImputer(strategy='most_frequent', missing_values='')
df[:] = imputer.fit_transform(df.fillna(''))
</code></pre>
<p>Output:</p>
<pre><code>      Car      Fuel_Type Transmission
0  Toyota       Gasoline    Automatic
1   Honda  E85 Flex Fuel    Automatic
2   Tesla  E85 Flex Fuel    Automatic
3    Ford         Hybrid       Manual
4    Ford  E85 Flex Fuel       Manual
</code></pre>
","0","Answer"
"79033818","79033786","<p>Looking at <a href=""https://pypi.org/project/causalml/#files"" rel=""nofollow noreferrer"">causalml on PyPi</a>, there are no wheels available for Python 3.12.</p>
<p>However, there are wheels for Python 3.8 - 3.11. You may have to downgrade your Python to a supported version perhaps 3.11 and try again.</p>
","1","Answer"
"79035164","79033026","<blockquote>
<p>If rounding occurs, does this mean some data samples might be skipped in each epoch?</p>
</blockquote>
<p>A fractional step simply means that the set of examples in the very last step of an epoch is smaller than the batch size.</p>
<ul>
<li>To round down the step size means to ignore this last set of examples. This can be somewhat okay to do if the batch size is small in relation to the dataset size and if data gets shuffled for each epoch.</li>
<li>To round up the step size means to train with a smaller batch at the end of the epoch. This could throw some infrastructural components off balance in large scale ML systems and complicate our analysis (because it is now not precisely using the same batch size for all batches).</li>
</ul>
<blockquote>
<p>Are fractional steps typically rounded down in practice?
How do common machine learning frameworks handle this situation?</p>
</blockquote>
<p>Let's see how Pytorch handles this. In Pytorch we usually load data through <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer"">data loaders</a>. It has a parameter called <code>drop_last</code> whose purpose is exactly to specify whether to round down or round up. By default it is set to <code>False</code> which means to round up. This could be a good reference point for your implementation as well.</p>
","0","Answer"
"79035704","79031959","<p>I recommend you run your code in the terminal that your virtual environment is activated</p>
","0","Answer"
"79035969","79035928","<p><a href=""https://docs.rubixml.com/latest/classifiers/k-nearest-neighbors.html"" rel=""nofollow noreferrer"">K Nearest Neighbors</a> is a <a href=""https://docs.rubixml.com/latest/what-is-machine-learning.html#classification"" rel=""nofollow noreferrer"">classifier</a>. The classes are the labels that you provided in the training data (this is a <strong>fixed</strong> list). In your case, they are:</p>
<ul>
<li>260</li>
<li>500</li>
<li>300</li>
<li>450</li>
<li>150</li>
</ul>
<p>A prediction will always return one of those classes.</p>
<p>See the <a href=""https://docs.rubixml.com/latest/basic-introduction.html#choosing-an-estimator"" rel=""nofollow noreferrer"">documentation</a> and the <a href=""https://github.com/RubixML/Iris"" rel=""nofollow noreferrer"">Iris Flower Classifier</a> example to learn more about KNN.</p>
","0","Answer"
"79036329","79036243","<p>The functionality of the <code>pROC::roc</code> function is described in the manual at <a href=""https://cran.r-project.org/web/packages/pROC/pROC.pdf"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/pROC/pROC.pdf</a> pages 69 to 75.</p>
<p>The second argument to the function is called <code>predictor</code> and explaines as</p>
<blockquote>
<p>a numeric or ordered vector of the same length than response, containing the
predicted value of each observation. If the first argument was a data.frame,
<code>predictor</code> should be the name of the column in data containing the predictor, quoted for <code>roc_</code>, and optionally quoted for <code>roc.data.frame</code> (non-standard
evaluation or NSE).`</p>
</blockquote>
<p>A (numeric or ordered) vector cannot contain two columns. Therefore this is not part of what the function offers.</p>
","0","Answer"
"79037614","78704542","<p>The problem is that you're converting the model using the shape of your dummy_input, which is 1 x 12, and the <code>ct.convert()</code> function assumes this will always be the shape of your input. If you want to be able to use any number of tokens up to the context length of your model, you need to modify the shape argument to look like this:</p>
<pre><code>    inputs=[
        ct.TensorType(name=&quot;input_ids&quot;, shape=(1, ct.RangeDim(1, 512)), dtype=np.int32),
        ct.TensorType(name=&quot;attention_mask&quot;, shape=(1, ct.RangeDim(1, 512)), dtype=np.int32)
    ],
</code></pre>
<p>Where the <code>1</code> argument in <code>ct.RangeDim()</code> is the minimum context size and the <code>512</code> argument is your model's maximum context size.</p>
<p>You'll also want to add some arguments to the <code>tokenizer()</code> function to ensure it pads out the remainder of these tensors so it fits your model. You'll want to find the dimensions of your model's input size so you can give it the correct size.</p>
<pre><code>dummy_input = tokenizer(&quot;This is a sample input&quot;, return_tensors=&quot;pt&quot;, padding='max_length', max_length=512)
</code></pre>
<p>You'll want to add the same arguments to your <code>tokenizer()</code> function when inferencing as well.</p>
<p>Apple's CoreML Documentation: <a href=""https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#set-the-range-for-each-dimension"" rel=""nofollow noreferrer"">https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#set-the-range-for-each-dimension</a></p>
<p><code>ct.RangeDim()</code> Documentation: <a href=""https://apple.github.io/coremltools/source/coremltools.converters.mil.input_types.html#rangedim"" rel=""nofollow noreferrer"">https://apple.github.io/coremltools/source/coremltools.converters.mil.input_types.html#rangedim</a></p>
","0","Answer"
"79040649","79039003","<p>this happens when you try to combine image data (shape: (None, 256, 256, 1)) with CSV data (shape: (None, 6)). The issue arises because the CSV data and image data are being mixed incorrectly in the model.</p>
<p>this code should work for you:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy

# Assuming `attention_unet` is your function that returns the U-Net model
def attention_unet():
    inputs = layers.Input(shape=(256, 256, 1), name='image_input')  # Input for images
    # Build your U-Net architecture here
    # outputs = ... (this would be your segmentation output)
    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(inputs)  # Example output
    return inputs, outputs

# Model definition
image_input, segmentation_output = attention_unet()  # UNet model for images

# CSV data input
csv_input = layers.Input(shape=(6,), name='csv_input')

# Mask input for segmentation task
mask_input = layers.Input(shape=(256, 256, 1), name='mask_input')

# CSV data processing
csv_x = layers.Dense(64, activation='relu')(csv_input)
csv_x = layers.Dense(32, activation='relu')(csv_x)

# Flatten the U-Net's segmentation output to combine with CSV data
flatten_segmentation_output = layers.Flatten()(segmentation_output)

# Combine flattened segmentation output with CSV processed data
combined = layers.Concatenate()([flatten_segmentation_output, csv_x])

# Classification output (predicting hemorrhage type)
classification_output = layers.Dense(1, activation='sigmoid', name='final_output')(combined)

# Define the model with inputs and outputs
model = models.Model(inputs=[image_input, csv_input, mask_input],
                     outputs=[segmentation_output, classification_output])

# Compile the model
model.compile(optimizer=optimizers.Adam(),
              loss={'final_output': 'binary_crossentropy', 'conv2d': 'categorical_crossentropy'},  # replace 'conv2d' with the correct name of the segmentation output
              metrics=['accuracy'])

# Print model summary to verify the architecture
model.summary()

# Example data shapes (for illustration purposes):
X_train = np.random.rand(32, 256, 256, 1)  # Images
csv_train = np.random.rand(32, 6)  # CSV Data
y_train = np.random.rand(32, 256, 256, 1)  # Masks

X_val = np.random.rand(32, 256, 256, 1)  # Validation images
csv_val = np.random.rand(32, 6)  # Validation CSV data
y_val = np.random.rand(32, 256, 256, 1)  # Validation masks

# Training the model
model.fit({'image_input': X_train, 'csv_input': csv_train, 'mask_input': y_train},
          {'final_output': csv_train, 'conv2d': y_train},  # 'conv2d' needs to match the segmentation output's layer name
          validation_data=({'image_input': X_val, 'csv_input': csv_val, 'mask_input': y_val},
                           {'final_output': csv_val, 'conv2d': y_val}),
          epochs=50, batch_size=32)
</code></pre>
","0","Answer"
"79040744","79038884","<blockquote>
<p>(1) how can we build such a model and export it as PMML file?</p>
</blockquote>
<p>You have pictured a (6, 3, 6) NN. It is architecturally identical to what is implemented by Scikit-Learn's <code>MLPRegressor</code> class.</p>
<p>You can emulate autoencoder using <code>MLPRegressor</code>; in the current case, you would define a NN with a single hidden layer (containing three neurons), and train it with <code>X == y</code>:</p>
<pre class=""lang-py prettyprint-override""><code>autoencoder = MLPRegressor(hidden_layer_sizes = (3, ))
autoencoder.fit(X, X)
</code></pre>
<blockquote>
<p>(2) is PMML capable to encode such model structure?</p>
</blockquote>
<p>PMML is capable of representing full-blown <code>MLPRegressor</code> objects using the <a href=""https://dmg.org/pmml/v4-4-1/NeuralNetwork.html"" rel=""nofollow noreferrer""><code>NeuralNetwork</code></a> model element. Therefore, it's also capable of representing its &quot;truncated&quot; variants such as autoencoders.</p>
<p>The idea is to simply ignore the last (ie. rightmost) layer during conversion. Effectively, the pictured (6, 3, 6) NN gets truncated to (6, 3) NN.</p>
<p>The <a href=""https://github.com/jpmml/sklearn2pmml"" rel=""nofollow noreferrer"">SkLearn2PMML</a> package provides the <code>sklearn2pmml.neural_network.MLPTransformer</code> transformer for this purpose.</p>
<blockquote>
<p>(3) what are the necessary component in PMML to generate N output nodes in this model?</p>
</blockquote>
<p>There is no need to generate anything extra.</p>
<p>The truncated (6, 3) NN provides three outputs <code>y(0)</code>, <code>y(1)</code> and <code>y(2)</code>, which you may then pass forward to other transformers or models.</p>
","0","Answer"
"79040947","79039465","<p>The code you have returns the R2 score on the training set. It's there in <code>cv_results_</code>, as <code>mean_train_score</code>, etc.</p>
<p>Realize that the <code>grid_search</code> object keeps track of the best score and best parameters for you, you don't need your <code>if</code> block.</p>
<p>The <code>grid_search</code> object is an estimator, and ends up being fit with the best parameters (as per the folded grid search) on <strong>all</strong> the data you give it (<code>X</code> and <code>y</code> in your case). <a href=""https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">This is because by default, <code>refit=True</code>.</a> So when you do <code>predict</code>, you are getting the best model determined by the grid search. (This model is also available as <code>grid_search.best_estimator_</code>.)</p>
<p>If you do <code>grid_search.predict(X_test)</code> then you are predicting on <code>X_test</code>. So hopefully this data was not part of <code>X</code>.</p>
<p>Remember to refit your best model to <strong>absolutely all</strong> your data before using in production! (That is, combine <code>X</code> and <code>X_test</code> to fit the best model.)</p>
","0","Answer"
"79047071","78582076","<p>It's simultaneous because both <code>w</code> and <code>b</code> are updated before the next iteration, meaning <code>w</code> and <code>b</code> both have updated values when the next differentiation occurs.</p>
","0","Answer"
"79048442","79047845","<p>Was the model compiled before saving? If not, you should try compiling before saving. You can also try setting compile to false, and seeing if the compile step is where things are failing. you can also skip the &quot;tf.keras.config.enable_unsafe_deserialization()&quot; by setting safe_mode=False.</p>
<pre><code>tf.keras.models.load_model(
    filepath, custom_objects=None, compile=True, safe_mode=True
)
</code></pre>
","0","Answer"
"79051312","79047845","<p>The only thing that was disabling this to run was not updating keras.</p>
","1","Answer"
"79055306","79032297","<p><a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-series#modeling-pipeline"" rel=""nofollow noreferrer"">Arima plus</a> is sensitive to outliers and sudden shifts in data. Just summing values loses crucial information about the 15-minute resolution variation. Instead of summing, probably you can consider aggregating based on meaningful time periods like daily or weekly totals/averages. This reduces the number of time series and helps smooth out the noise.</p>
<p>Finding the optimal approach will involve experimenting with different combinations of preprocessing, models, and hyperparameters. You can also go check and see these <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-series#large-scale-time-series-forecasting-best-practices"" rel=""nofollow noreferrer"">best practices</a> for large-scale time series.</p>
","0","Answer"
"79057288","79057233","<pre><code>index = 2
categories_length = 3

tensor = torch.nn.functional.one_hot(
    torch.LongTensor([index]),
    categories_length,
)
</code></pre>
<p>Another pretty way:</p>
<pre><code>rock, paper, scissors = torch.eye(3) 
</code></pre>
","1","Answer"
"79058126","79057484","<p>I probably should wait till you reply to my questions, but ...  Here are a couple of observations</p>
<p>This loop, while it works:</p>
<pre><code>l=[]
for a in dataf['text']:
    l.append(a)
</code></pre>
<p>is slower than necessary.</p>
<p><code>dataf['text']</code> is a pandas Series.  That might be usable directly.  Otherwise try <code>dataf['text'].to_list()</code> or even <code>dataf['text'].to_numpy()</code>.  Check the docs to verify the method names.</p>
<p>According to the docs a <code>CountVectorizer()</code> produces a sparse <code>csr_matrix</code>.  If so <code>np.array(X_train)</code> will produce an array of single row <code>csr_matrices</code>.  That isn't the correct way to convert a sparse matrix into a dense array.</p>
<pre><code>anArray = sparse_matrix.toarray()
</code></pre>
<p>is the correct way</p>
<p><a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray</a></p>
<p>My guess is the error occurs deep in the keras model calls, at the point where it tries to convert the input arrays into tensors.  That should be evident if you look at the full context of the error message.  When we ask for full error (with traceback) we want YOU to look at it as well, trying to deduce what is happening.  Tensor conversion normally requires a numeric dtype array.  If my above guess is right your <code>X_train</code> etc are object dtype arrays.</p>
<p>Check the shape and dtype of arrays in all cases like this.</p>
<p>I see you have a <code>print(X_train)</code>.  Didn't you see anything unusual in that print?  That's another thing - I like to see the results of prints like this.</p>
","0","Answer"
"79068638","79065062","<p>when you install <strong>mlflow-export-import</strong> in libraries tab using <strong>PyPi</strong> option you get these errors because of bugs in the package installed via <strong>PyPi</strong> it is mentioned here <a href=""https://github.com/mlflow/mlflow-export-import/tree/master#5-install-from-pypi"" rel=""nofollow noreferrer"">document</a> also you can check the limitations <a href=""https://github.com/mlflow/mlflow-export-import/blob/master/README_limitations.md"" rel=""nofollow noreferrer"">here </a>, but still it exports the model with errors when using below code and adding library in cluster like below.</p>
<p><a href=""https://i.sstatic.net/Tp8jctJj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tp8jctJj.png"" alt=""enter image description here"" /></a></p>
<p>Code</p>
<pre><code>from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient
import os
mlflow_client = MlflowClient()

os.environ[&quot;MLFLOW_TRACKING_URI&quot;]=&quot;https://adb-112347890.10.azuredatabricks.net/&quot;
os.environ[&quot;DATABRICKS_TOKEN&quot;]=&quot;dapiexxxxxxxxxxxxx7b3d13cd155-3&quot;

exporter = ModelExporter(mlflow_client)

exporter.export_model(
    model_name=&quot;RandomForestDiabetesModel&quot;,
    output_dir=&quot;/tmp/mlflow_exports/diabetes_rf&quot;
)
</code></pre>
<p>But sometimes it creates bug and error to avoid it you use the bash command to export</p>
<p>Below is the code.</p>
<pre><code>%sh 
export DATABRICKS_HOST=https://ad1384282929690.10.azuredatabricks.net
export DATABRICKS_TOKEN=dapixxxxxxxxxxxxyyyyyyyyyycd155-3
export-model --output-dir /tmp/diabmodels/ --model RandomForestDiabetesModel
</code></pre>
<p>Output</p>
<pre><code>Wed Oct  9 04:54:52 2024 Connection to spark from PID  9626
Wed Oct  9 04:54:52 2024 Initialized gateway on port 37871
Wed Oct  9 04:54:52 2024 Connected to spark.
MLflow Version: 2.16.2
MLflow Tracking URI: databricks
Options:
  model: RandomForestDiabetesModel
  output_dir: /tmp/diabmodels/
  notebook_formats: 
  stages: None
  versions: None
  export_latest_versions: False
Databricks REST client: https://adb-1004898840459690.10.azuredatabricks.net/api/2.0
Found 1 'all' versions for model 'RandomForestDiabetesModel'
Exporting model 'RandomForestDiabetesModel' version 1 stage 'None' to '/tmp/diabmodels/d33df583d9214485a9793a9112b785fd'

Downloading artifacts:   0%|          | 0/11 [00:00&lt;?, ?it/s]
Downloading artifacts:   9%|▉         | 1/11 [00:00&lt;00:00, 16194.22it/s]
Downloading artifacts:  18%|█▊        | 2/11 [00:00&lt;00:01,  6.24it/s]   
Downloading artifacts:  18%|█▊        | 2/11 [00:00&lt;00:01,  6.24it/s]
Downloading artifacts:  27%|██▋       | 3/11 [00:00&lt;00:01,  6.24it/s]
Downloading artifacts:  36%|███▋      | 4/11 [00:00&lt;00:01,  6.24it/s]
Downloading artifacts:  45%|████▌     | 5/11 [00:00&lt;00:00,  6.24it/s]
Downloading artifacts:  55%|█████▍    | 6/11 [00:00&lt;00:00,  6.24it/s]
Downloading artifacts:  64%|██████▎   | 7/11 [00:00&lt;00:00,  7.62it/s]
Downloading artifacts:  64%|██████▎   | 7/11 [00:00&lt;00:00,  7.62it/s]
Downloading artifacts:  73%|███████▎  | 8/11 [00:00&lt;00:00,  7.62it/s]
Downloading artifacts:  82%|████████▏ | 9/11 [00:01&lt;00:00,  6.29it/s]
Downloading artifacts:  82%|████████▏ | 9/11 [00:01&lt;00:00,  6.29it/s]
Downloading artifacts:  91%|█████████ | 10/11 [00:01&lt;00:00,  5.93it/s]
Downloading artifacts:  91%|█████████ | 10/11 [00:01&lt;00:00,  5.93it/s]
Downloading artifacts: 100%|██████████| 11/11 [00:01&lt;00:00,  6.13it/s]
Downloading artifacts: 100%|██████████| 11/11 [00:01&lt;00:00,  6.13it/s]
Downloading artifacts: 100%|██████████| 11/11 [00:01&lt;00:00,  6.35it/s]
Exported 1/1 'all' versions for model 'RandomForestDiabetesModel'
</code></pre>
<p>and model artifacts.</p>
<p><a href=""https://i.sstatic.net/AJL7iE18.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJL7iE18.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79070813","79057824","<p>So, I wrote a process that compared LogisticRegression and RandomForestClassifier and pushed my data through it. The accuracy was much better using RandomForestClassifier.</p>
<p>Turns out the LogisticRegression returning all twos was not an error, it was just results with low accuracy. I rewrote the program using RandomForestClassifier, and added RandomizedSearchCV which creates multiple trees, chooses the tree with the highest accuracy, and uses it to predict classifications.</p>
<p>The new returned values were still mostly 2's, but the accuracy was greatly increased.</p>
<pre><code>param_dist = {'n_estimators': randint(100, 375),
              'max_depth': randint(5, 20)}

rf = RandomForestClassifier()

rand_search = RandomizedSearchCV(rf, param_distributions = param_dist, n_iter=10, cv=5)

rand_search.fit(X, y)

best_rf = rand_search.best_estimator_

print('Best hyperparameters:',  rand_search.best_params_)

predictions = pd.Series(best_rf.predict(to_pred_covariates))
</code></pre>
","0","Answer"
"79072141","79071235","<ol>
<li><p>When concatenating the input tensor, ensure that the dimensions match. Instead of torch.cat([input] * bs), you might need to adjust the dimensions explicitly:
<code>input = input.unsqueeze(0).repeat(bs, 1, 1, 1)</code></p>
</li>
<li><p>The error suggests a mismatch between the expected and actual tensor sizes. Ensure that the output of <code>self.unet</code> matches the expected dimensions.</p>
</li>
<li><p>Ensure that the scheduler’s timesteps and alpha values are correctly handled for batch processing. The <code>alphas_cumprod</code> values should be broadcasted to match the batch size</p>
</li>
<li><p>Debug the tensor shapes by adding <code>print</code> statements to check the shapes of tensors at different stages in your code, you might find where the mismatch occurs</p>
</li>
</ol>
","0","Answer"
"79074028","79073506","<p>When you use ReduceLROnPlateau with mode='min', the learning rate will be reduced when the monitored quantity does not decrease. Since you monitor accuracy, which you want to increase, you should use mode='max'.</p>
","0","Answer"
"79075077","79074519","<p>The dimensions of the output from AveragePooling2D can differ from the dimensions of the input, since  default value for padding is 'valid'. Maybe setting padding='same' would fix your issue.</p>
","0","Answer"
"79077616","79076968","<p>Try this:</p>
<pre><code>import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut
from xgboost import XGBRegressor
from collections import Counter


FDODB = pd.read_excel('Final Training Set for LOGO.xlsx')
array = FDODB.values

X = array[:, 2:126]  
Y = array[:, 1]     
Compd = array[:, 0]  

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

params = {
    'max_depth': [5, 7, 9],
    'learning_rate': [0.03, 0.05, 0.07],
    'n_estimators': [200, 300, 400],
    'min_child_weight': [5, 7, 9],
    'subsample': [0.3, 0.5, 0.7],
    'base_score': [0.4, 0.5, 0.6]
}
xgb_reg = XGBRegressor(tree_method='hist', device='cuda')

logo = LeaveOneGroupOut()

grid_search = GridSearchCV(
    estimator=xgb_reg,
    param_grid=params,
    scoring='r2',
    error_score='raise',
    cv=logo,  
    verbose=2,  
    n_jobs=-1, 
    return_train_score=True 
)

grid_search.fit(X_scaled, Y, groups=Compd)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print('Best hyperparameters:', best_params)
print('Best R2 score:', best_score)

print('Group distribution:', Counter(Compd))
</code></pre>
","0","Answer"
"79080449","79075311","<p>Try this content for the <strong>data.yaml</strong> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>path: OIT_model/customOIT/customdatasetyolo
train: train
val: val
nc: 1
names: ['5']
</code></pre>
<p>The <code>train</code> and <code>val</code> paths should be relative to the dataset root dir  <code>path</code>, so for instance, the full train path will be like <code>os.path.join(path, train)</code>. Example for object detection task dataset.yaml: <a href=""https://docs.ultralytics.com/datasets/detect/#supported-dataset-formats"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/datasets/detect/#supported-dataset-formats</a></p>
","0","Answer"
"79080890","78846949","<p>Had the same issue, just try implementing the same architecture with Functional API and it'll work.</p>
","0","Answer"
"79081678","79063665","<p>The <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"" rel=""nofollow noreferrer"">MLPRegressor class</a> is capable of modeling non-linear relationships using a multi-layer perceptron. It supports <code>partial_fit()</code>. Note that <code>partial_fit()</code> only works if solver is not set to 'lbfgs.'</p>
<p>Based on a <a href=""https://scikit-learn.org/dev/search.html?q=partial_fit"" rel=""nofollow noreferrer"">search through the docs</a>, I could not find any other examples that fulfill all of 1) being a regressor, 2) having support for <code>partial_fit()</code>, and 3) being nonlinear.</p>
<p>Alternatively, you might consider kernel methods. For example, you could pick a subset of your data, perform <a href=""https://scikit-learn.org/dev/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF"" rel=""nofollow noreferrer"">RBF</a> between that subset and all other points, and you would be able to model non-linear relationships using a linear regressor on the transformed RBF space. The RBF kernel would not learn incrementally, but the linear component would. This would give you more options, such as SGDRegressor or PassiveAgressiveRegressor.</p>
","0","Answer"
"79089603","78969286","<p>I used this model and logic to get better image expansion results:</p>
<pre><code>image_data = await image.read()
        image_stream = BytesIO(image_data)
        img = Image.open(image_stream)
        
        processor = BlipProcessor.from_pretrained(&quot;Salesforce/blip-image-captioning-large&quot;)
        model = BlipForConditionalGeneration.from_pretrained(&quot;Salesforce/blip-image-captioning-large&quot;)
        pipe = StableDiffusionInpaintPipeline.from_pretrained(
            &quot;benjamin-paine/stable-diffusion-v1-5-inpainting&quot;,
            variant=&quot;fp16&quot;,
            torch_dtype=torch.float16,
        ).to(&quot;cuda&quot;)

        inputs = processor(img, return_tensors='pt')
        out = model.generate(**inputs)
        prompt = processor.decode(out[0], skip_special_tokens=True)
        image = np.array(img)
        height, width = image.shape[:2]
        padding = 100
        mask = np.ones((height+2*padding, width+2*padding), dtype=np.uint8) * 255
        mask[padding:-padding, padding:-padding] = 0

        image_extended = np.pad(image, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=128)
        init_image = Image.fromarray(image_extended).convert(&quot;RGB&quot;)
        mask_image = Image.fromarray(mask).convert(&quot;RGB&quot;)

        image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]
</code></pre>
<p>Main issue was of transitions and by this model's use, there were no transitions.</p>
","0","Answer"
"79090497","79090407","<p>Have you tried to lower the value of <code>C</code> in <code>svm = OneVsRestClassifier(SVC(kernel='linear', C=1e5))</code>? Maybe you are overfitting to your data because of this (rather high?) value.</p>
<hr />
<p>I here try to plot the data set (using only the first two of the four features):</p>
<p><a href=""https://i.sstatic.net/0bLOkMWC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0bLOkMWC.png"" alt=""enter image description here"" /></a></p>
<p>In your code, you end up finding that the classes are linearly separable (which might be more evident when plotting the other two features). Since you are using the <code>OneVsRestClassifier</code>, three dividing planes are produced. Maybe the feature space is completely separable because the method can use many points to divide feature space. Thus, a great number of support vectors are required.</p>
<p>I am only speculating here, so my interpretation might not be completely correct.</p>
","0","Answer"
"79091924","79084869","<p>I just removed <code>input_shape=base_model.output_shape[1:]</code> and it worked.</p>
","0","Answer"
"79095931","79091245","<p>After further investigation, I've identified the root cause of the issue with running H2O in WSL. The problem arises specifically after updating from WSL version 2.1.5 to 2.2.1.</p>
<p><strong>What I Found:</strong></p>
<p>The H2O execution works perfectly in WSL version 2.1.5.</p>
<p>The issue appears immediately after updating to WSL version 2.2.1.</p>
<p>Changes in version 2.2.1, such as enabling DNS tunneling by default and other kernel updates, might be contributing factors.</p>
<p><strong>Solution:</strong></p>
<p>For now, the solution is to revert to WSL version 2.1.5, where H2O runs without any problems. Here are the steps I took to resolve the issue:</p>
<p><strong>Uninstall WSL</strong></p>
<p><em>Removed the current WSL version using PowerShell.</em>
wsl --uninstall</p>
<p>Obs.: Although I haven't done it, I think it would be a good safety measure to make a backup of the Linux distro before uninstalling WSL.</p>
<p>Install WSL Version 2.1.5:
Downloaded and installed the WSL 2.1.5
<a href=""https://github.com/microsoft/WSL/releases/tag/2.1.5"" rel=""nofollow noreferrer"">https://github.com/microsoft/WSL/releases/tag/2.1.5</a></p>
<p>This workaround allows me to use H2O effectively until a permanent fix is provided in future updates of WSL.</p>
<p>I hope this helps anyone facing similar issues. I opened an issue on the WSL GitHub, and I hope this problem is fixed in future WSL updates.</p>
","0","Answer"
"79098787","78636029","<p>you can use a sklearn pipeline and use the classic Gradient Boosting:</p>
<pre><code>from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

one_hot_encode = [
    'categorical_v1',
    'categorical_v2', 
    ...
]
# One-hot encode categorical features
ct = ColumnTransformer([
    ('onehot', OneHotEncoder(handle_unknown='ignore'), one_hot_encode)
], remainder='passthrough')

X_encoded = ct.fit_transform(X)

# Fit the model
clf = GradientBoostingClassifier(n_estimators=10, random_state=0)
clf.fit(X_encoded, y)
</code></pre>
<p>and then use the plot_tree function:</p>
<pre><code>plot_tree(clf.estimators_[0, 0],
    filled=True,
    feature_names=ct.get_feature_names_out(),rounded=True,
    proportion=True)
</code></pre>
","0","Answer"
"79099582","79084313","<p>The Stable Baselines3 environment interface expects the reset function to return a tuple: (observation, info).</p>
<p><strong>Original code:</strong>
`def reset(self, seed=None):
# If a seed is provided, set it here
if seed is not None:
self.seed(seed)</p>
<pre><code>    self.agents = [Agent(position) for position in self.read_agent_locations()]

    for agent in self.agents:
        agent.acceleration = np.zeros(2)
        agent.velocity = np.round(np.random.uniform(-SimulationVariables[&quot;VelocityUpperLimit&quot;], SimulationVariables[&quot;VelocityUpperLimit&quot;], size=2), 2)

    observation = self.get_observation().flatten()

    ################################
    self.current_timestep = 0  # Reset time step count
    ################################
    
    return observation`
</code></pre>
<p><strong>Error:</strong>
Was returning just the observation, which caused the framework to throw an unpacking error.</p>
<p><strong>Debugged:</strong>
Modified the reset method of your custom environment to return both the observation and an empty info dictionary.</p>
<p>Reset function:
` def reset(self, seed=None, options=None):
# If a seed is provided, set it here
if seed is not None:
self.seed(seed)</p>
<pre><code>    self.agents = [Agent(position) for position in self.read_agent_locations()]

    for agent in self.agents:
        agent.acceleration = np.zeros(2)
        agent.velocity = np.round(np.random.uniform(-SimulationVariables[&quot;VelocityUpperLimit&quot;], SimulationVariables[&quot;VelocityUpperLimit&quot;], size=2), 2)

    observation = self.get_observation().flatten()

    ################################
    self.current_timestep = 0  # Reset time step count
    ################################

    super().reset(seed=seed)
    info = {}  # This is the extra information dictionary, you can populate it with useful info if needed
    return observation, info `
</code></pre>
","0","Answer"
"79100297","79086293","<p>It turns out GPy's Bernoulli likelihood class maps the latent predictions to a probability using the standard normal CDF (<a href=""https://gpy.readthedocs.io/en/devel/_modules/GPy/likelihoods/bernoulli.html"" rel=""nofollow noreferrer"">source code</a> shows the default is the probit link function). The dataset in my example instead uses the sigmoid function, meaning that a logit link function would instead be needed. Even though it's not implemented in the GPy library, we can create our own link function object and pass it to the GPy model when initializing it. Here's an updated version of my code (see changes in Part 3):</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# Part 1: toy dataset creation
##############################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2  # latent function
t = bernoulli.rvs(sigmoid(a))    # Bernoulli training data (0s and 1s)

#####################################
# Part 2: scikit-learn implementation
#####################################

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import ConstantKernel, RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
        * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
    kernel=rbf,
    optimizer='fmin_l_bfgs_b',
    n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.95**2 * RBF(length_scale=0.898)

############################
# Part 3: GPy implementation
############################

import GPy
from GPy.likelihoods.link_functions import GPTransformation

kern = GPy.kern.RBF(
    input_dim=1,
    variance=1.,
    lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

# Create custom link function
class LogitLink(GPTransformation):
    def transf(self, f):
        &quot;&quot;&quot;Sigmoid function&quot;&quot;&quot;
        return 1 / (1 + np.exp(-f))

    def dtransf_df(self, f):
        &quot;&quot;&quot;First derivative of sigmoid&quot;&quot;&quot;
        p = self.transf(f)
        return p * (1 - p)

    def d2transf_df2(self, f):
        &quot;&quot;&quot;Second derivative of sigmoid&quot;&quot;&quot;
        p = self.transf(f)
        return p * (1 - p) * (1 - 2 * p)

    def d3transf_df3(self, f):
        &quot;&quot;&quot;Third derivative of sigmoid&quot;&quot;&quot;
        p = self.transf(f)
        return p * (1 - p) * (1 - 6 * p + 6 * p**2)

logit_link = LogitLink()

m = GPy.core.GP(
    X=X,Y=t, kernel=kern, 
    inference_method=GPy.inference.latent_function_inference.laplace.Laplace(),    
    likelihood=GPy.likelihoods.Bernoulli(gp_link=logit_link))

m.optimize_restarts(
    num_restarts=10, optimizer='lbfgs',
    verbose=True, robust=True)

print(m.kern)
#  rbf.         |               value  |  constraints  |  priors
#  variance     |  3.7922168280178328  |  0.001,10.0   |        
#  lengthscale  |  0.8977846144774547  |  0.001,10.0   |
</code></pre>
<p>The scikit-learn implementation (Part 2 of the code) uses the sigmoid mapping (logit link function), which is why the results with it made more sense. Now, both implementation (scikit-learn and GPy) return the same lengthscale and variance values after optimization.</p>
","0","Answer"
"79101756","79088393","<p>The T5 tokenizer should end sequences by EOS token by default. Pretrained T5 tokenizer on HuggingFace does that by default. In fact, I found the function that is responsible for that in the <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py"" rel=""nofollow noreferrer"">source code</a> on line 256:</p>
<pre><code>def _add_eos_if_not_present(self, token_ids: List[int]) -&gt; List[int]:
        &quot;&quot;&quot;Do not add eos again if user already added it.&quot;&quot;&quot;
        if len(token_ids) &gt; 0 and token_ids[-1] == self.eos_token_id:
            warnings.warn(
                f&quot;This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated&quot;
                &quot; eos tokens being added.&quot;
            )
            return token_ids
        else:
            return token_ids + [self.eos_token_id]
</code></pre>
<p>If EOS token is not appended by default, you can add a post processor to your tokenizer using TemplateProcessing:</p>
<pre><code>from tokenizers.processors import TemplateProcessing

tokenizer._tokenizer.post_processor = TemplateProcessing(
    single=&quot;$A &lt;/s&gt;&quot;,
    pair=&quot;$A &lt;/s&gt; $B &lt;/s&gt;&quot;,
    special_tokens=[(&quot;&lt;/s&gt;&quot;, tokenizer.eos_token_id)]
)

inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=&quot;pt&quot;)
labels = inputs[&quot;input_ids&quot;]
print(labels)
</code></pre>
<p>This should give:</p>
<pre><code>tensor([[1, 10356, 1, 5056, 16001],
        [1, 10356, 16001, 16002, 16002]])
</code></pre>
","0","Answer"
"79102338","79096421","<p>You are describing <strong>folded cross-validation</strong>, whereas <code>train_test_split</code> is really designed for 'hold out' validation. Read <a href=""https://arxiv.org/abs/1811.12808"" rel=""nofollow noreferrer"">Raschka 2018</a> for the full lowdown on this.</p>
<p>To avoid samples appearing in more than one 'fold', you need groups, eg as implemented in <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html"" rel=""nofollow noreferrer""><code>GroupKFold</code></a>.</p>
<p>This way, you can assign the samples to groups yourself, however you like, then use those to split the data for cross-validation, for example using <code>sklearn.model_selection.cross_val_score()</code> or <code>sklearn.model_selection.cross_validate()</code>, as described <a href=""https://scikit-learn.org/stable/modules/cross_validation.html"" rel=""nofollow noreferrer"">in the User Guide</a>.</p>
","0","Answer"
"79103975","79103126","<p>There doesn't seem to be a direct and generic way of doing this other than to iterate through the layers and count the parameters depending on what kind of a layer it is.</p>
<p>For example,</p>
<p>Store a CoreML model with a convolution layer and a linear layer</p>
<pre><code>import torch
import torch.nn as nn
import coremltools as ct
import os

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(16 * 32 * 32, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x

model = SimpleNet()

dummy_input = torch.rand(1, 3, 32, 32)
traced_model = torch.jit.trace(model, dummy_input)
coreml_model = ct.convert(
    traced_model,
    inputs=[ct.TensorType(name=&quot;input&quot;, shape=dummy_input.shape)]
)

model_name = &quot;simple_net&quot;
coreml_model.save(f&quot;{model_name}.mlpackage&quot;)

</code></pre>
<p>This model has two layers: a conv and a linear layer. So we just iterate through the layers using <code>model.get_spec().neuralNetwork.layers</code> and for the conv layer, <code>kernel_channels * output_channels * kernel_height * kernel_width</code> gives the weights and <code>output_channels</code> is the number of bias terms. For the linear layer it's simply the number of inputs times the number of outputs.</p>
<pre><code>def count_parameters(model):
    total_params = 0
    for layer in model.get_spec().neuralNetwork.layers:
        if layer.HasField('convolution'):
            conv = layer.convolution
            kernel_channels = conv.kernelChannels
            output_channels = conv.outputChannels
            kernel_height = conv.kernelSize[0]
            kernel_width = conv.kernelSize[1]
            params = kernel_channels * output_channels * kernel_height * kernel_width
            if conv.hasBias:
                params += output_channels
            total_params += params
        elif layer.HasField('innerProduct'):
            inner = layer.innerProduct
            input_channels = inner.inputChannels
            output_channels = inner.outputChannels
            params = input_channels * output_channels
            if inner.hasBias:
                params += output_channels
            total_params += params
    return total_params

coreml_params = count_parameters(coreml_model)
print(f&quot;Total parameters: {coreml_params}&quot;)
</code></pre>
<p>It prints</p>
<pre><code>Number of parameters in PyTorch model: 164298
</code></pre>
<p>If you directly count it on the Pytorch model through</p>
<pre><code>total_params = sum(p.numel() for p in model.parameters())
print(f&quot;Total parameters: {total_params}&quot;)
</code></pre>
<p>It matches</p>
<pre><code>Total parameters: 164298
</code></pre>
<p>Not the best approach (and I hope someone else has a more generic approach) but it's a start.</p>
","1","Answer"
"79107062","79105371","<p>I found a solution.
I continued searching and came across this tutorial:
<a href=""https://www.kaggle.com/code/bumbleboss/u2net-pth-to-onnx"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/bumbleboss/u2net-pth-to-onnx</a></p>
<p>I finalized the conversion according to the tutorial:</p>
<pre><code>import torch
import torch.onnx
from model import U2NET

def convert(model_path, output_path):
torch_model = U2NET(3, 1)
torch_model.load_state_dict(torch.load(model_path, 
map_location=torch.device('cpu')), strict=False)  # Добавлен map_location
torch_model.eval()

x = torch.randn(1, 3, 320, 320, requires_grad=True)
torch_out = torch_model(x)

torch.onnx.export(
    torch_model,
    x,
    output_path,
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
)

print(f&quot;success {output_path}&quot;)

if __name__ == &quot;__main__&quot;:
import argparse

parser = argparse.ArgumentParser(description=&quot;conversion PyTorch to ONNX&quot;)
parser.add_argument('--model-path', type=str, required=True, help='path to .pth file')
parser.add_argument('--output-path', type=str, required=True, help='save ONNX file')

args = parser.parse_args()
convert(args.model_path, args.output_path)
</code></pre>
<p>Now rembg perfectly cuts out the background on car rims:</p>
<p><a href=""https://i.sstatic.net/YjGhoDRx.jpg"" rel=""nofollow noreferrer"">origin</a><br>
<a href=""https://i.sstatic.net/TsuVckJj.png"" rel=""nofollow noreferrer"">removed_bg</a></p>
<p>I hope it helps someone!</p>
","1","Answer"
"79107627","78996950","<p>This is what worked for me</p>
<pre><code>pip install &quot;python-doctr[torch,viz,html,contrib]&quot;  
pip install onnx==1.16.1
</code></pre>
<p>Overall, the main solution for me was downgrading onnx, which is represented in the line</p>
<pre><code>pip install onnx==1.16.1
</code></pre>
<p>And is the recommended solution for this error from onnx, which I found here:  <a href=""https://github.com/onnx/onnx/issues/6267"" rel=""noreferrer"">https://github.com/onnx/onnx/issues/6267</a></p>
<p>Note that I'm running Windows 11, Python 3.12.5</p>
","15","Answer"
"79108118","79107349","<p>The error is caused by the presence of &quot;?&quot; in the data. From the Kaggle link, I noticed that the horsepower column has it.</p>
<p>You could replace the &quot;?&quot; with NaN to indicate missing value(s), and then handle these missing value(s) using appropriate techniques.</p>
","2","Answer"
"79108137","79107349","<p>There are multiple problems in your code.</p>
<p>To create your features' matrix <code>X</code> you should not modify the whole dataset and set the variable equal to it but you <em>should</em>:</p>
<ol>
<li><p>Identify those fields (<em>features</em>) that <strong>actually</strong> influence the result (<em>target</em>).</p>
</li>
<li><p>Create <code>X</code> accessing only those fields.</p>
</li>
</ol>
<p><strong>IMPORTANT</strong>: <em>The features' matrix <strong>should not</strong> contain the target field</em>.</p>
<p>Example:</p>
<pre><code>y = df['mpg']
X = df[['cylinders', 'weight', 'displacement', 'acceleration', 'model year', 'origin']]
</code></pre>
<p>I purposely avoided to put the <em>horsepower</em> field because it is the one giving you problems since it contains strings: '<strong>?</strong>'. You could solve this problem by deleting the inconsistent rows or setting them equal to a standard value.</p>
<p>Other than this, during the testing of your code, I encountered one more problem. The last row <code>RMSE_CV = (MSE_CV_scores.mean())**(1/2)</code> is trying to calculate the square root of a negative value and is giving me this error: <code>&lt;ipython-input-52-7db3997c77e2&gt;:27: RuntimeWarning: invalid value encountered in scalar power RMSE_CV = (MSE_CV_scores.mean())**(1/2)</code>.</p>
","1","Answer"
"79109984","79109880","<p>If you need to ensure that specific IDs are not shared between your training and test datasets, you should use the <code>groups</code> argument in the splitter to account for them.</p>
<p>A good approach is to utilize the <code>GroupShuffleSplit</code> class from <code>sklearn.model_selection</code>, which allows you to split the data based on group labels, ensuring that no group appears in both the train and test sets.</p>
<p>For more details, check out this related question: <a href=""https://stackoverflow.com/a/54814482/7911130"">How to generate a train-test-split based on a group ID?</a>. There, you'll find a clear example of how to set the <code>groups</code> argument when using <code>GroupShuffleSplit</code>.</p>
","0","Answer"
"79112177","78440449","<p>Updating <code>imbalanced-learner</code> from 0.11.1 to 0.12.4 should work.</p>
","0","Answer"
"79113636","79106002","<p>If I get it right, the problem is that the first 10 timesteps from <code>X_test</code> use the last 10 timesteps from <code>X</code> (or more precise, <code>X_train</code>) for the predicition. With big enough <code>X_test</code>, this does not make much difference, but is theoretically data leakage from the training set to the test set.</p>
<p>I demonstrate it with a small example (correct me if I'm wrong):</p>
<pre class=""lang-py prettyprint-override""><code>df_data = [0, 1, 2, .., 15]  # len 16
window_size = 3
X = [[0,1,2], [1,2,3], [2,3,4], ..., [12,13,14]]  # len 13
y = [3, 4, 5, .., 15]  # len 13
# split the data 10-3 for train-test
X_train = [[0,1,2], [1,2,3], [2,3,4], ..., [9,10,11]]
y_train = [3, 4, 5, .., 12]
X_test = [[10,11,12], [11,12,13], [12,13,14]]
y_test = [13, 14, 15]
</code></pre>
<p>The problem in this example is that 10 and 11 are both used in sequences for <code>X_train</code> and <code>X_test</code>. So you have to first split <code>df_data</code> into train/test (without shuffling) and then do the sequencing separately. With this, you'd lose the first n-th values for <code>y</code> in both train and test.</p>
<hr />
<p>Edit: To clarify how the actual split would look like for a bit smaller <code>X_train</code> (the <code>X_train</code> from above is too big to leave predictions for <code>X_test</code> without leakage):</p>
<pre class=""lang-py prettyprint-override""><code>df_data = [0, 1, 2, .., 15]  # len 16
window_size = 3
train = [0, 1, 2, .., 10]
test = [11, 12, .., 15]
# make windows
X_train = [[0,1,2], [1,2,3], [2,3,4], ..., [7,8,9]]
y_train = [3, 4, 5, .., 10]
X_test = [[11,12,13], [12,13,14]]
y_test = [14,15]
</code></pre>
<p>Here, no values that are present in <code>X_train</code> or <code>y_train</code> are present in either <code>X_test</code> or <code>y_test</code>. The first 3 values in both <code>train</code> and <code>test</code> can not be predicted and are only used for windowing.</p>
","0","Answer"
"79114810","79114762","<p>It seems you are using a newer version of scikit-learn, which is most likely to be different from the video you are watching. The <code>categorical_features</code> argument is not valid for <code>OneHotEncoder</code>.</p>
<p>You can try the ColumnTransformer or directly specify which columns to encode.. something like this:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

ohe = OneHotEncoder()

column_transformer = ColumnTransformer(
    transformers=[
        ('ohe', ohe, [1])  # Index of 'Car_name' column
    ],
    remainder='passthrough'  # Keep the other columns as they are
)

vehicles_enc = column_transformer.fit_transform(vehicles).toarray()
</code></pre>
<p>And as an extra recommendation, always check the versions of the libraries that are used in tutorials, and make sure to check the official documentation of the libraries you are using:</p>
<ul>
<li><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a></li>
<li><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html</a></li>
</ul>
","1","Answer"
"79116680","78826248","<p>You can ignore this <code>ApplePersistenceIgnoreState</code> warning from your system by turning off this unnecessary feature for Python by using below command in your Terminal:</p>
<pre><code>defaults write org.python.python ApplePersistenceIgnoreState NO
</code></pre>
<p>Please have a look at this similar <a href=""https://stackoverflow.com/questions/18733965/annoying-message-when-opening-windows-from-python-on-os-x-10-8/21567601#21567601"">issue</a> for your reference. Let us know if you are still facing this issue. Thank you</p>
","0","Answer"
"79118565","78596363","<p>Yeah... I got this answer too.
You need to build a custom loader - this should help.</p>
<pre><code>import tensorflow as tf
from keras.layers import DepthwiseConv2D
from keras.models import load_model
import cv2
import numpy as np

# Define a custom DepthwiseConv2D class without the groups parameter
class CustomDepthwiseConv2D(DepthwiseConv2D):
    def __init__(self, **kwargs):
        # Remove the 'groups' parameter if it exists
        if 'groups' in kwargs:
            del kwargs['groups']  # Remove the groups parameter
        super().__init__(**kwargs)

# Create a dictionary of custom objects to pass to the load_model function
custom_objects = {
    'DepthwiseConv2D': CustomDepthwiseConv2D,
}

# Load the model with the custom object
try:
    model = load_model(&quot;keras_model.h5&quot;, custom_objects=custom_objects, 
compile=False)
    print(&quot;Model loaded successfully.&quot;)
except Exception as e:
    print(f&quot;Error loading model: {e}&quot;)
</code></pre>
","0","Answer"
"79118990","79055573","<p>To anyone who runs across this thread, I solved my problem. Turns out, I was misreading the formula, and had the locations of two of the terms mixed up. The comments in my original code were actually correct. The variables wj_xi and wyi_xi should actually be defined like this (in both the gradient and the loss methods):</p>
<pre><code>wj_xi = incorrect_scores     #these should be a vector of INCORRECT scores
wyi_xi = correct_score       #this should be a vector of the CORRECT score
</code></pre>
<p>I had them flipped around. Also, as was mentioned in the replies, it is important to update the weights in the negative direction of the gradient, like so:</p>
<pre><code>self.weights -= step
</code></pre>
<p>Full code:</p>
<pre><code>class SVM:
  def __init__(self):
    self.weights = np.random.randn(len(labels), X_train.shape[1]) * 0.1  #9 sets of weights (9 classes) and 4 entries per set (3 features + 1 bias, shape= (9x4))
    self.history = []

  def predict(self, X):
    '''
    returns class predictions in np array of size
    n x 10, where n is the number of examples in X
    '''

    #matrix multiplication to apply weights to X
    bounds = self.weights @ X.T

    #return the predictions
    return np.array(bounds).T

  def loss(self, scores, y, delta=1):
    '''
    returns the average hinge loss of the batch
    '''
    #calculate and return the loss for a prediction and corresponding truth label
    #hinge loss in this case
    total_loss = 0

    #compute loss for each example...
    for i in range(len(scores)):
      #extract values for this example
      scores_of_x = scores[i]
      label = y[i]
      correct_score = scores_of_x[label]
      incorrect_scores = np.concatenate((scores_of_x[:label], scores_of_x[label+1:]))

      #use the scores for example x to compute the loss at x
      wj_xi = incorrect_scores           #these should be a vector of INCORRECT scores
      wyi_xi = correct_score       #this should be a vector of the CORRECT score
      wy_xi = wj_xi - wyi_xi + delta  #core of the hinge loss formula
      losses = np.maximum(0, wy_xi)   #lower bound the losses at 0
      loss = np.sum(losses)           #sum the losses

      #add to the total loss
      total_loss += loss

    #return the loss
    avg_loss = total_loss / len(scores)  #divide by the number of examples to fund the average hinge loss per-example
    return avg_loss

  def gradient(self, scores, X, y, delta=1):
    '''
    returns the gradient of the loss function
    '''
    #calculate the loss and the gradient of the loss function
    #gradient of hinge loss function
    gradient = np.zeros(self.weights.shape)

    #calculate the gradient in each example in x
    for i in range(len(X)):
      #extract values for this example
      scores_of_x = scores[i]
      label = y[i]
      x = X[i]
      correct_score = scores_of_x[label]  #because arrays are 0-indexed, but the labels are 1-indexed
      incorrect_scores = np.concatenate((scores_of_x[:label], scores_of_x[label+1:]))

      #
      ##
      ### start by computing the gradient of the weights of the correct classifier
      ##
      #
      wj_xi = incorrect_scores           #these should be a vector of INCORRECT scores
      wyi_xi = correct_score       #this should be a vector of the CORRECT score
      wy_xi = wj_xi - wyi_xi + delta  #core of the hinge loss formula
      losses = np.maximum(0, wy_xi)   #lower bound the losses at 0

      #get number of nonzero losses, and scale data vector by them to get the loss
      num_contributing_classifiers = np.count_nonzero(losses)
      #print(f&quot;Num loss contributors: {num_contributing_classifiers}&quot;)
      g = -1 * x * num_contributing_classifiers   #NOTE the -, very important here, doesn't apply to other scores

      #add the gradient of the correct classifier to the gradient
      gradient[label] += g  #because arrays are 0-indexed, but the labels are 1-indexed
      # print(f&quot;correct label: {label}&quot;)
      #print(f&quot;gradient:\n{gradient}&quot;)
      #
      ##
      ### then, compute the gradient of the weights for each incorrect classifier
      ##
      #
      for j in range(len(scores_of_x)):

        #skip the correct score, since we already did it
        if j == label:
          continue
        wj_xi = scores_of_x[j]          #should be a vector containing the score of the CURRENT classifier
        wyi_xi = correct_score          #should be a vector containing the score of the CORRECT classifier
        wy_xi = wj_xi - wyi_xi + delta  #core of the hinge loss formula
        loss = np.maximum(0, wy_xi)   #lower bound the loss at 0

        #get whether this classifier contributed to the loss, and scale the data vector by that to get the gradient
        contributed_to_loss = 0
        if loss &gt; 0:
          contributed_to_loss = 1

        g = x * contributed_to_loss        #either times 1 or times 0

        #add the gradient of the incorrect classifier to the gradient
        gradient[j] += g


    #divide the gradient by number of examples to get the average gradient
    return gradient / len(X)

  def fit(self, X, y, epochs = 1000, batch_size = 256, lr=1e-2, verbose=True):
    '''
    trains the model on the training set
    '''
    #gradient descent loop
    for epoch in range(epochs):
      self.history.append({'epoch': epoch})

      #create a batch of samples to calculate the gradient
      #NOTE: this significantly boosts the speed of training
      indices = np.random.choice(len(X), batch_size, replace=False)
      X_batch = X.iloc[indices]
      y_batch = y.iloc[indices]
      X_batch = X_batch.to_numpy()
      y_batch = y_batch.to_numpy()

      #evaluate class scores on training set
      predictions = self.predict(X_batch)
      predicted_classes = np.argmax(predictions, axis=1)

      if epoch%50 == 0 and verbose:
        print(f&quot;pred: {predicted_classes[:10]}&quot;)
        print(f&quot;true: {y_batch[:10]}&quot;)


      #compute the loss: average hinge loss
      loss = self.loss(predictions, y_batch)
      self.history[-1]['loss'] = loss


      #compute accuracy on the test set, for an intuitive metric
      accuracy = np.mean(predicted_classes == y_batch)
      self.history[-1]['accuracy'] = accuracy

      #reduce the learning rate as training progresses
      # lr *= 0.999
      # self.history[-1]['lr'] = lr

      if epoch%50 == 0 and verbose:
        print(f&quot;Epoch: {epoch} | Loss: {loss} | Accuracy: {accuracy} | LR: {lr} \n&quot;)


      #compute the gradient on the scores assigned by the classifier
      gradient = self.gradient(predictions, X_batch, y_batch)

      #print(gradient)

      #backpropagate the gradient to the weights + bias
      step = gradient * lr

      #perform a parameter update, in the negative direction of the gradient
      self.weights -= step

sm = SVM()
pred = sm.predict(np.array(X_train[0:1]))

sm.fit(X_train, y_train)
</code></pre>
","0","Answer"
"79121333","78626998","<p>Since one-hot encoding uses hashing, there is a probability of collisions
where different words are assigned the same index. The probability of
collision increases with the vocabulary size. However, a larger vocabulary can
also improve the representation of distinct words. <a href=""https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/"" rel=""nofollow noreferrer"">Jason Brownlee</a> suggests
using a vocabulary size about 25% larger than the number of unique words to
balance collision risk and representation quality. By increasing the
vocabulary size in the provided code, the numerical representation of the text
changes, resulting in different encoded values.Please refer this <a href=""https://colab.sandbox.google.com/gist/malla456/0825271b5c70d6f9119b5419b4edec43/78626998.ipynb"" rel=""nofollow noreferrer"">gist</a></p>
","0","Answer"
"79125712","79125178","<p>Accord does not have such class. I suggest to try ML.NET.</p>
<p>Here is ML.NET tutorial for anomaly detection:</p>
<p><a href=""https://learn.microsoft.com/en-us/dotnet/machine-learning/tutorials/sales-anomaly-detection"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/machine-learning/tutorials/sales-anomaly-detection</a></p>
","0","Answer"
"79127894","79127884","<p>i suggest plot your loss curves and accuracy for your train and test set separately to see if your model is under-fitting or over-fitting.</p>
<p>I think your model must be under-fitting because you are running for only 4 epochs. I suggest you increase it to 100 to maybe more to see if result changes. there are more techniques to like increasing dataset or increase complexity of your model but it's all up to experimentation you do.</p>
","0","Answer"
"79131282","79130753","<p>Let's assume that your input shape is <code>(N, 1, 32, 32)</code> ((N,C,H,W) format).
Now, let us list the output shape after each layer.</p>
<p><code>self.conv1</code>: <code>(N, 32, 32, 32)</code>. <strong>32 filters of shape <code>5x5x1</code></strong> are convolved over the padded input (5x5 is the kernel size, 1 is the number of input channels). Hence, the output has 32 feature maps of shape <code>32x32</code>.</p>
<p><code>self.pool1</code>: <code>(N, 32, 16, 16)</code>. Pooling layer downsamples feature maps by a factor of 2.</p>
<p><code>self.conv2</code>: <code>(N, 64, 16, 16)</code>. <strong>64 filters of shape <code>5x5x32</code></strong> are applied to the padded input (5x5 is the kernel size, 32 is the number of input channels). Output shape after performing convolution operation with one filter is <code>16x16</code>. Hence, the output of the layer has 64 feature maps of shape <code>16x16</code>.</p>
<p><code>self.pool2</code>: <code>(N, 64, 8, 8)</code>. Pooling layer downsamples feature maps by a factor of 2.</p>
","0","Answer"
"79131824","78875648","<p>Try to use <code>super().__init__()</code> to properly initializes the base class constructor. The warning says you should call the base class constructor using <code>super().__init__(**kwargs)</code>. Hope this works for you.</p>
","-1","Answer"
"79135195","79110858","<p>You are close to an option in sketch2.js in the code you shared in the p5.js web editor, using <a href=""https://p5js.org/reference/p5/blendMode/"" rel=""nofollow noreferrer""><code>blendMode()</code></a>.</p>
<p>This case, you can use a blend mode that makes the brightest colours transparent, such as <code>MULTIPLY</code>. (Rember to apply the blend mode before rendering):</p>
<p>e.g.</p>
<pre class=""lang-js prettyprint-override""><code>function draw() {
  image(video, 0, 0);
  if (segmentation) {
    blendMode(MULTIPLY);
    image(segmentation.mask, 0, 0, width, height);
    blendMode(BLEND);
  }
}
</code></pre>
<p>If you want to explicitly make the white pixels transparent, you can.
It's a 3 step process:</p>
<ol>
<li>call <code>loadPixels()</code> to read pixels</li>
<li>access the <a href=""https://p5js.org/reference/p5/pixels/"" rel=""nofollow noreferrer""><code>pixels[]</code></a> array (and modify as needed)</li>
<li>call <code>updatePixels()</code> to commit the pixel changes</li>
</ol>
<p>for example:</p>
<pre class=""lang-js prettyprint-override""><code>function draw() {
  image(video, 0, 0);
  if (segmentation) {
    makeWhiteTransparent(segmentation.mask);
    image(segmentation.mask, 0, 0, width, height);
  }
}

function makeWhiteTransparent(img){
  // make pixels readable
  img.loadPixels();
  const imgWidth    = img.width;
  const imgHeight   = img.height;
  // p5.Image pixels are in [r1, g1, b1, a1, ..., rn, gn, bn, an] order
  const imgChannels = 4;
  const numValues   = imgWidth * imgHeight * imgChannels;
  const imgPixels   = img.pixels;
  // loop through each pixels = skipping every 4 values (r, g, b, a)
  for(let i = 0 ; i &lt; numValues ; i += imgChannels){
    
    let r = imgPixels[i  ];
    let g = imgPixels[i+1];
    let b = imgPixels[i+2];
    // change each pixel's transparency to 0 if the colour is white  
    if ((r === 255) &amp;&amp; 
        (g === 255) &amp;&amp; 
        (b === 255)){
      
      imgPixels[i+3] = 0;  
      
    }
    
  }
  
  // update all pixels
  img.updatePixels();
}
</code></pre>
<p>Notice that the <code>background()</code> call is completely removed as it is redundant since the following <code>image()</code> call will draw on top of the background.</p>
<p>I suspect what you aim to achieve is actually <a href=""https://p5js.org/reference/p5.Image/mask/"" rel=""nofollow noreferrer""><code>mask()</code></a> the video frame when a person is detected so you only see pixels of the person and background is transparent/masked. In theory, you could threshold the segmentation mask ( <a href=""https://p5js.org/reference/p5.Image/filter/"" rel=""nofollow noreferrer""><code>segmentation.mask.filter(THREHOLD, .95)</code></a>) so it's a binary (black / white pixels only) image, then invert (<code>segmentation.mask.filter(INVERT);</code> as needed) to turn the segmentation multicolour image into a mask. This didn't work for me in the p5.js editor however. (Either user error or a bug with a specific version of p5.js: happy to wrong and see a simpler solution).</p>
<p>If masking the background pixels is what you're after and <code>mask()</code> is misbehaving you can apply the technique above to manually update pixels.
In this case you want to initialise a separate image as the result / person pixels opaque / bg pixels transparent to update (copy video pixels only where the pixels aren't full white in the mask):</p>
<pre class=""lang-js prettyprint-override""><code>// ...

function draw() {
  // change bg color to test background
  background(map(sin(frameCount * 0.1), -1.0, 1.0, 0, 255));

  if (segmentation) {
    //image(segmentation.mask, 0, 0, width, height);
    copyForegroundPixels(video, segmentation.mask, personImage);
    image(personImage, 0, 0);
  }
}

// ...

function copyForegroundPixels(imgSource, imgMask, imgResult){
  // make pixels readable
  imgSource.loadPixels();
  imgMask.loadPixels();
  imgResult.loadPixels();
  
  const imgWidth    = imgSource.width;
  const imgHeight   = imgSource.height;
  // p5.Image pixels are in [r1, g1, b1, a1, ..., rn, gn, bn, an] order
  const imgChannels = 4;
  const numValues   = imgWidth * imgHeight * imgChannels;
  const imgPixels   = imgSource.pixels;
  const maskPixels  = imgMask.pixels;
  const resultPixels= imgResult.pixels;
  // loop through each pixels = skipping every 4 values (r, g, b, a)
  for(let i = 0 ; i &lt; numValues ; i += imgChannels){

    let maskR = maskPixels[i  ];
    let maskG = maskPixels[i+1];
    let maskB = maskPixels[i+2];
    
    // copy pixels from source into result if the mask is not white
    if (((maskR === 255) &amp;&amp; 
         (maskG === 255) &amp;&amp; 
         (maskB === 255))){
      
      // clear bg pixels
      resultPixels[i]     = 0;
      resultPixels[i + 1] = 0;
      resultPixels[i + 2] = 0;
      resultPixels[i + 3] = 0;
      
    }else{
      // copy source pixels
      resultPixels[i]     = imgPixels[i];
      resultPixels[i + 1] = imgPixels[i + 1];
      resultPixels[i + 2] = imgPixels[i + 2];
      // ensure fg pixels are opaque
      resultPixels[i + 3] = 255;
    }

  }

  // update all pixels
  imgResult.updatePixels();
}
</code></pre>
<p>full sketch:</p>
<pre class=""lang-js prettyprint-override""><code>let bodySegmentation;
let video;
let segmentation;

let personImage;

let options = {
  maskType: &quot;parts&quot;,
};

function preload() {
  bodySegmentation = ml5.bodySegmentation(&quot;BodyPix&quot;, options);
}

function setup() {
  createCanvas(640, 480);
  // Create the video
  video = createCapture(VIDEO);
  video.size(640, 480);
  video.hide();

  bodySegmentation.detectStart(video, gotResults);
  
  personImage = createImage(video.width, video.height);
}

function draw() {
  background(map(sin(frameCount * 0.1), -1.0, 1.0, 0, 255));
  //image(video, 0, 0);
  if (segmentation) {
    //image(segmentation.mask, 0, 0, width, height);
    copyForegroundPixels(video, segmentation.mask, personImage);
    image(personImage, 0, 0);
  }
}

// callback function for body segmentation
function gotResults(result) {
  segmentation = result;
}

function copyForegroundPixels(imgSource, imgMask, imgResult){
  // make pixels readable
  imgSource.loadPixels();
  imgMask.loadPixels();
  imgResult.loadPixels();
  
  const imgWidth    = imgSource.width;
  const imgHeight   = imgSource.height;
  // p5.Image pixels are in [r1, g1, b1, a1, ..., rn, gn, bn, an] order
  const imgChannels = 4;
  const numValues   = imgWidth * imgHeight * imgChannels;
  const imgPixels   = imgSource.pixels;
  const maskPixels  = imgMask.pixels;
  const resultPixels= imgResult.pixels;
  // loop through each pixels = skipping every 4 values (r, g, b, a)
  for(let i = 0 ; i &lt; numValues ; i += imgChannels){

    let maskR = maskPixels[i  ];
    let maskG = maskPixels[i+1];
    let maskB = maskPixels[i+2];
    
    // copy pixels from source into result if the mask is not white
    if (((maskR === 255) &amp;&amp; 
         (maskG === 255) &amp;&amp; 
         (maskB === 255))){
      
      // clear bg pixels
      resultPixels[i]     = 0;
      resultPixels[i + 1] = 0;
      resultPixels[i + 2] = 0;
      resultPixels[i + 3] = 0;
      
    }else{
      // copy source pixels
      resultPixels[i]     = imgPixels[i];
      resultPixels[i + 1] = imgPixels[i + 1];
      resultPixels[i + 2] = imgPixels[i + 2];
      // ensure fg pixels are opaque
      resultPixels[i + 3] = 255;
    }

  }

  // update all pixels
  imgResult.updatePixels();
}
</code></pre>
","0","Answer"
"79139032","79135894","<p>The problem was fixed when I deleted these two lines:</p>
<pre><code># 2 - stage
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - stage
X = res_block(X, filter= [128,128,512], stage= 3)
</code></pre>
","0","Answer"
"79139702","78540179","<p>Both the Sequential and Functional APIs produced identical results in
your code due to the similar architecture,i would prefer method-2 i.e
<code>Functional</code> API is generally preferred for its flexibility in handling complex
models with multiple inputs and outputs.The Functional API allows for shared
layers, which can reuse features for different tasks. The Sequential API has
independent layers, making it difficult to share information between branches.</p>
","0","Answer"
"79140338","79062004","<p><em>Machining feature recognition</em> is a still an active and quite narrow research field so I'd be surprised if you get any out-of-the-box solution here. The best you can do IMO is to keep trying out solutions from the literature and see how they work for you.</p>
<p>You could indeed get a pointcloud by sampling your original mesh, compute local descriptors and use them to train a classifier. But there is no guarantee that the local geometry around a point is enough to recognize a machining feature.</p>
<p>The solution described in the paper <a href=""https://www.sciencedirect.com/science/article/pii/S2212827117311940"" rel=""nofollow noreferrer"">Freeform Machining Features: New Concepts and Classification</a> uses a mesh as input geometry and combines differential geometry and graph theory to classify machining features. Maybe worth a try?</p>
","0","Answer"
"79146999","79114762","<p>if you are loading your data using pandas , you can use pd.get_dummies</p>
<p>it will automaticly encode categorical features , you can also set columns yourself</p>
<p>more on <a href=""https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html</a></p>
","0","Answer"
"79159395","79134937","<p>Why don't you store the scores in a dataframe and then select the best combo?</p>
<p>Below is a toy example with fake data:</p>
<pre><code>from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc
from sklearn.model_selection import cross_val_score
import statsmodels.api as sm
import numpy as np
import pandas as pd

np.random.seed(0)

# fake dataset
X, Y = make_classification(
    n_samples=1000, 
    n_features=20, 
    n_informative=10, 
    n_redundant=5, 
    n_clusters_per_class=1, 
    weights=[0.9, 0.1], 
    flip_y=0, 
    random_state=0
)


X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])
X_resampled, X_test, Y_resampled, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=42)

# estimators
estimators = [
    ('Logistic Regression', LogisticRegression(max_iter=500)),
    ('Random Forest', RandomForestClassifier())
]

num_features_to_select = [4, 5, 7, 9, 11, 15]


results = []
for estimator_name, estimator in estimators:
    for n_features in num_features_to_select:
        
        rfe = RFE(estimator=estimator, n_features_to_select=n_features)
        rfe.fit(X_resampled, Y_resampled)
        
        
        selected_features = X_resampled.columns[rfe.support_]
        X_train_selected = X_resampled[selected_features]
        X_test_selected = X_test[selected_features]
        
        
        log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit(disp=False)
        pred_test = log_reg_model.predict(X_test_selected)
        pred_test_1 = np.where(pred_test &gt; 0.5, 1, 0)
        
        
        logit_roc_auc = roc_auc_score(Y_test, pred_test)
        f1 = f1_score(Y_test, pred_test_1)
        precision, recall, _ = precision_recall_curve(Y_test, pred_test)
        prc_auc = auc(recall, precision)
        
        
        results.append({
            'Estimator': estimator_name,
            'Num_Features': n_features,
            'F1_Score': f1,
            'ROC_AUC': logit_roc_auc,
            'PRC_AUC': prc_auc
        })


results_df = pd.DataFrame(results)


best_result = results_df.sort_values(['F1_Score', 'ROC_AUC'], ascending=[False, False]).iloc[0]

# best configuration
print(&quot;Best Configuration:&quot;)
print(best_result)
</code></pre>
<p>This prints:</p>
<pre><code>Best Configuration:
Estimator       Random Forest
Num_Features                9
F1_Score             0.896552
ROC_AUC                  0.99
PRC_AUC              0.937778
</code></pre>
","0","Answer"
"79165904","78861705","<p>I've successfully replicated your code using the  versions that you
specified, and it's functioning . The model is training  with defined
epochs. It looks like the problem might be related to how the image data generator is processing the data from the directory. Please check the provided path and verify that your main directory contains sub-folders, each representing a different class, matching the total number of classes in your dataset.</p>
<p>Please refer to this <a href=""https://colab.sandbox.google.com/gist/malla456/8ae17a5172f3977d16dc435de6da7525/78861705.ipynb"" rel=""nofollow noreferrer"">gist</a></p>
","0","Answer"
"79180901","78911175","<p>The error indicates that you're attempting to create a dataset  with two tensors (<code>src_tensor</code> and <code>tgt_tensor</code>), but they have different shapes (i.e, one could have 21 rows and the other could have 22 rows) which is making incompatible for creating a dataset. To resolve this issue, you need to ensure that both tensors have same lengths. This can be achieved by either truncating or padding the shorter tensor.
Please refer to this <a href=""https://colab.sandbox.google.com/gist/Kayyuri/203ed986a25ed44b0c54034eb3898c92/a-language-model-for-machine-translation-between-a-low-resource-language-and-portuguese-using-tensorflow.ipynb#scrollTo=lUp9BPXEO7cd"" rel=""nofollow noreferrer"">gist</a></p>
","0","Answer"
"79190488","78554891","<p>If you fit them the following way you should get only one posterior mean, I think in theory you should also get two means but they are indentical so the model will give you just once:</p>
<pre><code># Input data X 
X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0],[1.0], [2.0], [3.0], [4.0], [5.0]])

# Output data Y 
Y = np.array([[1.5], [2.5], [2.5], [3.5], [3.5], [4.5], [4.5], [5.5], [5.5], [6.5]])
kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

# Fitting the model
gp.fit(X, Y)

mean_prediction, cov_prediction = gp.predict([[1.0], [2.0], [3.0], [4.0], [5.0]], return_cov=True)
</code></pre>
","0","Answer"
"79202237","78493742","<p>After checking the mediapipe repo, it appears that they are changing the code base, and this is an issue in the latest mediapipe release.</p>
<p>So the better solution is to downgrade your mediapipe version.</p>
<p>Use this version : mediapipe==0.10.9</p>
<pre><code>pip uninstall mediapipe #do this to uninstall your current version
pip install mediapipe==0.10.9
</code></pre>
","1","Answer"
"79231185","78483192","<p>Currently, Rasa officially supports Python 3.7 to 3.10. Python 3.11 is not yet fully supported.</p>
<p>First install python version between 3.7 and 3.10 , and then retry <code>pip install rasa</code> command</p>
<p>For additional details, please refer to this.
<a href=""https://rasa.com/docs/rasa-pro/installation/python/environment-set-up/#:%7E:text=Currently%2C%20rasa%20supports%20the%20following,only%20supported%20starting%20from%203.5."" rel=""nofollow noreferrer"">RASA docs</a></p>
","1","Answer"
"79248390","78536168","<p>I think main problem here is size of your neural network. The correlation between date and weather should be relatively strong, we call it climate. Knowledge of this correlation was important milestone in ancient agriculture, as far I know. Correlation between today and tomorrow weather could be also somewhat significant, but there are lot of external variables, which can't be obtained from measurements on single station (frontal systems, clouds carried by wind...).</p>
<p>You should be able to find at least some correlation between input and output, but your network just have no space to perform any useful math.
As I'm looking on your network, you have 6 inputs (temp, wind, humidity, pressure, visibility and date), then single neuron connected to those inputs and then some number of output neurons, one for each weather type. This just can't work.</p>
<p>Single <a href=""https://en.wikipedia.org/wiki/Artificial_neuron"" rel=""nofollow noreferrer"">artificial neuron</a> is basically just one quite simple math equation. Plus it has internal parameters called weights. The weights are the thing, which is tweaked during training. One for each input plus one extra for threshold. Neuron firstly takes all inputs, multiply each with its weight and sums them together. Result is passed to transfer function, in your case sigmoid. Then the threshold weight is added to this result and it is passed to activation function, in your case just default linear activation a(x) = x. Output of single neuron is single float value.</p>
<p>As you can see, you have decimated all weather info into single float value, which can't hold it. And you are trying to describe all weather correlation basically with equation with just 7 degrees of freedom. Try to experiment with shape and size of your network and maybe you can get better results.</p>
<p>I would also recommend you this <a href=""https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.97898&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""nofollow noreferrer"">tensorflow playgound</a>, where you have just two inputs X and Y and datasets are points of two colors. You can add or remove layers, add neurons on each layer, add data preprocessed by some nonlinear math function (like X squared, sin(Y)...). It's not most useful application for AI, but it's nice and visual, great for learning basics.</p>
","2","Answer"
"79249225","78536168","<p>I have to agree with @Thugmek, you need to experiment with your neural architecture.  I'm going to put some code to his suggestion:</p>
<p>Thugnek's Suggestion: Add layers plus a few modifications to experiment with</p>
<pre><code>    // add layers to the model
    model.add(tf.layers.dense({ units: 64, activation: 'relu', inputShape: [6] }));
    model.add(tf.layers.dropout({ rate: 0.2 })); // Add dropout to prevent overfitting
    model.add(tf.layers.dense({ units: 32, activation: 'relu' }));
    model.add(tf.layers.dropout({ rate: 0.2 })); // Add dropout to prevent overfitting

    // softmax because multiclass
    model.add(tf.layers.dense({ units: uniqLabels.length, activation: 'softmax' }));

    // try a better optimizer like adam
    model.compile({
        optimizer: 'adam', // Adam is a robust optimizer over SGD for deep learning
        loss: 'sparseCategoricalCrossentropy',
        metrics: ['accuracy']
    });
</code></pre>
","1","Answer"
"79263230","79116828","<pre><code>! pip install keras==2.10.0 tensorflow==2.10.0
</code></pre>
","0","Answer"
"79270888","78432197","<p>Picture this: You're on a game show, and the host throws a curveball question your way. You've got two ways to tackle it:</p>
<p><strong>Option 1: The Quick-Draw Approach (LLM Way)</strong></p>
<blockquote>
<p>You're like that kid in class who somehow memorized the entire
textbook. The moment you hear the question, bam! The answer's right
there on the tip of your tongue. It's fast, it's impressive, but it's
limited to what you've crammed into your brain.</p>
</blockquote>
<p><strong>Option 2: The Sidekick Strategy (RAG Way)</strong></p>
<blockquote>
<p>Now, imagine you've got this crazy-fast friend hiding just offstage.
When the host finishes the question, you give your buddy a quick
nudge. They dash off to this massive library, grab a bunch of relevant
books, skim through them at lightning speed, and whisper the juicy
bits back to you. You then mix this fresh info with your own smarts
and voila! You've got a killer answer. It takes a bit longer, sure,
but it's like having Google in your ear.</p>
</blockquote>
<p>That's basically the difference between a regular AI and one using RAG. The RAG version might be a tad slower, but it's like comparing fast food to a home-cooked meal - sometimes it's worth the extra few minutes for something really satisfying.</p>
","2","Answer"
"79278645","78846949","<p>I encountered the same issue. In my case, the problem was caused by a mismatch between the TensorFlow versions used to save and load the model. Specifically, I trained and saved the model using TensorFlow <code>3.16</code>, but I attempted to load it in a virtual environment with TensorFlow <code>3.18</code>.</p>
<p>This version mismatch caused compatibility issues, which led to this error:</p>
<p><code>ValueError: Layer 'dense' expects 1 inputs, but it received 2 input tensors.</code></p>
<p>So I created a new virtual environment with TensorFlow <code>3.16</code> (the same version used to train and save the model). After loading the model in this environment, everything worked perfectly.</p>
","0","Answer"
"79281851","78820971","<p>Looks like pyannote cannot download <code>model.bin</code> anymore : (</p>
<p>But I found that it only needs authentication to download once, after which the script can load the models from <code>~/.cache</code>.</p>
","0","Answer"
"79295076","78462277","<p>I faced the same issue, and was able to resolve it using the below code:</p>
<pre><code>from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense
from tensorflow.keras.models import Sequential

# Ensure total_word and max_seq are defined correctly
model = Sequential()
model.add(Embedding(input_dim=total_word, output_dim=100, input_length=max_seq - 1))
model.build((None,max_seq)) # build the Embedding to inilizices the weight
model.add(LSTM(150, return_sequences=True))  
model.add(Dropout(0.2))
model.add(LSTM(100))  
model.add(Dense(total_word, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
</code></pre>
<p><strong>Reason:</strong>
The reason our model works after adding <code>model.build((None, max_seq))</code> is that the Embedding layer requires the input shape to be defined before it can initialize its weights. By calling <code>model.build((None, max_seq))</code>, you explicitly define the input shape, allowing the Embedding layer to initialize its weights properly.</p>
<p>In Keras, layers like Embedding can be added to a model without specifying the input shape. However, the layer's weights are not initialized until the model's input shape is known. Calling <code>model.build()</code> with the input shape as an argument triggers the weight initialization process. This is particularly useful when the input shape is dynamic or not known at the time of model definition.</p>
","1","Answer"
"79307055","78853571","<p>Looks like dynamo_export is a bit buggy... or it doesn't really support earlier models.
Try using the original onnx export instead:</p>
<pre><code>x = torch.randn(&lt;your input dimensions&gt;)    

model.to('cpu') 

with torch.no_grad():
    torch_out = model(x)

    # Export the model
    torch.onnx.export(model,               # model being run
                    x,                         # model input (or a tuple for multiple inputs)
                    &quot;&lt;Your model name&gt;.onnx&quot;,   # where to save the model (can be a file or file-like object)
                    export_params=True,        # store the trained parameter weights inside the model file
                    opset_version=15,          # the ONNX version to export the model to
                    do_constant_folding=True,  # whether to execute constant folding for optimization
                    input_names = ['input'],   # the model's input names
                    output_names = ['output'], # the model's output names
                    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes
                                    'output' : {0 : 'batch_size'}})
</code></pre>
","0","Answer"
"79316924","78854257","<p>MMPoseInferrer outputs an iterator, and you need to use next(result) to extract the results from it.</p>
<pre><code>#Make key point predictions
result = inferencer(frame_rgb)
#Output key point results (can be processed or saved as needed)
print(next(result))
</code></pre>
","-1","Answer"
"79340992","78907608","<p>'messages' have to be a list of <a href=""https://python.langchain.com/api_reference/core/messages.html"" rel=""nofollow noreferrer"">message</a> of type HumanMessage/AIMessage/SystemMessage/ToolMessage. You can prepend the context (assuming it's a string) as a SystemMessage to your existing message list and then invoke.</p>
<pre><code>llm.invoke([SystemMessage(content=context)] + existing_messages)
</code></pre>
","1","Answer"
"79352981","78626157","<p>late by 7 months, yet answering it, if anyone needs it for reference</p>
<ul>
<li>we first make predictions on the image</li>
<li>then we create an white image with the same dimensions of the original image</li>
<li>then we iterate through all the bbox and create a rectangular mask of the same, here mask means just storing the coords of the bbox and the contents of bbox</li>
<li>after creating mask with the help of cv2, all the bbox contents ( basically indexs or coords ) are overwritten to the same indexs or coords of the white image</li>
<li>and lastly both the images are saved</li>
</ul>
<pre><code>import numpy as np

viz = Visualizer(img[:, :, ::-1], metadata={}, scale=1,instance_mode=ColorMode.SEGMENTATION)  # initalising the visualizer
output = viz.draw_instance_predictions(out[&quot;instances&quot;].to(&quot;cpu&quot;))  # making predictions

image = output.get_image()  # getting the image data
white_image = np.ones_like(image) * 255  # creating a white image of same size as original image

for i in range(len(out[&quot;instances&quot;])):
    pred_box = out[&quot;instances&quot;].pred_boxes[i].tensor.tolist()[0]  # getting the coords of each bbox
    pred_box = [int(x) for x in pred_box]  # Convert float coords to int
    mask = np.zeros(image.shape[:2], dtype=np.uint8)  # creating an mask of the same size as original image
    cv2.rectangle(mask, (pred_box[0], pred_box[1]), (pred_box[2], pred_box[3]), 255,-1)  # creating a mask with the bbox coords
    white_image[mask == 255] = image[mask == 255]  # Copy the regions from the original image where the mask

cv2.imwrite(&quot;whited_image.png&quot;, white_image)  # saving the whited image
cv2.imwrite(&quot;original_image.png&quot;, image)  # saving the original predicted image
</code></pre>
<p><strong>hope this is what you were looking, if any queries or problem related to anything feel free to contact</strong></p>
","0","Answer"
"79357227","78583802","<p>though its been 7 months since the question has been asked, but still answering it you or someone else might need it</p>
<p>as mentioned by Christoph Rackwit, to sum over the instances, i would be using the same method to calculate the total number of pixels, along with the code mentioned by you to find the total pixels for each instance, dervied from instance segmentation</p>
<ul>
<li>so first we extract the predicted class, which are basically index values (pred_classes) and predicted masks (pred_mask), the order of these both tensor values are the same</li>
<li>then we iterate over pred_classes and pred_masks and using a dict we add the previous sum of pixel of a particular instance to the current</li>
<li>then store the class names used for training in a list</li>
<li>lastly iterate over the dict to using the list of classes and the pred_classes index values, <strong>we find the total number of pixel for each class</strong> ( containing multiple instances )</li>
</ul>
<pre><code>import locale

pre_classes = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).thing_classes # this contains the class names in the same order used for training, replace it with your custom dataset class names
masks = predictions[&quot;instances&quot;].pred_masks # this extracts the pred_masks from the predicitons
classes = predictions[&quot;instances&quot;].pred_classes.numpy().tolist() # this extracts the pred_classes (contains index values corresponding to pre_classes) to a simple from the predicitons

results = torch.sum(torch.flatten(masks, start_dim=1), dim=1).numpy().tolist() # this calculates the total pixels of each instance

count = dict() # create a dict to store unique classes and their total pixel
for i in range(len(classes)): # itearte over the predicted classes
    count[classes[i]] = count.get(classes[i], 0) + results[i] # add the current sum of pixel of particular class and instance to the previous sum of the same class, adds 0 if the class didnt already exist 

locale.setlocale(locale.LC_ALL, 'en_IN.UTF-8') # set the locale to Indian format
for k, v in count.items(): # itearte over the dict
    print(f&quot;{pre_classes[k]} class contains {locale.format_string('%d', v, grouping=True)} pixels&quot;) # printing each class and its total pixel, pres_classes[k] for accessing corresponding class names for class index, locale.format_string for formating the raw number to Indian format
</code></pre>
<blockquote>
<p>i used the predefined model to perform instance segmentation on the following image</p>
</blockquote>
<p><a href=""https://i.sstatic.net/0bTdw63C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0bTdw63C.png"" alt=""Input Image"" /></a></p>
<blockquote>
<p>which resulted in the following image</p>
</blockquote>
<p><a href=""https://i.sstatic.net/Qs2q8BJn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qs2q8BJn.png"" alt=""Predicted Image"" /></a></p>
<blockquote>
<p>which also produced your required results</p>
</blockquote>
<pre><code>dog class contains 1,39,454 pixels
cat class contains 95,975 pixels
</code></pre>
<p>as i havent had any hands on experience on semantic segmentation, so the solution is provided using instance segmentation,<strong>but if you insist on achieving the same using semantic segmentation please provide the weights, and the inference methods and test dataset, so that i can help you out</strong></p>
<p>anyways i hope that this is what you were looking for, any questions related to the code, logic or working, feel free to contact</p>
","0","Answer"
"79387269","79104305","<p>He buddy, I'm try to fine-tune llama3.2:1b, 3b and llama3.1:8b and I have that same max_seq_length 2048 for all models but after fine-tuning the model context lenght will not be limited to 2048, I think!. I'm having some great problems in fine-tuning.</p>
<p>Sequence length:
This is the specific length of the input you provide to the model, which can be shorter than the model's overall context length.</p>
<p>Context length:
This is the maximum amount of text (in tokens) that the model can process and reference when generating an output, representing its ability to remember and understand the broader context of a conversation or document.</p>
","0","Answer"
"79434845","78523154","<p>you might try calling your training script like so:</p>
<pre class=""lang-bash prettyprint-override""><code>PYTORCH_ENABLE_MPS_FALLBACK=1 python train_me.py
</code></pre>
<p>that would make sure that the variable is set when you pytorch goes looking for it :)</p>
","1","Answer"
"79457624","78844279","<p>I think the version you use in Clip have output embeddings in shape of (batch_size, max_sequence_length, <code>512</code>)</p>
<p>so you will have two solutions</p>
<ol>
<li>in UNet2DConditionModel initialization set <code>cross_attention_dim=512</code> which by default set to 1280</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>unet = UNet2DConditionModel(
    in_channels=4,
    out_channels=4,
    layers_per_block=2,
    sample_size=64,
    block_out_channels=(128, 256, 512, 512),
    down_block_types=(&quot;DownBlock2D&quot;, &quot;DownBlock2D&quot;, &quot;DownBlock2D&quot;, &quot;AttnDownBlock2D&quot;),
    up_block_types=(&quot;AttnUpBlock2D&quot;, &quot;UpBlock2D&quot;, &quot;UpBlock2D&quot;, &quot;UpBlock2D&quot;),
    cross_attention_dim=512,
)

print(unet(torch.randn(1, 4, 64, 64), torch.randn((1, )), torch.randn(1, 77, 512)).sample.shape)
</code></pre>
<ol start=""2"">
<li>add projection layer to clip output embeddings to go from 512 to 1280</li>
</ol>
<p><strong>I recommend to recheck output shape of this CLIP version</strong></p>
","0","Answer"
"79463573","78759790","<p>It might be due to version caching of dataset. Without explicit version attribute, the library's default versioning may not preserve all metadata like Description.</p>
<p>Please include VERSION in a wrapper class like:</p>
<pre><code>import datasets

class My_dataset(datasets.GeneratorBasedBuilder):

    VERSION = datasets.Version(&quot;1.0.0&quot;)
    def _info(self) -&gt; datasets.DatasetInfo:
        return datasets.DatasetInfo(
            description=&quot;my happy lil dataset&quot;,
            features=datasets.Features(
                {
                    &quot;f1&quot;: datasets.Value(&quot;string&quot;), # list of features provided by your dataset with their types
                    &quot;f2&quot;: datasets.Value(&quot;string&quot;),
                }
            ),
            homepage=&quot;https://www.myhomepage.co.uk&quot;,
        )  
</code></pre>
","0","Answer"
"79474609","78620402","<p>I believe the size of each annotation varies each batch that is why it is complaining. Need to make a collate_fn.</p>
","0","Answer"
"79481573","78604530","<p>The error occurs because the <code>adaboost</code> object was created using the <code>boosting.cv()</code> function, which returns a different type of object than <code>boosting()</code>. In <code>adabag</code>, the <code>predict.boosting()</code> function only works with objects created using <code>boosting()</code>.</p>
<p>You can read more using: <code>?predict.boosting</code> or here: <a href=""https://www.rdocumentation.org/packages/adabag/versions/5.0/topics/predict.boosting"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/adabag/versions/5.0/topics/predict.boosting</a></p>
<p>If you want to see the classes assigned based on majority voting in cross-validation, use <code>adaboost$class</code>.</p>
","0","Answer"
"79561427","78857269","<p>i have 2 recommendation :</p>
<ol>
<li><p>please make sure that the matrics value is scalar, you can use like float(tr_loss).</p>
</li>
<li><p>add_hparams do not have global_step as it is not a add_scalar that track your progress, add_hparams is used 1 time only usually in beginning or end.</p>
</li>
</ol>
","0","Answer"
"79604524","78527569","<p>TensorFlow Decision Forest models use the Keras / TensorFlow Saved Model format, which does not store the necessary training meta-data. For this reason, you cannot &quot;continue&quot; training.</p>
<p>There are two solutions:</p>
<p>1.</p>
<p>Train separate models, and stitch them together.
As a downside, this approach is manual and requires some known-how knowledge--notably gradient boosted trees models are a bit harder to stitch than random forests models.</p>
<p>2.</p>
<p>An alternative (and better) solution is to set a working-directory, set &quot;resume_training=True&quot;, and adjust the &quot;num_trees&quot; parameters.
Resume training helps resume training if interrupted (e.g., your computer shuts down), but you can also use it to train on different datasets or change the training parameters during training.</p>
<p>Let's say you want to train the model for 50 steps on a dataset #1, and later train the model for an extra 25 steps on dataset #2. Here are the steps:</p>
<ul>
<li>Configure a learner with 100 trees (a learner is just a class containing the training parameters).</li>
<li>Run the learner (i.e., train model) on dataset #1</li>
<li>Change the number of trees in the learner to 150 i.e., 50 more trees</li>
<li>Run the same learner on dataset #2</li>
</ul>
<p>Notes:</p>
<ul>
<li>I am using <a href=""https://ydf.readthedocs.io/"" rel=""nofollow noreferrer"">YDF</a> (the successor of <a href=""https://www.tensorflow.org/decision_forests"" rel=""nofollow noreferrer"">TensorFlow Decision Forest</a>). YDF is simpler for this kind of operation but gives the exact same results (TensorFlow Decision Forest is calling YDF C++ to train models).</li>
<li>You can use different datasets, but it is important that they have the same features (same name, same type).</li>
<li>You can also change some of the hyper-parameters (e.g., shrinkage).</li>
<li>If you run twice the code above, no training will be done the second time. To train from scratch, you need to remove or change the work_dir.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># Install YDF
pip install ydf -U

# Load libs
import ydf
import pandas as pd

# Load two datasets
ds_path = &quot;https://raw.githubusercontent.com/google/yggdrasil-decision-forests/main/yggdrasil_decision_forests/test_data/dataset&quot;
ds1 = pd.read_csv(f&quot;{ds_path}/adult_train.csv&quot;)
ds2 = pd.read_csv(f&quot;{ds_path}/adult_test.csv&quot;)

# Train a model with 75 trees on the first dataset.
learner = ydf.GradientBoostedTreesLearner(
    label=&quot;income&quot;,
    num_trees=50,
    resume_training=True,
    working_dir=&quot;/tmp/working_dir&quot;)

model_1 = learner.train(ds1)
print(f&quot;The model has {model_1.num_trees()} trees&quot;)

# Train an extra 25 trees
learner.hyperparameters[&quot;num_trees&quot;] += 25
model_2 = learner.train(ds2)
print(f&quot;The model has {model_2.num_trees()} trees&quot;)
# PRINT: The model 2 has 75 trees
</code></pre>
","2","Answer"
"79638422","79134990","<p>In the newer fastai version,</p>
<ul>
<li><code>resnet50()</code>: no pre-trained weights are used.</li>
<li><code>resnet50(weights='DEFAULT')</code>: the pre-trained weights are used.</li>
</ul>
","0","Answer"
"79140577","79139968","<p>You do the following step to your training data.</p>
<pre><code># Add a constant for the intercept
X_count = sm.add_constant(X_count)
X_zero = sm.add_constant(X_zero)
</code></pre>
<p>However, you do not do it to your testing data. I believe that can be your problem, as the dimensions are one off according to your error.</p>
","1","Answer"
"79145730","79143377","<p>The short answer is no, Keras doesn't have &quot;inference only slimmed down version of the library&quot;. You either need :</p>
<ul>
<li>a full thing,</li>
<li>carve the library yourself intot the smallest subset that your model needs,</li>
<li>convert model to something else, e.g. you can use pure TensorFlow since Keras uses it underneath</li>
<li>deploy your model to a separate server, and then use your .exe to start said server and communicate with it</li>
</ul>
","1","Answer"
"79145875","79141566","<p>In general, DQN has little to no convergence guarantees. For experimental, first studies, it might be better to start with a standard Policy Gradient, which under simple conditions (small enough learning rate/big enough batch size) will converge... but potentially very slowly. Q-Learning has nice properties due to ability to learn offpolicy etc. but if you are ok learning directly from experience, policy gradient is a more grounded method (as it has convergence guarantees with deep networks).</p>
<p>And to answer things more explicitly - yes, DQN can diverge, have chaotic behaviour etc. it is not out of ordinary.</p>
","0","Answer"
"79147015","79140091","<p>The model can be trained and inferenced successfully now:
Set safe_serialization to False in model training file tutorial_train_plus.py:</p>
<pre><code>accelerator.save_state(save_path, safe_serialization=False)
</code></pre>
<p>It will generate pytorch_model.bin instead of model.safetensors during training.</p>
<p>Once training is complete, modify the model conversion code as below based on the original instructions in readme:</p>
<pre><code>ckpt = &quot;pytorch_model.bin&quot; # set correct path
sd = torch.load(ckpt)
</code></pre>
<p>Model file ip_adapter.bin will be generated for inference.</p>
","0","Answer"
"79147267","79147000","<p>Before trying your suggested methods, I would take a more simple approach than the ones suggested, for example have a look at n-grams. Then train a simple classifier, for example the logistic regression or a naive Bayes classifier like <a href=""https://stackoverflow.com/questions/48003907/how-to-train-naive-bayes-classifier-for-n-gram-movie-reviews"">this post</a>.</p>
<p>I think that should work for most columns that you have here, possibly only the ID columns such as Group ID and Company ID might get difficult as there are no clear patterns in what they look like. Maybe have a look on how you would distinguish such column and whether they are too random for this approach.</p>
<p>This would probably be easier than going towards embeddings, I think it should be quite effective.</p>
","1","Answer"
"79149342","79139121","<p>Just add Input layer to your model with the same shape (4,). Then it should work.</p>
","0","Answer"
"79150211","79149427","<p>This issue mainly results from how class labels are encoded and loaded during training. Ensure that the labels are encoded and loaded the same way as you did during training so that the indexing is not messed up, that is if the classes are sorted during training make sure they are also sorted during inference. I hope this helps.</p>
","1","Answer"
"79150710","79147122","<p>To compensate for uneven illumination, a standard technique is to first <em>estimate</em> illumination and then divide.</p>
<p>The background is a white sheet of paper, so that's great. I'll estimate illumination with a median blur. The kernel size must be large enough such that no part of the foreground (written text) remains.</p>
<p>This will also, coincidentally, correct white balance. If the text were colored, it'd still be colored.</p>
<pre class=""lang-py prettyprint-override""><code>im = cv.imread(&quot;KifRNuGy.jpg&quot;)

illumination = cv.medianBlur(im, 101)

compensated = im / illumination

# arbitrary 0.8 to keep the bright background within range
compensated = (0.8 * 255 * np.clip(compensated, 0, 1)).astype(np.uint8)
</code></pre>
<p><a href=""https://i.sstatic.net/f5xl58u6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f5xl58u6.jpg"" alt=""compensated"" /></a></p>
<hr />
<p>For the segmentation, perform morphological &quot;closing&quot;. That will erase the fine lines. Now you can get connected components from that, get their bounding boxes, but take the images from the <em>source</em>, because all that will have distorted the handwritten digits.</p>
<p><a href=""https://i.sstatic.net/BOnqjpRz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BOnqjpRz.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/EdFH4IZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EdFH4IZP.png"" alt=""enter image description here"" /></a></p>
<p>Or with a fixed 256x256 region:</p>
<p><a href=""https://i.sstatic.net/i9kYuRj8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i9kYuRj8.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/IxwDMJ1W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IxwDMJ1W.png"" alt=""enter image description here"" /></a></p>
","3","Answer"
"79154038","79152799","<p>The <code>shap.Explainer</code> in <code>auto</code> algorithm selected <code>shap.PermutationExplainer</code> by itself. The class <code>shap.PermutationExplainer</code> sets the seed in <code>__init__</code>, but it calls <code>np.random.shuffle</code> in one of the functions in this class for each sample repeatedly. So the sample order may influence the final results</p>
","1","Answer"
"79156470","79145419","<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>
<pre class=""lang-rust prettyprint-override""><code>// from
let vb_m = vb.pp(&quot;model&quot;);
//to
let vb_m = vb.pp(&quot;embedding_model&quot;);
</code></pre>
","2","Answer"
"79157295","79157138","<p>You are computing the mean/var values along the wrong dimensions.</p>
<p>Pytorch's layer norm computes mean/var values along the dimensions specified by <code>normalized_shape</code>. From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">documentation</a>, the input dimensions are expected to be <code>(*, normalized_shape[0], normalized_shape[1], ...)</code>.</p>
<p>Your code has <code>normalized_shape=(channels,)</code> with an input permuted to have channels as the last dimension, so you should be computing mean/var along that dimension. Instead you compute it along dims <code>(1, 2)</code>.</p>
<p>This is the correct implementation:</p>
<pre class=""lang-py prettyprint-override""><code>def layer_norm(x, weight, bias, eps=1e-5):
    # x shape: [bs, h, w, c]
    # Calculate mean and variance across the spatial dimensions (height, width)
    mean = np.mean(x, axis=-1, keepdims=True)  # shape: (batch_size, 1, 1, channels)
    var = np.var(x, axis=-1, keepdims=True, ddof=0)  # Use ddof=0 for biased variance

    # Normalize
    x_normalized = (x - mean) / np.sqrt(var + eps)

    # Applying weight and bias
    out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
    return out
</code></pre>
<p>Note that I also set <code>eps=1e-5</code> which is the pytorch default.</p>
<p>With this, running:</p>
<pre class=""lang-py prettyprint-override""><code>batch, channels, height, width = 4, 3, 8, 8
# Generate random input
x = np.random.randint(-10, 10, (batch, channels, height, width))

# Calculate outputs from both implementations
layernorm1 = test1(x)
layernorm2 = test2(x)

# Check if outputs are close
are_close = np.allclose(layernorm1, layernorm2, atol=1e-6)
</code></pre>
<p>Results in <code>are_close == True</code></p>
","0","Answer"
"79158128","79157457","<p>Consider what useful features might be. What is the representation of <code>Location</code> or <code>Created At</code> as a number? Also, is the <code>Hashtags</code> indicator a count of the hashtags, or is it just indicator of a specific hashtag?</p>
<p>If those are indicators of a specific location, time of creation or a specific hashtag, then the model may not be able to distinguish those, as it is not a relevant representation. Dependent on what the columns mean, the same probably holds for <code>Username</code> and <code>Tweet</code>. Think of it in the following way: does a higher value mean something? For counts that's the case, for an indicator of a username it does not.</p>
<p>With all the counts, you can check the distribution, as counts tend to be skewed. In that case consider transformations such as a log transformation.</p>
","0","Answer"
"79158244","79139121","<p>The error suggests that the shape of the input data is not compatible with what the model expects. The model expects input data with a shape of <code>(None, 4)</code>, but it's receiving input with the shape <code>(4,)</code>. To fix this, add input layer to your model with the <code>shape (4,)</code> and batch the validation data using <code>.batch()</code> before passing it to <code>model.fit()</code> as below:</p>
<pre><code># Build a simple neural network model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(4,)),  # added Input layer 
    tf.keras.layers.Dense(4, activation='relu'),
    tf.keras.layers.Dense(2)
])
# Load and batch the training/validation data
training_data = load_data(&quot;binarydatatraining.csv&quot;).batch(9)
print(training_data)

validation_data = load_data(&quot;binarydatavalidation.csv&quot;).batch(9)
print(validation_data)
</code></pre>
<p>Please find the attached replicated <a href=""https://colab.sandbox.google.com/gist/Kayyuri/1fb66ab347d115397654ea37b6d7b5d4/invalid-input-shape-for-validation-data.ipynb"" rel=""nofollow noreferrer"">gist</a> for your reference. Thank you.</p>
","1","Answer"
"79159039","79159003","<p><code>X_train_flattened</code> provides the images as input, <code>y_train</code> (the label telling the model which digit it is 0-9) tells the model what it should aim to predict for each image.</p>
<p>This is necessary in supervised machine learning (<a href=""https://www.geeksforgeeks.org/supervised-machine-learning/"" rel=""nofollow noreferrer"">Supervised machine learning tutorial</a>) for the model to learn what classification each image belongs to.</p>
<p>The loss function (<code>sparse_categorical_crossentropy</code> here) computes how far off the model's predictions are from the true labels (<code>y_train</code>). Without <code>y_train</code>, the model wouldn't have any basis for calculating this error and wouldn’t know how to improve.</p>
<p>During training, the model uses the error (or loss) calculated from comparing its predictions to <code>y_train</code> to update its parameters through backpropagation. Here is the original paper on backpropagation if it helps (<a href=""https://www.cs.toronto.edu/%7Ehinton/backprop.html"" rel=""nofollow noreferrer"">George Hinton backpropagation</a>) .</p>
","2","Answer"
"79164545","79164281","<p>You can add an option to save the workflow in the results then use <code>fit_best()</code> to get the trained models on the best parameter combination:</p>
<pre><code>library(tidymodels)

set.seed(1)
sim_data &lt;- sim_regression(250)
sim_rs &lt;- vfold_cv(sim_data)

cart_spec &lt;- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;% 
  set_mode(&quot;regression&quot;)

mars_spec &lt;- 
  mars(prod_degree =tune()) %&gt;% 
  set_mode(&quot;regression&quot;)

wflows &lt;- 
  workflow_set(
    preproc = list(basic = outcome ~ .),
    models = list(cart = cart_spec, mars = mars_spec),
    cross = TRUE)

wflow_res &lt;- 
  wflows %&gt;% 
  workflow_map(
    seed = 2,
    resamples = sim_rs,
    grid = 10,
    control = control_grid(save_workflow = TRUE)
  )

with_best_fits &lt;- 
  wflow_res %&gt;% 
  mutate(fit = map(result, ~ fit_best(.x, metric = &quot;rmse&quot;)))

with_best_fits$fit
#&gt; [[1]]
#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════
#&gt; Preprocessor: Formula
#&gt; Model: decision_tree()
#&gt; 
#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────
#&gt; outcome ~ .
#&gt; 
#&gt; ── Model ───────────────────────────────────────────────────────────────────────
#&gt; n= 250 
#&gt; 
#&gt; node), split, n, deviance, yval
#&gt;       * denotes terminal node
#&gt; 
#&gt;   1) root 250 138175.0000  15.7340300  
#&gt;     2) predictor_04&lt; 5.005281 233 108125.5000  13.2361200  
#&gt;       4) predictor_04&gt;=-5.67335 221  81891.5800  10.9861600  
#&gt;         8) predictor_18&gt;=-0.04129457 118  40858.3500   4.2061130  
#&gt;          16) predictor_14&lt; 3.064134 95  26406.7600   0.6664635  
#&gt;            32) predictor_14&lt; -2.539472 22   7014.9180 -10.0433000  
#&gt;              64) predictor_18&gt;=2.279312 9   2418.6340 -24.2686700 *
#&gt;              65) predictor_18&lt; 2.279312 13   1514.1670  -0.1949626 *
#&gt;            33) predictor_14&gt;=-2.539472 73  16108.0000   3.8940630  
#&gt;              66) predictor_09&lt; -0.7610177 42  10034.8300  -0.5414930  
#&gt;               132) predictor_04&lt; 0.9336627 30   6400.4920  -4.7811980  
#&gt;                 264) predictor_14&lt; -1.061535 9   4314.9580 -13.9170600 *
#&gt;                 265) predictor_14&gt;=-1.061535 21   1012.4270  -0.8658293  
#&gt;                   530) predictor_16&lt; 0.2395412 14    393.8731  -4.3145150 *
#&gt;                   531) predictor_16&gt;=0.2395412 7    119.0292   6.0315430 *
#&gt;               133) predictor_04&gt;=0.9336627 12   1746.9510  10.0577700 *
#&gt;              67) predictor_09&gt;=-0.7610177 31   4127.3320   9.9035250  
#&gt;               134) predictor_04&gt;=-2.137203 22   2225.2690   6.3287210  
#&gt;                 268) predictor_10&gt;=-1.785821 15   1172.6710   2.6434810 *
#&gt;                 269) predictor_10&lt; -1.785821 7    412.3508  14.2256600 *
#&gt;               135) predictor_04&lt; -2.137203 9    933.6820  18.6419400 *
#&gt;          17) predictor_14&gt;=3.064134 23   8345.0050  18.8264000  
#&gt;            34) predictor_06&lt; 2.689544 16   2544.6270  13.0448800 *
#&gt;            35) predictor_06&gt;=2.689544 7   4043.1270  32.0413100 *
#&gt;         9) predictor_18&lt; -0.04129457 103  29394.5700  18.7536000  
#&gt;          18) predictor_14&lt; 2.859504 83  21343.1800  15.7495900  
#&gt;            36) predictor_18&gt;=-2.907324 56  10884.7500  10.5042200  
#&gt;              72) predictor_07&lt; -3.013126 12   3536.4490   0.6905080 *
#&gt;              73) predictor_07&gt;=-3.013126 44   5877.4010  13.1806900  
#&gt;               146) predictor_20&gt;=0.7343885 15   1681.9700   7.2229070 *
#&gt;               147) predictor_20&lt; 0.7343885 29   3387.6110  16.2623000  
#&gt;                 294) predictor_14&lt; -0.6088161 10   1057.5280   8.9745340 *
#&gt;                 295) predictor_14&gt;=-0.6088161 19   1519.4330  20.0979600 *
#&gt;            37) predictor_18&lt; -2.907324 27   5721.9630  26.6288700  
#&gt;              74) predictor_12&gt;=-1.279433 19   3490.4090  21.8447500 *
#&gt;              75) predictor_12&lt; -1.279433 8    763.8750  37.9911500 *
#&gt;          19) predictor_14&gt;=2.859504 20   4194.0540  31.2202300  
#&gt;            38) predictor_16&gt;=-0.5423066 10    322.7158  21.1905000 *
#&gt;            39) predictor_16&lt; -0.5423066 10   1859.4290  41.2499600 *
#&gt;       5) predictor_04&lt; -5.67335 12   4511.2250  54.6727800 *
#&gt;     3) predictor_04&gt;=5.005281 17   8669.7690  49.9701500 *
#&gt; 
#&gt; [[2]]
#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════
#&gt; Preprocessor: Formula
#&gt; Model: mars()
#&gt; 
#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────
#&gt; outcome ~ .
#&gt; 
#&gt; ── Model ───────────────────────────────────────────────────────────────────────
#&gt; Selected 24 of 39 terms, and 11 of 20 predictors
#&gt; Termination condition: Reached nk 41
#&gt; Importance: predictor_04, predictor_18, predictor_19, predictor_20, ...
#&gt; Number of terms at each degree of interaction: 1 11 12
#&gt; GCV 91.03479    RSS 13353.8    GRSq 0.8366059    RSq 0.9033559
</code></pre>
","0","Answer"
"79164582","79162036","<p>Try Image hashing it creates a compact, fixed-length hash that represents an image's visual features. I have used it on images of BIOS it worked well in my case.</p>
","-1","Answer"
"79165081","79165072","<p>Reference: <a href=""https://python.langchain.com/docs/how_to/add_scores_retriever/#selfqueryretriever"" rel=""nofollow noreferrer"">how to add scores</a></p>
<p>What you can do is sub-class <code>SelfQueryRetriever</code> and ensure score is returned along with the documents. This can be achieved by using the <code>similarity_search_with_score</code> method</p>
<pre class=""lang-py prettyprint-override""><code>from langchain.retrievers.self_query.base import SelfQueryRetriever
from typing import Any, Dict

from typing import Any, Dict


class CustomSelfQueryRetriever(SelfQueryRetriever):
    def _get_docs_with_query(
        self, query: str, search_kwargs: Dict[str, Any]
    ) -&gt; List[Document]:
        &quot;&quot;&quot;Get docs, adding score information.&quot;&quot;&quot;
        docs, scores = zip(
            *self.vectorstore.similarity_search_with_score(query, **search_kwargs)
        )
        for doc, score in zip(docs, scores):
            doc.metadata[&quot;score&quot;] = score

        return docs

retriever = CustomSelfQueryRetriever.from_llm(
    llm = llm,
    vectorstore = vectorstore,
    document_contents = document_content_description,
    metadata_field_info = metadata_field_info,
    enable_limit=True, 
    search_type = &quot;similarity_score_threshold&quot;,
    search_kwargs={&quot;score_threshold&quot;: 0.80, &quot;k&quot;: 5},
    verbose=True
)


result = retriever.invoke(&quot;your query&quot;)
</code></pre>
<p>Now your <code>Document</code> metadata willcontain a key named <code>score</code></p>
","0","Answer"
"79165155","79165030","<p>I am assuming you are talking about CUDA version 12.6 that is mentioned when you run the nvidia-smi command in terminal.<a href=""https://i.sstatic.net/Jp8qyPe2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jp8qyPe2.png"" alt=""demo of nvidia-smi output"" /></a></p>
<p>This shows the version of CUDA that is supported by your GPU and does not mean that CUDA 12.6 is installed on your system.</p>
<p>You need to run the following commands based on OS:</p>
<p>On Windows System:</p>
<pre><code>pip3 install torch --index-url https://download.pytorch.org/whl/cu124
</code></pre>
<p>On Linux System:</p>
<pre><code>pip3 install torch
</code></pre>
<p>Note: Also make sure your virtual environment is active in pycharm terminal before running the above pip commands.</p>
","-2","Answer"
"79166074","79165974","<p>In this case, setting the <code>random_state</code> depends on the specific algorithm you’re using, rather than on the <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> class.</p>
<p>For <code>KNeighborsClassifier</code>, adding <code>random_state</code> is actually unnecessary because this classifier is a deterministic algorithm, meaning it doesn’t rely on randomness to make predictions. Therefore, it won’t be affected by a <code>random_state</code> parameter. As a result:</p>
<ol>
<li><p><strong>For <code>KNeighborsClassifier</code>:</strong> You don’t need to set <code>random_state</code> at all in either the classifier or in the <code>GridSearchCV</code>/<code>RandomizedSearchCV</code>.</p>
</li>
<li><p><strong>For Randomized Algorithms:</strong> If you’re using an algorithm that involves randomness, like a decision tree or a random forest, you can set the <code>random_state</code> in the estimator (like <code>RandomForestClassifier(random_state=42)</code>). You don’t need to set <code>random_state</code> in <code>GridSearchCV</code>, as it only influences the cross-validation process, which is deterministic.</p>
</li>
</ol>
<p>In summary:</p>
<ul>
<li><strong>For <code>KNeighborsClassifier</code>:</strong> No <code>random_state</code> is needed.</li>
<li><strong>For Randomized algorithms:</strong> Set <code>random_state</code> in the estimator, not in <code>GridSearchCV</code>/<code>RandomizedSearchCV</code>.</li>
<li><strong>For <code>RandomizedSearchCV</code>:</strong> You might set <code>random_state</code> there if the search itself is randomized and you want reproducibility.</li>
</ul>
","1","Answer"
"79166175","79165974","<p>As described above, for <code>sklearn</code> you can set the <code>random_state</code> parameter within the model for the relevant models.
The <code>GridSearchCV</code> also uses some randomization in the cross validation, so best to fix the seed in the model and in the grid search as well:</p>
<pre><code>grid_search_knn = GridSearchCV(
    estimator=KNeighborsClassifier(),
    cv=KFold(3, random_state=42),
    n_jobs=-1

)
</code></pre>
<p>or</p>
<pre><code>grid_search_dt = GridSearchCV(
        estimator=DecisionTreeClassifier(random_state=42),
        cv=KFold(3, random_state=42),
        n_jobs=-1)
</code></pre>
<p>Beside that, you might want to set the seed for other packages as well if you use them for processing the data:</p>
<pre><code>import numpy as np
import random 

def set_random_seed(random_seed=42):
    &quot;&quot;&quot;
    Set seed for hte random, numpy.random
    : param random_seed (int) : initial value for the random seed generators.
    &quot;&quot;&quot;
    random.seed(random_seed)
    np.random.seed(random_seed)
    #torch.manual_seed(random_seed)


set_random_seed()
</code></pre>
<p>And also in you train-test split:</p>
<pre><code>Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)
</code></pre>
","0","Answer"
"79166943","79165030","<p>There are 2 versions of CUDA installed on your computer:</p>
<ul>
<li>the driver version, (shown by <code>nvidia-smi</code>, also called CUDA-toolkit) is the version that is installed when you update your driver, this is the cuda used any time some process want execute something in GPU. This should be the most recent one (or so) in any cases</li>
<li>the runtime version, it's the cuda used specifically by Pytorch to make parallelized computations via its routines. The version of pytorch and the runtime version should match. The driver-CUDA is called after the runtime routines, the calling scheme is:</li>
</ul>
<p>Pytorch Python API -&gt; Pytorch C++ API -&gt; runtime CUDA routines -&gt; local driver CUDA -&gt; GPU</p>
<p><strong>Since Pytorch 2.0 the runtime cuda libraries are automatically installed in your environment so you only need to update your nvidia drivers (and upgrade pip) before calling <code>pip install torch</code></strong></p>
<p>Make sure your python version is 3.9+ (using a virtual environment is highly recommended). Your graphic card is quite recent so there is no problem regarding it (some old graphic card could have cuda compatibility issues)</p>
<p><code>pip install torch</code> should install torch==2.5.1 with runtime cuda 12.4 if everything is good (most recent version when I'm writing this)</p>
","-1","Answer"
"79168870","79168636","<p>Your question does not follow StackOverFlow Community Guidelines on how to post a question. Specifically, your question is non coding question, and is asking for recommendations which is generally frowned upon. But anyway, let me help you :)</p>
<p>First thing, I could think of from your question is sheer quantity of images that you have in your dataset. I would recommend you to collect more good quality images to improve your model accuracy and performance.</p>
<p>If, you can't get hold of more images, you can apply Data Argumentation Techniques like</p>
<ol>
<li>Geometric Transformations: Try to rotate/scale/flipping your images.</li>
<li>Color and Brightness Adjustments: Play with your dataset's color and brightness levels.</li>
</ol>
<p>This will significantly, increase the number of images. The next important thing is to make sure that you have proper Data distribution. That is, if your dataset is skewed towards one class, model performance will degrade. Make sure you use techniques like Oversampling to minority class. Ensure that the train and validation sets have a similar class distribution to avoid skewed evaluation.</p>
<p>Last piece of advise, play with different models and constantly hypertune your model params.</p>
","-1","Answer"
"79170369","79170289","<p>Your issue is probably that your <code>pd.get_dummies</code> generates a column for each category it sees. This means that if there are different cases in your training and testing data, you will have other columns in the resulting dataset.</p>
<p>To solve your issue, you can <code>pd.Categorical</code> for any categorical column, as follows. Then fix the categories for the training data and generate dummies using that column.</p>
<pre><code>categories = train_df['category'].unique()

train_df['category'] = pd.Categorical(train_df['category'], categories=categories)
train_dummies = pd.get_dummies(train_df['category'], prefix='category')

test_df['category'] = pd.Categorical(test_df['category'], categories=categories)
test_dummies = pd.get_dummies(test_df['category'], prefix='category')
</code></pre>
<p>This gives consistent columns, but it leaves out columns not existing in the training data that are in testing. However, that is probably the best way to handle.</p>
<p>A few separate notes on your code. First, if your <code>income</code> column is not categorical, it doesn't make sense to make that into a dummy column. Also, you're imputing your <code>age</code> column, but not using it afterwards. And lastly, you could consider fitting and transforming the imputer on only the training data and only doing the transformation on the testing data. This makes more sense in my opinion, as you are imputing with the same value.</p>
","1","Answer"
"79170456","79170289","<p>When encoding categorical variables, you need to be careful, if you decide to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer""><code>pd.get_dummies</code></a>. After you use it for your training data, it's not trivial to keep the same encoding it came up with on your test data. That is, if on your training data you had categorical values in a <code>gender</code> column, such as <code>[&quot;female&quot;, &quot;male&quot;]</code>, and it encoded it as <code>[0, 1]</code> respectively, there is no guarantee that it will do the same if you run it on your test data with the same categorical values.</p>
<p>On the other hand, if you have categorical values that only appear on your training set, <code>pd.get_dummies</code> will naturally only encode those and create the respective new columns, that is, <code>[&quot;gender_male&quot;, &quot;gender_female&quot;]</code>. If coincidentally after making your train/test split, the training set only has the <code>&quot;male&quot;</code> values, then your current code will create <code>&quot;gender_male&quot;</code> for that DataFrame and the testing set will have a <code>&quot;gender_female&quot;</code> column. Hence, both having different columns. Note that I purposely avoided the <code>drop_first=True</code> conversation to make my point, but you might consider doing that, as discussed heavily in this <a href=""https://stackoverflow.com/questions/63661560/drop-first-true-during-dummy-variable-creation-in-pandas#:%7E:text=drop_first%3DTrue%20is%20important%20to,correlations%20created%20among%20dummy%20variables"">StackOverflow post</a>.</p>
<p>This post: <a href=""https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data"">Keep same dummy variable in training and testing data</a> also goes over this topic in detail.</p>
<p>The following example aims to exemplify this with some made-up data (since we don't have access to yours):</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

# Generate sample data
np.random.seed(42)
num_rows = 1000

data = pd.DataFrame(
    {
        &quot;occupation&quot;: np.random.choice(
            [&quot;Tech-support&quot;, &quot;Priv-house-serv&quot;, &quot;Protective-serv&quot;, &quot;Armed-Forces&quot;], num_rows
        ),
        &quot;race&quot;: np.random.choice([&quot;White&quot;, &quot;Asian-Pac-Islander&quot;, &quot;Amer-Indian-Eskimo&quot;, &quot;Other&quot;, &quot;Black&quot;], num_rows),
        &quot;gender&quot;: np.random.choice([&quot;Male&quot;, &quot;Female&quot;], num_rows),
        &quot;native-country&quot;: np.random.choice([&quot;United-States&quot;, &quot;Cambodia&quot;, &quot;England&quot;, &quot;Puerto-Rico&quot;], num_rows),
        &quot;age&quot;: np.random.randint(18, 81, num_rows),
        &quot;income&quot;: np.where(np.random.rand(num_rows) &lt; 0.25, &quot;&gt;50K&quot;, &quot;&lt;=50K&quot;),
    }
)

# Split the data into train and test sets
X = data.drop(columns=&quot;income&quot;)
y = data[&quot;income&quot;]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Simulate a scenario in which &quot;Other&quot; is coincidentally missing from 
# the test set
X_test[X_test[&quot;race&quot;] == &quot;Other&quot;] = np.nan

# Create dummy variables on the train and test sets separately
X_train = pd.get_dummies(
    X_train,
    columns=[&quot;occupation&quot;, &quot;race&quot;, &quot;gender&quot;, &quot;native-country&quot;],
    drop_first=False,
)
X_test = pd.get_dummies(
    X_test,
    columns=[&quot;occupation&quot;, &quot;race&quot;, &quot;gender&quot;, &quot;native-country&quot;],
    drop_first=False,
)

print(&quot;X_train columns:&quot;, X_train.columns)
print(&quot;X_test columns:&quot;, X_test.columns)

# Test if the columns are the same
assert X_train.columns.equals(X_test.columns)  # This will fail!
</code></pre>
<p><strong>What you should do instead is to use <code>pd.get_dummies</code> on your whole dataset first and then train/test split!</strong></p>
<p>This should avoid any columns being different issues. That is,</p>
<pre><code># Generate sample data
data = ...

# Split the data into train and test sets
X = data.drop(columns=&quot;income&quot;)
y = data[&quot;income&quot;]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Simulate a scenario in which &quot;Other&quot; is coincidentally missing from the test set
X_test[X_test[&quot;race&quot;] == &quot;Other&quot;] = np.nan

# Combine the train and test sets before creating dummy variables
# (Note that we can just do this on X first, then train/test split, but it helps me
#  make my point regarding categorical values missing in X_test)
X_all = pd.concat([X_train, X_test], ignore_index=True)
X_all = pd.get_dummies(
    X_all,
    columns=[&quot;occupation&quot;, &quot;race&quot;, &quot;gender&quot;, &quot;native-country&quot;],
    drop_first=False,
)

# Split the data back into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_all, y, test_size=0.2, random_state=42
)

print(&quot;X_train columns:&quot;, X_train.columns)
print(&quot;X_test columns:&quot;, X_test.columns)

# Test if the columns are the same
assert X_train.columns.equals(X_test.columns)  # This won't fail!
</code></pre>
<p>Finally, I recommend using scikit-learn's <a href=""https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer"">OneHotEncoder</a> and your code would look like something like this with the added benefit of saving how exactly it did the encoding to the categorical values:</p>
<pre><code># Simulate a scenario in which &quot;Other&quot; is coincidentally missing from the test set
# This time doing it on the original dataset, X
X[X[&quot;race&quot;] == &quot;Other&quot;] = np.nan
original_columns = X.columns

# Create dummy variables using OneHotEncoder
encoder = OneHotEncoder(handle_unknown=&quot;ignore&quot;, sparse_output=False)
X = encoder.fit_transform(X)

# Convert the encoded data to DataFrames
X = pd.DataFrame(X, columns=encoder.get_feature_names_out(original_columns))

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(&quot;X_train columns:&quot;, X_train_encoded.columns)
print(&quot;X_test columns:&quot;, X_test_encoded.columns)

# Test if the columns are the same
# This also won't fail!
assert X_train_encoded.columns.equals(X_test_encoded.columns)
</code></pre>
<p>You can read up more about it <a href=""https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe"">here</a>.</p>
","1","Answer"
"79172942","79172053","<p>I believe your problem might be arising from the last batch having fewer samples than the specified batch size (<code>64</code>), because the total number of samples is not divisible by <code>64</code>. You might have to do something like this:</p>
<pre><code>train_dataloader = DataLoader(
    train_dataset, 
    batch_size=64, 
    shuffle=True,
    drop_last=True  # This prevents the last incomplete batch
)
</code></pre>
<p>See the <a href=""https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data"" rel=""nofollow noreferrer"">official documentation</a> for more details.</p>
<p>On the other hand, I assume your first dimension is the batch dimension. If your <code>x</code> array is going through these changes in the <code>forward()</code> method of your <code>Network(nn.Module)</code> class: <code>torch.Size([64, 1, 64, 64]) -&gt; torch.Size([64, 32, 31, 31]) -&gt; torch.Size([64, 64, 14, 14]) -&gt; torch.Size([49, 16384]) -&gt; torch.Size([49, 128]) -&gt; torch.Size([49, 64]) -&gt; torch.Size([49, 16])</code>, you can see that the first dimension (batch) starts with <code>64</code> and ends up being <code>49</code>, the flattening being likely the problem. I advice you to get the batch dimension out in the first line, doing something like this: <code>batch_size = x.size(0)</code> and then, do <code>x = x.view(batch_size, -1)</code> to flatten, instead of <code>x = x.view(-1, 64 * 16 * 16)</code>, which hardcodes values.</p>
","0","Answer"
"79173918","79173890","<p>do you shuffle your training data? if cats and dogs are inputed as a separate group your model will predict the last group in your case &quot;cats&quot;. this is the nature of the classification models. you need to mix the data before giving it to the network. hope helped.</p>
<p>try to add <code>shuffle=True</code> to the <code>generate_datasets</code> function</p>
","1","Answer"
"79175332","79175150","<p>This reproduces your error message</p>
<pre><code>In [4]: a = np.ones((10,100)); b = np.ones((10,1)); a@b
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[4], line 1
----&gt; 1 a = np.ones((10,100)); b = np.ones((10,1)); a@b

ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0,
with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) 
(size 10 is different from 100)
</code></pre>
<p>Which inidicates that <code>KXX_star</code> is the transpose of what you think!</p>
<p>In your question, it wasn't always clear which shapes were verified, and which were just 'wishes'.</p>
<h2>edit</h2>
<pre><code>cov_matrix(test_data,...)
</code></pre>
<p><code>test_data</code> size is 100, and the column dimension of the <code>cov</code> array,</p>
","0","Answer"
"79175601","79165072","<p>Yes, The similarity score isn't returned in the Document object
So, You can use below code to get similarity scores,</p>
<pre><code># retrieves the top k documents and their similarity scores based on the query.
results_with_scores = vectorstore.similarity_search_with_score(query, k=5)
for doc, score in results_with_scores:
    print(f&quot;Document: {doc.page_content}&quot;)
    print(f&quot;Similarity Score: {score}&quot;)
</code></pre>
<p>results_with_scores is a list of tuples with below,</p>
<ul>
<li>doc: Document with page_content and metadata.</li>
<li>score: The similarity score comparing the query and the document.</li>
</ul>
","0","Answer"
"79179500","79175150","<p>I will use kriging functions for <code>mu</code> and <code>var</code>:</p>
<p>here is how you could solve the problem:</p>
<pre><code>class SquaredExponentialKernel:
    def __init__(self, length_scale=1.0, variance=1.0):
        self.length_scale = length_scale
        self.variance = variance
    
    def __call__(self, x1, x2):
        x1 = np.atleast_2d(x1)
        x2 = np.atleast_2d(x2)
        dist_sq = np.square(x1[:,None] - x2).sum(2)
        # use vectorization. If not, use `cdist` from `scipy.spatial.distance` 
        #dist_sq = scipy.spatial.distance.cdist(data_x, test_data, 'sqeuclidean')
        return self.variance * np.exp(-0.5 * dist_sq / self.length_scale**2)
    


class GPR:
    def __init__(self, data_x, data_y, covariance_function=SquaredExponentialKernel(), white_noise_sigma: float = 0):
        self.noise = white_noise_sigma
        self.data_x = data_x
        self.data_y = data_y
        self.covariance_function = covariance_function
        self.kernel = covariance_function(data_x, data_x) + self.noise * np.identity(len(self.data_x))
        self._inv = np.linalg.inv(self.kernel)
        self._ones = np.ones_like(data_x).ravel()
        self._denom = self._ones @ self._inv @ self._ones
        self._mean = self._ones @ self._inv @ data_y /self._denom
        self._diff = (data_y.ravel() - self._mean)
        self._var =  self._diff @ self._inv @ self._diff / data_y.size
                        

    def predict(self, test_data: np.ndarray) -&gt; np.ndarray:
        KXX_star = self.covariance_function(self.data_x, test_data)
        mean_test_data = self._mean + KXX_star.T @ self._inv @ self._diff
        a = (self._inv @ KXX_star)
        cov_test_data = self._var * (1 - KXX_star.T @ a + 
                                    (1 - self._ones @ a)**2/self._denom)
        var_test_data = np.diag(cov_test_data)
        self._memory = {'mean': mean_test_data, 'covariance_matrix': cov_test_data, 'variance': var_test_data}
        return self._memory
    
#%%
gpr_se = GPR(data_x, data_y, covariance_function=SquaredExponentialKernel(a.kernel_.get_params()['length_scale']), white_noise_sigma=0)
m = gpr_se.predict(test_data)
</code></pre>
","0","Answer"
"79179901","79179803","<p>Here is the function simplified:</p>
<pre><code>f1_score &lt;- function(y_true, y_pred, positive = '1'){
  tt &lt;- table(y_true, y_pred)
  TP &lt;- tt[positive, positive]
  FP &lt;- tt[rownames(tt)!=positive, positive]
  FN &lt;- sum(tt[positive, colnames(tt) !=positive])
  precision &lt;- TP/(TP+FP)
  recall &lt;- TP/(TP+FN)
  2 * (precision * recall) / (precision + recall)
}

f1_score(y, x, &quot;1&quot;)
[1] 0.3333333

f1_score(y, x_preds, &quot;1&quot;)
[1] 0.9090909
</code></pre>
<p>Notice how the <code>FP</code> is not summed since we assume there are only two categories in <code>y_true</code>. If more categories, ie non-binary, then use <code>FP &lt;- sum(....)</code></p>
","1","Answer"
"79183020","79181943","<p>I figured it out.</p>
<p>The lines:</p>
<pre><code>optimizer = torch.optim.SGD(  # Tune the optimizer
    model.parameters(), lr=config[&quot;lr&quot;], momentum=config[&quot;momentum&quot;]
)
</code></pre>
<p>Were not failing at compile time because <code>model</code> happens to be a different valid variable with a <code>.parameters()</code> function further up in the file. <code>model</code> did not refer to the models I was training in this context.</p>
","0","Answer"
"79184872","79184686","<p>You can use the booster object using <code>xgb_model.get_booster()</code>. Then use the <code>get_score()</code> method with the <code>best iteration</code> to get the feature importances for the best model. Finally, Convert the feature scores into a format similar to feature_importances_.</p>
<p>Here is a sample code you can use:</p>
<pre><code>from xgboost import XGBRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Prepare data
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model with early stopping
xgb_model = XGBRegressor(n_estimators=1000, early_stopping_rounds=100, eval_metric=&quot;rmse&quot;)
xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

# Get the best iteration
best_iteration = xgb_model.best_iteration

# Access the booster and retrieve feature importances up to best_iteration
booster = xgb_model.get_booster()
importance_dict = booster.get_score(importance_type='weight', iteration=best_iteration)

# Convert the importance dict into a list of feature importances
importances = [importance_dict.get(f'f{i}', 0) for i in range(X.shape[1])]

# Print the importances
print(importances)
</code></pre>
","0","Answer"
"79185627","79185545","<p>Firstly,</p>
<p>There is an indentation problem. The <code>vectorize_sequence</code> return is indented incorrectly.</p>
<p>Should be this:</p>
<pre><code>def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
    return results
</code></pre>
<p>Secondly:</p>
<p>Import layers:</p>
<pre><code>from tensorflow.keras import layers
</code></pre>
<p>Thirdly:</p>
<p>You need to have the labels in categorical format:</p>
<pre><code>y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)
</code></pre>
<p>Then you should get an accuracy of mid 80's as stated in the textbook. I looked at the textbook and it says 95% is for state of the art methods not the naïve approach defined in the book. (Unless you were referring to training accuracy? In which case, yes, &gt;95% is correct)</p>
<p><a href=""https://i.sstatic.net/BHjExKOz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHjExKOz.png"" alt=""result"" /></a></p>
","1","Answer"
"79189612","79189607","<p><code>EarlyStoppingRounds</code> was added to <code>XGBoost</code> in <code>1.4.0</code>.</p>
<p>If the version is below 1.4.0, upgrade XGBoost with:</p>
<pre><code>pip install --upgrade xgboost
</code></pre>
<p><a href=""https://github.com/dmlc/xgboost/releases/tag/v1.4.0"" rel=""nofollow noreferrer"">xgboost/releases/tag/v1.4.0</a></p>
","0","Answer"
"79194757","79189607","<p>I think you're using <code>xgboost&gt;=2.0</code>. In this case, <code>early_stopping_rounds</code> should be in <code>params</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def objective(trial):
    # Suggest values for hyperparameters
    params = {
        &quot;objective&quot;: &quot;reg:squarederror&quot;,
        &quot;eval_metric&quot;: &quot;rmse&quot;,
        &quot;tree_method&quot;: &quot;hist&quot;,  # Use hist method
        &quot;device&quot;: &quot;cuda&quot;,       # Specify using GPU
        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
        &quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
        &quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
        &quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
        &quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
        &quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
        &quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
        &quot;n_estimators&quot;: 1000,  # Define n_estimators in the initialization of the model
        &quot;early_stopping_rounds&quot;: 50,
    }

    # Initialize the model
    model = xgb.XGBRegressor(**params)

    # Train the model with early stopping callback
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_valid, y_valid)],
        verbose=False,
        # early_stopping_rounds=50  # Stops if no improvement after 50 rounds
    )
</code></pre>
<p>Ref: <a href=""https://github.com/dmlc/xgboost/pull/9986"" rel=""nofollow noreferrer"">https://github.com/dmlc/xgboost/pull/9986</a></p>
","0","Answer"
"79198942","79165030","<p>You need to download the version of Pytorch that is compatible with CUDA for that, head over to the website: <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">https://pytorch.org/</a></p>
<p><a href=""https://i.sstatic.net/fzj0QO26.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fzj0QO26.png"" alt=""As shown in the picture"" /></a></p>
<p>Download the latest one (12.4) and Run the command that is shown in the image on your Command Terminal.</p>
<p>Then Install Cuda same version (12.4) URL: <a href=""https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exe_local"" rel=""nofollow noreferrer"">https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exe_local</a></p>
<p><a href=""https://i.sstatic.net/oJsZsfjA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oJsZsfjA.png"" alt=""Download Same Version for CUDA that is compatible with Pytorch"" /></a></p>
<p>Then check the commands,</p>
<pre><code>Nvidia-smi
nvcc --version 
</code></pre>
<p>Ensure the versions for Pytorch &amp; CUDA are same, Hope this helps!</p>
","-1","Answer"
"79200895","79199199","<p>Adding a dense layer without finetuning doesn’t seem to be practical because its weights are initialized randomly, therefore it disrupts the semantic space and making embeddings less meaningful for tasks like similarity searches. As you mentioned you can use techniques like <code>PCA</code> or <code>UMAP</code> to reduce embeddings to your desired size. This is computationally efficient and preserves relationships in the embedding space.</p>
<p>However, I think the best approach would be to explore other sentence transformer models like <code>all-MiniLM-L6-v2</code> (384 dimensions), which are already optimized for smaller embedding sizes while maintaining good accuracy. You can find a list of such models <a href=""https://sbert.net/docs/sentence_transformer/pretrained_models.html"" rel=""nofollow noreferrer"">here.</a></p>
","1","Answer"
"79201439","79201296","<p>You could drop the 4th band of data, typically an alpha channel, while reading it in with OpenCV like this.</p>
<pre><code>import cv2

img = cv2.imread(filename)
</code></pre>
<p>and if the workflow requires an image path instead of a <code>numpy</code> object, then I might run a pre-processing workflow that copies 3-channel images to a new directory <code>train_img_path</code>.</p>
","0","Answer"
"79202461","79153512","<p>1.) This error typically arises when converting a TensorFlow/Keras model to Core ML using coremltools. It occurs because the perm argument (used for reshaping or reordering tensor dimensions) does not match the rank (number of dimensions) of the tensor being operated on.</p>
<p>2.) Rank Mismatch in Bidirectional LSTM Layers;</p>
<p>The Bidirectional(LSTM(...)) layer can return 3D tensors, but Core ML may expect 2D tensors. Use the following adjustments;</p>
<p>Return Last Output Only: Set return_sequences=False in the last Bidirectional(LSTM(...)) layer. This ensures the output tensor is 2D, which is compatible with Core ML:</p>
<pre><code>model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=32, return_sequences=False)))
</code></pre>
","0","Answer"
"79208781","79205991","<p>I have a solution that requires an extra forward pass from <code>model2</code>. If someone can think of a solution that doesn't require this, feel free to chime in.</p>
<p>First, a bit about backprop. When you call <code>backward</code> on a tensor, pytorch computes gradients for all tensors in the computational graph. Since <code>loss1</code> and <code>loss2</code> are both output from <code>model2</code>, calling <code>backward</code> on either results in computing gradients for all parameters in both <code>model1</code> and <code>model2</code>.</p>
<p>When these gradients are computed, pytorch uses current parameter values and activations stored from the forward pass. This means that the parameters used in the backward computation need to be unchanged from the forward pass.</p>
<p>When you call <code>optimizer.step()</code>, you update all parameters in the optimizer with an in-place update. This is why you get the error you see.</p>
<pre class=""lang-py prettyprint-override""><code>loss1.backward(retain_graph=True) # computes gradients
optimizer.step() # updates parameters in place
...
 
loss2.backward() # throws an error due to the in-place update from optimizer step
</code></pre>
<p>Because of this, the <code>step</code> calls must happen after all the <code>backward</code> calls. This leads to an issue of how to control gradients. The following does not work:</p>
<pre class=""lang-py prettyprint-override""><code># trying to zero grads fails
loss1.backward() # backward first loss
optimizer1.zero_grad() # zero model2 grads
loss2.backward() # loss 2 grads computes for model1

# changing the loss order results in the same
</code></pre>
<p>I think there are two ways around this:</p>
<ol>
<li>Cache grads outside pytorch parameters and manually re-add them to parameters before calling <code>step</code> (hacky, probably breaks a bunch of stuff)</li>
<li>Do an extra forward pass (inefficient from compute, but works within Pytorch's structure)</li>
</ol>
<p>I opted for the second option</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

torch.manual_seed(42)

class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.conv1 = nn.Linear(20, 10)
        self.conv2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

# Define the second model
class Net2(nn.Module):
    def __init__(self):
        super(Net2, self).__init__()
        self.conv1 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.conv1(x)
        return x

# Initialize models
model1 = Net1()
model2 = Net2()

# Initialize separate optimizers for each model
opt1 = torch.optim.SGD(model1.parameters(), lr=0.1)
opt2 = torch.optim.SGD(model2.parameters(), lr=0.1)

p1_1 = next(model1.parameters()).data.clone()
p2_1 = next(model2.parameters()).data.clone()

criterion = nn.MSELoss()

inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs)
out1 = model2(features)
out2 = model2(features.detach()) # detach removes from the computational graph

opt1.zero_grad()
opt2.zero_grad()

# update only model1 with loss1
loss1 = criterion(out1[0], labels[0]) 
loss1.backward()
opt2.zero_grad()
opt1.step()

# check parameters after update
p1_2 = next(model1.parameters()).data.clone()
p2_2 = next(model2.parameters()).data.clone()
assert not (p1_1 == p1_2).any() # all parameters from model1 updated
assert (p2_1 == p2_2).all() # no parameters from model2 updated

# update only model2 with loss2
loss2 = criterion(out2, labels)
loss2.backward()
opt2.step()

# check parameters after update
p1_3 = next(model1.parameters()).data.clone()
p2_3 = next(model2.parameters()).data.clone()
assert (p1_2 == p1_3).all() # no parameters from model1 updated
assert not (p2_2 == p2_3).any() # all parameters from model2 updated
</code></pre>
<p>Note that I changed your loss from cross entropy to MSE. Your model produces an output of size <code>(bs, 1)</code> while cross entropy expects an output of shape <code>(bs, num_classes)</code>. The output of shape <code>(bs, 1)</code> implies you are computing cross entropy of a single class, which will always return 0 loss</p>
<pre class=""lang-py prettyprint-override""><code>pred = torch.randn(64, 1)
labels = torch.randn(64, 1)
criterion = nn.CrossEntropyLoss()
criterion(pred, labels)
&gt; tensor(-0.)
</code></pre>
","1","Answer"
"79209144","79192127","<p>To best use the text classifiers, you should be utilising <a href=""https://developer.apple.com/documentation/naturallanguage/nlmodel"" rel=""nofollow noreferrer"">NLModel</a> to generate the predictions.</p>
<p>The key is using <a href=""https://developer.apple.com/documentation/naturallanguage/nlmodel/predictedlabelhypotheses(for:maximumcount:)"" rel=""nofollow noreferrer"">predictedLabelHypotheses(for:maximumCount:)</a></p>
<p>Your new code would look a little similar to;</p>
<pre><code>let model: FVCompeitionTextClassifier? = {
    let config = MLModelConfiguration()
    return try? Roll(configuration: config)
}()

func predict(phrase:String) throws -&gt; String {
    let rollModel = try NLModel(mlModel: model!.model)
    
    for labelHypothese in rollModel.predictedLabelHypotheses(for: phrase, maximumCount: 1) {
        print(&quot;Label: \(labelHypothese.key), Confidence: \(labelHypothese.value)&quot;)
        // Do logic that you need to do ie sort, then return the label you need
    }
    
}
</code></pre>
","1","Answer"
"79210858","79205991","<p>There's a way that require you to collect gradients explicitly and then update them later. Pytorch has support for collecting gradient for parameters of will using <code>torch.autograd.grad</code> and optimize them using the functional approach <code>torch.optim.adam.adam()</code>. I may update a more comprehensive answer when I get some free time but here are some docs that you can look through yourself:</p>
<ul>
<li><a href=""https://pytorch.org/docs/stable/generated/torch.autograd.grad.html"" rel=""nofollow noreferrer"">How to explicitly get the gradients of the parameters you want.</a></li>
<li><a href=""https://discuss.pytorch.org/t/explicitly-passing-gradients-to-torch-optim/8578/4"" rel=""nofollow noreferrer"">How to update using explicit gradients.</a></li>
</ul>
<p><strong>In short</strong>, get explicit gradients of <code>model1</code> using <code>torch.autograd.grad</code> on <code>loss1</code> -&gt; <code>loss2.backward()</code> -&gt; update <code>model2</code> then <code>zero_grads</code> both model -&gt; explicitly update <code>model1</code> using <code>torch.optim.adam.adam()</code></p>
","1","Answer"
"79224412","79224395","<p>Numpy may be inferring the datatype to be 2 characters for <code>labels = np.array(['No']*10000)</code> since all elements of the array have two characters.</p>
<p>Try <code>labels = np.array(['No']*10000, dtype='&lt;U3')</code></p>
","1","Answer"
"79224726","79224304","<p>If you comment out your writing then ensure that the model has been already pickled and present in the specified directory which you can check by:</p>
<pre><code>import os
print(os.listdir(&quot;home/jovyan/work&quot;))
</code></pre>
<p>Also, since its a &quot;FileNotFoundError&quot; and if you're running this in different environment while you mentioned before that it can be a cloud issue....it can be because you might be using the relative path rather than absolute path.</p>
","0","Answer"
"79225483","79225295","<p>It might be something to do with the line of code</p>
<pre><code>img = tf.transpose(img, perm=[1, 0, 2])
</code></pre>
<p>in <code>encode_single_sample</code> function.</p>
<p>I ran the code and checked the output by using matplotlib's matshow() function and it seemed the image was the same but was rotated by 90 degrees. Check the order of the matrix that you want for your input node and transpose the image matrix accordingly.</p>
","1","Answer"
"79228158","79226995","<p>I would suggest to:</p>
<ul>
<li>Run the training from a python script instead of a jupyter notebook. Write the output to a file instead of printing to the standard output and check if the issue persist</li>
<li>Run the training on a reduced version of the dataset: in this way you should be able to reach the same amount of epochs in less time. If the problem persists after the same time interval, the problem may be related to the number of epochs, otherwise it may be related to the running time.</li>
<li>Run the training on a different machine/on your laptop, to check if the problem is related to the remote machine you're running on. (If it's too computationally heavy for your laptop, you might use a reduced version of the dataset here as well)</li>
</ul>
","0","Answer"
"79228229","79212687","<p>If you use <code>shuffle=True</code> it should reshuffle the data at each epoch (<a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer"">from PyTorch documentation</a>). This means that at each epoch, the batches of data will be arranged differently with respect to the previous one.</p>
<p>During testing, since during testing the model doesn't change and you don't usually test for multiple epochs, it shouldn't make any difference.</p>
<p>However, setting <code>shuffle=False</code>, ensures consistency in the order of predictions, and it may make debugging easier.</p>
","0","Answer"
"79229089","79195592","<p>The steps to download Kaggle datasets in Google Colab, including installing
the Kaggle API, uploading the Kaggle API key, setting up authentication,
downloading the dataset, unzipping the data (if necessary), and accessing
the data.I have downloaded garbage_classification dataset and split it into
training and validation are all executed in the following <a href=""https://colab.sandbox.google.com/gist/malla456/f789a1878ed40a3406a7b542df730eeb/79195592.ipynb#scrollTo=RLHE0y6SuyZy"" rel=""nofollow noreferrer"">gist</a>.</p>
","1","Answer"
"79232708","79232596","<p>What is the reason why you have been using PHP as a broker between the client and Flask? Why not use Flask directly?</p>
<p>But, maybe you need to rename the files when they are moved between folders in the PHP script, for example, if you receive an image called test1.jpg you could try renaming the file by adding some of the timestamp then the file name finally called test1-12123434.png the point is to avoid the probable cache problems with the files at the PHP server</p>
","0","Answer"
"79234950","79232257","<p>When using huggingface <code>transformer</code> library, the output returned by the model includes the model loss. Before starting the training, simply perform a forward pass on the dataset and obtain the model loss.</p>
<pre class=""lang-py prettyprint-override""><code>with torch.no_grad():
    outputs = model(**inputs, labels=labels)

loss = outputs.loss
print(f&quot;Loss: {loss.item()}&quot;)
</code></pre>
","1","Answer"
"79235796","79228528","<p>In the creation of <code>MultiscaleCNN</code>, you want to divide the embeddings dim to 3 parts, but <code>4096</code> is not divisible by <code>3</code>, instead each dimension of a subnetwork is cast to <code>4096//3 = 1365</code>, then multiply by <code>3</code> which give out <code>1365 * 3 = 4095</code>. For a quick fix, to inititalize <code>DeepCNN</code>, you can pass <code>out_dim - (out_dim // 3) * 2</code> as the residual dimension.</p>
","0","Answer"
"79236168","79227390","<p><strong>Two different approach:</strong></p>
<p><strong>1. BERT NER</strong></p>
<p>If you are low on resources, then consider trying BERT NER. Fine-tune it with your data. The thing with BERT NER is that under the hood, it uses an attention mechanism which allows it to get a good understanding of context. By using the context around the entities, it can learn which information to extract and which ones to ignore.</p>
<p><strong>2. Small LLM</strong></p>
<p>Apart from that, if you have ample resources, you can fine-tune a smaller LLM, such as LLaMA 1B, on your dataset. Even if you do not fine-tune it, I am sure you will easily achieve around 70% accuracy for extracting recipes given any sentences.</p>
<p>When training any model, make sure the data is balanced. This means you should not only include examples where entities are present, but also include sentences where there are no entities. This way, the model will learn to understand the difference between sentences that contain entities  and those that do not, helping it to accurately return no entities when none are present.</p>
<p><strong>Example:</strong></p>
<p><strong>Sentences with Food Entities:</strong></p>
<ul>
<li>&quot;Add an onion to a bowl of carrots.&quot;</li>
<li>&quot;Sprinkle with paprika.&quot;</li>
</ul>
<p><strong>Sentences without Entities:</strong></p>
<ul>
<li><p>Stir well, and cook an additional minute.</p>
</li>
<li><p>Please pass me the book.</p>
</li>
<li><p>The cat sat on the mat.</p>
</li>
</ul>
<p>By incorporating a different types of sentences, your fine tuned model will learn better.</p>
","0","Answer"
"79238361","79219306","<p>Using Joblib instead of Numpy will solve this problem. Remember to place <code>import joblib</code> at the start of your program, and replace</p>
<pre><code>import numpy as np
np.save('card.npy',card)
card=np.load('card.npy',allow_pickle=True)
</code></pre>
<p>with</p>
<pre><code>joblib.dump(card, 'card.joblib')
card = joblib.load('card.joblib')
</code></pre>
<p>This should not give you any errors.</p>
","-1","Answer"
"79239295","79239232","<p>I think there are different mathematical definitions of softmax in different contexts.</p>
<ul>
<li>Wikipedia definition (on real numbers): <code>exp(z) / sum(exp(z))</code></li>
<li>What I inferred from your code: <code>(1&lt;&lt;(z-z_max + 16)) / sum((1 &lt;&lt; (z-z_max + 16)))</code> or something similar. <code>1&lt;&lt;</code> === <code>2**</code> obviously.</li>
</ul>
<p>The major difference is the base number of the exponential. With base too high you are highly likely to get underflow and get a lot of <code>-128</code>. Besides there are also a biase that maps the result to [-128, 127] range, which is trival and less important</p>
<p>It's highly likely that the library that you takes test cases from use a different definition than both of above.</p>
<p>I did some testing with your test case and floating point definition of softmax with matplotlib, and the following expression gives a good fit:</p>
<pre><code>softmax_naive = (np.exp(inarr / 128) / np.sum(np.exp(inarr / 128)) * 256) - 100
</code></pre>
<p>You can imagine that you probably need to do a <code>&gt;&gt;7</code> to input bytes before doing <code>1&lt;&lt;</code> 2-based exponential. To give completely identical result, surely you should dig into that library code, which I didn't have time to do.</p>
<p>Below are validation codes:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

inarr = np.array([101, 49, 6, -34, -75, -79, -38, 120, -55, 115], dtype=np.int8).astype(np.double)
expected_arr = np.array([-57, -70, -79, -86, -92, -94, -88, -54, -91, -56], dtype=np.int8).astype(np.double)
print(expected_arr)

softmax_naive = (np.exp(inarr / 128) / np.sum(np.exp(inarr / 128)) * 256) - 100
print(softmax_naive - expected_arr)
plt.plot(inarr)
plt.plot(expected_arr)
plt.plot(softmax_naive)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/xFRdCfWi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFRdCfWi.png"" alt=""validation of softmax"" /></a></p>
","2","Answer"
"79239313","79239232","<pre><code>import numpy as np


def softmax(x):
    &quot;&quot;&quot;
    Softmax函数实现
    &quot;&quot;&quot;
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()


# 假设的带符号8个整数输入
input_numbers = np.array([-2, 1, 3, -1, 0, 2, 4, -3])

softmax_result = softmax(input_numbers)

print(softmax_result)
</code></pre>
","-2","Answer"
"79240258","79239232","<blockquote>
<p>what am I doing wrong?</p>
</blockquote>
<p>You desire identical output to what<br />
<a href=""https://github.com/ARM-software/CMSIS-NN/blob/main/Tests/UnitTest/generate_test_data.py"" rel=""nofollow noreferrer"">https://github.com/ARM-software/CMSIS-NN/blob/main/Tests/UnitTest/generate_test_data.py</a><br />
produces, but you didn't call the routines that it calls,
and you didn't specify parameters of <code>x_in=5</code> and <code>y_in=2</code>.</p>
<p>You want to call <code>generate_data()</code> in<br />
<a href=""https://github.com/ARM-software/CMSIS-NN/blob/main/Tests/UnitTest/softmax_settings.py"" rel=""nofollow noreferrer"">https://github.com/ARM-software/CMSIS-NN/blob/main/Tests/UnitTest/softmax_settings.py</a></p>
<p>If you desire a somewhat different output format for those figures,
start by creating a
<a href=""https://docs.python.org/3/library/unittest.html#unittest.TestCase"" rel=""nofollow noreferrer"">unit test</a>
that successfully reproduces the figures of interest.
Then tweak your target code, while still keeping that test Green.
The test runner will help you to notice if / when a small edit broke something.</p>
","-1","Answer"
"79243092","79243091","<p>In order to fix this issue you need to install some dependencies and make the downloaded file executable.</p>
<pre class=""lang-bash prettyprint-override""><code># Install missing libraries
!sudo apt-get update
!sudo apt-get install -y libglu1-mesa libx11-dev libxi-dev libxmu-dev libgl-dev

# Download and install CUDA 9.0 silently with --override to replace the current one
!wget -q https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run -O cuda_9.0.176_384.81_linux-run
!chmod +x cuda_9.0.176_384.81_linux-run
!sudo sh cuda_9.0.176_384.81_linux-run --silent --override --toolkit --samples

# Manually specify the toolkit and sample directories
!sudo sh /usr/local/cuda-9.0/bin/cuda-install-samples-9.0.sh /root
</code></pre>
<p>after this, you can install and <code>import thundersvm</code> library.</p>
","0","Answer"
"79244606","79241634","<p>I solved the problem using a dictionary instead of a list.</p>
<pre class=""lang-py prettyprint-override""><code>def generator(file_paths, batch_size, files_per_batch, size, value):
    return tf.data.Dataset.from_generator(
        lambda: data_generator(file_paths, batch_size, files_per_batch, size, value),
        output_signature=(
            {f&quot;input_{i}&quot;: tf.TensorSpec(shape=(batch_size, size), dtype=tf.float32) for i in range(size + 1)},  # Inputs
            tf.TensorSpec(shape=(batch_size, size), dtype=tf.float32)  # Labels
        )
    )
</code></pre>
<p>To achieve this, I adjusted the input layer to:</p>
<pre class=""lang-py prettyprint-override""><code>inputArray = [Input(shape=(size,), name=f&quot;input_{i}&quot;) for i in range(size + 1)]
</code></pre>
<p>This adjustment ensures that the keys from the generator match the keys expected by the model at the input.</p>
","0","Answer"
"79246911","79236682","<p>Yes, variable <code>example</code> is a slice of dataset (a batch), not a one item from the dataset. In your particular case you have batch_size set to 1:</p>
<p><code>per_device_train_batch_size=1</code></p>
<p>It means that entire dataset is splitted into batches of size 1, i.e. arrays of size 1. So one batch is of the following representation:</p>
<pre><code>[{&quot;quote&quot;: [&quot;Quote 1&quot;], &quot;author&quot;: [&quot;Author 1&quot;]}]
</code></pre>
<p>So in order to get the values you use <code>0</code> as index.</p>
<p>Normally if you use batch size larger than or equal to 2 2, the representation would like:</p>
<pre><code>[{&quot;quote&quot;: [&quot;Quote 1&quot;, &quot;Quote 2&quot;, ...], &quot;author&quot;: [&quot;Author 1&quot;, &quot;Author 2&quot;, ...]}]
</code></pre>
<p>So in line with documentation you would like to use other formatting_func, where you iterate over both arrays like in this case:</p>
<pre><code>def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['instruction'])):
        text = f&quot;### Question: {example['instruction'][i]}\n ### Answer: {example['output'][i]}&quot;
        output_texts.append(text)
    return output_texts
</code></pre>
<p>Source: <a href=""https://huggingface.co/docs/trl/en/sft_trainer"" rel=""nofollow noreferrer"">https://huggingface.co/docs/trl/en/sft_trainer</a></p>
","1","Answer"
"79247093","79241735","<p>This is not the way how you pick up the dataset items. First you need to indicate the slice:</p>
<pre><code>prepared_ds_batch = prepared_ds['train'][0:10]
</code></pre>
<p>by using indexing.</p>
<p>Then you can use the key <code>labels</code></p>
<pre><code>prepared_ds_batch['labels']
[out]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>
<p>Regarding the second issue with saving the data: you are not able to save it because of the known issue with transform functions: <a href=""https://github.com/huggingface/datasets/issues/6221"" rel=""nofollow noreferrer"">https://github.com/huggingface/datasets/issues/6221</a></p>
<p>You might however save the dataset as <code>prepared_ds.with_format(None).save_to_disk('test_path')</code>. But after loading it again from disk you need to launch again the transform function.</p>
<p>Edited: You cannot use <code>prepared_ds['train']['labels']</code> as 'labels' is expected to be integers representing indices of the items.</p>
","1","Answer"
"79248379","79247672","<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>
<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=""https://captum.ai/tutorials/Bert_SQUAD_Interpret"" rel=""nofollow noreferrer"">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>
<p>Below please find snippet that works:</p>
<pre><code>from captum.attr import LayerIntegratedGradients


def custom_forward(inputs):
    preds = predict(inputs)
    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)
lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)
def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)
    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)

    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

results = []

for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
        print(influential_words)
</code></pre>
","1","Answer"
"79250666","79250549","<p>Note that the signature of <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html"" rel=""nofollow noreferrer""><code>scipy.stats.t</code></a> is <code>interval(confidence, df, loc=0, scale=1)</code>. There is no <code>alpha</code> keyword, pass it as positional or relabel it to <code>confidence</code>.</p>
","3","Answer"
"79255838","79249247","<p>Yes, the input layer needs to be size 49.  The output layer needs to be size 1.  Then it'll work for both the train and test data.</p>
","-1","Answer"
"79258097","79257966","<p>The encoder is being applied to the entire dataset, which is probably giving the incorrect results</p>
<p>I think it's better to define separate transforms</p>
<pre><code>from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

num_transformer = StandardScaler()
cat_transformer = OneHotEncoder(handle_unknown='ignore')

# create a preprocessor using columntransformer from sklearn
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_transformer, numerical_cols),
        ('cat', cat_transformer, categorical_cols),
    ]
)

# combine into single pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])
</code></pre>
<p>Then apply</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model.fit(X_train, y_train)
</code></pre>
<p>Modify the inference similarly.
Let me know if this helps with your problem.</p>
<p>Edit:
Another way to separate and convert x data to ohencoded values in a simpler way is:</p>
<pre><code>x_categorical = df.select_dtypes(include=['object']).apply(ohe.fit_transform)
x_numerical = df.select_dtypes(exclude=['object']).values
</code></pre>
<p>Then combine</p>
<pre><code>x = pd.concat([pd.DataFrame(x_numerical), x_categorical], axis=1).values
</code></pre>
","0","Answer"
"79259788","79259076","<p>Using the co-occurence matrix as word representations + PCA later on to reduce dimensionality for K means is a fine approach.</p>
<p>Some improvements you can consider can be</p>
<ul>
<li>Using word embeddings instead of a co-occurence matrix (from a model like Word2Vec trained or fine tuned on your documents) as this can give you a richer representation of words</li>
<li>Using the <a href=""https://en.wikipedia.org/wiki/Elbow_method_(clustering)"" rel=""nofollow noreferrer"">elbow method</a> to find the optimal number of clusters in k means, or (usually preferably) <a href=""https://en.wikipedia.org/wiki/Silhouette_(clustering)#Silhouette_Clustering"" rel=""nofollow noreferrer"">Silhouette clustering</a></li>
<li>For dimensionality reduction, PCA is efficient but is known to preserve global structure. An alternative would be t-SNE which preserve local structure. This is explained well in <a href=""https://datascience.stackexchange.com/a/36890"">this</a> answer. Or <a href=""https://pair-code.github.io/understanding-umap/"" rel=""nofollow noreferrer"">U-map</a> which can help balance between local and global structure.</li>
</ul>
","1","Answer"
"79268436","79247785","<p>I understand your problem and tbh I don't have a clear cut answer. However, using the same validation set repeatedly like you propose is also not desired since you risk (over)fitting your hyperparams on that specific part of the data.</p>
<p>You could pre-specify the folds and impute the validation fold based on the other training folds. You could also accept that there is a small chance of leakage due to one or two observations ending up in the validation fold. Of course, this totally depends on your data and method of imputation.
Wildly varying performance across the CV folds is an indicator that leakage is a problem.</p>
","0","Answer"
"79269574","79268711","<p>I have found a solution. I have replaced &quot;N&quot; with &quot;-1&quot; by using this code:</p>
<pre><code>df2 = df2[df2['Model Year'] != 'N/']  # Filter out rows where 'Model Year' is 'N/'

for col in cols:
    df2[col] = df2[col].replace('N/', -1)
    le.fit(df2[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)
</code></pre>
","1","Answer"
"79271428","79271387","<p>There is nothing wrong in your implementation or results. In fact, it is as expected. In your code, you are using <code>make_blobs()</code> with random selection of Gaussian samples. The <code>make_blobs()</code> function draws samples from a special Gaussian mixture model.</p>
<p>As explained <a href=""https://stats.stackexchange.com/questions/534543/what-is-the-meaning-of-isotropic-gaussian-blobs-which-are-generated-by-sklearn"">here</a> in more detail, for the <code>make_blobs()</code> function, each cluster or component has equal probability of being sampled and the cluster centers can be either specified or in the case of your code randomly generated.</p>
","0","Answer"
"79273491","79271387","<p>Have you tried setting seed values for all the packages that use random values?</p>
<p>Can you try this in your the file where you implemented kmeans?</p>
<pre class=""lang-py prettyprint-override""><code>seed_value = 42
np.random.seed(seed_value)
random.seed(seed_value)
</code></pre>
","0","Answer"
"79274525","79244294","<p>This does not seem like Hardware accelerator - GPU/TPU error as I tried replicating the same using Google Colab by selecting the available - CPU/GPU and TPU and did not notice any issue. Please find the replicated <a href=""https://colab.sandbox.google.com/gist/RenuPatelGoogle/78d102ed9d6270fd45c3ee56b17d1bc2/79244294.ipynb"" rel=""nofollow noreferrer"">gist</a> for the same.</p>
<p>Please try again by installing the latest <code>Tensorflow 2.18</code> and <code>Keras 3.7</code> version and let us know if the issues still persists. Thank you.</p>
","1","Answer"
"79275601","79275458","<p>Instead of</p>
<pre><code>blobFromImage(frame, 1.0, Size(640, 480), Scalar(), true, false);
</code></pre>
<p>Try</p>
<pre><code>Mat blob = blobFromImage(frame, 1/127.5, Size(300, 300), Scalar(127.5, 127.5, 127.5), true, false);
net.setInput(blob);
Mat detections = net.forward();
</code></pre>
","0","Answer"
"79276814","79271439","<p>Seems that adding a property decorator <code>hyperparameter_length_scale</code> as in the <a href=""https://github.com/scikit-learn/scikit-learn/blob/15eb9f30c77ec8166a0135ca14b8de7fdfe15b91/sklearn/gaussian_process/kernels.py#L1517"" rel=""nofollow noreferrer"">source code for RBF</a>, resolves the issue (otherwise there is a dimension mismatch in the kernel hyperparameters <code>gaussian_process.kernel.theta</code> ):</p>
<pre><code>class RangeLimitedRBFTest(Kernel):

    @property
    def hyperparameter_length_scale(self):
        return Hyperparameter(&quot;length_scale&quot;, &quot;numeric&quot;, self.length_scale_bounds)
    
    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5), x_min = 0., x_max = 1.):
        self.length_scale = length_scale
        self.length_scale_bounds = length_scale_bounds
        self.rbf_kernel = RBF(length_scale, length_scale_bounds)
        self.x_min, self.x_max = x_min, x_max
    
    def __call__(self, X, Y=None, eval_gradient=False):
        if eval_gradient and Y is not None:
            raise ValueError(&quot;Gradient can only be evaluated when Y is None.&quot;)
        X = np.atleast_2d(X)
        if Y is not None:
            Y = np.atleast_2d(Y)
        return self.rbf_kernel(X, Y, eval_gradient=eval_gradient) 
        
    def diag(self, X):
         return self.rbf_kernel.diag(X)

    def is_stationary(self):
        return self.rbf_kernel.is_stationary()
</code></pre>
<p>Testing with small synthetic data from <a href=""https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py"" rel=""nofollow noreferrer"">scikit-learn's GaussianProcessRegressor example</a>:</p>
<pre><code>from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

def train_pred_plot(X, y, X_train, y_train, kernel):
    gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
    gaussian_process.fit(X_train, y_train)        
    mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)        
    plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
    plt.scatter(X_train, y_train, label=&quot;Observations&quot;)
    plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
    plt.fill_between(
        X.ravel(),
        mean_prediction - 1.96 * std_prediction,
        mean_prediction + 1.96 * std_prediction,
        alpha=0.5,
        label=r&quot;95% confidence interval&quot;,
    )
    plt.legend()
    plt.xlabel(&quot;$x$&quot;)
    plt.ylabel(&quot;$f(x)$&quot;)
    plt.title(&quot;Gaussian process regression on noise-free dataset&quot;)
</code></pre>
<p>with RBF kernel</p>
<pre><code>kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))
train_pred_plot(X, y, X_train, y_train, kernel)
</code></pre>
<p>we get the following prediction confidence intervals:</p>
<p><a href=""https://i.sstatic.net/3h2kDvlD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3h2kDvlD.png"" alt=""enter image description here"" /></a></p>
<p>whereas with the custom kernel</p>
<pre><code>kernel = 1.0 * RangeLimitedRBFTest(length_scale=0.1, length_scale_bounds=(8e-2, 8e-1), x_min=0., x_max=2.5) + WhiteKernel(noise_level=0.5, noise_level_bounds=(1e-2, 1e1))
train_pred_plot(X, y, X_train, y_train, kernel)
</code></pre>
<p>we get the following prediction confidence intervals:</p>
<p><a href=""https://i.sstatic.net/AJA1W2k8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJA1W2k8.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79278254","79276804","<p>I used some simulated data to try to reproduce the results (and could).</p>
<p>Some of the workflows have tuning parameters and some don't. <a href=""https://workflowsets.tidymodels.org/reference/workflow_map.html"" rel=""nofollow noreferrer""><code>workflow_map()</code></a> has the default argument of <code>fn = &quot;tune_grid&quot;</code> but will fall back to <code>&quot;fit_resamples&quot;</code> if the workflow doesn't have tuning parameters.</p>
<p>If you take out <code>fn = &quot;tune_grid&quot;</code> from your code, it runs.</p>
<p>I can't reproduce</p>
<blockquote>
<p>&quot;Error in summary.connection(connection) : invalid connection&quot;</p>
</blockquote>
<p>I assume it is related to parallel processing? If you are working over a remote session, it could be related to a connection problem too.</p>
<p>One other thing... we won't have an obvious way of adding custom grids (yet). You can do this though:</p>
<pre class=""lang-r prettyprint-override""><code>basic_models &lt;- basic_mlb  |&gt; 
  workflow_map(seed = 100,         #&lt;- removed &quot;fit_resamples&quot;
               verbose = TRUE,
               resamples = team_rs, 
               control = grid_ctrl) %&gt;% 
  option_add(grid = xgb_grid, id = &quot;basic_xgb&quot;) %&gt;% 
  option_add(grid = rf_grid,  id = &quot;basic_rf&quot;) 
</code></pre>
","1","Answer"
"79279437","79279124","<ol>
<li><p>No, but the <code>Conv2D()</code> layer can work on variable image sizes. The way the <code>Conv2D()</code> layer works is by applying the kernel on each set of kernel_size x kernel_size pixels. The padding adds to the dimensions before the convolution as well. So as long as the image dimensions are at least kernel_size x kernel_size after padding, the convolution will work. In this case, kernel_size is 3 and padding is 1 so even a single pixel will work because after padding, the image will be 3x3. The reason this CNN can't take variable image sizes is because of the linear layer. The linear layer requires a 7x7 image after max-pooling twice, so a 28x28 image to start with.</p>
</li>
<li><p>It depends on the type of layer, linear layers take in a set input size and a set output size, which determines the number of parameters (neurons). The number of parameters for convolution layers are determined by the kernel size and number of output channels so it doesn't rely on input size.</p>
</li>
<li><p>It depends on the model, some models allow variable image sizes even if they were trained on a specific resolution.</p>
</li>
</ol>
","1","Answer"
"79281673","79281636","<p>Well, decision trees, and therefore forests, don't really care about scale. They don't perform any computation on values anyway (only on their distribution. I mean, conditional medians, quantiles, etc. But they never add them, multiply them, etc. Except maybe, when using trees/forest for a quantitative regression, to apply some sort of interpolation inside the leaves of the tree, but even then, it is just an interpolation. And anyway, you are using this for binary classification).</p>
<p>So, it doesn't really hurt them to scale the input. Just, it is useless. And since one of the quality of decision trees (less so for random forest) is that they are explainable, you loose that quality, and gain nothing in exchange (if a Decision tree says that, for example, to grant a loan, you can perform classification with <code>if age&gt;70 ⇒ no else if salary &gt; 30000 ⇒ yes else no</code>, that makes sense (you may even be right now thinking that my criteria are strange. Which is probably true, but point is, you can have an opinion on that). If you had scale the data before, the tree would have been identical, but for the scaling, but that would be <code>if age&gt;0.44 ⇒ no else if salary &gt; -0.01 ⇒ else no</code>. Which is less informative.</p>
<p>MLP on another hand, badly need some sort of scaling (not necessarily this one. But something that makes values vaguely between -1 and 1)</p>
<p>Logistic Regression, in theory shouldn't care, but in practice, iterative algorithms can fail if number condition is bad, so it is better to have all data roughly at the same scale.</p>
<p>Anyway, generally speaking, there is a reason why sklearn (or other) don't simply include data scaling in the algorithm. It is because there can't be a general rule about how to scale. Not even for a given algorithm (except that  decision trees don't care, that, you can take as a general rule).</p>
<p>For example, nothing says that the &quot;logic&quot; of your data is the same as the distribution of the dataset. You may want to map, in my last example, age between 18 and 100 linearly on -1,1, (with 0 being 59), even tho in your training dataset mean age is 35. You may decide to map not linearly between -1 and 1, but using distribution (so -1 is the min, +1 the max, 0 the median, 0.5 75% percentile, -0.5 25% percentile, etc.). And there again you can do it using the distribution of the dataset, or the distribution in population.</p>
<p>Even more, you may want to use more global scaling, scaling not each feature one by one, but along some axis (for example, using PCA), reduce feature, etc.</p>
<p>So, it doesn't really make sense to try to make a generic pipeline, for any kind of data, and any kind of algorithm, with a predefined scaler. If it did, sklearn would already do it.</p>
","1","Answer"
"79283428","79274150","<p>There is a known issue with PyTorch 2.5.</p>
<p>You can read more about it <a href=""https://github.com/pytorch/pytorch/issues/142344"" rel=""nofollow noreferrer"">here</a>.</p>
<p>One solution is to download the source code, modify the two files as described by <a href=""https://github.com/pytorch/pytorch/issues/142344#issuecomment-2541767272"" rel=""nofollow noreferrer"">malfet</a>, and then compile the source code. This issue is expected to be resolved in PyTorch 2.6.</p>
<p>However, the simplest solution I found was to install the nightly versions of PyTorch:</p>
<p><code>pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu</code></p>
<p>Best regards,</p>
<p>--KC</p>
","1","Answer"
"79285615","79281350","<p>Don't mix the dataframes df/df2/t for working with the training set and only encode once the training set. The code below will not lead to any Python error:</p>
<pre><code>df_train = pd.read_csv('train.csv',header=0)
le = preprocessing.LabelEncoder()
cols = ['County', 'City', 'State', 'ZIP Code', 'Model Year', 'Make', 'Model', 
        'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']

for col in cols:
    le.fit(df_train[col])
    df_train[col] = le.transform(df_train[col]) 
</code></pre>
<p>But as scikit-learn doc says about LabelEncoder &quot;This transformer should be used to encode target values, i.e. y, and not the input X.&quot;
Instead you can use OrdinalEncoder() which will gives the same result as above. It will also make the code much shorter for the training set as you will need only 1 line:</p>
<pre><code>df_train[cols] = oe.fit_transform(df_train[cols]) 
</code></pre>
<p>with</p>
<pre><code>oe = preprocessing.OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1)
</code></pre>
<p>Notice you will have to do the same for the test set BUT use the encoder fitted on the training set and use it to encode the test set (to avoid information leakage from test to train set) i.e. just use oe.transform.</p>
<p>I added the 2 parameters when creating oe so that it puts -1 each time it finds an unknown category in the test set (if not you would get an error).</p>
<p>See the doc for OrdinalEnncoder at <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html</a></p>
<p>PS: before doing the encoding you should deal with the missing values (impute/delete/...).</p>
","1","Answer"
"79285671","79283938","<p>I ran into an issue while setting up Vertex AI Model Monitoring using the aiplatform SDK, specifically when configuring BatchPredictionJob.create(). The challenge was understanding how to pass model_monitoring_objective_config and model_monitoring_alert_config, as the SDK documentation lacked clear examples.</p>
<p><strong>Solution:</strong> <br>
To resolve this, I used the following setup:</p>
<ol>
<li><p>Import the Required Libraries:</p>
<p><code>from google.cloud import aiplatform</code></p>
</li>
<li><p>Define Model Monitoring Configurations:</p>
<pre><code># Define skew detection configuration
objective_config = aiplatform.model_monitoring.ObjectiveConfig(
skew_detection_config=aiplatform.model_monitoring.SkewDetectionConfig(
    skew_thresholds={&quot;feature1&quot;: 0.3},
    data_source=&quot;gs://my-data-classification/train_data_new.csv&quot;,
    target_field=&quot;type&quot;,
    data_format=&quot;csv&quot;,
    )
 )

 # Define email alert configuration
 alert_config = aiplatform.model_monitoring.EmailAlertConfig(
 user_emails=[&quot;rajm893@gmail.com&quot;]
 )
</code></pre>
</li>
<li><p>Create the Batch Prediction Job:</p>
</li>
</ol>
<pre><code>batch_job = aiplatform.BatchPredictionJob.create(
    job_display_name=&quot;vertex_monitoring_job&quot;,
    model_name=&quot;projects/your_project_id/models/your_model_id&quot;,
    instances_format=&quot;jsonl&quot;,
    predictions_format=&quot;jsonl&quot;,
    gcs_source=[&quot;gs://your-input-data/input.jsonl&quot;],
    gcs_destination_prefix=&quot;gs://your-output-data/&quot;,
    machine_type=&quot;n1-standard-4&quot;,
    model_monitoring_objective_config=objective_config,
    model_monitoring_alert_config=alert_config,
)
print(f&quot;Batch job created: {batch_job.resource_name}&quot;)
</code></pre>
<p>This resolved the issue, and the job was successfully created with model monitoring enabled.</p>
<p>The main challenge was understanding that the model_monitoring_objective_config and model_monitoring_alert_config need to be constructed using aiplatform.model_monitoring classes. I hope this helps anyone facing similar confusion!</p>
<p>For more details on creating a complete batch prediction pipeline with Vertex AI, you can refer to this <a href=""https://medium.com/@rajmudigonda893/building-a-batch-prediction-pipeline-on-google-cloud-with-model-monitoring-9535e8d4ba81"" rel=""nofollow noreferrer"">blog post</a> I wrote.</p>
","0","Answer"
"79287918","79287799","<p>If you have tried GPT for the same task and you are still facing same issue. Then no other Pre-trained model is going to work. Because GPT right now now is far superior then other Pre-trained model.</p>
<p>And most of the Bert based NER model are not fine tuned for Indian based dataset. And that is the reason you see incorrect result.</p>
<p>And the <strong>2nd reason</strong> is insufficient context. The example that you provided has not sufficient context. Not even human can tell what does your example is about. And if you try to make your example meaningful then passing modified example to already used model will give better result. And having correct context is prerequisite because most of these deep learning model do prediction based on context word.</p>
<hr />
<p>I am adding one code which might work for you because it was fine tune on Indian dataset.</p>
<p><strong>Code:</strong></p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# Checking if GPU is available
device = 0 if torch.cuda.is_available() else -1  # GPU: 0, CPU: -1

# Load pre-trained tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(&quot;Venkatesh4342/NER-Indian-xlm-roberta&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;Venkatesh4342/NER-Indian-xlm-roberta&quot;)

# Initiating NER Model
ner_pipeline = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, device=device)

# Example data
input_text = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;

# Calling NER Model
results = ner_pipeline(input_text)

# Display the output
for entity in results:
    print(f&quot;Entity: {entity['word']}, Label: {entity['entity']}, Confidence: {entity['score']:.2f}&quot;)
</code></pre>
<p><strong>Final Output:</strong></p>
<pre><code>Entity: ▁U, Label: organization, Confidence: 0.99
Entity: PI, Label: organization, Confidence: 0.99
Entity: ▁DR, Label: organization, Confidence: 0.99
Entity: ▁BEN, Label: location, Confidence: 0.76
Entity: GAL, Label: location, Confidence: 0.83
Entity: ORE, Label: location, Confidence: 0.79
Entity: ▁MEDI, Label: organization, Confidence: 1.00
Entity: CAL, Label: organization, Confidence: 0.99
Entity: ▁LTD, Label: organization, Confidence: 1.00
Entity: ▁HD, Label: organization, Confidence: 0.99
Entity: FC, Label: organization, Confidence: 0.98
</code></pre>
<p>The final output is little better and after post processing logic you can combine the broken word together as well.</p>
<p>Final note: I would suggest to go for fine tuning a NER based model on your data. And try to have meaningful context</p>
","1","Answer"
"79290863","79226790","<p>I fixed it somehow like, in the <code>chat_history</code> that I passed to <code>get_chat_message_content()</code>, it will have the System Prompt as it's first message. There i added the session_id at the end of the prompt mentioning it like <code>My Session ID: {session_id}</code>. Then I added the <code>session_id</code> parameter to the the methods which uses the session id in the <code>MovesPlugin()</code> like,</p>
<pre class=""lang-py prettyprint-override""><code>def generate_query(self, session_id, input_query):
    # generate query logic
</code></pre>
<p>When the kernel calls this method, automatically it passes the <code>session_id</code> which i already given in the <strong>System Prompt</strong> will be automatically passed to it and thus it solves the issue.</p>
<p><strong>Note:</strong> This is a temporary solution to fix this issue. if anyone has a better solution to pass it directly as an argument to the plugin, you are most welcome.</p>
","0","Answer"
"79291260","79290968","<p>Scikit-learn version 1.6.0 modified the API around its &quot;tags&quot;, and that's the cause of this error.  XGBoost made the necessary changes in version 2.1.4 (specifically in <a href=""https://github.com/dmlc/xgboost/pull/11021"" rel=""nofollow noreferrer"">PR11021</a>).  In sklearn 1.6.1, the error was downgraded to a warning (to be returned to an error in 1.7). So you should be OK with any of:</p>
<ol>
<li><code>xgboost &gt;=2.1.4</code></li>
<li><code>sklearn &gt;=1.6.1,&lt;1.7</code>, and expect <code>DeprecationWarning</code>s</li>
<li><code>sklearn &lt;1.6</code></li>
</ol>
<p>See also sklearn <a href=""https://github.com/scikit-learn/scikit-learn/issues/30479"" rel=""nofollow noreferrer"">Issue#30479</a> and <a href=""https://scikit-learn.org/stable/whats_new/v1.6.html#sklearn-utils"" rel=""nofollow noreferrer"">1.6.1 release notes</a>, and <a href=""https://github.com/dmlc/xgboost/releases/tag/v2.1.4"" rel=""nofollow noreferrer"">xgboost 2.1.4 release notes</a>.</p>
","31","Answer"
"79293600","79291357","<p>You're not wrong in thinking that your model is associating the yellow/red color combination to presence of this object in the image. From your dataset, it seems you have already added some augmentations to your train data (rotations, brightness etc.). Your model may work well with positive examples but it also needs negative examples. You have to train your model to understand what is NOT this object - similar to the yellow box in your 3rd picture. You don't have to specifically label them as negative example, just having such objects in your image in the background or near the object that you want to detect will suffice. When trained, the model will optimize to not detect these as the object of interest.</p>
","2","Answer"
"79296062","79293139","<p>As you said you are using DeiT model and the learning rate for training the model like Deit is relatively high which leads model to converge to a sub optimal solution and that is why your model is favouring only one class.You try to reduce the learning rate(for ex:1-e4)and check now it may give you altogether solution.</p>
","0","Answer"
"79298322","79288128","<p>I suggest <a href=""https://cloud.google.com/ai-platform/docs/getting-started-keras#developing_the_keras_model_from_scratch"" rel=""nofollow noreferrer"">downloading</a> the model to local storage (<code>/tmp</code>) and load it once at the cold start. Here’s an overview of the code:</p>
<pre><code>import os
import tempfile # add these imports after [from PIL import Image]

model = None
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

def get_model():
    global model
    if model is None: 
        local_model_path = os.path.join(tmpfile.gettmpdir(), &quot;cifar10_model.keras&quot;) # download the model only once to /tmp directory
        if not os.path.exists(local_model_path):
            print(&quot;Downloading model...&quot;)
            tf.io.gfile.copy(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;, local_model_path, overwrite=True)
        print(&quot;Loading model...&quot;)
        model = load_model(local_model_path)
    return model

def preprocess_image(image_file):
    img = Image.open(io.BytesIO(image_file.read()))
    img = img.resize((32, 32))
    img = np.array(img) / 255.0
    img = img.reshape(1, 32, 32, 3)
    return img

@functions_framework.http
def predict(request):
    global model
    model = get_model() # get or load the model
    image = preprocess_image(request.files['image_file'])
    print(&quot;Image shape:&quot;, image.shape)
    prediction = model.predict(image)
    print(&quot;Prediction:&quot;, prediction) 
    predicted_class = class_names[np.argmax(prediction)]
    return f&quot;Predicted class: {predicted_class}&quot;
</code></pre>
<p>Hope this works on your end!</p>
","1","Answer"
"79298500","79269787","<p>As a first step to investigate this error, please check if this error reproduces in your local environment by installing xgboost locally.</p>
<p>If this error reproduces also locally, you can eliminate amazon-web-services / amazon-sagemaker tags from this post.</p>
<p>It is also recommended to add some information how you are calling the xgboost APIs to load the model, and how your model files are structured under the extracted model directory, as the error may be happening because of how the model is structured, and how to load it.</p>
","0","Answer"
"79299610","79297188","<p>If you want them all in one figure and horizontally, you can use matplotlib's <code>subplots()</code> method</p>
<pre class=""lang-py prettyprint-override""><code>fig, ax = plt.subplots(1, 10, figsize=(20, 3))

for i in range(10):
    ax[i].imshow(predictions[i].astype(&quot;float32&quot;), cmap=&quot;gray_r&quot;)

plt.show()
</code></pre>
","0","Answer"
"79299666","79280552","<p>This is just not possible because a dense layer has a fixed number of weights. When you call a dense layer after flattening, it is effectively doing</p>
<p><code>w_0 * x_0 + w_1 * x_1 + w_2 * x_2 + .... + w_n-1 * x_n-1 + bias</code> where the <code>w</code>s are the weights and the <code>x</code>s are the flattened input feature values.</p>
<p>So if due to your unknown dimension, <code>n</code> can't be known ahead of time, then it's just not possible for the network to be configured with the appropriate number of weights.</p>
<p>Even if you knew the &quot;max time&quot; and want to preallocate the number of weights in the network to support that, it would likely suffer from two problems</p>
<ol>
<li>the network gets too large</li>
<li>the network overfits because it is treating each pixel in each time step as a completely separate feature which is going to bloat up the dimensionality with no real benefits</li>
</ol>
<p>So the alternatives to capture the time axis would be to either make it a time-series network like LSTMs or recurrent neural networks, or a 3D convolution network which relies on pooling across time.</p>
","1","Answer"
"79299903","79299410","<p>Some observations about your optimization function:</p>
<ol>
<li>Why are you dividing by N? You're making your gradient smaller than necessary, and you're not applying that scaling to the second term in your gradient, changing the relationship between the components of your gradient (making the residual-driven term much smaller than the coefficient-driven term).</li>
<li>Why are you calculating your <code>B0</code> and <code>B1</code> gradients separately? You can take the gradient in terms of all of <code>B</code> using all of <code>X</code> instead of fragmenting your solution. You've applied regularization to <code>B[1:]</code> but not to <code>B[0]</code>.</li>
</ol>
<p>Here is an amended solution for you, without dividing the residual-driven term by <code>n</code>, and adding regularization to <code>B[0]</code> as well:</p>
<pre><code>def ridge_regression_gradient_descent(X, y, lambda_reg, alpha, num_iterations):
    n, p = X.shape  
    B = np.zeros(p)  
    # Gradient descent loop
    for _ in range(num_iterations):
        y_pred = X.dot(B).reshape(-1, 1)  
        gradient_B0 = -np.sum(y - y_pred)  + lambda_reg * B[0]
        gradient_B = -(X[:, 1:].T @ (y - y_pred)) + lambda_reg * B[1:].reshape(-1, 1)  # Gradients for B1 to Bp   

        B[0] -= alpha * gradient_B0  
        B[1:] -= alpha * gradient_B.reshape(-1)  
    return B


B = ridge_regression_gradient_descent(X_train, y, lambda_reg, alpha=.001, num_iterations=10000)
print(B)

&gt; [ 0.53250528 -0.21478985 -0.15051757]
</code></pre>
<p>Indeed you can check the output of this function by calculating B explicitly as you've described.</p>
<pre><code>beta = np.linalg.inv(X_train.T@X_train+np.identity(X_train.shape[1])*lambda_reg)@(X_train.T@y)

print(beta)

&gt; [[ 0.53256638]
 [-0.21485239]
 [-0.15058472]]
</code></pre>
<p>Here is a separate solution calculating the gradient in terms of all of <code>B</code> using all of <code>X</code>, it accomplishes the same thing, but it is more straightforward and efficient:</p>
<pre><code>def descent(X,y,lambda_reg,alpha,num_iterations):

    B = np.zeros((X.shape[1],1))
    
    for _ in range(num_iterations):

        grad = (X.T@((X@B).reshape(y.shape) - y) + lambda_reg*B)

        B = B - alpha*grad

    return B


B = descent(X_train, y, lambda_reg, alpha=.1, num_iterations=10000)
print(B)

&gt; [[ 0.53256638]
 [-0.21485239]
 [-0.15058472]]
</code></pre>
","0","Answer"
"79300183","79300055","<p>From <code>MaxVit</code> Args parameters:</p>
<blockquote>
<p>block_channels (List[int]): Number of channels in each block. <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L575"" rel=""nofollow noreferrer"">Source</a></p>
</blockquote>
<p>The classifier <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L823C5-L833C6"" rel=""nofollow noreferrer"">Source</a></p>
<pre><code>self.classifier = nn.Sequential(
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.LayerNorm(block_channels[-1]),
        nn.Linear(block_channels[-1], block_channels[-1]),
        nn.Tanh(),
        nn.Linear(block_channels[-1], num_classes, bias=False),
    )
</code></pre>
<p>Since <code>block_channels</code> is a list,`block_channels[-1] returns the last item in the list, 512 in the following case <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L823C5-L833C6https://"" rel=""nofollow noreferrer"">Source</a></p>
<pre><code>return _maxvit(
    stem_channels=64,
    block_channels=[64, 128, 256, 512],
    block_layers=[2, 2, 5, 2],
    head_dim=32,
    stochastic_depth_prob=0.2,
    partition_size=7,
    weights=weights,
    progress=progress,
    **kwargs,
)
</code></pre>
","0","Answer"
"79300277","79288128","<p>Even though the previous suggestion worked, I was invested into writing as little code as possible. Therefore, I came up with this solution:</p>
<pre><code>import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

def preprocess_image(image_file):
    img = Image.open(io.BytesIO(image_file.read()))
    img = img.resize((32, 32))
    img = np.array(img) / 255.0
    img = img.reshape(1, 32, 32, 3)
    return img

@functions_framework.http
def predict(request):
    model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)
    image = preprocess_image(request.files['image_file'])
    prediction = model.predict(image)
    predicted_class = class_names[np.argmax(prediction)]
    return f&quot;Predicted class: {predicted_class}&quot;
</code></pre>
<p>It seems that the issue is that the model must be instantiated inside the function that processes the request. I would have never guessed!</p>
","0","Answer"
"79300623","79290968","<p>I also had a similiar error,Xg boost too. But what the guy above me said, uninstall and reinstalling a lower version of sklearn (i used version 1.5.2) fixed this issue for me!</p>
<pre><code>!pip uninstall -y scikit-learn
!pip install scikit-learn==1.5.2
</code></pre>
","16","Answer"
"79307072","79306951","<p>You didn't include an example braille.jpg image.
I'm going to assume your captured images resemble
<a href=""https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.66DL0p1XNMZK3c2W_U1QcQHaEK%26pid%3DApi&amp;f=1&amp;ipt=36a083790fe93b040a04d0176fa888e08d043cb4037f9e1404697f010577350e&amp;ipo=images"" rel=""nofollow noreferrer"">this one</a>.</p>
<p>This looks like a relatively simple OCR task,
in that glyphs are mono-spaced, don't have descenders,
and there's just 2^6 of them.</p>
<ol>
<li>Either assume the image is not skewed, or else deskew it.</li>
<li>Binarize the image, perhaps using
<a href=""https://en.wikipedia.org/wiki/Otsu%27s_method"" rel=""nofollow noreferrer"">Otsu</a>.</li>
<li>Vertical whitespace: identify baselines and cell size.</li>
<li>Horizontal whitespace: identify word and cell boundaries.</li>
<li>Decode to ASCII.</li>
</ol>
<p>Steps 1 &amp; 3 are related -- at some rotation angle we can
scan very long horizontal rasters finding only blank pixels.
(There may be some salt and pepper noise that needs to be ignored.)
In numpy this is simply a matter of summing the values in a raster.
Some use cases may first impose a bounding box mask before summing.</p>
<p>At this point you have a pair of vertical measurements for each
line of text: how many pixels tall the cell is, and how many pixels
between lines of text. We expect same font size for many lines.
Apply some technique to filter outliers, perhaps a simple median filter.</p>
<p>Using that cell height you're already in a good position to
predict cell width. Do horizontal raster scans to identify
cell width, the space between cells, and individual horizontal offset
for each character. This is slightly complicated by the fact that
most cells have fewer than six marks, but a median- or quantile- filter
should still suffice. And it's easy to evaluate how well your
algorithm is doing, by drawing horizontal and vertical lines
on the scanned image and seeing how well the estimates line up.</p>
<p>With a grid projected on top of that mono-spaced text,
it should be straightforward recover the six bits and turn them to ASCII.
Identifying specific shapes, such as circles in the image,
should not be necessary.
Carve up each cell into bounding boxes for the six dot positions,
and just sum the binarized pixels within each box.
There will be some noise in this sum;
comparing it with ground truth on some example images
will let you pick an appropriate threshold sum for a classifier.</p>
","1","Answer"
"79309143","79309133","<p>When you create <code>train_dataset</code>, it's already batched into <code>size 32</code> because you specified <code>batch_size=32</code> in <code>image_dataset_from_directory()</code></p>
<blockquote>
<p>When you call <code>.batch()</code> again on an already batched dataset, it tries
to batch the batches, which effectively creates batches of batches</p>
</blockquote>
<h2>Code Correction</h2>
<pre><code>train_dataset = image_dataset_from_directory(
    dataset_dir,
    image_size=image_size,
    batch_size=batch_size,
    label_mode=&quot;binary&quot;,
    validation_split=0.2,
    subset=&quot;training&quot;,
    seed=123,
)


train_dataset = train_dataset.shuffle(1000).prefetch(buffer_size=AUTOTUNE)
</code></pre>
","0","Answer"
"79310121","79278625","<p>If you do not care about the score itself, but do care about who is the best in class, you should use ranker + binary classes as a target. Create a column &quot;Is_first&quot;, and set it to 1 for the best students in each class and to 0 for the rest, then fit the ranker (not a classifier).</p>
<p>In this case the result is going to be be a value between 1 and 0, and model will consider all the students in class to predict who is the best. Not sure if result values will sum up to 1 for all the students in class (but you can write some code to normalize values if they do not sum up to 1).</p>
","2","Answer"
"79310904","79288128","<p>The thing is the your function stuck when trying to predict and that issue is not in your code but with the how google cloud environment works. Generally cloud functions have certain limits like memory, time, specifically where and how files are loaded and processed and etc....</p>
<p>Try out these steps:</p>
<ol>
<li>Download the model file to a temporary folder inside the function before using it.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from google.cloud import storage
import tensorflow as tf

def load_model_from_cloud(bucket_name, model_file):
    # Connect to source (in your case it's cloud storage bucket)
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(model_file)

    # Download the model to a temporary folder or any other you like
    local_path = f&quot;/tmp/{model_file}&quot;
    blob.download_to_filename(local_path)

    return tf.keras.models.load_model(local_path)

model = load_model_from_cloud(&quot;&lt;your-bucket-name&gt;&quot;, &quot;cifar10_model.keras&quot;)
</code></pre>
<ol start=""2"">
<li>Add debugging (<strong>Extra Logs</strong>) for identifying what's exactly going on under the hood. That will give more clarity about your problem.</li>
</ol>
","0","Answer"
"79311834","79295972","<p>you can collect data within your app, transmit it securely to your server, and periodically retrain your model using this aggregated data. The updated model can then be deployed to users through app updates or by downloading the new model file at runtime. Additionally, Apple's Personalization APIs introduced in iOS 13 allow for limited on-device model personalization, enabling adjustments based on individual user interactions without full model retraining.</p>
","1","Answer"
"79311862","79306951","<p>Here's a PoC on translating Braille to text from a well defined image. Real images can be more complicated specially for hand written Braille since dot/cell spacing is not constant. Also, this image is of uncontracted type (Grade I) so translating <a href=""https://www.seewritehear.com/learn/an-explanation-of-braille-formats/"" rel=""nofollow noreferrer"">contracted Braille</a> (Grade II) could require a <a href=""https://www.brailleauthority.org/ueb/symbols_list.pdf"" rel=""nofollow noreferrer"">significant mapping dictionary</a> and a more elaborated algorithm to identify cell indexes.</p>
<p>Key points of the algorithm:</p>
<ul>
<li>Extract and sort coordinates of detected keypoints.</li>
<li>Find x,y differences between contiguous dots. X negative values mean second/third rows in a cell and the start of a line.
e.g.: previous point <code>p0=(520,69)</code>, current point <code>p1=(69, 140)</code>.<br />
<code>xydiff = (-451, 71)</code>.
xdiff is negative, ydiff is greater than vertical cell size --&gt; current dot is starting a line.</li>
<li>Find cell parameters: Min/max x/y coord., x min cell spacing, y min cell spacing.</li>
<li>Group coordinates by line into lists (group_by_lines()).</li>
<li>Find dot indexes on each cell in the line. <code>. . --&gt; (1,4) --&gt; 'c'</code></li>
<li>Map the tuple to a text character.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import sys
import cv2
import numpy as np

cell_map = {
    (1,): 'a', (1,2): 'b', (1,4): 'c', (1,4,5): 'd', (1,5): 'e',
    (1,2,4): 'f', (1,2,4,5): 'g', (1,2,5): 'h', (2,4): 'i', (2,4,5): 'j',
    (1,3): 'k', (1,2,3): 'l', (1,3,4): 'm', (1,3,4,5): 'n', (1,3,5): 'o',
    (1,2,3,4): 'p', (1,2,3,4,5): 'q', (1,2,3,5): 'r', (2,3,4): 's', (2,3,4,5): 't',
    (1,3,6): 'u', (1,2,3,6): 'v', (2,4,5,6): 'w', (1,3,4,6): 'x', (1,3,4,5,6): 'y', (1,3,5,6): 'z'
    }

def get_build_detector_params():
    # Set up SimpleBlobDetector
    params = cv2.SimpleBlobDetector_Params()
# Filter by area (size of the blob)
    params.filterByArea = True
    params.minArea = 10 # Adjust based on dot size
    params.maxArea = 1000 # Filter by circularity
    params.filterByCircularity = True
    params.minCircularity = 0.9 # Adjust for shape of the dots
# Filter by convexity
    params.filterByConvexity = False
    params.minConvexity = 0.7
# Filter by inertia (roundness)
    params.filterByInertia = True
    params.minInertiaRatio = 0.95
    return params

def show_detection(image, detected_lines, xcell, xsep, xmin, ymax):
    &quot;&quot;&quot;Help to visually debug if lines are correctly detected since dots would be colored by line.
    Black dots represent not correctly detected cells/lines.
    Color will repeat every for lines.&quot;&quot;&quot;
    
    colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (178,102,255)]
    while len(colors) &lt; len(detected_lines):
        colors.extend(colors)
    # Draw detected blobs as red circles
    output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
    
#    x = int(xmin)
#    for i in range(1,7):
#        output_image = cv2.line(image, ( x * i + int(xsep), 50), (x * i + int(xsep), ymax), (0, 255, 0), thickness=2)

    for i, line in enumerate(detected_lines):
        output_image = cv2.drawKeypoints(output_image, line, np.array([]), colors[i], cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    print(&quot;output image&quot;)
    cv2.imshow(&quot;outputimage&quot;, output_image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

def get_cell_parameters(blob_coords, xydiff):
    &quot;&quot;&quot;Parameters to help find cells from detected coordinates.
    &quot;&quot;&quot;
    xuniq = np.unique(np.array([round(xy[0]) for xy in xydiff if xy[0] &gt; 1]))
    yuniq = np.unique(np.array([round(xy[1]) for xy in xydiff if xy[1] &gt; 1]))
    # x separation between dots in a cell
    xcell = round(xuniq.min())
    # y separation between dots in a cell
    ycell = round(yuniq.min())
    xcoords = [xy[0] for xy in blob_coords if xy[0] &gt; 1]
    ycoords = [xy[1] for xy in blob_coords if xy[1] &gt; 1]
    # minimum x in the whole image
    xmin = round(np.array(xcoords).min())
    # max x in the whole image. Represents last dot in a line.
    xmax = round(np.array(xcoords).max())
    # x separation between cells
    xsep = np.unique(np.array([round(xy[0]) for xy in xydiff if xy[0] &gt; xcell + xcell / 2])).min()
    # minimum y in the whole image
    ymin = round(np.array(ycoords).min())
    return ycell, ymin, xmin, xcell, xsep, xmax

def group_by_lines(kp_map, blob_coords, xydiff, ycell):
    &quot;&quot;&quot;Group coordinates by lines.&quot;&quot;&quot;
    
    detected_lines = [[kp_map[blob_coords[0][0], blob_coords[0][1]]]]
    print(f&quot;new line at: {int(blob_coords[0][0])},{int(blob_coords[0][1])}&quot;)
# split coordinates by lines
    for i, d in enumerate(xydiff):
        curr_pt = blob_coords[i + 1]
    #print(d, curr_pt, blob_coords[i+1], f&quot;xdiff {d}, ydiff: {blob_coords[i+1][1] - blob_coords[i][1]}&quot;)
        if d[0] &lt; 0 and d[1] &gt;= ycell * 3:
            print(f&quot;new line at: {curr_pt}, curr xdiff: {d}, {ycell * 3}, previous: {blob_coords[i]}&quot;)
            detected_lines.append([kp_map[curr_pt[0], curr_pt[1]]])
        else:
            detected_lines[-1].append(kp_map[curr_pt[0], curr_pt[1]])
    
    return detected_lines

def char_to_tuple(ycell, offset, cur_char):
    &quot;&quot;&quot;Return a sorted tuple representing dot indexes in the cell.
    The tuple should map to a text character in cell_map dict.
    Indexes are
    1 4
    2 5
    3 6
    
    Cell    Indexes       Text
    .
    .
    . . --&gt; (1,2,3,6) --&gt; 'v'
    
    &quot;&quot;&quot;
    cell = []
    x1 = np.array([cc[0] for cc in cur_char]).min() + offset
    y1 = np.array([cc[1] for cc in cur_char]).min() + offset
    y2 = y1 + ycell + offset
    cell_idx = None
    for cc in cur_char:
        if cc[0] &lt;= x1:
            if cc[1] &lt;= y1:
                cell_idx = 1
            elif cc[1] &gt; y1 and cc[1] &lt;= y2:
                cell_idx = 2
            elif cc[1] &gt;= y2:
                cell_idx = 3
        elif cc[0] &gt;= x1:
            if cc[1] &lt;= y1:
                cell_idx = 4
            elif cc[1] &gt; y1 and cc[1] &lt;= y2:
                cell_idx = 5
            elif cc[1] &gt;= y2:
                cell_idx = 6 
        #print(&quot;       &quot;, cell_idx, cc, x1, y1)
        if cell_idx is None or cell_idx in cell:
            print(f&quot;WARNING. cell_idx duplicate or not found: {cell_idx}, {cc}&quot;, x1, y1, y2)
        if len(cell) == 0 and cell_idx == 3:
            print(&quot;ERROR. First cell_idx can't be 3&quot;)
        cell.append(cell_idx)
        cell_idx = None
    return tuple(sorted(cell))

text = ''
count = 0
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

params = get_build_detector_params()
# Create a detector with the parameters
detector = cv2.SimpleBlobDetector_create(params)
# Detect blobs
keypoints = detector.detect(image)
# map of keypoints coordinates to keypoints
kp_map = { (round(kp.pt[0], 2), round(kp.pt[1], 2)): kp for kp in keypoints}

# all dots coordinates, sorted to help find lines.
blob_coords = np.array(list(kp_map.keys()))
blob_coords = blob_coords[np.lexsort((blob_coords[:,0], blob_coords[:,1]))]

# x,y differences between contiguous dots. Negative values mean second/third rows in a cell and the start of a line.
# e.g.: previous point p0=(520,69), current point =(69, 140). xydiff = (-451, 71).
# xdiff is negative, ydiff is greater than vertical cell size --&gt; current dot is starting a line.
xydiff = np.array([ (kp[0] - blob_coords[i-1][0], kp[1] - blob_coords[i-1][1]) for i,kp in enumerate(blob_coords) if i &gt; 0 ])

print(f&quot;blob_coords: {len(blob_coords)}, xydiff: {len(xydiff)}&quot;)
ycell, ymin, xmin, xcell, xsep, xmax = get_cell_parameters(blob_coords, xydiff)

#print(xuniq)
#print(yuniq)
print(f&quot;x params: xcell {xcell}, xmin {xmin}, xsep {xsep}, xmax {xmax}&quot;)
print(f&quot;y params: ycell {ycell}, ymin {ymin}&quot;)
print(f&quot;max cells per line: {round((xmax)/(xcell + xsep))}&quot;)
offset = 2

# List of list of cells by line
detected_lines = group_by_lines(kp_map, blob_coords, xydiff, ycell)
p0 = 0
p1 = len(detected_lines[0])
# process each line
for j, line in enumerate(detected_lines):
    if j &gt; 0:
        p0 = p0 + len(detected_lines[j - 1])
        p1 = p0 + len(detected_lines[j])
    print(f&quot;\nSTARTING line: {j}, p0: {p0}, p1: {p1} {len(xydiff[p0:p1])}/{len(line)}&quot;)

    # coordinates of a line
    cur_coor = blob_coords[p0:p1]
    xchar1 = xmin
    xchar2 = xmin + xcell + xsep
    while xchar1 &lt;= xmax:
        #cur_char = np.array([xy for xy in cur_coor if xy[0] &gt;= xchar1 and xy[0] &lt;= xchar2])
        cur_char =  cur_coor[ (xchar1 &lt;= cur_coor[:,0]) &amp; (cur_coor[:,0] &lt;= xchar2) ]
        if len(cur_char) == 0 or cur_char[0][0] &gt; xchar2:
            # No coordinates found at the x-range. It's a space so shift x range and move to next cell.
            print(f&quot;WARNING: No char found. adding space to text. {xchar1}, {xchar2}&quot;)
            text += ' '
            xchar1 += xcell + xsep
            xchar2 = xchar1 + xcell + xsep
            continue
        cur_char = cur_char[np.lexsort((cur_char[:,0], ))]
        # build a cell indexes tuple to finally decode the cell to text
        cell = char_to_tuple(ycell, offset, cur_char)
        
        if cell in cell_map:
            print(cell_map[cell], ': ', cell )
            text += cell_map[cell]
        else:
            print('ERROR. Cell to text mapping not found: ', cell, cur_char)
            text += '?'
            
        xchar1 += xcell + xsep
        xchar2 = xchar1 + xcell + xsep
        if xchar2 &gt; xmax:
            xchar2 = xmax

print(f&quot;\nFound text:\n'{text}'&quot;)
show_detection(image, detected_lines, xcell, xsep, xmin, 400)
sys.exit()
</code></pre>
<p>Text from below image (bug: extra space at the end).</p>
<p><code>'abcdefghijklmnopqrstu vwxyz '</code></p>
<p><a href=""https://i.sstatic.net/IOfuJgWk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IOfuJgWk.jpg"" alt=""Braille alphabet"" /></a></p>
","1","Answer"
"79313580","79290968","<p>Scikit-learn version 1.5.2 does not solve the problem. Try downgrading further. What worked for me was 1.3.1.</p>
","9","Answer"
"79314772","79313854","<p>Using Yolo tracker I was able to solve my solution.</p>
<p>Adding snippet here</p>
<p>replace these lines:</p>
<pre><code>detections_model = coco_model(frame)[0]
</code></pre>
<p>To this:</p>
<pre><code>detections_model = coco_model.track(frame, persist=True)
track_ids = detections_model[0].boxes.id.int().cpu().tolist()
</code></pre>
","1","Answer"
"79314782","79309306","<p>pip install transformers==4.46.0 tokenizers==0.20.3</p>
","0","Answer"
"79315535","79314811","<p>The problem is in your test code.</p>
<pre><code>IEnumerable&lt;double&gt; vector = Enumerable.Range(0, 100).Select(n =&gt; new Random().NextDouble());
var softMaxVec = SoftMax(vector);
Console.WriteLine(softMaxVec.Sum());
</code></pre>
<p>Due to deferred execution, if you iterate through the same vector multiple times, you get different numbers. For example</p>
<pre><code>Random random = new Random();
IEnumerable&lt;double&gt; randomNumbers = Enumerable.Range(0, 3).Select(_ =&gt; random.NextDouble());
foreach (var number in randomNumbers)
{
     Console.WriteLine(number);
}
Console.WriteLine('\n')
foreach (var number in randomNumbers)
{
     Console.WriteLine(number);
}
</code></pre>
<p>produces</p>
<pre><code>0.63632565114476
0.440262559540692
0.254312404549826

0.566282945483123
0.348965338128137
0.983789030920616
</code></pre>
<p>And in your softmax implementation you iterate through the vector multiple times and each time it is effectively using different random number vectors (one vector for exponentiation, one vector for summation).</p>
<p>Just test it with a hardcoded test case like</p>
<pre><code>SoftMax(new List&lt;double&gt; { 0.443426089567796, 0.234234, 0.343454, 0.3 });
</code></pre>
<p>And it shows that your softmax works fine. In general to achieve test stability it's not a good idea to write test cases that use random inputs anyway.</p>
","3","Answer"
"79317481","79316482","<p>In the end, I just used a workaround using <code>python3 -c 'import torch;print(torch.utils.cmake_prefix_path)'</code>, linking manually seems to be not possible.</p>
","0","Answer"
"79317853","79317350","<p>In general you need to scale the input features when using gradient descent (<code>SGDRegressor</code> uses gradient descent, whilst <code>LinearRegression</code> does <a href=""https://stackoverflow.com/a/78104023/21896093"">not</a>).</p>
<p>You only have a single input feature, so I thought it would be okay to skip the scaling step. However, I think there is some instability arising from the large values of X, and I found that you need to either divide it down by 100, or use <code>StandardScaler</code> as per the example below.</p>
<p><a href=""https://i.sstatic.net/Znh8fVmS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Znh8fVmS.png"" alt=""enter image description here"" /></a></p>
<p>In short, I replaced <code>SGDRegressor()</code> with a pipeline that first scales X before fitting the regressor:</p>
<pre class=""lang-py prettyprint-override""><code>sgd_reg = make_pipeline(
    StandardScaler(),
    SGDRegressor()
)
</code></pre>
<hr />
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from matplotlib import pyplot as plt

#
# Load and visualise data
#

#Data from
# https://github.com/tidyverse/ggplot2/blob/main/data-raw/economics.csv
df = pd.read_csv('economics.csv')
display(df)

x_name = 'pce'
y_name = 'psavert'

#Visualise data
f, ax = plt.subplots(figsize=(5, 3), layout='tight')

df.plot.scatter(
    x=x_name, y=y_name,
    s=10, edgecolor='none', alpha=0.5, c='dodgerblue',
    label='data', ax=ax
)

#
# Fit SGDRegressor
#
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDRegressor

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

x = df[[x_name]].to_numpy()
y = df[y_name].to_numpy()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

#Compose a pipeline that first standardizes X before fitting
sgd_reg = make_pipeline(
    StandardScaler(),
    SGDRegressor()
)
sgd_reg.fit(x_train, y_train)

#Plot results
ax.plot(x_train, sgd_reg.predict(x_train), c='tab:red', linewidth=1, label='linear fit')

ax.legend()
ax.set(xlabel='PCE (billions)', ylabel='saving rate', title='Fit using SGDRegressor')
ax.spines[['top', 'right']].set_visible(False)
</code></pre>
","0","Answer"
"79318132","79316664","<p>I think what you miss here is the <em>backpropagation</em> step of the algorithm:</p>
<p>When B is given a good score, then W will receive a <em>bad score</em> during that backpropagation step: W's simulation count will increment, but the number of wins will not (when B was played), so that the ratio of wins over simulations will become low for W.</p>
<p>This bad score of W will make it less likely to be chosen in a next execution of the <em>selection</em> step. Note that the selection phase always starts at the root, so it is expected that after some simulations have been performed, <em>other</em> moves than W will get more attention.</p>
<p>The first scenario of the two you presented will happen, but the following statement in its analysis is not true:</p>
<blockquote>
<p>Then W will be the most simulated child node of the root node and therefore be selected to be played despite being a blunder</p>
</blockquote>
<p>The number of simulations is not the only factor that determines selection. First of all, the number of wins must be taken into account (as it appears in the <em>numerator</em>), and secondly, the UCB1 formula (See <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_tree_search#Exploration_and_exploitation"" rel=""nofollow noreferrer"">Wikipedia</a>) balances out moves with a lot of simulations against other moves which didn't get much attention yet.</p>
","0","Answer"
"79321147","79305588","<p>How to export YOLO segmentation model with flexible input sizes</p>
<pre><code>from ultralytics import YOLO
import coremltools as ct

# Export to torchscript first
model = YOLO(&quot;yolov8n-seg.pt&quot;)
model.export(format=&quot;torchscript&quot;)

# Convert to CoreML with flexible input size
input_shape = ct.Shape(
    shape=(1, 3, 
           ct.RangeDim(lower_bound=32, upper_bound=1024, default=640),
           ct.RangeDim(lower_bound=32, upper_bound=1024, default=640))
)

mlmodel = ct.convert(
    &quot;yolov8n-seg.torchscript&quot;,
    inputs=[ct.ImageType(
        name=&quot;images&quot;,
        shape=input_shape,
        color_layout=ct.colorlayout.RGB,
        scale=1.0/255.0
    )],
    minimum_deployment_target=ct.target.iOS16
)

mlmodel.save(&quot;yolov8n-seg-flexible.mlpackage&quot;)
</code></pre>
<p>This creates an .mlpackage that accepts images from 32 32 to 1024 1024 (you can modify these bounds as needed). Default is 640 640.</p>
<p>Read about the stuff here:</p>
<ul>
<li><p><a href=""https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html"" rel=""nofollow noreferrer"">https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html</a></p>
</li>
<li><p><a href=""https://docs.ultralytics.com/modes/export/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/export/</a></p>
</li>
<li><p><a href=""https://apple.github.io/coremltools/source/coremltools.converters.mil.input_types.html#coremltools.converters.mil.input_types.ImageType"" rel=""nofollow noreferrer"">https://apple.github.io/coremltools/source/coremltools.converters.mil.input_types.html#coremltools.converters.mil.input_types.ImageType</a></p>
</li>
</ul>
","1","Answer"
"79321377","79320289","<p>Update: So the core issue is that LGBM isn't being properly initialized by just doing super().<strong>init</strong>(**kwargs). I still don't know why that is. However, I managed to get around the problem with the missing random_state by changing my init function to;</p>
<pre><code>class MSLELGBM(LGBMRegressor):
def __init__(self, **kwargs):
    super().__init__(**kwargs)
    self.set_params(**kwargs)
</code></pre>
<p>So, calling set_params manually seems to resolve the issue of the kwargs not being propagated. I still don't know why this is though, so I'll leave the question here.</p>
","-1","Answer"
"79321595","79318939","<p>That error is due to the <code>mask_value</code> that you pass into <code>tf.keras.layers.Masking</code> not getting serialized compatibly for deserialization. But because you masking layer is a tensor containing all 0s anyway, you can instead just pass a scalar value like this and it will eliminate the need to serialize a tensor while storing the model</p>
<pre><code>tf.keras.layers.Masking(mask_value=0.0)
</code></pre>
<p>and it broadcasts it to effectively make it equivalent to comparing it against the tensor containing all 0s. <a href=""https://github.com/keras-team/keras/blob/f6c4ac55692c132cd16211f4877fac6dbeead749/keras/src/layers/core/masking.py#L54-L66"" rel=""nofollow noreferrer"">Here</a> is the source where the mask is applied like this</p>
<pre><code>ops.any(ops.not_equal(inputs, self.mask_value), axis=-1, keepdims=True)
</code></pre>
<p>and <code>ops.not_equal</code> supports broadcasting.</p>
","1","Answer"
"79322502","79316958","<p>Try deleting your unity project and making a new one. Unity says to use conda so try that too. Use python 3.9.</p>
","2","Answer"
"79323584","79270683","<p>try regularization techniques : L1 and L2 which will remove the unnecessary columns eventually.
Regularization can help the model generalize better when dealing with high-cardinality features.
Ensure that you're applying appropriate regularization to avoid overfitting,</p>
","0","Answer"
"79323903","79312660","<p>Thanks to chrslg: I removed the division by n in the derivatives. After changing the parametres for the learning rate and max iteration, the algorithm is much faster and also optimizes b. Using more data than only the small toy data set was helpful. And thanks Dr. Snoopy: It was helpful to plot the loss for each epoch.</p>
","0","Answer"
"79329388","79316648","<p>The first problem is that this model does not accept the data:</p>
<pre><code>julia&gt; batches[1][1] |&gt; summary  # batch of 8 images, 1 channel
&quot;64×64×1×8 Array{Float32, 4}&quot;

julia&gt; model(batches[1][1])
ERROR: BoundsError: attempt to access 49×49×1×8 Array{Float32, 4} at index [1:49, 1:64, 1:64, 1:8]
</code></pre>
<p>I presume your <code>x -&gt; x[:, 1:64, 1:64, :]</code> intends to trim the image size, but acts on 1 image axis and the channel dim. <code>x -&gt; x[1:64, 1:64, :, :]</code>.</p>
<p>However, <code>49×49×1×8</code> is still too small for this. Perhaps the stride is wrong? Here's a version which runs:</p>
<pre><code>julia&gt; function unet(input_channels::Int, output_channels::Int)
           encoder = Chain(
               Conv((3, 3), input_channels =&gt; 64, pad=1, relu), MaxPool((2, 2), stride=(2, 2)),
               Conv((3, 3), 64 =&gt; 128, relu, pad=1), MaxPool((2, 2), stride=(2, 2)),
               Conv((3, 3), 128 =&gt; 256, relu, pad=1), MaxPool((2, 2), stride=(2, 2)),
               Conv((3, 3), 256 =&gt; 512, relu, pad=1), MaxPool((2, 2), stride=(2, 2))
           )
           decoder = Chain(
               ConvTranspose((3, 3), 512 =&gt; 256, relu; stride=2, pad=1),  # relu inside!
               ConvTranspose((3, 3), 256 =&gt; 128, relu; stride=2, pad=1),
               ConvTranspose((3, 3), 128 =&gt; 64, relu, stride=2, pad=1),
               ConvTranspose((3, 3), 64 =&gt; output_channels, stride=4, pad=1)  # changed stride?
           )
           return Chain(encoder, decoder, x -&gt; x[1:64, 1:64, :, :])  # select on image axes
       end;

julia&gt; model = unet(1, 1);

julia&gt; model(batches[1][1]) |&gt; summary
&quot;64×64×1×8 Array{Float32, 4}&quot;
</code></pre>
<p>Here's how training should look. Writing <code>ADAM</code> means it is likely you are following some very old guide, and you should not follow the whole weird implicit <code>gradient(() -&gt; ..., params(model))</code> path. The recommended way looks like this:</p>
<pre><code>julia&gt; function train_unet(model, train_data, num_epochs::Int, learning_rate::Float64)
           # Set up the optimiser for this model:
           opt_state = Flux.setup(Adam(learning_rate), model)
           
           # The loss is always an explicit function of the model:
           loss(m, x, y) = Flux.logitcrossentropy(m(x), float(y))
           
           for epoch in 1:num_epochs
               for (input_batch, mask_batch) in train_data
                   # Gradient with respect to the model itself:
                   grads = Flux.gradient(m -&gt; loss(m, input_batch, mask_batch), model)
                   Flux.update!(opt_state, model, grads[1])
               end
               @show epoch
           end
       end;

julia&gt; train_unet(model, batches, 10, 0.001)
epoch = 1
epoch = 2
epoch = 3
epoch = 4
epoch = 5
epoch = 6
epoch = 7
epoch = 8
epoch = 9
epoch = 10
</code></pre>
<p>Now you can try it out... although this is all just noise of course:</p>
<pre><code>julia&gt; test_image, _ = data[1];

julia&gt; test_image |&gt; summary  # lacks a batch dimension
&quot;64×64×1 Array{Float32, 3}&quot;

julia&gt; test_unet(model, test_image)
ERROR: DimensionMismatch: layer Conv((3, 3), 1 =&gt; 64, relu, pad=1) expects ndims(input) == 4, but got 64×64×1 Array{Float32, 3}

julia&gt; test_unet(model, reshape(test_image,64,64,1,1))  # `plot` is drawing lines

julia&gt; function test_unet(model, test_image)
           prediction = model(test_image)
           plot(heatmap(test_image[:, :, 1, 1], title=&quot;Input Image&quot;),
                heatmap(prediction[:, :, 1, 1], title=&quot;Predicted Mask&quot;),
                layout=(1, 2))
       end

       # Example usage
test_unet (generic function with 1 method)

julia&gt; test_unet(model, reshape(test_image,64,64,1,1))
</code></pre>
","0","Answer"
"79334323","79290974","<p>The {ranger} package offers this option via <code>always.split.variables</code>. The package is very fast as well.</p>
","2","Answer"
"79344862","79320289","<h2>Root Cause</h2>
<p><code>scikit-learn</code> expects that each of the keyword arguments to an estimator's <code>__init__()</code> will exactly correspond to a public attribute on instances of the class. Per <a href=""https://scikit-learn.org/stable/developers/develop.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/developers/develop.html</a></p>
<blockquote>
<p><em>every keyword argument accepted by <code>__init__</code> should correspond to an attribute on the instance. Scikit-learn relies on this to find the relevant attributes to set on an estimator when doing model selection</em></p>
</blockquote>
<p>Its <code>.get_params()</code> method on estimators take advantage of this by inspecting the signature of <code>__init__()</code> to figure out which attributes to expect (<a href=""https://github.com/scikit-learn/scikit-learn/blob/08ed01fd21500a4e4beca0dbfbb2b8631155b6f1/sklearn/base.py#L212"" rel=""nofollow noreferrer"">scikit-learn / sklearn / base.py</a>).</p>
<p><code>lightgbm</code>'s estimators call <code>.get_params()</code> and then expect the key <code>&quot;random_state&quot;</code> to exist in the dictionary it returns... because that parameter is in the keyword arguments to <code>LGBMRegressor</code> (<a href=""https://github.com/microsoft/LightGBM/blob/e0c34e7b2f68793bbf46854b85fd750be134446d/python-package/lightgbm/sklearn.py#L507"" rel=""nofollow noreferrer"">LightGBM / python-package / lightgbm / sklearn.py</a>).</p>
<p>Your estimator's <code>__init__()</code> does not have <code>random_state</code> as a keyword argument, so when <code>self.get_params()</code> is called it returns a dictionary that does not contain <code>&quot;random_state&quot;</code>, leading to the error your observed.</p>
<h2>How to fix this</h2>
<p>If you do not need to add any other custom parameters, then just do not define an <code>__init__()</code> method on your subclass.</p>
<p>Here's a minimal, reproducible example that works with <code>lightgbm</code> 4.5.0 and Python 3.11:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from lightgbm import LGBMRegressor
from sklearn.datasets import make_regression


class MSLELGBM(LGBMRegressor):

    def predict(self, X):
        return np.exp(super().predict(X))

    def fit(self, X, y, eval_set=None, callbacks=None):
        y_log = np.log(y.copy())
        if eval_set:
            eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
        super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

# modifying bias and tail_strength to ensure every value in 'y' is positive
X, y = make_regression(
    n_samples=5_000,
    n_features=3,
    bias=500.0,
    tail_strength=0.001,
    random_state=708,
)

reg = MSLELGBM(num_boost_round=5)

# print params (you'll see all the LGBMRegressor params)
reg.get_params()

# fit the model
reg.fit(X, y)
</code></pre>
<p>If you <strong>do</strong> need to define any custom parameters, then for <code>lightgbm&lt;=4.5.0</code>:</p>
<ul>
<li>add an <code>__init__()</code> on your subclass</li>
<li>copy all of the parameters from the signature of <code>lightgbm.LGBMModel.__init__()</code> into that <code>__init__()</code></li>
<li>call <code>super().__init__()</code> in your subclass's <code>__init__()</code>, and pass it all of the keyword arguments explicitly 1 at a time with <code>=</code></li>
</ul>
<p>Like this:</p>
<pre class=""lang-py prettyprint-override""><code>class MSLELGBM(LGBMRegressor):
    
    # just including 'random_state' to keep it short... you
    # need to include more params here, depending on LightGBM version
    def __init__(self, random_state=None, **kwargs):
        super().__init__(
            random_state=random_state,
            **kwargs
        )
</code></pre>
","1","Answer"
"79388313","79308237","<p>You can view metrics of different runs using <code>Chart View</code> . Follow these steps:</p>
<ol>
<li>Click <code>Chart view</code></li>
<li>Select the runs that you want to view</li>
<li>Click <code>Add chart</code> and configure the metrics that you want to view</li>
</ol>
<p><a href=""https://i.sstatic.net/pBTKwVPf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBTKwVPf.png"" alt=""MLFlow Multi-runs Chart View"" /></a></p>
","1","Answer"
"79615494","79290968","<p>i have faced same issue with sklearn and scikeras, i changed hyper parameter tunning method with keras_tuner library.</p>
","0","Answer"