Post Id,Parent Id,Body,Score,PostType
"79323285","","<p>I am trying to set up environmenty like in tutorial: <a href=""https://pytorch.org/executorch/stable/getting-started-setup"" rel=""nofollow noreferrer"">https://pytorch.org/executorch/stable/getting-started-setup</a></p>
<p>When running ./install_requirements.sh
I get an error:
CMake Error at CMakeLists.txt:45 (project):
Generator</p>
<pre><code>  NMake Makefiles

does not support toolset specification, but toolset

  Clang

was specified.
</code></pre>
<p>I tried to change     CMAKE_ARGS += &quot; -T ClangCL&quot; to different values but nothing helps</p>
","0","Question"
"79327723","","<p>When setting up image augmentation pipelines using <a href=""https://keras.io/api/layers/preprocessing_layers/image_augmentation/"" rel=""nofollow noreferrer""><code>keras.layers.Random*</code></a> or other augmentation or <a href=""https://keras.io/api/layers/preprocessing_layers/"" rel=""nofollow noreferrer"">processing</a> methods, we often integrate these pipelines with a data loader, such as the <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">tf.data</a> API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.</p>
<p>To address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?</p>
","0","Question"
"79328514","","<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(
            self,
            input_ids: torch.LongTensor = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[List[torch.FloatTensor]] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
            interact_ids = None,
            candidate_ids = None,
        ):
            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none
    
            interact_embs = []
            candidate_embs = []
            for i in range(interact_ids.shape(0)):
                # O_i = F_i (e_i)
                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))
                # O_i = F_i (e_i)
                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))
                # replace [CandidateEmb] and [HistoryEmb]
                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)
    
            return super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                labels = labels
            )
</code></pre>
<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>
","2","Question"
"79328783","","<p>Strange thing happened today, when i opened VSC and run jupter notebook with some python code. I wanted to import torch library that was downloaded couple days ago and it worked till today.</p>
<p>Got info that there is no such packaged installed so i hit terminal and &quot;pip install torch&quot; again. But then I got this:</p>
<pre><code>~ % pip install pandas
zsh: command not found: pip
</code></pre>
<p>Which is strange, because I used pip install for every other library. Found out that now only pip3 install works, have no clue why, but still got this error:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch
</code></pre>
<p>Using &quot;<code>python3 -m pip install --upgrade pip</code>&quot; says that &quot;<code>Requirement already satisfied</code>&quot;.</p>
<p>To be frank, I have no idea what's happened during night on my computer but i guess nothing works now in VSC. Any ideas what can be causing that pip install stopped working? ChatGPT gave me hints to install homebrew but got lost so much and had to uninstall it.</p>
<p><strong>EDIT:</strong>
I've used this solution to be able to use pip as pip3 [it looks like it isn't a big deal, but still, when trying to install pytorch there is the same error as above: https://stackoverflow.com/questions/42870537/zsh-command-cannot-found-pip</p>
","-1","Question"
"79330498","","<p>When using PyTorch's <code>torch.fft.rfft</code> function, I observed that specifying an output tensor using the <code>out</code> parameter is slower than letting PyTorch manage the output internally. Here is a simple benchmark:</p>
<pre><code>input_len = 2_000_000
x = torch.rand(input_len)
y = torch.zeros(input_len // 2 + 1, dtype=torch.complex64)
</code></pre>
<pre><code>%%timeit
torch.fft.rfft(x)
# 7.45 ms ± 54.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<pre><code>%%timeit
torch.fft.rfft(x, out=y)
# 9.6 ms ± 70.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>Why is the version without the <code>out</code> parameter faster? Shouldn't specifying the output tensor avoid memory allocation and thus be more efficient?</p>
<p>Any insights into this behaviour would be greatly appreciated.</p>
","-1","Question"
"79331255","","<p>I have been able to use conda env with pytorch on MacOS. But after upgrading to Sequoia 15.2, things were not working well. I have created a new conda env and able to run jupyterlab. However, when it comes to torch, I ran into Symbol not found issue. Here are the details:</p>
<pre class=""lang-bash prettyprint-override""><code>libprotobuf 5.28.2
libtorch 2.5.1
pytorch  2.5.1
on Darwin 24.2.0

The error msg:
python
&gt;&gt;&gt; import torch
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/opt/conda/envs/llm-ft-cpu/lib/python3.11/site-packages/torch/__init__.py&quot;, line 367, in &lt;module&gt;
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: dlopen(/opt/conda/envs/llm-ft-cpu/lib/python3.11/site-packages/torch/_C.cpython-311-darwin.so, 0x0002): Symbol not found: __ZN4absl12lts_2024072212log_internal10LogMessagelsIiLi0EEERS2_RKT_
  Referenced from: &lt;7DD6F527-7A4A-3649-87B6-D68B25F8B594&gt; /opt/conda/envs/llm-ft-cpu/lib/libprotobuf.28.2.0.dylib
  Expected in:     &lt;D623F952-8116-35EC-859D-F7F8D5DD7699&gt; /opt/conda/envs/llm-ft-cpu/lib/libabsl_log_internal_message.2407.0.0.dylib
&gt;&gt;&gt;
</code></pre>
<p>Thanks</p>
<p>import torch should work with no error, if there is no Symbol Not Found issue.</p>
","1","Question"
"79332960","","<p>I got the following U-net architecture causing problems:</p>
<pre><code>class UNet(nn.Module): 
    def __init__(self, in_channels, out_channels):
        super(UNet, self).__init__()
        self.encoder1 = self.double_conv(in_channels, 64)
        self.encoder2 = self.down(64, 128)
        self.encoder3 = self.down(128, 256)
        self.encoder4 = self.down(256, 512)
        self.bottleneck = self.double_conv(512, 1024)
        self.decoder4 = self.up(1024, 512)
        self.decoder3 = self.up(512, 256)
        self.decoder2 = self.up(256, 128)
        self.decoder1 = self.up(128, 64)
        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1) # SAME convolution/padding

    def double_conv(self, in_channels, out_channels): # Convo Block
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

    def down(self, in_channels, out_channels):
        return nn.Sequential(
            nn.MaxPool2d(kernel_size=2, stride=2),
            self.double_conv(in_channels, out_channels),
        )

    def up(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            self.double_conv(in_channels, out_channels),
        )

    def forward(self, x):
        # Encoder
        enc1 = self.encoder1(x)  # Output: [1, 64, 256, 256]
        print(&quot;enc1.shape&quot;,enc1.shape)
        enc2 = self.encoder2(enc1)  # Output: [1, 128, 128, 128]
        print(&quot;enc2.shape&quot;,enc2.shape)
        enc3 = self.encoder3(enc2)  # Output: [1, 256, 64, 64]
        print(&quot;enc3.shape&quot;,enc3.shape)
        enc4 = self.encoder4(enc3)  # Output: [1, 512, 32, 32]
        print(&quot;enc4.shape&quot;,enc4.shape)
        bottleneck_output = self.bottleneck(enc4)  # Output: [1, 1024, 32, 32]
        print(&quot;bottleneck_output&quot;,bottleneck_output.shape)
        
        # Decoder
        dec4 = self.decoder4(bottleneck_output)#bottleneck_output)  # Output: [1, 512, 64, 64]
        print(dec4.shape)
        dec4 = torch.cat((dec4, enc4), dim=1)  # skip connect, Concatenate: [1, 1024, 64, 64]
        dec4 = self.double_conv(1024, 512)(dec4)  # Corrected input channels to 1024

        dec3 = self.decoder3(dec4)  # Output: [1, 256, 128, 128]
        dec3 = torch.cat((dec3, enc3), dim=1)  # Concatenate: [1, 512, 128, 128]
        dec3 = self.double_conv(512, 256)(dec3)  # Corrected input channels to 512

        dec2 = self.decoder2(dec3)  # Output: [1, 128, 256, 256]
        dec2 = torch.cat((dec2, enc2), dim=1)  # Concatenate: [1, 256, 256, 256]
        dec2 = self.double_conv(256, 128)(dec2)  # Corrected input channels to 256

        dec1 = self.decoder1(dec2)  # Output: [1, 64, 512, 512]
        dec1 = torch.cat((dec1, enc1), dim=1)  # Concatenate: [1, 128, 512, 512]
        dec1 = self.double_conv(128, 64)(dec1)  # Corrected input channels to 128

        return self.final_conv(dec1)  # Output: [1, 1, 512, 512]```
</code></pre>
<p>When executing in a main method via</p>
<pre><code>unet = UNet(in_channels=1, out_channels=1)
sample_input = torch.randn(1, 1, 256, 256)
output = unet(sample_input)
</code></pre>
<p>I get:</p>
<pre><code>enc1.shape torch.Size([1, 64, 256, 256])
enc2.shape torch.Size([1, 128, 128, 128])
enc3.shape torch.Size([1, 256, 64, 64])
enc4.shape torch.Size([1, 512, 32, 32])
bottleneck_output torch.Size([1, 1024, 32, 32])
</code></pre>
<p>and the following error:</p>
<pre><code>---&gt; 55 dec4 = self.decoder4(bottleneck_output)

RuntimeError: Given groups=1, weight of size [512, 1024, 3, 3], expected input[1, 512, 64, 64] to have 1024 channels, but got 512 channels instead
</code></pre>
<p>So the problem apparently is the <code>bottleneck_output</code> shape which does have 1024 channels, but the <code>decoder4</code> does not seem to recognise it or sth. like that.</p>
<p>I tried matching the dimensions and other things like an align function but nothing worked so far. Also printing out the output shapes didn't really help. Thanks for any hints.</p>
","1","Question"
"79335746","","<pre class=""lang-none prettyprint-override""><code>
 def runTpoly(rank, size, pp, cs, pkArithmetics_evals, 
       pkSelectors_evals, domain):

    init_process(rank, size)
    group2 = torch.distributed.new_group([1,2])
    if rank == 0:
        device = torch.device(f&quot;cuda:{rank}&quot;)
        wo_eval_8n = torch.ones(SCALE * 8 * 1, 4, dtype=torch.int64, device='cuda')

    if rank == 1:
        wo_eval_8n = torch.ones(SCALE * 8 * 10, 4, dtype=torch.int64, device='cuda')
        wo_eval_8n=wo_eval_8n+wo_eval_8n
        send(wo_eval_8n, 2)
    if rank == 2:
        wo_eval_8n = torch.ones(SCALE * 8 * 10, 4, dtype=torch.int64, device='cuda')
        print(wo_eval_8n.size())
        recv(wo_eval_8n,1)
        print(wo_eval_8n)
    if rank == 3:[enter image description here](https://i.sstatic.net/M6FMng1p.png)
        wo_eval_8n = torch.ones(SCALE * 10 * 10, 4, dtype=torch.int64, device='cuda')
        print(wo_eval_8n.size())
    # 清理进程组
    dist.destroy_process_group()


if __name__ == &quot;__main__&quot;:
    
    world_size = 4  # GPU数目
    print(torch.__file__)
    pp, pk, cs = load(&quot;/home/whyin/data/9-data/&quot;)
    domain= Radix2EvaluationDomain.new(cs.circuit_bound())
    spawn(runTpoly, args=(world_size,pp,cs,pk.arithmetics_evals,pk.selectors_evals,domain), nprocs=world_size, join=True)
</code></pre>
<p>I want to conduct point-to-point communication between rank 1 and rank 2, but the following error will occur. However, I've already verified that in my code, all ranks can communicate with rank 0. Besides, the topology structure of my GPUs is a fully connected structure with four GPUs, and there is no situation where they can't be physically connected. my pytorch is 2.0</p>
<p><img src=""https://i.sstatic.net/oTuubLwA.png"" alt=""enter image description here"" /></p>
<pre class=""lang-none prettyprint-override""><code>RuntimeError: [2] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '1:2', but store-&gt;get('1:2') got error: Connection reset by peer
Exception raised from recvBytes at /home/whyin/pnp_new/PNP/torch/csrc/distributed/c10d/Utils.hpp:616 (most recent call first)
</code></pre>
<p>I want to create a communication group, but I find that communication is still not possible in this way. I hope to achieve direct communication between two ranks without going through rank 0.</p>
","0","Question"
"79342834","","<p><a href=""https://i.sstatic.net/BYTCIizu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BYTCIizu.png"" alt=""enter image description here"" /></a></p>
<p>I installed the torch library and when I try to upload it to jupyter lab I get the following error. The library in question is pytorch and on Python it loads correctly but when I try on Jupyter it tells me that ModuleNotFoundError: No module named 'torch'</p>
","0","Question"
"79343187","","<p>I'm trying to implement the recursion below for <code>tilde_alpha_t</code>:
<a href=""https://i.sstatic.net/YTfPlXx7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YTfPlXx7.png"" alt=""enter image description here"" /></a></p>
<p>I have <code>torch</code> tensors for <code>t</code> (time) which looks like this</p>
<pre><code>t
tensor([816, 724, 295,  54, 205, 373, 665, 656, 690, 280, 234, 155,  31, 684,
        159, 749, 893, 795, 375, 443, 121, 881, 477, 326, 337, 970, 384, 247,
        511, 432, 563, 753, 764,  77, 294, 803, 935, 507, 196, 744, 140, 641,
        746, 844, 337,   4, 259, 276, 909, 962, 460, 372, 620, 466,  15, 244,
        456, 829, 491, 620, 943, 925,  82, 856, 782, 117, 609, 909, 198, 626,
        992, 998, 672, 762, 602,  85,  46, 529,  42, 841, 441, 237, 839, 953,
         87, 941, 987, 980, 304, 690,  19, 598, 687, 483, 806, 366, 807, 136,
        997, 708, 902, 751, 560, 245, 375, 688, 912, 547,  11, 285,   5,  83,
        104, 346, 312, 236, 335, 664, 435, 762, 575, 184, 341, 618, 257, 634,
        355, 762])
</code></pre>
<p>and a tensor for corresponding <code>alphas_t</code> which is</p>
<pre><code>alphas
tensor([0.4948, 0.4966, 0.4992, 0.4999, 0.4995, 0.4990, 0.4973, 0.4974, 0.4970,
        0.4993, 0.4994, 0.4996, 0.4999, 0.4971, 0.4996, 0.4962, 0.4909, 0.4953,
        0.4989, 0.4987, 0.4997, 0.4918, 0.4985, 0.4991, 0.4991, 0.4683, 0.4989,
        0.4993, 0.4984, 0.4987, 0.4981, 0.4962, 0.4960, 0.4998, 0.4992, 0.4951,
        0.4850, 0.4984, 0.4995, 0.4963, 0.4996, 0.4975, 0.4963, 0.4938, 0.4991,
        0.4999, 0.4993, 0.4993, 0.4893, 0.4747, 0.4986, 0.4990, 0.4977, 0.4986,
        0.4999, 0.4994, 0.4986, 0.4943, 0.4985, 0.4977, 0.4830, 0.4870, 0.4998,
        0.4932, 0.4956, 0.4997, 0.4978, 0.4893, 0.4995, 0.4976, 0.3951, 0.2222,
        0.4972, 0.4960, 0.4978, 0.4998, 0.4999, 0.4983, 0.4999, 0.4939, 0.4987,
        0.4994, 0.4940, 0.4794, 0.4998, 0.4835, 0.4311, 0.4535, 0.4992, 0.4970,
        0.4999, 0.4979, 0.4971, 0.4985, 0.4950, 0.4990, 0.4950, 0.4996, 0.2813,
        0.4968, 0.4900, 0.4962, 0.4981, 0.4994, 0.4989, 0.4971, 0.4889, 0.4982,
        0.4999, 0.4992, 0.4999, 0.4998, 0.4997, 0.4990, 0.4992, 0.4994, 0.4991,
        0.4973, 0.4987, 0.4960, 0.4980, 0.4995, 0.4991, 0.4977, 0.4993, 0.4976,
        0.4990, 0.4960])
</code></pre>
<p>Any hint on how to achieve that? My attempt is as follows:</p>
<pre><code>def compute_tilde_alphas(times, alphas):
    &quot;&quot;&quot;
    Compute tilde_alpha_t for each t in the batch, starting from 0.
    
    Args:
        times: Tensor of times (shape [batch_size]).
        alphas: Tensor of alpha values corresponding to times (shape [batch_size]).
        
    Returns:
        tilde_alphas: Tensor of computed tilde_alpha values (shape [batch_size]).
    &quot;&quot;&quot;
    # Ensure times and alphas are in the same batch dimension
    assert times.shape == alphas.shape, &quot;times and alphas must have the same shape&quot;
    
    batch_size = times.shape[0]
    tilde_alphas = torch.zeros(batch_size, device=alphas.device)
    
    # Iterate over the batch
    for i in range(batch_size):
        t = int(times[i])  # Current time for this batch element
        alpha_t = alphas[i]  # Corresponding alpha
        
        # Recursively compute tilde_alpha for this t
        tilde_alpha_t = 0  # Start with tilde_alpha_0 = 0
        for step in range(1, t + 1):  # Assume t defines the recursion depth
            tilde_alpha_t = torch.sqrt(alpha_t) * (1 + tilde_alpha_t)
        
        # Store the result
        tilde_alphas[i] = tilde_alpha_t
    
    return tilde_alphas
</code></pre>
<p>I think it may be correct, although I'm pretty sure there may be faster ways of obtaining the result here.</p>
","1","Question"
"79344879","","<p>I am trying to run a simple torch.distributed script between two Docker containers running on separate instances. Below is the code I am using:</p>
<pre><code>import os
import torch
import torch.distributed as dist

def init_distributed():
    os.environ['MASTER_ADDR'] = &quot;10.12.27.241&quot;
    os.environ['MASTER_PORT'] = '29500'
    node_rank = int(os.environ.get('RANK', 0)) # 1 for worker
    world_size = 2

    dist.init_process_group(
        backend='gloo',
        rank=node_rank,
        world_size=world_size
    )
    print(f&quot;Initialized process group: rank {node_rank} of {world_size}&quot;)
    return node_rank, world_size

def send_receive_message(rank, world_size):
    if rank == 0:
        # Node 0 sends a message
        message = torch.tensor([42, 43, 44], dtype=torch.int64)
        dist.send(message, dst=1)
        print(f&quot;Rank {rank} sent message: {message}&quot;)
    else:
        # Node 1 receives the message
        message = torch.zeros(3, dtype=torch.int64)
        dist.recv(message, src=0)
        print(f&quot;Rank {rank} received message: {message}&quot;)

if __name__ == &quot;__main__&quot;:
    rank, world_size = init_distributed()
    send_receive_message(rank, world_size)
    # Barrier to ensure all processes have completed
    dist.barrier()
    # Clean up
    dist.destroy_process_group()
</code></pre>
<p>I am able to run this script successfully when using the --network=host option for docker run. However, due to organizational restrictions, I am required to use the --network=bridge option. When I use --network=bridge, I encounter the following error:</p>
<pre><code>[E110 05:59:45.095859745 ProcessGroupGloo.cpp:143] Gloo connectFullMesh failed with [../third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File &quot;/data/exp/com.py&quot;, line 36, in &lt;module&gt;
    rank, world_size = init_distributed()
  File &quot;/data/exp/com.py&quot;, line 12, in init_distributed
    dist.init_process_group(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py&quot;, line 83, in wrapper
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py&quot;, line 97, in wrapper
    func_return = func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py&quot;, line 1527, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py&quot;, line 1744, in _new_process_group_helper
    backend_class = ProcessGroupGloo(
RuntimeError: Gloo connectFullMesh failed with [../third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
</code></pre>
<p>How can I configure torch.distributed to work with the bridge network when running containers on separate instances? What additional steps or configurations are required to make Gloo backend communication succeed in this setup?</p>
<p>Any guidance or pointers would be greatly appreciated!</p>
","2","Question"
"79345260","","<p>I'm trying to use torchrl's SyncDataCollector with a DQN I implemented myself in torch. As the DQN uses Conv2d and Linear Layer I have to calculate the correct size for the input of the first Linear Layer, the <code>size</code> param in the following net</p>
<pre><code>class PixelDQN(nn.Module):
    def __init__(self, input_shape, n_actions) -&gt; None:
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]
        self.fc_adv = nn.Sequential(
            NoisyLinear(size, 256),
            nn.ReLU(),
            NoisyLinear(256, n_actions),
        )
        self.fc_val = nn.Sequential(
            NoisyLinear(size, 256),
            nn.ReLU(),
            NoisyLinear(256, 1)
        )

    def forward(self, x: torch.Tensor):
        print(x.shape)
        conv = self.conv(x)
        print(conv.shape)
        adv = self.fc_adv(conv)
        val = self.fc_val(conv)
        outp = val + (adv - adv.mean(dim=1, keepdim=True))
        return outp
</code></pre>
<p>is responsible for that. As you can see I expect batched inputs as I will use a replay buffer and sample a batch from that.</p>
<p>I wrap that DQN in the following way and then use the SyncDataCollector:</p>
<pre><code>n_obs = [4,84,84]
n_act = 6

agent = QValueActor(
  module=PixelDQN(n_obs, n_act), in_keys=[&quot;pixels&quot;], spec=env.action_spec
)
policy_explore = EGreedyModule(
  env.action_spec, eps_end=EPS_END, annealing_num_steps=ANNEALING_STEPS
)
agent_explore = TensorDictSequential(
  agent, policy_explore
)

collector = SyncDataCollector(
  env,
  agent_explore,
  frames_per_batch=FRAMES_PER_BATCH,
  init_random_frames=INIT_RND_STEPS,
  postproc=MultiStep(gamma=GAMMA, n_steps=N_STEPS)
)
</code></pre>
<p>This however fails as the SyncDataCollector doesn't batch the obs from the env before giving them to the DQN so <code>size</code> calc gets wrong and the Linear layer get a wrong input dimension.
RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x256)</p>
<p>I already tried to set <code>buffer=True</code> in SyncDataCollector. I also tried to use</p>
<pre><code>agent_explore = TensorDictSequential(
  UnsqueezeTransform(0, allow_positive_dim=True), agent, policy_explore
)
</code></pre>
<p>as this was kinda suggested by ChatGPT, however it didn't seem to have any effect.</p>
<p>I also tried the <code>UnsqueezeTransform</code> in my env creation, but that didn't work either, my env looks like this:</p>
<pre><code>def make_env(env_name: str):
    return TransformedEnv(
        GymEnv(env_name, from_pixels=True),
        Compose(
            RewardSum(),
            EndOfLifeTransform(),
            NoopResetEnv(noops=30),
            ToTensorImage(),
            Resize(84, 84),
            GrayScale(),
            FrameSkipTransform(frame_skip=4),
            CatFrames(N=4, dim=-3),
        )
    )
</code></pre>
<p>I could pull the <code>size</code> calc into the forward pass of my PixelDQN and check the size of the input tensor to adapt the calc, but this seems like a weird thing to do, since it would mean I'd need to run the size calc at each single forward pass.</p>
","0","Question"
"79350403","","<p>I have equations:</p>
<pre class=""lang-latex prettyprint-override""><code>$e_{ij} = \frac{X_i W^Q (X_j W^K + A^K_{ij}) }{\sqrt{D_z}}$
$\alpha_{ij} = softmax(e_{ij})$
$z_{i} = \sum_j \alpha_{ij} (X_j W^V + A^V_{ij})$
</code></pre>
<p>where sizes:</p>
<pre><code>X: [B, S, H,D]
each W: [H,D,D]
each A: [S, S, H,D]
</code></pre>
<p>how i can calculate it via matrix operations?</p>
<p>i have a partial solution</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

B, S, H, D = X.shape
d_z = D  # Assuming d_z is equal to D for simplicity

W_Q = torch.randn(H, D, D)
W_K = torch.randn(H, D, D)
W_V = torch.randn(H, D, D)

a_K = torch.randn(S, S, H, D)
a_V = torch.randn(S, S, H, D)
}
XW_Q = torch.einsum('bshd,hde-&gt;bshe', X, W_Q)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]
XW_K = torch.einsum('bshd,hde-&gt;bshe', X, W_K)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]

e_ij_numerator = XW_Q.unsqueeze(2) @ (XW_K.unsqueeze(1) + a_K).transpose(-1, -2)  # [B, S, 1, H, D] @ [B, 1, S, H, D] -&gt; [B, S, S, H, D]
e_ij = e_ij_numerator / torch.sqrt(torch.tensor(d_z, dtype=torch.float32))  # [B, S, S, H, D]

XW_V = torch.einsum('bshd,hde-&gt;bshe', X, W_V)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]
alpha = F.softmax(e_ij, dim=2)  # [B, S, S, H, D]

z_i = torch.einsum('bshij,bshjd-&gt;bshid', alpha, XW_V.unsqueeze(1) + a_V)  # [B, S, S, H, D] @ [B, 1, S, H, D] -&gt; [B, S, S, H, D]
</code></pre>
<p>but z should be [B, S, H,D]</p>
","1","Question"
"79351470","","<p>how to install pytorch for cuda 12.6? it is available till 12.4. but when I install 12.4, it installed. but it is showing kernel restarting issue in jupyter notebook. please help.</p>
<p>can you please suggest methods to overcome this problem</p>
","-4","Question"
"79352559","","<p>I'm working on a project that involves constructing a watertight triangle mesh from a point cloud (potentially using alpha shapes), optimizing point positions (with minimal recomputation of the mesh), projecting the mesh to 2D and finding boundary points, preventing self-intersections, calculating mesh volume, and integrating all this into a differentiable machine learning pipeline. I am looking to find a mesh library which will assist me. I'm choosing between Open3D and PyTorch3D currently, but am open to using both or using any other libraries which I have not yet come across.</p>
<p>I have looked at the documentation for both and my observations are as follows.</p>
<h4>Open3D vs PyTorch3D: Pros and Cons</h4>
<p><strong>Open3D</strong> provides functionality to create a mesh from a point cloud using alpha shapes (<code>create_from_point_cloud_alpha_shape</code>), check if a mesh is watertight (<code>is_watertight</code>), and calculate its volume (<code>get_volume</code>). It also includes an ML add-on, though this seems focused on batch processing and dataset handling rather than enabling backpropagation, and so to perform backpropagation, I would need to backpropagate through the point cloud to get new points, and then compute a new mesh based on these updated points.</p>
<p>On the other hand, <strong>PyTorch3D</strong> integrates well with PyTorch, making it fully compatible with a differentiable pipeline. However, it lacks built-in support for alpha shape-based mesh construction, watertightness checks, and direct volume calculation (though volume could be implemented manually using a 3D shoelace formula).</p>
<h4>Key Questions</h4>
<ul>
<li>Open3D seems feature-complete for geometry processing but lacks differentiability. How hard would it be to integrate Open3D into a differentiable pipeline?</li>
<li>PyTorch3D handles differentiability but lacks essential geometry processing tools. Are there workarounds or plugins to address this?</li>
<li>Are there better libraries that combine the strengths of these two, or am I underestimating the effort required to extend one of them?</li>
</ul>
<p>I’d appreciate any advice, alternative suggestions, or insights on whether these concerns are over- or under-emphasized.</p>
","1","Question"
"79355534","","<p>I’m encountering an issue when switching my device to mps. My training runs smoothly on cpu, but when I set the device to mps, I get the following error:</p>
<p><code>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</code></p>
<p>Could you please help me understand why this error occurs and how I might resolve it? Thank you very much for your assistance!</p>
<pre><code>import torch
from torch.utils.data import Dataset, DataLoader
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image, ImageDraw, ImageFont
import random
import string


def generate_text_image(text, width=384, height=96):
    image = Image.new(&quot;RGB&quot;, (width, height), color=&quot;white&quot;)
    draw = ImageDraw.Draw(image)

    try:
        font = ImageFont.truetype(&quot;/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf&quot;, 32)
    except:
        font = ImageFont.load_default()

    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]
    x = (width - text_width) // 2
    y = (height - text_height) // 2
    draw.text((x, y), text, fill=&quot;black&quot;, font=font)

    return image


class OCRDataset(Dataset):
    def __init__(self, num_samples=1000, processor=None):
        self.processor = processor
        self.samples = []
        chars = string.ascii_letters + string.digits
        for _ in range(num_samples):
            text = &quot;&quot;.join(random.choices(chars, k=random.randint(5, 10)))
            image = generate_text_image(text, 230, 100)
            self.samples.append((image, text))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image, text = self.samples[idx]
        pixel_values = self.processor(image, return_tensors=&quot;pt&quot;).pixel_values
        labels = self.processor.tokenizer(
            text, padding=&quot;max_length&quot;, max_length=20, return_tensors=&quot;pt&quot;
        ).input_ids

        return {&quot;pixel_values&quot;: pixel_values.squeeze(), &quot;labels&quot;: labels.squeeze()}


def main():
    processor = TrOCRProcessor.from_pretrained(
        &quot;microsoft/trocr-base-handwritten&quot;, use_fast=True
    )
    model = VisionEncoderDecoderModel.from_pretrained(
        &quot;microsoft/trocr-base-handwritten&quot;
    )

    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model.config.vocab_size = model.config.decoder.vocab_size

    train_dataset = OCRDataset(num_samples=1000, processor=processor)
    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

    device = &quot;mps&quot;  # or `cpu``
    print(f&quot;Training on device: {device}&quot;)
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    num_epochs = 3
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        print(f&quot;\nStarting Epoch {epoch+1}/{num_epochs}&quot;)
        for batch_idx, batch in enumerate(train_dataloader):
            pixel_values = batch[&quot;pixel_values&quot;].to(device)
            labels = batch[&quot;labels&quot;].to(device)
            outputs = model(pixel_values=pixel_values, labels=labels)
            loss = outputs.loss
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            if (batch_idx + 1) % 10 == 0:
                current_loss = total_loss / (batch_idx + 1)
                print(
                    f&quot;Batch {batch_idx+1}/{len(train_dataloader)} | &quot;
                    f&quot;Current Loss: {current_loss:.4f}&quot;
                )

        avg_loss = total_loss / len(train_dataloader)
        print(f&quot;\nEpoch {epoch+1} Summary:&quot;)
        print(f&quot;Average Loss: {avg_loss:.4f}&quot;)
        print(&quot;-&quot; * 50)

    print(&quot;\nTraining completed!&quot;)
    print(f&quot;Saving model to: models/trocr&quot;)

    model.save_pretrained(&quot;models/trocr&quot;)
    processor.save_pretrained(&quot;models/trocr&quot;)
    print(&quot;Model saved successfully!&quot;)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>I have confirmed that MPS is available on the Mac with the M4 chip.</p>
","0","Question"
"79355662","","<p>I have created a structure, which provided a dataset as opts to a ProcessPoolExecutor and the inputs are the indices for the dataset.</p>
<p>I could provide a MWE, but I tried several approaches and all resulted in something like a Forkbomb.</p>
<p>I think the cause was some internal multiprocess execution of pytorch, but Ian unable to provide some proof on this assumption.</p>
<p>So the question: Is there a way to go multiprocess in pytorch with data, which is not able to create a batch from (different shape), without creating a forkbomb?</p>
<p>The goal is to save data at the end of the process to disk, so I don't have to take care about sync (and the data is at the end too large to fit all in memory for all data).</p>
","0","Question"
"79355967","","<p>I believe I’m correctly following HuggingFace’s documentation on fine-tuning pretrained models, but I get a model with 100% trainable parameters. I thought only some layers would be unfrozen and optimized, but it looks like all of them are.</p>
<pre class=""lang-py prettyprint-override""><code>def print_trainable_parameters(model):
    &quot;&quot;&quot;
    Prints the number of trainable parameters in the model.
    &quot;&quot;&quot;
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f&quot;trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}&quot;
    )

...
# id2label and label2id represent 3 classes in my current problem

model_name = &quot;nvidia/segformer-b5-finetuned-cityscapes-1024-1024&quot;
model = AutoModelForSemanticSegmentation.from_pretrained(model_name, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)
print_trainable_parameters(model)
</code></pre>
<p>Prints the following:</p>
<pre><code>Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:
- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated
- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([3]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 84595651 || all params: 84595651 || trainable%: 100.00
</code></pre>
<p>Why 100% trainable parameters? I could use PEFT to reduce the number of trainable parameters, but I thought that only a small subset of the parameters would be free to be optimized based on the warning message of layer <code>decode_head.classifier</code>.</p>
","3","Question"
"79356290","","<p>I have a <code>nn.Module</code> subclass with a property which is also a module, but setting a new value does not work.</p>
<p>I realize <code>nn.Module</code> overrides <code>__setattr__</code> and therefore a lot of usual Python attribute behavior, but does that really mean the property mechanism is broken in <code>nn.Module</code> subclasses?</p>
<p>Here's a minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn


import torch
import torch.nn as nn

class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        self._submodule = nn.Linear(5, 5)

    @property
    def x(self):
        return self._submodule

    @x.setter
    def x(self, new_submodule):
        self._submodule = new_submodule

module = MyModule()
print(&quot;Initial module.x:&quot;, module.x)

# Replace the submodule
module.x = nn.Linear(5, 2)

print(&quot;After assignment attempt, module.x:&quot;, module.x)
</code></pre>
<p>Output:</p>
<pre><code>Initial module.x: Linear(in_features=5, out_features=5, bias=True)
After assignment attempt, module.x: Linear(in_features=5, out_features=5, bias=True)
</code></pre>
","0","Question"
"79356354","","<p>Long story short, I have a 2D matrix of ones and zeros and I need to retrieve, for each row, the indexes of the elements set to one. The “standard” way to do so would be torch.nonzero, but that function is well known for being 1) a real bottleneck, since it does not know in advance the size of the final vector, and 2) it cannot be applied to each row of a 2D tensor in one shot since different rows may have different amounts of ones.</p>
<p>Recently, at::nonzero_static has been introduced, which solves the first point by giving the function the expected maximum number of nonzero elements (which is fine for my application). However, it does not feature a “dim” argument, meaning that it cannot be applied to each row/column individually, which in my opinion makes no sense since setting the size of the output guarantees that each row would feature the same amount of items, thus making the output a tensor.</p>
<p>Using a for loop would obviously solve my issue, but that would mean calling the function several times which is not GPU efficient. Does anyone know a way to apply nonzero_static efficiently to each row, and returning a tensor where each row is the result of its application to each slice of the tensor? From my understanding, vmap may be a solution but I am not sure whether it is optimized for GPU.</p>
","0","Question"
"79358039","","<p>I am trying to compute the L2 norm between two tensors as part of a loss function, but somehow my loss ends up being NaN and I suspect it it because of the way the L2 norm is computed. Can you please explain me the difference between <code>torch.linalg.vector_norm(x-y)</code> and <code>torch.norm(x-y)</code> functions? I think that the x-y difference is getting to small and I want to make sure that these functions handle this case properly.</p>
","0","Question"
"79358379","","<p>I am aware variations of this have been asked multiple times, but even after working through many of those, I'm still stuck. I'm trying to get pytorch with CUDA support running on my Laptop. However, torch.cuda.is_available() returns False. Selected system information and diagnostic outputs are as follows:</p>
<ul>
<li>Lenovo ThinkPad P14S Gen4</li>
<li>NVIDIA RTX A500 Laptop GPU</li>
<li>Linux Kernel 6.11.11-1</li>
<li>NVIDIA Driver Version: 550.135</li>
</ul>
<p>nvidia-smi output:</p>
<pre><code>+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.135                Driver Version: 550.135        CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A500 Laptop GPU     Off |   00000000:03:00.0 Off |                  N/A |
| N/A   42C    P0              7W /   30W |       8MiB /   4096MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
</code></pre>
<p>nvcc --version:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Tue_Oct_29_23:50:19_PDT_2024
Cuda compilation tools, release 12.6, V12.6.85
Build cuda_12.6.r12.6/compiler.35059454_0
</code></pre>
<p>torch.utils.collect_env:</p>
<pre><code>PyTorch version: 2.5.1
Is debug build: False
CUDA used to build PyTorch: 12.6
ROCM used to build PyTorch: N/A

OS: Manjaro Linux (x86_64)
GCC version: (GCC) 14.2.1 20240910
Clang version: 18.1.8
CMake version: version 3.31.2
Libc version: glibc-2.40

Python version: 3.12.7 (main, Oct  1 2024, 11:15:50) [GCC 14.2.1 20240910] (64-bit runtime)
Python platform: Linux-6.11.11-1-MANJARO-x86_64-with-glibc2.40
Is CUDA available: False
CUDA runtime version: 12.6.85
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: GPU 0: NVIDIA RTX A500 Laptop GPU
Nvidia driver version: 550.135
cuDNN version: Probably one of the following:
/usr/lib/libcudnn.so.9.5.1
/usr/lib/libcudnn_adv.so.9.5.1
/usr/lib/libcudnn_cnn.so.9.5.1
/usr/lib/libcudnn_engines_precompiled.so.9.5.1
/usr/lib/libcudnn_engines_runtime_compiled.so.9.5.1
/usr/lib/libcudnn_graph.so.9.5.1
/usr/lib/libcudnn_heuristic.so.9.5.1
/usr/lib/libcudnn_ops.so.9.5.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
</code></pre>
<p>I've also tried using a both a venv and a conda env running pytorch 2.5.1 compiled against CUDA 12.4 with basically the same result.
Not that it should make any difference, but CUDA is both in my PATH and my LD_LIBRARY_PATH.</p>
<p>As far as I understand the whole setup, versions should be matching and I really don't understand what's going wrong. Please let me know if you need any additional information!</p>
<p>I also asked this in the PyTorch Forums (<a href=""https://discuss.pytorch.org/t/cuda-not-available/215422"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/cuda-not-available/215422</a>) and will add any solutions found there.</p>
","0","Question"
"79359767","","<p>This <a href=""https://www.sciencedirect.com/science/article/pii/S0010482524006759"" rel=""nofollow noreferrer"">paper</a> proposes a medical image segmentation hybrid CNN - Transformer model for segmenting organs and lesions in medical images simultaneously. Their model has two output branches, one to output organ mask, and the other to output lesion mask. Now they describe the testing process as follows:</p>
<blockquote>
<p>In order to compare the performance of our approach with the state-
of-the-art approaches, the following evaluation metrics have been used: F1-score (F1-S), Dice score (D-S), Intersection Over Union (IoU), and
HD95, which are defined as follows:</p>
<p><a href=""https://i.sstatic.net/lnUW339F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lnUW339F.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<blockquote>
<p><a href=""https://i.sstatic.net/53lIhglH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/53lIhglH.png"" alt=""enter image description here"" /></a></p>
<p>where T P is True Positives, T N is True Negatives, F P is False
Positives,and F N is False Negatives, all associated with the
segmentation classes of the test images. The Dice score is a macro
metric, which is calculated for N testing images as follow:</p>
<p><a href=""https://i.sstatic.net/AJvkURd8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJvkURd8.png"" alt=""enter image description here"" /></a>
where TPi, FPi and FNi are True Positives, True Negatives, False. Positives and False
Negative for the ith image, respectively.</p>
</blockquote>
<p>I am confused regarding how to implement those metrics (excluding HD95) like in this paper, what I understand is that to compute TP, FP, and FN for f1-score and IoU, I need to aggregate those 3 quantities (TP, FP, and FN) across all the samples in the test set for the two outputs (lesion and organ), and the aggregation is a sum operation. So for example to calculate the TP, I need to calculate it for every output of every sample and sum this TP. Then repeat this for calculating the TP  for every sample in a similar manner and then add all those TPs to get the overall TP. Then I do the same for FP and FN and then plug them in the formulas.</p>
<p>I am not sure if my understanding is correct or not. For Dice score, I need to calculate it for every output separately and then average them? I am not sure about that, so I accessed the <a href=""https://github.com/faresbougourzi/D-TrAttUnet"" rel=""nofollow noreferrer"">GitHub</a> for this paper. The model is defined <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/main2/Architecture.py"" rel=""nofollow noreferrer"">here</a>, and the coding for the testing procedure is defined <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/main2/detailed%20train%20and%20test/train_test_DTrAttUnet_BinarySegmentation.py"" rel=""nofollow noreferrer"">here</a>. The used framework is PyTorch. I don't have any knowledge regarding PyTorch, so still I can't understand how these metrics have been implemented, and hence, I cant confirm if my understanding is correct or not. So please can somebody explain the logic used to implement these metrics.</p>
<p>Edit 1 : I went through the code for calculating TP,FP, and FN in <code>train_test_DTrAttUnet_BinarySegmentation.py</code>:</p>
<pre><code>TP += np.sum(((preds == 1).astype(int) +
                             (yy == 1).astype(int)) == 2)
                TN += np.sum(((preds == 0).astype(int) +
                             (yy == 0).astype(int)) == 2)
                FP += np.sum(((preds == 1).astype(int) +
                             (yy == 0).astype(int)) == 2)
                FN += np.sum(((preds == 0).astype(int) +
                             (yy == 1).astype(int)) == 2)
</code></pre>
<p>It seems like they were doing the forward pass using a for loop and then accumulating the these quantities, and after this loop they calculate the metrics:</p>
<pre><code>    F1score = TP / (TP + ((1/2)*(FP+FN)) + 1e-8)
    IoU = TP / (TP+FP+FN)
</code></pre>
<p>So this means that they are accumulating the TP,FP and FN through all the images for both outputs and then they calculate the metrics, Is that correct ?
For Dice Score it seems tricky for me, they still inside the loop calculate some quantities :</p>
<pre><code>for idice in range(preds.shape[0]):
                    dice_scores += (2 * (preds[idice] * yy[idice]).sum()) / (
                        (preds[idice] + yy[idice]).sum() + 1e-8
                    )
    
                predss = np.logical_not(preds).astype(int)
                yyy = np.logical_not(yy).astype(int)
                for idice in range(preds.shape[0]):
                    dice_sc1 = (2 * (preds[idice] * yy[idice]).sum()) / (
                        (preds[idice] + yy[idice]).sum() + 1e-8
                    )
                    dice_sc2 = (2 * (predss[idice] * yyy[idice]).sum()) / (
                        (predss[idice] + yyy[idice]).sum() + 1e-8
                    )
                    dice_scores2 += (dice_sc1 + dice_sc2) / 2
</code></pre>
<p>Then at the end of the loop :</p>
<pre><code> epoch_dise = dice_scores/len(dataloader.dataset)
 epoch_dise2 = dice_scores2/len(dataloader.dataset)
</code></pre>
<p>Still, I cant understand what is going on for Dice Score.</p>
","3","Question"
"79360229","","<p>I want to inherit the <code>torch.utils.data.Dataset</code> class to load my custom image dataset, let's say for a classification task. here is the example of official pytorch website in this <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"" rel=""nofollow noreferrer"">link</a>:</p>
<pre><code>import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
</code></pre>
<p>I have noticed that:</p>
<ol>
<li>in the <code>__getitem__</code> we are reading an image from disk to memory. It means if we train our model for several epochs, we are re-reading the same image into memory several times. To my knowledge it is a costly action</li>
<li>a transform is applied each time an image is read from disk and that seems to me a nearly redundant action.</li>
</ol>
<p>I undrestand in very big datasets, we cannot fit the data fully into the memory and thus we have no choice but to read it this way (as we must iterate over all data in an epoch) and I was wondering, in the case that all my data can be fit into memory, isn't reading it all from the disk in the <code>__init__</code> function a better approach?</p>
<p>Through my little experience in computer vision I have noticed that croping images into fixed size images is very recurring in the <code>transform</code>. Then why shouldn't we crop the images once and store it on the disk somewhere else and throughout training only read the cropped images? This seems a more efficient approach to me.</p>
<p>I undrestand some transforms such as those used for augmentation rather than normalization would be better to be applied in the <code>__getitem__</code> to have a randomly generated data rather than a fixed one.</p>
<p>Can you clarify the subject for me?
If it is a common knowledge that I'm missing, please guide me to codebases with the proper approach.</p>
","0","Question"
"79360262","","<p>I learning the &quot;DIVE INTO DEEP LEARNING&quot; pytorch version, in <a href=""https://d2l.ai/chapter_preliminaries/calculus.html"" rel=""nofollow noreferrer"">https://d2l.ai/chapter_preliminaries/calculus.html</a>, I used 'jupyter notebook' command, and ran the pytorch code in the jupyter ,everything run ok, I rewrote the pytorch code into 1.py, so I can debug the pytorch code convenient, here is my 1.py code</p>
<pre><code>
#%matplotlib inline
import os
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l


def f(x):
    return 3 * x ** 2 - 4 * x
    
    
def numerical_lim(f, x, h):
    return (f(x + h) - f(x)) / h

h = 0.1
for i in range(5):
    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
    h *= 0.1

def use_svg_display():  #@save
    &quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;
    backend_inline.set_matplotlib_formats('svg')
    
def set_figsize(figsize=(3.5, 2.5)):  #@save
    &quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;
    use_svg_display()
    d2l.plt.rcParams['figure.figsize'] = figsize

#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    &quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    &quot;&quot;&quot;绘制数据点&quot;&quot;&quot;
    if legend is None:
        legend = []

    set_figsize(figsize)
    
        
    axes = axes if axes else d2l.plt.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, &quot;ndim&quot;) and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], &quot;__len__&quot;))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)


x = np.arange(0, 3, 0.1)
plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])


</code></pre>
<p>I run the command
&quot;conda activate d2l&quot;
then run
&quot;python 1.py&quot;
it shows error:</p>
<pre><code>
⇒  conda activate d2l
lee@Princekin-MacbookPro:~/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4|main⚡
⇒  python 1.py
h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
Traceback (most recent call last):
  File &quot;/Users/lee/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4/1.py&quot;, line 79, in &lt;module&gt;
    plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
  File &quot;/Users/lee/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4/1.py&quot;, line 54, in plot
    axes = axes if axes else d2l.plt.gca()
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 2309, in gca
    return gcf().gca()
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 906, in gcf
    return figure()
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/_api/deprecation.py&quot;, line 454, in wrapper
    return func(*args, **kwargs)
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 840, in figure
    manager = new_figure_manager(
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 383, in new_figure_manager
    _warn_if_gui_out_of_main_thread()
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 361, in _warn_if_gui_out_of_main_thread
    if _get_required_interactive_framework(_get_backend_mod()):
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 208, in _get_backend_mod
    switch_backend(rcParams._get(&quot;backend&quot;))
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 356, in switch_backend
    install_repl_displayhook()
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py&quot;, line 157, in install_repl_displayhook
    ip.enable_gui(ipython_gui_name)
  File &quot;/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/IPython/core/interactiveshell.py&quot;, line 3607, in enable_gui
    raise NotImplementedError('Implement enable_gui in a subclass')
NotImplementedError: Implement enable_gui in a subclass
lee@Princekin-MacbookPro:~/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4|main⚡
⇒

</code></pre>
<p>I have no idea how to solve this problem, if you have any ideas, it would be greatly appreciated,thanks</p>
","0","Question"
"79361940","","<pre><code>import torch
import torch.nn as nn

class PINN(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_layers, neurons_per_layer):
        super(PINN, self).__init__()
        layers = []
        layers.append(nn.Linear(input_dim, neurons_per_layer))
        for _ in range(hidden_layers):
            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))
        layers.append(nn.Linear(neurons_per_layer, output_dim))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Example: generating random input data
inputs = torch.rand((1000, 3))  # 3D input coordinates


model = PINN(input_dim=3, output_dim=3, hidden_layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
    optimizer.zero_grad()  
    nn_output = model(inputs) # Compute the NN prediction
    # Compute e.g gradient of nn_output
    loss.backward()  
    optimizer.step() 
</code></pre>
<p>I want to implement a physics-informed NN where the inputs are <code>N</code> 3d points (x,y,z) and the NN output is a vector-valued quantitiy at this point, that is, both input dimension and output dimension are the same.</p>
<p>To calculate the loss at every epoch, I need to have the value of the quantity at all points. Example: For <code>N=1000</code>points, I need all <code>1000</code> NN-predictions before I can proceed with the loss calculation.
In my code, I am basically giving a <code>1000x3</code> object to the input layer assuming that pytorch passes each row (<code>1x3</code>) separately to the network and at the end organizes it again as an <code>1000x3</code>object.</p>
<p>Does pytorch work like that or do I have to rethink this approach?</p>
","0","Question"
"79365374","","<p>I have access to a large CPU cluster that does not have GPUs. Is it possible to speed up YOLO training by parallelizing between multiple CPU nodes?<br />
The docs say that <code>device</code> parameter specifies the computational device(s) for training: a single GPU (device=0), multiple GPUs (device=0,1), CPU (device=cpu), or MPS for Apple silicon (device=mps).
What about multiple CPUs?</p>
","0","Question"
"79365805","","<p>I am trying to optimize the parameters of a function but the loss remains the same</p>
<pre><code>class BoardOptimizer(torch.nn.Module):
    def __init__(self):
        super().__init__()

        # Initialize parameters with current values but make them trainable
        self.highpass_freq = torch.nn.Parameter(torch.tensor(100.0), requires_grad=True)

    def get_pedalboard(self):
        highpass_freq = torch.clamp(self.highpass_freq, 20, 500)
        
        board = Pedalboard([
            HighpassFilter(cutoff_frequency_hz=float(highpass_freq)),
        ])
        
        return board

    def forward(self, sample_audio, sample_rate):
        board = self.get_pedalboard()
        music = board.process(sample_audio, sample_rate)
        return music

model = BoardOptimizer()
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)
print(type(criterion), y.shape, y_pred.shape)
for t in range(2000):
    y_pred = torch.Tensor(model(x, 16000))

    loss = criterion(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    optimizer.zero_grad()
    loss.requires_grad = True
    loss.backward()
    optimizer.step()
</code></pre>
<pre><code>99 0.011413631960749626
199 0.011413631960749626
299 0.011413631960749626
399 0.011413631960749626
</code></pre>
<p>I assumed the error was because of gradients not being turned on.</p>
<p>Tried printing this</p>
<pre><code>for name, param in model.named_parameters():
        print(f&quot;{name}: grad = {param.grad}&quot;)
</code></pre>
<p>but i kept getting</p>
<pre><code>highpass_freq: grad = None
</code></pre>
","0","Question"
"79366420","","<p>While using torch and matplotlib I was able to change the grayscale color #50 e.g. to red. However, only by saving the image inbetween.
How can it be solved without saving the image?
I am sorry for the simple question, but I am new with torch, torchvision and matplotlib.</p>
<p>Here is my code, first to save the image:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import matplotlib.pyplot as plt
from torchvision.io import read_image 

mask = read_image(&quot;A_mask.png&quot;)

mask_np = mask.numpy()
print(mask_np.shape)    #(1, 438, 567)
mask_np[mask_np == 4] = 50
output_image = torch.tensor(mask_np)

from torchvision import transforms
im = transforms.ToPILImage()(output_image).convert(&quot;RGB&quot;)
im.save(&quot;A-Mask-Colored-By-Hand.png&quot;)
</code></pre>
<p>Then to load it:</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import transforms

m = read_image(&quot;A-Mask-Colored-By-Hand.png&quot;)
#print(type(m))  # &lt;class 'torch.Tensor'&gt;
</code></pre>
<p>Here is my question, how to convert &quot;im&quot; into expected format, instead reading it. This code block is what I have tried and which is not working:
So commenting out this block ... will work but is not optimal by saving and loading again.</p>
<pre class=""lang-py prettyprint-override""><code>from PIL import Image
#tensor_image = im.permute(1,2,0) # #
tensor_image = im
pil_image = Image.fromarray((tensor_image.numpy() * 255).astype(&quot;uint8&quot;)) 
# here it stops, throwing an AttributeError: 'Image' object has no attribute 'numpy'
</code></pre>
<p>This code block works:</p>
<pre><code>m = pil_image
print(m.shape)    #(3, 438, 567)
m = m.numpy()
print(m.shape)    #(3, 438, 567)

cgrey = 50 # grey #50 &gt; into red #255
mask = (m[0]==cgrey) &amp; (m[1]==cgrey) &amp; (m[2]==cgrey)
m[0][mask] = 255
m[1][mask] = 0
m[2][mask] = 0

output_image = torch.tensor(m)

from torchvision import transforms
im = transforms.ToPILImage()(output_image).convert(&quot;RGB&quot;)
print(im) # tensor
im.save(&quot;A-Mask-Colored-RGB.png&quot;)
</code></pre>
<p>Thanks in advance!</p>
","0","Question"
"79366566","","<p>I have three inputs to my LSTM (x,y,z). My LSTM model is used to predict the next time step of z. I have a lookback period of 9 timesteps. I then need to forecast the next time steps of z using a recursive function. However, I get bad results when I plot my forecasted z values. Data comes from a csv file that I cannot share.</p>
<p>This is my code:</p>
<pre><code>data = np.column_stack((x,y, z))
def df_to_X_y(data, window_size=9):
    X = [ ]
    y = [ ] 
    for i in range(len(data) - window_size):  
        row = data[i:i + window_size]         
        X.append(row)                         
        label = data[i + window_size, 0]      
        y.append(label)                       
    return np.array(X), np.array(y)

X, y = df_to_X_y(data)

split_ratio = 0.8
split_idx = int(len(X) * split_ratio)  

X_train = X[:split_idx]  
X_test = X[split_idx:]   

y_train = y[:split_idx] 
y_test = y[split_idx:]    

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, target_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, target_size)  # Output layer for regression

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        # lstm_out, (hn, cn) = self.lstm(x)  
        out = self.fc(lstm_out[:, -1, :])  
        return out
    
    def forecast(self, initial_input, num_steps):
        predictions = []  # Store forecasted TMP values
        current_input = initial_input.clone()  # Clone to avoid modifying the original input

        for _ in range(num_steps):
            next_output = self.forward(current_input)  
            predictions.append(next_output.unsqueeze(1))  # Add time dimension
            
            next_input = current_input.clone()  # Clone the current input
            next_input[:, :-1, :] = current_input[:, 1:, :]  # Shift the window by 1 step
            next_input[:, -1, 0] = next_output.squeeze(1)  # Update z with the prediction
            # Leave x and y unchanged in next_input[:, -1, 1:] (automatically retained)

            current_input = next_input  # Update for the next iteration

        return torch.cat(predictions, dim=1)  # Concatenate predictions along the time dimension


input_size = 3  
hidden_size = 50  
num_layers = 3  
learning_rate = 0.04
num_epochs = 50  
target_size = 1

model = LSTMModel(input_size, hidden_size, num_layers, target_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

train_loss_values = []
test_loss_values = []


for epoch in range(num_epochs):
    model.train()  
    optimizer.zero_grad()  
    y_pred = model(X_train_tensor) 

    train_loss = criterion(y_pred.squeeze(), y_train_tensor) 
    train_loss.backward()  
    optimizer.step()  
    train_loss_values.append(train_loss.item())

    model.eval()  
    with torch.no_grad():
        y_test_pred = model(X_test_tensor)
        test_loss = criterion(y_test_pred.squeeze(), y_test_tensor)

    test_loss_values.append(test_loss.item())

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}')

# Number of steps to forecast
num_steps = 9
# Use the last sequence from the test set as the initial input
initial_input = X_test_tensor[-1].unsqueeze(0) # Shape: (1, seq_length, input_size)

# Perform multi-step forecasting
model.eval()
with torch.no_grad():
forecasted_values = model.forecast(initial_input, num_steps) 
</code></pre>
","0","Question"
"79367182","","<p>I’m trying to import <code>Tensor</code> from PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import Tensor
</code></pre>
<p>but I keep getting this error:</p>
<pre><code>ImportError: cannot import name 'Tensor' from 'torch' (unknown location)
</code></pre>
<h3>What I’ve Tried:</h3>
<ol>
<li>Checked that PyTorch is installed (<code>pip show torch</code>), and I’m using version <code>2.5.1</code>.</li>
<li>Reinstalled PyTorch:
<pre><code>pip uninstall torch
pip install torch
</code></pre>
</li>
<li>Tested the import in a Python shell, but the error persists.</li>
</ol>
<h3>Environment:</h3>
<ul>
<li>Python version: 3.10</li>
<li>PyTorch version: 2.5.1</li>
<li>OS: Windows 10</li>
<li>Virtual environment: Yes</li>
</ul>
<p>How can I fix this issue?</p>
","9","Question"
"79367460","","<pre><code>&gt;&gt;&gt; a = torch.tensor([[1,2,3],[2,3,4],[3,4,5]])
&gt;&gt;&gt; a
tensor([[1, 2, 3],
        [2, 3, 4],
        [3, 4, 5]])
</code></pre>
<p>If we convert this tensor to a numpy array and try to print it, this gives an error</p>
<pre><code>&gt;&gt;&gt; a = a.numpy()
&gt;&gt;&gt; a
AttributeError: module 'numpy.core.multiarray' has no attribute 'generic'

The above exception was the direct cause of the following exception:
...
--&gt; 794 output = repr(obj)
    795 lines = output.splitlines()
    796 with p.group():

RuntimeError: Unable to configure default ndarray.__repr__
</code></pre>
","2","Question"
"79368015","","<p>What I understand from the docs is that both <code>torch.as_tensor()</code> and <code>torch.asarray()</code> return the tensor that shares the memory with the input <code>data</code>, and return a copy otherwise. I noticed only two differencies in parameteres:</p>
<blockquote>
<ul>
<li>I can implicitly pass <code>copy=False</code> into <code>torch.asarray()</code> to require shared memory and get the exeption if the copy is not possible, or I can pass <code>copy=True</code> to require the copy.</li>
<li>I can specify <code>requires_grad</code> in <code>torch.asarray()</code>.</li>
</ul>
</blockquote>
<p>So does <code>torch.asarray()</code> just offer more capabilities than <code>torch.as_tensor()</code>?</p>
<p>But if I just want to get the shared memory if possible, what should I use: <code>torch.asarray()</code> or <code>torch.as_tensor()</code>? Is there any difference in performance or something?</p>
","1","Question"
"79369085","","<p>I have the following Custom dataset class for an image segmentation task.</p>
<pre><code>class LoadDataset(Dataset):
    def __init__(self, img_dir, mask_dir, apply_transforms = None):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.transforms = apply_transforms
        self.img_paths, self.mask_paths = self.__get_all_paths()
        self.__pil_to_tensor = transforms.PILToTensor()
        self.__float_tensor = transforms.ToDtype(torch.float32, scale = True)
        self.__grayscale = transforms.Grayscale()

    def __get_all_paths(self):
        img_paths = [os.path.join(self.img_dir, img_name.name) for img_name in os.scandir(self.img_dir) if os.path.isfile(img_name)]
        mask_paths = [os.path.join(self.mask_dir, mask_name.name) for mask_name in os.scandir(self.mask_dir) if os.path.isfile(mask_name)]
        img_paths = sorted(img_paths)
        mask_paths = sorted(mask_paths)
        return img_paths, mask_paths

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, index):
        img_path, mask_path = self.img_paths[index], self.mask_paths[index]
        img_PIL = Image.open(img_path)
        mask_PIL = Image.open(mask_path)
        img_tensor = self.__pil_to_tensor(img_PIL)
        img_tensor = self.__float_tensor(img_tensor)
        mask_tensor = self.__pil_to_tensor(mask_PIL)
        mask_tensor = self.__float_tensor(mask_tensor)
        mask_tensor = self.__grayscale(mask_tensor)
        if self.transforms:
            img_tensor, mask_tensor = self.transforms(img_tensor, mask_tensor)
        return img_tensor, mask_tensor
</code></pre>
<p>When I am applying the following transformation:</p>
<pre><code>transforms.RandomHorizontalFlip()
</code></pre>
<p>either the image or the mask is being flipped. But if I change the order of the transformations in <code>__getitem__</code> to the following, then it works fine.</p>
<pre><code>def __getitem__(self, index):
    img_path, mask_path = self.img_paths[index], self.mask_paths[index]
    img_PIL = Image.open(img_path)
    mask_PIL = Image.open(mask_path)
    if self.transforms:
        img_PIL, mask_PIL = self.transforms(img_PIL, mask_PIL)
    img_tensor = self.__pil_to_tensor(img_PIL)
    mask_tensor = self.__pil_to_tensor(mask_PIL)
    img_tensor = self.__float_tensor(img_tensor)
    mask_tensor = self.__float_tensor(mask_tensor)
    mask_tensor = self.__grayscale(mask_tensor)
    return img_tensor, mask_tensor
</code></pre>
<p>Does the order transformation matter? I'm using <code>torchvision.transforms.v2</code> for all the transformations.</p>
","2","Question"
"79369359","","<p>Say I have obtained some <strong>alphas</strong> and <strong>betas</strong> as parameters from a neural network, which will be parameters of the Beta distribution. Now, I sample from the Beta distribution and then calculate some loss and back-propagate via the samples obtained. Is it possible to do that? Given that after the sampling process, I do <strong>.requires_grad_(True)</strong> to the sample and then compute the loss? This surely works, but it looks like the loss is not converging, is there any other way to do this in PyTorch?</p>
<p>Say, I get the following variables via some neural network:</p>
<pre class=""lang-py prettyprint-override""><code>mu, sigma, pred = model.forward(input)
</code></pre>
<p>Where say, <strong>mu</strong> is the (<code>batch_size x 30</code>) shaped tensor, similarly <strong>sigma</strong> is (<code>batch_size x 30</code>) shaped tensor. I compute the <strong>alphas</strong> and <strong>betas</strong> using the <strong>mu</strong> and <strong>sigma</strong> obtained from a Neural Network (both of the same shape (<code>batch_size x 30</code>)), and then sample it via a beta distribution as follows:</p>
<pre class=""lang-py prettyprint-override""><code>def sample_from_beta_distribution(alpha, beta, eps=1e-6):
    # Clamp alpha and beta to be positive
    alpha_positive = torch.clamp(alpha, min=eps)
    beta_positive = torch.clamp(beta, min=eps)
    
    # Create a Beta distribution
    # This will automatically broadcast to handle the batch dimension
    beta_dist = torch.distributions.beta.Beta(alpha_positive, beta_positive)
    
    # Sample from the distribution
    # This will return samples of shape [38, 30]
    samples = beta_dist.sample()
    
    return samples
</code></pre>
<p>Here, I take the <strong>samples</strong> which is of the same shape as (<code>batch_size x 30</code>), perform some operations on it, and then calculate the loss. I expected the gradient to propagate through this, but looks like the loss is not converging.</p>
<p>Any leads would help. Please note, this is not as simple as the reparameterization trick in the standard Normal distribution.</p>
","0","Question"
"79372090","","<p>I have been coding an implementation for the following problem: Given two matrices A and B, both with the same shape (bs, n, m, m), I want to compute the following expression in an optimal manner.</p>
<p><code>out = torch.log(torch.exp(A).sum(dim=1)@torch.exp(B).sum(dim=1))</code></p>
<p>The problem is that when computing the exponents, they tend to be too large sometimes and I get overflow.</p>
<p>I know about the existence of a code for the expression:</p>
<p><code>out = torch.log(torch.exp(A)@torch.exp(B))</code></p>
<p>That I could find <a href=""https://stackoverflow.com/questions/36467022/handling-matrix-multiplication-in-log-space-in-python/74409968"">here</a> and it works in the regular case where the sum over dim=1 does not exist. I tried to use this code to compute the previous expression but not successfully, I would appreciate any help.</p>
","1","Question"
"79373190","","<p>I am trying to run Albumentation but I am getting an error:</p>
<blockquote>
<p>cannot import name 'preserve_channel_dim' from 'albucore.utils'</p>
</blockquote>
<p>I have already installed Albumentation and Abucore libraries.</p>
<pre><code>import albumentations as A
import matplotlib.pyplot as plt
import cv2

transform = A.Compose([
    A.RandomCrop(width=256, height=256),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
])

import matplotlib.pyplot as plt
def cv2_imshow(img):
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.show()

image = cv2.imread(&quot;C:\\Users\\raj\\Documents\\image_yolo\\1.jpg&quot;)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
cv2_imshow(image)
transformed = transform(image=image)
transformed_image = transformed[&quot;image&quot;]
cv2_imshow(transformed_image)
</code></pre>
<p><img src=""https://i.sstatic.net/FyHOF0GV.png"" alt=""enter image description here"" /></p>
","0","Question"
"79378157","","<p>Thank you very much for reading my question , sorry if it is an obvious question.</p>
<p>I use anaconda navigator : piped install the model whisper from OpenAi, which is an audio to text transformer model, I use jupyter notebook and when I just run the cell of the model, there is this summary of modules which is quite useful to get to know what the model is :</p>
<hr />
<p><a href=""https://i.sstatic.net/DdN7t3O4l.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DdN7t3O4l.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>However, with another pip installed model :  <a href=""https://huggingface.co/breezedeus/pix2text-mfr"" rel=""nofollow noreferrer"">https://huggingface.co/breezedeus/pix2text-mfr</a>
I notice the difference is it is optimum.onnxruntime</p>
<p><a href=""https://i.sstatic.net/7ovUOjRe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7ovUOjRe.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>and when I do the same thing as above, it instead returns a memory location ? or is it ?</p>
<p>sorry if it is a simple question, I tried googling a bit but dont quite know what keyword to search - &quot;onnx pytorch model summary&quot; ? is there a way to have a model summary as above ?</p>
<p>Thank you very much for reading my question .</p>
","0","Question"
"79378344","","<p>Full project available at <a href=""https://github.com/mwp-g/MWPG-DMR"" rel=""nofollow noreferrer"">https://github.com/mwp-g/MWPG-DMR</a></p>
<p><a href=""https://i.sstatic.net/kZfEAgPb.png"" rel=""nofollow noreferrer"">the output</a></p>
<p><a href=""https://i.sstatic.net/ySAqQT0w.png"" rel=""nofollow noreferrer"">the code</a></p>
<p>I am searching for a long time on net. But no use. Please help or try to give some ideas how to achieve this.</p>
<pre><code>Traceback (most recent call last):
File &quot;pretrain/pretrain_eq.py&quot;, line 313, in
main(args, 0)
File &quot;pretrain/pretrain_eq.py&quot;, line 262, in main
loss, acc, bsz = model(batch\['eq_tokens'\], batch\['wd_tokens'\], batch\['tgt_processed_tokens'\], args.label_smoothing)
AttributeError: 'NoneType' object has no attribute 'item'
</code></pre>
<p>I find the batch is not None, why model return None?
I tried use pdb to dubug, then I find it enter to a function but the result's value is actually none, many if and for is passed.</p>
<p>I hope the project I tried to learn will run without errors
the function is below:</p>
<pre><code>def _call_impl(self, *input, **kwargs):
    # Do not call functions when jit is used
    full_backward_hooks, non_full_backward_hooks = [], []
    if len(self._backward_hooks) &gt; 0 or len(_global_backward_hooks) &gt; 0:
        full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()

    for hook in itertools.chain(
            _global_forward_pre_hooks.values(),
            self._forward_pre_hooks.values()):
        result = hook(self, input)
        if result is not None:
            if not isinstance(result, tuple):
                result = (result,)
            input = result

    bw_hook = None
    if len(full_backward_hooks) &gt; 0:
        bw_hook = hooks.BackwardHook(self, full_backward_hooks)
        input = bw_hook.setup_input_hook(input)

    if torch._C._get_tracing_state():
        result = self._slow_forward(*input, **kwargs)
    else:
        result = self.forward(*input, **kwargs)

    for hook in itertools.chain(
            _global_forward_hooks.values(),
            self._forward_hooks.values()):
        hook_result = hook(self, input, result)
        if hook_result is not None:
            result = hook_result

    if bw_hook:
        result = bw_hook.setup_output_hook(result)

    # Handle the non-full backward hooks
    if len(non_full_backward_hooks) &gt; 0:
        var = result
        while not isinstance(var, torch.Tensor):
            if isinstance(var, dict):
                var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
            else:
                var = var[0]
        grad_fn = var.grad_fn
        if grad_fn is not None:
            for hook in non_full_backward_hooks:
                wrapper = functools.partial(hook, self)
                functools.update_wrapper(wrapper, hook)
                grad_fn.register_hook(wrapper)
            self._maybe_warn_non_full_backward_hook(input, result, grad_fn)

    return result
</code></pre>
","0","Question"
"79380954","","<p>I am implementing a training loop using two NVIDIA RTX 3090 GPUs (each with 24 GB memory). My setup involves loading a large sparse matrix (approximately 6 GB) on one GPU (device_matrix) and the model on another GPU (device_net). However, when I run the training, I encounter a CUDA out of memory error during the loss.backward() step, even with a batch size of 1.</p>
<p>when I move the sparse matrix to the CPU, the code runs without issues (although slower due to data transfer overhead). On the CPU, I observe that RAM usage increases by around 9 GB during the loss.backward() step, and the total CPU RAM is 40 GB.</p>
<p>Here is a simplified version of my training function</p>
<pre><code>def train_model(model, data_loader, num_epochs, sparse_matrix_path, save_path):
    device_net = torch.device('cuda:0')
    device_matrix = torch.device('cuda:1')
    model = model.to(device_net)

    # Load model and sparse matrix
    model.train()
    
    sparse_matrix = torch.load(sparse_matrix_path, weights_only=True).to_sparse().to(device_matrix)
    sparse_matrix.requires_grad = False

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
   

    for epoch in range(num_epochs):
        for step, input_data in enumerate(data_loader):
            input_data = input_data.to(device_net).unsqueeze(1)
            optimizer.zero_grad()

            # Model forward pass
            output, _ = model(input_data)
            

            # Sparse matrix operation
            batch_size = output.size(0)
            flat_output = output.view(batch_size, -1).to(device_matrix)
            sparse_output = torch.sparse.mm(sparse_matrix, flat_output.T)

            # Compute loss
            transformed_output = sparse_output.T.view(batch_size, 1, 700, 384)
            loss = F.mse_loss(transformed_output.to(device_net), input_data)
            loss.backward()  # Error occurs here
            
            optimizer.step()
</code></pre>
<p><strong>Key Observations:</strong></p>
<ul>
<li><p>The sparse matrix (6 GB) is loaded on device_matrix (GPU 1).</p>
</li>
<li><p>When running on the CPU, loss.backward() increases RAM usage by ~9 GB, with total CPU,40 GB.</p>
</li>
<li><p>On the GPU, memory usage increases significantly during loss.backward(), causing a CUDA out-of-memory error, even with a batch size of 1.</p>
<p><strong>Questions:</strong></p>
</li>
</ul>
<ol>
<li>Why does loss.backward() cause such a significant memory increase in GPU which there is a matrix on it ?</li>
<li>How can I optimize this setup to run on GPUs without running out of memory?
Are there specific strategies or PyTorch functionalities that can reduce GPU memory usage during the backward pass?</li>
</ol>
","0","Question"
"79382515","","<p>I’m training a PatchCore model with an image size of 128x512 on a GPU with 23.67 GiB memory. However, I’m encountering the following error:</p>
<p>CUDA Version: 12.4<br />
PyTorch Version: 2.5.1</p>
<pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 2.17 GiB. GPU 0 has a total capacity of 23.67 GiB of which 47.88 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.29 GiB is allocated by PyTorch, and 15.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation. See documentation for Memory Management.
</code></pre>
<h3>Configuration (<code>yaml</code>):</h3>
<pre><code>data:
  class_path: anomalib.data.Folder
  init_args:
    name: train_data
    root: &quot;&quot;
    image_size:
      - 128
      - 512
    normal_dir: &quot;&quot;
    abnormal_dir: &quot;&quot;
    normal_test_dir: &quot;&quot;
    mask_dir: &quot;&quot;
    normal_split_ratio: 0
    extensions: [&quot;.png&quot;]
    train_batch_size: 4
    eval_batch_size: 4
    num_workers: 8
    train_transform:
      class_path: torchvision.transforms.v2.Compose
      init_args:
        transforms:
          - class_path: torchvision.transforms.v2.RandomAdjustSharpness
            init_args:
              sharpness_factor: 0.7
              p: 0.5
          - class_path: torchvision.transforms.v2.RandomHorizontalFlip
            init_args:
              p: 0.5
          - class_path: torchvision.transforms.v2.Resize
            init_args:
              size: [128, 512]
          - class_path: torchvision.transforms.v2.Normalize
            init_args:
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
    eval_transform:
      class_path: torchvision.transforms.v2.Compose
      init_args:
        transforms:
          - class_path: torchvision.transforms.v2.Resize
            init_args:
              size: [128, 512]
          - class_path: torchvision.transforms.v2.Normalize
            init_args:
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]

model:
  class_path: anomalib.models.Patchcore
  init_args:
    backbone: wide_resnet50_2
    layers:
      - layer2
      - layer3
    pre_trained: true
    coreset_sampling_ratio: 0.1
    num_neighbors: 9
</code></pre>
<p>Steps I’ve Tried:</p>
<p>Lowering the batch size: I reduced the batch size to as low as 1, but the issue persists.</p>
<p>Checking for memory fragmentation: Followed the suggestion in the error to set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True. However, this did not solve the problem.</p>
<p>Ensuring no memory leakage: Verified that no other processes are consuming GPU memory using nvidia-smi, but the allocated memory remains maxed out during training.</p>
<p>Questions:</p>
<p>Are there specific optimizations for PatchCore or PyTorch that can help reduce memory usage?</p>
","0","Question"
"79382816","","<p>I am trying to compute some derivatives of neural network outputs. To be precise I need the jacobian matrix of the function that is represented by the neural network and the second derivative of the function with respect to its inputs.</p>
<p>I want to multiply the derivative of the jacobian with a vector of same size as the input for every sample.</p>
<p>I got the result tht I need with this implementation:</p>
<pre><code>import torch

x_1 = torch.tensor([[1.,1.,1.]], requires_grad=True)
x_2 = torch.tensor([[2.,2.,2.]], requires_grad=True)
# Input to the network with dim 2x3 --&gt; 2 Samples 3 Feature
x = torch.cat((x_1,x_2),dim=0)

def calculation(x):
    c = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]).float()
    return (x@c)**2

c = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]).float()

#output of the network with dimension 2x3 --&gt; 3 outputs per Sample
y = calculation(x)

#Calculation of my jacobian with dimension 2x3x3 (one for each sample)
g = torch.autograd.functional.jacobian(calculation,(x),create_graph=True)
jacobian_summarized = torch.sum(g, dim=0).transpose(0,1)
#Calculation of my second order derivative 2x3x3x3 (On Tensor for each Sample)
gg = torch.autograd.functional.jacobian(lambda x: torch.sum(torch.autograd.functional.jacobian(calculation, (x), create_graph=True), dim=0).transpose(0,1),
            (x),
            create_graph=True)
second_order_derivative = torch.sum(gg, dim=0).transpose(1,2).transpose(0,1)

print('x:', x)
print('c:',c)
print('y:',y)
print('First Order Derivative:',jacobian_summarized)
print('Second Order Derivative:',second_order_derivative)

# Multiplication with for loop
result = torch.empty(0)
for i in range(y.shape[0]):
    result_row = torch.empty(0)
    for ii in range(y.shape[1]):
        result_value = (x[i].unsqueeze(0))@second_order_derivative[i][ii]@(x[i].unsqueeze(0).T)
        result_row = torch.cat((result_row, result_value), dim=1)
    result = torch.cat((result, result_row))
print(result)
</code></pre>
<p>I would like to know if there is a way to get the same result of the multiplication without having to use 2 for loops but rather some simple multiplication of the matrices</p>
","2","Question"
"79383301","","<p>Is there a way to use list of indices to simultaneously access the modules of
<code>nn.ModuleList</code> in python?</p>
<p>I am working with pytorch <code>ModuleList</code> as described below,</p>
<pre><code>decision_modules = nn.ModuleList([nn.Linear(768, 768) for i in range(10)])
</code></pre>
<p>Our input data is of the shape <code>x=torch.rand(32,768)</code>. Here <code>32</code> is the batch size and <code>768</code> is the feature dimension.</p>
<p>Now for each input data point in a minibatch of <code>32</code> datapoints, we want to select <code>4</code> decision modules from the list of <code>decision_modules</code>. The <code>4</code> decision engines from <code>decision_engine</code> are selected using an index list as explained below.</p>
<p>I have a index matrix of dimensions <code>ind</code>. The <code>ind</code> matrix is of dimension <code>torch.randint(0,10,(32,4))</code>.</p>
<p>I want to us a solution without use of loops as loops slows down the xecution significantly.</p>
<p>But the following code throws and error.</p>
<pre><code>import torch
import torch.nn as nn

linears = nn.ModuleList([nn.Linear(768, 768) for i in range(10)])
ind=torch.randint(0,10,(32,4))
input=torch.rand(32,768)

out=linears[ind](input)
</code></pre>
<p>The following error was observed</p>
<blockquote>
<p>File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\container.py:334, in ModuleList.<strong>getitem</strong>(self, idx)
332     return self.<strong>class</strong>(list(self._modules.values())[idx])
333 else:
--&gt; 334     return self._modules[self._get_abs_string_index(idx)]</p>
<p>File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\container.py:314, in ModuleList._get_abs_string_index(self, idx)
312 def _get_abs_string_index(self, idx):
313     &quot;&quot;&quot;Get the absolute index for the list of modules.&quot;&quot;&quot;
--&gt; 314     idx = operator.index(idx)
315     if not (-len(self) &lt;= idx &lt; len(self)):
316         raise IndexError(f&quot;index {idx} is out of range&quot;)</p>
<p>TypeError: only integer tensors of a single element can be converted to an index</p>
</blockquote>
<p>The expected output shape is (32,4,768).</p>
<p>Any help will be highly useful.</p>
","1","Question"
"79386418","","<p>I am pretty new to coding and recently I've been trying to train a recognition model for real time recognition through the computer's webcam. This is the video I am following: <a href=""https://www.youtube.com/watch?v=fZiY7zUk3TU"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=fZiY7zUk3TU</a>. However, as soon as I go to train the model with the following code:</p>
<pre><code>python3 train_ssd.py --dataset-type=voc --data=data/MedModel/ --model-dir=models/MedModel --batch-size=2 --workers=3 --epochs=500
</code></pre>
<p>I keep getting this error message:</p>
<pre><code>ModuleNotFoundError: No module named 'torch.utils'
</code></pre>
<p>I've tried updating and uninstalling and reinstalling torch and torchvision, so everything is up to date. I've tried changing my Python interpreter but that didn't help either. I tried following some advice I found online, such as creating a virtual environment to run the code, but nothing has worked so far. I know that torch.utils is installed on my machine as I ran a code importing and printing torch.utils' directory, which worked. All files needed for this project are also located in the same directory. The expected outcome is for the code to run and train the model on the images, which are given in the same directory. If it helps, I'm writing this code in python, and I'm working on a Windows machine. Any advice would be super helpful, thank you so much! <a href=""https://i.sstatic.net/B8fI36zu.png"" rel=""nofollow noreferrer"">Here's a screenshot of the code where the importing torch.utils error keeps occuring</a></p>
","0","Question"
"79389115","","<p>I'm stuck with a Pytorch problem and could use some help:</p>
<p>I've got two tensors:</p>
<ul>
<li>A 3D tensor (shape: i, j, j) with integer values from 0 to n</li>
<li>A 1D tensor (shape: n)</li>
</ul>
<p>I need to create a new tensor that's the same shape as the first one (i, j, j), but where each value is replaced by the corresponding value from the second tensor. Like using the values in the first tensor as indices for the second one.</p>
<p>Here's a quick example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

# My tensors
big_tensor = torch.randint(0, 256, (10, 25, 25))
small_tensor = torch.rand(256)

# What I'm trying to do
result = magic_function(big_tensor, small_tensor)

# How it should work
print(big_tensor[0, 0, 0])    # Let's say this outputs 42
print(small_tensor[42])       # This might output 0.7853
print(result[0, 0, 0])        # This should also be 0.7853
</code></pre>
<p>I'm looking for a performant way to do this, preferably not using loops, as both tensors can be quite big. Is there an efficient Pytorch operation or method I can use for this?</p>
<p>Any help would be awesome - thanks in advance!</p>
","2","Question"
"79389502","","<p>There are basically two parts to the question, the first part is that while inferring on the model and storing minimal results, the python app takes up around 3GB of memory on my system ( i have hardcoded device='cpu'), this is unexpected as the model is a vit-msn-base model with 12 layers only and is around 160mb in size, my batch size is 1.. Also, even on using gc.collect(), it seems like the memory usage just keeps increasing</p>
<p>the second part is that the memory usage spikes to around 10GB when i use device='cuda', this was really unexpected, on some googling, it seems like pytorch's memory handling results in extensive fragmentation, but i am not sure on how to handle that</p>
<p>here are the relevant code snippets -</p>
<pre><code>model_name  = 'facebook/vit-msn-base-4'
layers = 1
</code></pre>
<pre><code>cka_score_list = []

tokenizer = AutoImageProcessor.from_pretrained(
    model_name,
    use_fast=True,             # Use the fast tokenizer implementation
    trust_remote_code=True,    # Trust remote code (required for some models)
    add_bos_token=False,       # Do not add beginning-of-sequence token
    add_eos_token=False,       # Do not add end-of-sequence token
    padding_side=&quot;left&quot;        # Pad sequences on the left side
)

# Load the pre-trained causal language model with appropriate settings
model = AutoModel.from_pretrained(
    model_name,
    trust_remote_code=True,    # Trust remote code (required for some models)
    device_map=&quot;auto&quot;,         # Automatically map layers to available devices
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,  # Use bfloat16 if supported
)
print(dir(model))
model.eval()
model = model.to(device)
cur_score = eval_meth(classA, classB, num_images)

cka_score_list.append(cur_score)
</code></pre>
<pre><code>def latent_embeddings_for_class(classA, num_images):
    cur_csv = os.path.join(val_csv , (classA+ '.csv'))
    df = pd.read_csv(cur_csv, header = None)
    embeddings = []
    for i in range(num_images):
        image_name = df.iloc[random.randint(1, 50), 0]
        image_path = os.path.join(val_images, image_name)
        image =Image.open(image_path)

        inputs = tokenizer(image, return_tensors=&quot;pt&quot;).to(device).to(torch.bfloat16)
        print(inputs['pixel_values'].shape)
        snapshot = tracemalloc.take_snapshot()

# Print top memory-consuming lines
        top_stats = snapshot.statistics('lineno')
        for stat in top_stats[:3]:
            print(stat)

        output = model(**inputs)
        embeddings.append(output.last_hidden_state[0,1])

        del inputs
        del image
        del output
        clear_memory()
        
        print(embeddings[0].shape)
    
    embeddings = torch.stack(embeddings)
    return embeddings



def eval_meth(classA, classB, num_images ):
    
    X= latent_embeddings_for_class( classA, num_images)
    Y= latent_embeddings_for_class( classB, num_images)
    
    cka_score = cka(X,Y)
    clear_memory()
    return cka_score
</code></pre>
<p>also, on CPU, this is the output -</p>
<pre><code>torch.Size([1, 3, 224, 224])
&lt;frozen importlib._bootstrap_external&gt;:647: size=3355 KiB, count=19535, average=176 B
c:\Users\hp\miniconda3\envs\mka_research\lib\ast.py:50: size=661 KiB, count=11348, average=60 B
c:\Users\hp\miniconda3\envs\mka_research\lib\selectors.py:315: size=288 KiB, count=6, average=48.0 KiB
torch.Size([768])
torch.Size([1, 3, 224, 224])
&lt;frozen importlib._bootstrap_external&gt;:647: size=3355 KiB, count=19535, average=176 B
c:\Users\hp\miniconda3\envs\mka_research\lib\ast.py:50: size=661 KiB, count=11348, average=60 B
c:\Users\hp\miniconda3\envs\mka_research\lib\tracemalloc.py:505: size=537 KiB, count=9815, average=56 B
torch.Size([768])
torch.Size([1, 3, 224, 224])
&lt;frozen importlib._bootstrap_external&gt;:647: size=3355 KiB, count=19535, average=176 B
c:\Users\hp\miniconda3\envs\mka_research\lib\ast.py:50: size=661 KiB, count=11348, average=60 B
c:\Users\hp\miniconda3\envs\mka_research\lib\tracemalloc.py:505: size=533 KiB, count=9732, average=56 B
torch.Size([768])
</code></pre>
<p>on GPU it shows -</p>
<pre><code>CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 144.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
<p>this is weird as this does not reflect the memory usage shown in task manager</p>
","0","Question"
"79396894","","<p>Currently working on a classifier using PyWavelets, here is my calculation block:</p>
<pre><code>class WaveletLayer(nn.Module):
    def __init__(self):
        super(WaveletLayer, self).__init__()

    def forward(self, x):
        def wavelet_transform(img):
            coeffs = pywt.dwt2(img.cpu().numpy(), &quot;haar&quot;)
            LL, (LH, HL, HH) = coeffs
            return (
                torch.from_numpy(LL).to(img.device),
                torch.from_numpy(LH).to(img.device),
                torch.from_numpy(HL).to(img.device),
                torch.from_numpy(HH).to(img.device),
            )

        # Apply wavelet transform to each channel separately
        LL, LH, HL, HH = zip(
            *[wavelet_transform(x[:, i : i + 1]) for i in range(x.shape[1])]
        )

        # Concatenate the results
        LL = torch.cat(LL, dim=1)
        LH = torch.cat(LH, dim=1)
        HL = torch.cat(HL, dim=1)
        HH = torch.cat(HH, dim=1)

        return torch.cat([LL, LH, HL, HH], dim=1)

</code></pre>
<p>The output from this module goes to a resnet block for learning, while doing this I find my CPU clogged and thus slowing down my training process</p>
<p>I am trying to use the GPUs for these calculations.</p>
","2","Question"
"79397701","","<p>So I am trying to install torch on one of my projects and my specs are as below</p>
<p>Python 3.12.3 /
pip latest /
Poetry (version 2.0.1)/
I am on Apple M3 Max</p>
<p>Here is my <em>pyproject.toml</em></p>
<pre><code>torch = { version = &quot;^2.5.1+cpu&quot;, source = &quot;pytorch-cpu&quot; }
torchvision = { version = &quot;^0.20.1+cpu&quot;, source = &quot;pytorch-cpu&quot; }
torchaudio = { version = &quot;^2.5.1+cpu&quot;, source = &quot;pytorch-cpu&quot; }

[[tool.poetry.source]]
name = &quot;pytorch-cpu&quot;
url = &quot;https://download.pytorch.org/whl/cpu&quot;
priority = &quot;explicit&quot;
</code></pre>
<p>Adding a screenshot for more context
<a href=""https://i.sstatic.net/82zj96dT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82zj96dT.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79399217","","<p>A deep neural network f consists of one fully connected network and one batch normalization layer. (e.g., f = FCN + BN)</p>
<p>Given a dataset with inputs x and y, can the following property hold?</p>
<pre><code>f(x+y) = f(x) + f(y) 
</code></pre>
<p>I think the fully connected network naturally preserves linearity. As for batch normalization, since x, y and x+y are normalized using the mean and standard deviation of the entire batch, they undergo normalization with the same mean and standard deviation. As a result, batch normalization also appears to be linear.</p>
<p>Is there a case where batch normalization preserves linearity, or does it introduce non-linearity that prevents this property from holding?</p>
","1","Question"
"79400482","","<p>Suppose i have a tensor <code>t</code> consisting only zeros and ones:</p>
<p><code>t = torch.Tensor([1, 0, 0, 1])</code></p>
<p>And a dict with the <code>weights</code>:</p>
<p><code>weights = {0: 0.1, 1: 0.9}</code></p>
<p>I want to form a new tensor <code>new_t</code>, such that every element in tensor <code>t</code> is mapped to the corresponding value in the dict <code>weights</code>:</p>
<p><code>new_t = torch.Tensor([0.9, 0.1, 0.1, 0.9])</code></p>
<p>Is there an elegant way to do this without iterating over tensor <code>t</code>? I've heard about <code>torch.apply</code>, but it only works if tensor <code>t</code> is on the CPU, is there any other options?</p>
","0","Question"
"79401951","","<p>I want to change onnx model and then restore the weights when using it with onnxruntime. However, the model doesn't seem to be changed.</p>
<p>First, I load the existing model and change the weights based on <a href=""https://github.com/onnx/onnx/issues/2978"" rel=""nofollow noreferrer"">this</a> output.
Then I save the model and try to check if the outputs are the same as in the original model.</p>
<p>The model could be restored (if restore_layer is True) with onnxruntime as in <a href=""https://stackoverflow.com/a/74139719/8668595"">this</a> example based on <a href=""https://github.com/microsoft/onnxruntime/issues/14545"" rel=""nofollow noreferrer"">this</a> output, but it doesn't seem to work, they are always the same and the model doesn't seem to be changed.
The expected behavior is when change_layer = True and restore_layer = False the script should raise an error, but it doesn't.</p>
<p>I tried to change a part of a model weights when loading it with onnxruntime like in <a href=""https://stackoverflow.com/a/74139719/8668595"">this</a> example based on <a href=""https://github.com/microsoft/onnxruntime/issues/14545"" rel=""nofollow noreferrer"">this</a> issue, but it doesn't seem to work.</p>
<p>How can I fix it?</p>
<p>Here is the code to reproduce.</p>
<pre class=""lang-py prettyprint-override""><code>import onnx # onnx==&quot;1.17.0&quot;
import torch # torch==&quot;2.3.0&quot;
from onnx import numpy_helper
import numpy as np # numpy==1.26.4
import torchvision.models as models # torchvision==0.18.0
from torchvision.models import ResNet18_Weights

change_layer = True
restore_layer = False
no_changed_layers = 20

model = models.resnet18(weights=ResNet18_Weights.DEFAULT)

model.eval()

input_data = torch.randn(1, 3, 224, 224)

torch.onnx.export(model, input_data, &quot;some_model.onnx&quot;, verbose=False)


MODEL_PATH = &quot;some_model.onnx&quot;
_model = onnx.load(MODEL_PATH)
INTIALIZERS=_model.graph.initializer
Weight= {}
initializer_num_name = {}
for num, initializer in enumerate(INTIALIZERS):
    W = numpy_helper.to_array(initializer)
    Weight[num] = W
    initializer_num_name[num] = initializer.name

Weight_num_name = {}
if change_layer:
    for weight_zeros in range(0, no_changed_layers):
        old_weight = numpy_helper.to_array(INTIALIZERS[weight_zeros])
        new_weight = np.zeros_like(Weight[weight_zeros])
        updated_weight = numpy_helper.from_array(new_weight)

        updating_weight_name = _model.graph.initializer[weight_zeros].name
        updated_weight.name = updating_weight_name
        Weight_num_name[weight_zeros] = updating_weight_name
        _model.graph.initializer[weight_zeros].CopyFrom(updated_weight)

onnx.save(_model, &quot;model.onnx&quot;)


import onnxruntime
import numpy as np


if restore_layer:
    options = onnxruntime.SessionOptions()
    # options.log_verbosity_level=2
    # options.log_severity_level=0
    # options.enable_profiling=1
    ortvalue_initializers = []
    for num in range(0, no_changed_layers):
        initializer = onnxruntime.OrtValue.ortvalue_from_numpy(
            Weight[num]
        )
        ortvalue_initializers.append(initializer)
        options.add_initializer(Weight_num_name[num], initializer)
    import logging
    logging.basicConfig(level=logging.DEBUG)
    onnx_session3 = onnxruntime.InferenceSession(&quot;model.onnx&quot;,
                                                 sess_options=options,
                                                 providers=[&quot;CPUExecutionProvider&quot;])
else:

    onnx_session3 = onnxruntime.InferenceSession(&quot;model.onnx&quot;,
                                                 providers=[&quot;CPUExecutionProvider&quot;])


onnx_session = onnxruntime.InferenceSession(MODEL_PATH)
input = onnx_session.get_inputs()[0]



generated_dummy_input_data = torch.randn(size=(input.shape[0],
                                               input.shape[1],
                                               input.shape[2],
                                               input.shape[3]))


onnx_inputs = {input.name: generated_dummy_input_data.numpy()}
onnx_output = onnx_session.run(None, onnx_inputs)

onnx_inputs3 = {onnx_session3.get_inputs()[0].name: generated_dummy_input_data.numpy()}
onnx_output3 = onnx_session3.run(None, onnx_inputs3)

def softmax(x: np.ndarray, axis: int = -1) -&gt; np.ndarray:
    x_max = np.max(x, axis=axis, keepdims=True)
    tmp = np.exp(x - x_max)
    s = np.sum(tmp, axis=axis, keepdims=True)
    return tmp / s

onnx_model_predictions = onnx_output[0][0]
onnx_model_predictions3 = onnx_output3[0][0]
diff = np.abs(onnx_model_predictions - onnx_model_predictions3)


assert (diff &gt; 1e-2).any(), &quot;Big difference between logits of an original model and onnx one: \n%s\n%s&quot;%(onnx_model_predictions[:50],
                                                                                       onnx_model_predictions3[:50])
print(&quot;all good&quot;)





</code></pre>
","3","Question"
"79403409","","<p>I have an LSTM model that receives 5 inputs to predict 3 outputs:</p>
<pre><code>import torch
import torch.nn as nn

class LstmModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CustomLSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        None
</code></pre>
<p>I want to prevent certain input from having any impact on a certain output. Let's say, the first input should not have any effect on the prediction of the second output. In other words, the second prediction should not be a function of the first input.</p>
<p>One solution I have tried is using separate LSTMs for each output:</p>
<pre><code>class LstmModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CustomLSTMModel, self).__init__()
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.lstm2 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.lstm3 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, output_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Assume x is of shape (batch_size, seq_length, input_size)
        # Split inputs
        input1, input2, input3, input4, input5 = x.split(1, dim=2)

        # Mask inputs for each output
        # For output1, exclude input2
        input1_for_output1 = torch.cat((input1, input3, input4, input5), dim=2)
        
        # For output2, exclude input3
        input2_for_output2 = torch.cat((input1, input2, input4, input5), dim=2)
        
        # For output3, exclude input4
        input3_for_output3 = torch.cat((input1, input2, input3, input5), dim=2)

        # Process through LSTM
        _, (hn1, _) = self.lstm1(input1_for_output1)
        output1 = self.fc1(hn1[-1])

        _, (hn2, _) = self.lstm2(input2_for_output2)
        output2 = self.fc2(hn2[-1])

        _, (hn3, _) = self.lstm3(input3_for_output3)
        output3 = self.fc2(hn3[-1])

        return output1, output2, output3
</code></pre>
<p>The problem with this approach is that it takes at least 3 times longer to run the model (since I am running LSTM 3 times, 1 for each output). Is it possible to do what I want to achieve more efficiently, with one run?</p>
","1","Question"
"79409149","","<p>I was doing the PyTorch Deep Learning course from FreeCodeCamp and the doubt is:</p>
<pre><code>weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y=weight*X + bias
X[:10], y[:10]
train_split=int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test=X[train_split:], y[train_split:]
</code></pre>
<p>Why the <strong>unsqueeze function</strong> is used to <strong>make the tensor of size [50, 1] and not [50]</strong>? The mentor was telling that it will cause error but I don't know why the error is happening?</p>
<p>Can you answer this question using maths and as well as basic fundamentals without maths as well ?</p>
<p>After trying to train the model I am getting this error:</p>
<pre><code>class LinearRegressionModelv2(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear_layer = nn.Linear(in_features=1, out_features=1)

  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    return self.linear_layer(x)

torch.manual_seed(42)
model_v2 = LinearRegressionModelv2()
</code></pre>
<p><code>y_prediction = model_v2(X_train)</code></p>
<p>IndexError: Dimension out of range (expected to be in range of [-1, 0], but got -2)</p>
","0","Question"
"79409259","","<p>In the configuration management library <a href=""https://hydra.cc/"" rel=""nofollow noreferrer"">Hydra</a>, it is possible to only partially instantiate classes defined in configuration using the <a href=""https://hydra.cc/docs/1.1/advanced/instantiate_objects/overview/#partial-instantiation-for-hydra-version--112"" rel=""nofollow noreferrer""><code>_partial_</code> keyword</a>. The library explains that this results in a <a href=""https://docs.python.org/3/library/functools.html#functools.partial"" rel=""nofollow noreferrer""><code>functools.partial</code></a>. I wonder how this interacts with seeding. E.g. with</p>
<ul>
<li><a href=""https://pytorch.org/docs/stable/notes/randomness.html"" rel=""nofollow noreferrer"">pytorch <code>torch.manual_seed()</code></a></li>
<li><a href=""https://pytorch-lightning.readthedocs.io/en/1.7.7/api/pytorch_lightning.utilities.seed.html#pytorch_lightning.utilities.seed.seed_everything"" rel=""nofollow noreferrer"">lightnings <code>seed_everything</code></a></li>
<li>etc.</li>
</ul>
<p>My reasoning is, that if I use the <code>_partial_</code> keyword while specifying <em>all</em> parameters for <code>__init__</code>, then I would essentially obtain a factory which could be called after specifying the seed to do multiple runs. But this assumes that <code>_partial_</code> does not bake the seed in already. To my understanding that should not be the case. Is that correct?</p>
","2","Question"
"79410822","","<p>I am trying to understand the example REINFORCE PyTorch implementation on PyTorch GitHub: <a href=""https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py"" rel=""nofollow noreferrer"">https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py</a></p>
<p>One particular point is a sticking point I am unable to understand at line 75.</p>
<pre><code>policy_loss.backward()
</code></pre>
<p>There are many non-tensor variables from state input to policy all the way to policy_loss.backward() which would stop autograd from back propagating, based on my understanding.</p>
<p>Eg, policy_loss.backward() is called on policy_loss, derived from policy.saved_log_probs and returns</p>
<pre><code>    for log_prob, R in zip(policy.saved_log_probs, returns):
        policy_loss.append(-log_prob * R)
</code></pre>
<p>policy.saved_log_probs is a non-tensor</p>
<pre><code>self.saved_log_probs = []
</code></pre>
<p>And so is returns, which in turn is calculated from policy.rewards (which is a non-tensor).</p>
<pre><code>        self.rewards = []
</code></pre>
<pre><code>    for r in policy.rewards[::-1]:
        R = r + args.gamma * R
        returns.appendleft(R)
</code></pre>
<p>So how would autograd back prop past these all the way to affine1 linear layer’s weights?</p>
<pre><code>class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.affine1 = nn.Linear(4, 128)
</code></pre>
","1","Question"
"79411478","","<p>I trained a CNN for emotion recognition and used two different transformation pipelines for image preprocessing:</p>
<p>Simple Transformation:</p>
<pre><code>TRANSFORM = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.Grayscale(num_output_channels=1), 
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485], std=[0.229])
])
</code></pre>
<p>Extended Data Augmentation:</p>
<pre><code>TRANSFORM = transforms.Compose([
    transforms.Resize((64, 64)),  
    transforms.Grayscale(num_output_channels=1), 
    transforms.RandomHorizontalFlip(p=0.5), 
    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1), fill=0),
    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1, 1.0)),
    transforms.ToTensor(), 
    transforms.Normalize(mean=[0.5], std=[0.5]),
])
</code></pre>
<p>I trained my model with both transformations separately and obtained two different accuracy curves for training and validation.</p>
<p>What key factors should I consider when <strong>interpreting the differences in these accuracy curves</strong>?
<a href=""https://i.sstatic.net/7TGvcIeK.png"" rel=""nofollow noreferrer"">Simple_Transformation_Curve</a>
<a href=""https://i.sstatic.net/WiBNT8wX.png"" rel=""nofollow noreferrer"">Augmenatation_Curve</a></p>
","1","Question"
"79413251","","<p>I am learning through PyTorch's seq2seq tutorial: <a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a></p>
<p>I have a question about the decoder</p>
<pre><code>class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size, output_size)

    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):
        batch_size = encoder_outputs.size(0)
        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)
        decoder_hidden = encoder_hidden
        decoder_outputs = []

        for i in range(MAX_LENGTH):
            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)
            decoder_outputs.append(decoder_output)

            if target_tensor is not None:
                # Teacher forcing: Feed the target as the next input
                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing
            else:
                # Without teacher forcing: use its own predictions as the next input
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(-1).detach()  # detach from history as input

        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)
        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop

</code></pre>
<p>Why is it that <code>if target_tensor is not None</code>:</p>
<pre><code>decoder_input = target_tensor[:, i].unsqueeze(1)
</code></pre>
<p>but <code>if target_tensor is None</code>:</p>
<pre><code>_, topi = decoder_output.topk(1)
decoder_input = topi.squeeze(-1).detach()
</code></pre>
<p>specifically, isn't the shape of decoder_input different in both cases?</p>
<p>I feel like in the first case, the shape of decoder_input is a 2D tensor but 1D in the second case.</p>
","0","Question"
"79415785","","<p>I'm using an iterableDataset because I have massive amounts of data. And since IterableDataset does not store all data in memory, we cannot directly compute min/max on the entire dataset before training. That is because for min-max we need to calculate the min x value and max x value observed in the data. My question would be how would you apply min-max scaling then?</p>
<p>How would you go on about that?</p>
<p>I'm unsure on how to solve this problem since I really have to scale the data as well.</p>
","0","Question"
"79415897","","<p>I am trying to run python using Jupyter-lab locally.</p>
<p>I installed PyTorch on my Mac using brew. However pip doesn't see it and so does python</p>
<p>I installed PyTorch using pipx</p>
<p>python inside Jupyter doesn't see torch and neither does python from the command line</p>
<p>In Jupyter I see this -</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Cell In[1], line 2
  1 # First, import PyTorch
  ----&gt; 2 import torch

ModuleNotFoundError: No module named 'torch'
</code></pre>
<p>From the command line I see this --</p>
<pre><code>&gt;&gt;&gt; import torch
Traceback (most recent call last):
File &quot;&lt;python-input-0&gt;&quot;, line 1, in &lt;module&gt;
import torch
ModuleNotFoundError: No module named 'torch'
</code></pre>
","0","Question"
"79416003","","<p>I have been trying out some of the Torchaudio functionalities and I can't seem to figure out why <code>lowpass_biquad</code> is running slower on the GPU than on the CPU. And this is true for other effects like, phaser, flanger, overdrive, which are even slower. Here I am pasting the example for the lowpass filter, but I apply the other effects the same way to obtain the measurements. The example code is taken from <a href=""https://github.com/pytorch/audio/issues/1408"" rel=""nofollow noreferrer"">this issue</a>, which seems to have been resolved, but it's still slower on the GPU.</p>
<pre><code>import time
import torch
from torchaudio.functional import lowpass_biquad

gpu_device = torch.device('cuda:0')
cpu_device = torch.device('cpu')

seconds = 1000
sample_rate = 44100
cutoff_freq = 1000.
Q = .7

# Run in cpu
x = torch.rand(sample_rate * seconds, device=cpu_device)
begin = time.time()
y = lowpass_biquad(x, sample_rate, cutoff_freq, Q)

print(f'Run in cpu: {time.time() - begin}')

# Run in gpu
x = torch.rand(sample_rate * seconds, device=gpu_device)
begin = time.time()
y = lowpass_biquad(x, sample_rate, cutoff_freq, Q)
torch.cuda.synchronize()
print(f'Run in gpu: {time.time() - begin}')
</code></pre>
<hr />
<pre><code>Run in cpu: 1.6084413528442383
Run in gpu: 6.183292865753174
</code></pre>
<p>For example for the overdrive effect, the GPU is more than 1000x slower. It would be understandable, if the Torchaudio doesn't have the GPU implementation of the said effects, but their documentation seems to suggest they do. Am I doing something wrong?</p>
","3","Question"
"79417066","","<p>In this example, I want to multiply each of the 10 (batch size) 3x3 matrices with the corresponding scalar. Is there a better solution without having to unsqueeze twice?</p>
<pre><code>import torch

# Create a batch of scalars (e.g., 10 scalars)
batch_size = 10
scalars = torch.randn(batch_size)  # Shape: (10,)

# Create a batch of 3x3 second-order tensors (e.g., 10 matrices of size 3x3)
tensors = torch.randn(batch_size, 3, 3)  # Shape: (10, 3, 3)

# Do we really have to unsqueeze twice to perform element wise multiplication?
result = scalars.unsqueeze(1).unsqueeze(2) * tensors  # Shape (10, 3, 3)
</code></pre>
","0","Question"
"79417386","","<p>I'm trying to train the CV-model with standard MNIST-data:</p>
<pre><code> import torch
 from torchvision.datasets import MNIST
 import torchvision.transforms as transforms

 img_transforms = transforms.Compose([
     transforms.ToTensor(),
     transforms.Normalize((0.1305,), (0.3081,))
 ])

 train_dataset = MNIST(root='../mnist_data/',
                  train=True,
                  download=True,
                  transform=img_transforms)

 train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                       batch_size=10,
                                       shuffle=True)
</code></pre>
<p>Model is declared as:</p>
<pre><code>import torch.nn as nn

class MNIST_ConvNet(nn.Module):
  def __init__(self):
     super().__init__()
     self.conv1 = ConvLayer(1, 14, 5, activation=nn.Tanh(),
                           dropout=0.8)
     self.conv2 = ConvLayer(14, 7, 5, activation=nn.Tanh(), flatten=True,
                           dropout=0.8)
     self.dense1 = DenseLayer(28 * 28 * 7, 32, activation=nn.Tanh(),
                             dropout=0.8)
     self.dense2 = DenseLayer(32, 10)

  def forward(self, x: Tensor) -&gt; Tensor:
     assert_dim(x, 4)

     x = self.conv1(x)
     x = self.conv2(x)

     x = self.dense1(x)
     x = self.dense2(x)
     return x
</code></pre>
<p>Then I invoke forward and estimate loss for this model, in accordance with pytorch approach:</p>
<pre><code>import torch.optim as optim

model = MNIST_ConvNet()
for X_batch, y_batch in train_dataloader:

                optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
                optimizer.zero_grad()
                output = model(X_batch)[0]
                loss = nn.CrossEntropyLoss()
                loss = loss(output, y_batch)
</code></pre>
<p>X_batch has the following content:</p>
<pre><code>tensor([[[[-0.4236, -0.4236, -0.4236,  ..., -0.4236, -0.4236, -0.4236],
      [-0.4236, -0.4236, -0.4236,  ..., -0.4236, -0.4236, -0.4236],
      [-0.4236, -0.4236, -0.4236,  ..., -0.4236, -0.4236, -0.4236],
      ...,
</code></pre>
<p>And for this line of code &quot;self.loss(output, y_batch)&quot;, I receive the following error:</p>
<blockquote>
<p>RuntimeError: Expected floating point type for target with class probabilities, got Long</p>
</blockquote>
<p>To solve the problem, I tried update data type:</p>
<pre><code> self.model(X_batch.type(torch.FloatTensor))[0]
</code></pre>
<p>But this does not working.</p>
","0","Question"
"79417996","","<p>In my framework, I am having an outer loop (here mocked by the variable <code>n</code>) and inside the loop body I have to perform matrix inversions/multiplications for multiple batch dimensions. I observed that manually looping over the batch dimensions and computing the inverse is measurable faster than passing all batches to the <code>torch.linalg.inv</code> function.
Similar statements can be made for computing matrix multiplications using <code>torch.einsum</code> function.</p>
<p>My expectation was that passing all batches at once performs better than for-loops. Any ideas/explanations/recommendations here?</p>
<p>Profiling output of function <code>inverse_batch</code>:</p>
<pre><code>ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    2.112    0.021    2.112    0.021 {built-in method torch._C._linalg.linalg_inv}
        1    0.000    0.000    2.112    2.112 mwe.py:5(inverse_batch)
        1    0.000    0.000    2.112    2.112 {built-in method builtins.exec}
        1    0.000    0.000    2.112    2.112 &lt;string&gt;:1(&lt;module&gt;)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
</code></pre>
<p>Profiling output of function <code>inverse_loop</code>:</p>
<pre><code>ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     8000    0.207    0.000    0.207    0.000 {built-in method torch._C._linalg.linalg_inv}
        1    0.022    0.022    0.229    0.229 mwe.py:9(inverse_loop)
        1    0.000    0.000    0.000    0.000 {method 'view' of 'torch._C.TensorBase' objects}
        1    0.000    0.000    0.229    0.229 {built-in method builtins.exec}
        1    0.000    0.000    0.229    0.229 &lt;string&gt;:1(&lt;module&gt;)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
</code></pre>
<p>Code:</p>
<pre><code>import torch
import cProfile
import pstats

def inverse_batch(tensors, n):
    for i in range(n):
        torch.linalg.inv(tensors)

def inverse_loop(tensors, n):
    tensors = tensors.view(-1, 3, 3)
    for i in range(n):
        for j in range(10 * 8):
            torch.linalg.inv(tensors[j])


# Create a batch of tensors
tensors = torch.randn(10, 8, 3, 3, dtype = torch.double)  # Shape: (10, 8, 3, 3)

# Profile code
n = 100 # Dummy outer loop variable
cProfile.run('inverse_batch(tensors, n)', 'profile_output')
stats = pstats.Stats('profile_output')
stats.strip_dirs().sort_stats('tottime').print_stats()
</code></pre>
","0","Question"
"79418845","","<p><strong>General question</strong> (hopefully useful for people coming from google): What to do when the gradient explodes? When working with transformers and deep NNs (with PyTorch), do you have a mental checklist of things to investigate and try when your gradient explodes (and when your loss becomes NaN)?</p>
<p><strong>More context</strong> (my specific situation):
I am training a model to take chemical formulas as input, represent the elements as embedding vectors and feed sequences of embedding vectors into several transformers until finally a MLP predicts a numerical property of the compound with that chemical formula.</p>
<p><em>Structure:</em></p>
<pre><code>class BandgapPredictionModel(nn.Module):
    def __init__(self, num_elements, embedding_dim, num_heads, num_layers, num_queries):
        super(BandgapPredictionModel, self).__init__()
        self.element_embedding = ElementEmbedding(num_elements, embedding_dim)
        self.attention_block = SelfAttentionBlock(embedding_dim, num_heads, num_layers)
        self.motif_discovery = MotifDiscovery(embedding_dim, num_queries)
        self.aggregation = HierarchicalAggregation(embedding_dim)
        self.prediction = PredictionMLP(embedding_dim)
    
    def forward(self, element_ids):
        embeddings = self.element_embedding(element_ids)  # Step 1
        mask = (element_ids == 0)
        attended_elements = self.attention_block(embeddings, src_key_padding_mask=mask)  # Step 2
        motifs = self.motif_discovery(attended_elements)  # Attention block (5 queries)
        #aggreagtion and attention of attended elemental embeddings and motif embeddings
        global_representation = self.aggregation(motifs)
        bandgap = self.prediction(global_representation).squeeze(-1) # Step 5
        return bandgap
</code></pre>
<p>and when called with lr=0.0001</p>
<pre><code>BandgapPredictionModel(num_elements=118, embedding_dim=32, 
                                   num_heads=4, num_layers=3, num_queries=5)
</code></pre>
<p>The model performed reasonably well on 10000 entries (sampled from bigger dataset with a fixed random seed) but on more data it would begin to given NaN loss.
The logs show:</p>
<pre><code>2025-02-06 17:59:08,309 - INFO - Predictions: no nan in predictions
2025-02-06 17:59:08,312 - INFO - NaN detected in gradient for parameter: element_embedding.embedding.weight
2025-02-06 17:59:08,313 - INFO - NaN detected in gradient for parameter: attention_block.transformer.layers.0.self_attn.in_proj_weight
2025-02-06 17:59:08,313 - INFO - NaN detected in gradient for parameter: attention_block.transformer.layers.0.self_attn.in_proj_bias
</code></pre>
<p><strong>Summary:</strong></p>
<p>Complex transformer model gives NaN gradient after a while. What steps to take?</p>
<p><strong>Steps Taken</strong></p>
<p>Reduced Learning rate to 0.0001 did not solve the problem.</p>
<p><strong>Weight Initialization</strong></p>
<p>Used Xavier init method for the weights.</p>
<p><strong>Self Attention Norm</strong>
self.norm = nn.LayerNorm(embedding_dim) in my Self Attention block</p>
<p><strong>Gradient Clipping</strong>
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1000000.0)
This helped when training with less samples but with more data, the clipping resulted in the model predicting NaN.
<em>How to choose the norm?</em></p>
<p>I hope this question respects the rules (I am new)! I am ready to edit if needed.</p>
<p><strong>Final Note</strong>: This model actually worked quite well when I got it to run on 10 000 samples so I don't think the architecture is completely stupid.
<a href=""https://i.sstatic.net/tr74n9Jy.png"" rel=""nofollow noreferrer"">Predicted VS Actual</a></p>
","1","Question"
"79419018","","<p>I trained a YOLOv8 detection model with 3 classes, but the raw forward pass still shows a final detect output of (1, 7, 8400) instead of (1, 8, 8400).</p>
<p>What I’ve Done:
Checked my data.yaml:</p>
<pre class=""lang-yaml prettyprint-override""><code>train: path/to/train/images
val: path/to/val/images
nc: 3
names: ['glioma', 'meningioma', 'pituitary']
</code></pre>
<p>Confirmed nc: 3 is correct.
Trained from scratch with the command:</p>
<pre class=""lang-bash prettyprint-override""><code>yolo detect train \
    data=path/to/data.yaml \
    model=yolov8x \
    epochs=1000 \
    imgsz=640 \
    device=1 \
    patience=100
</code></pre>
<p>The training runs without error and completes successfully.
Installed the latest Ultralytics version (v8.3.72) to ensure no version issues:</p>
<pre class=""lang-bash prettyprint-override""><code>pip uninstall ultralytics
pip install ultralytics
</code></pre>
<p>Loaded the new best.pt directly:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO
import torch

model = YOLO(r&quot;best.pt&quot;).model
model.eval()

dummy_input = torch.randn(1, 3, 640, 640)
with torch.no_grad():
    outputs = model(dummy_input)

for out in outputs:
    # Some outputs are lists; checking each element carefully
    if isinstance(out, torch.Tensor):
        print(out.shape)
    else:
        print(&quot;List output:&quot;, [o.shape for o in out if hasattr(o, 'shape')])
</code></pre>
<p>The console shows (1, 7, 8400) for the detection output.
Verified model metadata says nc=3 and model.names has 3 classes. However, the raw detect layer output is still 7 channels.</p>
<p>Observations:
If a YOLO detect layer is genuinely for 3 classes, it should output (5 + 3)=8 channels per anchor, not 7.
The mismatch (1, 7, 8400) typically indicates it’s still set for 2 classes despite nc=3.</p>
<p>Question / Request for Help:
Why is the raw detect head still (1, 7, 8400) even though I trained from scratch for 3 classes?
How can I ensure the detect layer is fully re-initialized to (5 + 3)=8 for 3-class detection?
I’ve tried deleting old .pt files, re-checking my data.yaml, reinstalling ultralytics, and confirming model.model.nc == 3. But the final detect layer continues to yield 7 channels instead of 8.</p>
<p>Any ideas on what might cause this persistent mismatch?</p>
","1","Question"
"79421842","","<p>Is their an equivalent for <code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot;</code> in the os module for isolating GPUs when using AMD ROCm?</p>
","3","Question"
"79424280","","<p>everyone! I have a problem - I am trying to make pytorch run on GPU, but I got no success.</p>
<p>I innstalled CUDA version 12.6, downloaded CUDA 12.6 compatible pytorch, reinstalled all things a few times, but python still outputs that no CUDA found. What else could I do?
My specs - RTX 3060, 24 GB RAM, AMD Ryzen 7. Windows 11 machine
I am trying to run this code:</p>
<pre><code>import torch

print(f&quot;Is CUDA supported by this system?{torch.cuda.is_available()}&quot;)
print(f&quot;CUDA version: {torch.version.cuda}&quot;)

# Storing ID of current CUDA device
cuda_id = torch.cuda.current_device()
print(f&quot;ID of current CUDA device:{torch.cuda.current_device()}&quot;)

print(f&quot;Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}&quot;)`
</code></pre>
<p>And keep getting this response:</p>
<pre><code>C:\\Users\\Valen\\PycharmProjects\\godhelpme.venv\\Scripts\\python.exe &quot;D:\\Programming shit\\godhelpme\\torchtest.py&quot;
Traceback (most recent call last):
File &quot;D:\\Programming shit\\godhelpme\\torchtest.py&quot;, line 7, in \&lt;module\&gt;
cuda_id = torch.cuda.current_device()
File &quot;C:\\Users\\Valen\\PycharmProjects\\godhelpme.venv\\Lib\\site-packages\\torch\\cuda\__init_\_.py&quot;, line 971, in current_device
_lazy_init()
\~\~\~\~\~\~\~\~\~\~^^
File &quot;C:\\Users\\Valen\\PycharmProjects\\godhelpme.venv\\Lib\\site-packages\\torch\\cuda_init_.py&quot;, line 310, in \_lazy_init
raise AssertionError(&quot;Torch not compiled with CUDA enabled&quot;)
AssertionError: Torch not compiled with CUDA enabled
Is CUDA supported by this system?False
CUDA version: None

Process finished with exit code 1
</code></pre>
","0","Question"
"79428887","","<p>On one Nvidia 4070 Super with 12GB VRAM, the loading of deepseek &quot;deepseek-ai/deepseek-llm-7b-chat&quot; model constantly throws error even the GPU VRAM is set at as low as 4GB in max_memory:</p>
<pre><code>Error loading quantized model: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. 
</code></pre>
<p>here is the portion of the code:</p>
<pre><code>                 model_name: str = &quot;deepseek-ai/deepseek-llm-7b-chat&quot;,
                 quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,  # You can try load_in_8bit=True if needed
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type=&quot;nf4&quot;  # Use NormalFloat4 quantization
                        )

                        # Step 2: Load config only (avoids full model load)
                        config = AutoConfig.from_pretrained(model_name) #, low_cpu_mem_usage=True)
                        with init_empty_weights():
                            model = AutoModelForCausalLM.from_config(config)

                        # Step 3: Infer the device map **without fully loading the model**
                        device_map1 = infer_auto_device_map(
                            model,
                            max_memory={0: &quot;4GB&quot;, &quot;cpu&quot;: &quot;32GB&quot;}  # Adjust based on available resources
                        )
                        device_map_update = device_map1
                        print(f&quot;Custom device map1 in 4bit: {device_map1}&quot;)
                        
                        torch.cuda.empty_cache()
                        model = AutoModelForCausalLM.from_pretrained(. ##&lt;&lt;== loading error
                            model_name,
                            #config=config,
                            quantization_config=quantization_config,
                            trust_remote_code=True,
                            device_map=device_map1,  # Let Transformers manage GPU allocation
                            **kwargs
                        )
</code></pre>
<p>Here is the output of Nvidia-smi and the GPU Vram occupied was still over 10GB:</p>
<pre><code>+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0 Off |                  N/A |
|  0%   37C    P8              4W /  220W |   10467MiB /  12282MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     54363      C   ...aiworker9/code/py/myenv/bin/python3      10460MiB |
+-----------------------------------------------------------------------------------------+
</code></pre>
<p>The code seems OK to me but it constantly throws OOM error for CUDA. device_map=&quot;cpu&quot; works fine. What is missing here for CUDA?</p>
","0","Question"
"79431134","","<p>I define some named tuple like this:</p>
<pre><code>class checkpoint_t(NamedTuple):
    epoch: int
    model_state_dict: Dict[str, Any]
    optimizer_state_dict: Dict[str, Any]
    model_name: str | None = None

</code></pre>
<p>However I after save I cannot load this namedtuple via</p>
<pre><code>import torch
from train import checkpoint_t
with torch.serialization.safe_globals([checkpoint_t]):
    print(&quot;safe globals: &quot;, torch.serialization.get_safe_globals())
    checkpoint: checkpoint_t = torch.load(parsed_args.checkpoint, weights_only=True)
</code></pre>
<p>it's still saying:</p>
<blockquote>
<p>WeightsUnpickler error: Unsupported global: GLOBAL <code>__main__.checkpoint_t</code> was not an allowed global by default. Please use <code>torch.serialization.add_safe_globals([checkpoint_t])</code> or the <code>torch.serialization.safe_globals([checkpoint_t])</code> context manager to allowlist this global if you trust this class/function.</p>
</blockquote>
<p>any idea why? and how to fix this?</p>
<p><strong>update about why</strong>
The module train.py has an ifmain block, so the module can be executed as <code>python -m package.subpackage.train</code>. If one run it like this instead of using exposed entry point of console_scripts, the train module name becomes <code>__main__</code>.</p>
","1","Question"
"79433165","","<p>I am having a weird issue with PyTorch's autograd functionality when implementing a custom loss calculation on a second order differential equation. In the code below, predictions of the neural network are checked if they satisfy a second order differential equation. This works fine. However, when I want to calculate the gradient of the loss with respect to the predictions, I get an error indicating that there seems to be no connection between loss and <code>u</code> in the computational graph.</p>
<blockquote>
<p>RuntimeError: One of the differentiated Tensors appears to not have
been used in the graph. Set <code>allow_unused=True</code> if this is the desired
behavior.</p>
</blockquote>
<p>This doesn't make sense because the loss is directly dependent and calculated with the prior derivatives that originate from <code>u</code>. Deriving the loss with respect to <code>u_xx</code> and <code>u_t</code> works, deriving to <code>u_x</code> does NOT. We verified that <code>.requires_grad</code> is set to <code>True</code> for all variables (<code>X</code>, <code>u</code>, <code>u_d</code>, <code>u_x</code>, <code>u_t</code>, <code>u_xx</code>).</p>
<p>Why does this happen, and how to fix this?</p>
<p><strong>Main code:</strong></p>
<pre><code># Ensure X requires gradients
X.requires_grad_(True)

# Get model predictions
u = self.pinn(X)

# Compute first-order gradients (∂u/∂x and ∂u/∂t)
u_d = torch.autograd.grad(
    u,
    X,
    grad_outputs=torch.ones_like(u),
    retain_graph=True,
    create_graph=True,  # Allow higher-order differentiation
)[0]

# Extract derivatives
u_x, u_t = u_d[:, 0], u_d[:, 1]  # ∂u/∂x and ∂u/∂t

# Compute second-order derivative ∂²u/∂x²
u_xx = torch.autograd.grad(
    u_x,
    X,
    grad_outputs=torch.ones_like(u_x),
    retain_graph=True,
    create_graph=True,
)[0][:, 0]

# Diffusion equation (∂u/∂t = κ * ∂²u/∂x²)
loss = nn.functional.mse_loss(u_t, self.kappa * u_xx)

## THIS FAILS
# Compute ∂loss/∂u
loss_u = torch.autograd.grad(
    loss,
    u,
    grad_outputs=torch.ones_like(loss),
    retain_graph=True,
    create_graph=True,
)[0]

# Return error on diffusion equation
return loss
</code></pre>
<p><strong>Model:</strong></p>
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [1, 1]                    --
├─Linear: 1-1                            [1, 50]                   150
├─Tanh: 1-2                              [1, 50]                   --
├─Linear: 1-3                            [1, 50]                   2,550
├─Tanh: 1-4                              [1, 50]                   --
├─Linear: 1-5                            [1, 50]                   2,550
├─Tanh: 1-6                              [1, 50]                   --
├─Linear: 1-7                            [1, 50]                   2,550
├─Tanh: 1-8                              [1, 50]                   --
├─Linear: 1-9                            [1, 1]                    51
==========================================================================================
Total params: 7,851
Trainable params: 7,851
Non-trainable params: 0
Total mult-adds (M): 0.01
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.03
Estimated Total Size (MB): 0.03
==========================================================================================
</code></pre>
<p><strong>What we have already tried:</strong></p>
<p>Reverted to an older PyTorch version (tested on 2.5.0, and 1.13.1). Same issue.</p>
<p>Putting <code>.requires_grad_(True)</code> after every variable assignment. This did not help.</p>
<p>We also tried to replace the tensor slicing by multiplying with zero/one vectors without results. We though this slicing might disturb the computational graph breaking the connection to <code>u</code>.</p>
<pre><code># Extract derivatives
u_x, u_t = u_d[:, 0], u_d[:, 1]  # ∂u/∂x and ∂u/∂t

# Extract derivatives alternative
u_x = torch.sum(
    torch.reshape(torch.tensor([1, 0], device=u_d.device), [1, -1]) * u_d,
    dim=1,
    keepdim=True,
)
u_t = ...
</code></pre>
<p>Thanks for your help!</p>
","0","Question"
"79434940","","<p>I get the same results in both Julia and Python. Singular Value Decomposition is slower on the GPU than on the CPU for Float64 arrays. (Float32 arrays behave how one would expect, with the GPU being faster.)</p>
<p>Python benchmark code using PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code>import time
import torch
from torch.linalg import svd

f64 = torch.double
f32 = torch.float

cpu = torch.device(&quot;cpu&quot;)
gpu = torch.device(&quot;cuda&quot;)


# X = torch.rand(5_000, 10_000, dtype=f32)
X = torch.rand(5_000, 10_000, dtype=f64)

X_cpu = X.to(cpu)
X_gpu = X.to(gpu)

print(X_cpu.type())
# Warmup
U, Sig, Vt = svd(X_cpu, full_matrices = True)
# Timed run (CPU)
t1 = time.perf_counter()
U, Sig, Vt = svd(X_cpu, full_matrices = True)
t2 = time.perf_counter()
print(U.type())


print(X_cpu.type())
# Warmup
U, Sig, Vt = svd(X_gpu, full_matrices = True)
# Timed run (GPU)
t3 = time.perf_counter()
U, Sig, Vt = svd(X_gpu, full_matrices = True)
t4 = time.perf_counter()
print(U.type())

print(f&quot;Time CPU (s): {t2-t1}&quot;)
print(f&quot;Time GPU (s): {t4-t3}&quot;)
</code></pre>
<p>For the above Float64 array I get:</p>
<pre class=""lang-bash prettyprint-override""><code>Time CPU (s): 14.52491476599971
Time GPU (s): 56.79755901500175
</code></pre>
<p>If I use a Float32 array instead I get the much more reasonable looking:</p>
<pre class=""lang-bash prettyprint-override""><code>Time CPU (s): 9.301500292000128
Time GPU (s): 6.969021153003268
</code></pre>
<p>Although it's still a little surprising that using the GPU only speeds things up by a couple of seconds.</p>
<p>The Julia code gives similar results:</p>
<pre class=""lang-julia prettyprint-override""><code>using LinearAlgebra
using Flux
using CUDA
using cuDNN

X = rand(5_000, 10_000)
println(&quot;typeof(X): $(typeof(X))&quot;) 
# Warmup
U, Sig, V = LinearAlgebra.svd(X)
# Timed run
t1 = time_ns()
U, Sig, V = LinearAlgebra.svd(X)
t2 = time_ns()

println(&quot;typeof(U): $(typeof(U))&quot;) 

X_gpu = X |&gt; gpu |&gt; f64
println(&quot;typeof(X_gpu): $(typeof(X_gpu))&quot;) 

# Warmup
U, Sig, V = CUDA.svd!(X_gpu)
# Timed run
t3 = time_ns()
U, Sig, V = CUDA.svd!(X_gpu)
t4 = time_ns()
println(&quot;typeof(U): $(typeof(U))&quot;) 


println(&quot;Time CPU (s): $((t2-t1)/1e9)&quot;)
println(&quot;Time GPU (s): $((t4-t3)/1e9)&quot;)
</code></pre>
<p>For this Float64 array the GPU again takes much longer than the CPU:</p>
<pre class=""lang-bash prettyprint-override""><code>Time CPU (s): 28.641290506
Time GPU (s): 57.069009417
</code></pre>
<p>However, switching to Float32 again yields a reasonable result:</p>
<pre class=""lang-bash prettyprint-override""><code>Time CPU (s): 15.096364932
Time GPU (s): 7.283513658
</code></pre>
<p>Two questions:</p>
<ol>
<li>Why do Float64 arrays run so poorly on the GPU? I am using an NVIDIA 40 series GPU if that is relevant.</li>
<li>Is there any way of improving the performance of svd run on the GPU? (Especially for Float64 arrays, but speeding up svd for Float32 arrays would also be nice.) One possible way would be by changing how svd is performed. I checked and there don't seem to be any optional arguments available for CUDA.jl's svd function. I tried setting <code>full_matrices=False</code> for PyTorch's svd function, but I got the same results.</li>
</ol>
","-1","Question"
"79439095","","<p>After running this code within a jupyter notebook, it runs properly. However, the memory is still stored in the GPU. How do I get rid of this memory to clear up space on my GPU. Sorry if I am formatting this question poorly, I am not used to posting. Provided is the code:</p>
<pre><code>llm = LLM(
  model=model_path, 
  gpu_memory_utilization=0.7, 
  max_model_len=2048,
)

llm = LLM(model=model_path, dtype=torch.bfloat16, trust_remote_code=True, max_model_len=2048, quantization=&quot;bitsandbytes&quot;, load_format=&quot;bitsandbytes&quot;, gpu_memory_utilization = 0.8)
</code></pre>
<p>I tried deleting llm and clearing cache which decreases the allocated and chached memory, but I cannot rerun the LLM method as I get an OOM Error (the previous call still has stored memory).</p>
","0","Question"
"79440564","","<p>I want to install the python package 'mistral-inference'.</p>
<p>My computer has a GPU and is using windows 11.<br />
Python 3.13.2<br />
pip 24.3.1</p>
<p>I created a new virtual environnment.
Then I ran 'pip install mistral-inference'.
I got the following error:</p>
<pre><code>Collecting xformers&gt;=0.0.24 (from mistral-inference)
  Using cached xformers-0.0.29.post3.tar.gz (8.5 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error
  ...

ModuleNotFoundError: No module named 'torch'
</code></pre>
<p>I then tried to install 'torch' in my venv beforehand, I tried many versions of it (CPU, CUDA 11.8, CUDA 12.4, CUDA 12.6) but I always get the same error.</p>
<p>I also tried to download the source code of 'mistral-inference' from github, but got here the same kind of problem:</p>
<pre><code>Collecting jnius==1.1.0 (from -r .\requirements.txt (line 16))
  Using cached jnius-1.1.0.tar.gz (28 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error
  ...

ModuleNotFoundError: No module named 'Cython'
</code></pre>
<p>Same I tried to install first Cython, I tried the latest version, I tried version 0.29.37, always the same error.</p>
<p>What should I do?</p>
","0","Question"
"79441614","","<p>I've seen several questions about in-place operations regarding their efficiency, but I'm actually more confused about the inner-workings of pytorch.</p>
<p>Let's take a simple example like</p>
<pre><code>a = torch.randn(10)
b = torch.randn(10)
c = torch.randn(10)

x1 = a * b
x2 = x1 * c 
</code></pre>
<p>In this case, things are easy. Backpropagations happens like this:</p>
<pre><code>x2.grad &lt;- 1
c.grad &lt;- x2.grad * x1 = x1 = a * b
x1.grad &lt;- x2.grad * c = c
b.grad &lt;- x1.grad * a = c * a
a.grad &lt;- x1.grad * b = c * b
</code></pre>
<p>Everything works correctly. However, in this scenario we have are allocating two buffers: <code>x1</code> and <code>x2</code>. Now, what happen when we do something like this:</p>
<pre><code>x = a * b
x = x * c 
</code></pre>
<p>It seems to me that the overall expression is the same. However, if we try to compute the gradients the same way we did before, we will run into the following problem:</p>
<pre><code>x.grad &lt;- 1
c.grad &lt;- x.grad * x = x = a * b * c
</code></pre>
<p>Uh oh. we already got a mistake. Since we performed the multiplication with c in place, we lost the buffer containing <code>a * b</code> which was needed in order to calculate the gradient of <code>c</code>.</p>
<p>I can imagine two possible solutions to this:</p>
<ol>
<li><p>The previous code actually gets 'compiled' into something like 'x = a * b * c'. But I feel like this kind of optimization might fail when we try more complicated expressions.</p>
</li>
<li><p>The previous code actually uses intermediate buffers (like <code>x1</code>) instead.</p>
</li>
</ol>
<p>However, in this case, what happens if we try to compute something like.</p>
<pre><code>x = a * b
x *= c
x *= d
x *= e
x *= f 
</code></pre>
<p>Does pytorch actually create 3 temporary buffers (<code>x1</code>, <code>x2</code> and <code>x3</code>)?</p>
<p>How is this kind of problem solved in modern frameworks?</p>
","1","Question"
"79444383","","<p>I'm reading through a PyTorch tutorial for transfer learning and I'm having a hard time figuring out exactly what the following block is doing, with regards to generating the dataset:</p>
<pre><code># Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = 'data/hymenoptera_data'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes
</code></pre>
<p>Particularly, operations of form similar to (excuse any misuse of terminology - I'm still learning Python) {x: len(image_datasets[x]) for x in ['train', 'val']}.</p>
<p>It was explained to me before but I've since forgotten and I'm not sure how to phrase the question for a general internet search, so I'm asking here. I know that the code is defining a loop but the syntax is confusing me. Any clarification would be greatly appreciated.</p>
<p>I tried googling some stuff.</p>
","1","Question"
"79447650","","<p>I want to measure the memory consumption of a Python list, where each list member is a tuple, and each tuple has three Pytorch tensors, two being dense and the other being COO sparse.</p>
<p>I know sys.getsizeof(). But it doesn't work.</p>
<p>I want a convenient way to get the memory size of the whole list.</p>
","1","Question"
"79451638","","<p>I get a very weird behavior for a 'smaller' operation on a float torch tensor.
Consider the following snippet</p>
<pre><code>t = torch.load(r&quot;value.pt&quot;) 
print(t.shape, t.dtype) 
#t = t.double()
for i in range(t.shape[0]): 
    print(i, &quot;%.20f&quot; % (t[i].sum(-1)-1)) 
print((t.sum(-1)-1).abs()&lt;1e-6) 
print(&quot;%.8e&quot;%(t[35].sum()-1),
(t[35].sum(-1)-1).abs()&lt;1e-6, (t[34:50].sum(-1)-1).abs()&lt;1e-6,
(t[34:40].sum(-1)-1).abs()&lt;1e-6)
</code></pre>
<p>which produces the output</p>
<pre><code>torch.Size([100, 1600]) torch.float32

...
33 -0.00000008132246875903
34 0.00000014945180737413
35 0.00000053211988415569
36 -0.00000006957179721212
37 -0.00000010645544534782
38 -0.00000000481304596178
...

tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True, False,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True],
       device='cuda:0')
7.15255737e-07 
tensor(True, device='cuda:0') 
tensor([ True, False,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True], device='cuda:0') 
tensor([True, True, True, True, True, True], device='cuda:0')
</code></pre>
<p>That one of the entries of t deviates from 1 more than 1e-6 is firstly wrong, but that result also changes when I slice/index t differently. How does this make any sense?
When I convert the tensor to a double tensor the problem is gone..</p>
","3","Question"
"79451702","","<p>Why does the code shown below either finish normally or hang depending on which lines are commented/uncommented, as described in the table below?</p>
<p>Summary of table: if I initialise sufficiently large tensors in both processes without using <code>&quot;spawn&quot;</code>, the program hangs. I can fix it by making either tensor smaller, or by using <code>&quot;spawn&quot;</code>.</p>
<p>Note:</p>
<ol>
<li>All memory is purely CPU, I don't even have CUDA installed on this computer</li>
<li>This issue does not occur if I replace <code>torch</code> with <code>numpy</code>, even if I make the array size 10x larger</li>
<li>Version information: <code>Ubuntu 22.04.1 LTS, Python 3.10.12, torch 2.1.2+cpu</code></li>
</ol>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Uncommented</th>
<th>Commented</th>
<th>Behaviour</th>
</tr>
</thead>
<tbody>
<tr>
<td>(1), (4)</td>
<td>(2), (3), (5)</td>
<td>Hang</td>
</tr>
<tr>
<td>(2), (4)</td>
<td>(1), (3), (5)</td>
<td>OK</td>
</tr>
<tr>
<td>(1), (5)</td>
<td>(2), (3), (4)</td>
<td>OK</td>
</tr>
<tr>
<td>(1), (3), (4)</td>
<td>(2), (5)</td>
<td>OK</td>
</tr>
</tbody>
</table></div>
<pre class=""lang-py prettyprint-override""><code>import multiprocessing as mp
import torch

def train():
    print(&quot;start of train&quot;)
    x = torch.arange(100000)            # (1)
    x = torch.arange(10000)             # (2)
    print(&quot;end of train&quot;)

if __name__ == &quot;__main__&quot;:
    mp.set_start_method('spawn')        # (3)
    x = torch.arange(100000)            # (4)
    x = torch.arange(10000)             # (5)
    p = mp.Process(target=train)
    p.start()
    p.join()
</code></pre>
","1","Question"
"79451900","","<p>I am trying to use a vision transformer repository (<a href=""https://github.com/wangjian123799/L-DETR"" rel=""nofollow noreferrer"">https://github.com/wangjian123799/L-DETR</a>) which uses a version of <code>torch</code> and <code>torchvision</code>. Anyway, because exact versions are not given and torch is super fast at deprecating versions I guess I am in the unpleasant situation to try to find what combination of those two packages would work for me.</p>
<p>The error message is this one:</p>
<pre><code>cannot import name '_new_empty_tensor' from 'torchvision.ops'
</code></pre>
<p>The simpler question would be which is the last version of torchvision that includes <code>_new_empty_tensor</code>?</p>
<p>But I would be happy with a workaround. For example inspecting the code I found the usage for the later to be:</p>
<pre><code>_new_empty_tensor(input, output_shape)
</code></pre>
<p>which to me it just creates an empty tensor with the given dimensions. Correct me if I am wrong here. So, could I just use torch instead and create a torch Tensor instead? Would there be any issue? And as a side question why torchvision even introduced such functions? Isn't it supposed to just provide some additional functionality for images? Am I missing something here?</p>
","0","Question"
"79453088","","<p>I have images of graph lines with trends, and I want to cluster similar trends together. However, after trying several clustering algorithms, they are not working as well as I expected. I believe that effective feature extraction is crucial, but since the images have a black background with only the graph lines, it seems to be challenging. In clustering, I feel that while there are lines that trend upwards similarly, they do not cluster well together, and instead, almost identical graphs are clustered together.
I can provide an example of the images.</p>
<p><a href=""https://i.sstatic.net/z1sCHIW5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/KTdOrrGy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/9nZSEHQK.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I would like to know techniques that can effectively cluster such images.</p>
<p>This is the code I used for feature extraction and clustering.</p>
<p>I extracted features from the above images using ResNet and clustered them using DBSCAN.</p>
<pre><code>import os
import numpy as np
import torch
from torchvision import models, transforms
from sklearn.cluster import DBSCAN
from PIL import Image

def load_images_from_directory(directory):
    images = []
    for filename in os.listdir(directory):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(directory, filename)
            img = Image.open(img_path).convert('RGB')
            images.append((img, filename))
    return images

def extract_features(images):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = models.resnet18(pretrained=True)
    model = torch.nn.Sequential(*(list(model.children())[:-1]))  
    model = model.to(device)
    model.eval()

    preprocess = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    features = []

    with torch.no_grad():
        for img, filename in images:
            img_tensor = preprocess(img).unsqueeze(0).to(device)  
            feature = model(img_tensor).cpu().numpy()  
            features.append((feature.flatten(), filename)) 장

    return features

def cluster_images(features, eps=0.8, min_samples=3):
    feature_vectors = np.array([f[0] for f in features])  
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    labels = dbscan.fit_predict(feature_vectors)

    return labels

def save_clustered_images(images, labels, output_dir):

    os.makedirs(output_dir, exist_ok=True)
    noise_dir = os.path.join(output_dir, 'noise') 
    os.makedirs(noise_dir, exist_ok=True)

    unique_labels = set(labels)
    for label in unique_labels:
        if label == -1:  
            for i, (feature, filename) in enumerate(images):
                if labels[i] == -1:
                    img_path = os.path.join(noise_dir, filename)
                    images[i][0].save(img_path)  
            continue

        cluster_dir = os.path.join(output_dir, f'cluster_{label}')
        os.makedirs(cluster_dir, exist_ok=True)

        for i, (feature, filename) in enumerate(images):
            if labels[i] == label: 
                img_path = os.path.join(cluster_dir, filename)
                images[i][0].save(img_path) 

input_dir = 'images' 
output_dir = 'clusterd_images'  

images = load_images_from_directory(input_dir)
features = extract_features(images)
labels = cluster_images(features, eps=0.3, min_samples=3) 
save_clustered_images(images, labels, output_dir)
</code></pre>
<p>I'm new to image processing, so I would appreciate any advice from my seniors.</p>
<p>Since all the images have a black background with only blue lines, it is essential to extract features effectively, and I hope to cluster similar trends together during the clustering process.</p>
","-1","Question"
"79453817","","<p>I'm trying to export my PyTorch model to ONNX format while ensuring that the batch size remains dynamic. the result always is Segmentation fault (core dumped) Here’s the code I’m using:</p>
<pre><code>import torch  # Import PyTorch library

# Create a dummy input tensor of shape (1, 3, 256, 256) and move it to the appropriate device
dummy_input = torch.randn(1, 3, 256, 256).to(device)  

# Create dummy camera and view labels, initialized to zeros, and move them to the device
dummy_cam = torch.zeros(1, dtype=torch.long).to(device)  
dummy_view = torch.zeros(1, dtype=torch.long).to(device)  

# Export the model to ONNX format
torch.onnx.export(
    model,  # The PyTorch model to be converted
    (dummy_input, dummy_cam, dummy_view),  # The input tuple
    &quot;deit_transreid_veri.onnx&quot;,  # The output file name
    input_names=[&quot;input&quot;, &quot;cam_label&quot;, &quot;view_label&quot;],  # Naming the input tensors
    output_names=[&quot;output&quot;],  # Naming the output tensor
    dynamic_axes={  # Define dynamic batch size for inputs and outputs
        'input': {0: 'batch_size'},
        'cam_label': {0: 'batch_size'},
        'view_label': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }, 
    opset_version=13,  # Specify the ONNX opset version
    verbose=True,  # Print detailed information during export
)

print(&quot;Model has been successfully exported to ONNX format!&quot;)  # Confirmation message
</code></pre>
","2","Question"
"79455504","","<p>I would like to visualize the attention layer of a <code>Phi-3-medium-4k-instruct</code> (or mini) model downloaded from hugging-face. In particular, I am using the following <code>model, tokenizer</code>:</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import pdb

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-medium-4k-instruct&quot;)

model = AutoModelForCausalLM.from_pretrained(
    &quot;microsoft/Phi-3-meduium-4k-instruct&quot;,
    device_map = &quot;auto&quot;,
    torch_dtype = &quot;auto&quot;,
    trust_remote_code = True
)

# Create a pipeline
generator = pipeline(
    &quot;text-generation&quot;,
    model = model,
    tokenizer = tokenizer,
    return_full_text= False,
    max_new_tokens = 50,
    do_sample = False
)

prompt = &quot;...&quot;
input_ids = tokenizer(prompt, return_tensors = &quot;pt&quot;).input_ids
# tokenize the input prompt
input_ids = input_ids.to(&quot;cuda:0&quot;)
# get the output of the model
model_output = model.model(input_ids)

# extract the attention layer
attention = model_output[-1] 
</code></pre>
<p>Firstly, I am wondering if that is the correct way to extract attention from my model. What should expect from this model and how can I visualize it properly? Isn't that I should expect a matrix <code>n_tokens x n_tokens</code>?</p>
<p>The <code>attention</code> variable I have extracted has a size of <code>1x40x40x15x15</code> (or <code>1x12x12x15x15</code> in the case of <code>mini</code> model), where the first dimension corresponds to different layers the second for the different <code>heads</code>, and the final two for the <code>attention matrix</code>. That is actually my assumption and I am not sure whether it is correct. When I am visualizing the attention I am getting some very weird matrices like:</p>
<p><a href=""https://i.sstatic.net/JLZsHi2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JLZsHi2C.png"" alt=""enter image description here"" /></a></p>
<p>What we see in this Figure, I assume is all the heads for one layer. However, most of the heads distribute the attention equally to all the tokens. Does that make sense?</p>
<p>Edit: For the visualization I am doing sth like:</p>
<pre><code># Save attention visualization code 
def save_attention_image(attention, tokens, filename='attention.png'):
    &quot;&quot;&quot;
    Save the attention weights for a specific layer and head as an image.
    
    :param attention: The attention weights from the model.
    :param tokens: The tokens corresponding to the input.
    :param layer_num: The layer number to visualize.
    :param head_num: The head number to visualize.
    :param filename: The filename to save the image.
    &quot;&quot;&quot;

    attn = attention[0].detach().cpu().float().numpy()    
    num_heads = attn.shape[0]
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))  # Adjust the grid size as needed
    
    for i, ax in enumerate(axes.flat):
        if i &lt; num_heads:
            cax = ax.matshow(attn[i], cmap='viridis')
            ax.set_title(f'Head {i + 1}')
            ax.set_xticks(range(len(tokens)))
            ax.set_yticks(range(len(tokens)))
            ax.set_xticklabels(tokens, rotation=90)
            ax.set_yticklabels(tokens)
        else:
            ax.axis('off')
    
    fig.colorbar(cax, ax=axes.ravel().tolist())
    plt.suptitle(f'Layer {1}')
    plt.savefig(filename)
    plt.close()
</code></pre>
","3","Question"
"79457741","","<p>I'd like to custom a nn.Linear()'s backward function:</p>
<pre><code>class Linear(torch.autograd.Function):
    @staticmethod
    def forward(ctx, inputs, weight, bias):
        e = F.linear(inputs, weight, bias)
        ctx.save_for_backward(inputs, e, weight)
        return e

    @staticmethod
    def backward(ctx, grad):
        input, e, weight = ctx.saved_tensors
        grad_weight = grad.permute(0, 2, 1).matmul(input)    
        grad_bias = grad.sum(dim=(0))
        grad_input_t = grad.matmul(weight)   

        def check_numerical_stability(tensor, name):
            if torch.isnan(tensor).any() or torch.isinf(tensor).any():
                print(f&quot;Warning: {name} contains NaN or Inf values!&quot;)
                tensor = torch.where(torch.isnan(tensor), torch.zeros_like(tensor), tensor)
                tensor = torch.where(torch.isinf(tensor), torch.zeros_like(tensor), tensor)
            return tensor

        grad_input_t = check_numerical_stability(grad_input_t, &quot;grad_input_t&quot;)
        grad_weight = check_numerical_stability(grad_weight, &quot;grad_weight&quot;)
        grad_bias = check_numerical_stability(grad_bias, &quot;grad_bias&quot;)
        return grad_input_t, grad_weight, grad_bias
</code></pre>
<p>But after training with this function, the loss is larger than the loss trained by Automatic Differentiation with torch.autograd. I don't know why.</p>
<p>First, I check the equation of differentiation. I believe there's no problem. By the way, the inputs is 3D [batch_size, len_seq, embed_size].</p>
<p>Second, I tried to avoid 0 or INF in the backward.</p>
<p>Third, add weights initialization.</p>
<p>I don't know why.
Thank you for your help.</p>
","1","Question"
"79462366","","<p>I'm trying to install PyTorch with CUDA support on a Jetson Nano. It's running the Ubuntu 18.04 that the jetson-nano-jp461-sd-card-image image from Nvidia put on it with Python 3.6.9, CUDA 10.2, and JetPack 4.6.6. I tried:</p>
<blockquote>
<p>pip3 install torch --index-url <a href=""https://download.pytorch.org/whl/cpu"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/cpu</a></p>
</blockquote>
<p>but this gives me the CPU-only version, and I need CUDA support for the Nano's GPU. I also tried</p>
<blockquote>
<p>git clone --recursive --branch v1.8.0 <a href=""https://github.com/pytorch/pytorch"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch</a></p>
</blockquote>
<p>and installing, but it is really, really, slow.</p>
<p>Is there a pre-built wheel available for this specific configuration? Many thanks in advance.</p>
","1","Question"
"79463165","","<p>I am new to <code>pytorch</code> and <code>torchvision</code>.</p>
<p>Apparently, the <code>__call__</code> method of a model returns different things if they are on train mode or not. The first case returns the loss, the second returns the outputs.</p>
<p>This is quite bothersome for two reasons:</p>
<ol>
<li>I need to do very strange things to compute the validation loss to plot the history as one will do with TF,
<ul>
<li>either I have a big error if I have layers like dropout or batch norm</li>
<li>or using a function of 100 lines of code.</li>
</ul>
</li>
</ol>
<p>source: <a href=""https://stackoverflow.com/a/71315672/5931672"">https://stackoverflow.com/a/71315672/5931672</a>.</p>
<ol start=""2"">
<li>Also, if I want to use <code>torchmetrics</code> with the train data, I need to change to eval and send again the <code>train_dataloader</code>. Seems like a waste of computation time.</li>
</ol>
<p>Is this indeed the case? Otherwise, how can I do this two things using best practices? My code looks quite messy with all this.</p>
","0","Question"
"79463660","","<p>I am currently working on Adversarial Attacks on Image dataset. The most important libraries in my project are :</p>
<ul>
<li>Tensorflow 2.10.1 (with CUDA) (the error is with tensorflow)</li>
<li>Pytorch 1.13.1+cu116</li>
<li>art (adversarial-robustness-toolbox)</li>
</ul>
<p>With certains attacks, like DeepFool, Elastic Net (EAD) for examples, I got this error :</p>
<blockquote>
<p><code>WARNING:tensorflow: Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.</code></p>
</blockquote>
<p>And here is my code :</p>
<pre><code>import torch
import tensorflow as tf
import numpy as np

from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.inception_v3 import InceptionV3

from art.attacks.evasion import FastGradientMethod, BasicIterativeMethod, CarliniL2Method, CarliniLInfMethod, \
    ProjectedGradientDescent, DeepFool, ThresholdAttack, PixelAttack, SpatialTransformation, SquareAttack, ZooAttack, \
    BoundaryAttack, HopSkipJump, SaliencyMapMethod, SimBA, AutoProjectedGradientDescent, HopSkipJump, ElasticNet

from art.estimators.classification import TensorFlowV2Classifier


for i, image_path in enumerate(imagenet_stubs.get_image_paths()):
    im = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))
    im = tf.keras.preprocessing.image.img_to_array(im)
    if 'tractor.jpg' in image_path:
        x = np.array(im)
x = (np.expand_dims(x, axis=0) / 255.0).astype(np.float32)
y = np.array([name_to_label(&quot;tractor&quot;)])

model = InceptionV3(include_top=True, weights='imagenet', classifier_activation='softmax')
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)

classifier = TensorFlowV2Classifier(model=model,
                                    nb_classes=nb_classes,
                                    loss_object=loss,
                                    preprocessing=preprocessing,
                                    preprocessing_defences=None,
                                    clip_values=clip_values,
                                    input_shape=input_shape)

# DeepFool is an Untargeted Attack ONLY
attack = DeepFool(classifier=classifier,
                  epsilon=0.02,
                  max_iter=100,
                 )

# Generate adversarial examples (DeepFool is untargeted only)
x_adv = attack.generate(x=x) # UNTARGETED ATTACK
</code></pre>
<p>It's not an error, I know that the code is compiling. But it's SO LONG. With DeepFool, I can wait 20 minutes, but with EAD for example, after 2h, no results appearing. Always loading.</p>
<p>Summary:<br />
I tried to generate adversarial attacks, (DeepFool / EAD), but instead of having a quick attack on one picture, I got a <code>GradientTape</code> error, which results that the code is compiling for a few hours before getting some results, which is way too long.<br />
How can I resolve this WARNING to be able to generate this attack quickly?</p>
","1","Question"
"79464907","","<p>I have a pytorch training script, and I'm getting an out-of-memory error after a few epochs even tho I'm calling <code>torch.cuda.empty_cache()</code>. The GPU memory just keeps going up and I can't figure out why.</p>
<p>Here's basically what I'm doing:</p>
<pre><code>import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class CustomDataset(Dataset):
    def __init__(self, data_paths):
        self.data_paths = data_paths
    
    def __len__(self):
        return len(self.data_paths)
    
    def __getitem__(self, idx):
        image = np.load(self.data_paths[idx]['image']).astype(np.float32)
        label = np.load(self.data_paths[idx]['label']).astype(np.int64)
        
        image = torch.tensor(image).cuda()
        label = torch.tensor(label).cuda()
        
        return image, label

data_paths = [{'image': f'img_{i}.npy', 'label': f'label_{i}.npy'} for i in range(10000)]
dataset = CustomDataset(data_paths)
dataloader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)

for epoch in range(10):
    for batch in dataloader:
        images, labels = batch  
        
        output = images.mean()  
        loss = output.sum()
        loss.backward()
        
        del images, labels, loss, output
        torch.cuda.empty_cache()
</code></pre>
<p>Even after deleting everything and <code>calling empty_cache()</code>, the VRAM just keeps going up and I don't understand why. This doesn't happen on CPU. If I run nvidia-smi, the memory usage increases after every batch until it crashes.</p>
<p>I tried:</p>
<ul>
<li>Calling <code>del</code> on everything after every batch</li>
<li>Setting <code>num_workers=0</code> (didn't help)</li>
<li>Using <code>.detach()</code> before moving tensors to GPU</li>
<li>Checked if the issue is in my model, but even without the model, just loading the Data already makes the memory increase</li>
</ul>
<p>Anyone seen this before? Is there something about DataLoader and <code>cuda()</code> that could be causing this?</p>
<p>Would appreciate any ideas. I'm out of things to try</p>
","2","Question"
"79467972","","<pre><code>import torch

x = torch.ones(3, 3)

factors = [lambda x: 2*x, lambda x: 3*x, lambda x: 4*x]
indices = torch.tensor([0, 1, 2])

def multiply_row_by_factor(row, idx):
    return factors[idx](row)

result = torch.vmap(multiply_row_by_factor, in_dims=(0, 0))(x, indices)

# Original Tensor
# tensor([[1., 1., 1.],
#         [1., 1., 1.],
#         [1., 1., 1.]])

# Desired Result
# tensor([[2., 2., 2.],
#         [3., 3., 3.],
#         [4., 4., 4.]])
</code></pre>
<p>As the title states, I am looking for a way to call multiple functions on multiple rows of a tensor. I show a minimally reproducible example for simplicity. I am aware that vmap is only meant to be called with one function. I am just using it here as an example to communicate what I am trying to do. This particular approach doesn't work due to the fact that idx is a BatchedTensor. The functions here are lambdas, but in reality, my functions are composed of complex transformations that I would prefer not to decompose to get this to work.</p>
<p>Is there any way to achieve something like this? Something cleaner than pytorch streams? Specifically calling the functions in parallel?</p>
","1","Question"
"79470053","","<p>I am training a Mamba model on two different GPU architectures: RTX 4090 and RTX A6000. Despite setting all random seeds and using deterministic algorithms, I am observing significant non-deterministic behavior in the training process on the RTX 4090, while the RTX A6000 exhibits almost deterministic results. This issue is due to the use of atomic adds in the backward pass of the Mamba model, as discussed in this <a href=""https://github.com/state-spaces/mamba/issues/137"" rel=""nofollow noreferrer"">GitHub issue</a>.</p>
<p>I would like to maintain the current hyperparameters, as they provide the best performance. Therefore, I am looking for any tricks or settings that can make the RTX 4090 behave more like the RTX A6000 environment to achieve consistent results.
Here are the details of my setup:</p>
<p><strong>RTX4090</strong></p>
<pre class=""lang-none prettyprint-override""><code>| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4090        On  | 00000000:26:00.0 Off |                  Off |
| 30%   22C    P8              12W / 450W |     17MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    426218      G   /usr/lib/xorg/Xorg                            4MiB |
+---------------------------------------------------------------------------------------+
</code></pre>
<p><strong>RTXA6000</strong></p>
<pre class=""lang-none prettyprint-override""><code>| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A6000               On  | 00000000:24:00.0 Off |                  Off |
| 30%   22C    P8              25W / 300W |     12MiB / 49140MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      7388      G   /usr/lib/xorg/Xorg                            4MiB |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>So far I've tried setting:</p>
<pre><code>import os
import torch

os.environ[&quot;CUBLAS_WORKSPACE_CONFIG&quot;] = &quot;:4096:8&quot;
torch.backends.cuda.matmul.allow_tf32 = False
torch.set_default_dtype(torch.float32)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
</code></pre>
<p>I've also tried using
<code>torch.use_deterministic_algorithms(True)</code>, but it raise the error</p>
<pre><code>  File &quot;/path/mamba_ssm/ops/triton/ssd_chunk_state.py&quot;, line 845, in _chunk_state_bwd_db
    torch.cumsum(ddA_cumsum, dim=-1, out=ddA_cumsum)
RuntimeError: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.
</code></pre>
<p>Any help is appreciated.</p>
","1","Question"
"79473125","","<p>I have a combined loss funcation like this:</p>
<pre><code>loss = alpha * loss0 + beta* loss1 + gamma * loss2 + delta* loss3
</code></pre>
<p>I would like to make alpha, beta, gamma, and delta learnable parameters. Notice that alpha, beta, gamma, and delta are outside the nn.Module. How can I do that?</p>
","0","Question"
"79473994","","<p>Consider the Multi-Label Classification with ANN where the targeted labels were of the form</p>
<pre><code>[0,0,0,0,0,1,0,1,0,0,0,1]
[0,0,0,1,0,0,0,0,0,0,0,0]
...
</code></pre>
<p>There were <code>N</code> labels each of <code>True(1)</code> or <code>False(0)</code> represented in a <code>N</code> length vector.</p>
<p>I encountered the issue with the loss function when training this network. Because, since the length of the vector <code>N</code> large compare to the number of the <code>True</code> values(the Multi-Labels are &quot;sparse&quot;), the ANN network can just constantly output vectors of <code>0</code>s <code>[0,0,0,0,0,0,0,0,0,0,0,0]</code> and still achieve over <code>90%</code> accuracy, since most of the labels are correctly predicted as <code>0</code>.</p>
<p>I tried <code>Binary Cross-Entropy (BCE)</code> and <code>Categorical Cross-Entropy (CCE)</code> in <code>pytorch</code> and <code>tensorflow</code>, but did not get any improvements.</p>
<p>I thought about to write something myself to increase the weight over the <code>True(1)</code> values, but I suspect that would just flip the results and make everything to be <code>[1,1,1,1,1,1,1,1,1,1,1,1]</code>?</p>
<p>What's the appropriate loss function for Multi-Label Classification of multiple labels and sparse outcomes? (Example in <code>pytorch</code> and <code>tensorflow</code>)</p>
<p>A related post could be found almost 10 years ago: <a href=""https://stackoverflow.com/questions/39697216/multilabel-image-classification-with-sparse-labels-in-tensorflow"">Multilabel image classification with sparse labels in TensorFlow?</a></p>
","-3","Question"
"79475046","","<p>I'm trying to compute the gradient of my loss function with respect to my model parameters in PyTorch.</p>
<p>That is, let <code>u(x; θ)</code> be the model, where <code>x</code> is the input (in <code>R^n</code>) and <code>θ</code> are the model parameters. I'm trying to compute <code>du/dθ</code>.</p>
<p>For a &quot;simple&quot; loss function, this is not a problem, but my loss function depends on the gradient of the model with respect to its inputs (i.e., <code>du/dx</code>). When I attempt to do this, I'm met with the following error message: <code>One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.</code></p>
<p>Here is a minimal example to illustrate the issue:</p>
<pre><code>import torch
import torch.nn as nn
from torch.autograd import grad

model = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))

def loss1(x, u):
    return torch.mean(u)

def loss2(x, u):
    d_u_x = grad(u, x, torch.ones_like(u), retain_graph=True, create_graph=True)[0]
    return torch.mean(d_u_x)

x = torch.randn(10, 1)
x.requires_grad_()
u = model(x)

loss = loss2(x, u)
d_loss_params = grad(loss, model.parameters(), retain_graph=True)
</code></pre>
<p>If I change the second to last line to read <code>loss = loss1(x, u)</code> things work as expected.</p>
<p>Update: it appears to be working if I set <code>bias=False</code> for the <code>nn.Linear</code>s. OK, that makes some sense since the bias is not trainable. But that begs the question, how do I extract only the trainable parameters to use in the gradient computation?</p>
","0","Question"
"79477154","","<p>I’m working on a neural Turing Machine (NTM) model in PyTorch that uses a controller with 2D attention fusion. During training, I encounter the following error when calling .backward() on my loss:</p>
<pre><code>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</code></pre>
<p>I’ve noticed that this error occurs during the backward pass of the NTM controller’s MLP. I originally attempted to work around this by using retain_graph=True, but that quickly blows up my memory. I want to fix the underlying issue so that the model computes gradients properly without having to retain the entire graph.</p>
<p>Below is a simplified snippet from my NTM controller (the actual module is more complex, involving convolutional fusion of 2D attention maps and subsequent MLP processing):</p>
<pre><code>class NTMController(nn.Module):
&quot;&quot;&quot;
A fully advanced NTM controller that fuses 2D attention maps with the base representation.
Produces control signals (read/write keys, etc.) via an MLP.
&quot;&quot;&quot;
def __init__(self, d_model=128, mem_dim=128, hidden_dim=256, n_layers=3,
             fuse_in_channels=32, fuse_out_channels=32):
    super().__init__()
    self.d_model = d_model
    self.mem_dim = mem_dim

    # Layers for 2D attention fusion:
    self.conv_merge = nn.Conv2d(fuse_in_channels, fuse_out_channels, kernel_size=3, padding=1)
    self.resblock = BasicResBlock2D(fuse_out_channels, fuse_out_channels, stride=1)
    self.final_linear = nn.Linear(fuse_out_channels, fuse_out_channels)

    # MLP for generating control signals; input is base representation concatenated with fused 2D features.
    mlp_in_dim = d_model + fuse_out_channels
    layers = []
    in_dim = mlp_in_dim
    for _ in range(n_layers):
        layers.append(nn.Linear(in_dim, hidden_dim))
        layers.append(nn.ReLU())
        in_dim = hidden_dim
    # Final layer produces 4*mem_dim + 1 outputs.
    layers.append(nn.Linear(hidden_dim, 4 * mem_dim + 1))
    self.mlp = nn.Sequential(*layers)

def _fuse_2d_maps(self, attn_intra: Optional[torch.Tensor], attn_hier: Optional[torch.Tensor]) -&gt; Optional[torch.Tensor]:
    if attn_intra is None and attn_hier is None:
        return None
    # If both maps are provided, interpolate to the same spatial size and concatenate along the channel dim.
    if attn_intra is not None and attn_hier is not None:
        B, C_i, Hi, Wi = attn_intra.shape
        B2, C_h, Hh, Wh = attn_hier.shape
        Hmax, Wmax = max(Hi, Hh), max(Wi, Wh)
        if (Hi, Wi) != (Hmax, Wmax):
            attn_intra = F.interpolate(attn_intra, size=(Hmax, Wmax), mode='bilinear', align_corners=False)
        if (Hh, Wh) != (Hmax, Wmax):
            attn_hier = F.interpolate(attn_hier, size=(Hmax, Wmax), mode='bilinear', align_corners=False)
        x = torch.cat([attn_intra, attn_hier], dim=1)
    else:
        x = attn_intra if attn_intra is not None else attn_hier

    x = F.relu(self.conv_merge(x))
    x = self.resblock(x)
    B, C, H, W = x.shape
    x_pool = F.adaptive_avg_pool2d(x, (1, 1)).view(B, C)
    return self.final_linear(x_pool)

def forward(self, reps: torch.Tensor,
            attn_hier: Optional[torch.Tensor] = None,
            attn_intra: Optional[torch.Tensor] = None) -&gt; tuple:
    # Fuse attention maps into a feature vector.
    fused_2d = self._fuse_2d_maps(attn_intra, attn_hier)
    if fused_2d is None:
        fused_2d = torch.zeros(reps.size(0), self.final_linear.out_features, device=reps.device)
    # Concatenate the base representation with the fused 2D features.
    cat_inp = torch.cat([reps, fused_2d], dim=-1)
    # Use clone() here to avoid potential in-place modifications:
    out = self.mlp(cat_inp.clone())    # &lt;--- is where the traceback points
    # Split the output into control signals.
    read_key, write_key, erase_raw, add_vec, scale_raw = torch.split(
        out, [self.mem_dim, self.mem_dim, self.mem_dim, self.mem_dim, 1], dim=-1
    )
    scale = torch.sigmoid(scale_raw).squeeze(-1)
    # For now, duplicate scale for read and write operations.
    return read_key, write_key, erase_raw, add_vec, scale, scale
</code></pre>
<p>I also have a memory module that uses these control signals to perform a read/write operation. The error occurs specifically at self.mlp(cat_inp.clone()) in NTMController.forward() when calling loss.backward() on the final output. And to clarify, I am only calling loss.backward() once, so not twice.</p>
<p>My questions are:</p>
<pre><code>1.  What might be causing the computation graph to be traversed twice?

2.  Are there any common pitfalls in combining cloned tensors, detached tensors, or using in-place operations that could lead to this error?

3.  How can I restructure my code so that each forward pass only builds a single graph for the backward pass, without resorting to retain_graph=True?
</code></pre>
<p>Any insights or suggestions for diagnosing this issue further would be greatly appreciated!</p>
","1","Question"
"79479038","","<p>Given data and mask tensors are there a pytorch-way to obtain masked aggregations of data (mean, max, min, etc.)?</p>
<pre><code>x = torch.tensor([
    [1, 2, -1, -1],
    [10, 20, 30, -1]
])

mask = torch.tensor([
    [True, True, False, False],
    [True, True, True, False]
])
</code></pre>
<p>To compute a masked mean I can do the following, yet are there any pytorch built-in or commonly used package to do that?</p>
<pre><code>n_mask = torch.sum(mask, axis=1)
x_mean = torch.sum(x * mask, axis=1) / n_mask

print(x_mean)
</code></pre>
<pre><code>&gt; tensor([ 1.50, 20.00])
</code></pre>
","0","Question"
"79482885","","<p>I'm trying to implement an image filter in PyTorch that takes in two filters of shapes (1,3), (3,1) that build up a filter of (3,3). An example application of this is the <a href=""https://en.wikipedia.org/wiki/Sobel_operator"" rel=""nofollow noreferrer"">Sobel filter</a> or <a href=""https://en.wikipedia.org/wiki/Gaussian_blur"" rel=""nofollow noreferrer"">Gaussian blurring</a></p>
<p>I have a NumPy implementation ready, but PyTorch has a different way of working with convolutions that makes it hard to wrap my head around for more traditional applications such as this. How should I proceed?</p>
<pre class=""lang-py prettyprint-override""><code>def decomposed_conv2d(arr,x_kernel,y_kernel):
  &quot;&quot;&quot;
  Apply two 1D kernels as a part of a 2D convolution.
  The kernels must be the decomposed from a 2D kernel
  that originally is intended to be convolved with the array.
  Inputs:
  - x_kernel: Column vector kernel, to be applied along the x axis (axis 0)
  - y_kernel: Row vector kernel, to be applied along the y axis (axis 1)
  &quot;&quot;&quot;
  arr = np.apply_along_axis(lambda x: np.convolve(x, x_kernel, mode='same'), 0, arr)
  arr = np.apply_along_axis(lambda x: np.convolve(x, y_kernel, mode='same'), 1, arr)
  return arr
</code></pre>
<p>Gaussian blurring example:</p>
<pre class=""lang-py prettyprint-override""><code>ax = np.array([-1.,0.,1.])
stdev = 0.5
kernel = np.exp(-0.5 * np.square(ax) / np.square(stdev)) / (stdev * np.sqrt(2*np.pi))
decomposed_conv2d(np.arange(9).reshape((3,3)),kernel,kernel)
&gt;&gt;&gt;array([[0.39126886, 1.24684326, 1.83682264],
       [2.86471127, 4.11155453, 4.48257929],
       [4.7279302 , 6.1004473 , 6.17348398]])
</code></pre>
<p>(Note: The total &quot;energy&quot; of this array may not be preserved, especially in small arrays like this because the convolution is discrete. It isn't that critical to this particular problem).</p>
<p>Attempting to do the same in PyTorch following this <a href=""https://stackoverflow.com/questions/67633879/implementing-a-3d-gaussian-blur-using-separable-2d-convolutions-in-pytorch?rq=2"">discussion</a> yields an error:</p>
<pre class=""lang-py prettyprint-override""><code>... # define ax,stdev,kernel, etc.
arr_in = torch.arange(9).reshape(3,3) # for example
arr = arr_in.double().unsqueeze(0) # tried both axes and not unsqueezing as well
x_kernel = torch.from_numpy(kernel)
y_kernel = torch.from_numpy(kernel)

x_kernel = x_kernel.view(1,1,-1)
y_kernel = y_kernel.view(1,1,-1)
arr = F.conv1d(arr,x_kernel,padding=x_kernel.shape[2]//2).squeeze(0)
arr = F.conv1d(arr.transpose(0,1),y_kernel, padding=y_kernel.shape[2] // 2).squeeze(0).transpose(2,1).squeeze(1)

&gt;&gt;&gt; RuntimeError: Given groups=1, weight of size [1, 1, 3], expected input[1, 3, 3] to have 1 channels, but got 3 channels instead
</code></pre>
<p>I've juggled with squeezes and unsqueezes so that the dimensions match but I still can't get it to do what I want. I just can't even get the first convolution done this way.</p>
","1","Question"
"79483120","","<pre><code>import os
import shutil
import random
import torch
import torchvision.transforms as transforms
import cv2
import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torchvision.models.video as models
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
from PIL import Image

# ------------------------
# Datasets =&gt; Train, Test, Val 
# ------------------------

source_dir = &quot;new_dataset&quot;
target_dir = &quot;data&quot;


for split in [&quot;train&quot;, &quot;test&quot;, &quot;val&quot;]:
    os.makedirs(os.path.join(target_dir, split, &quot;NonViolence&quot;), exist_ok=True)
    os.makedirs(os.path.join(target_dir, split, &quot;Violence&quot;), exist_ok=True)


train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1

for category in [&quot;NonViolence&quot;, &quot;Violence&quot;]:
    category_path = os.path.join(source_dir, category)
    files = os.listdir(category_path)
    random.shuffle(files)

    train_count = int(len(files) * train_ratio)
    val_count = int(len(files) * val_ratio)

    train_files = files[:train_count]
    val_files = files[train_count:train_count + val_count]
    test_files = files[train_count + val_count:]

    for file_set, split in [(train_files, &quot;train&quot;), (val_files, &quot;val&quot;), (test_files, &quot;test&quot;)]:
        for file in file_set:
            shutil.copy(os.path.join(category_path, file), os.path.join(target_dir, split, category, file))

total_train = len(os.listdir(&quot;data/train/Violence&quot;)) + len(os.listdir(&quot;data/train/NonViolence&quot;))
total_test = len(os.listdir(&quot;data/test/Violence&quot;)) + len(os.listdir(&quot;data/test/NonViolence&quot;))
total_val = len(os.listdir(&quot;data/val/Violence&quot;)) + len(os.listdir(&quot;data/val/NonViolence&quot;))
print(f&quot;Train: {total_train}&quot;)
print(f&quot;Test: {total_test}&quot;)
print(f&quot;Val: {total_val}&quot;)

class ViolenceDataset(Dataset):
    def __init__(self, dataset_folder, clip_length=16, transform=None):
        self.dataset_folder = dataset_folder
        self.clip_length = clip_length
        self.transform = transform if transform else transforms.Compose([
            transforms.Resize((112, 112)),
            transforms.ToTensor()
        ])

        self.video_paths = []
        self.labels = []

        for label, category in enumerate(os.listdir(dataset_folder)):
            folder_path = os.path.join(dataset_folder, category)
            if os.path.isdir(folder_path):
                for video_name in os.listdir(folder_path):
                    self.video_paths.append(os.path.join(folder_path, video_name))
                    self.labels.append(label)

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]

        frames = self.extract_frames(video_path)
        frames = torch.stack([self.transform(frame) for frame in frames])  # (frames, C, H, W)
        frames = frames.permute(1, 0, 2, 3)  # (C, frames, H, W)
        
        print(f&quot;Dataset Output: {frames.shape}&quot;)  # (C, frames, 112, 112)

        return frames, torch.tensor(label, dtype=torch.long)

    def extract_frames(self, video_path):
        cap = cv2.VideoCapture(video_path)
        frames = []
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        selected_frames = np.linspace(0, frame_count - 1, self.clip_length, dtype=int)

        for i in range(frame_count):
            ret, frame = cap.read()
            if not ret:
                break
            if i in selected_frames:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = cv2.resize(frame, (112, 112))
                frames.append(frame)

        cap.release()
        return [transforms.ToPILImage()(frame) for frame in frames]

dataset_folder = &quot;data&quot;
batch_size = 8

train_dataset = ViolenceDataset(os.path.join(dataset_folder, &quot;train&quot;))
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

for clips, labels in train_loader:
    print(f&quot;Loader Video Shape: {clips.shape}&quot;)  # (batch, 3, frames, 112, 112) 
    break

class ViolenceDetectionLSTM(nn.Module):
    def __init__(self, hidden_size=256, num_layers=2):
        super(ViolenceDetectionLSTM, self).__init__()
        self.cnn = models.r3d_18(pretrained=True)
        self.cnn.fc = nn.Identity()  

        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        print(&quot;\n--- Forward Started ---&quot;)
        print(&quot;Input Shape:&quot;, x.shape)  # (batch, 16, 3, 112, 112)

        # (batch, frames, 3, 112, 112)
        x = x.permute(0, 2, 1, 3, 4)  # (batch, frames, C, H, W)
        print(&quot;Permute:&quot;, x.shape)  # (batch, frames, 3, 112, 112)

        
        cnn_features = []
        for t in range(x.shape[1]):  
            frame = x[:, t, :, :, :]  # (batch, 3, 112, 112)
            cnn_out = self.cnn(frame) # (batch, 512)
            cnn_features.append(cnn_out.unsqueeze(1))  # (batch, 512) 
        #(batch, frames, 512)
        cnn_features = torch.cat(cnn_features, dim=1)
        print(&quot;LSTM, CNN:&quot;, cnn_features.shape)  # (batch, frames, 512)

        
        lstm_out, _ = self.lstm(cnn_features)
        lstm_out = lstm_out[:, -1, :] 
        output = self.fc(lstm_out)

        print(&quot;Model Output:&quot;, output.shape)  # (batch, 1)
        print(&quot;--- Forward Finished ---\n&quot;)

        return output
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = ViolenceDetectionLSTM().to(device)

# ------------------------
# Training
# ------------------------
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCEWithLogitsLoss()

num_epochs = 10
train_losses, val_losses = [], []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0

    for clips, labels in tqdm(train_loader, desc=f&quot;Epoch {epoch+1}/{num_epochs}&quot;):
        clips, labels = clips.to(device), labels.float().unsqueeze(1).to(device)

        # Debug: Input Check
        print(f&quot;Input: {clips.shape}&quot;)  # (batch, frames, C, H, W)
        
        optimizer.zero_grad()
        outputs = model(clips)

        # Debug: Output Check
        print(f&quot;Output: {outputs.shape}&quot;)  # (batch, 1)

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    print(f&quot;Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}&quot;)

    torch.save(model.state_dict(), &quot;best_violence_model_lstm.pth&quot;)

print(&quot;Training complete! Best model saved.&quot;)
</code></pre>
<p>RuntimeError: Given groups=1, weight of size [64, 3, 3, 7, 7], expected input[1, 8, 3, 112, 112] to have 3 channels, but got 8 channels instead
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...</p>
<p>I could not solved this problem for 2 days, even used chat gpt and other stuff. There is a problem with channels but i could not figured out and confused</p>
","1","Question"
"79484474","","<p>my group leader assigns the task to reduce the compute cost of mobileNetV2, initially did kernel-size refactoring. I thought to use OFA to check what are the possible modification that can be done. OFA doesn't come with mbv2 by default.</p>
<p>can we run custom model on OFA, to get the model which is more efficient on specific dataset?</p>
<p>Thank you.</p>
","0","Question"
"79484953","","<p>Let's suppose we have a <strong>binary matrix</strong> <code>A</code> with shape <code>n x m</code>,
I want to identify rows that have duplicates in the matrix, i.e. there is another index on the same dimension with the same elements in the same positions.</p>
<p>It's very important not to convert this matrix into a dense representation, since the real matrices I'm using are quite large and difficult to handle in terms of memory.</p>
<p>Using PyTorch for the implementation:</p>
<pre><code># This is just a toy sparse binary matrix with n = 10 and m = 100
A = torch.randint(0, 2, (10, 100), dtype=torch.float32).to_sparse()
</code></pre>
<p>Intuitively, we can perform the dot product of this matrix producing a new <code>m x m</code> matrix which contains in terms <code>i, j</code>, the number of 1s that the index <code>i</code> has in the same position of the index <code>j</code> at dimension <code>0</code>.</p>
<pre><code>B = A.T @ A # In PyTorch, this operation will also produce a sparse representation
</code></pre>
<p>At this point, I've tried to combine these values, comparing them with <code>A.sum(0)</code>,</p>
<pre><code>num_elements = A.sum(0)
duplicate_rows = torch.logical_and([
   num_elements[B.indices()[0]] == num_elements[B.indices()[1]],
   num_elements[B.indices()[0]] == B.values()
])
</code></pre>
<p>But this did not work!</p>
<p>I think that the solution can be written only by using operations on PyTorch Sparse tensors (without using Python loops and so on), and this could also be a benefit in terms of performance.</p>
","4","Question"
"79486105","","<p>I've been trying to train a language model (text classification), our lab has two GPUs, a 4090 and a 3090. However, I encountered a perplexing phenomenon during training: the model's performance varies slightly when using different GPUs (or different GPU combinations). If the difference were negligible, I might not be concerned. But I observed that the performance gap between using a single 4090 and a single 3090 reaches around 5%, which makes me feel that this is an issue worth addressing. Why does this happen and how to resolve it? Below is the terminal output for reference:</p>
<p><strong>Using single RTX 3090:</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=1 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 153.9599, 'train_samples_per_second': 14.134, 'train_steps_per_second': 1.767, 'train_loss': 0.38275421366972084, 'epoch': 8.0}

Evaluation loss: 0.4403

Model Performance:
accuracy: 0.7606
f1_score: 0.7619
precision: 0.7634
recall: 0.7606
</code></pre>
<p><strong>Using single RTX 4090:</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=0 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 143.1252, 'train_samples_per_second': 15.203, 'train_steps_per_second': 1.9, 'train_loss': 0.3407774532542509, 'epoch': 8.0}

Evaluation loss: 0.3961

Model Performance:
accuracy: 0.8169
f1_score: 0.8103
precision: 0.8132
recall: 0.8169
</code></pre>
<p><strong>Using RTX 4090 + RTX 3090:</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=0,1 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 195.1275, 'train_samples_per_second': 11.152, 'train_steps_per_second': 0.697, 'train_loss': 0.43211137547212486, 'epoch': 8.0}

Evaluation loss: 0.3961

Model Performance:
accuracy: 0.8028
f1_score: 0.8064
precision: 0.8152
recall: 0.8028

</code></pre>
<p><strong>GPU info:</strong></p>
<pre><code>$ nvidia-smi
Wed Mar  5 17:33:02 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.56.06    Driver Version: 520.56.06    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |
|  0%   44C    P8    19W / 450W |     59MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |
|  0%   46C    P8    28W / 350W |      5MiB / 24576MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
</code></pre>
<p><strong>What I have tried:</strong></p>
<p>✅ Ensured that random_seed is set to a fixed value. When using the same GPU, the model's performance remains consistent across runs, so I believe this is not a random_seed issue (e.g., <code>torch.backends.cudnn.deterministic = True</code>).</p>
<p>✅ Tried different servers. I tested on two different servers (one of them has eight 2080 Ti GPUs), but even with GPUs of the same model (but different CUDA indices), I observed similar issues.</p>
<p><strong>Using single RTX 2080ti(cuda:1):</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=1 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 157.7439, 'train_samples_per_second': 13.795, 'train_steps_per_second': 1.724, 'train_loss': 0.3536217072430779, 'epoch': 8.0}

Evaluation loss: 0.3865

Model Performance:
accuracy: 0.7887
f1_score: 0.7951
precision: 0.8265
recall: 0.7887
</code></pre>
<p><strong>Using single RTX 2080ti(cuda:0):</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=0 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 158.6497, 'train_samples_per_second': 13.716, 'train_steps_per_second': 1.714, 'train_loss': 0.36603568581973805, 'epoch': 8.0}

Evaluation loss: 0.4049

Model Performance:
accuracy: 0.8028
f1_score: 0.8028
precision: 0.8028
recall: 0.8028
</code></pre>
<pre><code>$ nvidia-smi
Wed Mar  5 18:02:33 2025
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti      Off| 00000000:1D:00.0 Off |                  N/A |
| 34%   51C    P2               72W / 250W|   4821MiB / 11264MiB |      4%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 2080 Ti      Off| 00000000:1E:00.0 Off |                  N/A |
| 28%   46C    P2               90W / 250W|   3795MiB / 11264MiB |      5%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+--------------------
...(ignore)
</code></pre>
","1","Question"
"79487620","","<p>I need to create a PyTorch tensor (CPU) and allocate space for it. <strong>The tensor is multi-gigabyte and fits in RAM only once.</strong></p>
<p>I need it shared, because it is later utilized by data retrieval workers which are executed in additional spawned processes (DataLoader with <code>num_workers &gt; 0</code>).</p>
<p>I tried several approaches:</p>
<ol>
<li><p>Just plain creation and then using <code>DataLoader</code>:</p>
<pre>v = torch.empty(25 * 2**30)
loader = DataLoader(dataset, num_workers=2, persistent_workers=True)  
# dataset uses v (actually previous line is its _init__())
</pre>
</li>
<li><p>Using explicit <code>share_memory_()</code></p>
<pre>v = torch.empty(25 * 2**30)
v.share_memory_()</pre>
</li>
</ol>
<p>Both these options lead to <code>Couldn't open shared file mapping:..</code> because the tensor is created first and then copied to shared memory,
and two copies do not fit in RAM.</p>
<ol start=""3"">
<li><p>There is another method with creating a file and then using <code>torch.from_file</code> which probably works, but it requires writing this huge tensor to disk, and it is slow and not desirable.</p>
</li>
<li><p>I have found mentions of <code>TorchStore</code> module which could help, but it seems to be not a part of PyTorch yet.</p>
</li>
<li><p>(based on gfdb answer) Using multiprocessing.shared_memory.SharedMemory():</p>
<pre>
import torch
import numpy as np
from multiprocessing import shared_memory
import psutil

print(f""Available memory: {psutil.virtual_memory().available / (1024 ** 3):.2f} GB"")
print(f""Total memory: {psutil.virtual_memory().total / (1024 ** 3):.2f} GB"")

tensor_shape = (2**30, 25)
dtype = np.float32
num_elements = np.prod(tensor_shape)
size = int(num_elements * np.dtype(dtype).itemsize)
print(f""Requesting allocation of {size / 2 ** 30:.2f} GB"")

sh_mem = shared_memory.SharedMemory(create=True, size=size)
np_array = np.ndarray(tensor_shape, dtype=dtype, buffer=sh_mem.buf)
tensor = torch.from_numpy(np_array)
tensor.fill_(0)
print(f""Allocated {size / 2**30:.2f} GB"")</pre>
</li>
</ol>
<p>In the latter case, I still get the error if requesting more than half of RAM (less than half goes OK):</p>
<pre>
Available memory: 176.63 GB
Total memory: 191.87 GB
Requesting allocation of 100.00 GB
Traceback (most recent call last):
  File ""D:\Sci\NetOnset\CheckShared.py"", line 22, in 
    sh_mem = shared_memory.SharedMemory(create=True, size=size)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Miniconda3\envs\cuda124\Lib\multiprocessing\shared_memory.py"", line 151, in __init__
    self._mmap = mmap.mmap(-1, size, tagname=temp_name)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [WinError 1455] The paging file is too small for this operation to complete
</pre>
<p><strong>Is there still a way to create a big PyTorch Tensor in shared memory without copying?</strong></p>
<p>This all works in the same way both on Windows 10 and Ubuntu 24.04.</p>
","5","Question"
"79492085","","<p>On <a href=""https://pytorch.org/docs/stable/onnx_torchscript_supported_aten_ops.html"" rel=""nofollow noreferrer"">this PyTorch webpage</a>, it has a list of torchscript operators that are supported/unsupported by ONNX export. What I want to know is, is there a PyTorch/ONNX API-based method to look-up/tell if a particular PyTorch module, in my case a quantization enabled activation function module, is supported?</p>
<p>E.g.,</p>
<pre><code>from api import supported_modules

if &quot;quantized::silu&quot; in supported_modules:
    print(&quot;Hooray! Quantized SiLU's are supported.&quot;)
</code></pre>
<p>At the moment it's just a case of trial and error exporting the model to ONNX and seeing if it fails or not.</p>
","0","Question"
"79493986","","<p>I'm encountering an error during the supervised fine-tuning (SFT) of Qwen2.5-Coder-1.5B. The error, shown in the log below, seems to indicate that something is interrupting gradient computation during backpropagation, but I haven't been able to pinpoint the cause. Could someone help me understand what might be triggering this issue?</p>
<p>Below is a simplified version of my code and the corresponding log output:</p>
<p>my code:</p>
<pre class=""lang-py prettyprint-override""><code># Download model
import os
import tokenize
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling

model_id = &quot;Qwen/Qwen2.5-Coder-1.5B&quot;
save_dir = f&quot;/root/autodl-tmp/NL2SQL/models/{model_id[5:]}/&quot;
os.makedirs(save_dir, exist_ok=True)
# snapshot_download(repo_id=model_id, local_dir=save_dir)

# Load model
model = AutoModelForCausalLM.from_pretrained(save_dir, device_map=&quot;cuda&quot;)
tokenizer = AutoTokenizer.from_pretrained(save_dir, device_map=&quot;cuda&quot;)

# Data processing
import pandas as pd
from datasets import Dataset

# Read CSV file and create Dataset
data_dir = '/root/autodl-tmp/NL2SQL/cot-qa.csv'
df = pd.read_csv(data_dir)
dataset = Dataset.from_pandas(df)

def combined_preprocess(batch):
    texts = []
    # Iterate over each sample to construct the complete prompt and completion text
    for q, a, t in zip(batch[&quot;query&quot;], batch[&quot;answer&quot;], batch[&quot;thinking_process&quot;]):
        question = str(q)
        answer = str(a)
        thinking = str(t)
        prompt = (
            f&quot;For the question: {question}.\n&quot;
            &quot;Please think step by step, list your thinking process between &lt;think&gt; and &lt;/think&gt; and then show the final SQL answer:&quot;
        )
        completion = (
            f&quot;&lt;think&gt;{thinking}&lt;/think&gt;\nMy final answer is: ```sql\n{answer}\n```&quot;
        )
        texts.append(prompt + &quot;\n&quot; + completion)
    # Do not perform padding or return torch.Tensor; return a list for the collator to pad later
    tokenized = tokenizer(
        texts,
        truncation=True,
        max_length=1024 * 2,
        padding=False,
    )
    return tokenized

processed_dataset = dataset.map(combined_preprocess, batched=True, remove_columns=dataset.column_names)
# print(processed_dataset[0])

# LoRA configuration
from peft import LoraConfig, TaskType, get_peft_model
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    r=8,
    lora_alpha=16,  # 8*2
    lora_dropout=0.05,
    bias='none',
    inference_mode=False
)

model = get_peft_model(model, lora_config)
print(model.print_trainable_parameters())
model.config.use_cache = False

# Training configuration
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=&quot;./output/sft/&quot;,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    logging_first_step=5,
    num_train_epochs=2,
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True,
    report_to=&quot;none&quot;,
    remove_unused_columns=False,
)

# Swanlab setup
import swanlab
from swanlab.integration.transformers import SwanLabCallback
swanlab_callback = SwanLabCallback(
    ...
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    callbacks=[swanlab_callback],
)

trainer.train()
</code></pre>
<p>log output:</p>
<pre class=""lang-bash prettyprint-override""><code>root@autodl-container:~/autodl-tmp/NL2SQL# python sft.py 
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9399/9399 [00:03&lt;00:00, 2396.28 examples/s]
trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945
None
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
swanlab: Tracking run with swanlab version 0.4.11                                                   
swanlab: Run data will be saved locally in /root/autodl-tmp/NL2SQL/swanlog/run-
swanlab: 👋 Hi , welcome to swanlab!
swanlab: Syncing run  to the cloud
swanlab: 🌟 Run `swanlab watch /root/autodl-tmp/NL2SQL/swanlog` to view SwanLab Experiment Dashboard locally
swanlab: 🏠 View project at https://swanlab.cn/@/Qwen2.5-Coder-1.5B-NL2SQL-SFT
swanlab: 🚀 View run at https://swanlab.cn/@/Qwen2.5-Coder-1.5B-NL2SQL-SFT/runs/
  0%|                                                                                                                               | 0/1174 [00:00&lt;?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
swanlab: Error happened while training
swanlab: 🌟 Run `swanlab watch /root/autodl-tmp/NL2SQL/swanlog` to view SwanLab Experiment Dashboard locally
swanlab: 🏠 View project at https://swanlab.cn/@/Qwen2.5-Coder-1.5B-NL2SQL-SFT
swanlab: 🚀 View run at https://swanlab.cn/@/Qwen2.5-Coder-1.5B-NL2SQL-SFT/runs/
  File &quot;/root/autodl-tmp/NL2SQL/sft.py&quot;, line 117, in &lt;module&gt;                                      
    trainer.train()
  File &quot;/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py&quot;, line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py&quot;, line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py&quot;, line 3740, in training_step
    self.accelerator.backward(loss, **kwargs)
  File &quot;/root/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py&quot;, line 2329, in backward
    loss.backward(**kwargs)
  File &quot;/root/miniconda3/lib/python3.12/site-packages/torch/_tensor.py&quot;, line 626, in backward
    torch.autograd.backward(
  File &quot;/root/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py&quot;, line 347, in backward
    _engine_run_backward(
  File &quot;/root/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py&quot;, line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
element 0 of tensors does not require grad and does not have a grad_fn
  0%|          | 0/1174 [00:02&lt;?, ?it/s]
</code></pre>
<p>Any insights on what might be causing this gradient computation issue would be greatly appreciated. Thanks in advance!</p>
<p>Feel free to ask for any additional information if needed.</p>
","1","Question"
"79494100","","<p>I have one set of weights, one tokenizer, the same prompt, and identical generation parameters. Yet somehow, when I load the model using AutoModelForCausalLM, I get one output, and when I construct it manually with LlamaForCausalLM plus the same config and state_dict, I get another output entirely.</p>
<p>This code can show the difference on both a6000 and a100.</p>
<pre><code>import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaConfig
)

# 1) Adjust these as needed
model_name = &quot;meta-llama/Llama-3.1-8B&quot;
prompt = &quot;Hello from Llama 3.1! Tell me something interesting.&quot;
dtype = torch.float16  # or torch.float32 if needed

# 2) Get the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

# Prepare input
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

############################################
# A) Load with AutoModelForCausalLM
############################################

print(&quot;=== Loading with AutoModelForCausalLM ===&quot;)

model_auto = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,  # matches your usage
    torch_dtype=dtype
).cuda()
model_auto.eval()  # turn off dropout
config = model_auto.config
with torch.no_grad():
    out_auto = model_auto(**inputs)
logits_auto = out_auto.logits  # shape: [batch_size, seq_len, vocab_size]

del model_auto
torch.cuda.empty_cache()

############################################
# B) Load with LlamaForCausalLM + config
############################################

print(&quot;=== Loading with LlamaForCausalLM + config ===&quot;)

# Get config from the same checkpoint
# Build Llama model directly
model_llama = LlamaForCausalLM(config).cuda()
model_llama.eval()

# Load the same weights that AutoModelForCausalLM used
model_auto_temp = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype)
model_llama.load_state_dict(model_auto_temp.state_dict())
del model_auto_temp
torch.cuda.empty_cache()

with torch.no_grad():
    out_llama = model_llama(**inputs)
logits_llama = out_llama.logits

############################################
# C) Compare the Logits
############################################

# Compute maximum absolute difference
max_diff = (logits_auto - logits_llama).abs().max()
print(f&quot;\nMax absolute difference between logits: {max_diff.item()}&quot;)

if max_diff &lt; 1e-7:
    print(&quot;→ The logits are effectively identical (within floating-point precision).&quot;)
else:
    print(&quot;→ There is a non-trivial difference in logits!&quot;)
</code></pre>
","1","Question"
"79495211","","<p>I installed PyTorch using <code>pip install torch</code>, but when I try to import it, I get the following error:</p>
<pre><code>OSError                                   Traceback (most recent call last)  
Cell In[1], line 1  
----&gt; 1 import torch  
      2 print(torch.__version__)  
      3 print(torch.version.cuda) 

File &quot;\.venv\Lib\site-packages\torch\__init__.py&quot;, line 274  
    270                     raise err  
    272         kernel32.SetErrorMode(prev_error_mode)  
--&gt; 274     _load_dll_libraries()  
    275     del _load_dll_libraries


File &quot;\.venv\Lib\site-packages\torch\__init__.py&quot;, line 257, in _load_dll_libraries  
    253     err = ctypes.WinError(last_error)  
    254     err.strerror += (  
    255         f' Error loading &quot;{dll}&quot; or one of its dependencies.'  
    256     )  
--&gt; 257     raise err  
    258 elif res is not None:  
    259     is_loaded = True 

OSError: [WinError 127] The specified procedure could not be found. Error loading &quot;\.venv\Lib\site-packages\torch\lib\torch_python.dll&quot; or one of its dependencies.
</code></pre>
<p><strong>My System Configuration:</strong> <br>
OS: Windows  11 <br>
Python Version: 3.11 (pyenv) <br>
PyTorch Version: 2.6.0 <br>
CUDA Toolkit Installed: 11.8 <br>
Installation Command Used: pip install torch <br>
Virtual Environment: Yes (venv)</p>
<p><strong>Tried Different PyTorch Installation</strong></p>
<p>Installed using the recommended command from the official PyTorch
website:</p>
<pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</code></pre>
<p><a href=""https://stackoverflow.com/questions/69958526/oserror-winerror-127-the-specified-procedure-could-not-be-found"">OSError: [WinError 127] The specified procedure could not be found</a> suggests to create a conda environment, but is there a way to get it working without installing conda.</p>
","0","Question"
"79501609","","<p>Assume I have a network created as follows:</p>
<pre><code>p = torch.nn.Sequential(torch.nn.Linear(self.inputSize, self.outputSize))
</code></pre>
<p>I know that I can print the network with:</p>
<pre><code>print(p)
</code></pre>
<p>and get:</p>
<pre><code>Sequential(
(0): Linear(in_features=22, out_features=3, bias=True)
)
</code></pre>
<p>I want to pretty print the network thus, I can use</p>
<pre><code>for name, module in p.named_children():   
    print(f'{name:&gt;10} {module}')
</code></pre>
<p>to print each network layer's name. For the example network above I'd get:</p>
<pre><code>0 Linear(in_features=22, out_features=3, bias=True)
</code></pre>
<p>But how to I get the 'Sequential' part? It's the nn module container class so is the only method to do this to dissect the class name returned by type (&lt;class 'torch.nn.modules.container.Sequential'&gt;)?</p>
","1","Question"
"79501647","","<p>Let's say we have an <strong>ordered 1D tensor of ints/longs</strong> <code>t</code>.</p>
<p>I want to produce a new tensor <code>a</code> with size <code>max(t)</code> where each term <code>a[i]</code> contains the first occurrence of the value <code>i</code> in the tensor <code>t</code>.</p>
<p>We could easily compute the tensor <code>a</code> using standard Python code, but it would be too slow for large input as the one I'm using.</p>
<p>I'm looking for a fast solution that can run on GPU using the CUDA backend of PyTorch or simply a fast solution.</p>
","1","Question"
"79501962","","<p>I am trying to log the loss and auc for all 3 of my datasets - train, validation and test.
The datamodule defines the 3 loaders and I finally invoke the model as:</p>
<pre><code>trainer.fit(model,datamodule)
trainer.test(model,datamodule)
</code></pre>
<p>This ends up creating 2 different log files - 1 with train and validation metrics, and 1 with test metrics.
Is there a way to collect these together? Context is that I am using the mlflow logger and would like to see all 3 sets of metrics on the same chart, but because of this behavior,  it posts 2 different sets of metrics with the same run id, creating 2 different graphs.</p>
","1","Question"
"79502682","","<p>I am running a bunch of DNN experiments using <code>pytorch</code>, and saving an instance of my <code>ExperimentClass</code> as a .pth file. The ExperimentClass has instance variables including a torch model, a dictionary of multiple plotly figure objects, dataframes, and other metadata related to experimental parameters.</p>
<p>However, when I try to load the .pth file using <code>torch.load(full_model_path, weights_only=False)</code>, I get a cryptic <code>ValueError</code> related to some figure type that I never even use, but it's definitely occurring during the unpacking of the plotly figure objects.</p>
<pre><code>ValueError: Invalid property specified for object of type plotly.graph_objs.layout.template.Data: 'heatmapgl'
</code></pre>
<p>Here is the full traceback:</p>
<pre><code>loading model from experiment_plots/vLAYER/dnn_all_experiments_results.pth
Traceback (most recent call last):
  File &quot;/Users/name/Desktop/name/research/nn-layer-weight-experiments/create_additional_training_plots.py&quot;, line 163, in &lt;module&gt;
    create_final_epoch_layer_plots(experiment_versions=experiment_versions, dnn_names=dnn_names)
  File &quot;/Users/name/Desktop/name/research/nn-layer-weight-experiments/create_additional_training_plots.py&quot;, line 34, in create_final_epoch_layer_plots
    dnn_experiments = torch.load(full_model_path, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/torch/serialization.py&quot;, line 1471, in load
    return _load(
           ^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/torch/serialization.py&quot;, line 1964, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/graph_objs/_figure.py&quot;, line 641, in __init__
    super(Figure, self).__init__(data, layout, frames, skip_invalid, **kwargs)
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 556, in __init__
    self._layout_obj = self._layout_validator.validate_coerce(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/_plotly_utils/basevalidators.py&quot;, line 2504, in validate_coerce
    v = self.data_class(v, skip_invalid=skip_invalid, _validate=_validate)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/graph_objs/_layout.py&quot;, line 7124, in __init__
    self[&quot;template&quot;] = _v
    ~~~~^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 5898, in __setitem__
    super(BaseLayoutHierarchyType, self).__setitem__(prop, value)
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 4852, in __setitem__
    self._set_compound_prop(prop, value)
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 5263, in _set_compound_prop
    val = validator.validate_coerce(val, skip_invalid=self._skip_invalid)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/_plotly_utils/basevalidators.py&quot;, line 2797, in validate_coerce
    return super(BaseTemplateValidator, self).validate_coerce(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/_plotly_utils/basevalidators.py&quot;, line 2504, in validate_coerce
    v = self.data_class(v, skip_invalid=skip_invalid, _validate=_validate)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/graph_objs/layout/_template.py&quot;, line 327, in __init__
    self[&quot;data&quot;] = _v
    ~~~~^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 4852, in __setitem__
    self._set_compound_prop(prop, value)
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 5263, in _set_compound_prop
    val = validator.validate_coerce(val, skip_invalid=self._skip_invalid)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/_plotly_utils/basevalidators.py&quot;, line 2504, in validate_coerce
    v = self.data_class(v, skip_invalid=skip_invalid, _validate=_validate)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/graph_objs/layout/template/_data.py&quot;, line 1791, in __init__
    self._process_kwargs(**dict(arg, **kwargs))
  File &quot;/Users/name/miniconda3/lib/python3.11/site-packages/plotly/basedatatypes.py&quot;, line 4378, in _process_kwargs
    raise err
ValueError: Invalid property specified for object of type plotly.graph_objs.layout.template.Data: 'heatmapgl'
</code></pre>
<p>What is also strange is that I ran a smaller version of my test experiments locally about a day ago, and all of the .pth files were able to be loaded without any errors. Has anyone else encountered this?</p>
","2","Question"
"79502752","","<p>I am using Ubuntu 24.04.1 on an AWS EC2 instance g5.8xlarge.</p>
<p>I am receiving the following error message:</p>
<pre><code>OutOfMemoryError: Allocation on device 
</code></pre>
<p><strong>Code:</strong></p>
<pre><code>import os
os.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;backend:cudaMallocAsync&quot;
import torch
torch.cuda.empty_cache()
import transformers
    
if torch.cuda.is_available():
    torch.set_default_device(&quot;cuda&quot;)
    
device = torch.device(&quot;cuda&quot;)
    
model = transformers.AutoModelForCausalLM.from_pretrained(&quot;microsoft/Orca-2-13b&quot;, device_map=device)
</code></pre>
<p><strong>Full error:</strong></p>
<pre><code>/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML
  warnings.warn(&quot;Can't initialize NVML&quot;)

Loading checkpoint shards:  33%
 2/6 [00:04&lt;00:06,  1.72s/it]

/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML
  warnings.warn(&quot;Can't initialize NVML&quot;)

---------------------------------------------------------------------------
OutOfMemoryError                          Traceback (most recent call last)
Cell In[5], line 6
      2     torch.set_default_device(&quot;cuda&quot;)
      4 device = torch.device(&quot;cuda&quot;)
----&gt; 6 model = transformers.AutoModelForCausalLM.from_pretrained(&quot;microsoft/Orca-2-13b&quot;, device_map=device)
      8 # https://github.com/huggingface/transformers/issues/27132
      9 # please use the slow tokenizer since fast and slow tokenizer produces different tokens
     10 tokenizer = transformers.AutoTokenizer.from_pretrained(
     11         &quot;microsoft/Orca-2-13b&quot;,
     12         use_fast=True,
     13     )

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)
--&gt; 564     return model_class.from_pretrained(
    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    566     )
    567 raise ValueError(
    568     f&quot;Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n&quot;
    569     f&quot;Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.&quot;
    570 )

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:262, in restore_default_torch_dtype.&lt;locals&gt;._wrapper(*args, **kwargs)
    260 old_dtype = torch.get_default_dtype()
    261 try:
--&gt; 262     return func(*args, **kwargs)
    263 finally:
    264     torch.set_default_dtype(old_dtype)

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4319, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   4309     if dtype_orig is not None:
   4310         torch.set_default_dtype(dtype_orig)
   4312     (
   4313         model,
   4314         missing_keys,
   4315         unexpected_keys,
   4316         mismatched_keys,
   4317         offload_index,
   4318         error_msgs,
-&gt; 4319     ) = cls._load_pretrained_model(
   4320         model,
   4321         state_dict,
   4322         loaded_state_dict_keys,  # XXX: rename?
   4323         resolved_archive_file,
   4324         pretrained_model_name_or_path,
   4325         ignore_mismatched_sizes=ignore_mismatched_sizes,
   4326         sharded_metadata=sharded_metadata,
   4327         _fast_init=_fast_init,
   4328         low_cpu_mem_usage=low_cpu_mem_usage,
   4329         device_map=device_map,
   4330         offload_folder=offload_folder,
   4331         offload_state_dict=offload_state_dict,
   4332         dtype=torch_dtype,
   4333         hf_quantizer=hf_quantizer,
   4334         keep_in_fp32_modules=keep_in_fp32_modules,
   4335         gguf_path=gguf_path,
   4336         weights_only=weights_only,
   4337     )
   4339 # make sure token embedding weights are still tied if needed
   4340 model.tie_weights()

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4897, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)
   4895     else:
   4896         fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)
-&gt; 4897         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
   4898             model_to_load,
   4899             fixed_state_dict,
   4900             start_prefix,
   4901             expected_keys,
   4902             device_map=device_map,
   4903             offload_folder=offload_folder,
   4904             offload_index=offload_index,
   4905             state_dict_folder=state_dict_folder,
   4906             state_dict_index=state_dict_index,
   4907             dtype=dtype,
   4908             hf_quantizer=hf_quantizer,
   4909             is_safetensors=is_safetensors,
   4910             keep_in_fp32_modules=keep_in_fp32_modules,
   4911             unexpected_keys=unexpected_keys,
   4912         )
   4913         error_msgs += new_error_msgs
   4914 else:
   4915     # Sharded checkpoint or whole but low_cpu_mem_usage==True

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:896, in _load_state_dict_into_meta_model(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)
    893         param_device = &quot;cpu&quot; if is_local_dist_rank_0() else &quot;meta&quot;
    895     # For backward compatibility with older versions of `accelerate` and for non-quantized params
--&gt; 896     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
    897 else:
    898     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/accelerate/utils/modeling.py:330, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)
    328             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)
    329 elif isinstance(value, torch.Tensor):
--&gt; 330     new_value = value.to(device)
    331 else:
    332     new_value = torch.tensor(value, device=device)

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:104, in DeviceContext.__torch_function__(self, func, types, args, kwargs)
    102 if func in _device_constructors() and kwargs.get('device') is None:
    103     kwargs['device'] = self.device
--&gt; 104 return func(*args, **kwargs)

OutOfMemoryError: Allocation on device 
</code></pre>
","0","Question"
"79504403","","<p>I have a function to measure the allocated ram by python in megabytes:
<code>def getram(): print(psutil.Process(os.getpid()).memory_info().rss / 1024**2)</code></p>
<p>And also I have:
<code>device = &quot;cuda&quot;</code></p>
<p>My problem is that the following code allocates RAM and I'm going crazy because of that. Does it have an actual solution or do I have to accept my fate and switch to C++ or something?</p>
<p>The code:</p>
<pre><code>getram()

def load_dataset(dir, filenames):
  dataset = torch.zeros((len(filenames),3,256,256), device=device)
  getram()
  for i, filename in enumerate(filenames):
    f = read_image(f&quot;{dir}/{filename}&quot;)
    if 3 != f.shape[0]: print(filename)
    dataset[i] = f.to(device)
  getram()
  return dataset

dataset = load_dataset(dataset_dir, dataset_filenames)

getram()
</code></pre>
<p>The code printed out the following:</p>
<pre><code>533.28125
661.2890625
678.27734375
678.27734375
</code></pre>
<p>As you can see, as soon as I create the empty tensor with the <code>torch.zeros()</code>, it takes up the RAM for no reason.</p>
<p>I tried <code>gc.collect()</code>, but it didn't help at all.</p>
","0","Question"
"79504951","","<p>I’m working on a PyTorch model where I compute a “global representation” through a forward pipeline. This pipeline is subsequently used in an extra sampling procedure later on in the network. When I compute the global representation with a full recompute (i.e. without checkpointing), everything works fine and gradients flow back correctly. However, when I try to use torch.utils.checkpoint to save memory by recomputing the global representation during the backward pass, I get a runtime error similar to:</p>
<pre><code>torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 34:
  saved metadata: {'shape': torch.Size([128, 192]), 'dtype': torch.bfloat16, 'device': device(type='mps', index=0)}
  recomputed metadata: {'shape': torch.Size([128, 128, 192]), 'dtype': torch.float32, 'device': device(type='mps', index=0)}
... (more tensor mismatches follow) ...
</code></pre>
<p>Some details about my setup:</p>
<ul>
<li>I run on the MPS backend (Apple Silicon) with mixed precision (bfloat16) using autocast.</li>
<li>The global representation is computed in a module that later feeds into an extra sampling procedure, so gradients must flow back properly.</li>
<li>Recomputing the global representation fully (i.e. running the entire forward pass twice) is too inefficient, so checkpointing is critical.</li>
</ul>
<p>Besides this, I’ve already tried some fixes such as replacing all inplace operations with their out-of-place equivalents, but these modifications didn’t resolve the issue.</p>
<p>Additionally, I’m using the following line in my Gumbel sampling procedure:</p>
<p><code>cond_expanded = cond_cont.unsqueeze(1).expand(B, num_samples, -1).reshape(B * num_samples, -1)</code></p>
<p>I intended this to properly broadcast the condition over multiple Monte Carlo samples. However, I suspect that the unsqueeze/expand/reshape sequence might be contributing to the metadata mismatch between the tensors saved during the forward pass and those recomputed during the backward pass.</p>
<p>I suspect this issue is related either to interactions between checkpointing and autocast or possibly an inadvertent change in tensor dimensions during recomputation. Has anyone encountered a similar problem or know how to ensure that the recomputed tensors match the original forward pass (in terms of shape, dtype, and device) while still benefiting from checkpointing? Any suggestions on how to resolve this, or workarounds that allow efficient memory use without sacrificing gradient flow, would be very helpful.</p>
<p>Additional context or sample code snippets can be provided if needed.</p>
<p>(also maybe someone can create a &quot;torch.utils.checkpoint&quot; tag)</p>
","3","Question"
"79507748","","<p>I have not been able to wrap my head around this, and ChatGPT seems to think this shouldn't be the case. Why does setting <code>requires_grad=True</code> when creating a tensor cause it to lose its leaf status when transferring it to a GPU? For example (tested in a Google Colab notebook):</p>
<pre><code>b = torch.rand(10).cuda()
b.is_leaf  # True
</code></pre>
<p>and</p>
<pre><code>b = torch.rand(10, requires_grad=True)
b.is_leaf  # True
</code></pre>
<p>but</p>
<pre><code>b = torch.rand(10, requires_grad=True).cuda()
b.is_leaf  # False
</code></pre>
<p>I realize that <code>b = torch.rand(10, requires_grad=True, device='cuda')</code> causes <code>b</code> to retain its leaf status after being transfered to the GPU, which is a perfectly fine workaround. However, I am very confused by the above-mentioned behavior.</p>
","0","Question"
"79508526","","<p>Consider the following environment.yml file:</p>
<pre><code>channels:
  - nvidia
  - defaults
  - conda-forge
dependencies:
  - bottleneck&gt;=1.3.6
  - catboost&gt;=0.24.4
  ...
  - pip:
    - xgboost==2.1.4
    ...
</code></pre>
<p>How do I add the following pip command to the yml file without disrupting the current behavior of the other pip install commands above?</p>
<pre><code>pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
</code></pre>
","1","Question"
"79509349","","<p>I am new to torcheval and trying to measure the AUC of my binary classifier (<a href=""https://pytorch.org/torcheval/main/generated/torcheval.metrics.AUC.html"" rel=""nofollow noreferrer"">doc</a>).</p>
<p>I notice that while classifier accuracy is decent, the AUC metric evaluates to below 0.5, which is incorrect (given that accuracy is better than 50/50 and my classes are balanced). The AUC also differs from sklearn.metrics.roc_auc_score. For a simple example:</p>
<pre><code>from torcheval.metrics.aggregation.auc import AUC
from torcheval.metrics import BinaryAccuracy

from sklearn.metrics import roc_auc_score, accuracy_score

p_pred = torch.tensor([0.2, 0.3, 0.4, 0.6, 0.7, 0.8])  # model est likelihood of target class
y_true = torch.tensor([0.0, 0.0, 1.0, 0.0, 1.0, 1.0])  # ground truth, 1 = target class

# TorchEval Metrics
auc_metric = AUC()
accuracy_metric = BinaryAccuracy(threshold=0.5)
auc_metric.reset()
accuracy_metric.reset()
auc_metric.update(p_pred,y_true)
accuracy_metric.update(input=p_pred,target=y_true)

print(f&quot;TorchEval Accuracy = {accuracy_metric.compute().item():.3}&quot;)
print(f&quot;Sklearn Accuracy   = {accuracy_score(y_true=y_true,y_pred=p_pred.round()):.3}&quot;)
print(f&quot;TorchEval AUC      = {auc_metric.compute().item():.3}&quot;)
print(f&quot;Sklearn AUC        = {roc_auc_score(y_true=y_true,y_score=p_pred):.3}&quot;)
</code></pre>
<p>Return an unexpected value of TorchEval AUC:</p>
<pre><code>TorchEval Accuracy = 0.667
Sklearn Accuracy   = 0.667
TorchEval AUC      = 0.3
Sklearn AUC        = 0.889
</code></pre>
<p>How can I correctly invoke TorchEval AUC to get the expected value of ~0.9?</p>
","1","Question"
"79511518","","<p>When I want to accelerate the model training by using deepspeed, a problem occured when I want to evaluate the model on validation dataset. Here is the problem code snippet:</p>
<pre><code>def evaluate(self, epoch_num=None, keep_all=True):
        print(&quot;self.model:&quot;, self.model)

        self.model = self.model.eval()
        print(&quot;self.model after eval:&quot;, self.model)
</code></pre>
<p>Then the output log:</p>
<pre><code>self.model: DeepSpeedEngine(
  (module): TSTransformerEncoder(
    (project_inp): Linear(in_features=6, out_features=128, bias=True)
    (pos_enc): LearnablePositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-2): 3 x TransformerBatchNormEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=256, out_features=128, bias=True)
          (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_layer): Linear(in_features=128, out_features=6, bias=True)
    (dropout1): Dropout(p=0.1, inplace=False)
  )
)
self.model after eval: None
</code></pre>
<p>Without using the DeepSpeed tool, the model can be trained and evaluated normally. However, after using DeepSpeed, the above problem occurs.</p>
<p>The way I initialize the deepspeed:</p>
<pre><code>    model, optimizer, _, _ = deepspeed.initialize(
        model=model,
        optimizer=optimizer,
        config_params=ds_config
    )
</code></pre>
<p>The ds_config file:</p>
<pre><code>{
    &quot;fp16&quot;: {
        &quot;enabled&quot;: true,
        &quot;loss_scale&quot;: 0,
        &quot;loss_scale_window&quot;: 1000,
        &quot;initial_scale_power&quot;: 16,
        &quot;hysteresis&quot;: 2,
        &quot;min_loss_scale&quot;: 1
    },
 
    &quot;optimizer&quot;: {
        &quot;params&quot;: {
            &quot;lr&quot;: 0.001,
            &quot;weight_decay&quot;: 0,
            &quot;optimizer_class&quot;: &quot;optimizers.RAdam&quot;
        }
    },
 
    &quot;zero_optimization&quot;: {
        &quot;stage&quot;: 1,
        &quot;overlap_comm&quot;: true,
        &quot;contiguous_gradients&quot;: true
    },


    &quot;zero_allow_untested_optimizer&quot;: true,
    &quot;train_batch_size&quot;: 256,
    &quot;steps_per_print&quot;: 2000,
    &quot;wall_clock_breakdown&quot;: false
}
</code></pre>
<h2>Problem Analysis</h2>
<p>I originally expected that <code>self.model.eval()</code> would only set the model to evaluation mode, and the model itself would not become <code>None</code>. However, the actual output shows that <code>self.model</code> becomes <code>None</code> after calling the <code>eval()</code> method. I suspect that this might be related to the encapsulation or configuration of DeepSpeed, but I'm not sure about the specific cause.</p>
<h3>Relevant Environment Information</h3>
<ul>
<li><p><strong>Python Version</strong>: 3.8.20</p>
</li>
<li><p><strong>PyTorch Version</strong>: 2.4.1</p>
</li>
<li><p><strong>DeepSpeed Version</strong>: 0.16.4</p>
</li>
</ul>
","1","Question"
"79512485","","<p>I've tried to use pip to install pytorch but it's saying it's not found.</p>
<p>Command I used: <code>pip install torch</code></p>
<p>Context: I'm using MacOS 15.3.2; architecture arm64; python 3.13.2</p>
<p>Output from the command:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch (from versions: none) ERROR: No matching distribution found for torch
</code></pre>
<p>I've tried specifying index as well, but to no avail. None of the following worked:</p>
<pre><code>pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cpu

pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu 

pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu 
</code></pre>
","-1","Question"
"79512576","","<p>I've written a code which goes through different datasets grouped in labels and lets the created model learn each dataset/label separately.</p>
<p>The problem now is that the model learns the first dataset perfectly well, but after that only learns new labels a little bit, if anything.</p>
<p>My code essentially (I've written a little bit more than just that, but here lies the problem) is:</p>
<pre><code>for index, datasetbatch in enumerate(dataset_batches): # going through the datasets (so, first only has a dataset with the label '0', the next having '1' and so on)
    trainModel(params, model, datasetbatch[1], index + 1, device)

def trainModel(params: Parameters, model: CreatedModel, train_loader, currentBatchNum, device):
    criterion = nn.CrossEntropyLoss()
    optimizer = get_optimizer(params, model=model)

    fitnesses_each_epoch = []

    for i in range(params.epochs):
        fitnesses_in_epoch = []
        start_time = time.time()

        for b, (X_train, y_train) in enumerate(train_loader):
            
            X_train = X_train.to(device)
            y_train = y_train.to(device)

            y_pred = model(X_train)
            

            # measure loss
            loss = criterion(y_pred, y_train)  # pred vs train value
            is_correct = getIsCorrect(y_pred, y_train) # ret 10 if correct prediction, 1
                                                       # if not
            fitnesses_in_epoch.append(is_correct)

            # back propagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            b += 1
        
        fitnesses_each_epoch.append(math.fmean(fitnesses_in_epoch))

    manager.fitnessesResult('TRAIN in batch ' + str(currentBatchNum), fitnesses_each_epoch)
</code></pre>
<p>The output for once was essentially this, which shows that the &quot;knowledge&quot; created by going through the first dataset in the training phase is still perfectly there (10) in the testing phase, even after it should have learned some new and different labels, while there is no &quot;knowledge&quot; generated by training the model with those datasets (as seen that it doesn't go above the minimal number 1):</p>
<pre><code>TEST in type tensor([0]): mean: 10.0
TEST in type tensor([1]): mean: 1.0
TEST in type tensor([2]): mean: 1.0
</code></pre>
<p>Each dataset-batch has ca 6k elements.</p>
<p>The model goes through two epochs for each dataset</p>
<p>The model layers are:</p>
<pre><code>Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
GRU(400, 32)
RNN(32, 32)
GRU(32, 40)
Dropout(p=0.778, inplace=False)
</code></pre>
<p>extra information:</p>
<pre><code>activation functions: ['sigmoid', 'none', 'none', 'sigmoid']
epochs: 2
learn rate: 0.01
optimizer: adam
test size: 0.8
</code></pre>
<p>Though it can't be a problem with the configuration of the model, I think, as I've tried it with multiple different layer, optimizer, epochs, learn-rate and test-size configurations. The end-result, though it's not always as extreme as just &quot;10&quot; at first and after that only &quot;1&quot;s, but it's always in the nearby area.</p>
<p>Could this have something to do with the fact that, after the first dataset batch, the model is already trained for that batch and that somehow affects the training of the new batch?</p>
","0","Question"
"79513638","","<pre><code># create a Pytorch optimizer 
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):
    if iter % eval_iters == 0:
        losses = estimate_loss()
        print(f&quot;step: {iter}, loss {losses}&quot;)
        
    # sample a batch of data
    xb, yb = get_batch(&quot;train&quot;)
    
    #evaluate the loss
    logits, loss = model.forward(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    
print(loss.item())
</code></pre>
<p>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)</p>
<p>I tried adding model = model.to(device) and adding .cuda() after inputs, but none of them worked. I'm struggling to get it fixed.</p>
","1","Question"
"79515545","","<p>I have been training a CNN Autoencoder on binary images (pixels are either 0 or 1) of size 64x64. The model is shown below:</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNAutoencoder(nn.Module):
    &quot;&quot;&quot;
    Input shape: (B, 1, 64, 64)
    &quot;&quot;&quot;
    def __init__(self, grid_size=64):
        super().__init__()
        self.grid_size = grid_size

        # 1) Encoder layers
        self.enc_conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.bn_enc1   = nn.BatchNorm2d(16) 
        self.enc_conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn_enc2   = nn.BatchNorm2d(32)
        self.pool      = nn.MaxPool2d(kernel_size=2, stride=2)

        # After two pools =&gt; shape is (32, grid_size//4, grid_size//4)
        flat_dim    = 32 * (grid_size // 4) * (grid_size // 4)
        latent_dim  = 128

        # For linear layers, we can use BatchNorm1d:
        self.fc_enc    = nn.Linear(flat_dim, latent_dim)
        self.bn_fc_enc = nn.BatchNorm1d(latent_dim)

        # 2) Decoder layers
        self.fc_dec    = nn.Linear(latent_dim, flat_dim)
        self.bn_fc_dec = nn.BatchNorm1d(flat_dim)

        self.dec_tconv1 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)
        self.bn_dec1    = nn.BatchNorm2d(16)
        self.dec_tconv2 = nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2)

    def encoder(self, x):
        # x =&gt; (B,1,64,64)
        x = self.enc_conv1(x)          # =&gt; (B,16,64,64)
        x = self.bn_enc1(x)           
        x = F.relu(x)
        x = self.pool(x)              # =&gt; (B,16,32,32)

        x = self.enc_conv2(x)         # =&gt; (B,32,32,32)
        x = self.bn_enc2(x)          
        x = F.relu(x)
        x = self.pool(x)              # =&gt; (B,32,16,16)

        # Flatten
        x = x.view(x.size(0), -1)     # =&gt; (B, flat_dim=32*16*16)
        x = self.fc_enc(x)            # =&gt; (B, latent_dim=128)
        x = self.bn_fc_enc(x)         
        x = F.relu(x)                 
        return x

    def decoder(self, z):
        # z =&gt; (B,128)
        x = self.fc_dec(z)            # =&gt; (B, flat_dim)
        x = self.bn_fc_dec(x)
        x = F.relu(x)

        # Reshape to (B, 32, 16, 16)
        B = x.size(0)
        x = x.view(B, 32, self.grid_size // 4, self.grid_size // 4)

        x = self.dec_tconv1(x)        # =&gt; (B,16,32,32)
        x = self.bn_dec1(x)
        x = F.relu(x)

        # Final upsample =&gt; (B,1,64,64)
        x = self.dec_tconv2(x)
        x = torch.sigmoid(x)
        return x

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)
</code></pre>
<p>Train set includes 1079156 samples. I'm using batch size 128 and 7 epochs. Right now, 1 epoch takes approximately 3 hours to train. Does anyone have an idea what might be the problem?</p>
<p>I tried to switch between BatchNorm and LayerNorm/GroupNorm but the performance didn't change. I also tried early stopping, but this doesn't solve the problem of timing per epoch. For your reference, I've pasted my training code below:</p>
<pre><code>train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)
            val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

            # Model
            model = CNNAutoencoder(grid_size=64).to(device)
            optimizer = optim.Adam(model.parameters(), lr=1e-3)
            loss_fn = nn.BCELoss()

            best_val_loss = float('inf')
            model_fname = f&quot;best_{rt}_fold{fold}.pth&quot;

            global_step = 0

            for ep in range(num_epochs):
                if global_step &gt;= max_steps:
                    print(f&quot;Reached {max_steps} total steps; stopping early.&quot;)
                    break

                t0 = time.time()

                #############################
                # Train Loop (per epoch)
                #############################
                model.train()
                total_loss = 0.0
                for x_in, x_tgt in train_loader:
                    x_in = x_in.to(device)
                    x_tgt = x_tgt.to(device)

                    optimizer.zero_grad()
                    out = model(x_in)
                    loss = loss_fn(out, x_tgt)
                    loss.backward()
                    optimizer.step()

                    total_loss += loss.item() * x_in.size(0)

                    global_step += 1  # increment step count
                    if global_step &gt;= max_steps:
                        print(f&quot;Reached {max_steps} total steps; stopping in mid-epoch.&quot;)
                        break

                train_epoch_loss = total_loss / len(train_loader.dataset)

                # If we already hit max_steps, break out
                if global_step &gt;= max_steps:
                    break

                #############################
                # Validation Loop
                #############################
                model.eval()
                val_loss_sum = 0.0
                with torch.no_grad():
                    for x_in, x_tgt in val_loader:
                        x_in, x_tgt = x_in.to(device), x_tgt.to(device)
                        out = model(x_in)
                        loss = loss_fn(out, x_tgt)
                        val_loss_sum += loss.item() * x_in.size(0)

                current_val_loss = val_loss_sum / len(val_loader.dataset)
                dt = time.time() - t0

                print(f&quot;Epoch [{ep+1}/{num_epochs}] =&gt; &quot;
                      f&quot;train_loss={train_epoch_loss:.4f}, val_loss={current_val_loss:.4f}, dt={dt:.2f}s, &quot;
                      f&quot;global_step={global_step}&quot;)

                # Save best model
                if current_val_loss &lt; best_val_loss:
                    best_val_loss = current_val_loss
                    torch.save(model.state_dict(), model_fname)```
</code></pre>
","0","Question"
"79516939","","<p>I have 2 matrices P and V and when I take their dot product with triton I get results that are inconsistent with pytorch.</p>
<p>The P and V matrices are as follows. P is basically the softmax which is why it is mostly 0s except the final column, the result of the dot product should be the final row of V.</p>
<pre><code>P = torch.zeros((32,32), device = 'cuda',  dtype = torch.float32)
P[:,-1] = 1
V = torch.arange(32*64, 64 * 64, device = 'cuda', dtype = torch.float32). reshape(32, 64)
</code></pre>
<p>On calling tl.dot(P, V) both are loading correctly (or so they appear to me), but the output is</p>
<pre><code>[4032., 4032., 4034., 4034., 4036., 4036., 4038., 4038., 4040., 4040.,
         4042., 4042., 4044., 4044., 4046., 4046., 4048., 4048., 4050., 4050.,
         4052., 4052., 4054., 4054., 4056., 4056., 4058., 4058., 4060., 4060.,
         4062., 4062., 4064., 4064., 4066., 4066., 4068., 4068., 4070., 4070.,
         4072., 4072., 4074., 4074., 4076., 4076., 4078., 4078., 4080., 4080.,
         4082., 4082., 4084., 4084., 4086., 4086., 4088., 4088., 4090., 4090.,
         4092., 4092., 4094., 4094.]
</code></pre>
<p>instead of what I get from torch.matmul which is</p>
<pre><code>[4032., 4033., 4034., 4035., 4036., 4037., 4038., 4039., 4040., 4041.,
         4042., 4043., 4044., 4045., 4046., 4047., 4048., 4049., 4050., 4051.,
         4052., 4053., 4054., 4055., 4056., 4057., 4058., 4059., 4060., 4061.,
         4062., 4063., 4064., 4065., 4066., 4067., 4068., 4069., 4070., 4071.,
         4072., 4073., 4074., 4075., 4076., 4077., 4078., 4079., 4080., 4081.,
         4082., 4083., 4084., 4085., 4086., 4087., 4088., 4089., 4090., 4091.,
         4092., 4093., 4094., 4095.]
</code></pre>
<p>The following is the code I'm testing this out in</p>
<pre><code>import triton
import triton.language as tl
import torch
torch.cuda.is_available()
torch.set_printoptions(profile=&quot;full&quot;)
@triton.jit
def test_kernel(x_ptr,y_ptr,output_ptr,
                M, K, N,
                stride_xm, stride_xk,
                stride_yk, stride_yn,
                stride_om, stride_on,
                BLOCK_SIZE_M: tl.constexpr,
                BLOCK_SIZE_K: tl.constexpr,
                BLOCK_SIZE_N: tl.constexpr):
    pid_m = tl.program_id(axis = 0) * BLOCK_SIZE_M
    pid_n = tl.program_id(axis = 1) * BLOCK_SIZE_N
    x_ptr += (pid_m + tl.arange(0, BLOCK_SIZE_M))[:,None] * stride_xm +  (pid_n + tl.arange(0, BLOCK_SIZE_K))[None,:]*stride_xk
    y_ptr += (pid_m + tl.arange(0, BLOCK_SIZE_K))[:,None] * stride_yk +  (pid_n + tl.arange(0, BLOCK_SIZE_N))[None,:]*stride_yn
    x = tl.load(x_ptr) 
    y = tl.load(y_ptr) 
    output_offset = (pid_m + tl.arange(0, BLOCK_SIZE_M))[:,None] *stride_om + (pid_n + tl.arange(0, BLOCK_SIZE_N))[None, :] *stride_on
    tl.store(output_ptr + output_offset,  tl.dot(x,y))

def helper(x: torch.Tensor, y: torch.Tensor):
    M , K = x.shape
    K1, N = y.shape
    assert K == K1
    output = torch.empty((M, N), device = 'cuda', dtype = torch.float32)
    assert x.is_cuda and y.is_cuda and output.is_cuda

    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']), triton.cdiv(N, meta['BLOCK_SIZE_N']),)
    test_kernel[grid](x, y, output, 
                        M, K, N, 
                        x.stride(0), x.stride(1),
                        y.stride(0), y.stride(1),
                        output.stride(0), output.stride(1),
                        BLOCK_SIZE_N = 64, 
                        BLOCK_SIZE_K = 32, 
                        BLOCK_SIZE_M = 32, 
                        )
    return output
</code></pre>
<p>The strangest thing is when I define <code>V = torch.arange(0, 32*64, device = 'cuda', dtype = torch.float32). reshape(32, 64)</code> it works as expected. Is there something with pointer operations that I'm missing here?</p>
","1","Question"
"79517641","","<p>I'm currently working on transfer learning with InsightFace using the glint360k_cosface_r100_fp16_0.1 model from the ArcFace Torch section. However, I'm facing issues with either overfitting or underfitting on my dataset, and I'm not sure what I'm doing wrong. Here are the problems I'm encountering:</p>
<ol>
<li>My dataset consists of 127 individuals, with only 7 images per person taken from different angles: front view, 3/4 left and right, upper view, lower view, and profile left and right. This results in a total dataset of 889 images.</li>
<li>Initially, I split the data 80% for training and 20% for validation at the folder level, meaning each person has 5 training images and 2 validation images. This led to underfitting, likely due to insufficient training data per person.</li>
<li>To address this, I performed data augmentation first and then applied the same 80/20 split. However, this resulted in overfitting, as I suspect the model was &quot;cheating&quot; by memorizing patterns from the augmented images rather than generalizing.</li>
</ol>
<p>Since I'm new to this field, I would really appreciate detailed feedback on what mistakes I might be making in my approach or code. Thanks in advance!</p>
<p>Below is the pseudocode representing my approach:</p>
<pre><code>BEGIN

# ---- SETUP ENVIRONMENT ----
SET CUDA and OpenCV paths
SET PyTorch memory allocation config

# ---- IMPORT LIBRARIES ----
IMPORT required libraries (Torch, NumPy, OpenCV, InsightFace, etc.)

# ---- DEFINE FaceDataset CLASS ----
CLASS FaceDataset:
    INITIALIZE dataset directory, transformations, and cache
    IF cache exists:
        LOAD dataset from cache
    ELSE:
        INITIALIZE face detection model (InsightFace)
        SCAN dataset directory
        FOR each image folder:
            FOR each image:
                DETECT face
                IF face detected:
                    CROP and RESIZE to (112,112)
                    STORE in dataset
        SAVE dataset to cache
    
    FUNCTION _detect_face(image):
        READ image
        CONVERT to RGB
        DETECT faces using InsightFace
        IF face detected:
            CROP, RESIZE, RETURN face
        ELSE:
            RETURN None

    FUNCTION __getitem__(index):
        RETURN image and label

    FUNCTION __len__():
        RETURN number of samples

# ---- DEFINE FaceRecognitionModel CLASS ----
CLASS FaceRecognitionModel:
    INITIALIZE ResNet50 backbone
    FREEZE lower layers, fine-tune upper layers
    ADD fully connected classifier with dropout
    FUNCTION forward(input):
        PASS through backbone
        PASS through classifier head
        RETURN output

# ---- DEFINE TRAINING FUNCTION ----
FUNCTION train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):
    INITIALIZE metrics storage
    SET early stopping threshold
    
    FOR epoch in range(num_epochs):
        IF warm-up phase:
            ADJUST learning rate
        
        # ---- TRAIN PHASE ----
        SET model to training mode
        FOR batch in train_loader:
            LOAD input images and labels
            COMPUTE predictions
            CALCULATE loss
            BACKPROPAGATE and update weights

        # ---- VALIDATION PHASE ----
        SET model to evaluation mode
        FOR batch in val_loader:
            COMPUTE predictions
            CALCULATE validation loss
        UPDATE scheduler with validation loss
        CHECK for early stopping condition

    RETURN best model

# ---- DEFINE VALIDATION ----
FUNCTION split_data():
    EXTRACT person identity from filenames
    PERFORM GroupShuffleSplit to avoid identity leakage
    RETURN train and validation indices

# ---- DEFINE DATA AUGMENTATION ----
FUNCTION get_transforms():
    RETURN image augmentation pipeline (flip, resize, normalize)

# ---- DEFINE ONNX EXPORT FUNCTION ----
FUNCTION export_to_onnx(model, save_path):
    CONVERT PyTorch model to ONNX format
    VERIFY conversion
    RETURN ONNX model

# ---- MAIN FUNCTION ----
FUNCTION main():
    SET dataset path, cache directory, and logging path
    INITIALIZE dataset with caching enabled
    SPLIT dataset ensuring unique individuals in train and validation
    APPLY data augmentation
    CREATE data loaders for training and validation
    
    # ---- MODEL INITIALIZATION ----
    LOAD ResNet50 backbone
    INITIALIZE FaceRecognitionModel
    SET loss function, optimizer, and scheduler
    
    # ---- TRAIN THE MODEL ----
    CALL train_model()
    
    # ---- EXPORT TRAINED MODEL ----
    CALL export_to_onnx()

    PRINT &quot;Training Complete!&quot;

# ---- RUN MAIN FUNCTION ----
IF __name__ == &quot;__main__&quot;:
    CALL main()

END
</code></pre>
","1","Question"
"79518779","","<p>I found single LLM input get different output logits when merging into a batch for inference.</p>
<p>Besides, I need to use inputs_embeds as model input.</p>
<p>My test LLM is &quot;Qwen/Qwen2.5-1.5B-Instruct&quot; and the test code is below.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# load model and tokenizezr
model_name = &quot;Qwen/Qwen2.5-1.5B-Instruct&quot;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# set model eval
model.eval()

# input texts
texts = ['a', 'b', 'c']

# tokenize
inputs = tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True, truncation=True).to(model.device)

# get inputs_embeds
with torch.no_grad():
    inputs_embeds = model.get_input_embeddings()(inputs.input_ids)

# get attention_mask and position_ids
attention_mask = inputs.attention_mask
position_ids = torch.arange(inputs.input_ids.shape[1], device=model.device).unsqueeze(0).expand(inputs.input_ids.shape[0], -1)

# batch
with torch.no_grad():
    output_batch = model(
        inputs_embeds=inputs_embeds,
        attention_mask=attention_mask,
        position_ids=position_ids
    ).logits[0]  # 取第一个文本的 logits

# single
with torch.no_grad():
    output_single = model(
        inputs_embeds=inputs_embeds[0].unsqueeze(0),  # 添加批次维度
        attention_mask=attention_mask[0].unsqueeze(0),
        position_ids=position_ids[0].unsqueeze(0)
    ).logits[0]  # 取第一个文本的 logits

# check consistency
is_close = torch.allclose(output_batch, output_single, atol=1e-5, rtol=1e-3)
print(is_close)
</code></pre>
<p>I tried all the methods Deepseek suggested and then all faild, like setting attention masks, positions and so on.</p>
<p>I want the same output logits of a single input text as the ones extracted from batch output results.</p>
","0","Question"
"79519074","","<p>Test Time Augmentation (TTA) in FastAI should be easily applied with <code>learn.tta</code>, yet has led to numerous issues in my Cloud Run deployment. I have a working Cloud Run deployment that does base learner and metalearner scoring as a prediction endpoint using <code>load_learner</code> from FastAI.</p>
<p>I want to switch <code>learn.predict</code> to <code>learn.tta</code> but issues keep arising. FastAI requires a slightly different input shape for <code>tta</code> and has different shape of returned values. I wanted to make it more of a direct drop-in replacement for <code>learn.predict</code>. This function worked to accomplish that in a minimalistic <a href=""https://colab.research.google.com/drive/1X_GLZHmiTOKfjkmdrJwJZmgzC_U4dhk9?usp=sharing"" rel=""nofollow noreferrer"">test notebook</a> on Colab:</p>
<pre><code>import random
from fastai.vision.all import *

# Function to perform TTA and format the output to match predict
def tta_predict(learner, img):
    # Create a DataLoader for the single image using the test DataLoader
    test_dl = learner.dls.test_dl([img])
    
    # Perform TTA on the single image using the test DataLoader
    preds, _ = learner.tta(dl=test_dl)
    
    # Get the average probabilities
    avg_probs = preds.mean(dim=0)
    
    # Get the predicted class index
    pred_idx = avg_probs.argmax().item()
    
    # Get the class label
    class_label = learner.dls.vocab[pred_idx]
    
    # Format the output to match the structure of the predict method
    return (class_label, pred_idx, avg_probs)

# Use the tta_predict function
prediction = tta_predict(learn, grayscale_img)

# Print the results
print(type(prediction))  # Print the type of the prediction object
print(prediction)  # Print the prediction itself (class label, index, probabilities)
print(prediction[0])  # Print the predicted class label
print(prediction[2])  # Print the average probabilities
</code></pre>
<p>Although it seemed to work fine in the notebook, when I add that to the top of my production script and switch <code>learn.predict</code> to <code>tta_predict(learn, img)</code> for my base learners, the entire image starts to fail to build with Python 3.9:</p>
<pre><code>Traceback (most recent call last): File &quot;/app/main.py&quot;, line 11, in &lt;module&gt; 
from fastai.vision.all import PILImage, BCEWithLogitsLossFlat, load_learner 
    File &quot;/usr/local/lib/python3.9/site-packages/fastai/vision/all.py&quot;, line 4, 
in &lt;module&gt; from .augment import * File &quot;/usr/local/lib/python3.9/
site-packages/fastai/vision/augment.py&quot;, line 8, in &lt;module&gt; from .core import * File &quot;/usr/local/lib/python3.9/site-packages/fastai/vision/core.py&quot;, line 259, in &lt;module&gt; class PointScaler(Transform): File &quot;/usr/local/lib/python3.9/site-packages/fasttransform/transform.py&quot;, line 75, in __new__ if funcs: setattr(new_cls, nm, _merge_funcs(*funcs)) File &quot;/usr/local/lib/python3.9/site-packages/fasttransform/transform.py&quot;, line 42, in _merge_funcs res = Function(fs[-1].methods[0].implementation) File &quot;/usr/local/lib/python3.9/site-packages/plum/function.py&quot;, line 181, in methods self._resolve_pending_registrations() File &quot;/usr/local/lib/python3.9/site-packages/plum/function.py&quot;, line 280, in _resolve_pending_registrations signature = Signature.from_callable(f, precedence=precedence) File &quot;/usr/local/lib/python3.9/site-packages/plum/signature.py&quot;, line 88, in from_callable types, varargs = _extract_signature(f) File &quot;/usr/local/lib/python3.9/site-packages/plum/signature.py&quot;, line 346, in _extract_signature resolve_pep563(f) File &quot;/usr/local/lib/python3.9/site-packages/plum/signature.py&quot;, line 329, in resolve_pep563 beartype_resolve_pep563(f) # This mutates `f`. File &quot;/usr/local/lib/python3.9/site-packages/beartype/peps/_pep563.py&quot;, line 263, in resolve_pep563 arg_name_to_hint[arg_name] = resolve_hint( File &quot;/usr/local/lib/python3.9/site-packages/beartype/_check/forward/fwdmain.py&quot;, line 308, in resolve_hint return _resolve_func_scope_forward_hint( File &quot;/usr/local/lib/python3.9/site-packages/beartype/_check/forward/fwdmain.py&quot;, line 855, in _resolve_func_scope_forward_hint raise exception_cls(exception_message) from exception beartype.roar.BeartypeDecorHintPep604Exception: Stringified PEP 604 type hint 'PILBase | TensorImageBase' syntactically invalid under Python &lt; 3.10 (i.e., TypeError(&quot;unsupported operand type(s) for |: 'BypassNewMeta' and 'torch._C._TensorMeta'&quot;)). Consider either:
        * Requiring Python &gt;= 3.10. Abandon Python &lt; 3.10 all ye who code here.
        * Refactoring PEP 604 type hints into equivalent PEP 484 type hints: e.g.,
        # Instead of this...
        from __future__ import annotations
        def bad_func() -&gt; int | str: ...
        # Do this. Ugly, yet it works. Worky &gt;&gt;&gt;&gt; pretty.
        from typing import Union
</code></pre>
<p>I don't see anything in my code that could've caused that, yet there it is. I noticed somewhere in those messages it mentions &quot;augment&quot;, which I take as confirmation that TTA is at fault (it was also the only thing that changed). So, I tried switching the Python version to 3.10. Now it builds but it's clearly broken:</p>
<pre><code>ERROR loading model.pkl: Could not import 'Pipeline' from fastcore.transform - this module has been moved to the fasttransform package.
To migrate your code, please see the migration guide at: https://answerdotai.github.io/fasttransform/fastcore_migration_guide.html
</code></pre>
<p>The migration guide it mentions says to change</p>
<p><code>from fastcore.transform import Transform, Pipeline</code> to</p>
<p><code>from fasttransform import Transform, Pipeline</code></p>
<p>but my code never directly imports <code>Pipeline</code> or <code>Transform</code>, nor does it directly import <code>fastcore</code>.</p>
","0","Question"
"79519918","","<p>I can't understand if <a href=""https://pytorch.org/docs/stable/generated/torch.scatter.html"" rel=""nofollow noreferrer""><code>torch.scatter</code></a> or <a href=""https://pytorch.org/docs/stable/generated/torch.gather.html"" rel=""nofollow noreferrer""><code>torch.gather</code></a> could be used to reduce values of a tensor according to a reduction function over specified indices.</p>
<p>I've frequently used the <a href=""https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.Aggregation.html"" rel=""nofollow noreferrer""><code>torch_geometric.nn.aggr</code></a>` module's functions in order to perform aggregations over indices, and I want to reproduce these very useful functions using pure PyTorch operations.</p>
","0","Question"
"79520329","","<p>I am using MNIST in the c++ frontend, and I want to use a reduced version of it. However, the c++ frontend does not appear to have torch.utils.data.Subset or RandomizedSubsetSampler to use a subset of the dataset like you would in normal pytorch, is there any other possibility?</p>
<p>I am using the MNIST example from <a href=""https://github.com/pytorch/examples/blob/main/cpp/mnist/mnist.cpp"" rel=""nofollow noreferrer"">the official torch c++ example repo</a></p>
<p>The way it loads the dataset is the following:</p>
<pre><code>auto train_dataset = torch::data::datasets::MNIST(kDataRoot)
                           .map(torch::data::transforms::Normalize&lt;&gt;(0.1307, 0.3081))
                           .map(torch::data::transforms::Stack&lt;&gt;());
const size_t train_dataset_size = train_dataset.size().value();
</code></pre>
<p>I tried creating a custom sampler like this:</p>
<pre><code>class SubsetSampler : public torch::data::samplers::Sampler&lt;&gt; {
    public:
    explicit SubsetSampler(std::vector&lt;size_t&gt; indices)
        : indices_(std::move(indices)) {}

    // Return the next batch of indices.
    c10::optional&lt;std::vector&lt;size_t&gt;&gt; next(size_t batch_size) override {
        std::vector&lt;size_t&gt; batch;
        while (batch.size() &lt; batch_size &amp;&amp; current_ &lt; indices_.size()) {
            batch.push_back(indices_[current_++]);
        }
        if (batch.empty()) {
            return c10::nullopt;  // No more data.
        }
            return batch;
    }

    // Reset the sampler's state.
    void reset()  {
        current_ = 0;
    }

    // Return the total number of samples.
    c10::optional&lt;size_t&gt; size() {
        return indices_.size();
    }

    private: 
    std::vector&lt;size_t&gt; indices_; size_t current_ = 0; 

};
</code></pre>
<p>But when I use it in the MNIST example like this and build it:</p>
<pre><code>  std::vector&lt;size_t&gt; subset_indices(subset_size);
  std::iota(subset_indices.begin(), subset_indices.end(), 0);

  auto sampler = std::make_shared&lt;SubsetSampler&gt;(std::move(subset_indices));

  auto train_loader = torch::data::make_data_loader(
    std::move(train_dataset),
    sampler,
    torch::data::DataLoaderOptions().batch_size(kTrainBatchSize));
</code></pre>
<p>I get these errors:</p>
<pre class=""lang-none prettyprint-override""><code>error: no type named ‘BatchRequestType’ in ‘class std::shared_ptr&lt;SubsetSampler&gt;’
   23 | class StatelessDataLoader : public DataLoaderBase&lt;
      |       ^~~~~~~~~~~~~~~~~~~
</code></pre>
","1","Question"
"79525759","","<p>I'm loading a model checkpoint using</p>
<pre><code>model.load_state_dict(state_dict, strict=False)
</code></pre>
<p>because the model architecture does not fully match the weights. As expected, this results in a warning message like this:</p>
<pre><code>The model and loaded state dict do not match exactly
unexpected key in source state_dict: backbone.blocks.0.mlp.experts.0.weight, ...
</code></pre>
<p>The warning is very long because there are many unexpected keys. I understand this is just a warning and not an actual error, and I want to suppress or hide this message entirely.</p>
<p>Is there a clean way to do this in PyTorch?</p>
","0","Question"
"79528937","","<p>Consider a regression task where the parameters of the model differ significantly in magnitude, say:</p>
<pre class=""lang-py prettyprint-override""><code>def func(x, p):
    p1, p2, p3 = p
    return np.sin(p1*x) * np.exp(p2*x) * p3

# True Parameters:
p1, p2, p3 = np.pi/0.01, -1.25, 1.2356  # 314.1592, -1.25, 1.2356
</code></pre>
<p>The system requires a feasible initial guess to converge to the correct solution, for example:</p>
<pre class=""lang-py prettyprint-override""><code>p0 = [np.pi/0.01-80, -1.25-1, 1.2356-1]
</code></pre>
<p>However, if <strong>the initial guess is too far from the true solution</strong>, the regression may fail to converge. For example, the following initial guess might not work:</p>
<pre class=""lang-py prettyprint-override""><code>p0 = [np.pi/0.02-10, -1.25-1, 1.2356-1]
</code></pre>
<p>The reproducible code is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.optimize import least_squares
import matplotlib.pyplot as plt

def func(x, p):
    p1, p2, p3 = p
    return np.sin(p1*x) * np.exp(p2*x) * p3

def residuals(p, y, x):
    return y - func(x, p)

x = np.linspace(0, 0.05, 50)
p1, p2, p3 = np.pi/0.01, -1.25, 1.2356
y0 = func(x, [p1, p2, p3])
y = y0 + np.random.randn(len(x)) * 0.05

p0 = [np.pi/0.02-10, -1.25-1, 1.2356-1]

result = least_squares(residuals, p0, args=(y, x))

print(&quot;True parameters:&quot;, [p1, p2, p3])
print(&quot;Approximated parameters:&quot;, result.x)

x_test = np.linspace(0, 0.05, 200)
y_test = func(x_test, result.x)
y_real = func(x_test, [p1, p2, p3])
plt.plot(x_test, y_test, label=&quot;predict&quot;)
plt.plot(x, y, '.r', label=&quot;real&quot;)
</code></pre>
<p>Or here is a PyTorch version:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
from torch import nn
import matplotlib.pyplot as plt

class guessEq(nn.Module):
    def __init__(self):
        super(guessEq, self).__init__()
        self.params = nn.Parameter(torch.tensor([np.pi/0.01-10, -1.25+1, 1.2356-1]))
    
    def forward(self, x):
        out = torch.sin(self.params[0]*x) * \
            torch.exp(-self.params[1]*x) * \
            self.params[2]
        return out

x = np.linspace(0, 0.05, 100)
y = np.sin(np.pi/0.01*x) * np.exp(-1.25*x) * 1.2356 + np.random.rand(x.shape[0]) * 0.05

x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)
x = x.reshape((-1, 1))
y = y.reshape((-1, 1))

model = guessEq()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
mse = nn.MSELoss()
for i in range(2000):
    optimizer.zero_grad()
    y_pred = model(x)
    loss = torch.mean(torch.square(y_pred - y))
    loss.backward()
    if i % 100 == 0:
        print(loss)
    optimizer.step()

x_test = torch.linspace(0, 0.05, 200).reshape((-1, 1))
y_test = model(x_test)
print(&quot;True parameters: [{} {} {}]&quot;.format(np.pi/0.01, -1.25, 1.2356))
print(&quot;Approximated parameters: {}&quot;.format(model.params.detach().numpy()))
plt.plot(x_test.detach().cpu().numpy().flatten(), y_test.detach().cpu().numpy().flatten(), c=&quot;blue&quot;, linewidth=2, label=&quot;predict&quot;)
plt.plot(x.detach().cpu().numpy().flatten(), y.detach().cpu().numpy().flatten(), '.r', label=&quot;train&quot;)
plt.legend()
plt.show()
</code></pre>
<p>How to address the issue when the initial guess is far from the true solution, or when there is no prior of the initial guess?</p>
","0","Question"
"79533110","","<p>I'm just testing AWS tools to serve using sagemaker. My current progress is that I've already created a trained model, and compressed pth, inference.py, requirement.txt, and other necessary files into tar.gz and deployed them to an s3 bucket.</p>
<pre><code>$ tar -tzf birefnet.tar.gz
./
./ckpt/
./ckpt/BiRefNet-general-epoch_244.pth
./config.py
./dataset.py
./evaluation/
./inference.py
./requirements.txt
.
.
</code></pre>
<p>Here are the contents of the compressed file.
After uploading to the bucket, I tried to load the tar.gz file in the bucket using pytorchmodel in the sagemaker notebook. However, I keep getting an error that inference.py is missing. I tried asking for help from chatgpt and many others, but was unsuccessful.
Here is the code I wrote:</p>
<pre><code>import sagemaker
from sagemaker.pytorch import PyTorchModel


sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()  

model_data = 's3://sagemaker-birefnet-up/path/to/birefnet.tar.gz'

pytorch_model = PyTorchModel(
    model_data=model_data,
    role=role,
    entry_point='inference.py',
    framework_version='2.5.1',
    py_version='py311',
    sagemaker_session=sagemaker_session
)

predictor = pytorch_model.deploy(
    initial_instance_count=1,
    instance_type='ml.t2.medium',
)
</code></pre>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory: 'inference.py'</p>
</blockquote>
<p>What did I do wrong?</p>
<p>I thought what I wanted to do was simple. I tried recompressing and changing the model path, but nothing worked.</p>
","0","Question"
"79536891","","<p>Is using two different <code>nn.ModuleList()</code> zipped lists correct to build the computational graph for training a neural net in <code>PyTorch</code>? <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"" rel=""nofollow noreferrer"">nn.ModuleList</a> is a wrapper around Python's list with a registration of a module for training.</p>
<p>I'm building a network which consists of 2x interchanging types of blocks in <code>__init__</code>:</p>
<pre class=""lang-py prettyprint-override""><code>    def __init__(self, in_channels):
        super().__init__()

        self.encoder_conv_blocks = nn.ModuleList()
        self.downsample_blocks = nn.ModuleList()

        for out_channels in _FILTERS:
            conv_block = _ConvBlock(in_channels, _CONV_BLOCK_LEN, _CONV_BLOCK_GROWTH_RATE)
            downsample_block = _DownsampleBlock(conv_block.out_channels, out_channels)

            self.encoder_conv_blocks.append(conv_block)
            self.downsample_blocks.append(downsample_block)

            in_channels = out_channels
</code></pre>
<p>later in <code>forward</code>, I'm zipping the layers, as I need the outputs of the first type of block later in skip connections:</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(self, x):
        skip_connections = []
        
        for conv_block, downsample_block in zip(self.encoder_conv_blocks,
                                                self.downsample_blocks):
            x = conv_block(x)
            skip_connections.append(x)
            x = downsample_block(x)
</code></pre>
<p>However when pritting the summary <a href=""https://github.com/TylerYep/torchinfo"" rel=""nofollow noreferrer"">torchinfo</a>, we can see that summary of the registered methods using 2x zipped <code>nn.ModuleList</code> looks different compared to the summary where one single <code>nn.ModuleList</code> was used. I suspect that this can cause issues for training and inference in the future.</p>
<p><code>zip(nn.ModuleList(), nn.ModuleList())</code>:</p>
<pre><code>========================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #
========================================================================================================================
MyNet                                     [16, 4, 128, 256]         [16, 3, 128, 256]         --
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-1                        [16, 4, 128, 256]         [16, 84, 128, 256]        26,360
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-2                  [16, 84, 128, 256]        [16, 64, 64, 128]         48,448
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-3                        [16, 64, 64, 128]         [16, 144, 64, 128]        70,160
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-4                  [16, 144, 64, 128]        [16, 128, 32, 64]         166,016
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-5                        [16, 128, 32, 64]         [16, 208, 32, 64]         116,880
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-6                  [16, 208, 32, 64]         [16, 128, 16, 32]         239,744
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-7                        [16, 128, 16, 32]         [16, 208, 16, 32]         116,880
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-8                  [16, 208, 16, 32]         [16, 128, 8, 16]          239,744
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-9                        [16, 128, 8, 16]          [16, 208, 8, 16]          116,880
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-10                 [16, 208, 8, 16]          [16, 256, 4, 8]           479,488
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-11                       [16, 256, 4, 8]           [16, 336, 4, 8]           210,320
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-12                 [16, 336, 4, 8]           [16, 256, 2, 4]           774,400
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-13                       [16, 256, 2, 4]           [16, 336, 2, 4]           210,320
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-14                 [16, 336, 2, 4]           [16, 512, 1, 2]           1,548,800
</code></pre>
<p>single <code>nn.ModuleList()</code>:</p>
<pre><code>MyNet                                     [16, 4, 128, 256]         [16, 3, 128, 256]         --
├─ModuleList: 1-1                             --                        --                        --
│    └─_ConvBlock: 2-1                        [16, 4, 128, 256]         [16, 84, 128, 256]        26,360
│    └─_DownsampleBlock: 2-2                  [16, 84, 128, 256]        [16, 64, 64, 128]         48,448
│    └─_ConvBlock: 2-3                        [16, 64, 64, 128]         [16, 144, 64, 128]        70,160
│    └─_DownsampleBlock: 2-4                  [16, 144, 64, 128]        [16, 128, 32, 64]         166,016
│    └─_ConvBlock: 2-5                        [16, 128, 32, 64]         [16, 208, 32, 64]         116,880
│    └─_DownsampleBlock: 2-6                  [16, 208, 32, 64]         [16, 128, 16, 32]         239,744
│    └─_ConvBlock: 2-7                        [16, 128, 16, 32]         [16, 208, 16, 32]         116,880
│    └─_DownsampleBlock: 2-8                  [16, 208, 16, 32]         [16, 128, 8, 16]          239,744
│    └─_ConvBlock: 2-9                        [16, 128, 8, 16]          [16, 208, 8, 16]          116,880
│    └─_DownsampleBlock: 2-10                 [16, 208, 8, 16]          [16, 256, 4, 8]           479,488
│    └─_ConvBlock: 2-11                       [16, 256, 4, 8]           [16, 336, 4, 8]           210,320
│    └─_DownsampleBlock: 2-12                 [16, 336, 4, 8]           [16, 256, 2, 4]           774,400
│    └─_ConvBlock: 2-13                       [16, 256, 2, 4]           [16, 336, 2, 4]           210,320
│    └─_DownsampleBlock: 2-14                 [16, 336, 2, 4]           [16, 512, 1, 2]           1,548,800
</code></pre>
","0","Question"
"79537716","","<p>im getting this problem where i have installed pytorch following these steps (<a href=""https://medium.com/@harunijaz/a-step-by-step-guide-to-installing-cuda-with-pytorch-in-conda-on-windows-verifying-via-console-9ba4cd5ccbef"" rel=""nofollow noreferrer"">https://medium.com/@harunijaz/a-step-by-step-guide-to-installing-cuda-with-pytorch-in-conda-on-windows-verifying-via-console-9ba4cd5ccbef</a>) exactly, but i can't get the cuda env to recognise the torch install</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"" data-babel-preset-react=""false"" data-babel-preset-ts=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&gt;&gt;&gt; import torch
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'torch'
&gt;&gt;&gt;</code></pre>
</div>
</div>
</p>
<p><a href=""https://i.sstatic.net/7C325SeK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7C325SeK.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79537819","","<p>I notice on the website of pytorch (<a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a>) there is a command for cuda 12.6 but not for the latest version of cuda which is 12.8.</p>
<p>Is there a command specifically for cuda 12.8?</p>
<p>Will the command for cuda 12.6 work if I'm using cuda 12.8?</p>
","2","Question"
"79540216","","<p>I have a complex tensor: <code>tensor1 = tensor([0.0000+0.j, -106990.0794+0.j], device='cuda:1', dtype=torch.complex128)</code></p>
<p>The both elements have no imaginary part (0.j), however, when squaring the variable <code>tensor1**2</code> it yields an imaginary part in the second element</p>
<p><code>tensor([0.0000e+00+0.0000e+00j, 1.1447e+10-2.8037e-06j], device='cuda:1', dtype=torch.complex128)</code></p>
<p>This behavior is not exhibited when performing the square in numpy</p>
<p><code>tensor1.cpu().numpy()**2</code></p>
<p><code>Out  : array([0.00000000e+00+0.j, 1.14468771e+10-0.j])</code></p>
<p>Why are they different?</p>
","0","Question"
"79542733","","<p>In my PyTorch implementation of multi-head attention, i have those in <code>__init__()</code></p>
<pre><code>class MultiHeadAttentionLayer(nn.Module):
    def __init__(self,d_in,d_out,context_length,dropout,num_heads,use_bias=False):
        super().__init__()
        self.d_out=d_out
        self.num_heads=num_heads
        # In multi-head attention, the output dimension (d_out) is split across multiple attention heads.
        # Each head processes a portion of the total output dimensions independently before being concatenated back together.
        self.head_dim=d_out//num_heads
        self.query_weight = nn.Linear(d_in, d_out, bias=use_bias)
        self.key_weight = nn.Linear(d_in, d_out, bias=use_bias)
        self.value_weight = nn.Linear(d_in, d_out, bias=use_bias)
</code></pre>
<p>this is the forward method</p>
<pre><code>def forward(self,x):
    batch_size,sequence_length,d_in=x.shape
    keys=self.key_weight(x)
    queries=self.query_weight(x)
    values=self.value_weight(x)
    # RESHAPING
    # .view() is a PyTorch tensor method that reshapes a tensor without changing its underlying data. It returns a new tensor with the same data but in a different shape.
    keys=keys.view(batch_size,sequence_length,self.num_heads,self.head_dim)
    values=values.view(batch_size,sequence_length,self.num_heads,self.head_dim)
    queries=queries.view(batch_size,sequence_length,self.num_heads,self.head_dim)
</code></pre>
<p>I understand that <code>d_out</code> is split across multiple attention heads, but I'm not entirely sure why this reshaping is necessary. How does adding <code>num_heads</code> as a new dimension affect the computation of attention, and what would happen if we skipped this step and kept the shape as &quot;batch_size,sequence_length,d_in&quot;</p>
","1","Question"
"79545477","","<p>In following code</p>
<pre><code>import torch
from torch.nn.functional import linear
a=torch.ones(2,3).type(torch.float16)
b=torch.ones(2,3).type(torch.float16)
linear(a,b)
</code></pre>
<p>what is the computetype of linear, fp32 or fp16 or other?</p>
<p>Thanks</p>
<p>I try to look into the repo and the torch.nn.functional.linear, but it is too hard.</p>
","1","Question"
"79546578","","<p>I'm creating an AI model to generate density plots of crowds. When splitting the dataset into two, one for training and one for validation, I create the two data sets and try to load the datasets using <code>torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)</code>. After that, to test the data I iterate through and use the next function to get the next element of the dataset and then I get the TypeError.
The dataset I'm using his the ShanghaiTech Crowd Counting data set from Kaggle: <a href=""https://www.kaggle.com/datasets/tthien/shanghaitech"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/tthien/shanghaitech</a></p>
<p>Here is the full code:</p>
<pre><code>batch_size = 8 
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

train_root_dir = &quot;data/part_A/train_data/&quot;
init_training_set = DataLoader(train_root_dir, shuffle=True)

# split part of the training set as validation set
train_size = int(0.9 * len(init_training_set))
val_size = len(init_training_set) - train_size

train_indices = list(range(train_size))
val_indices = list(range(train_size, len(init_training_set)))
train_dataset = torch.utils.data.dataset.Subset(init_training_set, train_indices)
val_dataset = torch.utils.data.dataset.Subset(init_training_set, val_indices)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

test_root_dir = &quot;data/part_A/test_data/&quot;
test_set = DataLoader(test_root_dir, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)

dataiter = iter(train_loader)
ex_images, ex_dmaps, ex_n_people = next(dataiter)


# Show images and density map
plot_corresponding_pairs(ex_images, ex_dmaps)
</code></pre>
<p>The specific error is:</p>
<pre><code>Traceback (most recent call last):
 line 61, in &lt;module&gt;
    for ex_images, ex_dmaps, ex_n_people in train_loader
TypeError: 'DataLoader' object is not subscriptable
</code></pre>
","0","Question"
"79552821","","<p>I was reading pytorch docs about <code>Conv2DTranspose</code> on <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d</a>, and I was curious as to what happens if we increase the stride to be larger than 1. I know how transposed convolution works for stride = 1. Are zeros in the output inserted in every 2nd element when we set stride = 2, or something else happens. Also whats the advantage of upsampling the image this way, rather than using <code>upSampling</code>?</p>
<p>Thanks for help in advance.</p>
","0","Question"
"79554290","","<p>Why can't I use a vscode debugger to debug jax code, specifically pure functions. I understand that they provide their own framework for debugging but vscode debugger is quite comfortable. Is this because vscode debugger in the first attempt captures compile time?</p>
","2","Question"
"79556482","","<p>I have a search on my site that does both tradition full text search and searches using embeddings. So, for example, when you search 'red balloon' I want both the text and image results. The problem is that not all search terms make sense for object detection (like, say 'William' or even like an identifier like a driver license number) I know there are libraries that will tell me if a word is a noun but is there anything that tells me if a phrase is searchable. So like this:</p>
<ul>
<li>Red Apple YES</li>
<li>Big Idea No</li>
<li>Driver's License YES</li>
<li>Suspended License No</li>
</ul>
","0","Question"
"79559122","","<p>I'm trying to use the Qwen2.5-VL-7B model for my project, and I've downloaded a local file named model-00002-of-00005.safetensors. However, when I run my code, I encounter the following error:</p>
<p>OSError: Qwen/Qwen2.5-VL-7B-Instruct does not appear to have files named model-00002-of-00005.safetensors</p>
<p>Here's what I've done so far:</p>
<pre><code>I verified that the file model-00002-of-00005.safetensors is located in the expected directory.
I checked that the file permissions allow read access.
I ensured that I am using the correct path to load the model.
</code></pre>
<p>Despite these steps, the error persists.</p>
<p>Questions:</p>
<pre><code>Why is the model unable to recognize the local .safetensors file?
Is there something specific I might be missing regarding the model loading process?
</code></pre>
<p>Any insights or suggestions would be greatly appreciated!</p>
","3","Question"
"79560879","","<p>Basically I am trying to create a model that will detect angle on which a specific image is rotated. Also I have a dataset of 1500 documents resulting with images rotated on</p>
<pre><code>random.sample([0, 90, -90, 180], 2)
</code></pre>
<p>and each of this angles has variation of</p>
<pre><code>random.uniform(-10, 10)
</code></pre>
<p>resulting in ~4k rotated images.</p>
<p>So I've come up with current model to predict sin and cos of the desired angle:</p>
<pre><code>class CnnRotateRegression(nn.Module):
    def __init__(self):
        super(CnnRotateRegression, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3)
        self.conv5 = nn.Conv2d(512, 512, kernel_size=3)
        
        self.bn1 = nn.BatchNorm2d(64)
        self.bn2 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(256)
        self.bn4 = nn.BatchNorm2d(512)
        self.bn5 = nn.BatchNorm2d(512)
        
        self.activation = nn.ReLU()
        self.pool = nn.AvgPool2d(kernel_size=2)
        self.pool2 = nn.AdaptiveAvgPool2d((8,8))

        self.linear_l1 = nn.Linear(512*8*8, 512)
        self.linear_l2 = nn.Linear(512, 256)
        self.linear_l3 = nn.Linear(256, 2) # sin + cos

    def forward(self, x):
        x = self.activation(self.pool(self.bn1(self.conv1(x))))
        x = self.activation(self.pool(self.bn2(self.conv2(x))))
        x = self.activation(self.pool(self.bn3(self.conv3(x))))
        x = self.activation(self.pool(self.bn4(self.conv4(x))))
        x = self.activation(self.pool(self.bn5(self.conv5(x))))
        
        x = self.pool2(x)
        x = x.view(x.size(0), -1)
        
        x = self.activation(self.linear_l1(x))
        x = self.activation(self.linear_l2(x))
        x = self.linear_l3(x)

        x = F.normalize(x, p=2, dim=1)

        return x
</code></pre>
<p>training part:</p>
<pre><code>model = CnnRotateRegression()
model = model.to(device)

loss_function = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_of_epochs = 11


for epoch in range(num_of_epochs):
    model.train()
    running_loss = 0.0
    for images, labels in tqdm(train_Loader, desc=&quot;training loop&quot;):

        images, labels = images.to(device), labels.to(device).float()

        angles = angle_to_sin_cos(labels)
        norm_angles = F.normalize(angles, p=2, dim=1)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_function(outputs, norm_angles)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    train_loss = running_loss / len(train_Loader)
</code></pre>
<p>functions to convert sin and cos to angle and vice versa:</p>
<pre><code>def angle_to_sin_cos(angle):
    tensor_angle = angle.clone().detach()
    radian = tensor_angle * torch.pi / 180.0
    return torch.stack([torch.cos(radian), torch.sin(radian)], dim=1)

def sin_cos_to_angle(outputs):
    cos_val, sin_val = outputs[:, 0], outputs[:, 1]
    angle_rad = torch.atan2(sin_val, cos_val)
    angle_deg = angle_rad * (180 / torch.pi)
    return angle_deg
</code></pre>
<p>My model performs poorly in determining small angles in range +-10 degrees. What would you suggest to improve/enhance to achieve better &quot;small-degree&quot; prediction?</p>
","0","Question"
"79561884","","<p>I am training on CIFAR10 the following simple CNN</p>
<pre class=""lang-py prettyprint-override""><code>class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()

        self.fc1 = nn.Linear(64 * 8 * 8, 128, bias=False)
        self.fc2 = nn.Linear(128, 128, bias=False)
        self.fc3 = nn.Linear(128, 10, bias=False)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x
</code></pre>
<p>I would like to perform global structured pruning on the last part of the network, the MLP part (so <code>fc1</code>, <code>fc2</code>, <code>fc3</code>) with a percentage cutoff. Basically I would like to cut off <code>x</code> percent of the neurons (and all the relative connections) on the basis of the total connectivity. In PyTorch there is a function that performs a similar job: <code>ln_structured</code></p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn.utils.prune as prune
prune.ln_structured(model.fc1, name='weight', amount=fraction_of_neurons_to_prune)
prune.ln_structured(model.fc2, name='weight', amount=fraction_of_neurons_to_prune)
prune.ln_structured(model.fc3, name='weight', amount=fraction_of_neurons_to_prune)
</code></pre>
<p><strong>But the main problem is</strong> that this function will apply the fraction to prune layer by layer, and not globally; I want something to cutoff let's say 80 percent of the neurons in the MLP portion of the network, and not 80 percent layer by layer.</p>
<p>Is there a function that performs what I want? How could I write it myself? The absence of a function in PyTorch that performs this very common operation seems very strange to me, but I can't find anything.</p>
","1","Question"
"79570067","","<p>I found the PyTorch's matmul backward is slightly faster than forward.</p>
<p>I think it is intuitive, because for <code>A*B=C</code>, forward propagate this operator only takes one matrix multiplication.</p>
<p>Compared to two matrix multiplications, <code>dC/dA</code> and <code>dC/dB</code> in backward propagate, the forward process should be way more faster.</p>
<p>However, the experimental results are counter intuitive, it shows there is minor latency difference, no matter what the problem scale is.</p>
<p>Dose anybody have some ideas? Thanks!</p>
","1","Question"
"79571227","","<p>I have a deep learning mode which I am running in the jit transformed manner by:</p>
<pre><code>my_function_checked = checkify.checkify(model.apply)
    model_jitted = jax.jit(my_function_checked)
    err, pred = model_jitted({&quot;params&quot;: params}, batch, training=training, rng=rng)
    err.throw()
</code></pre>
<p>The code is compiling fine, but now I want to debug the intermediate values after every few steps, save the arrays, and then compare them with pytorch tensors. For this, I need to repeatedly save the arrays. The easiest way to do this is to use any IDE's inbuilt debugger and evaluate the save expression after every few steps. But jax.jit transformed code doesn't allow external debuggers. But, I can do this after disabling the jit. Should I be expecting any discrepancies between the two runs? Can I assume that the values in jit and non-jit runs will remain same?</p>
","1","Question"
"79573774","","<p>I'm implementing a differentially private recommendation system using PyTorch Lightning and Opacus, but I'm encountering a <code>RecursionError</code> during training. Here's my setup:</p>
<p><strong>Problem</strong></p>
<p>When I run my DPModel (which wraps a base recommendation model with Opacus's PrivacyEngine), I get:</p>
<pre><code>RecursionError: maximum recursion depth exceeded while calling a Python object
</code></pre>
<p><strong>Code Structure</strong></p>
<p>Base Model (<code>BaseModel.py</code>):</p>
<pre><code>class BaseModel(pl.LightningModule):
    # Basic recommendation system with user/item embeddings
    def forward(self, user_ids, item_ids):
        user_embed = self.user_embedding(user_ids)
        item_embed = self.item_embedding(item_ids)
        x = torch.cat([user_embed, item_embed], dim=1)
        return self.fc(x).squeeze()
</code></pre>
<p>DP Model (<code>DPModel.py</code>):</p>
<pre><code>class DPModel(BaseModel):
    def on_train_start(self):
        if self.enable_dp:
            self.dp_model, dp_optimizer, _ = self.privacy_engine.make_private(
                module=self,
                optimizer=optimizer,
                data_loader=train_loader,
                noise_multiplier=self.hparams.noise_multiplier,
                max_grad_norm=self.hparams.max_grad_norm
            )
    
    def forward(self, *args, **kwargs):
        if hasattr(self, 'dp_model'):
            return self.dp_model(*args, **kwargs)
        return super().forward(*args, **kwargs)
</code></pre>
<p>DataModule</p>
<pre><code>def train_dataloader(self):
    return DPDataLoader(  # Using Opacus's DPDataLoader
        self.train_dataset,
        sample_rate=self.batch_size/len(self.train_dataset),
        num_workers=self.num_workers
    )
</code></pre>
<p><strong>What I've Tried</strong></p>
<p>Initializing self.dp_model = None in <strong>init</strong>
Storing the original model reference before make_private
Different versions of Opacus (1.5.3) and PyTorch (2.6.0)
Error Details</p>
<p>How can I properly integrate Opacus with PyTorch Lightning without hitting this recursion issue? Specifically:</p>
<p>Is there a correct way to handle model wrapping with make_private?
Should I modify how Lightning accesses the model's parameters?</p>
","0","Question"
"79575684","","<p>I am using <code>PyTorch</code> for a face recognition coursework and I have to calculate the MAPE value.</p>
<p>My first attempt was with <code>torchmetrics.MeanAbsolutePercentageError</code> class, but the result doesn't make sense.</p>
<p>For this reason I wrote a function to calculate it and it seems to work fine.</p>
<p>Investigating a bit, it seems to me the problem is related to the presence of <code>0</code> in the truth value array, but I didn't find anything in the <code>torchmetrics</code> documentation.</p>
<p>Is there a way to avoid this problem in <code>torchmetrics</code>?</p>
<p>Is it possible that the <code>epsilon</code> value in the MAPE formula is not set? If this is the case, how can I give it a value?</p>
<p>I am happy to use the other function, but I am curious to understand the reason of those results with <code>torchmetrics</code>.</p>
<p>These are the 2 function to calculate the MAPE:</p>
<pre><code>def calculate_mape_torch(preds, targets):
    &quot;&quot;&quot;Calculate MAPE using PyTorch method.
    
    Args:
        preds: array with ground truth values
        targets: array with predictions from model

    Returns:
        MAPE
    &quot;&quot;&quot;
    if not isinstance(preds, torch.Tensor):
        preds = torch.tensor(preds)
    if not isinstance(targets, torch.Tensor):
        targets = torch.tensor(targets)

    mape = MeanAbsolutePercentageError()

    return mape(preds, targets) * 100


def calculate_mape(preds, targets, epsilon=1):
    &quot;&quot;&quot;Calculate the Mean Absolute Percentage Error.
    
    Args:
        preds: array with ground truth values
        targets: array with predictions from model
        epsilon: value to avoid divide by zero problem

    Returns:
        MAPE
    &quot;&quot;&quot;
    preds_flatten = preds.flatten(&quot;F&quot;)
    targets_flatten = targets.flatten(&quot;F&quot;)

    return np.sum(np.abs(targets_flatten - preds_flatten) / np.maximum(epsilon, targets_flatten)) / len(preds_flatten) * 100
</code></pre>
<p>With these values:</p>
<pre><code>y_true = np.array([[1, 0, 3], [4, 5, 6]])
y_pred = np.array([[3, 2, 2], [7, 3, 6]])
</code></pre>
<p>the 2 functions give the results:</p>
<pre><code>&gt;&gt;&gt; calculate_mape(y_pred, y_true)
91.38888888888889

&gt;&gt;&gt; calculate_mape_torch(y_pred, y_true)
tensor(28490084.)
</code></pre>
<p>With these values:</p>
<pre><code>y_true = np.array([[1, 2, 3], [4, 5, 6]])
y_pred = np.array([[3, 2, 2], [7, 3, 6]])
</code></pre>
<p>the 2 functions give the results:</p>
<pre><code>&gt;&gt;&gt; calculate_mape(y_pred, y_true)
58.05555555555556

&gt;&gt;&gt; calculate_mape_torch(y_pred, y_true)
tensor(58.0556)
</code></pre>
","0","Question"
"79578968","","<p>As I understand, nn.Linear from pyTorch works as matrix multiplication. Mathematically,
[W U][x h]^T = Wx + Uh,
but when using nn.Linear, the following two are not equivalent</p>
<pre><code>Wf = nn.Linear(x_size, o_size)
Uf = nn.Linear(h_size, o_size)
Wf(x) + Uf(h)
</code></pre>
<p>and</p>
<pre><code>W = nn.Linear(x_size + h_size, o_size)
W([x h])
</code></pre>
<p>where [x h] is concatenated tensor.
What actually happens inside this function?</p>
","0","Question"
"79582585","","<p>I'm facing an issue when training a model using PEFT and LoRA on a multi-GPU setup with PyTorch and Hugging Face Transformers. The error I get is:</p>
<pre class=""lang-sql prettyprint-override""><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!
</code></pre>
<p>Here are the details of my setup and code:</p>
<p><strong>Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>data = load_dataset(data_path, split=&quot;train&quot;).map(formatting_prompts_func)

model_name = &quot;yandex/YandexGPT-5-Lite-8B-pretrain&quot;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;
)

tokenizer = AutoTokenizer.from_pretrained(
    model_name, trust_remote_code=True,
    padding_side=&quot;left&quot;,
    add_eos_token=True, add_bos_token=True,
    use_fast=True
)
tokenizer.pad_token = tokenizer.eos_token

instruction_template = &quot;### PROMPT:&quot;
response_template = &quot;### OUTPUT:&quot;

collator = SafeCollator(
    instruction_template=instruction_template,
    response_template=response_template,
    tokenizer=tokenizer, mlm=False
)

peft_config = LoraConfig(...)

training_args = SFTConfig(...)

trainer = SFTTrainer(model,
                    peft_config=peft_config,
                    train_dataset=data,
                    data_collator=collator,
                    args=training_args
)
trainer.train()

</code></pre>
<p><strong>Dataset</strong>:</p>
<pre><code>Dataset({
    features: ['instruction', 'output', 'retrieved_context', 'text'],
    num_rows: 7317
})
</code></pre>
<p><strong>Details</strong></p>
<p>I'm using Kaggle's 2xT4 configuration. My model cant fit only one GPU's memory</p>
","0","Question"
"79583142","","<p>I have a tensor <code>p</code> of shape <code>(B, 3, N)</code> in PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code># 2 batches, 3 channels (x, y, z), 5 points
p = torch.rand(2, 3, 5, requires_grad=True)
&quot;&quot;&quot;
p: tensor([[[0.8365, 0.0505, 0.4208, 0.7465, 0.6843],
         [0.9922, 0.2684, 0.6898, 0.3983, 0.4227],
         [0.3188, 0.2471, 0.9552, 0.5181, 0.6877]],

        [[0.1079, 0.7694, 0.2194, 0.7801, 0.8043],
         [0.8554, 0.3505, 0.4622, 0.0339, 0.7909],
         [0.5806, 0.7593, 0.0193, 0.5191, 0.1589]]], requires_grad=True)
&quot;&quot;&quot;
</code></pre>
<p>And then another <code>z_shift</code> of shape <code>[B, 1]</code>:</p>
<pre class=""lang-py prettyprint-override""><code>z_shift = torch.tensor([[1.0], [10.0]], requires_grad=True)
&quot;&quot;&quot;
z_shift: tensor([[1.],
        [10.]], requires_grad=True)
&quot;&quot;&quot;
</code></pre>
<p>I want to apply the appropriate z-shift of all points in each batch, leaving x and y unchanged:</p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;
p: tensor([[[0.8365, 0.0505, 0.4208, 0.7465, 0.6843],
         [0.9922, 0.2684, 0.6898, 0.3983, 0.4227],
         [1.3188, 1.2471, 1.9552, 1.5181, 1.6877]],

        [[0.1079, 0.7694, 0.2194, 0.7801, 0.8043],
         [0.8554, 0.3505, 0.4622, 0.0339, 0.7909],
         [10.5806, 10.7593, 10.0193, 10.5191, 10.1589]]])
&quot;&quot;&quot;
</code></pre>
<p>I managed to do it like:</p>
<pre class=""lang-py prettyprint-override""><code>p[:, 2, :] += z_shift
</code></pre>
<p>for the case where <code>requires_grad=False</code>, but this fails inside the <code>forward</code> of my <code>nn.Module</code> (which I assume is equivalent to <code>requires_grad=True</code>) with:</p>
<pre><code>RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.
</code></pre>
","3","Question"
"79583654","","<p><strong>The problem</strong></p>
<p>The similarity scores are almost the same for texts that describe both a photo of a cat and a dog (the photo is of a cat).</p>
<pre><code>Cat similarity: tensor([[-3.5724]], grad_fn=&lt;MulBackward0&gt;)
Dog similarity: tensor([[-3.4155]], grad_fn=&lt;MulBackward0&gt;)
</code></pre>
<p><strong>The code for CLIP model</strong></p>
<p>The code is based on the checkpoint of <em>openai/clip-vit-base-patch32</em>. The encode_text function takes a raw input and turns it into embeddings later fed into the forward method.
I'm certain that the layers' names and sizes are correct, as the checkpoint fits the model without errors due to missing or unexpected layers.</p>
<pre class=""lang-py prettyprint-override""><code>class CLIP(nn.Module):
    def __init__(self, project_dim: int = 768, embed_dim: int = 512):
        super(CLIP, self).__init__()

        self.vision_model = ImageEncoder(project_dim = project_dim)
        self.text_model = TextEncoder(embed_dim = embed_dim)
        self.tokenizer = TorchTokenizer()
        
        self.logit_scale = nn.Parameter(torch.ones([]) * 0.7) 
        self.visual_projection = nn.Linear(project_dim, embed_dim, bias = False)
        self.text_projection = nn.Linear(embed_dim, embed_dim, bias = False)

        self.vision_model.eval()
        self.text_model.eval()

    def forward(self, image: torch.Tensor, text_embed: torch.Tensor) -&gt; torch.Tensor:

        &quot; Compute the relationship between image and text  &quot;

        # get fixed size to comply with the checkpoint position_embeddings nn.Embedding(50, embed_dim)
        image = Resize(size=(224, 224))(image)

        image_features = self.vision_model(image)

        # projections
        text_features = self.text_projection(text_embed)
        image_features = self.visual_projection(image_features)
        
        # normalization
        text_features = F.normalize(text_features, dim = -1)
        image_features = F.normalize(image_features, dim = -1)

        logits = self.logit_scale.exp() * (image_features @ text_features.t())

        return logits
    
    def encode_text(self, input_ids, attention_mask = None):
        &quot;&quot;&quot; Tokenize (if needed) and encode texts, returning embeddings and mask. Function for ConditionalPromptNorm &quot;&quot;&quot;

        # tokenize strings if raw text passed
        if attention_mask is None:
            input_ids, attention_mask = self.tokenizer.tokenize(input_ids)
        
        # ensure batch dim
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)

        with torch.no_grad():
            text_emb = self.text_model(input_ids.long(), attention_mask)

        return text_emb
</code></pre>
<p><strong>The code for the text encoder</strong></p>
<p>I have checked that getting the EOS token does work correctly. Also, the types of layers, like nn.Embedding and nn.Parameter are correct for each layer as it would conflict with the checkpoint if it weren't the same type.</p>
<pre class=""lang-py prettyprint-override""><code>class TextEncoder(nn.Module):
    def __init__(self, embed_dim: int = 512):
        super(TextEncoder, self).__init__()

        vocab_size = 49408

        self.embeddings = nn.Module()
        self.embeddings.token_embedding = nn.Embedding(vocab_size, embed_dim)
        # tokenizer's context_length must be set to 77 tokens
        self.embeddings.position_embedding = nn.Embedding(77, embed_dim) # 77 = context length

        self.encoder = Encoder(embed_size = embed_dim)

        self.final_layer_norm = nn.LayerNorm(embed_dim)

    def forward(self, text: torch.Tensor, attention_mask: torch.Tensor):

        x = self.embeddings.token_embedding(text.long())

        #                       seq_length
        positions = torch.arange(x.size(1))
        pos_embed = self.embeddings.position_embedding(positions)

        x += pos_embed.to(x.dtype).to(x.device)

        # obtain text embeddings
        x = x.permute(1, 0, 2)
        x = self.encoder(x, attention_mask)
        x = x.permute(1, 0, 2)

        # ensure batch dim
        if x.dim() == 2: x = x.unsqueeze(0)
        if attention_mask.dim() == 1: attention_mask = attention_mask.unsqueeze(0)

        # for each batch, get the last token (eos)
        x = x[torch.arange(x.size(0)), text.argmax(dim = -1)]

        return self.final_layer_norm(x)
</code></pre>
<p>The attention class is from <a href=""https://github.com/openai/CLIP/blob/main/clip/model.py#L58"" rel=""nofollow noreferrer"">https://github.com/openai/CLIP/blob/main/clip/model.py#L58</a> with a slight modification to allow self and pooled attention (x and x[:1]).</p>
<p><strong>The Encoder</strong></p>
<p>I have checked that the tokenizer code works correctly. The MLP is the same as in the CLIP original code. Two linear layers with a ratio of 4 and a GELU in the middle.</p>
<pre class=""lang-py prettyprint-override""><code>class EncoderLayer(nn.Module):
    def __init__(self, embed_size: int = 768, ratio: int = 4, num_heads: int = 8):
        super().__init__()

        self.layer_norm1 = nn.LayerNorm(embed_size)
        self.layer_norm2 = nn.LayerNorm(embed_size)
        self.mlp = MLP(embed_size = embed_size, ratio = ratio)

        self.self_attn = AttentionPool2d(num_heads = num_heads, embed_dim = embed_size)

    def forward(self, x: torch.Tensor, src_pad_key = None):

        x = self.layer_norm1(x)
        
        if src_pad_key is not None: attn_out = self.self_attn(x, src_pad_key = src_pad_key, use_self_attention = True)
        else: attn_out = self.self_attn(x)

        # normalize and apply residual connections
        x += attn_out
        x = self.layer_norm2(x)
        x += self.mlp(x)

        return x

class Encoder(nn.Module):
    def __init__(self, embed_size = 768):
        super().__init__()

        self.layers = nn.ModuleList([EncoderLayer(embed_size = embed_size) for _ in range(12)])

    def forward(self, x: torch.Tensor, attention_mask = None):

        if attention_mask is not None:
            src_key_mask = attention_mask == 0
            if src_key_mask.dim() == 1: src_key_mask = src_key_mask.unsqueeze(0)

            for layer in self.layers:
                x = layer(x, src_key_mask)
        
        else:
            for layer in self.layers:
                x = layer(x)

        return x
</code></pre>
","2","Question"
"79584215","","<p>I have a situation where I need to add one PyTorch tensor to parts of another tensor. An example is like this:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

x = torch.randn([10, 7, 128, 128])  # [batch, channel, height, width]

# In the actual program, batch_idx and channel_idx are generated dynamically
batch_idx = torch.tensor([1,3], dtype=torch.int64)
channel_idx = torch.tensor([2,3,5], dtype=torch.int64)

y = torch.randn([2, 3, 128, 128])  # [len(batch_idx), len(channel_idx), height, width]

x[batch_idx, channel_idx, :, :] += y
</code></pre>
<p>Running this code raises the following error:</p>
<pre><code>IndexError: shape mismatch: indexing tensors could not be broadcast together with shapes [2], [3]
</code></pre>
<p>How can I perform the desired operation without looping through each index of each dimension?</p>
","2","Question"
"79584485","","<p>I am trying to use a pytorch model present on this link:</p>
<p><a href=""https://drive.google.com/drive/folders/121kucsuGxoYQu03-Jmy6VCDcPzqlG4cG"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/121kucsuGxoYQu03-Jmy6VCDcPzqlG4cG</a></p>
<p>since it is used by this project I am trying to run:</p>
<p><a href=""https://github.com/amaljoseph/EndToEnd_Signature-Detection-Cleaning-Verification_System_using_YOLOv5-and-CycleGAN"" rel=""nofollow noreferrer"">https://github.com/amaljoseph/EndToEnd_Signature-Detection-Cleaning-Verification_System_using_YOLOv5-and-CycleGAN</a></p>
<p>When I do</p>
<pre class=""lang-py prettyprint-override""><code>torch.load(model, &quot;cpu&quot;)
</code></pre>
<p>I get</p>
<pre><code>raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. 
        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
        WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
</code></pre>
<p>I don't want to follow suggestion (1) since I don't fully trust the creator, and trying with solution (2) I would do something like:</p>
<pre class=""lang-py prettyprint-override""><code>from numpy.core.multiarray import _reconstruct
import torch
    
torch.serialization.add_safe_globals([_reconstruct])
torch.load(model, &quot;cpu&quot;)
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>with torch.serialization.safe_globals([_reconstruct]):
    torch.load(model, &quot;cpu&quot;)
</code></pre>
<p>But I get the exact same error. How can I load the model in a safe way?</p>
<p>Details:</p>
<ul>
<li>Python version: 3.12.3</li>
<li>Pytorch version: 2.6.0+cu124</li>
<li>Numpy version: 2.1.3</li>
<li>OS: Ubuntu 24.04.2 LTS x86_64</li>
</ul>
","2","Question"
"79589222","","<p>Thanks for giving this a read...</p>
<p>I am getting going with PyTorch. I’m building a tool that wraps HuggingFace models in a custom WrappedModel so I can trace their execution using torch.fx.symbolic_trace. The goal is to analyze the traced graph and detect certain ops like float32 usage.</p>
<p>To do this, I:</p>
<ul>
<li>Wrap the model in a subclass of torch.nn.Module.</li>
<li>Run a forward() pass with dummy input_ids.</li>
<li>Call symbolic_trace(wrapped_model) or fall back to torch.jit.trace().</li>
</ul>
<p><strong>What’s going wrong:</strong></p>
<p>I consistently see:</p>
<blockquote>
<p>Forward pass failed in WrappedModel — slice indices must be integers
or None or have an index method</p>
</blockquote>
<p>And ultimately:</p>
<blockquote>
<p>Rule run failed: ‘method’ object is not iterable</p>
</blockquote>
<p><strong>Likely problematic code:</strong></p>
<pre><code>class WrappedModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids):
        try:
            batch_size = input_ids.size(0)
            seq_len = input_ids.size(1)

            # This line fails during symbolic tracing
            attention_mask = torch.ones((batch_size, seq_len), dtype=torch.int64)

            output = self.model(input_ids=input_ids, attention_mask=attention_mask)
        except Exception as e:
            logging.warning(f&quot;TRACE ERROR inside wrapped forward: {e}&quot;)
            return torch.zeros(1, 1)

        if hasattr(output, &quot;last_hidden_state&quot;):
            return output.last_hidden_state
        elif hasattr(output, &quot;logits&quot;):
            return output.logits
        return output
</code></pre>
<p><strong>What I have already tried:</strong></p>
<ul>
<li>Using input_ids.size(0) instead of input_ids.shape[0]</li>
<li>Making sure the dummy input has fixed dimensions: torch.randint(0, 1000, (1, 10))</li>
<li>Hardcoding the mask shape (e.g., torch.ones((1, 10), dtype=torch.int64))</li>
<li>Falling back to torch.jit.trace — same error during forward Switching between BertModel and BertForSequenceClassification</li>
</ul>
<p><strong>What (I think) I am asking for:</strong></p>
<p>How do I make torch.ones(...) work inside a traced wrapper model during symbolic_trace()?</p>
<p>Thank you in advance for any guidance.</p>
","1","Question"
"79589904","","<p>I am trying to train a model for spell check errors and corrections using <strong>T5-small</strong> model in <strong>colab notebook using T4 GPU</strong>. Initially I started with <strong>10k reviews</strong> but gradually reduced it to <strong>2k</strong> since I was receiving CUDA out of memory error. Now with 2k reviews I'm able to train the model and store it in my drive but then when evaluating the model it is giving the same memory error again. I have tried restarting the session and evaluate again but still no fix found.</p>
<p>Error I'm receiving is :</p>
<blockquote>
<p>CUDA out of memory. Tried to allocate 6.99 GiB. GPU 0 has a total
capacity of 14.74 GiB of which 6.95 GiB is free. Process 79643 has
7.79 GiB memory in use. Of the allocated memory 7.59 GiB is allocated by PyTorch, and 76.39 MiB is reserved by PyTorch but unallocated. If
reserved but unallocated memory is large try setting
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid
fragmentation.  See documentation for Memory Management
(<a href=""https://pytorch.org/docs/stable/notes/cuda.html#environment-variables"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/notes/cuda.html#environment-variables</a>)</p>
</blockquote>
<p>I have tried increasing gradient_accumulation_steps but still the issue is not resolved
below is my code <strong>(all functions are defined in above so there are no errors over there)</strong></p>
<pre class=""lang-none prettyprint-override""><code>os.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;expandable_segments:True&quot;
from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import load_dataset

# Load dataset
dataset = load_dataset(&quot;csv&quot;, data_files=&quot;elaborate_cosmetic_reviews.csv&quot;)

# load the pre-trained Tokenizer and Model
fine_model_name = &quot;./spellcheck_model&quot;
tokenizer = T5Tokenizer.from_pretrained(fine_model_name, local_files_only=True)
model = T5ForConditionalGeneration.from_pretrained(fine_model_name, local_files_only=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir=&quot;./spellcheck_model_base&quot;,
    per_device_train_batch_size=2,
    report_to=&quot;none&quot;,
    fp16=True, 
    gradient_accumulation_steps = 256,
)

# Create data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    eval_dataset=tokenized_train, # Use tokenized_eval for test set
)
import torch
torch.cuda.empty_cache()

# Evaluate on the training set
print(&quot;Training set Evaluation: &quot;)
train_metrics = trainer.evaluate(eval_dataset=tokenized_train)
print(train_metrics)```
</code></pre>
","0","Question"
"79591629","","<p>I have a simple application of <code>torch.DataLoader</code> that gets a nice performance boost. It's created by the <code>tensor_loader</code> in the following example.</p>
<pre><code>from torch.utils.data import DataLoader, TensorDataset, BatchSampler, RandomSampler
import numpy as np
import pandas as pd

def tensor_loader(dataset: TensorDataset, batch_size: int):
    return DataLoader(
        dataset=dataset,
        sampler=BatchSampler(
            sampler=RandomSampler(dataset),  # randomly sample indexes, same as shuffle=True
            batch_size=batch_size,
            drop_last=True
        )
    )

dataset = TensorDataset(torch.tensor(np.random.random(1_000_000).reshape(-1, 10), dtype=torch.float32))

start = pd.Timestamp.now()
for i in tensor_loader(dataset, 4096):
    i[0]
end = pd.Timestamp.now()
print(end - start)
assert i[0].shape == torch.Size([1, 4096, 10])

start = pd.Timestamp.now()
simple_loader =  DataLoader(dataset=dataset, batch_size=4096)
for i in simple_loader:
    pass
end = pd.Timestamp.now()
print(end - start)
assert next(iter(simple_loader))[0].shape == torch.Size([4096, 10])
</code></pre>
<p>However, the difference in the shapes is a little annoying: the <code>data_loader</code> adds an exterior dimension that I don't want. It means the the two loaders can't be substituted for one another, which would mean a lot of niggling changes to existing code to substitute the <code>tensor_loader</code> for the existing one.</p>
<p>Obviously, I can subclass the <code>DataLoader</code> to drop the outer dimension, but this feels more complicated than it should be. Is there are way to create something like the above <code>data_loader</code>'s return value that will produce the shape of the <code>simpler_loader</code> when iterating?</p>
","0","Question"
"79597119","","<p>I'm trying to run Stable Diffusion using HuggingFace's <code>diffusers</code> library, but I keep getting CUDA out of memory errors on my RTX 3060 (12GB VRAM). I'm using the standard <code>StableDiffusionPipeline</code> from the <code>&quot;CompVis/stable-diffusion-v1-4&quot;</code> checkpoint.</p>
<p>Here's my code:</p>
<pre><code>from diffusers import StableDiffusionPipeline
import torch

# Load the pre-trained Stable Diffusion model
pipe = StableDiffusionPipeline.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;)
pipe = pipe.to(&quot;cuda&quot;)

# Define the prompt
prompt = &quot;a futuristic cityscape, high resolution, ultra detailed&quot;

# Generate the image
image = pipe(prompt).images[0]

# Show the image
image.show()
</code></pre>
<p>I tried running the code as shown above, expecting it to generate a high-quality image from the text prompt without memory issues. I also tried adding <code>torch_dtype=torch.float16</code> when loading the pipeline to reduce memory usage, like this:</p>
<pre><code>pipe = StableDiffusionPipeline.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, torch_dtype=torch.float16)
pipe = pipe.to(&quot;cuda&quot;)
</code></pre>
<p>However, I still encounter CUDA out of memory errors, especially with larger prompts or higher resolutions. I expected the float16 setting to help, but it didn't fully solve the issue.</p>
<p>I also expected to control the resolution by adjusting parameters, but I'm not sure how to properly do that with this pipeline.</p>
","1","Question"
"79599360","","<p>In <code>pytorch</code> I can create a custom module as follows (this code example is taken from <a href=""https://discuss.pytorch.org/t/practical-usage-of-nn-variable-and-nn-parameter/148644/2"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>from torch import nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.param = nn.Parameter(torch.randn(1, 1))
        
    def forward(self, x):
        x = x * self.param
        return x

model = MyModel()
print(dict(model.named_parameters()))
# {'param': Parameter containing:
# tensor([[0.6077]], requires_grad=True)}

out = model(torch.randn(1, 1))
loss = out.mean()
loss.backward()

print(model.param.grad)
# tensor([[-1.3033]])
</code></pre>
<p>I want to be able to do the same using <code>julia</code>'s <code>flux</code>. Specifically I am interested in knowing what the equivalent of <code>nn.Parameter(torch.randn(1, 1))</code> is in <code>flux</code>.</p>
<p>P.S. I am tagging both <code>python</code> and <code>julia</code> as I believe this would increase the chance of this post reaching to people who have knowledge in both languages.</p>
","1","Question"
"79600870","","<p>I have jupyter notebook mess that I am attempting to refactor into proper unit tests.  I would like to copy-paste some output from actual data into my test cases.  However, my data are long gnarly fractional numbers.  For conciseness, I would prefer to have less verbose values.  I cannot, however, simply round my values, as my values span many orders of magnitude, and it is not the case that only certain orders of magnitude are important.  What would be great is to round only <a href=""https://en.wikipedia.org/wiki/Scientific_notation"" rel=""nofollow noreferrer"">the mantissa</a>.  For example:</p>
<pre><code>3.5876 e -4 --&gt; 3.6 e -4
2.1234 e  5 --&gt; 2.1 e  5
</code></pre>
<p>What is the easiest way to round my values in this way in pytorch?</p>
","0","Question"
"79603672","","<p>I'm trying to use ChatTTS which uses Torch. I installed it via the Pycharm packages repository (PIP) with no errors.</p>
<p>But when I run the main webpages basic example code.</p>
<pre><code>import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = [&quot;PUT YOUR 1st TEXT HERE&quot;, &quot;PUT YOUR 2nd TEXT HERE&quot;]

wavs = chat.infer(texts)

for i in range(len(wavs)):
    &quot;&quot;&quot;
    In some versions of torchaudio, the first line works but in other versions, so does the second line.
    &quot;&quot;&quot;
    try:
        torchaudio.save(f&quot;basic_output{i}.wav&quot;, torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f&quot;basic_output{i}.wav&quot;, torch.from_numpy(wavs[i]), 24000)
</code></pre>
<p>It returns the following error,</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\GOOD\Coding\.Coding_Projects\Composite_media\Text to Speech\Audiobook_Maker\test3.py&quot;, line 8, in &lt;module&gt;
    import ChatTTS
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\ChatTTS\__init__.py&quot;, line 1, in &lt;module&gt;
    from .core import Chat
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\ChatTTS\core.py&quot;, line 17, in &lt;module&gt;
    from .model import DVAE, Embed, GPT, gen_logits, Tokenizer, Speaker
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\ChatTTS\model\__init__.py&quot;, line 6, in &lt;module&gt;
    from .tokenizer import Tokenizer
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\ChatTTS\model\tokenizer.py&quot;, line 16, in &lt;module&gt;
    class Tokenizer:
    ...&lt;121 lines&gt;...
            )
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python313\Lib\site-packages\ChatTTS\model\tokenizer.py&quot;, line 19, in Tokenizer
    tokenizer_path: torch.serialization.FILE_LIKE,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'torch.serialization' has no attribute 'FILE_LIKE'
</code></pre>
<p>I googled the error message. But there was no matching results.</p>
","0","Question"
"79605098","","<p>I am trying to implement cross-validation with PyTorch;</p>
<p>I am used to Sklearn's cross_validation function which is much simpler to use (most likely because I did not fully understand what was happening under the hood); I was hoping someone could advise if first of all this looks correct and, if it does, if there is a better way to write this.</p>
<p>I have looked at other posts, and got inspiration from there but I could not find exactly what I am trying to create, which is:</p>
<ol>
<li>Loop through the dataset for n_splits (in my case 5).</li>
<li>On each loop, train the model on the train split.</li>
<li>Evaluate the model on the test split.</li>
<li>Calculate the RMSE of the model evaluation on the test split.</li>
<li>At the end of the 5 folds, return the average RMSE of the 5 folds.</li>
</ol>
<p>And as I was creating this, I have started wondering: how should one decide the number of epochs for cross-validation? I always assumed that this would just be 1 but from what I saw in other posts this number can actually vary.</p>
<p>Here is my code:</p>
<pre><code>X = main_df.drop(columns=['target'])
y = main_df['target']

X_train_t = torch.tensor(X.values, dtype=torch.float32)
y_train_t = torch.tensor(y.values, dtype=torch.float32)

cv_dataset = TensorDataset(X_train_t, y_train_t)
</code></pre>
<pre><code>class NetModel(nn.Module):
    def __init__(self, in_count):
        super(NetModel, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(in_count, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
            )

    def forward(self, x):
        out = self.layers(x)
        return out
</code></pre>
<pre><code>num_epochs = 1
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
tot_rmse = 0.0

for fold, (train_ids, test_ids) in enumerate(kfold.split(cv_dataset)):

    print(f'FOLD {fold+1}')

    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)
    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)

    train_loader = DataLoader(cv_dataset, batch_size=5, sampler=train_subsampler)
    test_loader = DataLoader(cv_dataset, batch_size=5, sampler=test_subsampler)

    # initialize model
    model = NetModel(X_train_t.shape[1])
    criterion = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)

    for epoch in range(num_epochs):
        current_loss = 0.0

        model.train()
        for i, data in enumerate(train_loader, 0):

            inputs, targets = data
            targets = targets.reshape((targets.shape[0], 1))

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

        fold_rmse = 0.0
        model.eval()
        with torch.no_grad():
            for i, data in enumerate(test_loader, 0):

                inputs, targets = data
                targets = targets.reshape((targets.shape[0], 1))

                outputs = model(inputs)

                rmse = root_mean_squared_error(targets, outputs)

                # accumulate rmse of the fold
                fold_rmse += rmse

    print(f&quot;Sum of fold {fold+1} RMSE: {fold_rmse:.3f}  |  Count batches: {i+1}&quot;)

    # calculate average of the fold rmse (accumulated rmse / number of batches)
    tot_rmse += fold_rmse/(i+1)
    print(&quot;--------------------------------------------------&quot;)
print(f&quot;Avg. RMSE: {tot_rmse/5}&quot;)
</code></pre>
<p>This is the output of the above:</p>
<pre><code>FOLD 1
Sum of fold 1 RMSE: 2124.690  |  Count batches: 89
--------------------------------------------------
FOLD 2
Sum of fold 2 RMSE: 1817.517  |  Count batches: 89
--------------------------------------------------
FOLD 3
Sum of fold 3 RMSE: 2160.074  |  Count batches: 89
--------------------------------------------------
FOLD 4
Sum of fold 4 RMSE: 1613.786  |  Count batches: 89
--------------------------------------------------
FOLD 5
Sum of fold 5 RMSE: 1713.522  |  Count batches: 89
--------------------------------------------------
Avg. RMSE: 21.190088516406796
</code></pre>
<p>Thank you!</p>
","1","Question"
"79606201","","<p>I am trying to implement simple gradient descent to find the root of a quadratic equation using PyTorch. I'm doing this to get a better sense of how the autograd function works but it's not going very well. Let's say that I want to find the roots of</p>
<p>y = 3x^2 + 4x + 9</p>
<p>as a random example. Below was my first attempt to run one step of gradient descent and re-calculate the gradient:</p>
<pre><code>import torch

# Step size
alpha = 0.1

# Random starting point
x = torch.tensor([42.0], requires_grad=True)

# Function y = 3 * x^2 + 4 * x + 9
# Find the minimum of this with gradient descent
y = 3 * x ** 2 + 4 * x + 9
y.backward()
print(x.grad)

with torch.no_grad():
    x -= alpha * x.grad

y.backward()
print(x.grad)
</code></pre>
<p>This didn't like calling <code>.backward()</code> multiple times, so I updated it to this:</p>
<pre><code>import torch

# Step size
alpha = 0.1

# Random starting point
x = torch.tensor([42.0], requires_grad=True)

# Function y = 3 * x^2 + 4 * x + 9
# Find the minimum of this with gradient descent
y = 3 * x ** 2 + 4 * x + 9
y.backward(retain_graph=True) # &lt;--- Change here
print(x.grad)

with torch.no_grad():
    x -= alpha * x.grad

y.backward(retain_graph=True) # &lt;--- And here
print(x.grad)
</code></pre>
<p>and I get the error:</p>
<pre><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
</code></pre>
<p>I get the sense that I am fundamentally misunderstanding something about PyTorch here. How would I do this correctly?</p>
","1","Question"
"79607611","","<p>I’m currently using PyTorch’s <code>torch.autograd.functional.jacobian</code> to compute per-sample, <em>elementwise</em> gradients of a scalar-valued model output w.r.t. its inputs. I need to keep <code>create_graph=True</code> because I want the resulting Jacobian entries to themselves require gradients (for further calculations).</p>
<p>Here’s a minimal example of what I’m doing:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.autograd.functional import jacobian

def method_jac_strict(inputs, forward_fn):
    # inputs: (N, F)
    # forward_fn: (N, F) -&gt; (N, 1)
    # output: (N, F).

    # compute full Jacobian: 
    d = jacobian(forward_fn, inputs, create_graph=True, strict=True)  # (N, 1, N, F)
    d = d.squeeze()  # (N, N, F)

    # extract only the diagonal block (each output wrt its own input sample): (N, F)
    d = torch.einsum('iif-&gt;if', d)
    return d

</code></pre>
<p><strong>A clarification - Batch-sample dependencies</strong></p>
<p>My model may include layers like BatchNorm, so samples in the batch aren’t truly independent. However, I only care about the “elementwise” gradients—i.e. treating each scalar output as if it only depended on its own input sample, and ignoring cross-sample terms.</p>
<h2>Question</h2>
<p>Is there a more efficient/idiomatic way in PyTorch to compute this <em>elementwise</em> gradient preserving create_graph, without materializing the full (N, N, F) tensor and extracting its diagonal?</p>
<p>Any pointers to built-in functions or custom tricks (e.g. clever use of torch.einsum, custom autograd.Function, batching hacks, etc.) would be much appreciated!</p>
","2","Question"
"79609199","","<p>I need numerically stable non-central chi2 distribution in torch.</p>
","0","Question"
"79609919","","<p>Imagine a simple problem: make a function that gets a month index as input (zero-based: 0=Jan, 1=Feb, etc) and outputs number of days in this month (leap year ignored).</p>
<p>Of course, using NN for that task is an overkill, but I wondered, can NN actually be trained for that. Education purposes only.</p>
<p>In fact, it is possible to hand-tailor the accurate solution. I.e.</p>
<pre class=""lang-none prettyprint-override""><code>model = Sequential(
    Linear(1, 10),
    ReLU(),
    Linear(10, 5), 
    ReLU(),
    Linear(5, 1),    
)

state_dict = {
    '0.weight': [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]],
    '0.bias':   [ 0, -1, -2, -3, -4, -5, -7, -8, -9, -10],
    '2.weight': [
        [1, -2,  0,  0,  0,  0,  0,  0,  0,  0],
        [0,  0,  1, -2,  0,  0,  0,  0,  0,  0],
        [0,  0,  0,  0,  1, -2,  0,  0,  0,  0],
        [0,  0,  0,  0,  0,  0,  1, -2,  0,  0],
        [0,  0,  0,  0,  0,  0,  0,  0,  1, -2],
    ],
    '2.bias':   [0, 0, 0, 0, 0],
    '4.weight': [[-3, -1, -1, -1, -1]],
    '4.bias' :  [31]
}
model.load_state_dict({k:torch.tensor(v, dtype=torch.float32) for k,v in state_dict.items()})

inputs = torch.tensor([[0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11]], dtype=torch.float32)
with torch.no_grad():
    pred = model(inputs)
print(pred)
</code></pre>
<p>Output:</p>
<pre><code>tensor([[31.],[28.],[31.],[30.],[31.],[30.],[31.],[31.],[30.],[31.],[30.],[31.]])
</code></pre>
<p>Probably more compact and elegant solution is possible, but the only thing I care about is that optimal solution actually exists.</p>
<p>Though it turns out that it's totally impossible to train NN. Adding more weights and layers, normalizing input and output and adjusting loss function doesn't help at all: it stucks on a loss around 0.25, and output is something like &quot;every month has 30.5 days&quot;.</p>
<p>Is there any way to make training process smarter?</p>
","0","Question"
"79618775","","<p>I am using the Zinc graph dataset via torch geometric which I access as</p>
<pre><code>zinc_dataset = ZINC(root='my_path', split='train')
</code></pre>
<p>Each data element is a graph <code>zinc_dataset[0]</code> looks like</p>
<pre><code>Data(x=[33, 1], edge_index=[2, 72], edge_attr=[72], y=[1])
</code></pre>
<p>I have computed a tensor valued feature for each graph in the dataset. I have stored these tensors in a list with the ith element of the list being the feature for the ith graph in zinc_dataset.</p>
<p>I would like to add these new features to the data object. So ideally I want the result to be</p>
<pre><code>Data(x=[33, 1], edge_index=[2, 72], edge_attr=[72], y=[1], new_feature=[33,12])
</code></pre>
<p>I have looked at the solution provided by <a href=""https://stackoverflow.com/questions/77369606/how-to-add-a-new-attribute-to-a-torch-geometric-data-data-object-element"">How to add a new attribute to a torch_geometric.data Data object element?</a> but that hasn't worked for me.</p>
<p>Could someone please help me take my list of new features and include them in the data object?</p>
<p>Thanks</p>
","2","Question"
"79619981","","<p>I trained a YOLOv8 model for hand line detection. When I test the model in Python, it correctly detects all four hand lines, but when I convert the model to .tflite format and test it in Android Studio (Kotlin), it only predicts three lines.</p>
<p>Problem:
Training Process:
The model was trained using the Ultralytics YOLOv8 for hand line detection, and it works perfectly when tested in Python with the .pt model.</p>
<p>Conversion to .tflite:
After converting the model to .tflite, the shape of the output tensor is [1, 8, 8400].</p>
<p>This is the breakdown of the output tensor:</p>
<p>4 values for bounding box coordinates (x1, y1, x2, y2)</p>
<p>1 value for the objectness score (confidence that an object exists in the bounding box)</p>
<p>3 values for class probabilities (for detecting different types of hand lines, if there are 3 classes)</p>
<p>Issue:
The .tflite model, when used in Android Studio, only detects 3 lines of the hand instead of the expected 4 lines. I suspect the issue might be related to the shape of the .tflite model output ([1, 8, 8400]). Since there are 3 class probabilities in the output, it might not be properly detecting all lines.</p>
<p>What I’ve Tried:
I trained the model with the following script:</p>
<pre><code>from ultralytics import YOLO

# Load the YOLOv8 keypoint detection model
model_path = &quot;&quot;  # Update with the correct path
model = YOLO(model_path)  # Load the custom model

# Train the model on GPU
model.train(
    data=&quot;&quot;,  # Correct dataset path
    epochs=10,          # Number of training epochs
batch=8,            # Reduce batch size if low memory
imgsz=640,          # Image size
device=&quot;cuda&quot;       # Use GPU for training
)

results = model.val(device=&quot;cuda&quot;)

# Export the model to ONNX
model.export(format=&quot;onnx&quot;)
</code></pre>
<p>I tested the .pt model like this:</p>
<pre><code>from ultralytics import YOLO

model = YOLO(&quot;model_path/model_name&quot;)
results = model.predict(source=&quot;your_image_path&quot;, save=True, imgsz=640)
results[0].show()
# This shows the image with detections
</code></pre>
<p>My Question:
Why is the .tflite model only detecting 3 lines of the hand when the .pt model correctly detects all 4 lines? Could it be an issue with the output shape ([1, 8, 8400]), and if so, how can I resolve this issue to get all lines detected in Android?</p>
","0","Question"
"79621735","","<p>I have a PyTorch model and a simple inference script that I want to run a few times per day on GPU (specifically H100) in Azure. Ideally, this should be triggered by an event, like a new file in Blob Storage or data becoming available through an API, and not by a fixed timer.</p>
<p>I initially set up a VM that starts at certain hours and shuts down after a fixed time, but I’d prefer a more efficient solution where the compute resource shuts down automatically after inference completes.</p>
<p>I’ve read that Azure Machine Learning is better suited for this kind of workflow, but I’m new to it. What’s the best way to set this up using Azure ML, preferably without relying on notebooks? I’d like to run the Python script using the Azure SDK or CLI, access cloud data, and store outputs, all while minimizing idle GPU time.</p>
<p>Any guidance on which AML components to use and how to structure this would be greatly appreciated.</p>
","0","Question"
"79623038","","<p>I am running into the weirdest error.</p>
<p>Using PyCharm's Jupyter Notebook IDE (2025.1) I get the following error when I run <code>import torch</code></p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[2], line 1
----&gt; 1 import torch

File ~/.local/lib/python3.10/site-packages/torch/__init__.py:237
    235     if USE_GLOBAL_DEPS:
    236         _load_global_deps()
--&gt; 237     from torch._C import *  # noqa: F403
    239 # Appease the type checker; ordinarily this binding is inserted by the
    240 # torch._C module initialization code in C
    241 if TYPE_CHECKING:

ImportError: libnccl.so.2: cannot open shared object file: No such file or directory
</code></pre>
<p>However, when I run ipython in the terminal using the same virtual environment I get no issues:</p>
<pre><code>Python 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.11.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import torch

In [2]: torch.__version__
Out[2]: '2.2.2+cu121'
</code></pre>
<p>Any ideas of what may be causing that error?</p>
<p><strong>EDIT 1: Search Paths</strong></p>
<p>The search paths in the terminal and in PyCharm are the same:
(using <code>set(os.getenv(&quot;LD_LIBRARY_PATH&quot;).split(&quot;:&quot;))</code> to make them clearer)</p>
<p>PyCharm: <code>{'/usr/local/cuda-12.2/lib64', '/usr/local/cuda-12.2/extras/CUPTI/lib64'}</code>
Terminal: <code>{'/usr/local/cuda-12.2/lib64', '/usr/local/cuda-12.2/extras/CUPTI/lib64'}</code></p>
<p>Thus, that cannot be the difference between the two.</p>
","0","Question"
"79623585","","<p>I'm trying to run YOLOv8 inference using Ultralytics on CPU, but I get a <code>RuntimeError: could not create a primitive</code>. I'm using Windows and working in a virtual environment with PyTorch. I'm new to YOLO and would appreciate help understanding this.</p>
<h3>🔁 Code I'm using:</h3>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO

model = YOLO('yolov8n')
results = model.predict('input_videos/08fd33_4.mp4', save=True, device='cpu', stream=True)

for r in results:
    print(r)
    print('======================================')
    for box in r.boxes:
        print(box)
</code></pre>
<p>Error:</p>
<pre><code>RuntimeError: could not create a primitive  
  File &quot;.../torch/nn/modules/conv.py&quot;, line 549, in _conv_forward  
    return F.conv2d(...)
</code></pre>
<p>My environment:</p>
<pre><code>OS: Windows 11
Python: 3.12.9
Torch: 2.7.0
Ultralytics: 8.3.133
Device: CPU (no GPU)
</code></pre>
<p>My questions:</p>
<ul>
<li>How can I fix this error when running YOLOv8 on CPU?</li>
<li>Is it possible the model needs GPU even for basic prediction?</li>
<li>Are there config settings to make it work safely on CPU under Windows?</li>
</ul>
","0","Question"
"79628713","","<p>I got an error in jupyternotebook said that
<code>RuntimeError: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.</code></p>
<p>This is the result I got in jupyter notebook</p>
<pre class=""lang-py prettyprint-override""><code>torch.cuda.is_available() # True
torch.cuda.device_count() # 0
sys.executable # '/home/quiet98k/anaconda3/envs/lol/bin/python'
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] # -1
</code></pre>
<p>This is what I got in python terminal</p>
<pre class=""lang-py prettyprint-override""><code>Python 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import sys
&gt;&gt;&gt; torch.cuda.is_available()
True
&gt;&gt;&gt; torch.cuda.device_count()
1
&gt;&gt;&gt; torch.cuda.get_device_name(0)
'NVIDIA GeForce RTX 3070'
&gt;&gt;&gt; sys.executable
'/home/quiet98k/anaconda3/envs/lol/bin/python'
</code></pre>
<p>I'm sure that cuda is available in my wsl</p>
<pre><code>└─[$] &lt;git:(main*)&gt; nvidia-smi                             
Mon May 19 05:06:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 572.83         CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3070        On  |   00000000:01:00.0  On |                  N/A |
| 53%   33C    P5             31W /  280W |    2371MiB /   8192MiB |     34%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      5838      C   /python3.10                                 N/A      |
+-----------------------------------------------------------------------------------------+
</code></pre>
<p>I tried to reinstall pytorch and even the entire python environment, but the problem remains. What is wrong here?</p>
","2","Question"
"79628802","","<p>1.Follow the example in this link <a href=""https://huggingface.co/google/madlad400-3b-mt"" rel=""nofollow noreferrer"">https://huggingface.co/google/madlad400-3b-mt</a></p>
<p>2.Using AUTO does not seem to work on XPU, but it can run normally on CPU</p>
<p>3.What am I missing? How can I get it to work properly on the XPU?</p>
<pre><code> import torch
    from transformers import T5ForConditionalGeneration, T5Tokenizer
    
    print(torch.xpu.memory_allocated())
    print(torch.xpu.device_count())
    print(torch.xpu.get_device_name(0))
    
    
    model_name = 'google/madlad400-3b-mt'
    model = T5ForConditionalGeneration.from_pretrained(model_name,device_map=&quot;xpu&quot;, torch_dtype=torch.float16)
     
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    
    text = &quot;&lt;2pt&gt; I love pizza!&quot;
    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids.to(device_name)
    outputs = model.generate(input_ids=input_ids)
    
    tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Eu adoro pizza!
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    print(outputs[0])
</code></pre>
<p>Errors</p>
<p>0
1
Intel(R) Arc(TM) A770 Graphics
Traceback (most recent call last):
File &quot;C:\Users\zhang\source\repos\intel_pytorch2.6.10\translater.py&quot;, line 10, in 
model = T5ForConditionalGeneration.from_pretrained(model_name,device_map=&quot;xpu&quot;, torch_dtype=torch.float16)
File &quot;C:\Users\zhang\anaconda3\envs\Pytorch-ipx\Lib\site-packages\transformers\modeling_utils.py&quot;, line 279, in _wrapper
return func(*args, **kwargs)
File &quot;C:\Users\zhang\anaconda3\envs\Pytorch-ipx\Lib\site-packages\transformers\modeling_utils.py&quot;, line 4399, in from_pretrained
) = cls._load_pretrained_model(
~~~~~~~~~~~~~~~~~~~~~~~~~~^
model,
^^^^^^
...&lt;13 lines&gt;...
weights_only=weights_only,
^^^^^^^^^^^^^^^^^^^^^^^^^^
)
^
File &quot;C:\Users\zhang\anaconda3\envs\Pytorch-ipx\Lib\site-packages\transformers\modeling_utils.py&quot;, line 4793, in _load_pretrained_model
caching_allocator_warmup(model_to_load, expanded_device_map, factor=2 if hf_quantizer is None else 4)
~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;C:\Users\zhang\anaconda3\envs\Pytorch-ipx\Lib\site-packages\transformers\modeling_utils.py&quot;, line 5803, in caching_allocator_warmup
_ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
torch.OutOfMemoryError: XPU out of memory. Tried to allocate 7.46 GiB. GPU 0 has a total capacity of 15.56 GiB. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. Please use <code>empty_cache</code> to release all unoccupied cached memory.</p>
","0","Question"
"79629559","","<p>I have written a conv. neural network from scratch before, but I've decided to use Pytorch for its speed. However, I could not find documentation as to how to format for the conv2d layer. In general, there seems to be a lot of overheads and wrappers which prevents me from viewing what exactly is happening and writing my code accordingly.</p>
<p>I have trained a model on the MNIST dataset, and imported the model in order to run it (as per the tutorial):</p>
<pre><code>class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 8, 3, stride = 1, padding = 1)
        self.pool = nn.MaxPool2d(2, stride = 2)
        self.conv2 = nn.Conv2d(8, 8, 3, stride = 1, padding = 1)
        self.linear1 = nn.Linear(7 * 7 * 8, 128)
        self.linear2 = nn.Linear(128, 128)
        self.linear3 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = self.linear3(x)
        return x

my_model = NeuralNetwork()
my_model.load_state_dict(torch.load(&quot;model_weights.pth&quot;, weights_only=True))
my_model.eval()
</code></pre>
<p>Now, I have a web application where:</p>
<ol>
<li>The user draws on a 28x28 canvas in black and white.</li>
<li>The drawing is put into a flattened array of size 784, consisting of 0's (white on canvas) and 1's (black on canvas). (e.g. [0, 0, 1, 1, 1, 1, 0, 0, ..., 1, 1])</li>
</ol>
<p>I have a sample code of what I wish to perform:</p>
<pre><code>formatted_array = some_formatting_function(flattened_array_of_0_and_1)
x = torch.tensor(formatted_array)
pred = my_model(x)
guessed_digit = some_reading_function(pred)
print(guessed_digit)

# eventually return the guessed_digit
</code></pre>
<p>What should my <code>some_formatting_function</code> and <code>some_reading_function</code> be?</p>
","0","Question"
"79629787","","<p>Pytorch official website used to have an installation option using conda (see printscreen in this answer: <a href=""https://stackoverflow.com/a/51229368/1273751"">https://stackoverflow.com/a/51229368/1273751</a>)</p>
<p>But currently no <code>conda</code> option is available:
<a href=""https://i.sstatic.net/19ODdWY3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/19ODdWY3.png"" alt=""printscreen pytorch website"" /></a></p>
<p>Nevertheless, there still is pytorch on conda-forge: <a href=""https://anaconda.org/conda-forge/pytorch"" rel=""nofollow noreferrer"">https://anaconda.org/conda-forge/pytorch</a></p>
<p>Is there, and what is a currently recommended way to install pytorch using conda with CUDA enabled?</p>
","2","Question"
"79631387","","<p>I have a custom Subset:</p>
<pre><code>class TestSubset2(Subset):
    def __init__(self, dataset, indices, days=False):
        super().__init__(dataset, indices)
        self.days = days
    def __getitem__(self, idx):
        original_idx = self.indices[idx]
        coordinates = self.dataset.all_indices[original_idx]
        day = coordinates[0]

        if self.dataset.transform:
            original_transform = self.dataset.transform
            self.dataset.transform = None
            image, label = self.dataset[original_idx]
            self.dataset.transform = original_transform
        else:
            image, label = self.dataset[original_idx]

        if self.days:
            result = (image, label, torch.tensor(day, dtype=torch.int64))
            assert len(result) == 3, f&quot;Got {len(result)} pieces at idx={idx}&quot;
            return result
        else:
            result = (image, label)
            return result 
</code></pre>
<p>Here, my Subset/Dataset is clearly set on returning three items provided self.days = True, and this is confirmed by my later usage:</p>
<p><code>test_dataset = TestSubset2(data, test_idx, days=True)</code>, where <code>assert all(len(test_dataset[i]) == 3 for i in range(len(test_dataset)))</code> executes without any problems. Yet when it comes to using my dataloader:
<code>test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False), print(len(next(iter(test_loader))))</code> , the output of the cell is 2, not 3. It seems that the dataloader is not bothering to return the last item.</p>
","0","Question"
"79635282","","<p>I'm working on a computer vision project, but I'm having trouble installing python packages as my MacBook seems to be confused about its own architecture.</p>
<p>I'm trying to install pytorch using</p>
<pre><code>uv add torch
</code></pre>
<p>However, I get the following error:</p>
<pre><code>Resolved 60 packages in 113ms                                        
error: Distribution `torch==2.7.0 @ registry+https://pypi.org/simple` can't be installed because it doesn't have a source distribution or wheel for the current platform

hint: You're on macOS (`macosx_15_0_x86_64`), but `torch` (v2.7.0) only has wheels for the following platforms: `manylinux_2_28_aarch64`, `manylinux_2_28_x86_64`, `macosx_11_0_arm64`, `win_amd64`
</code></pre>
<p>The problem seems to be that it makes wrong assumptions about what system it's running on.
I'm working on a <strong>MacBook with M1 Max</strong> (ARM64), not an Intel chip. Also, my MacOS is not 15.0, but <strong>Sequoia 15.4.1</strong>. So this differs from what it states in the &quot;hint&quot; of the error message.</p>
<p>My hypothesis is that it originates from a timemachine migration from an Intel MacBook to the current one with Apple Silicon chip.</p>
<p>Does anyone have an idea how to fix this?</p>
<p>I've spent quite some time and tried many other ways of installing pytorch (pip, conda, manual installation) and trying different Python versions, but I always end up getting this same message for pytorch (interestingly only for pytorch, other packages are installed without any issues).</p>
","0","Question"
"79638455","","<p>I have a padded tensor <strong>X</strong> with shape <em>(B, T1, C)</em> and a padded tensor <strong>Y</strong> with shape <em>(B, T2, C)</em>, I also know the sample lengths <strong>L</strong> for <strong>X</strong>. I want to insert the samples of <strong>X</strong> into <strong>Y</strong> at certain index <em>I</em> and pad at the end.</p>
<p>For example, if <em>I = 5</em>, the goal of the transformation is something like:</p>
<pre><code>inputs = []
for i in range(X.shape[0]):
    input = torch.cat([Y[i][0:5],
                       X[i][:L[i]],
                       Y[i][5:],
                       torch.zeros(max(L) - L[i], Y.shape[2])],
                      dim=0)
    inputs.append(input)
outputs = torch.stack(inputs, dim=0)
</code></pre>
<p>I want to know how to do this in tensor version instead of the for loop, which is too slow for training.</p>
","0","Question"
"79639173","","<p>My training set has dimensions <code>[7000, 2]</code>, but my output has single number in it. I want to configure the model to understand that &quot;I want one output for each row in X_train&quot;, but I don't know how to do that. In other words, my input has 7000 rows and 2 columns, and I want my output to also have 7000 rows (and a single column). Here's a short snippet:</p>
<pre><code>model = LSTMModel(input_dim=2, hidden_dim=100, layer_dim=1, output_dim=1, batch_first=True)
h0 = torch.randn([1, 1, 100])
c0 = torch.randn([1, 1, 100])

for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs, h0, c0 = model(X_train.reshape(1, 7000, 2), h0, c0)
    print (X_train.shape, outputs.shape, Y_train.shape) 
    # prints torch.Size([7000, 2]) torch.Size([1, 1]) torch.Size([7000])
</code></pre>
<p>For reproducibility, I am pasting the full code below:</p>
<pre><code>import torch
import torch.nn.functional as F
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim,  batch_first=True):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=batch_first)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, h0=None, c0=None):
        if h0 is None or c0 is None:
            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        
        out, (hn, cn) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out, hn, cn

g = torch.Generator().manual_seed(2147483647) # for reproducibility
X  = torch.randn((ROWS, FEATS), generator=g)
Y = X[:,0] + X[:,1].roll(1)
Y[0] = X[0,0]
print (X[:10], '\n------------\n', Y[:10], '\n', X.shape, Y.shape)

last_train_ix = int(TRAIN_FRAC*ROWS)

X_train = X[:last_train_ix,]
Y_train = Y[:last_train_ix,]


model = LSTMModel(input_dim=2, hidden_dim=100, layer_dim=1, output_dim=1, batch_first=True)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)



num_epochs = 100
#h0, c0 = None, None
h0 = torch.randn([1, 1, 100])
c0 = torch.randn([1, 1, 100])


for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs, h0, c0 = model(X_train.reshape(1, 7000, 2), h0, c0)
    print (X_train.shape, outputs.shape, Y_train.shape)
    raise Exception(23243)
    loss = criterion(outputs, Y_train)
    loss.backward()
    optimizer.step()

    h0 = h0.detach()
    c0 = c0.detach()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
</code></pre>
","0","Question"
"79641245","","<p>I am curious, as I have not found a way to save a Pytorch model and load the same model to continue training it on a new dataset without needing the original model definition.</p>
<p>So far, I know how to save the model state_dict and optimizer state_dict with Torch.save(), but to use Torch.load(), I would need the model class definition pre-defined.</p>
<p>I also know I can save the model with torch.jit.script() and then .save() to get the model for inferencing without needing the model class definition but I can't continue training the model with this method.
Thus, does anyone know of a way to save and load a Pytorch model without the class definition?</p>
","2","Question"
"79641308","","<p>I'm trying to add more than one generator training step per cycle to a GAN, i.e. I want my Generator to update its parameters <em>n</em> times every <em>m</em> updates of the Discriminator, where <em>n &gt; m</em>.</p>
<p>I wrote this piece of code:</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(num_epochs):
    for batch_idx, (real, _) in enumerate(loader):
        real = real.view(-1, 784).to(device)
        batch_size = real.shape[0]

        # Training Generator
        for i in range(gen_advantage):
            noise = torch.randn(batch_size, z_dim).to(device)
            fake = gen(noise)
            output = disc(fake).view(-1)
            lossG = criterion(output, torch.ones_like(output))
            lossG.backward()
            opt_gen.step()
            gen.zero_grad()

        # Training Discriminator
        for i in range(disc_advantage):
            disc_real = disc(real).view(-1)
            lossD_real = criterion(disc_real, torch.ones_like(disc_real))
            disc_fake = disc(fake).view(-1)
            lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))
            lossD = (lossD_real + lossD_fake) * 0.5
            lossD.backward() # Breaks here
            opt_disc.step()
            disc.zero_grad()

</code></pre>
<p>For context, <code>criterion</code> is a <code>BCELoss</code>, <code>opt_gen</code> and <code>opt_disc</code> are <code>optim.Adam</code>, <code>disc</code> and <code>gen</code> are my Discriminator and Generator instances and the images in the <code>loader</code> are 28x28.</p>
<p>So, this code throws an error at the <code>lossD.backward()</code> line, even if <code>disc_advantage == 1</code>:</p>
<pre><code>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</code></pre>
<p>I can't get why, since in my understanding, I'm neither accessing freed tensors nor backwarding the <code>lossD</code> multiple times.
Anyhow, i tried as suggested to put <code>retain_graph=True</code> in the <code>lossG.backward()</code> line (in the generator loop), but it throws another different error:</p>
<pre><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [512, 784]], which is output 0 of AsStridedBackward0, is at version 15; expected version 14 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
</code></pre>
<p>which I truly can't understand neither, since the error is thrown at the same line as before, i.e. <code>lossD.backward()</code>.</p>
<p>That's it. I tried figuring it out alone by scraping the net for explanations on how PyTorch gradients work, but I only found some theoretical articles on how the gradients are computed, which although interesting, not what I needed.</p>
<p>So please help.</p>
","0","Question"
"79646476","","<p>Just found I randomly coded <code>some_data.unsqueeze(0).to(device)</code> and <code>some_data.to(device).unsqueeze(0)</code>.</p>
<p>If I recall correctly, <code>torch.Tensor.to</code> involves data transferring, something like <code>cudaMemcpyHostToDevice</code>?.</p>
<p>Which makes me wonder, suppose device is set to GPU, does different order between <code>.to(device)</code> and tensor operation yield different performance, etc?</p>
","1","Question"
"79647152","","<p>I'm working on a Python NLP project and using the following library versions:</p>
<p>PyTorch 2.2.2
Transformers 4.52.3
Sentence-Transformers 4.1.0
When I run this code:</p>
<p>from sentence_transformers import SentenceTransformer, util</p>
<p>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</p>
<p>t5_tokenizer = AutoTokenizer.from_pretrained(&quot;ramsrigouthamg/t5_paraphraser&quot;)</p>
<p>t5_model = AutoModelForSeq2SeqLM.from_pretrained(&quot;ramsrigouthamg/t5_paraphraser&quot;)</p>
<p>embedder = SentenceTransformer('all-MiniLM-L6-v2')</p>
<p>I get this error:</p>
<p>AttributeError: module 'torch' has no attribute 'get_default_device'
I suspect this is a compatibility issue between Sentence-Transformers and the version of PyTorch I'm using.</p>
<p>Has anyone run into this?</p>
<p>What version of PyTorch is compatible with Sentence-Transformers 4.1.0?</p>
<p>Is there a fix that doesn’t involve downgrading PyTorch?</p>
<p>Is get_default_device a new method that SentenceTransformer expects?
I’ve checked the Sentence-Transformers GitHub but haven’t found clear compatibility documentation for this specific error.</p>
<p>Any help would be appreciated!</p>
","2","Question"
"79650016","","<p>I'm working on my final studies project, which is detecting anomalies in an irrigation system.
We simulated the irrigation network on a limited environment as you can see in the photo below.<a href=""https://i.sstatic.net/bZph1giU.jpg"" rel=""nofollow noreferrer"">the model we created</a></p>
<p>There is a PCB where an ESP32 microcontroller reads the values from the pressure sensor.
In our model, 4 valves give us 2^4 = 16 combinations. My idea is to make it simple and create a simple neural network model using Pytorch that gets the state of valves as input (0 is off and 1 is on ex:0110) and one output, which is the pressure value. So that means our system will be predicting pressure from the valve combinations. The data set is created by getting certain valve combinations(using fractional factorial design), but I added other combinations like 1000, 0100, 0010, 00001, 1110, and 0111, and then it is augmented to get a large data set.
here is the model :</p>
<pre><code>import torch
import torch.nn as nn   

class PressureNeuralNet(nn.Module):
def __init__(self, n_valves, hidden_size=32, dropout_rate=0.1):
    super().__init__()
    self.network = nn.Sequential(
        nn.Linear(n_valves, hidden_size),
        nn.ReLU(),
        nn.Dropout(dropout_rate),

        nn.Linear(hidden_size, hidden_size),
        nn.ReLU(),
        nn.Linear(hidden_size, 1)
    )

def forward(self, x): 
    return self.network(x)
</code></pre>
<p>learning process:</p>
<pre><code>    nn_model = PressureNeuralNet(n_valves)

    # Define loss and optimizers
    criterion = nn.MSELoss()
    nn_optimizer = optim.Adam(nn_model.parameters(), lr=0.01, weight_decay=1e-5)
    nn_scheduler = optim.lr_scheduler.ReduceLROnPlateau(nn_optimizer, mode='min', patience=5, factor=0.5, verbose=True)

   

    print(&quot;\nTraining Neural Network...&quot;)
    nn_train_losses, nn_test_losses = train_model(
        nn_model, train_loader, test_loader, criterion, nn_optimizer, nn_scheduler
    )
</code></pre>
<p>train model function :</p>
<pre><code>def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler=None, epochs=1000):
train_losses = []
test_losses = []
best_test_loss = float('inf')

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for valve_states, pressures in train_loader:
        optimizer.zero_grad()
        outputs = model(valve_states).squeeze()
        loss = criterion(outputs, pressures)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    
    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for valve_states, pressures in test_loader:
            outputs = model(valve_states).squeeze()
            loss = criterion(outputs, pressures)
            test_loss += loss.item()
    
    avg_test_loss = test_loss / len(test_loader)
    test_losses.append(avg_test_loss)

    if avg_test_loss &lt; best_test_loss:
        best_test_loss = avg_test_loss
        torch.save(model.state_dict(), 'best_model.pth')
        print(f&quot;Saved best model with Test Loss: {best_test_loss:.4f}&quot;)

    if scheduler:
        scheduler.step(avg_test_loss)
    
    if (epoch + 1) % 10 == 0:
        print(f&quot;Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}&quot;)
        if scheduler:
            print(f&quot;Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}&quot;)

return train_losses, test_losses
</code></pre>
<p>The MSE value is: 0.0004
That meas the model predicts very well. The problem now is that when predicting a combination that it didn't learn from the data set  (1101 read value: 0.53 predicted: 0.10, which is a false alert), due to the sampling method that we used (fractional factorial design).
(If you ask why we used this method is to get certain samples that have influence on the output and to prevent water and energy consumption.)
But the other combinations are well predicted.
there might be a problem because of the values of the single valves combinations they all heve the same value: 0.5 but the combination 0100 has a pressure value : 0.45 .</p>
<p>What I'm asking is whether my method is wrong or if there is a problem in my model.</p>
<p>The motivation of this project is the use it on larger irrigation systems with more valves.</p>
","-1","Question"
"79652783","","<p>I'm building regression neural network model for my NLP data.</p>
<p>But can't get through this error: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
<p>What should I do?</p>
<p>ChatGPT says:</p>
<ul>
<li>it's requires_grad=False somewhere in the code which isn't true</li>
<li>my data isn't float which isn't true</li>
<li>my data isn't on same device as model which isn't true</li>
</ul>
<p>Currently, the model with data looks like this:</p>
<ol>
<li><p>My data:</p>
<pre><code>X_val = torch.tensor(X_val_scaled, dtype=torch.float32)
X_test = torch.tensor(X_test_scaled, dtype=torch.float32)

y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
</code></pre>
</li>
<li><p>My model:</p>
<pre class=""lang-none prettyprint-override""><code>    def __init__(self, in_features=780, h1=8, h2=6, output=1):
        super().__init__()
        self.fc1 = nn.Linear(in_features, h1)
        self.fc2 = nn.Linear(h1, h2)
        self.out = nn.Linear(h2, output)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)
        return x

model_nn = Model(in_features=780)
optimizer = torch.optim.Adam(model_nn.parameters(), lr=0.001)
criterion = nn.MSELoss()

num_epochs = 100
loss_list = []

for epoch in range(num_epochs):
    model_nn.train()

    # Forward + loss + backward + step
    y_pred = model_nn(X_train)
    loss = F.mse_loss(y_pred, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    loss_list.append(loss.item())

    # Evaluate on val set
    model_nn.eval()
    with torch.no_grad():
        val_pred = model_nn(X_val)
        val_loss = F.mse_loss(val_pred, y_val)

    print(f&quot;Epoch {epoch+1:03d}/{num_epochs} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}&quot;)

</code></pre>
</li>
</ol>
","1","Question"
"79653147","","<p>I have a Mac M1 laptop, where I have conda env with python=3.9.21, and pip=25.1.1.</p>
<p>Currently I have torch=2.2.0 and I wanted to update it to the latest version, but when I tried <code>pip install -U torch</code> it shows that I have the latest version, and trying <code>pip install -U torch==2.7.0</code> ends in:</p>
<pre class=""lang-none prettyprint-override""><code>ERROR: Could not find a version that satisfies the requirement torch==2.7.0 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)
ERROR: No matching distribution found for torch==2.7.0
</code></pre>
<p>I also downloaded the corresponding <code>whl</code> file and tried to install that, but got:</p>
<pre class=""lang-none prettyprint-override""><code>pip install ~/Downloads/torch-2.7.1-cp39-none-macosx_11_0_arm64.whl 
ERROR: torch-2.7.1-cp39-none-macosx_11_0_arm64.whl is not a supported wheel on this platform.
</code></pre>
<p>Not sure what is going on.</p>
","2","Question"
"79653745","","<p>I'm working with an AWS S3 instance and trying to deploy a SSL model loading a dataset from a bucket list I have defined on S3. The DL framework I'm using is PyTorch and more concretely to load the images dataset from S3 I'm using Torchvsion. However, when I try to load the images from S3 with torchvision.dataset.ImageFolder it raises an error.</p>
<p>Apparently, this is not possible (the post is from 2019): <a href=""https://discuss.pytorch.org/t/can-i-use-torchvision-dataset-and-dataloader-with-aws-s3/34096"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/can-i-use-torchvision-dataset-and-dataloader-with-aws-s3/34096</a></p>
<p>But I would like to know if there is an option rather than the specified here in 2021: <a href=""https://aws.amazon.com/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/</a></p>
<p>EDIT:</p>
<p>I'm also trying to address this problem using the 'new' Amazon's S3 connector for PyTorch:</p>
<p><a href=""https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-s3-connector-pytorch/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-s3-connector-pytorch/</a></p>
<p><a href=""https://github.com/awslabs/s3-connector-for-pytorch"" rel=""nofollow noreferrer"">https://github.com/awslabs/s3-connector-for-pytorch</a></p>
","0","Question"
"79656972","","<p>I wrote a custom activation function in torch, but it is painfully slow:</p>
<pre><code>class NoisyRelu(nn.Module):
  def __init__(self):
    super(NoisyRelu, self).__init__()

  def forward(self, x: torch.Tensor)-&gt; torch.Tensor:
    relu_result = F.relu(x)
    return relu_result + torch.randn(x.size()).to(x.device) * 0.05
</code></pre>
<p>Is there any way I can accelerate it?
Decorating it with torch.jit.script did also not help.</p>
","0","Question"
"79657416","","<p>I'm testing a Self-supervise pre-training model, independently of the model I'm getting the same Error. My environment is Jupiter Labs for AWS Sagemaker, the problem comes when, using args and trying to load the data (either from an S3 bucket or a local file on the instance) I keep getting this error:</p>
<pre><code>usage: VICReg training script [-h] [--data_path DATA_PATH] [--exp-dir EXP_DIR]
                              [--log-freq-time LOG_FREQ_TIME] [--arch ARCH]
                              [--mlp MLP] [--epochs EPOCHS]
                              [--batch-size BATCH_SIZE] [--base-lr BASE_LR]
                              [--wd WD] [--sim-coeff SIM_COEFF]
                              [--std-coeff STD_COEFF] [--cov-coeff COV_COEFF]
                              [--num-workers NUM_WORKERS] [--device DEVICE]
                              [--world-size WORLD_SIZE]
                              [--local_rank LOCAL_RANK] [--dist-url DIST_URL]
                              [--rank RANK]
VICReg training script: error: unrecognized arguments: -f /home/sagemaker-user/.local/share/jupyter/runtime/kernel-299646e9-32c7-47f8-869a-72e80aa9271b.json
</code></pre>
<p>On more information about how I'm setting my code, this is where the Error most probably is:</p>
<pre><code>def get_arguments():
    parser = argparse.ArgumentParser(description=&quot;Pretrain a resnet model with VICReg&quot;, add_help=False)

    # Data
    parser.add_argument(&quot;--data_path&quot;,
                        default='AnnualCrop/', type=Path,
                        help='dataset path')

    # Checkpoints
    parser.add_argument(&quot;--exp-dir&quot;, type=Path, default=&quot;./exp&quot;,
                        help='Path to the experiment folder, where all logs/checkpoints will be stored')
    parser.add_argument(&quot;--log-freq-time&quot;, type=int, default=60,
                        help='Print logs to the stats.txt file every [log-freq-time] seconds')
</code></pre>
<p>NOTE: I've tried with other models and codes as well but the same args.pars structure and keep getting the very same error.</p>
","0","Question"
"79657483","","<pre><code>from torch.utils.data import Dataset, DataLoader
import time
import multiprocessing as mp
import torch

class Sleep(Dataset):
    def __len__(self): return 20
    def __getitem__(self, i):
        import time, os
        time.sleep(1)
        return os.getpid()

if __name__ == &quot;__main__&quot;:
    mp.set_start_method(&quot;fork&quot;, force=True)

    loader = DataLoader(Sleep(), batch_size=20, num_workers=10, persistent_workers=True)

    t0 = time.time()
    next(iter(loader))
    print(&quot;wall = &quot;, time.time() - t0)   # should be ≈ 3-4 s, not 180 s

    next(iter(loader))
    print(&quot;wall = &quot;, time.time() - t0)   # should be ≈ 3-4 s, not 180 s

    next(iter(loader))
    print(&quot;wall = &quot;, time.time() - t0)   # should be ≈ 3-4 s, not 180 s

    import pdb; pdb.set_trace()
</code></pre>
<p>I have this simple test script. On different systems, with at least 10 cpus each, I run it - each time, each batch takes 20 seconds to load, meaning things are running serially and not in parallel with multiprocessing. Why? How do we actually parallelize the dataloader?</p>
","1","Question"
"79658001","","<p>I can't quite figure out how to fine-tune my neural network with jigsaw and make it a multi-classifier rather than a binary-classifier (multiple types of output rather than just positive and negative) and need assistance with doing so.</p>
<p>(With PyTorch).</p>
<p>Here is the source code:</p>
<pre><code>from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

example = &quot;Dude, that's amazing!&quot;

tokens = word_tokenize(example, preserve_line=True)

MODEL = &quot;cardiffnlp/twitter-roberta-base-offensive&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

def evaluate_text(text):
    encoded_text = tokenizer(text, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)

    neg_value = float(scores[1])
    pos_value = float(scores[0])

    print(&quot;Negative value:&quot;, neg_value)
    print(&quot;Positive value:&quot;, pos_value)

evaluate_text(example)
</code></pre>
<p>Well, I really just need assistance rather than just resolving an issue.</p>
","0","Question"
"79658051","","<p>I have a simple tensor</p>
<pre><code>  test_tensor =torch.tensor([ [.1, .2, .3],[.4, .5, .6]])
</code></pre>
<p>when I use softmax
weights = torch.softmax ( test_tensor, dim = -1), I get:</p>
<pre><code> tensor([[0.3006, 0.3322, 0.3672],
    [0.3006, 0.3322, 0.3672]])
</code></pre>
<p>I thought if I do this manually, I was hoping that for the first weight, the weighted normalized value should have been = 0.1/(0.1+0.2+0.3) = 0.166666.
But I am getting 0.3006 with torch softmax. Am I doing it wrong ?</p>
","1","Question"
"79658224","","<p>I've recently been attempting to fine-tune the pretrained neural network, &quot;cardiffnlp/twitter-roberta-base-offensive.&quot; However, when I attempted to run a Python program I created, &quot;evaluation_strategy&quot; was considered to be an &quot;unexpected argument&quot; regardless that I have version 4.52.6 installed. Could anyone help me with fixing this error? (There also might be another one with the classifiers since I'm attempting to have it transition to a 6-label classifier rather than a binary classifier since the Jigsaw Dataset uses 6 labels, so if that seems incorrect in my code, please fix that as well).</p>
<p>Here's the source code:</p>
<pre><code>
from datasets import load_dataset
from transformers import *
import torch
import numpy as np
from sklearn.metrics import f1_score, accuracy_score
from sklearn.preprocessing import MultiLabelBinarizer

# Load dataset (adjust path if needed)
dataset = load_dataset('csv', data_files={
    &quot;train&quot;: &quot;data/train_split.csv&quot;,
    &quot;validation&quot;: &quot;data/validation_split.csv&quot;
})

# Define label columns used in Jigsaw multi-label setup
LABEL_COLUMNS = [&quot;toxic&quot;, &quot;severe_toxic&quot;, &quot;obscene&quot;, &quot;threat&quot;, &quot;insult&quot;, &quot;identity_hate&quot;]

# Tokenizer and model setup
MODEL_NAME = &quot;cardiffnlp/twitter-roberta-base-offensive&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Define label count and problem type
config = AutoConfig.from_pretrained(
    MODEL_NAME,
    num_labels=len(LABEL_COLUMNS),
    problem_type=&quot;multi_label_classification&quot;
)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    config=config,
    ignore_mismatched_sizes=True
)

# Preprocessing function
def preprocess(example):
    encoding = tokenizer(
        example[&quot;comment_text&quot;],
        truncation=True,
        padding=&quot;max_length&quot;,
        max_length=128
    )
    labels = [example[col] for col in LABEL_COLUMNS]
    encoding[&quot;labels&quot;] = torch.tensor(labels, dtype=torch.float)
    return encoding

# Apply preprocessing
encoded_dataset = dataset.map(preprocess)

# Training configuration
training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    save_strategy=&quot;epoch&quot;,
    evaluation_strategy=&quot;epoch&quot;,   # &lt;-- Add this line
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;f1&quot;
)


# Metric function
def compute_metrics(pred):
    preds = torch.sigmoid(torch.tensor(pred.predictions)).numpy()
    labels = pred.label_ids
    preds = (preds &gt; 0.5).astype(int)
    f1 = f1_score(labels, preds, average=&quot;macro&quot;)
    acc = accuracy_score(labels, preds)
    return {&quot;f1&quot;: f1, &quot;accuracy&quot;: acc}

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[&quot;validation&quot;],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()
</code></pre>
<p>Could anyone fix it?</p>
","1","Question"
"79659212","","<p>While working on a SSL model, before start training the first epochs I got the following Error:</p>
<pre><code>        samples = samples.to(device, non_blocking=True)
              ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'to'
</code></pre>
<p>However, after starting debugging I realized that the object is a list of Tensors as depicted in the following image:</p>
<p><a href=""https://i.sstatic.net/WxUTYcJw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxUTYcJw.jpg"" alt=""enter image description here"" /></a></p>
<p>As far as I know usually this problem raises when the object is not a Tensor, but at least the list I'm trying to train is a tensor.</p>
<p>EDIT:</p>
<p>Python indicates that my samples is a list of Tensors, not just a list.
<a href=""https://i.sstatic.net/AJGEkvr8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJGEkvr8.jpg"" alt=""enter image description here"" /></a></p>
","-1","Question"
"79659981","","<p>The spec file I used is as follows, based on <a href=""https://github.com/pyinstaller/pyinstaller/issues/6290"" rel=""nofollow noreferrer"">this discussion</a>.</p>
<pre><code># -*- mode: python ; coding: utf-8 -*-
from PyInstaller.utils.hooks import collect_data_files

datas  = []
datas += collect_data_files('triton', include_py_files=True)
datas += collect_data_files('torchvision', include_py_files=True)

a = Analysis(
    ['python/sglang/launch_server.py'],
    pathex=[],
    binaries=[],
    datas=datas,
    hiddenimports=['triton'],
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    noarchive=False,
    optimize=0,
)
pyz = PYZ(a.pure)

exe = EXE(
    pyz,
    a.scripts,
    [],
    exclude_binaries=True,
    name='launch_server',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)
coll = COLLECT(
    exe,
    a.binaries,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='launch_server',
)
</code></pre>
<p>The packaging process had no errors, but the following error occurred at runtime:</p>
<pre><code>Traceback (most recent call last):
File &quot;sglang/launch_server.py&quot;, line 6, in &lt;module&gt;
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
...
File &quot;PyInstaller/loader/pyimod02_importers.py&quot;, line 457, in exec_module
File &quot;sglang/srt/layers/quantization/int8_utils.py&quot;, line 5, in &lt;module&gt;
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
File &quot;PyInstaller/loader/pyimod02_importers.py&quot;, line 457, in exec_module
File &quot;sglang/srt/layers/quantization/int8_kernel.py&quot;, line 21, in &lt;module&gt;
File &quot;xxx/dist/launch_server/_internal/triton/runtime/jit.py&quot;, line 852, in jit
  return decorator(fn)
File &quot;xxx/dist/launch_server/_internal/triton/runtime/jit.py&quot;, line 840, in decorator
  return JITFunction(
File &quot;xxx/dist/launch_server/_internal/triton/runtime/jit.py&quot;, line 668, in __init__
  self.starting_line_number = inspect.getsourcelines(fn)[1]
File &quot;inspect.py&quot;, line 1121, in getsourcelines
File &quot;inspect.py&quot;, line 958, in findsource
OSError: could not get source code
[PYI-195669:ERROR] Failed to execute script 'launch_server' due to unhandled exception!
</code></pre>
<p>It seems to be a JIT-related issue. Could it be that the JIT code in Triton, which exists as a string, was not packaged correctly? However, I have explicitly added Triton to the spec file and forced the loading of all Python files (through the <code>collect_data_files</code> interface). Are there any other methods to resolve this issue?</p>
<p>The Python project I packaged is sglang, based on the Ubuntu environment, with PyInstaller version 6.14.1 and Python version 3.10.12.</p>
","2","Question"
"79662773","","<p>I want to use ImageFolder from Torchvision with an AWS S3 bucket where I have my dataset saved, however, it requires a os.Path like file and with boto3 I can just get a directory or a list which is not allowed by ImageFolder. Do you know how can I get a S3 directory as os.Path in Python with boto3 or a similar strategy?</p>
<p>I'm loading my dataset as follows:</p>
<pre><code># Initialize S3 client
s3 = boto3.client('s3')
response_test = s3.list_objects_v2(Bucket=BUCKET_NAME_test, Prefix=PREFIX_test)
jpg_files_test = [item['Key'] for item in response_test.get('Contents', []) if  item['Key'].lower().endswith('.jpeg')]
</code></pre>
<p>And using this code to load the data with Torchvision:</p>
<pre><code>transform_train = transforms.Compose([
        transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.10231429, 0.18161748, 0.26542443], std=[0.06512465, 0.10374635, 0.16232868])])
dataset_train = datasets.ImageFolder(os.path.join(args.data_path, 'train'), transform=transform_train)   # 'train'
print(dataset_train)
</code></pre>
<p>But as it's expected I get this error:</p>
<pre><code>TypeError: expected str, bytes or os.PathLike object, not list
</code></pre>
","-1","Question"
"79511915","79323285","<p>I also faced the same issue.</p>
<p>I didn't have clang installed in my system.</p>
<p><a href=""https://stackoverflow.com/questions/63914108/using-clang-in-windows-10-for-c-c"">using Clang in windows 10 for C/C++</a></p>
<p>Installed it with the help of the above link.</p>
<p>Then the default generator is Nmake-Makefiles. I changed it to Visual Studio 17 2022.</p>
<p>This worked for me</p>
","0","Answer"
"79327724","79327723","<p>We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in <code>keras</code>. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.</p>
<h2>Classification</h2>
<p>Let’s take a classification task as an example. If we use the <code>tf.data</code> API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from keras import layers

a = np.ones((4, 224, 224, 3)).astype(np.float32)
b = np.ones((4, 2)).astype(np.float32)

augmentation_layers = keras.Sequential(
    [
        layers.RandomFlip(&quot;horizontal&quot;),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
    ]
)

dataset = tf.data.Dataset.from_tensor_slices((a, b))
dataset = dataset.batch(3, drop_remainder=True)
dataset = dataset.map(
    lambda x, y: (augmentation_layers(x), y), 
    num_parallel_calls=tf.data.AUTOTUNE
)
x.shape, y.shape
(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))
</code></pre>
<p>But for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.</p>
<pre class=""lang-py prettyprint-override""><code>inputs = keras.Input(shape=(224, 224, 3))
processed = augmentation_layers(inputs)
backbone = keras.applications.EfficientNetB0(
    include_top=True, pooling='avg'
)(processed)
output = keras.layers.Dense(10)(backbone)
model = keras.Model(inputs, output)
model.count_params() / 1e6
5.340581
</code></pre>
<p>Here, we set the augmentation pipeline right after <code>keras.Input</code>. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.</p>
<h2>Segmentation</h2>
<p>I'll use this <a href=""https://www.kaggle.com/datasets/ipythonx/carvana-image-masking-png"" rel=""nofollow noreferrer"">dataset</a> for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using <code>keras-3</code>, which might allow for multi-backend support.</p>
<pre class=""lang-py prettyprint-override""><code>import os
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;tensorflow&quot; # torch, jax

import keras
from keras import layers
import tensorflow as tf
keras.__version__ # 3.4.1
</code></pre>
<pre class=""lang-py prettyprint-override""><code># ref https://keras.io/examples/vision/oxford_pets_image_segmentation/
# u-net model
def get_model(img_size, num_classes, classifier_activation):
    ...
    # Add a per-pixel classification layer
    outputs = layers.Conv2D(
        num_classes, 
        3, 
        activation=classifier_activation, 
        padding=&quot;same&quot;, 
        dtype='float32'
    )(x)

    # Define the model
    model = keras.Model(inputs, outputs)
    return model


img_size = (224, 224)
num_classes = 1
classifier_activation = 'sigmoid'
model = get_model(
    img_size, 
    num_classes=num_classes, 
    classifier_activation=classifier_activation
)
</code></pre>
<p>Let's define the augmentation pipelines.</p>
<pre class=""lang-py prettyprint-override""><code>augmentation_layers = [
    layers.RandomFlip(&quot;horizontal_and_vertical&quot;)
]

def augment_data(images, masks):
    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)
    for layer in augmentation_layers:
        combined = layer(combined)
    images_augmented = combined[..., :3]
    masks_augmented = tf.cast(combined[..., 3:], tf.int32)
    return images_augmented, masks_augmented
</code></pre>
<p>Let’s define the <code>tf.data API</code> to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.</p>
<pre class=""lang-py prettyprint-override""><code>def read_image(image_path, mask=False):
    image = tf.io.read_file(image_path)
    
    if mask:
        image = tf.image.decode_png(image, channels=1)
        image.set_shape([None, None, 1])
        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
        image = tf.cast(image, tf.int32)
    else:
        image = tf.image.decode_png(image, channels=3)
        image.set_shape([None, None, 3])
        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
        image = image / 255.
        
    return image

def load_data(image_list, mask_list):
    image = read_image(image_list)
    mask  = read_image(mask_list, mask=True)
    return image, mask

def data_generator(image_list, mask_list):
    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))
    dataset = dataset.shuffle(8*BATCH_SIZE) 
    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

    # Augmenting on CPU
    dataset = dataset.map(
        augment_data, num_parallel_calls=tf.data.AUTOTUNE
    )
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset
</code></pre>
<pre><code>IMAGE_SIZE = 224
BATCH_SIZE = 16

train_dataset = data_generator(images, masks)
print(&quot;Train Dataset:&quot;, train_dataset)
Train Dataset: &lt;_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))&gt;
</code></pre>
<p>Now, let's compile it and run it.</p>
<pre class=""lang-py prettyprint-override""><code>optim = keras.optimizers.Adam(0.001)
bce   = keras.losses.BinaryCrossentropy()
metrics = [&quot;accuracy&quot;]
model.compile(
    optimizer=optim, 
    loss=bce, 
    metrics=metrics
)

%%time
epochs = 2
model.fit(
    train_dataset, 
    epochs=epochs, 
)
Epoch 1/2
318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087
Epoch 2/2
318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338
CPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s
Wall time: 1min 48s
</code></pre>
<p>Next, we will remove the augmentation layers from the dataloader.</p>
<pre class=""lang-py prettyprint-override""><code>def data_generator(image_list, mask_list):
    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))
    dataset = dataset.shuffle(8*BATCH_SIZE)
    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

IMAGE_SIZE = 224
BATCH_SIZE = 16

train_dataset = data_generator(images, masks)
</code></pre>
<p>To offload augmentation to the GPU, we’ll create a custom model class, override the <a href=""https://keras.io/guides/custom_train_step_in_tensorflow/"" rel=""nofollow noreferrer""><code>train_step</code></a>, and use the <code>augment_data</code> method that we defined earlier. Here's how to structure it:</p>
<pre class=""lang-py prettyprint-override""><code>class ExtendedModel(keras.Model):
    def __init__(self, model, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.model = model

    def train_step(self, data):
        x, y = data
        x, y = augment_data(x, y)
        return super().train_step((x, y))

    def call(self, inputs):
        return self.model(inputs)

    def save(
        self, filepath, 
        overwrite=True, 
        include_optimizer=True, 
        save_format=None, 
        add_loss=None, 
    ):
        # Overriding this method will allow us to use the `ModelCheckpoint`
        self.model.save(
            filepath=filepath,
            overwrite=overwrite,
            save_format=save_format,
            include_optimizer=include_optimizer,
        )
</code></pre>
<p>Now that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.</p>
<pre class=""lang-py prettyprint-override""><code>model = get_model(
    img_size, 
    num_classes=num_classes, 
    classifier_activation=classifier_activation
)
emodel = ExtendedModel(model)
optim = keras.optimizers.Adam(0.001)
bce   = keras.losses.BinaryCrossentropy()
metrics = [&quot;accuracy&quot;]
emodel.compile(
    optimizer=optim, 
    loss=bce, 
    metrics=metrics
)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>%%time
epochs = 2
emodel.fit(
    train_dataset, 
    epochs=epochs, 
    callbacks=[
        keras.callbacks.ModelCheckpoint(
            filepath='model.{epoch:02d}-{loss:.3f}.keras',
            monitor='loss',
            mode='min',
            save_best_only=True
        )
    ]
)
Epoch 1/2
318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748
Epoch 2/2
318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585
CPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s
Wall time: 1min 29s
</code></pre>
<p>So, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around <strong>18.35%</strong> improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.</p>
<p>As shown in the <code>ExtendedModel</code> class above, we override the <a href=""https://keras.io/api/models/model_saving_apis/"" rel=""nofollow noreferrer""><code>save</code></a> method, allowing the <code>callbacks.ModelCheckpoint</code> to save the full model. Inference can then be performed as shown below.</p>
<pre class=""lang-py prettyprint-override""><code>loaded_model = keras.saving.load_model(
    &quot;/kaggle/working/model.02-0.0585.keras&quot;
)
x, y = next(iter(train_dataset))
output = loaded_model.predict(x)
1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step
</code></pre>
<h2>Update</h2>
<p>In order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the <code>augment_data</code> that is used in <code>ExtendedModel</code> use the following backend agnostic <code>keras.ops</code> functions.</p>
<pre class=""lang-py prettyprint-override""><code>def augment_data(images, masks):
    combined = keras.ops.concatenate(
        [images, keras.ops.cast(masks, 'float32')], axis=-1
    )
    for layer in augmentation_layers:
        combined = layer(combined)
    images_augmented = combined[..., :3]
    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')
    return images_augmented, masks_augmented
</code></pre>
<p>Additionally, to make the pipeline flexible for all backend, we can update the <code>ExtendedModel</code> as follows. Now, this code can run with tensorflow, jax, and torch backends.</p>
<pre class=""lang-py prettyprint-override""><code>class ExtendedModel(keras.Model):
    ...

    def train_step(self, *args, **kwargs):
        if keras.backend.backend() == &quot;jax&quot;:
            return self._jax_train_step(*args, **kwargs)
        elif keras.backend.backend() == &quot;tensorflow&quot;:
            return self._tensorflow_train_step(*args, **kwargs)
        elif keras.backend.backend() == &quot;torch&quot;:
            return self._torch_train_step(*args, **kwargs)

    def _jax_train_step(self, state, data):
        x, y = data
        x, y = augment_data(x, y)
        return super().train_step(state, (x, y))

    def _tensorflow_train_step(self, data):
        x, y = data
        x, y = augment_data(x, y)
        return super().train_step((x, y))

    def _torch_train_step(self, data):
        x, y = data
        x, y = augment_data(x, y)
        return super().train_step((x, y))

    ...
</code></pre>
","0","Answer"
"79328698","79328514","<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>
<p>To modify the <strong>data collator</strong></p>
<pre class=""lang-py prettyprint-override""><code>class CustomDataCollator(DataCollatorWithPadding):
    def __call__(self, features):
        batch = super().__call__(features)
        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])
        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])
        return batch
</code></pre>
<p>then pass it to <code>Trainer</code></p>
<pre class=""lang-py prettyprint-override""><code>trainer = Trainer(
    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=CustomDataCollator(tokenizer)
)
</code></pre>
<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>
<p>Hope, it will work!</p>
","1","Answer"
"79328840","79328783","<p>Not a pure answer to your question, but you could create a virtualenv and see if you can get your modules installed that way.</p>
<pre><code>mkdir temp-test-pip
cd temp-test-pip
python3 -m venv venv
. ./venv/bin/activate
( ~your shell prompt should now change slightly to show you are in a virtualenv~ )
python3 -m pip install pandas
python3 -m pip install torch
python3 -m pip list
</code></pre>
<p>The last command will show what you've installed, version numbers and such.
You could use those to troubleshoot what you've got outside your virtualenv.
To drop back out of your virtualenv type 'deactivate'.</p>
<p>If I do the above I get this (in python 3.11.0):</p>
<pre><code>    Package           Version
----------------- -----------
filelock          3.16.1
fsspec            2024.12.0
Jinja2            3.1.5
MarkupSafe        3.0.2
mpmath            1.3.0
networkx          3.4.2
numpy             2.2.1
pandas            2.2.3
pip               22.3
python-dateutil   2.9.0.post0
pytz              2024.2
setuptools        65.5.0
six               1.17.0
sympy             1.13.1
torch             2.5.1
typing_extensions 4.12.2
tzdata            2024.2
</code></pre>
","-1","Answer"
"79331296","79331255","<p>I found a way to resolve the issue: basically reinstall pytorch and update protobuf to a newer version:
Original version: protobuf 5.28.2 and pytorch  2.5.1</p>
<pre class=""lang-bash prettyprint-override""><code>conda install --force-reinstall pytorch=2.5.1 protobuf=5.28.3
</code></pre>
<p>Now python -c 'import torch' works perfectly.</p>
","1","Answer"
"79338282","79330498","<p>After some testing I came to the point that the difference depends on tensor size.</p>
<p>For example, when input_len is 200000, timing is</p>
<pre><code>Without out: 0.0036034584045410156
With out: 0.003452301025390625
</code></pre>
<p>when input_len is 200000000,</p>
<pre><code>Without out: 5.12121057510376
With out: 5.473931550979614
</code></pre>
<p>For small tensors, using out=y is faster because it skips dynamic memory allocation.
For large tensors torch.fft.rfft(x) becomes faster because PyTorch can optimize memory allocation internally.</p>
<p>Using out=y introduces additional validation overhead (checking tensor size, dtype, etc) which slows things down.</p>
","0","Answer"
"79332985","79332960","<p>Your problem is with the definition of <code>up</code> method:</p>
<pre><code>def up(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            self.double_conv(in_channels, out_channels),
        )
</code></pre>
<p><code>ConvTranspose2d</code> outputs a tensor with <code>out_channels</code> channels but <code>double_conv</code> expects an input tensor of <code>in_channels</code> channels.</p>
<p>You should probably use something like:</p>
<pre><code>def up(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            self.double_conv(out_channels, out_channels), # NOTE CHANGE HERE
        )

</code></pre>
","0","Answer"
"79342853","79342834","<ol>
<li>Verify the Python Environment Jupyter is Using</li>
</ol>
<p>Run this command in a Jupyter notebook cell: !which python</p>
<p>Compare the output with the Python environment where 'torch' is installed.</p>
<ol start=""2"">
<li>Install 'torch' in Jupyter's Environment</li>
</ol>
<p>Run this command in a Jupyter notebook cell: !pip install torch</p>
<p>Or, if using 'conda': !conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</p>
<ol start=""3"">
<li>Ensure the Jupyter Kernel Matches the Correct Python Environment</li>
</ol>
<p>If you want Jupyter to use the Python environment where 'torch' is already installed:</p>
<p>Install the 'ipykernel' package in your Python environment: pip install ipykernel</p>
<p>Add your Python environment to Jupyter: python -m ipykernel install --user --name=myenv --display-name &quot;Python (myenv)&quot;</p>
<p>Replace 'myenv' with the name of your Python environment.</p>
<p>Restart Jupyter Lab and select the correct kernel:</p>
<p>Open Jupyter Lab.
Go to the 'Kernel' menu &gt; 'Change Kernel' &gt; Select the kernel named &quot;Python (myenv)&quot;.
4. Restart Jupyter Lab</p>
<p>After ensuring that 'torch' is installed in the correct environment or Jupyter is using the desired environment, restart Jupyter Lab and test again.</p>
","0","Answer"
"79347581","79345260","<p>I found the solution, I changed to <code>UnsqueezeTransform(-4, in_keys=[&quot;pixels&quot;])</code> within agent_explore and now I have the wanted behaviour ... (:</p>
","0","Answer"
"79351684","79351470","<p>First, check the nvdia driver version by typing <strong>nvidia-smi</strong> in your cli terminal. For example, in the upper right corner of my server, Driver Version: 535.171.04 CUDA Version: 12.2 appears, which means that cuda can be installed up to version 12.2. In my experience, the newer the nvdia driver version, the better.</p>
<p>I suggest you use a virtual environment, such as <a href=""https://docs.anaconda.com/miniconda/install/"" rel=""nofollow noreferrer"">minicoda</a>, to distinguish different versions of torch, cuda, etc.</p>
<pre><code>mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
</code></pre>
<p>Start the virtual environment and then in your virtual environment, install the latest pytoch and the desired cuda version, which is currently only supported up to 12.4.</p>
<pre><code>conda install pytorch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 pytorch-cuda=12.4 -c pytorch -c nvidia
</code></pre>
<p>Other versions can be found on the <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">pytorch</a> official website.</p>
<p>Finally install jupyter notebook.</p>
<pre><code>pip install notebook
</code></pre>
<p>I didn't encounter any errors when using jupyter notebook according to this process. I think installing the wrong version of cuda will not cause an error, but will return to cpu mode. You can check whether it returns true by entering <strong>torch.cuda.is_available</strong>, which means that the gpu is used correctly.</p>
","1","Answer"
"79352146","79350403","<p>So, If I understood your question correctly, you're implementing an attention mechanism between the <code>i</code>th and <code>j</code>th sequences in a batch. First you linearly project your data (X) to get the queries: XW_Q, then you linearly project your data to get the keys: XW_K. You then add bias a_K and finally you want compute the dot product (similarity) between XW_Q @ (XW_K + a_K).</p>
<p>In this case, each D-dimensional embedding from the queries is multiplied (in the dot product sense) with every D-dimensional embedding from the keys. The output of a dot product of two vectors is a scalar, to the shape of e_ij should be [B, S, S, H], rather than [B, S, H, D].</p>
<p>Then, after normalization, you apply softmax such that every ith row sums to 1 to get the scaling matrix alpha which is also [B, S, S, H]</p>
<p>Now, you project your input the get the values: X@W_V. This should result in a [B, S, H, D] Tensor.</p>
<p>Finally, you get the new ith sequence (z_i) by scaling every jth sequence column of XW_V by the jth scaling factor in alpha_i and sum, resulting in a [B, S, H, D] tensor as you expected.
See the modified code below.</p>
<p>Hopefully, this is a clear enough explanation. I hope I got your intention right and I that I didn't mix up any indices.</p>
<pre><code>import torch
import torch.nn.functional as F
X = torch.randn((10, 20, 30, 40))
B, S, H, D = X.shape
d_z = D  # Assuming d_z is equal to D for simplicity

W_Q = torch.randn(H, D, D)
W_K = torch.randn(H, D, D)
W_V = torch.randn(H, D, D)

a_K = torch.randn(S, S, H, D)
a_V = torch.randn(S, S, H, D)

XW_Q = torch.einsum('bshd,hde-&gt;bshe', X, W_Q)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]
XW_K = torch.einsum('bshd,hde-&gt;bshe', X, W_K)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]

e_ij_numerator = (XW_Q.unsqueeze(2) * (XW_K.unsqueeze(1) + a_K)).sum(dim=-1)  # [B, S, S, H]
e_ij = e_ij_numerator / torch.sqrt(torch.tensor(d_z, dtype=torch.float32))  # [B, S, S, H]
alpha = F.softmax(e_ij, dim=2)  # [B, S, S, H]
XW_V = torch.einsum('bshd,hde-&gt;bshe', X, W_V)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]


z_i = torch.einsum('bijh,bijhd -&gt; bihd', alpha, (XW_V.unsqueeze(1) + a_V))  # [B, S, S, H] * [B, S, S, H, D] -&gt; [B, S, H, D]
print(z_i.shape) # [B, S, H, D]. 
</code></pre>
","0","Answer"
"79357079","79343187","<p>So here is a analytical implementation to the code above (assume that <code>alpha_t</code> and <code>w_t</code> is not varying for each timestep):</p>
<pre class=""lang-py prettyprint-override""><code>def analytical_tilde_alphas(times, alphas):
    sqrt_alphas_t = torch.sqrt(alphas)
    tilde_alphas = sqrt_alphas_t / (sqrt_alphas_t - 1) * (sqrt_alphas_t.pow(times) - 1)
    
    return tilde_alphas
</code></pre>
<p>==============================================================================================================================</p>
<p>For the case where <code>alpha_t</code> or <code>w_t</code> varying in the inner loop, there should be some note on the input: <code>alphas</code> should now be length <code>max(t)</code></p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

def compute_in_parallel_special_case(coeffs, values):
    log_coeffs = torch.log(coeffs)
    log_values = torch.log(values)
    a_star = F.pad(torch.cumsum(log_coeffs, dim=-1), (1, 0))        
    log_x0_plus_b_star = torch.logcumsumexp(log_values - a_star, dim=-1) 
    log_x = a_star + log_x0_plus_b_star                         
    return torch.exp(log_x)                                         


t = torch.tensor([816, 724, 295,  54, 205, 373, 665, 656, 690, 280, 234, 155,  31, 684,
        159, 749, 893, 795, 375, 443, 121, 881, 477, 326, 337, 970, 384, 247,
        511, 432, 563, 753, 764,  77, 294, 803, 935, 507, 196, 744, 140, 641,
        746, 844, 337,   4, 259, 276, 909, 962, 460, 372, 620, 466,  15, 244,
        456, 829, 491, 620, 943, 925,  82, 856, 782, 117, 609, 909, 198, 626,
        992, 998, 672, 762, 602,  85,  46, 529,  42, 841, 441, 237, 839, 953,
                87, 941, 987, 980, 304, 690,  19, 598, 687, 483, 806, 366, 807, 136,
        997, 708, 902, 751, 560, 245, 375, 688, 912, 547,  11, 285,   5,  83,
        104, 346, 312, 236, 335, 664, 435, 762, 575, 184, 341, 618, 257, 634,
        355, 762])

alphas_t = torch.rand(t.amax().item()) ## length of max(t)

def compute_tilde_alphas_varied_naive(times, alphas):
    
    batch_size = times.shape[0]
    tilde_alphas = torch.zeros(batch_size, device=alphas.device)
    alphas = torch.sqrt(alphas)
    # Iterate over the batch
    for i in range(batch_size):
        t = int(times[i])  # Current time for this batch element
        
        tilde_alpha_t = 0  # Start with tilde_alpha_0 = 0
        for step in range( t):  # Assume t defines the recursion depth
            alpha_t = alphas[step]  ### &lt;-------- Varying alpha_t
            tilde_alpha_t = alpha_t * (1 + tilde_alpha_t)
        
        # Store the result
        tilde_alphas[i] = tilde_alpha_t
    
    return tilde_alphas

def compute_tilde_alphas_varied_parallel(times: torch.Tensor, alphas: torch.Tensor):

    alphas = torch.sqrt(alphas)
    alphas_batch = alphas.unsqueeze(0).repeat(times.shape[0], 1)

    # handle the case where the initial value is 0
    new_t = (times - 1 ).unsqueeze(1)
    initial_tilde_alphas = alphas_batch[:, 0:1]
    alphas_batch = alphas_batch[:, 1:]
  
    time_range = torch.arange(0, alphas_batch.shape[1]).unsqueeze(0)
    time_range_mask = time_range &lt; new_t

    coeffs = torch.where(time_range_mask, alphas_batch   , 1)
    values = torch.where(time_range_mask, alphas_batch   , 0)
    values = torch.cat([initial_tilde_alphas, values], dim = 1)
    tilde_alphas = compute_in_parallel_special_case(coeffs, values)[:,-1]

    return tilde_alphas

def compute_tilde_alphas_varied_sequential(times: torch.Tensor, alphas: torch.Tensor):
    
    batch_size = times.shape[0]
    tilde_alphas = torch.zeros(batch_size, device=alphas.device)
    alphas = torch.sqrt(alphas)
    for i, t in enumerate(times):
        # handle the case where the initial value is 0
        alphas2t = alphas[:t]
        coeffs = alphas2t[1:]
        values = alphas2t
        tilde_alphas[i] = compute_in_parallel_special_case(coeffs, values)[-1]
    return tilde_alphas

</code></pre>
<p>Above is three equivalent implementations, the first one is the naive version (the control version), one looping through each item in a batch of time, one is fully parallelize (be careful when using the parallel version - it may not always faster as it contains some unnecessary computation). One may change it to add <code>w_t</code> if needed,</p>
","0","Answer"
"79369483","79344879","<p>There are a couple of ways to solve this.</p>
<p>Option 1: Use docker compose and define a network in compose file to be used by all the containers i.e. running distributed workload.</p>
<p>Option 2: Create a docker network and while issuing the run command pass the same network name to the containers. Mount a shared volume where each node writes an entry file i.e. empty with name as obtained IP address.</p>
<p>In either case, the key thing is to run the containers in the same network. This allows them to peer with each other. In case you want to declare the IP address before start on can pass <code>--ip &lt;Intended IP&gt;</code>. Remember that the supplied IP should lie within the address range of the docker network provided. To know the subnet range use <code>docker network inspect &lt;network name&gt;</code>. Where the network name can be obtained with <code>docker network ls</code></p>
","0","Answer"
"79451223","79335746","<p>The problem is that every time we run the program, need to clean the gpu environment. The code is:</p>
<pre><code>def clear_nccl_environment():
    dist.barrier() 
    torch.cuda.empty_cache()  
</code></pre>
","0","Answer"
"79352602","79352559","<p>If I were you, I would use Open3D to create the initialization of the mesh, after mesh is created, I would not adjust vertex interconnection, i.e. not changing edges via optimization.</p>
<p>Then convert this mesh into PyTorch3D, optimize vertex positions with custom written / found on the Internet loss on mesh volume, self-intersection, etc. Optimize for a few hundreds steps, convert to Open3D, use it to do whatever deterministic algorithms (e.g. simplify mesh, compute volume exactly for sanity check, etc).</p>
<p>Then convert new mesh to PyTorch3D mesh and restart optimization (with resetting optimizer state for mesh data).</p>
<blockquote>
<p>&quot;I underestimating the effort required to extend one of them?&quot;</p>
</blockquote>
<p>You do, it's very hard to do most of mesh manipulations differentiably, so extending Open3D for differentiability is not an option</p>
<p>Writing custom mesh algorithms that you want on top of PyTorch3D is more realistic, but I would anyway suggest to stay away from it if possible</p>
","2","Answer"
"79356335","79356290","<p>It does seem like property setters cannot be used with <code>nn.Module</code> because of the overriding of <code>__setattr__</code>. Writing <code>module.x = new_module</code> will essentially invoke <code>self.add_module('x', new_module)</code>, regardless of <code>x</code> being a property with a setter or not.</p>
<p>Therefore, using an attribute <code>_x</code> for storing the submodule will not work because you can't write a setter to set <code>_x</code>; instead, <code>self.module['x']</code> will be set instead, and must be used as the store for the submodule. This means the property getter must read the value from there.</p>
<p>In code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn


class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.x = nn.Linear(5, 5)  # set using setter

    @property
    def x(self):
        return self._modules['x']


module = MyModule()
print(&quot;Initial module.x:&quot;, module.x)

# Replace the submodule
module.x = nn.Linear(5, 2)

print(&quot;After assignment attempt, module.x:&quot;, module.x)
</code></pre>
<p>This produces the expected output:</p>
<pre><code>Initial module.x: Linear(in_features=5, out_features=5, bias=True)
After assignment attempt, module.x: Linear(in_features=5, out_features=2, bias=True)
</code></pre>
","0","Answer"
"79358565","79355534","<p>I faced the same issue with other transformers models.</p>
<p>I cant retrieve the GitHub issue associated to your problem, but it has not been fixed yet. All I know is that if you want to use some transformers pretrained models you can't use 'mps' as a device.</p>
<p>One possibility that you have is using your 'cpu' which signify much more training time.</p>
<pre><code>device = &quot;cpu&quot;  
</code></pre>
<blockquote>
<p>instead of</p>
</blockquote>
<pre><code>device = &quot;mps&quot;  # or `cpu``
</code></pre>
<p>Another possibility is to use similar pretrained models supporting mps configuration. From my personal experience, the timm library, containing SOTA computer vision models (<a href=""https://huggingface.co/timm"" rel=""nofollow noreferrer"">https://huggingface.co/timm</a>), is working quite smoothly with mps processor (I have the M3 Pro).</p>
<p>Best,
Arnaud</p>
","0","Answer"
"79359100","79358379","<p>My answer is quite embarrassing: out of a sudden, torch.cuda.is_available() returns True now. And, now that it does so, it’s working on the host, in both the venv and the conda env and also in another conda env using CUDA 12.1. Unfortunately for anyone ending up here via a search, I haven’t got the slightest idea what has changed in the meantime. So, sorry everyone for bothering you and thanks for the comment anyways!</p>
<p>Edit: Please see the thread at <a href=""https://discuss.pytorch.org/t/cuda-not-available/215422"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/cuda-not-available/215422</a> for a slightly more detailed discussion of what might have been going on.</p>
","0","Answer"
"79360324","79360262","<p>after times trying, I found the solution,</p>
<ol>
<li>change the 1.py</li>
</ol>
<pre><code>
#%matplotlib inline
import os
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l
import matplotlib.pyplot as plt


def f(x):
    return 3 * x ** 2 - 4 * x
    
    
def numerical_lim(f, x, h):
    return (f(x + h) - f(x)) / h

h = 0.1
for i in range(5):
    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
    h *= 0.1

def use_svg_display():  #@save
    &quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;
    backend_inline.set_matplotlib_formats('svg')
    
def set_figsize(figsize=(3.5, 2.5)):  #@save
    &quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;
    use_svg_display()
    d2l.plt.rcParams['figure.figsize'] = figsize

#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    &quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    &quot;&quot;&quot;绘制数据点&quot;&quot;&quot;
    if legend is None:
        legend = []

    set_figsize(figsize)
    
        
    axes = axes if axes else d2l.plt.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, &quot;ndim&quot;) and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], &quot;__len__&quot;))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)


x = np.arange(0, 3, 0.1)
plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])


plt.show()


</code></pre>
<ol start=""2"">
<li>run the command &quot;IPython 1.py&quot;;</li>
</ol>
<p>then it worked successfully, and showed the figure, hope it can help someone!</p>
","0","Answer"
"79361202","79359767","<p>Disclaimers:</p>
<ul>
<li>My answer is a mix of code reading and &quot;educated guessing&quot;. I did not run the actual code, but a run with the help of a debugger should help you verify/falsify my assumptions.</li>
<li>The code shared below is a condensed version of the <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/51079c65a3669ef91186315eea837d7b7830c6cb/detailed%20train%20and%20test/train_test_DTrAttUnet_BinarySegmentation.py#L220-L311"" rel=""nofollow noreferrer"">relevant section</a> of the score/metrics calculations, to help focus on the essentials. It is not runnable and should be understood as pseudocode.</li>
</ul>
<p>Anyway, let's break down their code (maybe put the code sample side by side with the explanations below it):</p>
<pre class=""lang-py prettyprint-override""><code>dice_scores, dice_scores2, TP, TN, FP, FN = 0, 0, 0, 0, 0, 0

for batch in tqdm(dataloader):

    x, y, _, _ = batch
    outputs, _ = model(x)
    preds = segm(outputs) &gt; 0.5
    yy = y &gt; 0.5

    TP += np.sum(((preds == 1) + (yy == 1)) == 2)
    TN += np.sum(((preds == 0) + (yy == 0)) == 2)
    FP += np.sum(((preds == 1) + (yy == 0)) == 2)
    FN += np.sum(((preds == 0) + (yy == 1)) == 2)

    for idice in range(preds.shape[0]):
        dice_scores += ((2 * (preds[idice] * yy[idice]).sum()) /
                        ((preds[idice] + yy[idice]).sum() + 1e-8))

    predss = np.logical_not(preds)
    yyy = np.logical_not(yy)

    for idice in range(preds.shape[0]):
        dice_sc1 = ((2 * (preds[idice] * yy[idice]).sum()) /
                    ((preds[idice] + yy[idice]).sum() + 1e-8))
        dice_sc2 = ((2 * (predss[idice] * yyy[idice]).sum()) /
                    ((predss[idice] + yyy[idice]).sum() + 1e-8))
        dice_scores2 += (dice_sc1 + dice_sc2) / 2

epoch_dise = dice_scores/len(dataloader.dataset)
epoch_dise2 = dice_scores2/len(dataloader.dataset)
F1score = TP / (TP + ((1/2)*(FP+FN)) + 1e-8)
IoU = TP / (TP+FP+FN)
</code></pre>
<ul>
<li>The first line initializes all accumulated values to <code>0</code>.</li>
<li>With <code>for batch in tqdm(dataloader)</code>, the code iterates over all samples in the data set (or rather, over all samples accessible to the used <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer""><code>DataLoader</code></a>, which might be a subset or an otherwise preprocessed version of the underlying data). This implies that the accumulated values represent the &quot;global&quot; results, i.e the results for the complete data set.</li>
<li>After applying the <code>model</code> to the sample data in the batch, <code>x</code>, via <code>model(x)</code>, the resulting predictions, <code>outputs</code>, are thresholded to a binary representation, <code>preds</code>, via <code>segm(outputs) &gt; 0.5</code>. The <code>segm</code> function, in this case, is simply a sigmoid (see line 190 in the original code), which maps all values to the range <code>[.0, .1]</code>. A similar step is performed for the &quot;ground truth&quot; (i.e. the true/known segmentation), <code>y</code>, to produce its binary representation, <code>yy</code>. <strong>[Update]</strong> The <code>outputs</code> variable, in this context, holds the outputs of one of the two branches of the model only <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/51079c65a3669ef91186315eea837d7b7830c6cb/Architecture.py#L252-L292"" rel=""nofollow noreferrer"">(compare relevant model code)</a>, thus either lesion or organ segmentation. Also compare Figure 2 in the paper: the corresponding branches are the ones with blue and green background. While suppressed in my pseudocode above (<code>outputs, _ = model(x)</code>), the actual code also uses the output of the other branch, though only for <em>loss</em> calculation and not for the calculation of the <em>scores/metrics</em>. <strong>[/Update]</strong></li>
<li>So, to summarize, what we have at the current point:
<ul>
<li>A binary mask <code>preds</code> that holds the predicted segmentations for all samples in the current batch.</li>
<li>A binary mask <code>yy</code> that holds the true/known segmentations for all samples in the current batch.</li>
</ul>
</li>
<li>The <code>sum()</code> calculations in the following lines are counting (albeit written in a bit of an unconventional way maybe) the number of voxels matching between predictions and true/known segmentations, over all samples in the current batch. For example, this means summing the number of matching voxels of predicted foreground and true/known background for the false positives, which is then added on top of their global count, <code>FP</code>.</li>
<li>The Dice scores are handled a bit differently: values are calculated for each index along dimension 0 separately, before adding them to the global value, <code>dice_scores</code>. Dimension 0 usually indexes individual samples in the batch, so this would mean calculating a separate Dice score for each sample. The values are later normalized by dividing through the number of samples, <code>len(dataloader.dataset)</code>, to gain <code>epoch_dise</code>. These two steps are in accordance with the equation <em>Dice score = …</em> shared in the question, which calculates the Dice score separately for each sample, adds all corresponding results, then divides by the number of samples (called <em>testing images</em> there), <code>N</code>.</li>
<li>A second Dice score is then calculated by not only measuring foreground overlaps, but also measuring background overlaps:
<ul>
<li>The masks <code>predss</code> and <code>yyy</code> are the negations of <code>preds</code> and <code>yy</code>, respectively, i.e. <code>True</code> for background voxels, <code>False</code> for foreground voxels.</li>
<li>For each sample, the Dice score for the foreground voxels is recalculated as <code>dice_sc1</code>; but then also the Dice score for the background voxels is calculated as <code>dice_sc2</code>. Then their average is taken.</li>
<li>This sample average is accumulated in the global value <code>dice_scores2</code>, which is later normalized to <code>epoch_dise2</code>, just as <code>dice_scores</code> and <code>epoch_dise</code> above.</li>
</ul>
</li>
<li>At that point what is missing is the calculation of the F1 score and IoU. This is done with <code>F1score = …</code> and <code>IoU = …</code> over the global values of the true/false positives/negatives, in accordance with the corresponding equations cited in the question.</li>
</ul>
<p>So, to summarize once more:</p>
<ul>
<li><strong>[Update]</strong> While the model produces two outputs (lesion and organ segmentation) for each sample via two branches, only one output is used for calculating the <em>scores/metrics</em>, while both outputs are used for calculating the <em>losses</em>. I am not sure which branch is used for the score calculation, but assuming the authors use the same order in their code as they use in their paper, it would be the lesion branch. <strong>[/Update]</strong></li>
<li>Indeed, as assumed in the question, F1 score and IoU are calculated over all samples.</li>
<li>Dice scores are calculated for each sample separately, then averaged over all samples. This is done in two versions:
<ul>
<li>Version 1 (<code>dice_scores</code>, <code>epoch_dise</code>) calculates what I would call the &quot;standard&quot; Dice coefficient, i.e. the score for overlapping foreground voxels.</li>
<li>Version 2 (<code>dice_scores2</code>, <code>epoch_dise2</code>) calculates what I would call a &quot;weighted&quot; Dice coefficient: for each sample, it calculates both the score for overlapping foreground voxels and the score for overlapping background voxels, then averages them as the sample's score, and only then accumulates and averages again to get the global score.</li>
</ul>
</li>
</ul>
","1","Answer"
"79362023","79360229","<blockquote>
<ol>
<li>in the <strong>getitem</strong> we are reading an image from disk to memory. It means if we train our model for several epochs, we are re-reading the same image into memory several times. To my knowledge it is a costly action</li>
</ol>
</blockquote>
<p>An alternative would be to cache the sample when it is first read, such that by the end of the first epoch all of the sample will be cached in memory for faster subsequent access. However, this would require enough RAM to hold the entire dataset. This is probably the limitation that the example is trying to circumvent - they read each sample each time it is needed because of memory limitations (i.e. trading off repeated reads vs available memory).</p>
<blockquote>
<ol start=""2"">
<li>a transform is applied each time an image is read from disk and that seems to me a nearly redundant action.</li>
</ol>
</blockquote>
<p>Transforms usually have randomness built-in as it helps prevent the net from overfitting. That's why we transform each time a sample is requested - we need a different random transformation each time. If the transformations were only applied a single time and re-used thereafter, it would defeat the purpose of random augmentation as the net could just learn the single static transformation.</p>
<blockquote>
<p>[...] in the case that all my data can be fit into memory, isn't reading it all from the disk in the <strong>init</strong> function a better approach?</p>
</blockquote>
<p>I think that would be worth trying if disk-to-RAM speed is a bottleneck in your pipeline. Pre-loading all the data once would result in a speed improvement in that case. Bear in mind that even though your data might fit in RAM, by the time it's going through the model, the RAM requirements will be higher due to the model's size and training gradients (there will be different RAM requirements at training vs inference, and CPU vs GPU).</p>
<blockquote>
<p>[...] why shouldn't we crop the images once and store it on the disk somewhere else and throughout training only read the cropped images?</p>
</blockquote>
<p>Random cropping is usually used to prevent the net from simply memorising the exact features of an image. The net is forced to generalise beyond the randomness to more robust and general properties. Cropping once and re-using the same image would mean the net could learn the single crop and not generalise as well to unseen data.</p>
","0","Answer"
"79362135","79361940","<p>Yes, PyTorch is set up to work with batches of samples exactly as you suggest. You can pass all 1000 samples in and you'll get 1000 samples out of your network, where, internally, each one of those 1000 samples will get passed through the network. Try it!</p>
<p>Take this simple network as an example:</p>
<pre class=""lang-py prettyprint-override""><code># linear network with 3 input parameters and 3 output parameters
network = torch.nn.Linear(3, 3, bias=False)

# input with 2 samples
inputs = torch.randn(2, 3)

outputs = network(inputs)
print(outputs)
tensor([[0.1277, 0.5881, 0.1048],
        [0.3140, 0.2438, 0.1175]], grad_fn=&lt;MmBackward0&gt;)

# which is equivalent to matrix multiplying the network weights
# by each input sample 
for sample in inputs:
    print(torch.matmul(sample, network.weight.T))
tensor([0.1277, 0.5881, 0.1048], grad_fn=&lt;SqueezeBackward4&gt;)
tensor([0.3140, 0.2438, 0.1175], grad_fn=&lt;SqueezeBackward4&gt;)
</code></pre>
<p>In general, in many cases of training a neural network training, you will have a very large number of training samples (hundreds of thousands, millions, etc!), and during each epoch you will want to pass all those samples through the network before calculating the overall loss. Often, that's not practical, and during each epoch you will have to pass through the samples in smaller batches (&quot;minibatches&quot;), calculate the loss on the output of each minibatch as an approximation of the overall loss, and optimize on that. If you only have 1000 samples, which are each only 3 parameters in size, then there's no issue in just passing them all through the network in one go during a training epoch.</p>
","1","Answer"
"79362540","79356354","<p>I implemented a few solutions. A few preliminaries:</p>
<ul>
<li><code>nonzero_static()</code> is unfortunately not compatible with cuda backend, which may be limiting for your use case</li>
<li><a href=""https://pytorch.org/docs/stable/generated/torch.vmap.html"" rel=""nofollow noreferrer""><code>vmap</code></a> will not likely work as it &quot;does not provide general autobatching or handle variable-length sequences out of the box.&quot; and creates a batched_tensor output. Running vmap on <code>nonzero_static</code> produces a warning <code>UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::nonzero_static. </code></li>
<li>In general, leaving the result in list-like form (i.e. two 1D tensors with row and column indices, respectively) is faster than putting these indices into a tensor of the original shape of the data, and sorting so that the useful indices are first adds a bit of additional time on top of that.</li>
<li>Takeaway from my very crude experiment was that for most reasonable sizes of tensor, vanilla <code>nonzero()</code> was the fastest or nearly as fast as the index-broadcasting solution. Seems that the unclear size of memory to be allocated is not in general a large bottleneck when compared to the relatively clunky workaround solutions. Would be interesting to re-evaluate if either <code>nonzero_static</code> was optimized for batched computation with <code>vmap</code> or CUDA backend was implemented for <code>nonzero_static</code> which hopefully will eventually happen as it's a relatively new function in pytorch.</li>
</ul>
<pre><code>import torch
import time
m = 2000
n = 1000
trials = 100

results = {}
for t in range(trials):
    
    device = torch.device(&quot;cpu&quot;)
    data = torch.rand([m,n],device = device).round().long()
    
    # use nonzero 
    name = &quot;nonzero&quot;
    t1 = time.time()
    idx = data.nonzero()
    midx = idx[:,0]
    nidx = idx[:,1]
    output = torch.zeros([m,n],device = device,dtype = torch.long)
    output[midx,nidx] = nidx 
    output = output.sort(dim = 1,descending = True)
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    
    # use nonzero_static and leave in &quot;listy&quot; form
    name = &quot;nonzero_static&quot;
    t1 = time.time()
    count_nonzero = int(data.sum().item())
    d = data.view(-1)
    idx = d.nonzero_static(size = count_nonzero)
    midx,nidx = idx//n, idx%n
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    # use nonzero_static and put in matrix form, leave unsorted
    name = &quot;nonzero_static -&gt; matrix&quot;
    t1 = time.time()
    count_nonzero = int(data.sum().item())
    d = data.view(-1)
    idx = d.nonzero_static(size = count_nonzero)
    midx,nidx = idx//n, idx%n
    output = torch.zeros([m,n],device = device,dtype = torch.long)
    output[midx,nidx] = nidx 
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    
    
    # use nonzero_static and put in matrix form, then sort
    name = &quot;nonzero_static -&gt; sorted matrix&quot;
    t1 = time.time()
    count_nonzero = int(data.sum().item())
    d = data.view(-1)
    idx = d.nonzero_static(size = count_nonzero)
    midx,nidx = idx//n, idx%n
    output = torch.zeros([m,n],device = device,dtype = torch.long)
    output[midx,nidx] = nidx 
    output = output.sort(dim = 1,descending = True)
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    
    # vmap nonzero_static
    name = &quot;vmap nonzero_static&quot;
    t1 = time.time()
    test = torch.func.vmap(torch.nonzero_static)
    output = test(data,size = n).squeeze(-1)
    torch.cuda.synchronize()
    torch.cuda.empty_cache()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    # use index broadcasting then sort
    name = &quot;index broadcasting&quot;
    t1 = time.time()
    index_array = torch.arange(n).unsqueeze(0).expand(m,n)
    output = data*index_array
    output = output.sort(dim = 1,descending = True)
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    
    
    device = torch.device(&quot;cuda:0&quot;)
    data = data.to(device)
    torch.cuda.synchronize()
    
    # use index broadcasting then sort on GPU
    name = &quot;GPU index broadcasting&quot;
    t1 = time.time()
    index_array = torch.arange(n,device = device).unsqueeze(0).expand(m,n)
    output = data*index_array
    output = output.sort(dim = 1,descending = True)
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
    
    del output
    torch.cuda.empty_cache()
    
    #use nonzero and leave in listy form
    name = &quot;GPU nonzero&quot;
    t1 = time.time()
    idx = data.nonzero()
    midx = idx[:,0]
    nidx = idx[:,1]
    output = torch.zeros([m,n],device = device,dtype = torch.long)
    output[midx,nidx] = nidx 
    output = output.sort(dim = 1,descending = True)
    
    torch.cuda.synchronize()
    try:
        results[name] += time.time()- t1
    except:
        results[name] = time.time() - t1
        
print(&quot;Results for [{},{}] over {} trials&quot;.format(m,n,trials))
for key in results:
    print(&quot;{:.5f}s for {}&quot;.format(results[key]/trials,key))
   

</code></pre>
<pre><code>Results for [200,100] over 100 trials
0.00051s for nonzero
0.00035s for nonzero_static
0.00037s for nonzero_static -&gt; matrix
0.00062s for nonzero_static -&gt; sorted matrix
0.00191s for vmap nonzero_static
0.00033s for index broadcasting
0.00015s for GPU index broadcasting
0.00019s for GPU nonzero
</code></pre>
<pre><code>Results for [2000,1000] over 100 trials
0.00575s for nonzero
0.01028s for nonzero_static
0.01036s for nonzero_static -&gt; matrix
0.01302s for nonzero_static -&gt; sorted matrix
0.03645s for vmap nonzero_static
0.00466s for index broadcasting
0.00129s for GPU index broadcasting
0.00198s for GPU nonzero
</code></pre>
<pre><code>Results for [20000,10000] over 20 trials
0.67861s for nonzero
1.10534s for nonzero_static
1.31800s for nonzero_static -&gt; matrix
1.66106s for nonzero_static -&gt; sorted matrix
2.68011s for vmap nonzero_static
0.55859s for index broadcasting
0.31346s for GPU index broadcasting
0.30350s for GPU nonzero
</code></pre>
","1","Answer"
"79366215","79365805","<p>The <code>loss</code> should explicitely require a gradient to be evaluated, so I would remove the <code>loss.requires_grad = True</code> line. Also, try to rewrite the first line</p>
<pre><code>def get_pedalboard(self):
    highpass_freq = self.highpass_freq.clamp(20, 500) 
</code></pre>
<p>Are you sure the</p>
<pre><code>board = Pedalboard([
            HighpassFilter(cutoff_frequency_hz=float(highpass_freq)),
        ])
</code></pre>
<p>works with PyTorch autograd?</p>
","0","Answer"
"79367782","79366420","<p>Meanwhile I resolved the issue with this function:</p>
<pre><code>import numpy as np 
def pil_to_tensor(images): 
    images = np.array(images) 
    images = torch.from_numpy(images.transpose(2, 0, 1)) 
    return images 
</code></pre>
<p>‍ ‍ ‍ ‍ ‍ ‍
And the two code blocks including saving the image can be deleted. Instead calling the function:</p>
<pre><code>pil_image = pil_to_tensor(im)
</code></pre>
","0","Answer"
"79369467","79365374","<p>You can use <code>torch.set_num_threads(int)</code> (<a href=""https://pytorch.org/docs/stable/generated/torch.set_num_threads.html"" rel=""nofollow noreferrer"">docs</a>) to control how many CPU processes pytorch uses to execute operations.</p>
","0","Answer"
"79369509","79366566","<p>Your <code>forecast</code> code doesn't use the hidden state from the previous time step, so the model isn't able to use any sequence information. You need to design your model to allow you to pass a hidden state to the <code>forward</code> method and use that hidden state during inference. Something like this:</p>
<pre class=""lang-py prettyprint-override""><code>class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, target_size):
        super(LSTMModel, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, target_size)  # Output layer for regression

    def forward(self, x, hidden=None):
        if hidden is None:
            hidden = self.get_hidden(x)
            
        lstm_out, hidden = self.lstm(x, hidden)
        out = self.fc(lstm_out[:, -1, :])  
        return out, hidden
    
    def get_hidden(self, x):
        # hidden state is of shape (n_layers, batch_size, d_hidden)
        # batch size is `x.shape[0]` for `batch_first=True`
        hidden = (
                torch.zeros(self.num_layers, x.shape[0], self.hidden_size, device=x.device),
                torch.zeros(self.num_layers, x.shape[0], self.hidden_size, device=x.device),
                )
        return hidden 
    
    def forecast(self, initial_input, num_steps):
        predictions = []  # Store forecasted TMP values
        current_input = initial_input.clone()  # Clone to avoid modifying the original input
        hidden = None # initial hidden state

        with torch.no_grad():
            for _ in range(num_steps):
                # use and update hidden state for forward pass
                next_output, hidden = self.forward(current_input, hidden) 
                current_input = next_output.unsqueeze(1)
                predictions.append(current_input)

        return torch.cat(predictions, dim=1)  # Concatenate predictions along the time dimension
</code></pre>
","1","Answer"
"79372438","79358039","<p>As mentioned in the comments, <code>torch.norm</code> is depreciated in favor of more explicit functions <code>torch.linalg.vector_norm</code>, <code>torch.linalg.matrix_norm</code> and <code>torch.linalg.norm</code>. This is to make more explicit how the different operations are flattening or preserving dimensions of the input. You can read the documentation (<a href=""https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm"" rel=""nofollow noreferrer"">link</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm"" rel=""nofollow noreferrer"">link</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm"" rel=""nofollow noreferrer"">link</a>) for more details.</p>
<p>For your question about the NaN, I doubt that is being caused by the norm operation. When the <code>x-y</code> difference is small, both <code>torch.linalg.vector_norm(x-y)</code> and <code>torch.norm(x-y)</code> will round to zero instead of producing a NaN.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(64)
for scale in range(10):
    y = x - (10**(-scale)) * x
    print(torch.linalg.vector_norm(x-y))
    
&gt; tensor(7.8801)
&gt; tensor(0.7880)
&gt; tensor(0.0788)
&gt; tensor(0.0079)
&gt; tensor(0.0008)
&gt; tensor(7.8811e-05)
&gt; tensor(7.8175e-06)
&gt; tensor(7.6573e-07)
&gt; tensor(0.)
&gt; tensor(0.)
</code></pre>
<p>If you are doing something like dividing a value by the result of the norm operation, that may produce a NaN from division by zero.</p>
","1","Answer"
"79406616","79355662","<p>It would be nice to have some code to try to replicate the issue.</p>
<p>I have two possible guesses for you:</p>
<p>It could be some interactions between the torch.multiprocess and the python futures, this could be happening because you have not pass any context (mp_context) Wich defaults to the multiprocess context when creating the pool. This might be breaking torch spawning. Try to set the context to the context of torch, which is returned by the spawn() call or you can do</p>
<pre><code>ctx = pmp.get_context(&quot;spawn&quot;)
</code></pre>
<p>At the cost of performance, try to limit the pytprch threads <code>set_num_thread</code> to 1 or 2, same thing with the pool. When doing this monitor the memory usage,</p>
<p>I think that either the copy process of the python multiprocess is making internal torch values not change and continue to fork until the bomb. Or since it doesn't have access to spawn context for his queue the tensors are being pickled and your memory is exploding, killing some of the processes, and python multiprocess es doesn't kill other processes when one dies (I believe), torch does, so that might be causing the bomb/memory overload.</p>
<p>Updates with any results, and maybe some code?</p>
","0","Answer"
"79416575","79355967","<p>This is the expected behavior. The library can't freeze the layers for you. You can freeze them yourself by setting <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"" rel=""nofollow noreferrer"">requires_grad</a> to <code>False</code> for certain layers as shown below:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSemanticSegmentation

model_name = &quot;nvidia/segformer-b5-finetuned-cityscapes-1024-1024&quot;
model = AutoModelForSemanticSegmentation.from_pretrained(model_name)

print_trainable_parameters(model)
# freezing everything except the decoder head
for name, param in model.named_parameters():
    if not name.startswith(&quot;decode_head&quot;): 
      param.requires_grad = False
print_trainable_parameters(model)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>trainable params: 84607955 || all params: 84607955 || trainable%: 100.00
trainable params: 3164947 || all params: 84607955 || trainable%: 3.74
</code></pre>
","1","Answer"
"79367461","79367460","<p>This happens because of the numpy version &gt; 1.24</p>
<pre><code>% pip list | grep numpy
numpy                     1.26.4
</code></pre>
<p>Let's downgrade the numpy version</p>
<pre><code>% pip install &quot;numpy&lt;1.24&quot; 
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 1.26.4
    Uninstalling numpy-1.26.4:
      Successfully uninstalled numpy-1.26.4
Successfully installed numpy-1.23.5
</code></pre>
<p>If we rerun our code, it will convert the tensor to numpy array properly</p>
<pre><code>&gt;&gt;&gt; a = a.numpy()
&gt;&gt;&gt; a
array([[1, 2, 3],
       [2, 3, 4],
       [3, 4, 5]])
</code></pre>
","1","Answer"
"79369398","79369359","<p>Looks like <code>.rsample()</code> is doing the trick here, which is keeping the computational graph alive...</p>
","1","Answer"
"79369530","79368015","<blockquote>
<p>“So does <code>torch.asarray()</code> just offer more capabilities than torch.as_tensor()?”</p>
</blockquote>
<p>Yes that's basically it.</p>
<p><code>torch.as_tensor</code> automatically tries to copy data and autograd information, while <code>torch.asarray</code> gives you more explicit control over data copying and autograd information.</p>
<p>If you want shared memory/autograd by default, I would just use <code>as_tensor</code>. To my knowledge there is no performance difference between the two provided the same memory/autograd sharing parameters are used.</p>
","0","Answer"
"79369616","79368015","<p>To give a little more context, I think the real &quot;difference&quot; between <code>torch.asarray()</code> and <code>torch.as_tensor()</code> is that the former is part of the standard <a href=""https://data-apis.org/array-api/latest/API_specification/generated/array_api.asarray.html"" rel=""nofollow noreferrer"">Array API</a> while the latter is not.  So in other words, it's not that these two functions are meant for different use-cases, it's that there's a standard and non-standard way to do the same thing.</p>
","1","Answer"
"79369869","79367460","<p>If you are really bothered by the version issue but not going to make any changes on the packages, probably you can detour a bit to achieve the goal, e.g.,</p>
<pre><code>import numpy as np
np.array(a.tolist())
</code></pre>
<p>and then you will obtain</p>
<pre><code>array([[1, 2, 3],
       [2, 3, 4],
       [3, 4, 5]])
</code></pre>
","1","Answer"
"79370650","79369085","<p>Yes, the order of transformations matters. In this case, the transform to tensors makes the difference. When <code>v2.RandomHorizontalFlip</code> is given two tensors, the flip will be applied independently. However, when two PIL images are given, the same transform will be applied to both images, thus keeping the image and mask aligned.</p>
<p>For a more consistent handling, you can try using <code>TVTensors</code> for the data augmentation. Using these, you can specify the type of each data input before transforming them. For example:</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import tv_tensors

img_tensor = tv_tensors.Image(img_tensor)
mask_tensor= tv_tensors.Mask(mask_tensor)
</code></pre>
","0","Answer"
"79378454","79378344","<p>It looks like an Exception happened when trying to execute the following code:</p>
<pre class=""lang-py prettyprint-override""><code>pdb.set_trace()
loss, acc, bsz = model(batch[&quot;eq_tokens&quot;], batch[&quot;wd_tokens&quot;], batch[&quot;tgt_processed_tokens&quot;], args.label_smoothing)
</code></pre>
<p>which caused <code>loss</code> to be set to <code>None</code> as per your line 265:</p>
<pre class=""lang-py prettyprint-override""><code>loss = acc = bsz = None
</code></pre>
<p>Hence, when you tried to call <code>loss.item()</code>, your code raises an exception as <code>loss</code> is <code>None</code>.</p>
<p>As to why you get this Exception, you should try to output the trace of the error when you catch it in line 264.</p>
<p>Please also note that calling <code>tensor.item()</code> during the forward pass is considered bad practice because it will cause unnecessary CPU/GPU transfers and slow down the training loop. Consider using <code>tensor.detach()</code> and/or a proper metric tracker like the ones provided by <code>torchmetrics</code>.</p>
","1","Answer"
"79378670","79378157","<p>The ORT model output is just the default string that represents an object in Python, providing the class name and then the memory address. They are both valid objects in Python but the first model overrides the <code>__str__()</code> method to show the layers when the model is printed.</p>
<p>The ORT model doesn't have the same methods, like the <code>__str__()</code> override, as the whisper model because it is not a PyTorch-based model. Instead, it uses ONNX graph definitions and operators. You can use a tool like Netron. You can try to print the encoder and decoder graphs, but it is not very readable.</p>
<pre class=""lang-py prettyprint-override""><code>import onnx

# Encoder model
encoder_onnx_model = onnx.load(model.encoder_model_path)
print(onnx.helper.printable_graph(encoder_onnx_model.graph))

# Decoder model
decoder_onnx_model = onnx.load(model.decoder_model_path)
print(onnx.helper.printable_graph(decoder_onnx_model.graph))
</code></pre>
","1","Answer"
"79382883","79382816","<p>It seems like you're looking for <a href=""https://pytorch.org/docs/stable/generated/torch.einsum.html"" rel=""nofollow noreferrer"">einsum</a>.</p>
<p>Should be something like:</p>
<pre class=""lang-py prettyprint-override""><code>result = torch.einsum('bi,bijk,bk-&gt;bj', x, second_order_derivative, x)
</code></pre>
","2","Answer"
"79383640","79383301","<p>According to <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList"" rel=""nofollow noreferrer"">PyTorch's implementation</a>, you cannot directly call <code>linears[ind]</code> when <code>ind</code> is neither an <code>int</code> nor a <code>slice</code>.</p>
<p>What you can do instead is:</p>
<pre class=""lang-py prettyprint-override""><code>out = input
for idx in ind:
    out = linears[idx](out)
</code></pre>
","1","Answer"
"79384111","79382515","<p>Have you tried using mixed precision?</p>
<p>You can usually set it using <code>precision=&quot;16-mixed&quot;</code> in a Lightning trainer. <code>anomalib</code> seem to have <a href=""https://anomalib.readthedocs.io/en/latest/markdown/guides/reference/deploy/index.html"" rel=""nofollow noreferrer"">implemented a way to use it during deployment</a>.</p>
","0","Answer"
"79384318","79380954","<blockquote>
<p>Why does loss.backward() cause such a significant memory increase in GPU which there is a matrix on it ?</p>
</blockquote>
<p>When you call <code>loss.backward()</code>, all the gradients of your network are computed. There is one gradient per tensor of parameter. Basically, this multiplies by two the memory usage.</p>
<blockquote>
<p>How can I optimize this setup to run on GPUs without running out of memory? Are there specific strategies or PyTorch functionalities that can reduce GPU memory usage during the backward pass?</p>
</blockquote>
<p>You could try using <a href=""https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html"" rel=""nofollow noreferrer"">mixed-precision training</a>, reduce the dimensions of your input, use SGD instead of Adam, use a network with fewer FLOPs (for instance one that downsamples your input quite early), or use a larger GPU.</p>
","0","Answer"
"79385246","79383301","<p>The <code>ind</code> tensor is of size <code>(bs, n_decisions)</code>, which means we're choosing a different set of experts for each item in the batch.</p>
<p>With this setup, the most efficient way to compute the output is to compute all experts for all batch items, then gather the desired choices after. This will be more performant in GPU compared to looping over the individual experts.</p>
<p>Since we're looking at a linear layer, you can compute all the experts using a single linear layer of size <code>n_experts * dim</code>.</p>
<pre class=""lang-py prettyprint-override""><code>d_in = 768
n_experts = 10
bs = 32
n_choice = 4

# Create a single large linear layer
fused_linear = nn.Linear(d_in, d_in * n_experts)

indices = torch.randint(0, n_experts, (bs, n_choice))
x = torch.randn(bs, d_in)

# Forward pass through the fused layer
y = fused_linear(x)  # Shape: [bs, d_in * n_experts]

# Reshape to separate the experts dimension
ys = y.reshape(bs, n_experts, d_in)  # Shape: [bs, n_experts, d_in]

# Gather the chosen experts
ys = torch.gather(ys, 1, indices.unsqueeze(-1).expand(-1, -1, d_in))
</code></pre>
<p>The output <code>ys</code> will be of shape <code>(bs, n_choice, d_in)</code></p>
","2","Answer"
"79386431","79386418","<p>It appears that the &quot;torch&quot; folder, from which you are trying to import the <code>torch.utils</code> module, is being overshadowed by the PyTorch package (<code>import torch</code>) because of the identical name. Try renaming the &quot;torch&quot; folder to something different, to avoid using the reserved name used by the PyTorch package.</p>
","1","Answer"
"79389444","79389115","<p>This should work:</p>
<pre class=""lang-py prettyprint-override""><code>small_tensor[big_tensor]
</code></pre>
<p>Take note that the type of the <code>big_tensor</code> must be long/int.</p>
<hr />
<p>Edit:</p>
<p>In response to the comment of @simon, I wrote a <a href=""https://colab.research.google.com/drive/1Wl7jFs3oWo4sMPw1r-K9VPFHoazYSqez?usp=sharing"" rel=""nofollow noreferrer"">colab notebook</a> that shows how this solution works without the need to perform any other operation.</p>
","2","Answer"
"79397282","79396894","<p>Since you only seem to be interested in the <a href=""https://en.wikipedia.org/wiki/Haar_wavelet"" rel=""nofollow noreferrer"">Haar wavelet</a>, you can pretty much implement it yourself:</p>
<ul>
<li>The high-frequency component of the Haar wavelet along each dimension can be written as a pairwise difference.</li>
<li>The low-frequency component of the Haar wavelet along each dimension can be written as a pairwise sum.</li>
</ul>
<p>The following code achieves this in pure PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code>class HaarWaveletLayer(nn.Module):
    
    def l_0(self, t):  # sum (&quot;low&quot;) along cols
        t = torch.cat([t, t[..., -1:, :]], dim=-2) if t.shape[-2] % 2 else t
        return (t[..., ::2, :] + t[..., 1::2, :])
    def l_1(self, t):  # sum (&quot;low&quot;) along rows
        t = torch.cat([t, t[..., :, -1:]], dim=-1) if t.shape[-1] % 2 else t
        return (t[..., :, ::2] + t[..., :, 1::2])
    def h_0(self, t):  # diff (&quot;hi&quot;) along cols
        t = torch.cat([t, t[..., -1:, :]], dim=-2) if t.shape[-2] % 2 else t
        return (t[..., ::2, :] - t[..., 1::2, :])
    def h_1(self, t):  # diff (&quot;hi&quot;) along rows
        t = torch.cat([t, t[..., :, -1:]], dim=-1) if t.shape[-1] % 2 else t
        return (t[..., :, ::2] - t[..., :, 1::2])
    
    def forward(self, x):
        
        x = .5 * x
        l_1 = self.l_1(x)
        h_1 = self.h_1(x)
        ll = self.l_0(l_1)
        lh = self.h_0(l_1)
        hl = self.l_0(h_1)
        hh = self.h_0(h_1)
        
        return torch.cat([ll, lh, hl, hh], dim=1)
</code></pre>
<p>In combination with your given code, you can convince yourself of the equivalence as follows:</p>
<pre class=""lang-py prettyprint-override""><code>t = torch.rand((7, 3, 127, 128)).to(&quot;cuda:0&quot;)
result_given = WaveletLayer()(t)
result_proposed = HaarWaveletLayer()(t)

# Same result?
assert (result_given - result_proposed).abs().max() &lt; 1e-5

# Time comparison
from timeit import Timer
num_timings = 100
print(&quot;time given:   &quot;, Timer(lambda: WaveletLayer()(t)).timeit(num_timings))
print(&quot;time proposed:&quot;, Timer(lambda: HaarWaveletLayer()(t)).timeit(num_timings))
</code></pre>
<p>The timing shows a speedup of more than a factor of 10 on my machine.</p>
<h2>Notes</h2>
<ul>
<li>The <code>t = torch.cat...</code> parts are only necessary if you want to be able to handle odd-shaped images: In that case, we pad by replicating the last row and column, respectively, mimicking the default padding of PyWavelets.</li>
<li>Multiplying <code>x</code> with .5 is done for normalization. Compare <a href=""https://dsp.stackexchange.com/questions/1739/"">this discussion</a> on the Signal Processing Stack Exchange for more details.</li>
</ul>
","4","Answer"
"79398172","79397701","<p>There is no version 2.5.1+cpu of torch available for macos.</p>
","1","Answer"
"79400010","79399217","<p>Very short answer: No, batch normalization cannot be considered a linear transformation.</p>
<p>Short answer: You have to be careful in assuming linearity for your described model, but not only for the reason that you mentioned. In short,</p>
<ul>
<li>a <strong>batch norm</strong> layer is generally not linear by definition,</li>
<li>enabling <strong>bias terms</strong> for either of your layers will likewise break the linearity property.</li>
</ul>
<h2>Linear functions</h2>
<p>Combining the criteria of <a href=""https://en.wikipedia.org/wiki/Linearity#Linear_maps"" rel=""nofollow noreferrer"">additivity and homogeneity</a>, we can call a function <em>f</em> linear if and only if it fulfills the following criterion:</p>
<p><em>f(α·x + β·y) = α·f(x) + β·f(y)</em>.</p>
<h2>Conditional linearity of the fully connected layer</h2>
<ul>
<li><p>A fully connected layer <em>l</em> <strong>without</strong> a <strong>bias</strong> term (e.g. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"" rel=""nofollow noreferrer""><code>torch.nn.Linear(…, bias=False)</code></a>) is defined as</p>
<p><em>l(x) = xW<sup>T</sup></em>.</p>
<p>It is <strong>linear</strong>, since</p>
<p><em>l(α·x + β·y) = (α·x + β·y)W<sup>T</sup> = α·xW<sup>T</sup> + β·yW<sup>T</sup> = α·l(x) + β·l(y).</em></p>
</li>
<li><p>A fully connected layer <em>l</em> <strong>with</strong> a <strong>bias</strong> term (e.g. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"" rel=""nofollow noreferrer""><code>torch.nn.Linear(…, bias=True)</code></a>) is defined as</p>
<p><em>l(x) = xW<sup>T</sup> + b</em>.</p>
<p>It is <strong>not linear</strong>, since</p>
<p><em>l(α·x + β·y) = (α·x + β·y)W<sup>T</sup> + <strong>b</strong></em>,</p>
<p>while</p>
<p><em>α·l(x) + β·l(y) = α·(xW<sup>T</sup> + b) + β·(yW<sup>T</sup> + b) = (α·x + β·y)W<sup>T</sup> + <strong>(α + β)·b</strong></em>.</p>
</li>
</ul>
<p>This might be confusing, since the very name of the fully connected layer, <code>Linear</code>, implies differently. However, its name stems from a <a href=""https://en.wikipedia.org/wiki/Linearity#Linear_polynomials"" rel=""nofollow noreferrer"">definition of &quot;linear&quot;</a> that is different from the one we use here.</p>
<h2>Nonlinearity of the batch norm layer</h2>
<p>We can similarly examine the batch norm layer, and we will find that even without its affine parameters, and thus without a bias term (e.g. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"" rel=""nofollow noreferrer""><code>torch.nn.BatchNorm1d(…, affine=False)</code></a>), it is not linear:</p>
<p>The batch norm layer <em>n</em> without its affine parameters (i.e. without the learned scaling parameters that are called <em>β</em> and <em>γ</em> in the <a href=""https://en.wikipedia.org/wiki/Batch_normalization"" rel=""nofollow noreferrer"">Wikipedia article on batch normalization</a>) is defined as</p>
<p><em>n(x) = (x - μ) / σ</em>.</p>
<p>It is <strong>not linear</strong> since</p>
<p><em>n(α·x + β·y) = (α·x + β·y - <strong>μ</strong>) / σ</em>,</p>
<p>while</p>
<p><em>α·n(x) + β·n(y) = α·(x - μ) / σ + β·(y - μ) / σ = (α·x + β·y - <strong>(α + β)·μ</strong>) / σ</em>.</p>
<p>The same argument applies with the affine parameters enabled (e.g. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"" rel=""nofollow noreferrer""><code>torch.nn.BatchNorm1d(…, affine=True)</code></a>).</p>
<h2>Sanity check</h2>
<p>Here is a basic example that demonstrates the results from above in code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

torch.manual_seed(42)
batch_size, input_size, output_size = 4, 3, 1
batch_x, batch_y = (torch.randn(batch_size, input_size) for _ in range(2))

class Model(torch.nn.Module):
    
    def __init__(self, input_size, output_size, use_bias, use_bn_mean):
        super().__init__()
        self.use_bias = use_bias
        self.use_bn_mean = use_bn_mean
        self.fc = torch.nn.Linear(input_size, output_size, bias=use_bias)
        self.bn = torch.nn.BatchNorm1d(output_size)
        # Init mean / var with non-identity values (i.e mean≠0, var≠1)
        torch.nn.init.constant_(self.bn.running_mean, 4.2 if use_bn_mean else 0.)
        torch.nn.init.constant_(self.bn.running_var, 1.5)
        # Init affine params with non-identity values (i.e bias≠0, weight≠1)
        torch.nn.init.constant_(self.bn.bias, 1.3 if use_bias else 0.)
        torch.nn.init.constant_(self.bn.weight, 2.5)
        
    def forward(self, x): return self.bn(self.fc(x))

with torch.no_grad():
    for use_bias in (True, False):
        for use_bn_mean in (True, False):
            model = Model(input_size, output_size, use_bias, use_bn_mean)
            model.eval()
            out_sum_before = model(.5 * batch_x + .3 * batch_y)
            out_sum_after = .5 * model(batch_x) + .3 * model(batch_y)
            is_linear = (out_sum_after - out_sum_before).abs().max() &lt; 1e-5
            print(f&quot;Model is {'' if is_linear else 'not '}linear for &quot;
                  f&quot;{model.use_bias=}, {model.use_bn_mean=}.&quot;)
</code></pre>
<p>This prints:</p>
<pre class=""lang-none prettyprint-override""><code>Model is not linear for model.use_bias=True, model.use_bn_mean=True.
Model is not linear for model.use_bias=True, model.use_bn_mean=False.
Model is not linear for model.use_bias=False, model.use_bn_mean=True.
Model is linear for model.use_bias=False, model.use_bn_mean=False.
</code></pre>
<p>That is: only if we suppress all bias terms, as well as the mean of the batch norm (which, I guess, defies the purpose of using batch normalization), our network will behave as a linear function. Also, this is only true during <em>inference</em>, when the batch norm layer uses the same, global standard deviation value for all batches.</p>
","2","Answer"
"79400920","79400482","<p>If you convert your weights dict into a tensor, you can index directly</p>
<pre class=""lang-py prettyprint-override""><code>t = torch.tensor([1, 0, 0, 1])
weights = torch.tensor([0.1, 0.9])

new_t = weights[t]
new_t
&gt;tensor([0.9000, 0.1000, 0.1000, 0.9000])
</code></pre>
","1","Answer"
"79409575","79409259","<p>Generally speaking Hydra is independent of PyTorch and does not directly interact with (except via plugins).
<code>_partial_</code> has nothing at all to do with PyTorch or seeding.</p>
<p>At a glance what you are suggesting should work, but it's best if you just verify it.</p>
","1","Answer"
"79410732","79409259","<p>Your understanding is correct.Using <em>partial</em> in Hydra simply returns <code>functools.partial</code> object and doesnt immediately execute class constructor or otherwise &quot;bake in&quot; seed.As result seed is not fixed at the time of creating that partial.You can safely call <code>torch.manual_seed(...)</code> or any other seed-setting functions just before you invoke the partial object multiple times for reproducible runs.</p>
<p>A common pattern is something like:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from hydra import compose, initialize

# For example your Hydra config defines partial for your model
with initialize(config_path=&quot;conf&quot;,version_base=None):
    cfg =compose(config_name=&quot;config&quot;)

model_partial=cfg.model  # A functools.partial(MyModel, ...)

# Then in your experiment loop :
for seed in [123, 456, 789]:
    torch.manual_seed(seed)
    model = model_partial()  # Actually instantiate model with given seed
    # train or evaluate your model
</code></pre>
","0","Answer"
"79410838","79410822","<p>As you suspected, the list is indeed not part of the computational graph. The fact that you hold the input or output tensor of an arithmetic operation in a list, dict or any other data structure is irrelevant. Every time a tensor is involved in a derivable operation (e.g. multiplication, addition, or even concatenation), the result has a reference to the location in the computational graph that is built by the operation.</p>
<p>In the examples you provided, note that later the tensors <em>inside</em> the list are used in the arithmetic ops, not the list that contains it.</p>
<p>For background, you may find it interesting to read a bit about how computational graphs are built.</p>
","0","Answer"
"79411171","79409259","<p>Before using <code>hydra.utils.instantiate</code> no third party code is not run by hydra. So you can set your seeds before each use of instantiate; or if a <code>partial</code> before each call to the partial.</p>
<p>Here a complete toy example, based on Hydra's <a href=""https://hydra.cc/docs/advanced/instantiate_objects/overview/"" rel=""nofollow noreferrer"">doc overview</a>, which creates a partial to instantiate an optimizer or a model, that takes a callable <code>optim_partial</code> as an argument.</p>
<pre class=""lang-yaml prettyprint-override""><code># config.yaml
model:
  _target_: &quot;__main.__.MyModel&quot;
  optim_partial:
    _partial_: true
    _target_: __main__.MyOptimizer
    algo: SGD
  lr: 0.01
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from functools import partial
from typing import Callable
import random
from pprint import pprint

import hydra
from omegaconf import DictConfig, OmegaConf


class MyModel:
    def __init__(self, lr, optim_partial: Callable[..., &quot;MyOptimizer&quot;]):
        self.optim_partial = optim_partial
        self.optim1 = self.optim_partial()
        self.optim2 = self.optim_partial()


class MyOptimizer:
    def __init__(self, algo):
        print(algo, random.randint(0, 10000))


@hydra.main(config_name=&quot;config&quot;, config_path=&quot;./&quot;, version_base=None)
def main(cfg: DictConfig):
    # Check out the config
    pprint(OmegaConf.to_container(cfg, resolve=False))
    print(type(cfg.model.optim_partial))
    
    # Create the functools.partial
    optim_partial: partial[MyOptimizer] = hydra.utils.instantiate(cfg.model.optim_partial)
    # Set the seed before you call the a partial
    random.seed(42)
    optimizer1: MyOptimizer = optim_partial()
    optimizer2: MyOptimizer = optim_partial()
    random.seed(42)
    optimizer1b: MyOptimizer = optim_partial()
    optimizer2b: MyOptimizer = optim_partial()

    # model is not a partial; use seed before creation
    random.seed(42)
    model: MyModel = hydra.utils.instantiate(cfg.model)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Output
{'model': {'_target_': '__main__.MyModel',
           'lr': 0.01,
           'optim_partial': {'_partial_': True,
                             '_target_': '__main__.MyOptimizer',
                             'algo': 'SGD'}}}
type of cfg.model.optim_partial &lt;class 'omegaconf.dictconfig.DictConfig'&gt;
SGD 1824
SGD 409
SGD 1824
SGD 409
SGD 1824
SGD 409
</code></pre>
","1","Answer"
"79411663","79411478","<p>First a few remarks that can help the analysis in general:</p>
<ul>
<li>use same scales for both methods</li>
<li>use logarithmic y axis for the loss, to have a better idea of the trend</li>
</ul>
<p>From the two curves I would say that you should consider:</p>
<ul>
<li>the gap between training and validation: you &quot;simple transform&quot; has a big gap, which suggests that your training dataset does not represent well all the data (it could also be overfitting, but usually overfitting is identified by an increasing validation loss)</li>
<li>the loss curve: the &quot;simple transform&quot; does not seem to improve after 25000 steps, while the &quot;augmentation&quot; case could still improve (the loss isn't flat at the end)</li>
<li>the trend in the loss curve: in a (x, log(y)) graph, it is easy to interpret the convergence rate (exponential, quadratic, ...). Here your augmentation seems to have a different convergence rate (not just a different factor), so it might take more epochs just to reach the same loss level.</li>
<li>of course the value of the accuracy, even though it does not really say why a model performs better, of what could be improved</li>
</ul>
<p>Finally, there are a lot of other metrics you could have a look at, to diagnose your models: true positive, true negative, precision, recall, f1-score, area under ROC curve... <a href=""https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/"" rel=""nofollow noreferrer"">Here</a> is a link to one of the numerous websites explaining these.</p>
","1","Answer"
"79416544","79389502","<p>The code you are running also calculates the gradients, which already doubles the memory requirements for the object returned by the model. Wrap your forward pass in <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.inference_mode.html"" rel=""nofollow noreferrer"">inference_mode</a> to disable gradient calculation:</p>
<pre class=""lang-py prettyprint-override""><code>with torch.inference_mode():
    output = model(**inputs)
</code></pre>
","1","Answer"
"79430422","79409149","<p>Mathematically, the formula for Linear regression is given by</p>
<p>(y = X.W<sup>T</sup> + b )</p>
<p>here,</p>
<ul>
<li>Input tensor, <code>X</code>, has shape (batch_size, in_features)</li>
<li>Weight matrix, <code>W</code> has shape (out_features, in_features)</li>
<li>Bias, <code>B</code> is a vector of shape (out_features).</li>
</ul>
<p>When you make <code>X</code> in 1D (e.g., [50] in your case), matrix multiplication is undefined because the dimensions don't align. Therefore, to ensure correct computation input data (<code>X</code>) should be 2 dimensional, where:</p>
<ul>
<li>each row represents a sample</li>
<li>and each column represents a feature.</li>
</ul>
","0","Answer"
"79435768","79372090","<p>Well, I (and DeepSeek) found a solution for this problem, I hope it will be helpful for someone else.</p>
<pre><code>def stable_implementation(A, B):
   log_S_A = torch.logsumexp(A, dim=1)  # Shape: (bs, m, m)
   log_S_B = torch.logsumexp(B, dim=1)  # Shape: (bs, m, m)
   combined = log_S_A.unsqueeze(3) + log_S_B.unsqueeze(1)  # Shape: (bs, m, m, m)
   out = torch.logsumexp(combined, dim=2)  # Shape: (bs, m, m)
   return out
</code></pre>
","0","Answer"
"79456422","79401951","<p>I can propose this solution - load the onnx model and restore the layers in it, not in the runtime. For some reason, it doesn't work properly. Also, I modified the assertion as the previous one didn't work properly. Upvote if it helped and also leave a comment or another answer if you have another approach</p>
<pre><code>import onnx  # onnx==&quot;1.17.0&quot;
import torch  # torch==&quot;2.3.0&quot;
from onnx import numpy_helper
import numpy as np  # numpy==1.26.4
import torchvision.models as models  # torchvision==0.18.0
from torchvision.models import ResNet18_Weights

change_layer = True
restore_layer = True
no_changed_layers = 20

# Load the pre-trained ResNet18 model
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
model.eval()

# Generate random input data
input_data = torch.randn(1, 3, 224, 224)

# Export the model to ONNX format
torch.onnx.export(model, input_data, &quot;some_model.onnx&quot;, verbose=False)

# Load the ONNX model
MODEL_PATH = &quot;some_model.onnx&quot;
_model = onnx.load(MODEL_PATH)
INTIALIZERS = _model.graph.initializer

# Store weights and initializer names
Weight = {}
initializer_num_name = {}
for num, initializer in enumerate(INTIALIZERS):
    W = numpy_helper.to_array(initializer)
    Weight[num] = W
    initializer_num_name[num] = initializer.name

# Change layers if required
Weight_num_name = {}
if change_layer:
    for weight_zeros in range(0, no_changed_layers):
        old_weight = numpy_helper.to_array(INTIALIZERS[weight_zeros])
        new_weight = np.zeros_like(Weight[weight_zeros])
        updated_weight = numpy_helper.from_array(new_weight)

        updating_weight_name = _model.graph.initializer[weight_zeros].name
        updated_weight.name = updating_weight_name
        Weight_num_name[weight_zeros] = updating_weight_name
        _model.graph.initializer[weight_zeros].CopyFrom(updated_weight)

# Save the modified ONNX model
onnx.save(_model, &quot;model.onnx&quot;)

# Load the modified ONNX model with ONNX Runtime
import onnxruntime

if restore_layer:
    # options = onnxruntime.SessionOptions()
    # ortvalue_initializers = []
    # for num in range(0, no_changed_layers):
    #     initializer = onnxruntime.OrtValue.ortvalue_from_numpy(Weight[num])
    #     ortvalue_initializers.append(initializer)
    #     options.add_initializer(Weight_num_name[num], initializer)
    #
    # onnx_session3 = onnxruntime.InferenceSession(&quot;model.onnx&quot;, sess_options=options, providers=[&quot;CPUExecutionProvider&quot;])

    # Load modified model
    with open(&quot;model.onnx&quot;, &quot;rb&quot;) as f:
        modified_onnx_bytes = f.read()

    modified_model = onnx.load_from_string(modified_onnx_bytes)

    # Replace weights in modified model
    for num, init in enumerate(modified_model.graph.initializer):
        if num&lt;=no_changed_layers:
            init.CopyFrom(numpy_helper.from_array(Weight[num], initializer_num_name[num]))

    # Create inference session with restored model
    restored_model_bytes = modified_model.SerializeToString()
    onnx_session3 = onnxruntime.InferenceSession(restored_model_bytes, providers=['CPUExecutionProvider'])


else:
    onnx_session3 = onnxruntime.InferenceSession(&quot;model.onnx&quot;, providers=[&quot;CPUExecutionProvider&quot;])

# Load the original ONNX model
onnx_session = onnxruntime.InferenceSession(MODEL_PATH)
input = onnx_session.get_inputs()[0]

# Generate dummy input data
generated_dummy_input_data = torch.randn(size=(input.shape[0], input.shape[1], input.shape[2], input.shape[3]))

# Run inference on the original model
onnx_inputs = {input.name: generated_dummy_input_data.numpy()}
onnx_output = onnx_session.run(None, onnx_inputs)

# Run inference on the modified model
onnx_inputs3 = {onnx_session3.get_inputs()[0].name: generated_dummy_input_data.numpy()}
onnx_output3 = onnx_session3.run(None, onnx_inputs3)

# Compare the outputs
onnx_model_predictions = onnx_output[0][0]
onnx_model_predictions3 = onnx_output3[0][0]
diff = np.abs(onnx_model_predictions - onnx_model_predictions3)

# Assert that the outputs are close
assert np.allclose(onnx_model_predictions, onnx_model_predictions3, atol=1e-2),  &quot;Big difference between logits of an original model and onnx one: \n%s\n%s&quot;%(onnx_model_predictions[:50],
                                                                                                                                                              onnx_model_predictions3[:50])
print(&quot;all good&quot;)
</code></pre>
","2","Answer"
"79515052","79373190","<p>I have you same problem ,then I try  the command <code>pip install albucore==0.0.16</code> ,finally it success!</p>
","0","Answer"
"79537449","79403409","<p>What you're trying to achieve is input masking. You can use separate fully connected layers (different people) for each output after running the LSTM once to capture all the information (one photo), instead of running the LSTM separately for each output (like taking multiple photos for different people).</p>
<p>You can mask or zero out portions of the LSTM's hidden state before sending it to the appropriate output layer if you want to make sure that particular inputs don't affect particular outputs. By running the LSTM just once, you can accomplish the same goal much more effectively.</p>
<p>I hope that helps you</p>
<p>Edit:</p>
<p>ok here's the modification needed base on your code:</p>
<pre><code>import torch
import torch.nn as nn

class LstmModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LstmModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        
        # Fully connected layers for different outputs
        self.fc1 = nn.Linear(hidden_size, output_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

        # Create masks to remove certain inputs' effects
        self.mask1 = nn.Parameter(torch.tensor([1, 1, 0, 1, 1, 0, 0, 0], dtype=torch.float32), requires_grad=False)  # Ignore input3 for output1, padded with 0's
        self.mask2 = nn.Parameter(torch.tensor([1, 0, 1, 1, 1, 0, 0, 0], dtype=torch.float32), requires_grad=False)  # Ignore input2 for output2, padded with 0's
        self.mask3 = nn.Parameter(torch.tensor([1, 1, 1, 0, 1, 0, 0, 0], dtype=torch.float32), requires_grad=False)  # Ignore input4 for output3, padded with 0's


    def forward(self, x):
        batch_size, seq_length, input_size = x.shape
        
        # LSTM Forward pass
        _, (hn, _) = self.lstm(x)  # hn shape: (num_layers, batch, hidden_size)
        hidden = hn[-1]  # Get the final hidden state (batch, hidden_size)

        # Apply masks by element-wise multiplication with the hidden state
        hidden_masked1 = hidden * self.mask1  # Apply mask to hidden state for output1
        hidden_masked2 = hidden * self.mask2  # Apply mask for output2
        hidden_masked3 = hidden * self.mask3  # Apply mask for output3

        # Generate outputs
        output1 = self.fc1(hidden_masked1)
        output2 = self.fc2(hidden_masked2)
        output3 = self.fc3(hidden_masked3)

        return output1, output2, output3


# === TESTING THE MODEL ===
# Example input: (batch_size=2, seq_length=10, input_size=5)
batch_size = 2
seq_length = 10
input_size = 5
hidden_size = 8
output_size = 1  # Single value per output

# Create model
model = LstmModel(input_size, hidden_size, output_size)

# Generate some random input data
x = torch.randn(batch_size, seq_length, input_size)

# Forward pass
output1, output2, output3 = model(x)

print(&quot;Output 1:&quot;, output1)
print(&quot;Output 2:&quot;, output2)
print(&quot;Output 3:&quot;, output3)
</code></pre>
<p>the results without the hidden_masked:</p>
<pre><code>Output 1: tensor([[-0.0487],
        [-0.0439]], grad_fn=&lt;AddmmBackward0&gt;)
Output 2: tensor([[-0.2588],
        [-0.2890]], grad_fn=&lt;AddmmBackward0&gt;)
Output 3: tensor([[0.1792],
        [0.1249]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre>
<p>with the hidden_masked:</p>
<pre><code>Output 1: tensor([[0.3568],
        [0.3477]], grad_fn=&lt;AddmmBackward0&gt;)
Output 2: tensor([[-0.3200],
        [-0.3470]], grad_fn=&lt;AddmmBackward0&gt;)
Output 3: tensor([[0.4120],
        [0.2970]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre>
<p>i realy hope that the comments i added in the code clarify each line and what it role.</p>
<p>use google colab to quick test the code</p>
<p>Edit 2:</p>
<p>since i used hard code values, here's a more rebost way:</p>
<pre><code>import torch
import torch.nn as nn

class MaskedLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, exclusion_map):
        &quot;&quot;&quot;
        Args:
            exclusion_map: Dictionary mapping output_idx to excluded_input_idx
            Example: {1: 2, 2: 1, 3: 3}  # output1 excludes input3, output2 excludes input2, etc.
        &quot;&quot;&quot;
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc_layers = nn.ModuleList([nn.Linear(hidden_size, output_size) 
                                      for _ in range(len(exclusion_map))])
        
        # Create trainable masks (modified)
        self.masks = nn.ParameterDict()
        for output_idx, excluded_input in exclusion_map.items():
            mask = torch.ones(hidden_size)
            
            # Zero 20% of dimensions associated with excluded input
            input_span = hidden_size // input_size
            start = excluded_input * input_span
            end = (excluded_input + 1) * input_span
            mask[start:end] = 0
            
            self.masks[f&quot;mask_{output_idx}&quot;] = nn.Parameter(mask, requires_grad=False)

    def forward(self, x):
        lstm_out, (hn, _) = self.lstm(x)
        final_hidden = hn[-1]  # (batch_size, hidden_size)
        
        outputs = []
        for idx, fc in enumerate(self.fc_layers):
            masked_hidden = final_hidden * self.masks[f&quot;mask_{idx+1}&quot;]
            outputs.append(fc(masked_hidden))
            
        return tuple(outputs)

# Configuration
exclusion_rules = {
    1: 2,  # Output1 excludes input3
    2: 1,  # Output2 excludes input2
    3: 3   # Output3 excludes input4
}

model = MaskedLSTM(input_size=5, hidden_size=10, 
                 output_size=1, exclusion_map=exclusion_rules)

# Test
x = torch.randn(3, 10, 5)  # batch_size=3, seq_len=10
out1, out2, out3 = model(x)

print(&quot;Output 1:&quot;, out1)
print(&quot;Output 2:&quot;, out2)
print(&quot;Output 3:&quot;, out3)
</code></pre>
","0","Answer"
"79541971","79367182","<p>Reinstalling torch using the following command (from <a href=""https://stackoverflow.com/a/27254355/2023370)"">here</a>) worked for me:</p>
<pre><code>(venv) $ pip install --upgrade --no-deps --force-reinstall torch
</code></pre>
<p>I issued the above command from within my venv, activated via <code>source /path/to/venv/bin/activate</code>. Then, things work again:</p>
<pre><code>(venv) $ cat hello.py 
from torch import Tensor

print(&quot;Hello, World!&quot;)
(venv) $ python hello.py 
Hello, World
</code></pre>
","0","Answer"
"79654105","79367182","<p>You meet this problem probably because there is a folder named 'torch' (but is not your package) somewhere else on your device.</p>
<p>Steps you may follow:</p>
<ol>
<li><p>open your command board and type:</p>
<p>pip show torch</p>
</li>
<li><p>the location of this package will be displayed. follow the location and ensure there is Torch.py in that folder.</p>
</li>
<li><p>search on your device if there is any other folder named 'Torch'. If yes, change its name.</p>
</li>
</ol>
","0","Answer"
"79413303","79413251","<p>It is about &quot;Teacher Forcing&quot; which is a trainign technique for seq2seq.</p>
<p>While teacher forcing it learns and when no theacher forcing it interferes( makes guesses.) So what is the realaiton with the shape?</p>
<p>Since decoder generates one token at time for each token in batch it needs <code>(batch_size, 1)</code> with 2D shape.
During teacher force it picks from batch (<code>target_tensor[:, i]</code>) and with unsqueze it generates the required shape.</p>
<p>In second case no teacher is not forcing so model makes a guess and it generate in shape of <code>(batch_size, 1)</code>.  But for tokens (words in nlp) represented with number making you guess token 1D is healtiher for general conssitency so <code>squeeze(-1)</code> to make it 1D. It will be reshaped down to GRU pipline but sending inside 1D makes it auto handle reshapes inside.</p>
","0","Answer"
"79415895","79415785","<p>You'll have to iterate over the dataset to compute the min/max values as part of data processing prior to training. Iterate once, compute the min/max values online as you iterate, then save them for future use.</p>
<p>For datasets too large to store in memory, it can be helpful to use a library like <a href=""https://huggingface.co/docs/datasets/en/index"" rel=""nofollow noreferrer"">datasets</a> which uses apache arrow as a backend. This allows you to work with the full dataset without needing to load it into memory.</p>
","0","Answer"
"79415956","79415897","<p>Because you are not using the environment, first activate the environment then open jupyter lab and then verify pytorch</p>
<p>For M1/M2 Macs, use the Apple Silicon version of PyTorch
Install Jupyter within the same virtual environment
<code>pip install jupyterlab</code></p>
<p>Kernel Configuration</p>
<p>Install ipykernel to make the virtual environment available in Jupyter
bash</p>
<pre><code>pip install ipykernel python -m ipykernel install --user --name=pytorch_env
</code></pre>
<p>Always activate your virtual environment before launching Jupyter Lab to ensure correct package visibility.</p>
","0","Answer"
"79417113","79417066","<p>You can use different syntax to create a broadcastable view. All of the following are equivalent.</p>
<pre><code>result = scalars.unsqueeze(1).unsqueeze(2) * tensors
result = scalars.view(-1, 1, 1) * tensors
result = scalars[:,None,None] * tensors
</code></pre>
","1","Answer"
"79417555","79417386","<p>Before answering the question please do not construct a new optimizer / loss criterion each iteration of the training. The code should be something like the following:</p>
<pre><code>import torch.optim as optim

model = MNIST_ConvNet()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
loss = nn.CrossEntropyLoss()

for X_batch, y_batch in train_dataloader:

            optimizer.zero_grad()
            output = model(X_batch)  
            loss = loss(output, y_batch)
</code></pre>
<p>Additionally in the output indexing <code>[0]</code> seems like a mistake, since you operate on batches, whereas this extracts the prediction of the first batch element.</p>
<p>If this does not solve it yet you might try casting the tensors to <code>float</code> by doing: <code>y_batch.float()</code> or <code>output.float()</code>.
Hope this helps.</p>
","1","Answer"
"79418177","79417996","<p>So I’ve been poking around and I think what is going on is this: PyTorch simplifies the problem of solving multiple matrices by reshaping the tensor into a single square matrix, and then solving for the entire tensor, before reshaping back. From the source code comments:</p>
<pre class=""lang-cpp prettyprint-override""><code>/*
  The idea is to reduce the problem to 2D square matrix inversion.
  Step 1. Calculate the shape of the result and the shape of the intermediate 2D matrix.
  Step 2. Reshape `self` to 2D matrix.
  Step 3. Invert the 2D matrix self.to_2D()
          There is no quick way to find out whether the matrix is invertible,
          so at this stage an error from at::inverse can be thrown.
          Note that for CUDA this causes cross-device memory synchronization that can be slow.
  Step 4. reshape the result.
  */
</code></pre>
<p>This sounds like it would be faster, but the time complexity for solving for the inverse scales with the cube of the matrix size, i.e. O(N<sup>3</sup>). By “breaking down” the tensor yourself through looping, you save PyTorch the trouble of having to solve this big tensor problem by encoding implicitly the information for byte-size (pun intended) tensors.</p>
<p>Smarter people than I can swoop in and correct this if I’m off base, but that’s what I’ve managed to find for you. Hope it helps.</p>
<p>ETA: You could test this pretty simply by running these same reshaping-inverse-reshape steps on your tensor in python and timing that. I’d suspect it will end up being very similar in time to the non-looped method.</p>
","2","Answer"
"79418696","79417996","<p>Use vmap to map a vector of inputs to the inverse function:</p>
<pre><code>batch_inv = torch.vmap(torch.linalg.inv)
</code></pre>
<p>Then view your matrices as a single vector of size <code>[n,.,.]</code>  and pass to <code>batch_inv</code>.</p>
<pre><code>original_shape = tensors.shape
tensors = tensors.view(-1,3,3)
output = batch_inv(tensors)
output = output.view(original_shape)
</code></pre>
","2","Answer"
"79419546","79416003","<p>A simple way to check if this code use the GPU is to <strong>profile it</strong>. Actually checking <code>nvidia-smi</code> while running it is also a simple way to see that.</p>
<p>In a profiler, we can see that 26 CUDA kernels are executed showing that <strong>the program does use the GPU</strong>.</p>
<p>The profiler also reports that some error happens during the execution (non-zero return code) which is suspicious (the error is not provided so debugging is needed for more information). I advise you to carefully check results.</p>
<p>Among all the kernels executed, 1 takes nearly all the time (&gt;99%): the kernel named <code>iir_cu_kernel</code>. It takes about 7-9 seconds on my Nvidia 1660S GPU. Meanwhile the CPU implementations takes 0.4 seconds...</p>
<p>One main issue is that <strong>this main kernel uses only 1 block</strong>! This is ridiculously small. My GPU contains 22 SM and each of them needs at least 2 blocks so not to starve. To hide the latency of the kernel, my GPU generally needs 8-16 blocks per SM (each SM can execute up to 16 blocks concurrently). This is like using a single core of a 128-core CPU! Actually, the profiler reports that less than <strong>1% of the GPU is actually used</strong>...</p>
<p>Running multiple times the operation does not make it faster. The kernel is just run asynchronously and multiple kernels can run simultaneously. On my machine, the second execution results in a device synchronization so only 2 kernels can overlap which is far from being enough to use a GPU efficiently...</p>
<p>I thought that the number of blocks was simply due to the number of row of the tensor but adding more rows did not increase the number of blocks used (though the execution time did not increase either).</p>
<p>I think the main issue is simply that the implementation is very inefficient so far (not GPU friendly) and it can certainly be massively improved. In fact, this is also what <a href=""https://github.com/pytorch/audio/issues/1408#issuecomment-1368094440"" rel=""nofollow noreferrer"">this message</a> (of the PR mentioned in the question) tends to indicate (they get a speed up of 100x~150x).</p>
<p>So far, I see 3 options for you: open a new issue explaining that and wait for a faster implementation to be merged (possibly few years), implementing this yourself (please contribute to torch if you can then), find a library doing that better than <code>torchaudio</code> (IDK which one).</p>
<p>It would also be good to understand why the number of block is 1, but I think it require to perform a deep analysis of the torch's code.</p>
<p>Please note that my version of Torch is the latest one on <a href=""https://pypi.org/project/torch/"" rel=""nofollow noreferrer"">pypi</a> at the time of writing: the version 2.6.0+cu124. It has been <a href=""https://github.com/pytorch/audio/releases"" rel=""nofollow noreferrer"">released just a week ago</a>!</p>
","1","Answer"
"79421932","79421842","<p>According to this <a href=""https://rocm.docs.amd.com/en/latest/conceptual/gpu-isolation.html"" rel=""nofollow noreferrer"">documentation</a>, <code>HIP_VISIBLE_DEVICES</code> environment variable should have the same effect. So:</p>
<pre><code>os.environ[&quot;HIP_VISIBLE_DEVICES&quot;]=&quot;0&quot;
</code></pre>
","3","Answer"
"79424423","79424280","<p>since <code>torch.version.cuda</code> is <code>None</code>, it seems like PyTorch installation is not compiled with CUDA.</p>
<p>Uninstall pytorch and reinstall it :</p>
<p><code>pip uninstall torch torchvision tourchaduio</code></p>
<p><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126</code></p>
<p>Check your virtual environment:</p>
<p><code>pip list | findstr torch</code></p>
<p>if pytorch in not listed, install it in the correct environment</p>
<p>Test pytorch with CUDA:</p>
<pre class=""lang-none prettyprint-override""><code>print(f&quot;Is CUDA supported by this system? {torch.cuda.is_available()}&quot;)
print(f&quot;CUDA version: {torch.version.cuda}&quot;)

if torch.cuda.is_available():
    cuda_id = torch.cuda.current_device()
    print(f&quot;ID of current CUDA device: {cuda_id}&quot;)
    print(f&quot;Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}&quot;)
else:
    print(&quot;CUDA is not available.&quot;)



</code></pre>
","1","Answer"
"79432924","79431134","<p>The problem is revealed in the error message that you get:</p>
<ul>
<li>PyTorch complains that <code>__main__.checkpoint_t</code> is not in <code>safe_globals</code>.</li>
<li>What you actually put into <code>safe_globals</code>, however, is <code>train.checkpoint_t</code> (as can be seen from your import <code>from train import checkpoint_t</code>).</li>
</ul>
<p>I guess what happened here is, that at some point, the <code>checkpoint_t</code> class was moved from <a href=""https://docs.python.org/3/library/__main__.html"" rel=""nofollow noreferrer"">the top-level code environment</a> to module <code>train</code>, while the weights that you are trying to load have been created with an earlier version of your code, with the <code>checkpoint_t</code> class still in its original place.</p>
<h2>A hacky solution</h2>
<p>There is no really easy way to fix this, as far as I know (other than putting the definition of <code>checkpoint_t</code> back to its original place, of course); however, we can make use of the following fact: Every  <code>*.pth</code> file that is not too old is nothing but a bunch of zipped files, into which the actual contents have been serialized using Python's standard <code>pickle</code> module. Based on this, we can use the following, somewhat hacky approach, drawing some inspiration from <a href=""https://snyk.io/de/articles/python-pickle-poisoning-and-backdooring-pth-files/"" rel=""nofollow noreferrer"">this blog post</a>:</p>
<ol>
<li>Unzip the <code>*.pth</code> file.</li>
<li>Unserialize the content of the contained <code>data.pkl</code> file and adjust the module name for <code>checkpoint_t</code> instances from <code>__main__</code> to <code>train</code>. Or, to be more precise: when the unpickling process is looking for class <code>__main__.checkpoint_t</code> into which to deserialize the pickled instance, return class <code>train.checkpoint_t</code> instead. Here, we can follow <a href=""https://stackoverflow.com/a/53327348/7395592"">this answer to a related question</a> and employ our own <code>Unpickler</code> subclass to make the adjustments.</li>
<li>Reserialize the content of the <code>data.pkl</code> file and rezip everything into a new <code>*.pth</code> file.</li>
</ol>
<p>It should then be possible to load the new <code>*.pth</code> file in the way you tried in the question.</p>
<p>All in all, this could look as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle
import zipfile

# TODO: Provide the paths to be read (original_path) and written (converted_path)
original_path = &quot;test.pth&quot;
converted_path = &quot;converted.pth&quot;

class RenameUnpickler(pickle.Unpickler):
    # Following https://stackoverflow.com/a/53327348/7395592
    def find_class(self, module, name):
        if module == &quot;__main__&quot; and name == &quot;checkpoint_t&quot;:
            module = &quot;train&quot;
        return super().find_class(module, name)

# Read and adjust pickled data
with zipfile.ZipFile(original_path, &quot;r&quot;) as z:
    pickle_path = next(n for n in z.namelist() if n.endswith(&quot;/data.pkl&quot;))
    with z.open(pickle_path) as f:
        unpickled_and_renamed = RenameUnpickler(f, encoding=&quot;utf-8&quot;).load()
        
# Re-zip adjusted pickled data
with zipfile.ZipFile(converted_path, &quot;w&quot;) as conv_z:
    with zipfile.ZipFile(original_path, &quot;r&quot;) as orig_z:
        for item in orig_z.infolist():
            if item.filename.endswith(&quot;/data.pkl&quot;):
                with conv_z.open(item.filename, &quot;w&quot;) as f:
                    pickle.dump(unpickled_and_renamed, f, protocol=2)
            else:
                conv_z.writestr(item.filename, orig_z.open(item).read())
</code></pre>
<p>There are a few caveats: I am not sure with which version of PyTorch your original <code>*.pth</code> file was written, so the approach might not work directly. Most notably, in my experiments, I had to force the pickle protocol version via <code>pickle.dump(..., protocol=2)</code> or else I could not load the re-written <code>*.pth</code> file. I am not sure if this is the case for all versions of PyTorch or if there are other conventions to follow for other versions (I tested with PyTorch 2.5.1).</p>
","1","Answer"
"79436534","79434940","<p>All consumer level NVidia GPUs have limited support for 64-bit floating point operations.<br />
Whereas every warp has 32 float32 cores, it only has a single float64 core.<br />
In my experiments I typically find that float64 code runs about 8x slower (more if the floating point operations are heavily optimized). The slowdown is not typically 32 times as you'd expect due to latency issues and memory fetch times etc.<br />
You really need to avoid double. If you really need the extra precision (which is rare), look at &quot;kahan summation&quot;.</p>
<p>NVidia <a href=""https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions"" rel=""nofollow noreferrer"">documents this in the CUDA programming guide</a></p>
<p>Compute Capability</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Throughput/cycle/SM</th>
<th>5.0, 5.2</th>
<th>5.3</th>
<th>6.0</th>
<th>6.1</th>
<th>6.2</th>
<th>7.x</th>
<th>8.0</th>
<th>8.6</th>
<th>8.9</th>
<th>9.0</th>
</tr>
</thead>
<tbody>
<tr>
<td>16-bit floating-point</td>
<td>N/A</td>
<td>256</td>
<td>128</td>
<td>2</td>
<td>256</td>
<td>128</td>
<td>256</td>
<td>256</td>
<td>128</td>
<td>256</td>
</tr>
<tr>
<td>32-bit floating-point</td>
<td>128</td>
<td>128</td>
<td>64</td>
<td>128</td>
<td>128</td>
<td>64</td>
<td>64</td>
<td>128</td>
<td>128</td>
<td>128</td>
</tr>
<tr>
<td>64-bit floating-point</td>
<td>4</td>
<td>4</td>
<td>32</td>
<td>4</td>
<td>4</td>
<td>32</td>
<td>32</td>
<td>2</td>
<td>2</td>
<td>64</td>
</tr>
</tbody>
</table></div>
<p>Note how on all non-server hardware the <code>double</code> throughput is severely limited. This limits the amount of chip real-estate needed to support rarely used 64-bit operations, which allows NVidia to make consumer GPU using smaller and cheaper dies (or alternatively spend more silicon optimizing 32-bit performance).</p>
<p>The compute capability numbers refer to the microarchiture, 8.9 is Ada Lovelace (aka RTX 4000 series), 8.6 is Ampere (aka RTX 3000 series) and so on, see: <a href=""https://en.wikipedia.org/wiki/CUDA"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/CUDA</a></p>
<p>As for better performance, have a look at different libraries. Esp have a look at python wrappers around native CUDA libraries (<a href=""https://docs.nvidia.com/cuda/cusolver/index.html"" rel=""nofollow noreferrer"">such as CuSolver</a>) that are optimized for the GPU.</p>
","1","Answer"
"79440032","79439095","<p>Well, how about killing the vllm related process using <code>pkill -9 -ef &lt;part or whole of the vllm process name or cli command&gt;</code>? You can check the vllm process consuming GPU RAM with <code>nvidia-smi</code>, <code>nvitop</code> or <code>nvtop</code>.</p>
","0","Answer"
"79442734","79441614","<p>You cannot backprop through an in-place operation. However, there are cases where it can <em>appear</em> possible because the in-place operation happened in a way that did not impact the gradient calculation.</p>
<p>First lets distinguish between what is happening in python vs pytorch. Pytorch operations return new tensor objects. Your example of using the <code>x</code> variable twice is not a problem as <code>x</code> in this case is pointing to two different tensors. You can check this with the <code>data_ptr</code> attribute:</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.randn(10)
b = torch.randn(10)
c = torch.randn(10)

x = a * b
print(x.data_ptr())
&gt; 605338432 # `x` points to 605338432
x = x * c 
print(x.data_ptr())
&gt; 605338624 # `x` points to 605338624, a different object
</code></pre>
<p>Now to the pytorch level. When you compute values, pytorch saves only the necessary values for backprop. If one of these values is modified by an in-place operation, you get an error. You can however do in-place operations to values not required for gradient calculation.</p>
<p>Consider <code>y = exp(x)</code>. In this case, <code>dy/dx = exp(x) = y</code>. For backprop, pytorch stores the value of <code>y</code> to use when the gradient of <code>x</code> is computed. This means that if you modify <code>y</code> with an in-place operation and try to backprop, you'll get an error:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(10, requires_grad=True)
y = x.exp() # computing `y=exp(x)` saves `y` for backprop
y.add_(10) # in-place addition
y.backward(torch.ones_like(y)) # backprop throws error
&gt; RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
</code></pre>
<p>Now consider <code>y = x ** 2</code>. In this case, <code>dy/dx = 2 * x</code>. For backprop, pytorch stores the value of <code>x</code>. In this case, <code>y</code> is not involved with the gradient computation. This means you can modify <code>y</code> with an in-place operation.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(10, requires_grad=True)
y = x.pow(2) # computing `y = x**2` saves `x` for backprop
y.add_(10) # in-place addition
y.backward(torch.ones_like(y)) # backprop runs without error
</code></pre>
<p>In general you should avoid using in-place operations for anything you want to backprop through.</p>
","2","Answer"
"79443484","79440564","<p>Faced the same issue with this Python version.
Python 3.13 is not fully supported by many libraries yet, including torch, xformers, and jnius!
I strongly recommend downgrading to Python 3.10 or 3.11, as these versions have better compatibility.</p>
<p>Some steps I used to run it in virtual enviroment after I set Python to 3.11 in PATH:</p>
<pre><code>python -m venv mistral_env
mistral_env\Scripts\activate
</code></pre>
<p>This way the following installs have completed successfully in the virtual enviroment:</p>
<pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install cython
pip install numpy scipy sentencepiece
pip install xformers
pip install mistral-inference
</code></pre>
","1","Answer"
"79445407","79444383","<p>All {}-blocks define dictionaries. In this example, it's used to generate dictionaries with two keys, 'train' and 'val'.</p>
<p>So, <code>data_transforms</code> will include two datasets, one for training (with augmented data) and one for validation. <code>image_datasets</code> are the path to the data and are used when defining a <code>DataLoader</code> for the training and validation data, respectively.</p>
<p>In <code>dataset_sizes</code>, the two data set sizes are saved and will look like <code>{'train': 523, 'val': 78}</code> if there are 523 entries in the training data and 78 in the validation data. Finally, <code>class_names</code> contain the classes for the training data (and will likely be used for training the model).</p>
","0","Answer"
"79445665","79444383","<pre><code>data_dir = 'data/hymenoptera_data' 
</code></pre>
<p>Defines directory where dataset is located</p>
<pre><code>image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
</code></pre>
<p>This is a list comprehension where the keys are 'train' and 'val' (training and validation datasets), and the values are ImageFolder datasets for each of these sets.
<code>ImageFolder(os.path.join(data_dir, x),data_transforms[x])</code> ImageFolder is a PyTorch function that loads images from a directory where each subfolder represents a class label, automatically assigns class labels, and applies the relevant transformations <code>data_transforms[x]</code> which are defined above for the train and val sets. <code> os.path.join(data_dir, x)</code> Constructs the full path to the image dataset directory to make sure the ImageFolder works correctly</p>
<pre><code>dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
</code></pre>
<p>This is another dictionary comprehension where it loops over x in ['train', 'val'], meaning when x = 'train', it creates a DataLoader for the training dataset, and when x = 'val', it creates a DataLoader for the validation dataset. It then assigns the created DataLoader to the corresponding key ('train' or 'val').
<code>torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)</code> Just defines the batch_size that will be used, shuffle=True defines whether to shuffle the data (important for training), and how many worker threads to load data in parallel (speeds up data loading)</p>
<pre><code>dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
</code></pre>
<p>This is another dictionary comprehension that loops over x in ['train', 'val'], meaning when x = 'train', it defines the value of the dictionary entry as the length of the training dataset, and when x = 'val', it defines the size of the dataset as the length of the validation dataset.</p>
<pre><code>class_names = image_datasets['train'].classes
</code></pre>
<p>This is a line of code that extracts the names of the classes from the image_datasets training set!</p>
<p>Hope this helps</p>
","0","Answer"
"79447696","79447650","<p>You said that sys.getsizeof() function doesn't work for you, May I get more info on that.
Here is a convenient way to get the memory usage using pympler.</p>
<p>Have a look at its implementation.</p>
<pre><code>import sys
a_random_list = [1, 2, 3, 4, 5]
memory_usage = sys.getsizeof(a_random_list)
print(f&quot;Memory usage of the list: {memory_usage} bytes&quot;)
</code></pre>
<p>If you don't wish to use the sys.getsizeof() function, you can use this</p>
<pre><code>from pympler import asizeof
# U can use Pympler to get more accurate info and open compatibility for larger types of datasets.

my_list = [1, 2, [3, 4, 5], {'a': 6, 'b': 7}]

memory_usage = asizeof.asizeof(my_list)

print(f&quot;Total memory usage of the list: {memory_usage} bytes&quot;)
</code></pre>
<p>To use this, you would have to install pympler.
Use command:
<code>pip install pympler</code></p>
<p>:)</p>
","-1","Answer"
"79447799","79447650","<p><code>sys.getsizeof()</code> alone does not capture nested object memory,  so you need to use a combination of <code>sys.getsizeof()</code>, <code>tensor.element_size()</code>, and handling PyTorch-specific behaviors. Also, you can use <code>torch.cuda.memory_allocated()</code> but need GPU.</p>
<pre><code>def get_tensor_memory_size(tensor):
    &quot;&quot;&quot;Calculate memory size of a PyTorch tensor.&quot;&quot;&quot;
    if tensor.is_sparse:
        # Ensure the sparse tensor is coalesced to have proper indices/values
        if not tensor.is_coalesced():
            tensor = tensor.coalesce()
        # Memory for sparse tensor: size of indices (ignoring values for simplicity)
        return tensor.indices().element_size() * tensor.indices().nelement()
    else:
        # Memory for dense tensor
        return tensor.element_size() * tensor.nelement()

def get_list_memory_size(lst):
    &quot;&quot;&quot;Calculate total memory size of a list of tuples containing tensors.&quot;&quot;&quot;
    total_size = sys.getsizeof(lst)
    for tup in lst:
        total_size += sys.getsizeof(tup)
        for tensor in tup:
            total_size += get_tensor_memory_size(tensor)
    return total_size
</code></pre>
<p>Please see the <a href=""https://colab.research.google.com/drive/112Io6MDmN0h_1quHxrBIU7VzH3R8vCQ-?usp=sharing"" rel=""nofollow noreferrer"">example here</a></p>
","0","Answer"
"79449888","79447650","<h2>Why sys.getsizeof() won't work</h2>
<p><code>sys.getsizeof()</code> just returns the size of the object itself, not what it is referring to. i.e. if you had a box with stuff in it, <code>sys.getsizeof()</code> will tell you how big the box is, but not what's inside. You could have a box of packing peanuts or a box of tungsten cubes, we need to know more what's inside the box. Note this is not a tensor specific issue, but for any container object (list tuples, np arrays, tensors, etc.)</p>
<h2>What will work to get the true memory size of tensor/container objects</h2>
<p>For a dense tensor, it's pretty easy. Just take the number of elements times the size of one element.
<code>dense_mem_size = tensor.nelement() * tensor.element_size()</code>
Where tensor.nelement() gives the number of items in the tensor, and tensor.element_size() gets the size of one of those elements.</p>
<p>For sparse coordinate tensors, same idea but now consider the values and indices of the coo tensor:</p>
<pre><code>vals_size = coo_tensor._values().nelement() * coo_tensor._values().element_size()
idxs_size = coo_tensor._indices().nelement() * coo_tensor._indices().element_size()

sparse_mem_size = vals_size + idxs_size 
</code></pre>
<h2>Putting this together to get a function for your data's structure</h2>
<p>We will just have a function that takes an object and recursively calls itself to go deeper</p>
<pre><code>def deep_getsizeof(obj, seen=None):    
    # seen checks for objects referenced in multiple places to avoid double counting
    if seen is None:
        seen = set()
        
    obj_id = id(obj)
    if obj_id in seen:
        return 0 
    seen.add(obj_id)
    
    size = 0
    if isinstance(obj, torch.Tensor):
        # start with the shallow size of the tensor object.
        size += sys.getsizeof(obj)
        
        if obj.is_sparse: # sparse tensor
            idxs = obj._indices()
            vals = obj._values()
            size += deep_getsizeof(indices, seen)
            size += deep_getsizeof(values, seen)

        else: # dense tensor
            size += obj.nelement() * obj.element_size()
        return size
    
    # For lists and tuples, include the size of each element.
    if isinstance(obj, (list, tuple)):
        size += sys.getsizeof(obj)
        for item in obj:
            size += deep_getsizeof(item, seen)
        return size
    
    # base case return size of item if not one of the container objects you mentioned
    return sys.getsizeof(obj)
</code></pre>
","1","Answer"
"79449949","79439095","<p>I regularly use the holy trinity of cleanup with pytorch</p>
<ol>
<li>Delete model object with Python <code>del</code></li>
<li>Empty cache with <code>torch.cuda.empty_cache()</code></li>
<li>Python garbage collection <code>gc.collect()</code> (import gc at top of script)</li>
</ol>
<pre><code>del llm
torch.cuda.empty_cache()
gc.collect()
</code></pre>
<p>That said Jupyter notebooks are weird, you may just have to restart the kernel if these don't work since Jupyter has its own cachine mechanisms.</p>
","0","Answer"
"79449994","79433165","<p>So I think that the issue is how you extract the derivatives. Whether you use the slicing version or the alternative you mentioned the same issue persists, being the resulting <code>u_x</code> and its subsequent <code>u_xx</code> are not used <em>directly</em> in the loss fn beyond <code>u_xx</code>, so torch can't find how to get from <code>u</code> to <code>u_xx</code> through <code>u_x</code>.</p>
<p>This leaves you with two options:</p>
<ol>
<li>If it is acceptable that part of u's gradient is unused (0) you can set the allow_unused flag to True in the calculation of u's loss</li>
</ol>
<p><code>loss_u = torch.autograd.grad(loss, u, grad_outputs=torch.ones_like(loss), allow_unused=True, retain_graph=True, create_graph=True)[0]</code></p>
<ol start=""2"">
<li>If you want the gradient to fully propogate through u including u_x, you can add an extra loss term that enforces the dependency</li>
</ol>
<pre><code>u_x_loss = alpha * mse_loss(u_x, u_x.detach())  
loss = mse_loss(u_t, self.kappa * u_xx) + u_x_loss 
</code></pre>
<p>Note: this extra <code>u_x_loss</code> term does not need to be mse_loss specifically, but any loss fn that enforces the dependency (e.g. MAE loss, Gradient penalty, etc.)</p>
<p>you can set <code>alpha</code> to be really small if you don't want it to have a large effect, but be careful if it's too small you can get weird values.</p>
<h3>TLDR</h3>
<p>doesn't matter how you extract the derivatives either with slicing or the scalar multiplication alternative, you are breaking the graph in either case. Either allow unused values in the gradient calculation, or add a u_x loss term to enforce the graph remaining intact.</p>
","0","Answer"
"79450020","79428887","<p>You can’t force the entire 7B model (even 4bit quantized) into just 4GB of GPU memory with the default auto device-mapping. Some parts of the model will need to be offloaded to CPU in 32-bit, so you need to (1) enable offloading with <code>llm_int8_enable_fp32_cpu_offload=True</code> and (2) provide or adjust a custom device map that tells PyTorch which layers go to the CPU (or GPU). Simply setting <code>max_memory={0: &quot;4GB&quot;, &quot;cpu&quot;: &quot;32GB&quot;}</code> does not guarantee a successful fit if the library determines it can’t keep all quantized modules within 4GB, since the max memory parameter is just a guideline, not a hard limit.</p>
<p>First try increasing max memory and see if it can figure itself out, I would put it at or just below your total available VRAM.</p>
<p>Failing that, you will need to inspect the auto generated device map (just <code>print(device_map1)</code>. Then make adjustments to which layers get offloaded to CPU. Ideally you wanna find a balance to where you can fit as many layers on GPU as possible without OOM errors. This map basically tells the model which layers to give to the CPU that you explicitly determine. Here is an example of moving a block to the cpu</p>
<pre><code>device_map1[&quot;lm_head&quot;] = &quot;cpu&quot;
for key in device_map1.keys():
    if key.startswith(&quot;transformer.h.&quot;):
        # Offload half or more of the layers
        device_map1[key] = &quot;cpu&quot;
</code></pre>
<p>Then init your model with the cpu offload flag true and giving it your new device_map</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map=device_map1,
    llm_int8_enable_fp32_cpu_offload=True,
    trust_remote_code=True,
    **kwargs
)
</code></pre>
","2","Answer"
"79452264","79451900","<p>The last version of torchvision that includes <code>ops._new_empty_tensor</code> is 0.8.2, which is compatible with PyTorch 1.7.1. That said, the <code>requirements.txt</code> in the repo you linked includes:</p>
<pre><code>torch&gt;=1.5.0
torchvision&gt;=0.6.0
</code></pre>
<p>From this, I would guess that PyTorch 1.5.0 and torchvision 0.6.0 are what the devs actually used when they trained that model.</p>
","1","Answer"
"79453135","79453088","<ul>
<li>You can simply use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"" rel=""nofollow noreferrer"">KMeans</a> for this task; since the images are not complex.</li>
<li>Prior to clustering, you can remove the background and reduce the quality and scale it down (<code>scale=256:128</code>). You can do it in python or using <code>ffmpeg</code> in bash/shell:</li>
</ul>
<pre><code>mkdir -p ~/path/to/your_original_images/raw

count=1
for file in ~path/to/your_original_images/*; do
    [[ &quot;$file&quot; =~ \.(webp|jpg|png)$ ]] || continue

    ffmpeg -i &quot;$file&quot; -vf &quot;scale=256:128,colorkey=black:0.1:0.1&quot; \
    -vcodec libwebp -compression_level 5 -q:v 70 \
    -preset picture -lossless 0 -map_metadata -1 -map v:0 -pix_fmt rgba \
    ~/path/to/your_original_images/raw/img${count}.webp

    ((count++))
done

</code></pre>
<p><img src=""https://i.sstatic.net/gbNC8oIz.webp"" alt=""enter image description here"" /></p>
<p><img src=""https://i.sstatic.net/KnUqHzTG.webp"" alt=""enter image description here"" /></p>
<p><img src=""https://i.sstatic.net/BOcBmjZz.webp"" alt=""enter image description here"" /></p>
<hr />
<p>This does not really need a CNN. CNN is usually being used for images with convoluted patterns.</p>
<p>If you had more complex images and wanted to experiment; I'd look into <a href=""https://keras.io/api/layers/convolution_layers/"" rel=""nofollow noreferrer"">Keras</a> which is an easy to use wrapper of TensorFlow.</p>
","0","Answer"
"79455225","79451702","<p>The reason your program hangs is because you are trying to fork a multithreaded process, which is destined for trouble. As stated in the <a href=""https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods"" rel=""nofollow noreferrer"">multiprocessing docs</a>:</p>
<blockquote>
<p>Note that safely forking a multithreaded process is problematic.</p>
</blockquote>
<p>You may think that your process is not creating other threads, but it turns out that just importing pytorch creates a bunch of background threads in order to (<code>#TODO: insert unknown wizardry here</code>). I verified this using a simple test program:</p>
<pre><code>import time
time.sleep(10)

import torch
time.sleep(10)

x = torch.arange(100000)
time.sleep(10)
</code></pre>
<p>During the first sleep, we have only one thread:
<a href=""https://i.sstatic.net/oTzk4T5A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTzk4T5A.png"" alt=""A single thread running"" /></a></p>
<p>During the second sleep (after importing <code>torch</code>):
<a href=""https://i.sstatic.net/JpEPVhI2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpEPVhI2.png"" alt=""20 threads running"" /></a></p>
<p>And for good measure, here's what we get during the third sleep, after calling <code>arange</code>:
<a href=""https://i.sstatic.net/lWUaIe9F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lWUaIe9F.png"" alt=""33 threads running"" /></a></p>
<p>It's hard to say what exactly is happening to cause this specific deadlock, but it's likely that a mutex is being copied by the fork operation while in a locked state, and the copy never gets unlocked.<br />
Regardless, the takeaway should be: Use the <code>spawn</code> or <code>forkserver</code> start methods when you already have multiple threads running, avoid <code>fork</code> unless you are willing to do a lot of tricky manual work to ensure all the forked threads will play nice (and you have a VERY good reason to do so). That's probably not feasible with <code>torch</code>, so just avoid <code>fork</code>.<br />
By the way, pytorch has a page of <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noreferrer"">best practices for multiprocessing</a>, which you might find helpful.</p>
","1","Answer"
"79459384","79457741","<p>One small issue that seems to arise with batched inputs is that you are manually summing the gradients for the bias via <code>grad_bias = grad.sum(dim=(0))</code>. This essentially accumulates the gradient across the batch, which you do not have to do as PyTorch automatically does this at the end of <code>backward()</code> (... if I am not mistaken ...)</p>
<p>Here are my experiments: <br />
First, using the <code>torch.autograd.Function</code> you implemented we can turn it into an <code>nn.Module</code> as follows</p>
<pre class=""lang-py prettyprint-override""><code>class CustomLinear(torch.nn.Module):
    def __init__(self, token_dim=64):
        super(CustomLinear, self).__init__()
        # you may change the weight init here ... 
        self.weight = torch.nn.Parameter(torch.randn(token_dim, token_dim))
        self.bias = torch.nn.Parameter(torch.randn(token_dim))
    
    def forward(self, x): 
      return Linear.apply(x, self.weight, self.bias)
</code></pre>
<p>Then we can initialize two linear layers: One with the &quot;custom implementation&quot; and one using <code>nn.Linear()</code>. In order to meaningfully compare the gradients, we need to make sure the weights of the two layers are the same and pass the same input to them:</p>
<pre class=""lang-py prettyprint-override""><code>seq_len = 2 
token_dim = 3
input1 = torch.rand((1, seq_len, token_dim))
input2 = torch.clone(input1)
input1.requires_grad, input2.requires_grad = True, True 

weights = torch.rand((3, 3), requires_grad=True)
bias = torch.rand((3), requires_grad=True)

layer1 = CustomLinear(token_dim=token_dim)
layer2 = torch.nn.Linear(token_dim, token_dim)

# Manually set weights/biases in the two linear layers to be the same weights/biases
with torch.no_grad():  
    layer1.weight.copy_(weights)  
    layer1.bias.copy_(bias)      

    layer2.weight.copy_(weights)  
    layer2.bias.copy_(bias)      


output1 = layer1(input1)
output2 = layer2(input2)

# Do backprop on a dummy loss 
loss1 = torch.sum(output1**2)
loss2 = torch.sum(output2**2)

loss1.backward()
loss2.backward()

print(f&quot;Difference of Gradients w.r.t. input: {torch.norm(input1.grad - input2.grad)}&quot;)
print(f&quot;Difference of Gradients w.r.t. weight parameter: {torch.norm(layer1.weight.grad - layer2.weight.grad)}&quot;)
print(f&quot;Difference of Gradients w.r.t. bias parameter: {torch.norm(layer1.bias.grad - layer2.bias.grad)}&quot;)
</code></pre>
<p>The result of all 3 checks there (gradient w.r.t. input, w.r.t. weight and w.r.t. bias) result in a difference of exactly 0 if we take inputs with batch_size=1.<br />
Changing the batch size to be a larger value will result in differences in the values <code>weight.grad</code> and <code>bias.grad</code>. The difference of the <code>bias.grad</code>'s disappears upon integrating the change I explained above, i.e. removing <code>.sum(dim=0)</code>. I have been unable to find a cause for the difference in weight, but I'll update my answer as soon as I find something.</p>
","1","Answer"
"79464456","79463660","<p>if you met the same problem as me, I have found a solution. Please find below some explaination. and the code needed to solve this problem.</p>
<p>This error :</p>
<pre><code>WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
</code></pre>
<p>appears because by default, TensorFlow try to keep in memory the GradientTape as long as it can.
But in some case, for example in some complex Adversarials Attacks, it's not good because it costs too much ressources to our computer.</p>
<p>So firstly, we can free the memory used by tensorflow at the start of our code:</p>
<pre><code>import gc
import tensorflow as tf

gc.collect()
tf.keras.backend.clear_session()
</code></pre>
<p>It will avoid memory accumulation through different sessions.</p>
<p>Then, add this line to avoid that TensorFlow stock too much operation in the GradientTape (not mandatory):</p>
<pre><code>tf.config.run_functions_eagerly(False)
</code></pre>
<p>Finally, because GradientTape(persistent=True) by default on the source code, and it's preferrable to not touch the source code of the tensorflow library, we will create our own GradientTape, using this code:</p>
<pre><code>import tensorflow as tf

# Create another version of GradientTape without persistent=True
class CustomGradientTape(tf.GradientTape):
    def __init__(self, persistent=False, *args, **kwargs):
        super().__init__(persistent=persistent, *args, **kwargs)

# Replace the GradientTape by our Custom Version
tf.GradientTape = CustomGradientTape
</code></pre>
<p>Now, the parameter persistent is equal to False.</p>
<p>It works for me. So I hope it will works for you.
Sincerely, Roku.</p>
","0","Answer"
"79465084","79462366","<p>For those who may be searching, this worked for me:</p>
<p>sudo apt-get install python3-pip libopenblas-base libopenmpi-dev libomp-dev</p>
<p>pip3 install 'Cython&lt;3'</p>
<p>pip3 install numpy</p>
<p>wget <a href=""https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl"" rel=""nofollow noreferrer"">https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl</a> -O torch-1.10.0-cp36-cp36m-linux_aarch64.whl</p>
<p>sudo pip3 install torch-1.10.0-cp36-cp36m-linux_aarch64.whl</p>
","0","Answer"
"79465892","79464907","<p>Yeah, the issue is that you're moving tensors to CUDA inside <code>__getitem__()</code>, which isn't a good idea when using multiple workers in DataLoader. When <code>num_workers</code> &gt; 0, PyTorch spawns separate processes for loading data, but CUDA operations should only happen in the main process. This can lead to memory not being freed properly, which is why your usage keeps increasing.</p>
<p>A better approach is to keep everything on the CPU inside <code>__getitem__()</code> and only move tensors to the GPU inside the training loop. Change this:</p>
<pre class=""lang-py prettyprint-override""><code>def __getitem__(self, idx):
    image = np.load(self.data_paths[idx]['image']).astype(np.float32)
    label = np.load(self.data_paths[idx]['label']).astype(np.int64)

    return torch.from_numpy(image), torch.from_numpy(label)
</code></pre>
<p>And move them to CUDA in the training loop:</p>
<pre class=""lang-py prettyprint-override""><code>for batch in dataloader:
    images, labels = batch
    images = images.cuda(non_blocking=True)
    labels = labels.cuda(non_blocking=True)
</code></pre>
<p>This should already solve most of the issue.</p>
<p>If the memory still increases, try setting <code>persistent_workers=True</code> in DataLoader, since it helps with memory handling when using multiple workers:</p>
<pre class=""lang-py prettyprint-override""><code>dataloader = DataLoader(dataset, batch_size=32, num_workers=4,
                        pin_memory=True, persistent_workers=True)
</code></pre>
<p>If that doesn't work, test with <code>num_workers=0</code>. If the leak stops, then it's definitely related to the worker processes holding onto tensors.</p>
<p>As a last resort, manually force garbage collection after each batch:</p>
<pre class=""lang-py prettyprint-override""><code>import gc
gc.collect()
torch.cuda.empty_cache()
</code></pre>
<p>But in general, the main problem here is that CUDA tensors shouldn’t be created inside <code>__getitem__()</code>, especially with multiprocessing. Move them in the main loop, and it should fix the issue</p>
","1","Answer"
"79465931","79463165","<p>Ok, apparently I'm not the only one to think this is not good, and many others had the same issue:</p>
<ul>
<li>Add an option for Dection Models to return cost in eval mode as well
(validation, test loss) · <a href=""https://github.com/pytorch/vision/issues/1574"" rel=""nofollow noreferrer"">Issue #1574</a></li>
<li>[models] Suggestion of GeneralizedRCNN forward output · <a href=""https://github.com/pytorch/vision/issues/1775"" rel=""nofollow noreferrer"">Issue #1775</a></li>
<li>Calculate Training Accuracy on resnet152 · <a href=""https://github.com/pytorch/vision/issues/2578"" rel=""nofollow noreferrer"">Issue #2578</a></li>
</ul>
","0","Answer"
"79468005","79467972","<p>While torch.vmap is designed to vectorize a single function over inputs, it doesn't natively support applying multiple functions across tensor rows. Instead you can iterate over the rows and apply the corresponding function to each:</p>
<pre><code>import torch

# Define the input tensor
x = torch.ones(3, 3)

# Define the list of functions
factors = [lambda x: 2 * x, lambda x: 3 * x, lambda x: 4 * x]

# Initialize a list to store the results
result = []

# Iterate over each row and apply the corresponding function
for i, row in enumerate(x):
    result.append(factors[i](row))

# Stack the list of tensors into a single tensor
result = torch.stack(result)

print(result)
# Output:
# tensor([[2., 2., 2.],
#         [3., 3., 3.],
#         [4., 4., 4.]])
</code></pre>
<p>In my approach, I manually iterated over each row of the tensor x, applied the function from the factors list based on the row index, and collected the results in a list. Finally, I stacked the list into a single tensor using <code>torch.stack().</code></p>
<p>While this method involves an explicit loop, it provides clarity and flexibility, especially when dealing with complex transformations that are not easily vectorizable. Currently, PyTorch does not offer a built-in function to apply multiple distinct functions across tensor rows without such iteration.</p>
","0","Answer"
"79468222","79467972","<p>I have to concede that the approach below is not straightforward but could work as expected.</p>
<p>The basic idea is converting <code>Tensor</code> to <code>pandas.DataFrame</code>, using <code>apply</code> along <code>axis=1</code> and finally recovering it to <code>Tensor</code> from <code>numpy</code></p>
<pre><code>import torch
import pandas as pd

x = torch.ones(3, 3)
fcn = [lambda x: 2*x, lambda x: 3*x, lambda x: 4*x]
torch.Tensor(pd.DataFrame(x.numpy()).apply(lambda x: fcn[x.name](x), axis = 1).values)
</code></pre>
<p>and you will obtain</p>
<pre><code>tensor([[2., 2., 2.],
        [3., 3., 3.],
        [4., 4., 4.]])
</code></pre>
","-1","Answer"
"79473713","79473125","<p>Simply create them via one of the creation methods (e.g. <code>torch.tensor</code> or <code>torch.randn</code>) but with <code>requires_grad=True</code>:</p>
<pre class=""lang-py prettyprint-override""><code>alpha = torch.tensor(1., requires_grad=True)
gamma = torch.randn(1, requires_grad=True)
</code></pre>
","1","Answer"
"79473861","79470053","<p>Although you can, and should, try to replicate your experimental setup exactly on both GPUs by doing e.g.</p>
<pre><code>import torch
import numpy as np
import random

seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
</code></pre>
<p>The GPUs you mention have different architectures, which affects things such as floating point arithmetic, rounding, etc... Achieving bit-for-bit determinism across different GPU architectures is <em><strong>EXTREMELY</strong></em> hard, if not <em><strong>completely impossible</strong></em>. In my experience, training a model on an a100 vs v100  for example with the same hyperparameters, seeds, etc... can and more often than not will yield different results.</p>
<p>In order for you to run experiments that require you to compare performance of models, like for example ablation studies, you MUST train all the models on the same hardware. If not, there is no way for you to make any reliable conclusion about your results.</p>
<p>That being said, it is still important to set seeds so that running an experiment on the same hardware always yields the same results.</p>
","1","Answer"
"79474049","79473125","<p>Loss scaling factors are hyperparameters. They need to be set outside the learning loop - they cannot be learned.</p>
<p>The reason for this is the model can trivially achieve zero loss by ignoring the actual loss term and instead setting the scaling term to 0 or a large negative number.</p>
<p>The coefficients <code>alpha</code>, <code>beta</code>, etc cannot be part of the loss optimization itself. Look into hyperparameter tuning for methods on selecting and evaluating different hyperparameters outside the loss optimization loop.</p>
","3","Answer"
"79474065","79473994","<blockquote>
<p>thought about to write something myself to increase the weight over the True(1) values, but I suspect that would just flip the results and make everything to be <code>[1,1,1,1,1,1,1,1,1,1,1,1]</code>?</p>
</blockquote>
<p>No, it wouldn't. Largest contributor of the loss are <code>0</code> values if the vector is really sparse. Neural network will be biased to output <code>0</code> as it is way more common</p>
<blockquote>
<p>What's the appropriate loss function for Multi-Label Classification of multiple labels and sparse outcomes?</p>
</blockquote>
<p>For <code>pytorch</code> <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"" rel=""nofollow noreferrer"">BCEWithLogitsLoss</a> (in case your network outputs logits [output of the last layer without sigmoid applied], otherwise <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"" rel=""nofollow noreferrer"">BCELoss</a>)</p>
<p>An example implementation could be:</p>
<pre class=""lang-py prettyprint-override""><code>import typing
import torch


class WeightBCEWithLogitsLoss(torch.nn.Module):
    def __init__(
        self,
        weight: float,
        reduction: typing.Callable[[torch.Tensor], torch.Tensor] | None = None,
    ):
        super().__init__()
        self.weight = weight
        if reduction is None:
            reduction = torch.mean
        self.reduction = reduction

    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:
        loss_matrix = torch.nn.functional.binary_cross_entropy_with_logits(
            input,
            target,
            reduction=&quot;none&quot;,
        )
        loss_matrix[target] *= self.weight
        return self.reduction(loss_matrix)
</code></pre>
<p>and usage:</p>
<pre class=""lang-py prettyprint-override""><code># 7x more focus on the `training` positive samples
criterion = WeightBCEWithLogitsLoss(weight=7.)
</code></pre>
","1","Answer"
"79474076","79473994","<p>For multiclass classification, you want to use binary crossentropy. This loss tells the model to make an independent prediction for each output class.</p>
<p>With pytorch, you can add a <code>pos_weight</code> parameter to <code>BCEWithLogitsLoss</code> to increase the weight of the positive class. You'll need to experiment to determine what that weight should be.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# your example target variable
y = torch.tensor([[0,0,0,0,0,1,0,1,0,0,0,1],
                  [0,0,0,1,0,0,0,0,0,0,0,0]])

d_in = 8 # example input size
batch_size, n_outputs = y.shape
x = torch.randn(batch_size, d_in)

# example model
model = nn.Sequential(
    nn.Linear(d_in, d_in*2),
    nn.ReLU(),
    nn.Linear(d_in*2, n_outputs)
)

# initial guess for positive class weight is the ratio of 0s to 1s
# for each class clamped at 10 
pos_weight = ((y == 0).sum(0) / (y==1).sum(0)).clamp(0, 10)

criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

p = model(x)

# note for BCEWithLogitsLoss, targets must be float type
loss = criterion(p, y.float())
</code></pre>
<p>That said, I think the bigger issue is you are confused by metrics. You have a sparse classification problem where most of your ground truth values are 0. This means that any model can achieve high accuracy by predicting all 0s. This is simply a fact of your dataset - there is nothing you can do to change that. For this exact reason, accuracy is considered a bad metric and rarely used. Instead, look into metrics like precision and recall that work much better with unbalanced classes.</p>
","1","Answer"
"79474472","79473125","<p>Theoretically you can do it as follows:</p>
<pre class=""lang-py prettyprint-override""><code>
alpha = torch.tensor([1.], requires_grad=True)
beta = torch.tensor([1.], requires_grad=True)
gamma = torch.tensor([1.], requires_grad=True)
delta = torch.tensor([1.], requires_grad=True)

your_other_model = YourOtherModelClass()

optimizer = torch.optim.Adam([
    {'params': your_other_model.parameters(), 'lr': 1e-4},
    {'params': [alpha, beta, gamma, delta], 'lr': 1e-4}    # put trainable tensors in an iterable such as a list 
])

# training loop
for i in range(num_iters):
    loss0, ..., loss3 = ... # model forward and loss computation
    loss = alpha * loss0 + beta * loss1 + gamma * loss2 + delta* loss3
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
<p>But, as other answers pointed out, making hyperparameters optimizable may not make practical sense.</p>
","1","Answer"
"79474556","79451638","<p>Reduction operators (such as sum), are usually parallelized on GPU for performance. I haven't dug into the exact kernels, but based on my experience,  these parallel operators are optimized for different tensor shapes. Summing up a 2d tensor with <code>.sum(-1)</code> and its sliced version may involve different launch configurations or differently optimized kernels so the results could also be different. For float numbers even changing the summing order may lead to different results, let alone parallel reduction.</p>
<p>Below is a simple example:</p>
<pre class=""lang-py prettyprint-override""><code>def compare_sums(x):
  for i in range(x.shape[0]):
    if x[i].sum() != x.sum(-1)[i]:
       print(f'[Row {i}] {x[i].sum()} != {x.sum(-1)[i]}')

x = torch.rand(50, 500).cuda()  # for large-size tensor the problem is more likely to appear
compare_sums(x)

### output:
# [Row 4] 257.86065673828125 != 257.8606872558594
# [Row 5] 259.1666259765625 != 259.1666564941406
# [Row 7] 250.88583374023438 != 250.8858642578125
# ...
###
</code></pre>
<p>Note that the problem goes away with CPU tensors or double tensors</p>
","0","Answer"
"79476829","79475046","<p>Short answer:</p>
<p>The bias vector of your final layer plays no part in computing <code>d_loss_params</code>, hence the error. You can get around this by using <code>allow_unused=True</code>. This will result in the final bias vector having <code>None</code> for the gradient.</p>
<pre><code>d_loss_params = grad(loss, model.parameters(), retain_graph=True, allow_unused=False)
</code></pre>
<p>Long answer:</p>
<p>Take your example network with <code>bias=True</code> for both linear layers:</p>
<pre><code>model = nn.Sequential(nn.Linear(1, 10, bias=True), nn.Tanh(), nn.Linear(10, 1, bias=False))
</code></pre>
<p>Say our input is <code>x</code>. Then we have:</p>
<pre><code>x1 = a1 * x + b1  # first linear layer
x2 = tanh(x1)     # tanh activation
x3 = a1 * x2 + b2 # second linear layer
</code></pre>
<p>In your loss, you compute the gradient of <code>x3</code> with respect to <code>x</code>. Evaluate the gradient using the chain rule:</p>
<pre><code>d(x3)/d(x) = (d(x3)/d(x2)) * (d(x2)/d(x1)) * (d(x1)/d(x))

d(x3)/d(x2) = a2
d(x2)/d(x1) = 1 - x2**2
d(x1)/d(x) = a1

d(x3)/d(x) = a2 * (1 - x2**2) * a1
</code></pre>
<p>You can also verify this. The result of <code>loss2</code> will be equal to <code>(model[2].weight * (1 - x2.pow(2)) * model[0].weight.T).sum(1).mean()</code></p>
<p>Now substitute <code>x2</code> in the expression:</p>
<pre><code># substitute x2 = tanh(x1) = tanh(a1 * x + b1)
d(x3)/d(x) = a2 * (1 - tanh(a1 * x + b1)**2) * a1
</code></pre>
<p>From this, we can see that <code>d(x3)/d(x)</code> is computed from <code>a2</code> (weight of the final linear layer), <code>a1</code> (weight of first linear layer), <code>b1</code> (bias of first linear layer) and <code>x</code> (input value). Notably, the bias of the final linear layer <code>b2</code> does not appear in the expression.</p>
<p>Now we can see what happens when we try to compute <code>d_loss_params = grad(loss, model.parameters(), retain_graph=True, allow_unused=False)</code></p>
<p>Here <code>loss = (d(x3)/d(x)).mean() = (a2 * (1 - tanh(a1 * x + b1)**2) * a1).mean()</code>. Our <code>model.parameters()</code> list contains <code>a1</code>, <code>b1</code>, <code>a2</code>, and <code>b2</code>. Since <code>b2</code> does not contribute to our loss value, we cannot compute the gradient of the loss with respet to <code>b2</code>, hence the error.</p>
<p>The error can be avoided by setting <code>allow_unused=False</code>, which will return <code>None</code> values for any parameters not participating in the computation of the output value.</p>
","0","Answer"
"79476998","79455504","<p>here is what you need to know:
RUNNING COLAB CODE - <a href=""https://colab.research.google.com/drive/13gP71u_u_Ewx8u7aTwgzSlH0N_k9XBXx?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/13gP71u_u_Ewx8u7aTwgzSlH0N_k9XBXx?usp=sharing</a></p>
<p>you want see attention weights from your phi3 model. first thing: you must tell model to output attentions. usually you do</p>
<pre class=""lang-py prettyprint-override""><code>outputs = model(input_ids, output_attentions=True)
</code></pre>
<p>then outputs.attentions will be tuple with one element per layer. each element is tensor shape (batch, num_heads, seq_len, seq_len) – that is what you expect, a matrix n_tokens x n_tokens per head.</p>
<p>what you did using</p>
<pre class=""lang-py prettyprint-override""><code>model_output = model.model(input_ids)
attention = model_output[-1]
</code></pre>
<p>may or may not be correct – depends on how model.forward is coded. better use output_attentions flag so you get proper attention weights.</p>
<p>about the shape you see, e.g. 1x40x40x15x15 (or 1x12x12x15x15) – this likely means:</p>
<ul>
<li>1 is batch size,</li>
<li>next dimension is number of layers (40 for medium, 12 for mini),</li>
<li>next is number of heads per layer,</li>
<li>and last two are the attention matrices (each head gets a 15x15 attention matrix if you have 15 tokens).</li>
</ul>
<p>if many heads show nearly uniform attention it can be normal – sometimes heads do that, not focusing on any token particularly.</p>
<p>for proper visualization, select one layer and one head like:</p>
<pre class=""lang-py prettyprint-override""><code>attn = outputs.attentions[layer][0, head]  # shape (seq_len, seq_len)
</code></pre>
<p>and then use your plotting code (imshow or matshow) to visualize.</p>
<p>so summary: use model(..., output_attentions=True) to get correct attention, then each attention tensor will be (batch, heads, seq_len, seq_len) – that is the matrix you expect. if you see extra dimensions then check if you are calling the right forward method. and yes, many heads may show uniform distribution – that can be normal in transformer models.</p>
<p>hope this helps, and you can put my code in your colab as is.</p>
<p>note that</p>
<p>When using Hugging Face Transformers, the recommended approach is to run:</p>
<pre class=""lang-py prettyprint-override""><code>outputs = model(
    input_ids=inputs,
    output_attentions=True,
    # possibly also output_hidden_states=True if you want hidden states
)
</code></pre>
<p>Then outputs.attentions will be a tuple with one entry per layer, each entry shaped (batch_size, num_heads, seq_len, seq_len).</p>
<p>If you call model.model(input_ids) directly (as in your code snippet), you might be accessing a lower-level forward function that returns a different structure. Instead, call the top-level model with output_attentions=True. That yields attention shapes more in line with standard Hugging Face conventions.</p>
<p>Ok so basically you want see attention. You pass output_attentions=True when calling model, then get outputs.attentions. That is standard shape (batch, heads, seq_len, seq_len). Then pick layer and head to plot. Some heads look uniform, that is normal. If you do model.model(input_ids) directly, might not give the standard shape. Safer is:</p>
<pre><code># !pip install transformers torch

import torch
import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load tokenizer and model (make sure you have a valid license for the model)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-medium-4k-instruct&quot;)
model = AutoModelForCausalLM.from_pretrained(
    &quot;microsoft/Phi-3-medium-4k-instruct&quot;,  # note: check spelling if you get error
    device_map=&quot;auto&quot;,
    torch_dtype=torch.float16,            # or torch.float32 if preferred
    trust_remote_code=True
)

# Prepare a prompt
prompt = &quot;The quick brown fox jumps over the lazy dog.&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
inputs = inputs.to(&quot;cuda:0&quot;)  # send inputs to cuda

# Run the model with attention outputs enabled
# Make sure to pass output_attentions=True
outputs = model(input_ids=inputs.input_ids, output_attentions=True)

# outputs.attentions is a tuple with one element per layer
# Each element is a tensor of shape (batch_size, num_heads, seq_len, seq_len)
attentions = outputs.attentions

# For example, choose layer 0 and head 0 to visualize
layer = 0
head = 0
attn = attentions[layer][0, head].detach().cpu().numpy()  # shape (seq_len, seq_len)

# Get tokens for labeling the axes
tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])

# Visualize the attention matrix using matplotlib
plt.figure(figsize=(8,8))
plt.imshow(attn, cmap=&quot;viridis&quot;)
plt.colorbar()
plt.xticks(range(len(tokens)), tokens, rotation=90)
plt.yticks(range(len(tokens)), tokens)
plt.title(f&quot;Attention Matrix (Layer {layer}, Head {head})&quot;)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/TpPuo49J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpPuo49J.png"" alt=""COLAB PROOF OF OUTPUT "" /></a></p>
<p>Now you see nice n_tokens by n_tokens matrix. If model has 12 layers, you see 12 in outputs.attentions. If “medium” is 40 layers, you see 40. Each head is shape 15×15 if your input is 15 tokens. Some heads do uniform attention, that is normal. That is basically all.</p>
<p>NOTE -</p>
<p>When you do something like:</p>
<pre><code>model_output = model.model(input_ids)
attention = model_output[-1]
</code></pre>
<p>You’re relying on how the internal forward method organizes its return. Some models do return (hidden_states, present, attentions, ...) but some do not. It’s safer to rely on the official Hugging Face usage:</p>
<pre><code>outputs = model(..., output_attentions=True)
attention = outputs.attentions
</code></pre>
<p>That’s guaranteed to be the standard shape.</p>
","2","Answer"
"79479668","79479038","<p>If you don't want to use <code>torch.masked</code> due to it being in prototype stage, you can use <code>scatter_reduce</code> to aggregate based on sum, prod, mean, amax and amin.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.tensor([
    [1, 2, -1, -1],
    [10, 20, 30, -1]
]).float() # note you'll need to cast to float for this to work

mask = torch.tensor([
    [True, True, False, False],
    [True, True, True, False]
])

rows, cols = mask.nonzero().T

for reduction in ['mean', 'sum', 'prod', 'amax', 'amin']:
    output = torch.zeros(x.shape[0], device=x.device, dtype=x.dtype)
    output = output.scatter_reduce(0, rows, x[rows, cols], reduce=reduction, include_self=False)
    print(f&quot;{reduction}\t{output}&quot;)
    

# # printed output:
# mean  tensor([ 1.5000, 20.0000])
# sum   tensor([ 3., 60.])
# prod  tensor([2.0000e+00, 6.0000e+03])
# amax  tensor([ 2., 30.])
# amin  tensor([ 1., 10.])
</code></pre>
","2","Answer"
"79480723","79475046","<p>This was solved by passing <code>allow_unused=True</code> and <code>materialize_grads=True</code> to <code>grad</code>. That is:</p>
<p><code>d_loss_params = grad(loss, model.parameters(), retain_graph=True, allow_unused=True, materialize_grads=True)</code></p>
<p>See discussion on <a href=""https://discuss.pytorch.org/t/gradient-of-loss-that-depends-on-gradient-of-network-with-respect-to-parameters/217275"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/gradient-of-loss-that-depends-on-gradient-of-network-with-respect-to-parameters/217275</a> for more info.</p>
","0","Answer"
"79483247","79483120","<p>The issue occurs because it incorrectly processes the data before training. The model expects input shape - (batch, channels, frames, height, width). However, your data shape is incorrect when feeding into the CNN. Fix <code>forward</code> method like:</p>
<pre><code>def forward(self, x):
    # x comes as (batch, channels, frames, height, width)
    batch_size = x.size(0)
    
    # this is the exact format, no need to permute
    cnn_out = self.cnn(x)
    
    # reshape for LSTM
    cnn_features = cnn_out.unsqueeze(1)
    
    # pass through
    lstm_out, _ = self.lstm(cnn_features)
    lstm_out = lstm_out[:, -1, :] 
    output = self.fc(lstm_out)
    
    return output
</code></pre>
","2","Answer"
"79483770","79482885","<h2>Solution with <code>conv2d</code></h2>
<p>You can make your life a lot easier by using <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html"" rel=""nofollow noreferrer""><code>conv2d</code></a> rather than <code>conv1d</code>.</p>
<p>Although we use <code>conv2d</code> below, this is still a 1-d convolution (or rather, two 1-d convolutions) effectively, since we apply a 1×<em>n</em> kernel. Thus, we still have all benefits of a separable convolution (in particular, 2·<em>n</em> rather than <em>n</em>² multiplications per pixel for a kernel of length <em>n</em>).</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
from torch.nn.functional import conv2d
np.set_printoptions(precision=3)  # For better legibility: show fewer float digits

def decomposed_conv2d_np(arr, x_kernel, y_kernel):  # From the question
    arr = np.apply_along_axis(lambda x: np.convolve(x, x_kernel, mode='same'), 0, arr)
    arr = np.apply_along_axis(lambda x: np.convolve(x, y_kernel, mode='same'), 1, arr)
    return arr

def decomposed_conv2d_torch(arr, x_kernel, y_kernel):  # Proposed
    arr = arr.unsqueeze(0).unsqueeze_(0)  # Make copy, make 4D for ``conv2d()``
    arr = conv2d(arr, weight=x_kernel.view(1, 1, -1, 1), padding='same')
    arr = conv2d(arr, weight=y_kernel.view(1, 1, 1, -1), padding='same')
    return arr.squeeze_(0).squeeze_(0)  # Make 2D again

ax = np.array([-1.,0.,1.])
stdev = 0.5
kernel = np.exp(-0.5 * np.square(ax) / np.square(stdev)) / (stdev * np.sqrt(2 * np.pi))
array = np.arange(9).reshape((3,3))

print(result_np := decomposed_conv2d_np(array, kernel, kernel))
# [[0.391 1.247 1.837]
#  [2.865 4.112 4.483]
#  [4.728 6.1   6.173]]

array, kernel = torch.from_numpy(array).to(torch.float64), torch.from_numpy(kernel)
print(result_torch := decomposed_conv2d_torch(array, kernel, kernel).numpy())
# [[0.391 1.247 1.837]
#  [2.865 4.112 4.483]
#  [4.728 6.1   6.173]]

assert np.allclose(result_np, result_torch)
</code></pre>
<p>This solution is based on my <a href=""https://stackoverflow.com/a/75029908/7395592"">answer</a> to a related, earlier question that asked for an implementation of a Gaussian kernel in PyTorch.</p>
<h2>Solution with <code>conv1d</code></h2>
<p>Here is the corresponding solution using <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html"" rel=""nofollow noreferrer""><code>conv1d</code></a> instead:</p>
<pre class=""lang-py prettyprint-override""><code>from torch.nn.functional import conv1d
...
def decomposed_conv2d_with_conv1d(a, x_kernel, y_kernel):
    a = a.unsqueeze(1)  # Unsqueeze channels dimension for ``conv1d()``
    a = conv1d(a, weight=y_kernel.view(1, 1, -1), padding='same')  # Use y kernel
    a = a.transpose(0, -1)  # Swap image dims for using x kernel along last dim
    a = conv1d(a, weight=x_kernel.view(1, 1, -1), padding='same')  # Use x kernel
    return a.squeeze_(1).T  # Make 2D again, reestablish original order of dims
</code></pre>
<p>The key ideas here are:</p>
<ul>
<li>We always need to convolve along the last dimension, so before convolving with the appropriate kernel, we need to move the corresponding image dimension there.</li>
<li>For the remaining image dimension, we can &quot;misuse&quot; what <code>conv1d</code> assumes as the batch dimension (dimension 0) to hold its values. What does not work here is using the channels dimension (dimension 1), since we would need to adjust the kernel by repeating it to match the number of channels. We simply keep the channels dimension at 1 here (meaning we have one image channel), but we could use it for the actual image channels if we had a multichannel image (say, RGB).</li>
</ul>
<p>To me, it appears less straightforward than the <code>conv2d</code> solution, since it also involves the reordering of image dimensions. As to performance, I don't know which version is faster and I did not time them. This should be pretty easy to find out; however, what I assume is that performance differences should be negligible.</p>
","3","Answer"
"79485289","79453817","<p>You can test with <code>torch.onnx.export(..., dynamo=True, report=True)</code> using the latest torch-nightly.</p>
","-1","Answer"
"79485369","79484953","<p>Here is an implementation where the duplicate rows in a binary sparse matrix are identified.
It returns a mask of the rows to keep from the sparse matrix, but can easily be adjusted to give e.g. indices of duplicate rows.
It also handles cases where 3 or more rows are duplicates of each other and only keeps 1 row per group (the lowest index row is always kept for simplicity).</p>
<pre class=""lang-py prettyprint-override""><code>def get_unique_row_mask_sparse(A):
    # Number of matching 1s between each pair of rows
    B = A @ A.T
    
    # Number of 1s in each row
    row_sums = torch.sparse.sum(A, dim=1).to_dense()
    
    indices = B.indices()
    i, j = indices[0], indices[1]
    
    # Two rows i and j are duplicates if:
    # 1) B[i,j] == row_sums[i] == row_sums[j]
    # 2) i != j (exclude diagonal)
    # Moreover, we only keep the upper diagonal of the matrix to avoid duplicates 
    same_row_sums = row_sums[i] == row_sums[j]
    matches_equal_sums = B.values() == row_sums[i]
    not_diagonal = i != j
    upper_triangular = i &lt; j

    is_duplicate_pair = same_row_sums &amp; matches_equal_sums &amp; not_diagonal &amp; upper_triangular
    duplicate_pairs = indices[:, is_duplicate_pair]

    # For each duplicate pair (i,j), we keep row i
    keep_mask = torch.ones(A.size(0), dtype=torch.bool)
    for pair_idx in range(duplicate_pairs.size(1)):
        row_i, row_j = duplicate_pairs[:, pair_idx]
        keep_mask[row_j] = False

    return keep_mask
</code></pre>
<p>Testing code:</p>
<pre class=""lang-py prettyprint-override""><code>torch.manual_seed(42)
A = torch.randint(0, 2, (10, 100), dtype=torch.float32).to_sparse()

# Force some duplicate rows for testing
A_dense = A.to_dense()
A_dense[3] = A_dense[1]
A_dense[6] = A_dense[1]
A_dense[9] = A_dense[2]
A = A_dense.to_sparse()

keep_mask = get_unique_row_mask_sparse(A)
print(keep_mask)
</code></pre>
<p>Gives the result:</p>
<pre><code>tensor([ True,  True,  True, False,  True,  True, False,  True,  True, False])
</code></pre>
<p>You can run the following to create a new sparse tensor from this.</p>
<pre class=""lang-py prettyprint-override""><code>A_indices = A.indices()
rows_mask = keep_mask[A_indices[0]]
A_unique = torch.sparse_coo_tensor(
    A_indices[:, rows_mask],
    A.values()[rows_mask],
    (keep_mask.sum().item(), A.size(1))
).coalesce()
</code></pre>
","1","Answer"
"79486390","79486105","<p>Possible sources of non-determinism (including the ones you have tried, but to double-check) might be (roughly in the order of what I would check):</p>
<ol>
<li><strong>Multiple seeds</strong> (such as <code>python.random</code>, <code>numpy.random</code>, <code>pytorch.random</code>);<a href=""https://pytorch-lightning.readthedocs.io/en/1.7.7/api/pytorch_lightning.utilities.seed.html#pytorch_lightning.utilities.seed.seed_everything"" rel=""nofollow noreferrer""><code>pytorch_lightning.utilities.seed.seed_everything</code></a> is an example of more comprehensive seeding</li>
<li><strong>Batch size</strong> - given different GPUs and their maximum batch size, each weights update will be a little bit different, which, if the dataset's examples vary largely, will push the weights in different directions. Same with multi-GPU training and averaging (taking the mean) across them. See <a href=""https://stackoverflow.com/a/26094278/10886420"">mean of all means is not the same as mean of the whole dataset</a> as an example. <strong>This one affects the results for sure</strong></li>
<li>Non-deterministic per-gpu operations - see <a href=""https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms"" rel=""nofollow noreferrer"">PyTorch reproducibility</a> for more information. <strong>NOTE:</strong> Not all algorithms have fully deterministic variants as far as I remember, especially in bleeding-edge library like <code>huggingface</code></li>
<li><strong>Mixed precision training</strong> - might increase differences as the <code>float</code> precision is not enough to handle the updates. Support for different modes also differs per graphic card</li>
<li><strong>Different test/validation sets</strong> - if you take a random subset it might be different given some seeding problems (with, say, <code>python</code> seeding)</li>
<li><strong>Order of samples</strong> - Different sample order due to indeterminism or batch size</li>
<li><strong>Random weights initialization</strong> - might make a difference, but unlikely as you start from the same checkpoint</li>
</ol>
","1","Answer"
"79486534","79484953","<p>I've found a solution that only takes advantage of torch sparse representation and is very efficient in terms of memory computation and memory consumption:</p>
<pre class=""lang-py prettyprint-override""><code># A is the sparse matrix

B = A.T @ A # or A @ A.T depending on the dimension we are working on
num_elements = A.sum(0).to_dense()

duplicates = torch.logical_and(
   B.indices()[0] &lt; B.indices()[1], # Consider only elements over the upper diagonal
   torch.logical_and(
      B.values() == num_elements[B.indices()[0]],
      B.values() == num_elements[B.indices()[1]],
   )
)
duplicate_indices = B.indices()[1, duplicates].unique()
</code></pre>
<p>At this point we can use the generated mask <code>duplicate_indices</code> in order to remove duplicate indices.</p>
<pre class=""lang-py prettyprint-override""><code>unique_indices = A.indices()[:,
   ~torch.isin(
      A.indices()[1],
      duplicate_edges
)]
</code></pre>
<p><code>unique_indices</code> is a sparse representation of the filtered matrix <code>A</code>.</p>
<hr />
<p>Additionally, we can normalize the result to remove unused indices:</p>
<pre class=""lang-py prettyprint-override""><code>_, unique_indices[1] = torch.unique(unique_indices[1], return_inverse=True)
</code></pre>
","2","Answer"
"79490320","79487620","<p>Your question is missing some details but assuming my assumptions are correct:</p>
<blockquote>
<p>I get an error <code>Couldn't open shared file mapping:..</code> when running this code, most likely because the tensor is implicitly being copied to shared memory and second copy does not fit. There is exactly the same error if I call <code>share_memory_()</code> on this tensor explicitly, for the same reason.</p>
</blockquote>
<p>This is correct. You will end up with two tensors:</p>
<ol>
<li>Original CPU tensor (private memory)</li>
<li>Shared-memory tensor (a copy)</li>
</ol>
<p>And as you say, it won't fit.</p>
<p>One approach besides the file thing could be to use multiprocessing's shared_memory e.g.</p>
<pre><code>import torch
import numpy as np
from multiprocessing import shared_memory

tensor_shape = (1024, 1024, 512)
dtype = np.float32
num_elements = np.prod(tensor_shape)

sh_mem = shared_memory.SharedMemory(create=True, size=num_elements * np.dtype(dtype).itemsize)
np_array = np.ndarray(tensor_shape, dtype=dtype, buffer=sh_mem.buf)

# create tensor without actually copying data
tensor = torch.from_numpy(np_array)
</code></pre>
<p>You can do this in chunks, which is probably better e.g.:</p>
<pre><code>chunk_size = 10 * 2**30  # 10GB chunk size 
num_chunks = size // chunk_size

shm_list = [shared_memory.SharedMemory(create=True, size=chunk_size) for _ in range(num_chunks)]
</code></pre>
<p>As further proof of no copying, you can check the base pointer of each:</p>
<pre><code>&gt;&gt;&gt; print(np_array.ctypes.data)
133277195173888
&gt;&gt;&gt; print(tensor.data_ptr())
133277195173888
</code></pre>
<p>and they should match up.</p>
","4","Answer"
"79492086","79492085","<p>Are far as I can tell, there is no API that contains the information about which PyTorch modules are ONNX exportable. So, I've written a function that tries exporting a given module (wrapped in a quantizable class, as I'm interested whether a quantizable version is available), and checks if a <code>NotImplementedError</code> occurs. In my case I am interested in whether activation function modules are available, so I have:</p>
<pre class=""lang-py prettyprint-override""><code>from tempfile import NamedTemporaryFile
from typing import Type

import torch


def has_quantized_onnx_op(model: Type[torch.nn.Module], inputs, *args, **kwargs) -&gt; bool:
    &quot;&quot;&quot;
    Test if a {class}`torch.nn.Module` has a quantized ONNX op available.

    :param model: The module to test.
    :param inputs: Dummy input to the module.
    :param *args: Any positional arguments for the module initialisation.
    :param **kwargs: Any keyword arguments for the module initialisation.
    &quot;&quot;&quot;

    class _QModel(torch.nn.Module):
        &quot;&quot;&quot;
        Dummy class that wraps a module in quant/dequant stubs.
        &quot;&quot;&quot;
        
        def __init__(self, model, *args, **kwargs):
            super().__init__()
            self.quant = torch.ao.quantization.QuantStub()
            self.dequant = torch.ao.quantization.DeQuantStub()
            self.model = model(*args, **kwargs)

        def forward(self, inputs):
            inputs = self.quant(inputs)
            inputs = self.model(inputs)
            return self.dequant(inputs)

    opset = kwargs.pop(&quot;optset_version&quot;, 18)

    # create an instance of the (wrapped) module
    qmodel = _QModel(model, *args, **kwargs)

    # default quantisation config
    qmodel.qconfig = torch.ao.quantization.get_default_qconfig('x86')
    qmodel_prep = torch.ao.quantization.prepare(qmodel)
    qmodel_prep(inputs)
    qmodel_quant = torch.ao.quantization.convert(qmodel_prep)

    # try exporting
    try:
        with NamedTemporaryFile() as fp:
            torch.onnx.export(qmodel_quant, args=(inputs, ), f=fp.name, opset_version=opset)
    except NotImplementedError:
        return False

    return True


# test a couple of activation function modules
modules = [torch.nn.ReLU, torch.nn.SiLU]
for mod in modules:
    if has_quantized_onnx_op(mod, torch.randn(1, 100)):
        print(f&quot;Hooray! A quantized {mod.__name__} is supported by ONNX export&quot;)
    else:
        print(f&quot;Booo! A quantized {mod.__name__} is not supported by ONNX export&quot;)
</code></pre>
<pre class=""lang-none prettyprint-override""><code>Hooray! A quantized ReLU is supported by ONNX export
Booo! A quantized SiLU is not supported by ONNX export
</code></pre>
","0","Answer"
"79494162","79493986","<p>The error message suggests PyTorch can't compute gradients. You need to ensure your data collator creates proper labels for the loss calculation.</p>
<p>Add <code>return_tensors=&quot;pt&quot;</code> parameter to this line. It returns PyTorch <code>torch.Tensor</code> objects, ensuring you can use loss function</p>
<pre><code>data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=&quot;pt&quot;),
</code></pre>
","1","Answer"
"79501648","79501609","<p>You can get the class of the top-level container of the network using the <code>.__class__</code> attribute.  To get just the name, use <code>.__class__.__name__</code>.</p>
<pre><code>print(p.__class__.__name__)
for name, module in p.named_children():   
    print(f'   {name:&lt;2} {module}')
</code></pre>
<p>Prints:</p>
<pre class=""lang-none prettyprint-override""><code>Sequential
  0   Linear(in_features=22, out_features=3, bias=True)
</code></pre>
","1","Answer"
"79501727","79501647","<p>You can do this:</p>
<pre><code>def first_occurrence_indices(t):
    max_val = torch.max(t) + 1
    a = torch.full((max_val,), -1, dtype=torch.long, device=t.device)

    # Get the first occurrence index using `scatter_reduce`
    indices = torch.arange(len(t), device=t.device)
    a.scatter_reduce_(0, t, indices, reduce=&quot;amin&quot;, include_self=False)

    return a
</code></pre>
<p>Now you can use it like so:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([2, 1, 3, 2, 3, 0, 1, 5], device=&quot;cuda&quot;)
&gt;&gt;&gt; first_occurrence_indices(t)
tensor([ 5,  1,  0,  2, -1,  7])
</code></pre>
<p>Notice that &quot;4&quot; is not in our tensor, so the index is still the default fill value of -1.</p>
<p>That said, I would test to see how efficient <code>scatter_reduce</code> is with your full tensors.</p>
","1","Answer"
"79502327","79501962","<p>It looks like lightning creates a different runid for each invocation. However, if I noticed if I start a run (in my case by logging hyperparameters):</p>
<pre class=""lang-py prettyprint-override""><code>mlflow_logger = MLFlowLogger(
    experiment_name=MLFLOW_EXPERIMENT_NAME,
    tracking_uri=MLFLOW_TRACKING_URI,
    run_name=MLFLOW_RUN_NAME,
)
mlflow_logger.log_hyperparams(hyperparams)
</code></pre>
<p>then lightning reuses the same runId for emitting metrics for fit and test stages.</p>
","1","Answer"
"79504165","79502752","<p>You can check out information on the specific model <a href=""https://llm.extractum.io/model/microsoft%2FOrca-2-13b,61363fW16x75Cre5MxWM0A"" rel=""nofollow noreferrer"">here</a>. But you can see it requires <code>52.1 GB</code> of VRAM (GPU memory).</p>
<p>Based on <a href=""https://instances.vantage.sh/aws/ec2/g5.8xlarge"" rel=""nofollow noreferrer"">this table</a> we see that you have <code>24GB</code> of GPU memory. So it won't be able to fit. If you aren't able to get more GPU memory, you can look into quantized models.</p>
<p>You can check out the models on <a href=""https://huggingface.co/TheBloke/Orca-2-13B-GGUF"" rel=""nofollow noreferrer"">huggingface</a> that have quantized versions, the GPU memory required, and the best use case.</p>
","2","Answer"
"79505907","79502682","<p><em>The following is a summary from the comments to the question:</em></p>
<p>In earlier versions of <code>plotly</code>, its <code>Data</code> class had a <code>heatmapgl</code> property (see e.g. an earlier version of <code>Data</code>'s source code <a href=""https://github.com/plotly/plotly.py/blob/3678aa925489b9ed429dc28863040dbb391dadb1/plotly/graph_objs/layout/template/__init__.py#L242-L259"" rel=""nofollow noreferrer"">here</a>), while in the most recent versions, this property has been removed (see the current version of <code>Data</code>'s source code <a href=""https://github.com/plotly/plotly.py/blob/e3f8c1aacdb0b99632dfa46da9a4fe31905bb7af/plotly/graph_objs/layout/template/_data.py"" rel=""nofollow noreferrer"">here</a>).</p>
<p>The problem and its solution thus unfolded as follows:</p>
<ul>
<li>PyTorch experimental results, including instances of the <code>Data</code> class, were saved from a Python environment with an earlier version of <code>plotly</code> (v5.23.0 in this case), where the <code>heatmapgl</code> property was still present.</li>
<li>When trying to load these results, the <code>plotly</code> version had already been updated (to v6.0.0 in this case) and the <code>heatmapgl</code> property was already gone, so unpickling/unserializing an instance of the <code>Data</code> class resulted in the reported error (<em>ValueError: Invalid property specified for ….Data: 'heatmapgl'</em>).</li>
<li>The problem was solved by downgrading <code>plotly</code> to its previously used version (v5.23.0).</li>
</ul>
<p>My personal experience and advice for avoiding such a problem (which, as we have seen, essentially boils down to the problem of unpickling/unserializing an instance of a class whose definition has changed between pickling/serializing and unpickling/unserializing) is as follows: I would try to save only instances of built-in Python classes (numbers, strings, tuples, lists, dicts, etc.) as well as PyTorch tensors in PyTorch checkpoints, and try to recreate other instances from values stored in these instances.</p>
","1","Answer"
"79507056","79477154","<p>This error was resolved after I reworked my sampling method (see <a href=""https://stackoverflow.com/q/79504951/29845087"">PyTorch Checkpointing Error: Recomputed Tensor Metadata Mismatch in Global Representation with Extra Sampling</a>); it had to do with the fact that I wasn't computing a separate global representation through the forward pass.</p>
<p>Something I could have never figured out based on the traceback I got, as this mentioned nothing related to the sampling part of the code.</p>
","1","Answer"
"79516751","79484474","<p>According to the information provided on OFA github(<a href=""https://github.com/mit-han-lab/once-for-all/tree/master"" rel=""nofollow noreferrer"">https://github.com/mit-han-lab/once-for-all/tree/master</a>), you can use your own model, but you need to make sure that the way you construct your model conforms to the OFA reading method.</p>
<p>I would recommend modifying it through MobileNetV3(<a href=""https://github.com/mit-han-lab/once-for-all/blob/master/ofa/imagenet_classification/networks/mobilenet_v3.py"" rel=""nofollow noreferrer"">https://github.com/mit-han-lab/once-for-all/blob/master/ofa/imagenet_classification/networks/mobilenet_v3.py</a>) in OFA github.</p>
<p>Please refer to it. Thank you.</p>
","1","Answer"
"79521598","79494100","<p>In your example specifically, you set <code>attn_implementation=&quot;eager&quot;</code> in 1st AutoModelForCausalLM (the config you save), but not to the 2nd AutoModelForCausalLM (from which you actually load the weights).</p>
<pre><code>model_auto = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;, #&lt;---- You set this here
    torch_dtype=dtype
).cuda()
config = model_auto.config

# Later...
model_auto_temp = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=dtype
)

model_llama.load_state_dict(model_auto_temp.state_dict())
</code></pre>
<p>One of those “Auto” calls picks up a different default than the other. That can lead to differences in logits.</p>
<p><strong>Pass identical arguments to every load</strong></p>
<p>If you rely on AutoModelForCausalLM for everything:</p>
<pre><code>model_auto_1 = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
).cuda()

config = model_auto_1.config

# Ensure 2nd time also uses the same attn_implementation etc.
model_auto_2 = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
)

model_llama = LlamaForCausalLM(config).cuda()
model_llama.eval()

model_llama.load_state_dict(model_auto_2.state_dict())
</code></pre>
<p>Now both auto calls have same arguments.</p>
<p><strong>Skip the intermed. model</strong></p>
<p>Let LlamaForCausalLM do the work, LlamaForCausalLM supports from_pretrained:</p>
<pre><code>#  A) &quot;Auto&quot; way
model_auto = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
).cuda()

#  B) Direct Llama way
model_llama = LlamaForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
).cuda()
</code></pre>
<p>Now both read the same config plus the same checkpoint weights without you having to do any manual .state_dict() copy.</p>
<p>If you compare their outputs:</p>
<pre><code>out_auto = model_auto(**inputs).logits
out_llama = model_llama(**inputs).logits
diff = (out_auto - out_llama).abs().max()
print(diff.item())
</code></pre>
<p>…you should see almost no differences</p>
","0","Answer"
"79544205","79418845","<p>I dont see layer normalization in your code. Layer normalization ensures that the inputs to each layer have zero mean and unit variance. By normalizing the activations before they are passed to the next layer, layer normalization can limit the effect of large activations, preventing them from propagating as excessively large gradients.</p>
<p>you could applied <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">pytorch's LayerNorm</a> before each major layer. this is called Pre-LayerNorm (the more modern approach), where normalization is applied before the attention and feedforward layers.</p>
","0","Answer"
"79592294","79419018","<p>Regarding the output shape of your YOLOv8 detection model being <strong>(1, 7, 8400)</strong> for 3 classes, instead of perhaps what you might have expected, this is actually the correct and <strong>expected raw output format for YOLOv8 before post-processing.</strong></p>
<p>Let's break down the meaning of this shape:</p>
<ul>
<li><p><strong>1</strong>: Represents the <strong>batch size</strong>. It's typically `1` for single-image inference.</p>
</li>
<li><p><strong>7</strong>: This dimension contains all the relevant information for each prediction location. For a detection task with <em>3</em> classes, this <em>7</em> is the sum of the prediction scores for the <strong>3 classes</strong> plus the <strong>&quot;4 parameters for each bounding box&quot; <em>(x, y, width, height)</em></strong>. Thus, `7 = 3 (number of classes) + 4 (bounding box parameters)`. Each of the `8400` locations outputs these `7` values.</p>
</li>
<li><p><strong>8400</strong>: Represents the total number of **potential detection locations** considered across all output levels (different scales) by the model. YOLOv8 makes predictions on feature maps of different sizes, and `8400` is the flattened total count of these prediction locations.</p>
</li>
</ul>
<p>Contrast this with the standard <strong>YOLOv</strong>8 detection model (trained on 80 COCO classes), whose raw detection output shape is typically <strong>(1, 84, 8400)</strong>. Here, `<strong>84</strong>` also follows the same pattern: `<strong>80 (number of classes) + 4 (bounding box parameters) = 84</strong>`. This further confirms that the output dimension structure is &quot;<strong>number of classes + 4</strong>&quot;.</p>
<p>This <em>(1, 7, 8400)</em> tensor is the <strong>raw prediction result</strong> generated by the YOLOv8 model after the network layers. It still needs to go through **post-processing steps**, such as confidence thresholding and <strong>Non-Maximum Suppression (NMS)</strong>, to obtain the final list of detected bounding boxes (e.g., each detection including location, confidence, class ID, etc.). The final detection results you typically work with are the output after these post-processing steps, not this raw <em>(1, 7, 8400)</em> tensor itself.</p>
<p>Please note that within the YOLOv8 model family, the output shapes for different tasks (such as detection vs. segmentation) are different. For example, the output of a <strong>YOLOv8 segmentation model (like YOLOv8n-seg)</strong> might include a tensor with a shape like <em>(1, 116, 8400)</em> (combining classes, box parameters, and mask coefficients) and another output for prototype masks. This also illustrates that the output shape structure is determined by the specific task and configuration of the model.</p>
","1","Answer"
"79600867","79495211","<p>I don't think Pytorch 2.6.0 is out officially and that might cause problems. For your system configuration you can try the following install instead:</p>
<pre><code>pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu118
</code></pre>
","0","Answer"
"79504921","79504403","<p>The RAM usage you are seeing is caused by loading various CUDA libraries, not from the tensor itself. When you first use CUDA, Pytorch lazily loads CUDA libraries into RAM. You can verify this with the code below (RAM usage numbers are what I got for my system, you will probably get different numbers but the overall point should be the same):</p>
<pre><code>import os
import psutil
import torch
import time 

def getram(): 
    print(psutil.Process(os.getpid()).memory_info().rss / 1024**2)

device = 'cuda:0'
    
# get baseline ram
getram()
&gt; 331.7734375

# create first cuda tensor
# this causes a large RAM increase due to loading CUDA libraries
tmp = torch.zeros(1, device=device)
time.sleep(0.1)
getram()
&gt; 1251.25

# create dataset on GPU
dataset = torch.zeros((128,3,256,256), device=device)

# Slight RAM increase but mostly unchanged
getram()
&gt; 1252.203125
</code></pre>
<p>Note that the <code>time.sleep(0.1)</code> is there because I found running <code>getram</code> right after allocating <code>tmp</code> would sometimes return a value while CUDA libs were still loading (ie running <code>getram</code> again right after without allocating any other values would yield a different result). The sleep is to ensure the libs are fully loaded.</p>
","1","Answer"
"79508065","79507748","<p>From the pytorch <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>All Tensors that have <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"" rel=""nofollow noreferrer"" title=""torch.Tensor.requires_grad""><code>requires_grad</code></a> which is <code>False</code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"" rel=""nofollow noreferrer"" title=""torch.Tensor.requires_grad""><code>requires_grad</code></a> which is <code>True</code>, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so <code>grad_fn</code> is None.</p>
</blockquote>
<p>For your first example:</p>
<pre><code>b = torch.rand(10).cuda()
b.is_leaf  # True
</code></pre>
<p><code>b</code> has <code>requires_grad=False</code>, so it is considered a leaf tensor.</p>
<p>Your second example:</p>
<pre><code>b = torch.rand(10, requires_grad=True)
b.is_leaf  # True
</code></pre>
<p><code>b</code> is created by the user and has <code>requires_grad=True</code>, so it is a leaf tensor.</p>
<p>Your third example:</p>
<pre><code>b = torch.rand(10, requires_grad=True).cuda()
b.is_leaf  # False
</code></pre>
<p>Here it returns False because creating the tensor and moving the tensor to the GPU are two separate operations. The leaf tensor is the original tensor <code>torch.rand(10, requires_grad=True)</code>. Calling <code>.cuda()</code> is a separate operation that returns a new tensor.</p>
<p>From the pytorch documentation:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)
&gt;&gt;&gt; a.is_leaf
True
&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()
&gt;&gt;&gt; b.is_leaf
False
# b was created by the operation that cast a cpu Tensor into a cuda Tensor
</code></pre>
<p>This can be seen more explicitly by inspecting other tensor attributes:</p>
<pre><code>b = torch.rand(10, requires_grad=True)
b.data_ptr()
&gt; 1934898880
b.grad_fn
&gt; None
b.is_leaf
&gt; True

c = b.cuda()
c.data_ptr()
&gt; 140481214283776 # different data pointer
c.grad_fn
&gt; &lt;ToCopyBackward0 at 0x7fc5f44839a0&gt; # has grad function
c.is_leaf # is no longer a leaf tensor
&gt; False
</code></pre>
<p>In the above, <code>b</code> is created by the user and has <code>requires_grad=True</code>, so it is a leaf tensor. Consistent with this, <code>b</code>'s <code>grad_fn</code> is <code>None</code>.</p>
<p>We create <code>c = b.cuda()</code> which creates a new tensor object. We can verify this by seeing it has a different <code>data_ptr</code>. <code>c</code> has <code>grad_fn=ToCopyBackward0</code>, which is how pytorch will backward through the device switch operation.</p>
<p>Finally, when you use <code>b = torch.rand(10, requires_grad=True, device='cuda')</code>, you are creating <code>b</code> directly on the GPU in a single operation, so it is a leaf tensor.</p>
","2","Answer"
"79509004","79508526","<p>Depending on what exactly you mean by <em>without disrupting the current behavior of the other pip install commands</em>, the following may or may not be a viable solution for the <code>- pip</code> section of your <code>environment.yml</code> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>channels:
  - nvidia
  - defaults
  - conda-forge
dependencies:
  - pip:
    - --pre
    - --extra-index-url https://download.pytorch.org/whl/nightly/cu128
    - xgboost==2.1.4
    - torch
    - torchvision
    - torchaudio
</code></pre>
<h2>Why I think this should work in your case</h2>
<ul>
<li>The <code>--pre</code> flag is a <a href=""https://pip.pypa.io/en/stable/reference/requirements-file-format/#global-options"" rel=""nofollow noreferrer"">global option</a> for pip requirements, enabling the installation of pre-release versions. However, since you explicitly specified the exact version of all other shown packages (<code>xgboost==2.1.4</code>, in this case), it will only apply to those without an exact version specified (<code>torch</code>, <code>torchvision</code>, <code>torchaudio</code>, in this case).</li>
<li>Using <code>--extra-index-url</code> instead of <code>--index-url</code> (which are also both global options) will ensure that (a) packages not present in the specified URL will be installed from PyPI (i.e. from the default index), (b) packages present in the specified URL will be potentially installed from there, including pre-release versions since we specified the <code>--pre</code> flag.</li>
</ul>
<p>Using the corresponding YAML file and installing it with <code>conda</code> leads to the following packages being installed via <code>pip</code> in my case:</p>
<pre class=""lang-none prettyprint-override""><code>filelock                  3.18.0                   pypi_0    pypi
fsspec                    2025.3.0                 pypi_0    pypi
jinja2                    3.1.6                    pypi_0    pypi
markupsafe                3.0.2                    pypi_0    pypi
mpmath                    1.3.0                    pypi_0    pypi
networkx                  3.4.2                    pypi_0    pypi
numpy                     2.2.3                    pypi_0    pypi
nvidia-cublas-cu12        12.8.3.14                pypi_0    pypi
nvidia-cuda-cupti-cu12    12.8.57                  pypi_0    pypi
nvidia-cuda-nvrtc-cu12    12.8.61                  pypi_0    pypi
nvidia-cuda-runtime-cu12  12.8.57                  pypi_0    pypi
nvidia-cudnn-cu12         9.7.1.26                 pypi_0    pypi
nvidia-cufft-cu12         11.3.3.41                pypi_0    pypi
nvidia-cufile-cu12        1.13.0.11                pypi_0    pypi
nvidia-curand-cu12        10.3.9.55                pypi_0    pypi
nvidia-cusolver-cu12      11.7.2.55                pypi_0    pypi
nvidia-cusparse-cu12      12.5.7.53                pypi_0    pypi
nvidia-cusparselt-cu12    0.6.3                    pypi_0    pypi
nvidia-nccl-cu12          2.25.1                   pypi_0    pypi
nvidia-nvjitlink-cu12     12.8.61                  pypi_0    pypi
nvidia-nvtx-cu12          12.8.55                  pypi_0    pypi
pillow                    11.1.0                   pypi_0    pypi
pytorch-triton            3.3.0+git96316ce5          pypi_0    pypi
scipy                     1.15.2                   pypi_0    pypi
sympy                     1.13.3                   pypi_0    pypi
torch                     2.8.0.dev20250313+cu128          pypi_0    pypi
torchaudio                2.6.0.dev20250313+cu128          pypi_0    pypi
torchvision               0.22.0.dev20250313+cu128          pypi_0    pypi
typing-extensions         4.12.2                   pypi_0    pypi
xgboost                   2.1.4                    pypi_0    pypi
</code></pre>
<p>So, the only pre-release versions are indeed the ones for the PyTorch packages and their dependencies.</p>
<h2>Why this solution might still be problematic</h2>
<p>As noted above, the <code>--pre</code> flag is a global option. So if by <em>without disrupting the current behavior of the other pip install commands</em>, you mean that the <code>--pre</code> flag should only be applied to the PyTorch packages and their potential dependencies, this won't work unless all other packages and their dependencies are specified explicitly in a way that does not allow for pre-release versions.</p>
<p>Alternatively you could, of course, specify the exact pre-release version of the PyTorch packages and dump the <code>--pre</code> flag, but I guess that would defy the purpose of what you currently achieve with your <code>pip install</code> call (namely, installing the latest nightlies without exactly specifying them).</p>
<h2>What did <em>not</em> work</h2>
<p>I tried some other approaches that turned out to fail:</p>
<h3>Leaving out the <code>--pre</code> flag entirely</h3>
<p>You might have noticed that the call …</p>
<pre class=""lang-sh prettyprint-override""><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
</code></pre>
<p>… will still install the pre-release versions of <code>torch</code>, <code>torchvision</code>, and <code>torchaudio</code>, even though the <code>--pre</code> flag is omitted. This may appear counter-intuitive at first, since the <a href=""https://pip.pypa.io/en/stable/cli/pip_install/#pre-release-versions"" rel=""nofollow noreferrer""><code>pip</code> docs</a> explicitly state:</p>
<blockquote>
<p>Starting with v1.4, pip will only install stable versions as specified by <a href=""https://www.python.org/dev/peps/pep-0440/#handling-of-pre-releases"" rel=""nofollow noreferrer"">pre-releases</a> by default.</p>
</blockquote>
<p>However if we follow the link from the citation, we will find there:</p>
<blockquote>
<p>Pre-releases of any kind … are implicitly excluded from all version specifiers, <em>unless</em> … the only available version that satisfies the version specifier is a pre-release.</p>
</blockquote>
<p>This means:</p>
<ul>
<li><p>If we <em>only</em> specify <code>--index-url https://download.pytorch.org/whl/nightly/cu128</code>, then the pre-release versions of the PyTorch packages <em>will be</em> installed even without the <code>--pre</code> flag, because under the given URL, only pre-releases can be found, so each requirement can only be satisfied by installing a pre-release.</p>
<p>The problem here: Packages that cannot be found under this URL (<code>xgboost</code> in the given case) cannot be installed at all.</p>
</li>
<li><p>If we specify <code>--extra-index-url https://download.pytorch.org/whl/nightly/cu128</code> instead, and don't provide the <code>--pre</code> flag, then <code>xgboost</code> can be installed.</p>
<p>The problem here: Now <em>release</em> versions of the PyTorch packages will be installed, since they can be found on PyPI and thus the pre-release versions are no longer the only option.</p>
</li>
</ul>
<h3>Providing two separate <code>- pip</code> sections in the <code>environment.yml</code> file</h3>
<p>What <code>conda</code> does when installing the <code>pip</code> dependencies, is essentially (1) creating a temporary <code>requirements.txt</code> file from the <code>- pip</code> section of the YAML file, then (2) installing this <code>requirements.txt</code> file via a <code>pip install -r …</code> call. What I thought could work would be creating two separate <code>- pip</code> sections – one for your &quot;regular&quot; <code>pip</code> dependencies and one for the PyTorch packages. I thought that then perhaps <code>pip install -r …</code> would be called separately for each section.</p>
<p>However, all but the last such <code>- pip</code> section in the YAML file seem to be ignored.</p>
<h3>Referencing an external <code>requirements.txt</code> file</h3>
<p>I also tried to specify the following <code>torch_requirements.txt</code> file:</p>
<pre class=""lang-none prettyprint-override""><code>--index-url https://download.pytorch.org/whl/nightly/cu128
torch
torchvision
torchaudio
</code></pre>
<p>This should work on its own (in particular, it should install the pre-release versions even without specifying the <code>--pre</code> flag; see above). I then tried to include this in the YAML file as:</p>
<pre class=""lang-yaml prettyprint-override""><code>...
dependencies:
  - pip:
    - xgboost==2.1.4
    - -r torch_requirements.txt
</code></pre>
<p>However, while working in principle (i.e. incorporating the contents of the referenced file), this again made the <code>--index-url</code> apply <em>globally</em>, so prevented to install packages like <code>xgboost</code> from PyPI, as described above.</p>
<h2>Bottom line</h2>
<p>With the way that <code>conda</code> currently handles <code>pip</code> packages, I don't think there is a universally applicable solution. The particular problem, in my opinion, is that there is no way (at least I did not find any) to trigger <em>more than one separate</em> <code>pip install …</code> call. The latter, however, would be necessary to apply different global <code>pip</code> requirement options (such as <code>--pre</code> or <code>--index-url</code>) for different sets of <code>pip</code> packages.</p>
","1","Answer"
"79509396","79509349","<p>I should have been using <code>metrics.BinaryAUROC</code> (<a href=""https://pytorch.org/torcheval/stable/generated/torcheval.metrics.BinaryAUROC.html"" rel=""nofollow noreferrer"">doc</a>) not <code>metrics.AUC</code>. I think <code>AUC</code> is for when you already have the coordinates of the ROC.</p>
<p>Having different arguments for <code>AUC.update()</code> i.e., (x,y) instead of (input, target) like <code>BinaryAccuracy</code> should have been a giveaway that <code>AUC</code> wasn't what I wanted.</p>
<pre><code>from torcheval.metrics import BinaryAUROC
auc_metric2 = BinaryAUROC()
auc_metric2.reset()
auc_metric2.update(input=p_pred,target=y_true)
print(f&quot;TorchEval AUC      = {auc_metric2.compute().item():.3}&quot;)
</code></pre>
<p>returns the expected result.</p>
","1","Answer"
"79511818","79511518","<p>From the <a href=""https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/runtime/engine.py#L1900"" rel=""nofollow noreferrer"">source code</a>:</p>
<pre><code>class DeepSpeedEngine(Module):
    r&quot;&quot;&quot;DeepSpeed engine for training.&quot;&quot;&quot;
    ...

    def eval(self):
        r&quot;&quot;&quot;&quot;&quot;&quot;

        self.warn_unscaled_loss = True
        self.module.train(False)
</code></pre>
<p>The <code>eval</code> method updates the internal <code>train</code> status of the model but does not return anything. This is different from the standard Pytorch <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval"" rel=""nofollow noreferrer"">eval</a> code that returns the model itself.</p>
<p>This means <code>self.model.eval()</code> sets the model to eval mode internally, but returns <code>None</code>. This means that when you assign the output of <code>self.model.eval()</code> to <code>self.model</code> via <code>self.model = self.model.eval()</code> , you are essentially running <code>self.model = None</code>.</p>
<p>You can change your code to:</p>
<pre><code>def evaluate(self, epoch_num=None, keep_all=True):
        print(&quot;self.model:&quot;, self.model)

        self.model.eval() # simply call `eval`, no assignment necessary
        print(&quot;self.model after eval:&quot;, self.model)
</code></pre>
<p>Note that this also works for standard pytorch models - <code>eval</code> primarily updates the internal state of the model object, so reassigning the model object to the same variable name is unnecessary both for the <code>DeepSpeedEngine</code> model and standard pytorch models.</p>
","1","Answer"
"79512507","79512485","<p>I have found the root issue and a resolution. PyTorch currently works up to Python 3.12.x and I have Python 3.13.x. This is why I couldn't install it using pip.</p>
<p>Resolution: downgrade to using Python 3.12.9 worked for me.</p>
<p>Specifically, I used pyenv to manage multiple versions of my Python on my mac.</p>
<pre><code>arch -x86_64 pyenv install 3.12.9
pyenv global 3.12.9
</code></pre>
<p>NOTE: my mac has an <code>arm64</code> architecture, so I had to specify <code>arch -x86_64</code> (you can find out your mac arch by typing <code>arch</code> in your command line.</p>
<p>You can install pyenv with brew if you don't have it.</p>
<pre><code>brew install pyenv
echo 'eval &quot;$(pyenv init -)&quot;' &gt;&gt; ~/.zprofile
source ~/.zprofile
</code></pre>
<p>NOTE: Depends on where you have your bash init scripts, mine is .zprofile, you can switch out to yours.</p>
","1","Answer"
"79513668","79513638","<p>Your calculation failed because PyTorch detected a mix of GPU and CPU tensors. Specifically, some of your data is being processed on the graphics card (cuda:0), while other parts remain on the computer's main processor. This mismatch prevents PyTorch from performing operations that require all tensors to be on the same device.<br/><br/>
When you use a GPU with PyTorch, it's identified as <code>cuda:0</code> if it's your only GPU or if you haven't specified otherwise. Your first GPU is always indexed as 0.<br/> To use the GPU, ensure your <code>device</code> settings are set to <code>cuda</code>. If you have multiple GPUs, you can select them by changing the index (e.g., cuda:1, cuda:2). <br/></p>
<pre><code>import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f&quot;Using device: {device}&quot;)
</code></pre>
<p>First, transfer your model to the GPU using <code>model = model.to(device)</code>. Make sure to do this before setting up your optimizer.<br/>Next, ensure your input data (<code>xb</code> and <code>yb</code>) also resides on the GPU by using <code>xb = xb.to(device)</code> and <code>yb = yb.to(device)</code>.<br/><br/>
Maintain device consistency within <code>estimate_loss</code> by ensuring all operations and loaded data reside on the same <code>device</code> as the model.</p>
","1","Answer"
"79516211","79515545","<p>General advice would be to profile. It's hard to say anything without actual measurements.<br />
But I'll try to break things down on how one could approach optimizing this or similar cases.</p>
<h2>1. Use nvidia-smi to check GPU-utilization.</h2>
<p>I typically do <code>watch -n 0 nvidia-smi</code>, this will continuously poll <code>nvidia-smi</code> and you will see your &quot;GPU-utilization&quot; percentage updating in real time. Ideally you want it to be close to 100% as much as possible. If it is not the case, you fail to saturate your GPU with workload.</p>
<p>This is a very basic, but useful and easy-to-get metric.</p>
<p>It is worth noting that even 100% all the time does not mean that GPU actually busy with useful work. The number simply indicates the percentage of time the kernels are active. With DDP training, 100% could be that it just waits for NCCL to transfer gradients. Sometimes low power usage can give you a hint that something isn't right.</p>
<h2>2. Optimize data loading.</h2>
<p>Your network seems to be very small. Its very likely that it is so small that data loading is the bottleneck, and not GPU.</p>
<h3>2.1 Non-blocking transfer to GPU.</h3>
<p>Try removing <code>pin_memory=True</code>. Sometimes, using pinned memory might be slower. Generally, using <code>pin_memory=True</code> and non-blocking transfer to GPU should be faster. However I do not see non-blocking transfer in your code, without it <code>pin_memory=True</code> does not provide any benefit.
In order to do non-blocking transfer, you need to:</p>
<pre><code>x_in = x_in.to(device, non_blocking=True)
x_tgt = x_tgt.to(device, non_blocking=True)
</code></pre>
<p>See: <a href=""https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html</a></p>
<h3>2.2 Try to change number of workers in the dataloader.</h3>
<p>Ideally, you would want as many workers as the degree of parallelism. With that exception when the individual worker runs multiple threads, which you would want to avoid.</p>
<h3>2.3 Try to minimize the amount of data transferred from CPU to GPU.</h3>
<p>You said that images are 0 or 1. Even if you load them as 8bit images, it is much better to return them as such from the dataset. The in your training code you would do:</p>
<pre><code>x_in = x_in.to(device)
x_tgt = x_tgt.to(device)

out = model(x_in.float() / 255.0)
loss = loss_fn(out, x_tgt.float() / 255.0)
</code></pre>
<p>This way you would transfer 4 times less data to GPU.</p>
<h3>2.4 Preload.</h3>
<p>In your case, if that is actual dataset that you plan to use (not just some test case), I would recommend preload it to GPU memory. It isn't too large if bit-packed: <code>(64 * 64) / 8 * 1e6 ~ 512MB</code> which should fit GPU memory. Basically, you would just need to remove the dataloader, generate a permutation of the dataset indices and iterate through it youself and unpack the images from bitmap to float tensors directly on GPU. This approach is significantly more involved, but would largely eliminate data loading issues.</p>
<p>If you find it too difficult, you can always do a simpler thing such as preloading the dataset to a CPU tensor and writing a simple Dataset that wraps it. Then you would just use the DataLoader as you do now. You would not avoid CPU -&gt; GPU transfers, but at least you would avoid IO (and possibly decoding) latency.</p>
<h3>3. Delete gradients, do not zero them out.</h3>
<p>You should be doing <code>optimizer.zero_grad(set_to_none=True)</code>, this will just delete the gradient tensors (which is a virtually free operation) instead of zeroing out them. However, this advice is only useful for older PyTorch versions, as the newer already switched to set_to_none been the default.</p>
<h3>4. Avoid synchronization points.</h3>
<p>Doing <code>loss.item()</code> is a blocking call, which causes Pytorch to wait till all kernels finish to run so that it could transfer the result back to CPU. Try to avoid such operations. You could measure the loss once in a while, e.g. once in 100 iterations.</p>
<h3>5. Try torch.compile.</h3>
<p>Using <code>torch.compile</code> in some cases can significantly speed up things by merging kernels. Since you network is quite small, the overhead of data movement due to non-fused kernels can be significant compared to the actual compute, which means that <code>torch.compile</code> should help if GPU is the bottleneck.</p>
<h3>6. Profile.</h3>
<p>Use <code>torch.autograd.profiler</code> for profiling. For example:</p>
<pre><code>from torch.profiler import profile, ProfilerActivity

def save_trace(profiler) -&gt; None:
    profiler.export_chrome_trace(&quot;trace.json&quot;)

...

def train_func(...):
    
    ...
 
    with profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
        profile_memory=False,
        record_shapes=False,
        schedule=torch.profiler.schedule(
            skip_first=100, wait=1, warmup=5, active=1, repeat=1
        ),
        on_trace_ready=save_trace,
    ) as profiler:
        for ep in range(num_epochs):
            if global_step &gt;= max_steps:
                print(f&quot;Reached {max_steps} total steps; stopping early.&quot;)
                break

            t0 = time.time()

            #############################
            # Train Loop (per epoch)
            #############################
            model.train()
            total_loss = 0.0
            for x_in, x_tgt in train_loader:
                x_in = x_in.to(device)
                x_tgt = x_tgt.to(device)

                optimizer.zero_grad()
                out = model(x_in)
                loss = loss_fn(out, x_tgt)
                loss.backward()
                optimizer.step()
                profiler.step()  # MAKE SURE YOU CALL step!!!

                total_loss += loss.item() * x_in.size(0)

                global_step += 1  # increment step count
                if global_step &gt;= max_steps:
                    print(f&quot;Reached {max_steps} total steps; stopping in mid-epoch.&quot;)
                    break
</code></pre>
<p>Do not forget to call <code>profiler.step()</code>! The resulting traces could be open in chrome using <code>chrome://tracing</code>.
See <a href=""https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html</a></p>
","2","Answer"
"79518829","79518779","<p>This is caused by numerical issues. Running matmul operations on different input shapes yields small but nonzero differences. These errors are compounded over the model's layers, yielding large differences in the final output. The model in question uses <code>bfloat16</code> weights, which is more sensitive to this type of error. As an example:</p>
<pre><code>sizes = [256, 512, 1024, 1536, 2048]

for size in sizes:
    x = torch.randn(32, size, dtype=torch.bfloat16, device='cuda:0')
    layer = torch.nn.Linear(size, size, dtype=torch.bfloat16, device='cuda:0')
    y1 = layer(x[:1])
    y2 = layer(x)
    print(f&quot;{size}, {torch.allclose(y1[0], y2[0])}&quot;)
</code></pre>
<p>The code above compares single vs batch inference of a single linear layer at difference sizes, all using bfloat16. If you run the code multiple times, you will notice the results are not stable. Here is an example output of running several iterations:</p>
<pre><code>256, True
512, True
1024, False
1536, False
2048, False

256, True
512, True
1024, False
1536, True
2048, True

256, True
512, True
1024, True
1536, False
2048, False
</code></pre>
<p>The scale of the numerical error depends not just on the size of inputs, but the specific input/weight values.</p>
<p>For the model in question, you could reduce the error by changing the datatype to <code>float32</code> via <code>torch_dtype=torch.float32</code>, but even then the error will still be nonzero.</p>
","0","Answer"
"79519278","79517641","<p>InsightFace already has a model trained using a large dataset that considers a lot of races, angles, face shapes, etc. The data you built is very small, so unless you are aiming for overfitting, it will be difficult to achieve general performance better than the existing InsightFace model.</p>
<p>For models like InsightFace, it is better to focus on &quot;how to use&quot; those models well. If you want to create a face recognition model specialized for a specific dataset domain, it will be helpful to refer to the github of other face recognition SOTA models and train them.</p>
<p><a href=""https://github.com/deepinsight/insightface/blob/master/recognition/_datasets_/README.md"" rel=""nofollow noreferrer"">https://github.com/deepinsight/insightface/blob/master/recognition/_datasets_/README.md</a></p>
<p>Please refer to this page. According to this page, they use at least 100,000 to 500,000 data as training data. Unless you build a dataset larger than this, it is difficult to judge that it will perform better than InsightFace or avoid overfitting.</p>
","1","Answer"
"79520001","79519918","<p>you could use the function scatter_reduce_ (is in beta): <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html</a></p>
<p>however you have to specify an out tensor (which should have desired shape and initial values):</p>
<p>e.g. for sum aggregation:</p>
<pre><code>out = torch.zeros((index.max() + 1.), dtype=torch.float)
out.scatter_reduce_(dim=0, index=index, src=x, reduce=&quot;sum&quot;)
</code></pre>
<p>or min aggregation:</p>
<pre><code>out = torch.full((index.max() + 1,), torch.inf)
out.scatter_reduce_(dim=0, index=index, src=x, reduce=&quot;amin&quot;)
</code></pre>
","1","Answer"
"79521017","79520329","<p>I managed to create a custom subsampler that works, if you have any suggestions they are welcomed:</p>
<pre><code>#include &lt;torch/torch.h&gt;
#include &lt;optional&gt;
#include &lt;numeric&gt;
#include &lt;vector&gt;
#include &lt;cstring&gt; 

class SubsetSampler : public torch::data::samplers::Sampler&lt;std::vector&lt;size_t&gt;&gt; {
    private: 
    std::vector&lt;size_t&gt; indices_; size_t current_; 

    public:
    // Type alias required by the Sampler interface.
    using BatchRequestType = std::vector&lt;size_t&gt;;

    explicit SubsetSampler(std::vector&lt;size_t&gt; indices)
        : indices_(std::move(indices)), current_(0) {}

    // Reset the sampler with an optional new size.
    // Providing a default argument so that a call with no parameters is allowed.
    void reset(std::optional&lt;size_t&gt; new_size = std::nullopt) override {
        if (new_size.has_value()) {
            if (new_size.value() &lt; indices_.size()) {
                indices_.resize(new_size.value());
            }
        }
        current_ = 0;
    }

    // Returns the next batch.
    std::optional&lt;BatchRequestType&gt; next(size_t batch_size) override {
        BatchRequestType batch;
        while (batch.size() &lt; batch_size &amp;&amp; current_ &lt; indices_.size()) {
            batch.push_back(indices_[current_++]);
        }
        if (batch.empty()) {
            return std::nullopt;
        }
        return batch;
    }

    // Serialize the sampler state.
    void save(torch::serialize::OutputArchive&amp; archive) const override {
        // Convert indices_ to a tensor for serialization.
        torch::Tensor indices_tensor = torch::tensor(
        std::vector&lt;int64_t&gt;(indices_.begin(), indices_.end()), torch::kInt64);
        torch::Tensor current_tensor = torch::tensor(static_cast&lt;int64_t&gt;(current_), torch::kInt64);
        archive.write(&quot;indices&quot;, indices_tensor);
        archive.write(&quot;current&quot;, current_tensor);
    }

    // Deserialize the sampler state.
    void load(torch::serialize::InputArchive&amp; archive) override {
        torch::Tensor indices_tensor, current_tensor;
        archive.read(&quot;indices&quot;, indices_tensor);
        archive.read(&quot;current&quot;, current_tensor);
        auto numel = indices_tensor.numel();
        std::vector&lt;int64_t&gt; temp(numel);
        std::memcpy(temp.data(), indices_tensor.data_ptr&lt;int64_t&gt;(), numel * sizeof(int64_t));
        indices_.resize(numel);

        for (size_t i = 0; i &lt; numel; ++i) {
            indices_[i] = static_cast&lt;size_t&gt;(temp[i]);
        }
        current_ = static_cast&lt;size_t&gt;(current_tensor.item&lt;int64_t&gt;());
    }


};
</code></pre>
<p>Can be used during the loading of the dataset like this:</p>
<pre><code>  auto train_dataset = torch::data::datasets::MNIST(kDataRoot)
                           .map(torch::data::transforms::Normalize&lt;&gt;(0.1307, 0.3081))
                           .map(torch::data::transforms::Stack&lt;&gt;());
  const size_t train_dataset_size = train_dataset.size().value();

  std::vector&lt;size_t&gt; subset_indices(subset_size);
  std::iota(subset_indices.begin(), subset_indices.end(), 0);

  SubsetSampler sampler(subset_indices);
  auto train_loader = torch::data::make_data_loader(
    std::move(train_dataset),
    sampler,
    torch::data::DataLoaderOptions().batch_size(kTrainBatchSize));
</code></pre>
","1","Answer"
"79521977","79516939","<p>The problem has been solved (thanks to dai on discord). The issue is <code>input_precision</code> is tf32 by default for dot product, which has 10bits mantissa - leading to trailing digit loss. The problem was very pronounced with <code>V = torch.arange(4096, 4096 + 2048, device = 'cuda', dtype = torch.float32)</code>, where the output was <code>[6080., 6080., 6080., 6080., 6084., 6084., 6084., 6084., 6088.,...]</code> . Switching to &quot;ieee&quot; input precision <code>tl.dot(x,y, input_precision = &quot;ieee&quot;)</code>  solved the issue.</p>
","1","Answer"
"79525809","79525759","<p>Do you mean to simply hide or solve this warning? If it is hidden,</p>
<pre><code>import warnings

with warnings.catch_warnings():
    warnings.simplefilter(&quot;ignore&quot;, UserWarning)
    model.load_state_dict(state_dict, strict=False)
</code></pre>
","1","Answer"
"79528950","79528937","<p>This happens because your model parameters have very different scales, and optimisers (like least_squares or Adam) can struggle without a good initial guess.</p>
<p>Tips to fix it:</p>
<ul>
<li>Normalise parameters – Scale large values like <code>p1</code> down during fitting.</li>
<li>Use bounds – Guide the optimiser with realistic parameter ranges.</li>
<li>Try global optimisers – Use <code>scipy.optimize.differential_evolution</code> if you don’t have a good initial guess.</li>
<li>Reshape the problem – Sometimes rewriting the function can make it easier to fit.</li>
</ul>
","0","Answer"
"79529307","79528937","<p>Since it most likely gets stuck in the <strong>local minimum</strong>, and the optimization algorithms are very sensitive to the <strong>initial guess</strong>, we can start with <strong>random initialization</strong> <strong>multiple times</strong> and then choose the solution corresponding to minimum cost in all the runs. It increases the chance of obtaining the global minimum, as shown below (global minimum could be found in 20 runs):</p>
<pre><code>def residuals(p, y, x):
    return (y - func(x, p))**2 # squared error 

np.random.seed(10) # for reproducible result

num_runs = 20 
best_init = None
best_cost = np.inf
best_result = None
p0 = np.zeros(3)

for i in range(num_runs):
    p0 = np.random.normal(size=len(p0)) # random init, can use any other distribution
    result = least_squares(residuals, p0, args=(y, x), method='lm', verbose=2) 
    # print(result.cost)
    if best_cost &gt; result.cost:
        best_init = p0
        best_cost = result.cost
        best_result = result

print(best_init, best_cost)
# [ 0.91745894 -0.11227247 -0.36218045] 0.00041268250199614506
result = best_result
</code></pre>
<p><a href=""https://i.sstatic.net/jyBfV2yF.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jyBfV2yF.gif"" alt=""enter image description here"" /></a></p>
<p>We can try different methods (<em><code>trf</code>, <code>dogbox</code>, <code>lm</code></em>) for least squares and also explicitly provide the jacobian too and see if it gets better results.</p>
","0","Answer"
"79529709","79519074","<p>These two bugs stem from a recent change in <code>fastcore</code> 1.8 and the corresponding upgrade of <code>fastai</code> to version 2.8.0 from 2.7.29 affecting <code>load_learner</code> for vision models. The contributors from Answer.ai moved <code>Pipeline</code> to their <code>fasttransform</code> package but <code>fastai</code> still does an <code>import *</code> from <code>.core</code> causing an error to be raised, since the <code>*</code> imports the placeholder <code>transform.py</code> that currently only exists to raise the error in question.</p>
<p>For now, the issue is solved by downgrading <code>fastai</code> to v2.7.x. An issue has been opened on Github and is being addressed by the Answer.ai team.</p>
<p>If using an environment like Colab, you'll need to restart the session after downgrading <code>fastai</code>.</p>
","1","Answer"
"79537768","79537716","<p>Run <code>conda list torch</code> to see if it's installed. Make sure you run this inside your Conda environment.</p>
<p>If you don't see it run:</p>
<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
</code></pre>
<p>Change the version depending on your requirements and what your GPU supports. You can find more info <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">here</a>.</p>
","1","Answer"
"79537825","79537819","<p>PyTorch does not yet offer official support for CUDA 12.8 in its stable releases. You may need to wait for native support or consider installing WSL.</p>
<p>Reference: <a href=""https://discuss.pytorch.org/t/how-to-install-torch-version-that-supports-rtx-5090-on-windows-cuda-kernel-errors-might-be-asynchronously-reported-at-some-other-api-call/216644"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/how-to-install-torch-version-that-supports-rtx-5090-on-windows-cuda-kernel-errors-might-be-asynchronously-reported-at-some-other-api-call/216644</a></p>
","0","Answer"
"79539796","79536891","<p>Both methods are equivalent - change in print-out is just an artifact of how <code>torchinfo</code> crawls the model.</p>
<p><code>torchinfo</code> tracks the model's forward pass, looking at every module involved. If the same module appears more than once, it is labeled <code>recursive</code>. For <code>nn.ModuleList</code> objects, using an item in the same <code>ModuleList</code> at different points of the <code>forward</code> gets flagged as recursive simply because the <code>ModuleList</code> container is showing up more than once in different places. Here's a simple example:</p>
<p>Example 1:</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.ModuleList([nn.Linear(8, 8) for i in range(2)])
        self.l2 = nn.Linear(8,8)
        
    def forward(self, x):
        x = self.l1[0](x)
        x = self.l1[1](x)
        x = self.l2(x)
        return x

m = MyModel()
summary(m, (1, 8), depth=5)

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MyModel                                  [1, 8]                    --
├─ModuleList: 1-1                        --                        --
│    └─Linear: 2-1                       [1, 8]                    72
│    └─Linear: 2-2                       [1, 8]                    72
├─Linear: 1-2                            [1, 8]                    72
==========================================================================================
Total params: 216
Trainable params: 216
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
</code></pre>
<p>Example 2:</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.ModuleList([nn.Linear(8, 8) for i in range(2)])
        self.l2 = nn.Linear(8,8)
        
    def forward(self, x):
        x = self.l1[0](x)
        x = self.l2(x)
        x = self.l1[1](x)
        return x

m = MyModel()
summary(m, (1, 8), depth=5)

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MyModel                                  [1, 8]                    --
├─ModuleList: 1-3                        --                        (recursive)
│    └─Linear: 2-1                       [1, 8]                    72
├─Linear: 1-2                            [1, 8]                    72
├─ModuleList: 1-3                        --                        (recursive)
│    └─Linear: 2-2                       [1, 8]                    72
==========================================================================================
Total params: 216
Trainable params: 216
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
</code></pre>
<p>In the first example, we use all layers in the <code>ModuleList</code> in order, and get no <code>recursive</code> flag. In the second, we use the layers in the <code>ModuleList</code> at different times, and get the <code>recursive</code> flag on the <code>ModuleList</code> object itself. This is just an artifact of how <code>torchinfo</code> crawls the model.</p>
<p>As a purely style-based note, there's nothing wrong with zipping modulelists, but if you know each <code>_ConvBlock</code> will be paired 1-1 with a <code>_DownsampleBlock</code>, you might consider putting them into a combined module</p>
<pre><code>class CombinedBlock(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.conv_block = _ConvBlock(...)
        self.down_block = _DownsampleBlock(...)
    def forward(self, x):
        x = self.conv_block(x)
        skip = x
        x = self.down_block(x)
        return x, skip
</code></pre>
","1","Answer"
"79540434","79540216","<p>This looks to be a numeric issue impacting complex numbers on GPU. Weirdly, it impacts negative numbers but not positive.</p>
<pre><code>values = [5.0, -5.0]
devices = ['cpu', 'cuda']

for device in devices:
    for value in values:
        x = torch.tensor(value+0.j, device=device, dtype=torch.complex128)
        x_pow2 = x.pow(2)
        x_x_conj = x * x.conj()
        check1 = x_pow2 == x_x_conj
        check2 = x_pow2.imag == 0
        check3 = x_x_conj.imag==0
        
        print(f&quot;{device}\t{value}\t{check1.item()}\t{check2.item()}\t{check3.item()}&quot;)

cpu 5.0 True    True    True
cpu -5.0    True    True    True
cuda    5.0 False   True    True
cuda    -5.0    False   False   True
</code></pre>
<p>For a given input <code>x = value + 0.j</code>, the code computes <code>x_pow2 = x.pow(2)</code> and <code>x_x_conj = x * x.conj()</code>. The code checks if <code>x_pow2 == x_x_conj</code>, and if the imaginary component of those tensors is zero.</p>
<p>We see on CPU, <code>x_pow2 == x_x_conj</code> for both input values, and both <code>x_pow2</code> and <code>x_x_conj</code> have a zero imaginary component.</p>
<p>On GPU, we see that in both cases <code>x_pow2 != x_x_conj</code>. For the case of <code>value = 5.0</code>, both <code>x_pow2</code> and <code>x_x_conj</code> have zero imaginary component. For the case of <code>value = -5.0</code>, <code>x_x_conj</code> has zero imaginary component but <code>x_pow2</code> does not.</p>
<p>I've tested this with a few values, and I consistently see inputs with positive real values yielding <code>x_pow2</code> with zero imaginary component, while inputs with negative real values yielding <code>x_pow2</code> with nonzero imaginary component.</p>
<p>This is likely due to something weird happening in the <code>pow</code> kernel that isn't present in the <code>mul</code> kernel.</p>
","0","Answer"
"79542018","79540216","<p>NumPy has a <a href=""https://stackoverflow.com/questions/78122836/difference-between-numpy-power-and-for-certain-values/78130903#78130903"">special case</a> that implements <code>x**2</code> as <code>x*x</code>. For complex <code>x</code>, this uses the straightforward componentwise multiplication formula, which produces a purely real output for purely real inputs.</p>
<p>Apparently Torch isn't doing that. Torch seems to be doing a general <code>complex**real</code> exponentiation. That converts a complex number to angle-and-magnitude representation, then computes <code>magnitude**power</code> and <code>angle*power</code> and converts back to componentwise representation.</p>
<p>The angle of a negative number is pi, which is not exactly representable. So squaring a negative number this way produces rounding error in the angle, which translates to a very small imaginary component in the result when converted back to real and imaginary components.</p>
","0","Answer"
"79543895","79542733","<p>I'll give some intuition for multi-head attention first. Instead of taking the query, key and value embeddings and applying the <a href=""https://paperswithcode.com/method/scaled"" rel=""nofollow noreferrer"">scaled dot product operation</a> on them all at once, mulit-head attention splits the query, key and value vectors into smaller sub-vectors and applies the scaled dot product operation on these sub-vectors in parallel, so that the model during training can &quot;focus&quot; on different parts of the input.</p>
<p>For example,</p>
<pre><code>Single Head Attention
----------------------
Query: [q1 q2 q3 q4 q5 q6]  (dimension = 6)
Key:   [k1 k2 k3 k4 k5 k6]  (dimension = 6)
Value: [v1 v2 v3 v4 v5 v6]  (dimension = 6)

Multi-Head Attention (num_heads = 3)
-------------------------------------
       Head 1  | Head 2  |  Head 3
       (dim=2) | (dim=2) |  (dim=2)
Query: [q1 q2] | [q3 q4] | [q5 q6]
Key:   [k1 k2] | [k3 k4] | [k5 k6]
Value: [v1 v2] | [v3 v4] | [v5 v6]

Each head processes its own subset of the dimensions independently

Head 1: Attention( [q1 q2], [k1 k2] ) → [o1 o2]
Head 2: Attention( [q3 q4], [k3 k4] ) → [o3 o4]
Head 3: Attention( [q5 q6], [k5 k6] ) → [o5 o6]
</code></pre>
<p>So to answer this part of your question,</p>
<blockquote>
<p>How does adding <code>num_heads</code> as a new dimension affect the computation of attention, and what would happen if we skipped this step and kept the shape as &quot;batch_size,sequence_length,d_in&quot;</p>
</blockquote>
<p>This is fine, then it just becomes single head attention and you would have a simpler implementation of scaled dot product, but the model might not work in practice as well as multi-head attention.</p>
<p>Now to go into your code and show how scaled dot product on multiple heads would work:</p>
<pre><code>    keys=self.key_weight(x)
    queries=self.query_weight(x)
    values=self.value_weight(x)
</code></pre>
<p>at this stage, you have applied the query, key and value projections on the input. Each of these is of size <code>(batch_size, seq_len, d_in)</code>.</p>
<pre><code>    keys=keys.view(batch_size,sequence_length,self.num_heads,self.head_dim)
    values=values.view(batch_size,sequence_length,self.num_heads,self.head_dim)
    queries=queries.view(batch_size,sequence_length,self.num_heads,self.head_dim)
</code></pre>
<p>Now after the reshaping each of these is of size <code>(batch_size, seq_len, num_heads, head_dim)</code></p>
<p>With this, what you would do is a separate scaled dot product operation for each head. You have not written this part of the code.</p>
<p>Normally, you'd want to swap the positions of the <code>num_heads</code> dimension to the 2nd position to make it <code>(batch_size, num_heads, seq_len, head_dim)</code> like this</p>
<pre><code>        queries = queries.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)
</code></pre>
<p>Now to compute the dot products and get the weights,</p>
<pre><code>scores = torch.matmul(queries, keys.transpose(-1, -2))
</code></pre>
<p>Think of this operation as:</p>
<ul>
<li><p><code>queries</code>: it is a tensor of dimension <code>batch_size, num_heads, seq_len, head_dim</code> meaning that for each example in the batch, for each head in the example, you have a <code>seq_len, head_dim</code> matrix</p>
</li>
<li><p><code>keys</code>: ditto, same as <code>queries</code></p>
</li>
<li><p>Now what you want to do is, for each example in the batch, for each each head in the example, do a dot product between the 2D query and key matrices. The keys.transpose(-1, -2) helps flip the last two dimensions of keys to make it <code>head_dim, seq_len</code>  so that when right-multiplied with <code>seq_len, head_dim</code> it produces <code>seq_len, seq_len</code> which is just a weight for every pair of positions in the sequence.</p>
</li>
<li><p>So in the end you have <code>scores</code> as <code>batch_size, num_heads, seq_len, seq_len</code></p>
</li>
</ul>
<p>I'll leave the rest of the steps in scaled dot product out, because I think this adequately addresses the need for reshaping that you originall asked.</p>
","2","Answer"
"79544019","79537819","<p>as of now, pytorch which supports cuda 12.8 is not released yet.</p>
<p>but unofficial support released nightly version of it.</p>
<p>here are the commands to install it. so with this pytorch version you can use it on rtx 50XX. I've got 5080 and it works just fine.</p>
<p>pip install --pre torch==2.8.0.dev20250324+cu128 torchvision==0.22.0.dev20250325+cu128 torchaudio==2.6.0.dev20250325+cu128 --index-url <a href=""https://download.pytorch.org/whl/nightly/cu128"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/nightly/cu128</a></p>
","1","Answer"
"79545495","79545477","<p>The computation will be performed in fp32 (float32) by default (<a href=""https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16"" rel=""nofollow noreferrer"">https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16</a>), even though your inputs are fp16 (float16). This is PyTorch's default behavior for numerical stability reasons.</p>
<p>Why fp32 by Default:<br />
Reduced precision (fp16) can lead to numerical instability (overflow/underflow)<br />
Many operations in PyTorch use fp32 internally for accumulation even with fp16 inputs</p>
<p>You could verify:</p>
<pre><code>import torch
from torch.nn.functional import linear
a = torch.ones(2, 3, dtype=torch.float16)
b = torch.ones(4, 3, dtype=torch.float16)
output = linear(a, b)
print(output.dtype)
</code></pre>
<h4>Expected Output:</h4>
<ul>
<li><p>On CPU (Newer version)→ <code>torch.float16</code></p>
</li>
<li><p>On GPU (pre-Ampere) → <code>torch.float32</code></p>
</li>
<li><p>On Ampere+ GPUs (with autocast enabled) → <code>torch.float16</code></p>
</li>
</ul>
<p>When both inputs (<code>a</code> and <code>b</code>) are in <code>torch.float16</code>, PyTorch automatically upcasts computations to <code>torch.float32</code> by default for numerical stability, especially on CPU and some GPUs (e.g., older architectures).</p>
<p>If running on Ampere (or newer) GPUs with Tensor Cores enabled, the computation might stay in fp16 for efficiency.</p>
","0","Answer"
"79545694","79533110","<p>I think both <code>inference.py</code> and <code>requirements.txt</code> should be located in a subfolder named <code>code</code>, unless you specify a different folder using the parameter <code>source_dir</code> argument when creating PyTorch estimator.</p>
<p>See <a href=""https://sagemaker.readthedocs.io/en/v2.29.1/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models"" rel=""nofollow noreferrer"">here</a> for details.</p>
","0","Answer"
"79548870","79546578","<p>Alternative solution adapted from PyTorch <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network"" rel=""nofollow noreferrer"">docs</a>.:</p>
<pre><code>for i, data in enumerate(train_loader, 0):
    ex_images, ex_dmaps, ex_n_people = data
</code></pre>
","0","Answer"
"79551874","79537819","<p>I had to make a 4 into a 5 on the torch version to get it.</p>
<p>pip install --pre torch==2.8.0.dev20250325+cu128 torchvision==0.22.0.dev20250325+cu128 torchaudio==2.6.0.dev20250325+cu128 --index-url <a href=""https://download.pytorch.org/whl/nightly/cu128"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/nightly/cu128</a></p>
","0","Answer"
"79553465","79552821","<p>Here you can see the differences between <code>upsampling</code> and <code>ConvTranspose2D</code>: <a href=""https://discuss.pytorch.org/t/torch-nn-convtranspose2d-vs-torch-nn-upsample/30574"" rel=""nofollow noreferrer"">Pytorch Forumns discussion.</a></p>
","0","Answer"
"79553766","79512576","<p>is the second data was something new or similar.</p>
<p>if it is new check whether the data is clean for the model to process.</p>
<p>if the dataset is similar I think the model will learn little from it.</p>
<p>I think that the cause can be in data</p>
","0","Answer"
"79554301","79554290","<blockquote>
<p>Why can't I use a vscode debugger to debug jax code, specifically pure functions.</p>
</blockquote>
<p>You can use vscode interactive debugging – during <em>tracing</em>, which is when the Python part of the code is running. You can't (as far as I know) use vscode interactive debugging during execution of the compiled kernels that the traced code dispatches to.</p>
<p>For more information on JAX's computational model, you could start with <a href=""https://docs.jax.dev/en/latest/key-concepts.html#tracing"" rel=""nofollow noreferrer"">JAX Key Concepts: Tracing</a> and continue through the tutorials from there.</p>
<p>JAX provides some specific tools for debugging within this traced computational model; you can read about them at <a href=""https://docs.jax.dev/en/latest/debugging/print_breakpoint.html"" rel=""nofollow noreferrer"">JAX: debugging</a>.</p>
","0","Answer"
"79556702","79556482","<p>An idea to start with :</p>
<p>you must before starting the scripts :</p>
<p><code>pip install spacy, nltk, pywsd</code></p>
<p>then install the spacy small model :</p>
<p><code>python -m spacy download en_core_web_sm  </code></p>
<p><a href=""https://spacy.io/models"" rel=""nofollow noreferrer"">Available Models</a></p>
<p>Before the first run download the nltk necessary packages :</p>
<blockquote>
<p>init.py</p>
</blockquote>
<pre><code>import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
nltk.download('punkt_tab')
</code></pre>
<p><strong>When this all is done :</strong></p>
<blockquote>
<p>main.py:</p>
</blockquote>
<pre><code>from nltk.corpus import wordnet as wn
import spacy
from pywsd.lesk import simple_lesk


nlp = spacy.load(&quot;en_core_web_sm&quot;)

def is_visually_searchable(phrase):
    doc = nlp(phrase)
    
    if any(token.like_num or token.is_digit for token in doc):
        return False
    
    noun_chunks = list(doc.noun_chunks)
    if not noun_chunks:
        return False
    
    for token in doc:
        if token.pos_ in [&quot;NOUN&quot;, &quot;PROPN&quot;]:
            synset = simple_lesk(phrase, token.text)
            if synset:
                if synset.lexname() in [&quot;noun.attribute&quot;, &quot;noun.cognition&quot;, &quot;noun.communication&quot;]:
                    return False
    return True

phrases = [
    &quot;red apple&quot;,
    &quot;big idea&quot;,
    &quot;driver's license&quot;,
    &quot;suspended license&quot;,
    &quot;Veronica&quot;,       
    &quot;DL 1234&quot;,       
    &quot;flying plane&quot;,
    &quot;available items&quot;
]

for phrase in phrases:
    print(f&quot;{phrase}: {'YES' if is_visually_searchable(phrase) else 'NO'}&quot;)

</code></pre>
<p><strong>Results:</strong></p>
<pre><code>&gt; red apple: YES
&gt; big idea: NO
&gt; driver's license: YES
&gt; suspended license: YES
&gt; Veronica: YES
&gt; DL 1234: NO
&gt; flying plane: YES
&gt; available items: NO
&gt; &gt;
</code></pre>
<p>You see that Veronica and suspended licence are alway YES</p>
<p><strong>Additional custom filters:</strong></p>
<ul>
<li>First-name filter:</li>
</ul>
<pre><code>import requests

def load_names_from_url(url: str) -&gt; set:
    try:
        response = requests.get(url)
        response.raise_for_status()
        return {line.strip().lower() for line in response.text.splitlines()}
    except requests.RequestException as e:
        return set()

url = 'https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt'

NAMES_SET = load_names_from_url(url)
</code></pre>
<p>Then add the logic in the 'is_visually_searchable' function:</p>
<pre><code>if any(str(token).lower() in NAMES_SET for token in doc if not token.like_num):
     return False

</code></pre>
<p><strong>Results:</strong></p>
<pre><code>&gt; red apple: YES
&gt; big idea: NO
&gt; driver's license: YES
&gt; suspended license: YES
&gt; Veronica: NO
&gt; DL 1234: NO
&gt; flying plane: YES
&gt; available items: NO
&gt;  

</code></pre>
<p>Check <a href=""https://wordnet.princeton.edu/documentation/lexnames5wn"" rel=""nofollow noreferrer"">Lexical Categories</a> to tune the results:</p>
<p>You can check the <a href=""https://wordnet.princeton.edu/documentation/lexnames5wn"" rel=""nofollow noreferrer"">lexical Categories</a> of a word like this:</p>
<pre><code>from nltk.corpus import wordnet as wn

word = &quot;suspended&quot;
synsets = wn.synsets(word, pos=wn.VERB)
for syn in synsets:
    print(f&quot;Word: {word}, Lexname: {syn.lexname()}, Definition: {syn.definition()}&quot;)

</code></pre>
<p><strong>Results:</strong></p>
<pre><code>&gt; Word: suspended, Lexname: verb.contact, Definition: hang freely
&gt; Word: suspended, Lexname: verb.change, Definition: cause to be held in suspension in a fluid
&gt; Word: suspended, Lexname: verb.social, Definition: bar temporarily; from school, office, etc.
&gt; Word: suspended, Lexname: verb.change, Definition: stop a process or a habit by imposing a freeze on it
&gt; Word: suspended, Lexname: verb.change, Definition: make inoperative or stop
&gt; Word: suspended, Lexname: verb.stative, Definition: render temporarily ineffective
&gt; 
</code></pre>
<p>You can use: <code>wn.NOUN</code>,<code>wn.VERB</code>,<code>wn.ADJ</code>,<code>wn.ADV</code></p>
","0","Answer"
"79560937","79560879","<p>It is very hard to say what exactly is happening here without knowing the exact images, but when it comes to smaller angle rotations there is going to be some data lost. Square images will not have any data loss on multiples of 90 degree rotations, but anything off of 90 degrees will necessarily lose information around two opposing corners. I do not know how you are filling in this data, or if you are simply resizing the existing images till they are square again by some sort of zooming or filling function.</p>
<p>Having trouble with small angles might either mean that the whole image is being memorized and when a part of it is not present that it makes angle calculations harder OR however the missing data is being filled in might be throwing off any ability to recognize angle changes. If I am understanding the torch code correctly there are just a bunch of conv layers and some linear activations. This should mean that every part of every training image is getting the same amount of attention, which may be the thing that is throwing off small angle detection since corners are getting over-weighted if that same corner data is not present in the test data set.</p>
<p>Potential Solution:<br />
If you consider for a moment a square image being freely rotated from 0 to 360 degrees, you will see that a perfect circle of that square is always in frame. The radius of that circle is a half length of the square. The area that is outside of that circle will still be in frame somewhere between 1% of the time (furthest corners that sit right on the edge) to 99% of the time (spot closest to the circle, furthest from the edges). In this case you can see that the circle itself will always be in frame and certain areas outside the circle will be in frame to varying degrees. To improve your approach I would suggest to retrain the model with 100% attention on that inner circle and have progressively less attention towards the edges, with a weighting based on how much a certain pixel location is in-frame for a 0 - 360 degree rotation. When the corners are out of frame, they are not available for the NN to evaluate, thus, giving them progressively lower attention will help train the model on the area that is always available, the inner circle. If the NN can focus all of its learning on that inner circle I would expect it to become better with evaluating smaller angle changes!</p>
","1","Answer"
"79561518","79537819","<p>You're right, PyTorch currently provides pre-built binaries only up to <strong>CUDA 12.6</strong>, even though your system might have <strong>CUDA 12.8</strong> installed.</p>
<p>I had the same setup with CUDA 12.8 installed system-wide, and when I tried:<br />
<code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126</code></p>
<p>It didn’t work.</p>
<p>But then I used:<br />
<strong><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124</code></strong><br />
And it worked without any issues. PyTorch doesn't need the exact same version of the CUDA toolkit installed locally, it uses its own</p>
<p>You can check your driver’s supported CUDA version in the terminal with:<br />
<code>nvidia-smi</code></p>
","0","Answer"
"79562979","79561884","<p>As far as I'm aware, there is no built-in function to do what you want, but a function like this should work, I believe:</p>
<pre><code>import torch
import torch.nn.utils.prune as prune
import torch.nn as nn
import numpy as np

def global_structured_prune(model, layers, amount=0.8, norm=2):
    # Step 1: Collect all neuron norms
    all_norms = []
    layer_neuron_norms = {}

    for layer in layers:
        weight = getattr(model, layer).weight.detach().cpu()
        if isinstance(getattr(model, layer), nn.Linear):
            norms = torch.norm(weight, p=norm, dim=1)  # Norm across incoming connections
            all_norms.append(norms)
            layer_neuron_norms[layer] = norms

    all_norms_flat = torch.cat(all_norms)
    
    # Step 2: Compute global threshold
    k = int(amount * all_norms_flat.numel())
    if k == 0:
        return  # Nothing to prune
    threshold = torch.kthvalue(all_norms_flat, k).values.item()

    # Step 3: Apply structured pruning to neurons below threshold
    for layer in layers:
        norms = layer_neuron_norms[layer]
        to_prune = (norms &lt;= threshold).nonzero(as_tuple=True)[0].tolist()
        prune.ln_structured(getattr(model, layer), name=&quot;weight&quot;, amount=to_prune, n=norm, dim=0)

# Usage
model = SimpleCNN()
global_structured_prune(model, layers=['fc1', 'fc2', 'fc3'], amount=0.8, norm=2)
</code></pre>
","2","Answer"
"79570604","79570067","<p>In general pytorch will only compute gradients for parameters that require them. In the case of a <code>torch.nn.Linear</code> layer, it can compute 3 gradients at most: one for the <code>input</code>, one for the <code>weight</code> and one for the <code>bias</code>. Let's assume the backward receives a gradient <code>g</code>:</p>
<ul>
<li>Computed gradient for the input: <code>g.mm(weight)</code></li>
<li>Computed gradient for the weight: <code>g.t().mm(input)</code></li>
<li>Computed gradient for the bias: <code>g.sum(0)</code></li>
</ul>
<p>As you can see, the gradient for the input will have a very similar computational cost as the forward pass. However, the gradient for the weight and bias will be much cheaper to compute. Thus depending on which of your tensors require gradients, <strong>the backward pass might be faster or slower than the forward pass</strong>.</p>
<p>However there might be some additional overheads in the backward pass. Here's a small benchmark I ran on CPU:</p>
<pre class=""lang-py prettyprint-override""><code>import time

import matplotlib.pyplot as plt
import numpy as np
import torch

N = 1000
BATCH_SIZE = 16
IN_DIMS = [128, 512, 2048]
OUT_DIMS = [128, 512, 2048]

forward_times = np.zeros((len(IN_DIMS), len(OUT_DIMS)))
backward_times = np.zeros((len(IN_DIMS), len(OUT_DIMS)))

for in_dim in IN_DIMS:
    for out_dim in OUT_DIMS:
        layer = torch.nn.Linear(in_dim, out_dim, bias=False)
        layer.weight.requires_grad_(False)
        grad_out = torch.ones(BATCH_SIZE, out_dim)

        # Warm up layer
        X = torch.randn(BATCH_SIZE, in_dim, requires_grad=True)
        layer(X).backward(grad_out)

        forward_time = 0
        backward_time = 0
        for _ in range(N):
            layer.zero_grad()

            # Forward
            t = time.time()
            out = layer(X)
            forward_time += time.time() - t

            # Backward
            t = time.time()
            out.backward(grad_out)
            backward_time += time.time() - t

        forward_times[IN_DIMS.index(in_dim), OUT_DIMS.index(out_dim)] = (
            forward_time / N * 1000
        )
        backward_times[IN_DIMS.index(in_dim), OUT_DIMS.index(out_dim)] = (
            backward_time / N * 1000
        )
</code></pre>
<p>When computing only the gradient for the input, forward and backward are very similar (with the backward being a bit slower probably due to some overhead):
<a href=""https://i.sstatic.net/JfgOVRd2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JfgOVRd2.png"" alt=""Comparison of forward and backward times when computing gradients only for input."" /></a></p>
<p>On the contrary if we compute only the gradient for the bias, the backward pass is much faster than the forward:
<a href=""https://i.sstatic.net/rUmKA3Pk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUmKA3Pk.png"" alt=""Comparison of forward and backward times when computing gradients only for bias."" /></a></p>
<p><strong>Conclusion</strong>: backward computation time depends entirely on which tensors require gradients. These results may also vary depending on the hardware and software you are running.</p>
","1","Answer"
"79570904","79542733","<p>Reshaping is necessary for the softmax. You want to softmax head attention head separately. This is done by creating a new axis for the attention heads and softmaxing all heads separately in parallel.</p>
<p>If you didn't reshape for the head dimension, you would get the same result as using a single attention head.</p>
","1","Answer"
"79571984","79571227","<p>In general when comparing the same JAX operation with and without JIT, you should expect equivalence up to typical floating point rounding errors, but you should not expect bitwise equivalence, as the compiler may fuse operations in a way that leads to differing float error accumulation.</p>
","2","Answer"
"79573832","79573774","<p>To fix the recursion, I:</p>
<ul>
<li><p>Created a <strong>sub-model (like <code>self.model</code>)</strong> that holds the actual neural network (e.g. <code>nn.Sequential</code>,)</p>
</li>
<li><p>Wrap <strong>only that</strong> with Opacus.</p>
<p>final Code:</p>
</li>
</ul>
<pre><code>class DPModel(BaseModel):
    def __init__(self, ..., enable_dp=True, **kwargs):
        super().__init__(**kwargs)
        self.model = MyActualModel(...)  # ✅ wrap this, not self
        self.enable_dp = enable_dp
        if self.enable_dp:
            self.privacy_engine = PrivacyEngine()

    def forward(self, x):
        return self.model(x)

    def on_train_start(self):
        if self.enable_dp:
            self.model, dp_optimizer, _ = self.privacy_engine.make_private(
                module=self.model,  # ✅ wrap the core model
                optimizer=self.trainer.optimizers[0],
                data_loader=self.trainer.train_dataloader,
                noise_multiplier=self.hparams.noise_multiplier,
                max_grad_norm=self.hparams.max_grad_norm,
            )
            self.trainer.optimizers = [dp_optimizer]
</code></pre>
<p>Opacus wraps the model by injecting hooks and modifying the computation graph. Wrapping <code>self</code> — which includes custom hooks, overridden <code>forward()</code>, and trainer internals — leads to recursion or unexpected behavior.</p>
","0","Answer"
"79575787","79575684","<p>They don't do a very good job of documenting, but their <a href=""https://github.com/Lightning-AI/torchmetrics/blob/master/src/torchmetrics/functional/regression/mape.py#L25"" rel=""nofollow noreferrer"">code</a> uses an eps value of <code>1.17e-06</code>. Using your <code>calculate_mape</code> with this eps gives the same result. This is just a result of eps scaling for zero value targets.</p>
<p>If your outputs are all int values, you might want to consider using a classification metric rather than a regression metric.</p>
","1","Answer"
"79579793","79578968","<p>You're getting different answers because every time you create a <code>nn.Linear</code> object you get another set of randomly initialized weights. You can verify the expected linear relationship by using the same weights:</p>
<pre><code>x_size = 8
h_size = 12
o_size = 16

# first set of weights and bias
w1 = torch.randn(o_size, x_size)
b1 = torch.randn(o_size)

# second set of weights and bias
w2 = torch.randn(o_size, h_size)
b2 = torch.randn(o_size)

# inputs 
x = torch.randn(1, x_size)
h = torch.randn(1, h_size)

# outputs of individual inputs
out_x = F.linear(x, w1, b1)
out_h = F.linear(h, w2, b2)

# output of concatenated inputs
# note we concatenate the inputs/weights but sum the bias
out_xh = F.linear(torch.cat([x,h], -1), 
                  torch.cat([w1,w2], -1), 
                  b1+b2)

torch.allclose(out_x + out_h, out_xh)
&gt; True
</code></pre>
<p>Note the code above uses <code>F.linear</code> rather than <code>nn.Linear</code> - <code>F.linear</code> is the functional form and is easier to use when you're passing weights manually. <code>nn.Linear</code> will give the same results so long as you manually update the weight matrices in the <code>nn.Linear</code> object to use the same weights.</p>
","2","Answer"
"79583532","79583142","<p>In PyTorch, tensors directly created by users are termed <strong>leaf tensors</strong>, and their <strong>views</strong> share the same underlying storage. Performing in-place assignments on a view can modify the storage of the original tensor <strong>midst in the computational graph</strong>, leading to undefined behavior. Thus, directly assigning values to views should be avoided.</p>
<p>To achieve this safely, replace in-place operations with an out-of-place approach. For example:</p>
<pre class=""lang-py prettyprint-override""><code>p_shifted = torch.stack([
    p[:, 0, :],
    p[:, 1, :],
    p[:, 2, :] + z_shift,
], dim=1)
</code></pre>
<p>This constructs a new tensor via <code>torch.stack</code> instead of modifying the original storage in-place, ensuring computational graph integrity while fulfilling the intended functionality.</p>
","2","Answer"
"79584002","79582585","<p>This kind of error usually happens when you're doing multi-GPU training and parts of your model or inputs aren’t on the same device. In your case, since you're using <code>device_map=&quot;auto&quot;</code> with PEFT (LoRA), I think the issue is that the LoRA layers aren’t automatically distributed across GPUs like the base model is — leading to a device mismatch between <code>cuda:0</code> and <code>cuda:1</code>.<br />
After you wrap the model with get_peft_model(), you must manually re-dispatch it using dispatch_model() from accelerate. This step is essential because get_peft_model() does not respect the existing device_map.</p>
<pre><code>from peft import get_peft_model
from accelerate import dispatch_model
# Apply LoRA
model = get_peft_model(model, peft_config)
# Redispatch model across GPUs
model = dispatch_model(model, device_map=&quot;auto&quot;)
</code></pre>
<p>This ensures both the base model and the LoRA-adapted layers are placed consistently across GPUs.Also, make sure your inputs are moved to the same device as the model’s first parameter. I usually do something like this before feeding data into the model:</p>
<pre><code>device = next(model.parameters()).deviceinputs = {k: v.to(device) for k, v in inputs.items()}
</code></pre>
<p>Finally, double-check your <code>SafeCollator</code> (or any custom collator) — I’ve seen cases where the collator holds tensors on CPU or defaults to <code>cuda:0</code>, which can silently cause this type of issue when the model is split.</p>
","3","Answer"
"79584640","79584485","<p>The model in the Google folder with .pb extension is typical for the Tensorflow environment, not Pytorch.</p>
<p>Error</p>
<pre><code>Unsupported global
</code></pre>
<p>likely tells that Pytorch's torch.load() tries to read the .pb file but does not understand the format.</p>
<p>I suggest:</p>
<ol>
<li><p>trying to convert the file from .pb to torch supported file such as .pt, .pth. but this is often complex process, especially if the model has custom layers</p>
</li>
<li><p>Or better to train the model originally on PyTorch-supported files</p>
</li>
<li><p>Using Tensorflow to load the model</p>
</li>
</ol>
","2","Answer"
"79585875","79583654","<p>The issue was in the EncoderLayer where the residual calculations were done wrong. The correct way of calculating:</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(self, x: torch.Tensor, src_pad_key = None):
        
        residual = x
        x = self.layer_norm1(x)
        
        if src_pad_key is not None: x = self.self_attn(x, src_pad_key = src_pad_key, use_self_attention = True)
        else: x = self.self_attn(x)

        # normalize and apply residual connections
        x += residual

        residual = x
        x = self.layer_norm2(x)
        x = self.mlp(x)
        x += residual

        return x
</code></pre>
<p>Another change was that we must always use self attention (instead of pooled attention) as otherwise the calculations won't work with the image encoder. [query = x]</p>
<p>The results look like this:</p>
<pre><code>Cat similarity: tensor([[25.4132]], grad_fn=&lt;MulBackward0&gt;)
Dog similarity: tensor([[21.8544]], grad_fn=&lt;MulBackward0&gt;)
cosine cat/dog: 0.8438754677772522
</code></pre>
","1","Answer"
"79591730","79591629","<p>Your current tensor_loader implementation uses:</p>
<p>DataLoader(..., sampler=BatchSampler(...))</p>
<p>This causes each batch to be treated a s a single sample containing a list of indices,</p>
<p>instead of directly yielding the batch data. i[0] becomes:</p>
<pre><code>torch.Size([1, 4096, 10])
</code></pre>
<p>RandomSampler may fix this, and let dataloader handle batching with its batch_size.</p>
<pre><code>def tensor_loader(dataset: TensorDataset, batch_size: int):
    return DataLoader(
        dataset=dataset,
        sampler=RandomSampler(dataset),  # equivalent to shuffle=True
        batch_size=batch_size,
        drop_last=True
    )
</code></pre>
<pre><code>dataset = TensorDataset(torch.tensor(np.random.random(1_000_000).reshape(-1, 10), dtype=torch.float32))

start = pd.Timestamp.now()
for i in tensor_loader(dataset, 4096):
    i[0]  # shape: [4096, 10]
end = pd.Timestamp.now()
print(end - start)
assert i[0].shape == torch.Size([4096, 10])

start = pd.Timestamp.now()
simple_loader =  DataLoader(dataset=dataset, batch_size=4096)
for i in simple_loader:
    pass
end = pd.Timestamp.now()
print( end - start)
assert next(iter(simple_loader))[0].shape == torch.Size([4096, 10])
</code></pre>
","0","Answer"
"79591758","79537716","<p>The pytorch offical website:</p>
<p>newset version: <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">https://pytorch.org/</a></p>
<p>previous version: <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/previous-versions/</a></p>
<p>Make sure activating your conda environment before typing these installation commands.</p>
","-1","Answer"
"79592916","79537819","<p>The command to install the stable version of PyTorch (2.7.0) with CUDA 12.8 using pip on Linux is:</p>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
</code></pre>
","2","Answer"
"79594131","79582585","<p>Using older transformers version helped me</p>
<pre><code>pip install transformers==4.49.0
</code></pre>
","0","Answer"
"79594330","79589904","<p>Based on the code you shared, I think the evaluation batch size can be responsible for this Out Of Memory error.</p>
<p>In your code you set <code>per_device_train_batch_size=2</code>, probably because the default value caused OOM errors. But based on the <a href=""https://github.com/huggingface/transformers/blob/5f4ecf2d9f867a1255131d2461d75793c0cf1db2/src/transformers/training_args.py#L848"" rel=""nofollow noreferrer"">transformers source code</a> the evaluation batch size will be 8 by default. Thus you should modify your code like so:</p>
<pre class=""lang-py prettyprint-override""><code># Define training arguments
training_args = TrainingArguments(
    output_dir=&quot;./spellcheck_model_base&quot;,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    report_to=&quot;none&quot;,
    fp16=True, 
    gradient_accumulation_steps = 256,
)
</code></pre>
<p>Generally evaluation should use less memory than training because it doesn't need to store gradients and activations. On the other hand it can use more memory depending on the data, because memory usage depends on the sequence length. Thus it is important to carefully chose your evaluation batch size.</p>
","1","Answer"
"79594394","79589222","<p>Based on <a href=""https://github.com/pytorch/pytorch/issues/44887"" rel=""nofollow noreferrer"">this github issue</a>, it looks like operations such as <code>torch.ones</code> won't work as they don't get traced correctly. There seems to be at least two workarounds:</p>
<ol>
<li>Using <code>torch.ones_like</code></li>
<li>Using <code>torch.Tensor.new_ones</code></li>
</ol>
<p>For instance here's a minimum example where the wrapper module initializes a tensor based on some dimensions of the input:</p>
<pre><code>import torch
from torch.fx import symbolic_trace


class WrappedModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, x):
        i = x.shape[0]
        j = x.shape[1]
        y = x.new_ones((i, j))
        return self.model(y)


print(symbolic_trace(WrappedModel(torch.nn.Linear(10, 10))))
</code></pre>
<p>which returns:</p>
<pre class=""lang-py prettyprint-override""><code>WrappedModel(
  (model): Linear(in_features=10, out_features=10, bias=True)
)



def forward(self, x):
    getattr_1 = x.shape
    getitem = getattr_1[0];  getattr_1 = None
    getattr_2 = x.shape
    getitem_1 = getattr_2[1];  getattr_2 = None
    new_ones = x.new_ones((getitem, getitem_1));  x = getitem = getitem_1 = None
    model = self.model(new_ones);  new_ones = None
    return model
    
# To see more debug info, please use `graph_module.print_readable()`
</code></pre>
","0","Answer"
"79594863","79559122","<p>Actually i was getting the same error while using the qwen2.5 model for inference but there was one thing i overlooked by mistake and it was very silly i forgot to edit the pod container size large enough to fit the model weights, after i corrected that it RAN without any error.</p>
<p>I hope this helps .</p>
","1","Answer"
"79597306","79597119","<p>You could add</p>
<pre><code>pipe.enable_attention_slicing()
</code></pre>
<p>before sending it to cuda, however this will reduce the speed of the image generation.</p>
<p>Resolution can be changed by adding height and width arguments to the image definition.</p>
<pre><code>image = pipe(prompt, height=512, width=512).images[0]
</code></pre>
","2","Answer"
"79600435","79599360","<p>The line <code>nn.Parameter(torch.randn(1,1))</code> is creating a parameter out of a 1x1 tensor, whose value is randomly generated on a normal distribution centered at 0 with a standard deviation of 1.</p>
<p>Julia has a built in<code>randn()</code>, and Flux has a direct translation of `Parameter', <em>although it is deprecated</em>.</p>
<pre class=""lang-none prettyprint-override""><code>using Flux

myparam = Flux.params(randn(1, 1)) # param(...) must be qualified with Flux module name
</code></pre>
<p>NOTE:</p>
<p>The <code>params(...)</code> method is internal, which is why it has to be qualified, and also tells us that its use is likely not the intended way to create a model. I recommend looking through the documentation and examples for more: <a href=""https://fluxml.ai/Flux.jl/stable/reference/destructure/#Flux.params"" rel=""nofollow noreferrer"">https://fluxml.ai/Flux.jl/stable/reference/destructure/#Flux.params</a></p>
","0","Answer"
"79601395","79600870","<p>Do you need something like this?</p>
<pre class=""lang-py prettyprint-override""><code>import torch

v1 = torch.as_tensor(3.5876e-4)
v2 = torch.as_tensor(2.1234e+5)

decimals = 1
def mround(v, decimals):
    return torch.round(v, decimals=decimals-int(torch.floor(torch.log10(v)).item()))

v1r = mround(v1, decimals);
v2r = mround(v2, decimals);

print(f&quot;{v1:.4e} --&gt; {v1r:.4e} --&gt; {v1r:.{decimals}e}&quot;)
print(f&quot;{v2:.4e} --&gt; {v2r:.4e} --&gt; {v2r:.{decimals}e}&quot;)
</code></pre>
<p>Result:</p>
<pre><code>3.5876e-04 --&gt; 3.6000e-04 --&gt; 3.6e-04
2.1234e+05 --&gt; 2.1000e+05 --&gt; 2.1e+05
</code></pre>
<hr />
<p>Rounding the mantissa to any decimal place will not make it more compact, because it is binary. And rounded benchmark results in notepad will force you to always round up test results before comparing.</p>
","1","Answer"
"79605240","79603672","<p>This is happening because you're using <code>Python 3.13</code>, which is <strong>not currently supported by</strong> <code>PyTorch</code></p>
<p>You should downgrade to <code>Python 3.10</code> or <code>Python 3.9</code>, which are <strong>stable</strong> with <code>PyTorch</code> and all its associated packages.</p>
","0","Answer"
"79606212","79606201","<p>In-place operations like x -= ... can break the computation graph if the tensor is a leaf tensor that requires gradients, and the operation is not inside a torch.no_grad() context. This causes a version mismatch error during .backward() if you try to reuse the computation graph or modify variables that are part of it.</p>
<ol>
<li><p>Gradients in PyTorch accumulate by default for efficiency (useful during mini-batch training). So you must call <code>x.grad.zero_()</code> (or reinitialize <code>x</code> with <code>.detach()</code>) before the next <code>.backward()</code> pass if you're doing manual gradient descent.</p>
</li>
<li><p>Updating model parameters (or any tensor that requires gradients) must happen inside <code>torch.no_grad()</code> to prevent PyTorch from tracking the update operation itself. If you don't, the update becomes part of the graph and causes unwanted memory usage and errors.</p>
</li>
<li><p><code>retain_graph=True</code> is only needed if you plan to reuse the same computation graph across multiple backward passes (e.g., higher-order derivatives). In simple gradient descent (with one <code>.backward()</code> per step), there's no need for <code>retain_graph=True</code>.</p>
</li>
</ol>
<p>Here is the code with correction:</p>
<pre><code>import torch

alpha = 0.1
x = torch.tensor([42.0], requires_grad=True)

for i in range(10):
    y = 3 * x ** 2 + 4 * x + 9
    y.backward()
    print(f&quot;Step {i+1}: x = {x.item():.4f}, y = {y.item():.4f}, grad = {x.grad.item():.4f}&quot;)
    with torch.no_grad():
        x -= alpha * x.grad
    x.grad.zero_()
# you may replace  x.grad.zero_() with x = x.detach().clone().requires_grad_(True) for advanced control.
</code></pre>
","3","Answer"
"79609200","79609199","<p>Following description from <a href=""https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch
import matplotlib.pyplot as plt
plt.style.use('dark_background')

# log of bezel function approximation
# the sum must go to infinity, but we stop at j=100
def bezel(v,y,infinity=100):
    if not isinstance(y,torch.Tensor):
        y = torch.tensor(y)
    if not isinstance(v,torch.Tensor):
        v = torch.tensor(v)
    j = torch.arange(0,infinity)
    bottom = torch.lgamma(j+v+1)+torch.lgamma(j+1)
    top = 2*j*(0.5*y.unsqueeze(-1)).log()
    mult = (top-bottom)
    return (v*(y/2).log().unsqueeze_(-1)+mult)

def noncentral_chi2(x,mu,k):
    if not isinstance(mu,torch.Tensor):
        mu = torch.tensor(mu)
    if not isinstance(k,torch.Tensor):
        k = torch.tensor(k)
    if not isinstance(x,torch.Tensor):
        x = torch.tensor(x)
    
    # the key trick is to use log operations instead of * and / as much as possible
    bezel_out = bezel(0.5*k-1,(mu*x).sqrt())
    x=x.unsqueeze_(-1)
    return (torch.tensor(0.5).log()+(-0.5*(x+mu))+(x.log()-mu.log())*(0.25*k-0.5)+bezel_out).exp().sum(-1)

# count of normal random variables that we will sum
loc = torch.rand((5))
normal = torch.distributions.Normal(loc,1)

# distribution parameter, also named as lambda
mu = (loc**2).sum()

# count of simulated sums
events = 5000
Xs = normal.sample((events,))

# chi-square distribution
Y = (Xs**2).sum(-1)

t = torch.linspace(0.1,Y.max()+10,100)
dist = noncentral_chi2(t,mu,len(loc))

# plot produced hist againts computed density function
plt.title(f&quot;k={len(loc)}, mu={mu:0.2f}&quot;)
plt.hist(Y,bins=int(events**0.5),density=True)
plt.plot(t,dist)
</code></pre>
<p><a href=""https://i.sstatic.net/VCb9pEFt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VCb9pEFt.png"" alt=""k=100"" /></a>
<a href=""https://i.sstatic.net/bZQw1OrU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZQw1OrU.png"" alt=""k=10"" /></a></p>
","0","Answer"
"79609363","79607611","<p>You can just call <code>backward</code> on the raw output. Here's a simple example:</p>
<pre><code>import torch
import torch.nn as nn
from torch.autograd.functional import jacobian

x = torch.randn(4, 8)
model = nn.Sequential(*[nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 1)])

# method 1 - compute jacobian and take diagonal
j = jacobian(model, x, create_graph=True, strict=True)
out1 = torch.einsum('iif-&gt;if', j.squeeze())

# method 2 - set grad flag on inputs, run forward pass and backward
x.requires_grad = True
y = model(x)
y.backward(torch.ones_like(y))
out2 = x.grad

torch.allclose(out1, out2)
&gt; True
</code></pre>
","1","Answer"
"79609725","79599360","<p>If I understand correctly,  <code>nn.Parameter(torch.randn(1,1))</code> wraps a 1x1 array so that pytorch can think about its gradient.</p>
<p>In Flux.jl there is no such wrapping. The function <code>randn(1,1)</code> creates a 1x1 array (a plain <code>Array</code> from base julia). And this can be used as a Flux.jl  parameter array without extra steps. <code>Flux.gradient</code> works very differently -- it inspects the code that is run, rather than attaching objects to the arrays.</p>
<p>One way to construct an equivalent model looks like this:</p>
<pre class=""lang-julia prettyprint-override""><code>using Flux, Statistics

struct MyModel{T}  # container
  param::T
end

MyModel() = MyModel(randn(1,1))  # init

(m::MyModel)(x) = m.param * x  # forward
</code></pre>
<p>To use it:</p>
<pre class=""lang-julia prettyprint-override""><code>model = MyModel()  # make an instance
model.param  # is just a plain Array

data = randn(1,1)
out = mean(model(data))  # run with no gradient

# gradient of the loss with respect to the model:
out, grads = Flux.withgradient(m -&gt; mean(m(data)), model)
</code></pre>
<p>Now <code>grads[1]</code> contains the derivative. It's a nested structure matching <code>model</code> .</p>
","1","Answer"
"79610781","79609919","<p>Generally, when you have a set of inputs where the numbers are <em>labels</em> rather than a meaningful ordering, you want to encode them as a one-hot vector:
So April would be represented as [ 0 0 0 1 0 0 0 0 0 0 0 0 ].</p>
<p>What this does is allow the network to immediately map each possible input to a unique row  in the first weight matrix (take a look at how matrix-vector multiplication works if you don't see why that is), which makes the rest of the job a lot simpler, because it can <em>almost</em> treat them as independent problems at that point. (It <em>could</em> learn how to do that by itself, but you usually need a larger network and a lot more training time.)</p>
<p>You might also do a similar trick on the output, by treating the output as three categories rather than a number, and doing a softmax output and categorical cross-entropy as your loss function, but that's less essential than it is on the input.</p>
<p>Playing with the layer sizes is another thing to try: sometimes wider is better than deeper. And 10 and 5 are very small layers, although for this problem, they might suffice. I'd probably go with two layers of size 16-32, though, and then tweak it from there by seeing how changes affect the training speed. Keep in mind that sometimes smaller is better. A smaller network has less capacity, but also has fewer weights to learn. It's not uncommon to find a network train faster as you decrease the layer sizes, until you decrease them a bit too far and it stops being able to fit the data.</p>
<p>Finally, you might just need to train it longer. A lot of examples out there only train for a small number of epochs, but they can get away with that because each epoch has a bunch of samples in it. If you only have 12 samples, give it at least a few hundred epochs. If it still isn't learning after 1000, something else is wrong, though.</p>
","1","Answer"
"79618913","79618775","<p>To add your list of new features (e.g. <code>List[Tensor]</code>, with each tensor corresponding to a graph in the dataset) to each <code>torch_geometric.data.Data</code> object in a <code>Dataset</code> like <code>ZINC</code>You can do this by simply assigning your new tensor as an attribute of each <code>Data</code> object.</p>
<p>Here’s how you can do it step-by-step:</p>
<pre><code>import torch
from torch_geometric.datasets import ZINC
from torch_geometric.data import InMemoryDataset

# 1. Load the ZINC training dataset
zinc_dataset = ZINC(root='my_path', split='train')

# 2. Create a list of new features for each graph
# Replace this with your actual feature list (must match number of nodes per graph)
new_features = []
for data in zinc_dataset:
    num_nodes = data.x.size(0)  # data.x is [num_nodes, feature_dim]
    new_feat = torch.randn(num_nodes, 12)  # Example: [num_nodes, 12]
    new_features.append(new_feat)

# 3. Define a custom dataset that injects new_feature into each graph's Data object
class ModifiedZINC(InMemoryDataset):
    def __init__(self, original_dataset, new_features_list):
        self.data_list = []
        for i in range(len(original_dataset)):
            data = original_dataset[i]
            data.new_feature = new_features_list[i]
            self.data_list.append(data)
        super().__init__('.', transform=None, pre_transform=None)
        self.data, self.slices = self.collate(self.data_list)

    def __len__(self):
        return len(self.data_list)

    def get(self, idx):
        return self.data_list[idx]

# 4. Create the modified dataset with new features
modified_dataset = ModifiedZINC(zinc_dataset, new_features)

# 5. Check the result
sample = modified_dataset[0]
print(sample)
print(&quot;Shape of new feature:&quot;, sample.new_feature.shape)
</code></pre>
<p>output:</p>
<pre><code>Data(x=[33, 1], edge_index=[2, 72], edge_attr=[72], y=[1], new_feature=[33, 12])

Shape of new feature: torch.Size([33, 12])
</code></pre>
<p><a href=""https://i.sstatic.net/V0uNEbst.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/V0uNEbst.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"79621014","79619981","<p>I've encountered a similar issue with Ultralytics (specifically <strong>v8.3.48</strong>) when working with image segmentation models like <strong>YOLOv11-seg</strong>.</p>
<p>Upon inspecting the converted ONNX model using <strong>Netron.app</strong>, I discovered that the ONNX conversion process did not integrate the post-processing step (like NMS - Non-Maximum Suppression) into the resulting ONNX graph.</p>
<p>Consequently, the ONNX model outputs the raw, unprocessed logits/predictions directly.</p>
<p>This means that during inference, you <strong>must manually implement</strong> the post-processing logic yourself to obtain the final detections. Failing to do so will likely result in errors or incorrect outputs because the ONNX model itself doesn't handle the final result filtering.</p>
<p><strong>Recommendation</strong>:</p>
<p>1. Use <strong>Netron.app</strong> to inspect your generated ONNX model. Verify its structure and confirm whether the post-processing steps are included, and if the outputs match your expectations (i.e., are they the raw logits or already processed detections?).</p>
<p>2. Similarly, examine the output of your <strong>TFLite model</strong> to understand what data it is producing.</p>
<p>This discrepancy in the ONNX conversion seems to be the root cause requiring manual post-processing downstream.</p>
","0","Answer"
"79623368","79623038","<p>After some digging I found the issue.</p>
<p>TLDR: if you are using conda <code>conda install -c nvidia nccl</code> solves it. (my case).</p>
<p>If you interested on what went wrong:<br />
The <code>libnccl.so.2</code> shared library is <strong>inside PyTorch’s site-packages tree</strong>, not in any default search path.</p>
<ul>
<li><p>In a regular SSH shell you start Python from that same environment, so the loader can follow the binary’s relative <code>RPATH</code> and finds the library.</p>
</li>
<li><p>PyCharm’s remote-Jupyter helper copies code to <strong><code>/tmp/.../pycharm-helpers</code></strong> and starts the kernel there; the relative <code>RPATH</code> now points to a path that doesn’t exist, the loader falls back to <code>LD_LIBRARY_PATH</code>, doesn’t find <code>libnccl.so.2</code>, and you get</p>
</li>
</ul>
<pre><code>ImportError: libnccl.so.2: cannot open shared object file: No such file or directory
</code></pre>
<hr />
<p>The issue is that the expected file is somewhere else!</p>
<pre><code># where the library really is
$ find /usr /usr/local -name 'libnccl.so.2'
/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2
</code></pre>
<hr />
<h2>Fixes</h2>
<blockquote>
<p>Any of the three options below unblocks <code>import torch</code> in the PyCharm Jupyter kernel.</p>
</blockquote>
<h3>1 Add the directory to the system linker path (recommended if you have sudo)</h3>
<pre><code>echo /usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib \
  | sudo tee /etc/ld.so.conf.d/nccl.conf
sudo ldconfig            # refresh the linker cache
</code></pre>
<h3>2 Export <code>LD_LIBRARY_PATH</code> <strong>and</strong> let PyCharm see it</h3>
<pre><code># ~/.bashrc  (or ~/.zshrc)
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib
</code></pre>
<p>PyCharm » <strong>Settings ▸ Build, Execution &amp; Deployment ▸ Target Environments</strong><br />
→ your SSH target → <strong>Environment variables</strong> → add the same <code>LD_LIBRARY_PATH</code>.</p>
<h3>3 Launch the kernel from the env root</h3>
<p>PyCharm » <strong>Settings ▸ Jupyter</strong> → edit your remote interpreter</p>
<ul>
<li><p>check <strong>“Activate virtualenv / conda”</strong> <em>or</em></p>
</li>
<li><p>add <code>--cwd /usr/local/lib/python3.10/dist-packages</code> to the kernel-start options.</p>
</li>
</ul>
<hr />
<p>After applying one of those solutions restart the remote kernel; <code>import torch</code> should succeed. It worked for me!</p>
","1","Answer"
"79623687","79623585","<p>I think this is a known PyTorch/Windows compatibility issue. Try to install it using:</p>
<pre><code>pip uninstall torch torchvision torchaudio
pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cpu
</code></pre>
<p>Also, can try to downgrade Ultralytics and test</p>
<pre><code>pip install ultralytics==8.2.7
</code></pre>
","0","Answer"
"79623758","79623585","<p>You're encountering this error because <strong>YOLOv8 (via PyTorch 2.7.0 on Windows 11, CPU-only)</strong> hits a known issue related to <strong>oneDNN</strong> (formerly MKL-DNN) — the CPU backend used by PyTorch to accelerate operations like convolutions. The error:</p>
<pre><code>RuntimeError: could not create a primitive 
</code></pre>
<p>means PyTorch fails to initialize a low-level CPU primitive, usually due to an incompatibility between your CPU, PyTorch version, and oneDNN defaults.</p>
<h2>Why Does It Happen?</h2>
<ul>
<li><p>PyTorch uses <a href=""https://github.com/oneapi-src/oneDNN"" rel=""nofollow noreferrer"">oneDNN (DNNL)</a> for CPU acceleration.</p>
</li>
<li><p>Some ops (e.g., <code>conv2d</code>) rely on advanced CPU instructions (AVX2, AVX-512).</p>
</li>
<li><p>On some machines (especially Windows or non-Intel CPUs), <strong>creating the &quot;primitive&quot; for those ops fails</strong>.</p>
</li>
</ul>
<p>This is not a YOLO issue — it’s a <strong>PyTorch + oneDNN runtime incompatibility</strong></p>
<p>One simple fix is to <strong>downgrade PyTorch</strong> to a more compatible version.</p>
<p>I tested it on my local machine on the following configurations:</p>
<pre><code>OS: Windows 11
Python: 3.10.9
Torch: 2.6.0
Ultralytics: 8.3.135
Device: CPU (no GPU)
</code></pre>
<p>(It should also work with your Python version).</p>
<p><a href=""https://i.sstatic.net/winiOc3Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/winiOc3Y.png"" alt=""enter image description here"" /></a></p>
<p><strong>Use the following steps to downgrade the torch:</strong></p>
<ul>
<li><p>create a virtual environment:</p>
<pre><code>python -n venv torch_env
</code></pre>
</li>
<li><p>activate the environment:</p>
<pre><code>torch_env\Scripts\activate
</code></pre>
</li>
<li><p>install typing-extensions with the given version</p>
<pre><code>pip install typing-extensions==4.12.2
</code></pre>
</li>
<li><p>Downgrade the torch (I used 2.6.0, use the below versions since all of them have to compatible with eachother)</p>
<pre><code>pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu
</code></pre>
</li>
<li><p>and finally, install ultralytics:</p>
<pre><code>pip install ultralytics
</code></pre>
</li>
</ul>
<p>These are all the steps that I did to setup in order to test your code and it's working.</p>
<p>Now, you're good to go and run your script as it is.</p>
<p>Here's the code:</p>
<pre><code>from ultralytics import YOLO
model = YOLO('yolov8n')
results = model.predict('C:\\Users\\PC\\Downloads\\IMG_4770.MOV', save=True, device='cpu', stream=True)
for r in results:
        print(r)
        print('======================================')
        for box in r.boxes:
            print(box)
</code></pre>
<p>output:</p>
<p><a href=""https://i.sstatic.net/M6Hqsaxp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6Hqsaxp.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79627453","79504951","<p>I also came across a similar error. It turned out to be a var that was being initialized inside the model's forward definition, but was not typecasted to bf16.
To debug, you can first do this:
state_dict = model.state_dict()</p>
<pre><code>for i, (name, tensor) in enumerate(state_dict.items()):
    print(f&quot;{i}: {name} → shape={tensor.shape}, dtype={tensor.dtype}&quot;)
</code></pre>
<p>This will give you a list of layers mapped to their position, like this:</p>
<pre><code>74: transformer.block.10.attn.q_proj.weight → shape=[5322, 6912]
</code></pre>
<p>Then you need to check the position that the debug message points to(position 34 for you), and then look for dtypes in the variable around that line in your model's forward method code.</p>
","0","Answer"
"79629951","79629559","<h2>Formatting input data</h2>
<p>The input of the model should be the same shape as the input of the first layer, which is a <code>Conv2D</code> in your case. According to <a href=""https://docs.pytorch.org/docs/2.7/generated/torch.nn.Conv2d.html#torch.nn.Conv2d"" rel=""nofollow noreferrer"">PyTorch's documentation on <code>Conv2D</code></a>, the input of such a layer must of the shape <code>(N,C_in​,H_in​,W_in​)</code> or <code>(C_in,H_in,W_in)</code>, where <code>N</code> is the batch size, <code>C_in</code> is the number of channels (1 in your case), <code>H_in</code> is the image height (28) and <code>W_in</code> is the image width (28). Since you only evaluate inputs one by one, you can use the second form (or <code>N=1</code>).</p>
<p>This means you should pass a tensor of shape <code>(1,28,28)</code> to your model. To obtain it, you could do something like :</p>
<p><code>formatted_array = torch.tensor(flattened_array_of_0_and_1).view(1,28,28)</code>, optionally followed by a <code>.transpose(1, 2)</code> to swap the two spatial dimensions if they are inverted in the resulting tensor.</p>
<p>You may also consider not flattening the data between the user drawing and the neural network inference, but you should probably still use <code>.view(...)</code> to add the &quot;channels&quot; dimension to your input tensor.</p>
<h2>Reading a prediction</h2>
<p>Classifier neural networks use the <a href=""https://en.wikipedia.org/wiki/One-hot"" rel=""nofollow noreferrer"">one-hot encoding</a>, meaning they are trained to output (for each training sample) a target vector of all zeros, except for a one in the dimensions corresponding to the category of the training sample. During training, we are trying to get as close as possible to such a representation, and during inference we pick the dimension with the highest value from the output vector, and use this as the predicted label. You can do this using <a href=""https://docs.pytorch.org/docs/2.7/generated/torch.Tensor.argmax.html#torch.Tensor.argmax"" rel=""nofollow noreferrer""><code>argmax()</code></a> : <code>guessed_digit = pred.argmax()</code></p>
","0","Answer"
"79629972","79629787","<pre><code>conda install pytorch-gpu -c conda-forge 
</code></pre>
<p>will give you the cuda enabled version of pytorch.</p>
<p>If this is &quot;recommended&quot; is up to you to decide (The question is by whom should it be recommended?). The pytorch package on conda-forge is a community effort, not officially maintained by the pytorch developers, who have <a href=""https://github.com/pytorch/pytorch/issues/138506"" rel=""nofollow noreferrer"">deprecated their official conda channel</a>.</p>
","4","Answer"
"79631646","79628713","<p>The issue is that <code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] == -1</code> which masks all devices.</p>
<p>A quick workaround is to use:</p>
<pre class=""lang-py prettyprint-override""><code>import os  # very first import
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;  # or different indices as appropriate

# all other, especially ML libraries after
import torch
</code></pre>
<p>possibly also before starting the notebook:</p>
<pre class=""lang-bash prettyprint-override""><code>export CUDA_VISIBLE_DEVICES=0
jupyter notebook
</code></pre>
<hr />
<p>This should work but does not fix underlying problem which is tied to the launch of jupyter or the kernel.</p>
<p>One possibility is that <code>CUDA_VISIBLE_DEVICES</code> is loaded from an <code>.env</code> file according to this <a href=""https://discourse.jupyter.org/t/jupyter-notebook-not-detecting-gpu/2272"" rel=""nofollow noreferrer"">comment</a>.</p>
<p>Another location where the problem could occur and can be fixed is to set the environment variables in the <code>kernel.json</code> file. For a detailed step-by-step setup see this answer <a href=""https://stackoverflow.com/a/53595397/12439683"">answer</a></p>
<p>Essentially check or define <code>CUDA_VISIBLE_DEVICES</code> in the <code>kernel.json</code> in the appropriate environment folder</p>
<pre class=""lang-json prettyprint-override""><code>{
 &quot;display_name&quot;: &quot;Your Env Display name&quot;,
 &quot;language&quot;: &quot;python&quot;,
 &quot;argv&quot;: ...,
 &quot;env&quot;: {&quot;CUDA_VISIBLE_DEVICES&quot;:&quot;0,1,2,3&quot;}
}
</code></pre>
","0","Answer"
"79632455","79631387","<p>Resolved. When I iterate over a dataloader, it calls the Subset's <strong><code>getitems</code></strong>, and not <strong><code>getitem</code></strong> (the one which I had overriden). And the former call's the dataset's <strong><code>getitem</code></strong> instead of the Subset's.</p>
","0","Answer"
"79635342","79635282","<p>It's looking like you need a clean ARM64 Python installation. How about trying this below;<br />
The first step is to verify what's the problem</p>
<p><code>python3 -c &quot;import platform, sysconfig; print(f'Machine: {platform.machine()}, Platform: {sysconfig.get_platform()}')&quot;</code><br />
If you see <code>x86_64</code> anywhere, that confirms there is a migration issue.<br />
The next step would be to clean the homebrew installation</p>
<pre><code># Remove migrated Homebrew 
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/uninstall.sh)&quot;
# Then a fresh install for Apple Silicon
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
# Install Python natively
brew install python@3.11
# Install PyTorch using the new Python
/opt/homebrew/bin/python3 -m pip install torch torchvision torchaudio
</code></pre>
<p>Let me know if this fixes it.</p>
","-1","Answer"
"79637165","79584215","<pre><code>x[batch_idx, channel_idx, :, :]
</code></pre>
<p>The above indexing operation fails because you're expecting Pytorch to automatically do the Catersian product betwee <code>batch_idx</code> and <code>chanel_idx</code> to produce all combinations of them, but it doesn't do that.</p>
<p><a href=""https://i.sstatic.net/OJwuRi18.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OJwuRi18.png"" alt=""failure at indexing operation"" /></a></p>
<p>What you want to do is use <code>meshgrid</code> to do this Cartesian product and reshape it for you.</p>
<pre><code>b_idx, c_idx = torch.meshgrid(batch_idx, channel_idx, indexing='ij')
</code></pre>
<p>where,</p>
<pre><code>b_idx:
 tensor([[1, 1, 1],
        [3, 3, 3]])

c_idx:
 tensor([[2, 3, 5],
        [2, 3, 5]])
</code></pre>
<p>Now you can index and add <code>y</code> only to these indices of <code>x</code></p>
<pre><code>x[b_idx, c_idx, :, :] += y
</code></pre>
<p>And it then succeeds (you can see the tensor shapes below on the edges of the graph below)</p>
<p><a href=""https://i.sstatic.net/MBIt35rp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MBIt35rp.png"" alt=""successful operation with tensor shapes of meshgrid seen"" /></a></p>
","0","Answer"
"79637742","79628802","<p>Add <code>import intel_extension_for_pytorch as ipex</code></p>
<p>This can solve the problem of insufficient VRAM when VRAM is sufficient.</p>
","0","Answer"
"79639018","79638455","<p>One way to avoid the <code>for</code> loop is using <em>boolean masks</em> for the parts of the concatenation where different indexes are needed in each row. The function <code>stack_proposed()</code> below uses this approach and mixes it with simple indexing where possible:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
torch.manual_seed(0xC0FFEE)  # For reproducibility

# Provide some dummy data
B, C, T1, T2 = 3, 2, 5, 7
l = torch.randint(0, T1, size=(B,))
x = torch.rand(B, T1, C)
y = torch.rand(B, T2, C)

def stack_given(X, Y, L, *, I):
    inputs = []
    for i in range(X.shape[0]):
        input_i = torch.cat([Y[i][0:I],
                             X[i][:L[i]],
                             Y[i][I:],
                             torch.zeros(max(L) - L[i], Y.shape[2])],
                            dim=0)
        inputs.append(input_i)
    return torch.stack(inputs, dim=0)

def stack_proposed(X, Y, L, *, I):
    assert (dtype := X.dtype) == Y.dtype
    assert (rows := len(X)) == len(Y)
    assert (channels := X.shape[-1]) == Y.shape[-1]
    L = L.unsqueeze(-1).unsqueeze_(-1)
    out = torch.zeros((rows, Y.shape[1] + L.max(), channels), dtype=dtype)
    
    col_idxs = lambda n_cols: torch.arange(n_cols).unsqueeze_(0).unsqueeze_(-1).expand(rows, -1, channels)
    col_idxs_out = col_idxs(out.shape[1])  # B×(T2+max(L))×C
    col_idxs_x = col_idxs(X.shape[1])  # B×T1×C

    # Part 1 (get Y[:, :I]): Use simple indexing
    out[:, :I] = Y[:, :I]
    # Part 2 (get X[:, 0:L]): Use boolean masks
    out[(I &lt;= col_idxs_out) &amp; (col_idxs_out &lt; I + L)] = X[col_idxs_x &lt; L]
    # Part 3 (get Y[:, I:]): Use boolean mask for writing, simple indexing for reading
    out[(I + L &lt;= col_idxs_out) &amp; (col_idxs_out &lt; I + L + Y.shape[1] - I)] = Y[:, I:].reshape(-1)
    return out

g = stack_given(x, y, l, I=5)
p = stack_proposed(x, y, l, I=5)
assert (g == p).all()
</code></pre>
<p>I would definitely time both approaches given your actual data though: I tried a few combinations of different shapes and experienced both speedups and slowdowns for the proposed version as compared to your current version.</p>
","0","Answer"
"79640166","79559122","<p>I had the same issue with 32B-Instruct model. What solved my problem was <a href=""https://huggingface.co/docs/hub/storage-backends"" rel=""nofollow noreferrer"">following this tutorial</a> for Xet files. I reinstalled hf-hub with <code>pip install -U huggingface_hub[hf_xet]</code> command and it worked. Hope it will help</p>
","0","Answer"
"79642991","79641308","<p>PyTorch will free the computation graph for <code>fake</code> after the backward pass. But then you attempt to calculate <code>lossD</code> from it. So the below is actually your main issue:</p>
<pre class=""lang-py prettyprint-override""><code>disc_fake = disc(fake).view(-1)
</code></pre>
<p>Instead, you have a couple of options.</p>
<ol>
<li><p>Just detach it from the graph, i.e. tell PyTorch not to try to track a gradient. The reason your code fails at the <code>lossD.backward()</code> is because there is no gradient to track, yet it tries. So you can instead change it to <code>disc_fake = disc(fake.detach()).view(-1)</code></p>
</li>
<li><p>If for some reason you need it, but I don't think you do, you can tell PyTorch to preserve your graph from <code>lossG</code>, i.e. <code>lossG.backward(retain_graph=True)</code>.</p>
</li>
</ol>
<p>I'm guessing you want option 1. Option 2 is probly not recommended since I don't think it's actually what you want, and it will increase your memory footprint.</p>
","0","Answer"
"79642992","79641308","<p>I solved it by putting <code>disc_fake = disc(fake.detach()).view(-1)</code> instead of <code>disc_fake = disc(fake).view(-1)</code> and apparently it is because the old way the gradients would have been propagated to the generator too, because <code>fake = gen(noise)</code>. <code>detach()</code> break this bond and everything goes alright.</p>
","0","Answer"
"79643044","79639173","<p>The output tensor is shape <code>(batch_size, 1)</code> because your model code explicitly throws away all timesteps except for the last one:</p>
<pre><code>out, (hn, cn) = self.lstm(x, (h0, c0))
out = self.fc(out[:, -1, :])
</code></pre>
<p>The <code>out</code> tensor from the LSTM is shape <code>(batch_size, sequence_length, hidden_dim)</code>. When you index into it via <code>out[:, -1, :]</code>, you are specifically selecting the last sequence item, returning a tensor of shape <code>(batch_size, hidden_dim)</code>. If you want the output to have all time steps, don't do that - simply do <code>out = self.fc(out)</code>.</p>
","0","Answer"
"79643511","79641245","<p>Consider using MLflow to help manage model saving and loading.
<a href=""https://mlflow.org/docs/latest/deep-learning/pytorch/guide"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/deep-learning/pytorch/guide</a></p>
<p>Saving the model</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow.pytorch

with mlflow.start_run():
    mlflow.pytorch.log_model(model, &quot;model&quot;)
</code></pre>
<p>Loading the model later:</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow.pytorch

model = mlflow.pytorch.load_model(&quot;runs:/&lt;run_id&gt;/model&quot;)
</code></pre>
","1","Answer"
"79646650","79646476","<p>In general, it does. For instance, matrix multiplication (for large matrices) will generally run faster if you do it after moving to cuda than if you do it before (because the GPU is optimized for that).</p>
<p>In your particular case, I don't think it matters a lot, because <code>unsqueeze</code> <a href=""https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html"" rel=""nofollow noreferrer"">does not re-allocate memory</a>, so it should be an extremely cheap operation no matter on which device it runs.</p>
","3","Answer"
"79646947","79646476","<p>Operations are executed in the order they are called.</p>
<p><code>some_data.unsqueeze(0).to(device)</code> first performs <code>unsqueeze</code>, then moves <code>some_data</code> to <code>device</code></p>
<p><code>some_data.to(device).unsqueeze(0)</code> first moves <code>some_data</code> to <code>device</code>, then performs <code>unsqueeze</code>.</p>
<p>For the specific case of <code>unsqueeze</code>, you are only updating the tensor metadata (shape/stride), so the impact is minimal. However for other operations, the impact of operation order can be significant. Generally speaking, if you want to perform operations on <code>some_data</code> on <code>device</code>, you should move <code>some_data</code> to <code>device</code> first, then perform the operations.</p>
","3","Answer"
"79647673","79647152","<p>Someone had the same issue with a very similar torch version (2.2.1), and the fix seems to be to upgrade to torch&gt;=2.3:
<a href=""https://github.com/pytorch/pytorch/issues/126632#issuecomment-2119140224"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/issues/126632#issuecomment-2119140224</a></p>
","1","Answer"
"79648329","79621735","<p>You can use a combination of &quot;Azure Function and Azure Event grid&quot; to achieve this.</p>
<p><strong>Event Grid</strong> lets you build this event-driven pipeline efficiently.  Whenever a new file is uploaded to Azure Blob, <strong>Event Grid</strong> detects the event and call &quot;Azure Function&quot;. Here is the Python code that listens to Blob Storage or API events and submits a job.</p>
<pre><code>from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
from azure.ai.ml.entities import Job

def main(event: dict):
    ml_client = MLClient(
        DefaultAzureCredential(),
        subscription_id=&quot;your-sub-id&quot;,
        resource_group_name=&quot;my-rg&quot;,
        workspace_name=&quot;my-ml-ws&quot;
    )

    job = ml_client.jobs.create_or_update(Job(path=&quot;inference-job.yaml&quot;))
</code></pre>
","0","Answer"
"79651151","79650016","<blockquote>
<p><strong>The MSE value is: 0.0004. That means the model predicts very well.</strong></p>
</blockquote>
<p>This isn't necessarily true. Typically, you divide the dataset into training and testing subsets, and evaluate the model's accuracy on the test set to get a more reliable measure of performance.</p>
<blockquote>
<p><strong>The problem now is that when predicting a combination that it didn't learn from the data set</strong></p>
</blockquote>
<p>Statistical models like neural networks aren't designed to predict on data points that differ from the training data distribution .</p>
<p>To make this system work, you'd need to transform the inputs into meaningful features. I would recommend you read more about how machine learning models work before proceeding.</p>
","0","Answer"
"79651425","79605098","<p><strong>RMSE per fold</strong></p>
<p>The current implementation doesn't aggregate the RMSE correctly. One option is to sum up the squared errors over all minibatches, average that, and <em>then</em> take the square-root.</p>
<p>Also, unlike <code>sklearn</code>, PyTorch expects <code>(predictions, targets)</code> as inputs to its loss/metrics functions, rather than the reverse.</p>
<p>Consider the snippet below.</p>
<pre class=""lang-py prettyprint-override""><code>fold_sse = 0.0 #tracks sum of squared errors over all samples

model.eval()
with torch.no_grad():
  for (inputs, targets) in validation_loader:
    outputs = model(inputs)

    # add up the SSE of the validation fold
    fold_sse += nn.MSE(reduction='sum')(outputs.ravel(), targets.ravel()) #PyTorch expects (outputs, targets)

#Compute the validation fold's RMSE
fold_mse = fold_sse / len(test_loader.dataset) #mean squared error of fold
fold_rmse = fold_mse ** 0.5

print('Fold', fold + 1, 'RMSE:', fold_rmse)
</code></pre>
<p>The overall CV RMSE will be <code>(fold1_rmse + fold2_rmse + ... + fold5_rmse) / 5</code>.</p>
<p><strong>Number of epochs</strong></p>
<blockquote>
<p>[...] how should one decide the number of epochs for cross-validation? I always assumed that this would just be 1 [...]</p>
</blockquote>
<p>Since you're spitting using <code>cv=5</code> (by default), each split is training the model on no less than 80% of the dataset, so I suggest giving the model as many epochs as you would during regular training. Or a few epochs fewer, if you need faster iteration.</p>
<p>Usually a few epochs is enough for model to present early indicators of its relative performance. Too few epochs and it would be too early to judge performance, and too many epochs results in overfitting plus waiting too long.</p>
<p>Use the same number of epochs for each split, unless you have a reason not to (e.g. you are also tuning early stopping hyperparameters).</p>
<p><strong>Train and validation splitting</strong></p>
<p>I think you can implement fold splitting more simply, without needing the <code>SubSampler</code> objects.</p>
<p>The example below uses the fold indices to index directly into the dataset, thereby filtering the dataset down to a train and validation split. I specify <code>shuffle=True</code> for <code>train_loader</code> to enforce shuffling.</p>
<p>I have also scaled up the batch size of the <code>validation_loader</code>, since during evaluation you can get away with a larger batch size.</p>
<pre class=""lang-py prettyprint-override""><code>for fold, (train_ids, validation_ids) in enumerate(kfold.split(cv_dataset)):
    print(f'FOLD {fold+1}')

    #Split dataset by indexing directly into it, and use shuffle=True during training
    train_loader = DataLoader(cv_dataset[train_ids], batch_size=5, shuffle=True)
    validation_loader = DataLoader(cv_dataset[validation_ids], batch_size=5*4)
    .
    .
</code></pre>
","0","Answer"
"79652943","79652783","<p>In most cases, torch functions that are applied to tensors with <code>requires_grad=True</code> should produce tensors with a <code>grad_fn</code> and <code>requires_grad=True</code>: the autograd graph should be automatically constructed.</p>
<p>However, when inside a <code>torch.no_grad():</code> context, or in a function with a <code>@torch.no_grad()</code> decorator, this automatic construction of the autograd graph is disabled.</p>
<p>So it could be that your whole function is within a <code>with torch.no_grad():</code> context, or has a <code>@torch.no_grad()</code> decorator (or the function that calls it has this context / decorator).</p>
<p>To debug this, you can check that the model parameters have the <code>requires_grad=True</code> field (you can print <code>model_nn[0].weight</code> for that), and verify that the output of this model (<code>y_pred</code>) has a <code>grad_fn</code>, and lastly that <code>loss</code> has a <code>grad_fn</code>.</p>
<p>I hope this helps!</p>
","0","Answer"
"79654468","79653147","<p>According to your error message from pip, the maximum version of torch that you're allowed to install is 2.2.2. From what I see on PyPI, it seems that 2.2.2 was the last version for which they released wheels for MacOS 10.9+ (see for example that the <a href=""https://pypi.org/project/torch/2.3.0/#files"" rel=""nofollow noreferrer"">list of wheels for 2.3.0</a> does not contain <code> [...]-macosx_10_9[...].whl</code>, but <a href=""https://pypi.org/project/torch/2.2.2/#files"" rel=""nofollow noreferrer"">the list of wheels for 2.2.2</a> does).</p>
<p>What is your MacOS version? If my understanding is correct, the only version of PyTorch 2.7.1 available for Python 3.9 and MacOS is <code> torch-2.7.1-cp39-none-macosx_11_0_arm64.whl</code>, so you will need MacOS 11 or newer for it to work.</p>
","0","Answer"
"79655192","79537819","<p>I went to the URL <a href=""https://download.pytorch.org/whl/nightly/cu128"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/nightly/cu128</a> to see the last version for each package, and installed them:</p>
<p>pip install --pre torch==2.8.0.dev20250605+cu128 torchvision==0.23.0.dev20250605+cu128 torchaudio==2.8.0.dev20250605+cu128 --index-url <a href=""https://download.pytorch.org/whl/nightly/cu128"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/nightly/cu128</a></p>
<p>Now ComfyUI works with my 5060.</p>
","0","Answer"
"79656383","79653745","<p>This depends on how you're implementing it. The AWS solution of utilizing the <a href=""https://github.com/awslabs/s3-connector-for-pytorch"" rel=""nofollow noreferrer"">Pytorch-S3 connector</a> should work but since you prefer not to use it, Try this:</p>
<p>The simplest solution you could implement is to download the dataset to a local directory on your training instance and instantiate your <code>ImageFolder</code> by providing the local directory. If you're training on a Jupyter Notebook, you can use <a href=""https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html"" rel=""nofollow noreferrer"">AWS's S3 CLI Sync command</a>. If you're doing this from inside a SageMaker Training job, you can just pass the S3 bucket in as a <code>TrainingInput</code> object parameter when you fit (see <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-using-pysdk.html"" rel=""nofollow noreferrer"">this</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html"" rel=""nofollow noreferrer"">this</a>).</p>
","1","Answer"
"79657065","79656972","<p>Generate random directly on the device:</p>
<pre class=""lang-py prettyprint-override""><code>torch.randn(x.size(), device=x.device)
</code></pre>
","3","Answer"
"79658049","79658001","<p><code>AutoModelForSequenceClassification</code> creates a linear mapping <code>nn.Linear(config.n_embd, config.num_labels)</code> and allows you to change <code>num_labels</code> via config.</p>
<p>So for example you can do:</p>
<pre><code>AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)
</code></pre>
<p>But also if you do that, you need to train the model otherwise the outputs are going to be random.</p>
","0","Answer"
"79658060","79658051","<blockquote>
<p>the weighted normalized value should have been = 0.1/(0.1+0.2+0.3) = 0.166666</p>
</blockquote>
<p>softmax doesn't do <code>x_i / sum(x_j)</code>.</p>
<p>It does</p>
<pre><code>softmax(x_i) = exp(x_i) / sum_j exp(x_j)
</code></pre>
<p>Try this</p>
<pre><code>&gt; math.exp(0.1) / (math.exp(0.1) + math.exp(0.2) + math.exp(0.3))
0.30060960535572734
</code></pre>
","3","Answer"
"79658158","79657416","<p>When you run a Python script from a Jupyter notebook, Jupyter automatically adds a <code>-f</code> argument (pointing to the kernel connection file).</p>
<p>Your VICReg training script's argument parser doesn't recognize this argument and treats it as an invalid training parameter. The best solution is to run the script from a terminal instead.</p>
<p>If you need to run it from the notebook, use the <code>subprocess</code> module to execute it as a separate process.</p>
","1","Answer"
"79659173","79657416","<p>I found the solution on a website indicating to add an empty space on the following lines of my code:</p>
<pre><code>if __name__ == '__main__':
    args = get_args_parser()
    args = args.parse_args(&quot;&quot;) # here the empty space is added as &quot;&quot;
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)
</code></pre>
<p>That actually solved my problem. You can check the following link for detailed information: <a href=""https://medium.com/@data.scientist/ipython-trick-how-to-use-argparse-in-ipython-notebooks-a07423ab31fc"" rel=""nofollow noreferrer"">https://medium.com/@data.scientist/ipython-trick-how-to-use-argparse-in-ipython-notebooks-a07423ab31fc</a></p>
","0","Answer"
"79659417","79659212","<p>When you call <code>to</code>, you are referencing a specific function implemented for the <code>torch.Tensor</code> type. The python <code>list</code> type does not have the <code>to</code> function implemented, so you get the error.</p>
<p>You can get around this in one of two ways:</p>
<ol>
<li><p>Call <code>to</code> on each individual tensor, ie <code>samples = [i.to(device, non_blocking=True) for i in samples]</code></p>
</li>
<li><p>Convert the list of tensors into a tensor, then call <code>to</code>, ie <code>samples = torch.stack(samples, 0).to(device, non_blocking=True)</code>. Note that this requires all tensors in <code>samples</code> to be the same shape</p>
</li>
</ol>
","1","Answer"
"79660858","79659981","<p>According to me, you could include <code>PyTorch JIT</code> files by adding this flag to <code>PyInstaller</code> command as shown below:</p>
<pre class=""lang-bash prettyprint-override""><code>pyinstaller --collect-all torch your_script.py
</code></pre>
<p>If you are using a <code>.spec</code> file, you should make sure to include:</p>
<pre class=""lang-py prettyprint-override""><code>a = Analysis(...,
    datas=[('**/*.pt', 'torch')],
    ...
)
</code></pre>
<p>Last point, your <code>JIT</code> must be loaded with a relative path or if you prefer packages using <code>add-data</code></p>
<p>I hope, it will help you..</p>
","0","Answer"
"79661191","79657483","<p>You specified <code>batch_size</code> as 20. Since there are 20 items in the dataset, that means it only needs one subprocess.</p>
<p>Experimenting with the sizes should produce the results you seek.</p>
","0","Answer"
"79662997","79658224","<p>&quot;evaluation_strategy&quot; has been deprecated since version 4.46 of the Hugging Face Transformers library.
<a href=""https://github.com/huggingface/transformers/pull/30190"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/pull/30190</a></p>
<p>Changing <code>evaluation_strategy=&quot;&quot;</code> to <code>eval_strategy=&quot;&quot;</code> should fix the unexpected argument issue.</p>
<p>Your configuration for the 6-label classifier looks correct (num_labels=6, problem_type=&quot;multi_label_classification&quot;). If you run into any errors, please share the traceback for further assistance.</p>
","0","Answer"
"79663055","79662773","<p><code>ImageFolder</code> expects files to be stored locally, not on remote cloud storage like S3. So you'll need to first download the files from S3 to your local system and then load them using <code>ImageFolder</code>.</p>
<p>Here's an example of how you can do this:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import boto3

def download_s3_folder(bucket_name, prefix, local_dir, exts=('.jpeg', '.jpg', '.png')):
    # Download jpeg, jpg, png files by default
    s3 = boto3.client('s3')
    os.makedirs(local_dir, exist_ok=True)
    paginator = s3.get_paginator('list_objects_v2')

    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for item in page.get('Contents', []):
            key = item['Key']
            if key.endswith('/') or not key.lower().endswith(exts):
                continue
            local_path = os.path.join(local_dir, os.path.relpath(key, prefix))
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            print(f&quot;Downloading: s3://{bucket_name}/{key} -&gt; {local_path}&quot;)
            s3.download_file(bucket_name, key, local_path)

# Only download .jpeg files
download_s3_folder(BUCKET_NAME_test, PREFIX_test, 'local_data/train', exts=('.jpeg',))

dataset_train = datasets.ImageFolder('local_data/train', transform=transform_train)
</code></pre>
<p>If downloading files to local storage is not feasible, you might consider moving the training process to AWS and running it on SageMaker.</p>
<p>In your train.py, you can use the S3 bucket as a data input channel, which SageMaker will automatically mount at the specified directory (e.g., /opt/ml/data/train). Then, <code>ImageFolder</code> should be able to load the dataset directly from that mounted directory:</p>
<pre class=""lang-py prettyprint-override""><code>data_dir = '/opt/ml/data/train'  # S3 mount point provided by SageMaker
dataset = datasets.ImageFolder(data_dir, transform=transform_train)
</code></pre>
","0","Answer"
"79664399","79653745","<p>You're right that <code>torchvision.datasets.ImageFolder</code> doesn’t natively support loading images directly from S3. The 2019 limitation still stands — it expects a local file system path. However, AWS released the S3 plugin for PyTorch in 2021, which allows you to access S3 datasets as if they were local, using <code>torch.utils.data.DataLoader</code>. Alternatively, you can mount the S3 bucket using <code>s3fs</code> or <code>fsspec</code>, copy data to a temporary local directory, or create a custom <code>Dataset</code> class that streams images directly from S3 using <code>boto3</code>. For large datasets and training at scale, the S3 plugin is the cleanest and most efficient path.</p>
","1","Answer"
"79664584","79641245","<p>Maybe you can refer to the new features of PyTorch, torch.package</p>
<p><a href=""https://docs.pytorch.org/docs/stable/package.html"" rel=""nofollow noreferrer"">https://docs.pytorch.org/docs/stable/package.html</a></p>
<pre><code>import torch.package

# save
model = YourModel()
pkg = torch.package.PackageExporter(&quot;model_package.pt&quot;)
pkg.save_pickle(&quot;model&quot;, &quot;model.pkl&quot;, model)
</code></pre>
<pre><code>
import torch.package
import sys
import importlib.util

# load
imp = torch.package.PackageImporter(&quot;model_package.pt&quot;)
model = imp.load_pickle(&quot;model&quot;, &quot;model.pkl&quot;)
</code></pre>
","0","Answer"