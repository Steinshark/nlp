Hello and welcome. I'm James Murphy. In this video, we're going to be talking about how to automate testing for your Python project. The goal of this video is to show you what you would have to do to take your project that might be something simple like this, just in a Github repo somewhere and you have maybe a single file with some code in it and maybe you have some tests already, maybe you don't but they're not automatically running every time you commit. So, we're going to take a repo like this that's very bare-bones. and turn it into something more like this. This repo has a specific structure to it. And as you can see, I've got a little badge down here telling me that my tests are passing. And it's not just a badge. It actually runs the tests. So, I can click on this checkmark here and we can see the details of the last test that ran. And you can see that the tests have run across multiple different operating systems. I have Ubuntu and Windows and across multiple different versions of Python. For each one of these tests, we essentially have an isolated environment that checks out all of our code and runs all of our tests in a way that would be similar to what someone would see if they were to use your project in a fresh virtual environment. Moreover, all of these tests run automatically every time I push a commit to my repository. So, here's the overview of what we're going to go over. First, we're going to go over how to either set up or restructure your project in order to make it easily testable including all of these different configuration files what do they all mean and what are they used for? Second, we're going to learn how to use pytest for running your tests, mypy for checking type hints and flake8 for linting or checking code style. Third, we're going to learn about tox and how we can use tox in order to run all of your tests in multiple different isolated Python environments. Finally, we're going to learn how to use Github actions in order to run tox and run all of our tests every time that we push to our code repository. Now, here are a few things that I'm not going to talk about. First off, testing is a huge topic. And it's under the umbrella of continuous integration and continuous delivery or continuous deployment. These are very very broad topics and I'm not going to cover them in general. I just want to focus on automated testing. I'm not going to be talking about publishing your package to PyPi or automatically building documentation, automatically formatting your code on git commits or any other pre-commit hooks. I'm also not going to talk about how to generate downloadable artifacts like coverage reports. And I'm not going to talk about more complicated builds, like if you're building a C extension. I just want to focus on testing alone because I think that once you go from no automation to adding automated tests that's sort of the biggest leap that you have to make. Okay, with that in mind, let's just dive into it. First up, Project Structure. We're here in PyCharm and I've already cloned the repository. You can see, we have a very flat structure and this is the project that we're working on. Basically, it's called slap that like button. and what it is is a model of what happens when you hit the like or dislike buttons on youtube. So, we have three states. It can either be a liked video, a dislike video or nothing, an empty like state. And then we just have some functions. And then I have some tests for what happens if you start it empty and then hit the like or dislike button some number of times. Well, for once the code that we've got here is actually pretty irrelevant to what we're going to be doing. You can replace this with one of your own files and it's not going to make much difference. Most of the work that we're going to be doing is just in the setup and configuration. So, don't worry about trying to, you know, pause and like look at what all these functions do. It really doesn't matter. The first thing that I'm going to do is take any tests that you do have and separate them out from the actual library code or application code. Our tests are going to be in a completely separate place from the rest of the code. Okay. So, I've just moved the test to a separate file called test_slapping.py in the same directory for now. and imported everything that I need in order to run the test. So, right now I'm actually depending on the fact that the test is right next to the module that it's testing so that I can import it just by using the name. It just so happens that because they're right next to each other this import works. For automated testing, I don't want to depend on that behavior. My tests should run no matter where they are. The way to make that happen is to make your project an installable package. So, I went ahead and created a source directory containing a package called slapping. Slapping is going to be the name of this slap_that_like_button package, okay? So, here we have the slap_that_like_button file that we had before and then we now have a separate test directory where I put the test_slapping.py file into it. Now that the things are separated, you can see that my editor is warning me that it doesn't know how to import slap_that_like_button. So, our package is called slapping. So, eventually it should look like this. But currently this package is not installable. So, we need to fix that. In order to make the package instalable in a consistent way, we're going to have to add a bunch of configuration files. The first of which is this pyproject.toml. Now, this is kind of a touchy subject for the Python community. There really shouldn't be any reason that I need eight different configuration files in order to make a package. But that's kind of the state of affairs right now. There's no reason that you should be expected to know what these files are or what things go where. Basically, you should just find a project that you already know and trust. And copy what they're doing. Okay, first up, the pyproject.toml. Basically, in the olden days there was only one way to install packages. You needed this setup.py file. But nowadays, there are many different alternatives. You can use something like poetry or flit. Or you can still use setup tools. pyproject.toml just tells us that we're still using the old way. Next up is, setup.py setup.py used to be the place where you put your installation script. It would do everything that you needed to do in order to install the Python package. So, you can run arbitrary code inside of your setup.py file since it is a Python script. And this is seen increasingly as a security risk. And so more and more of the code is being stripped out of the setup.py file and put into one of these other configuration files. Even though it's basically empty, this exact file is what's going to allow us to install our package in editable mode which will be important later on. So then, where do I store all the metadata of my project like the title and description? Well, all that stuff now goes in the setup.cfg file. Since this is just a configuration file and not a Python script, you don't have to worry about it executing arbitrary code. Here, you can see basic metadata, like the name, description, author, that kind of thing. Then down here, we have some information about what packages we're trying to package up. So, in this case we're packaging up the slapping library. And to install this library, we're just going to pretend that it depends on the request library. I don't actually use requests in this repository. I'm just showing you so that you know this is the place where you would put those dependencies. Another important option here is that we're saying that our packages are contained in this source directory. Then I go ahead and make a requirements.txt file that has all of my dependencies. In this case, just requests and it's good practice to give very specific version numbers. So, in the setup I just said requests bigger than or equal to two. And in this file, I'm giving a specific version. So, at this point, all of this setup and configuration has led up to being able to install the package. So, here's what I can do. I say pip install -e current directory. It went ahead and downloaded all the dependencies, namely the request library and it installed the slapping library into the current virtual environment. If you're wondering about this dash e and what editable mode actually means, all it does is that in the virtual environment, it just puts a link to the actual source code directory. That way, if I make a change to like the slap that like button file, I don't have to reinstall the slapping package. You can see now that when I go back to my test file, I no longer have a red squiggly line under the slapping library. PyCharm knows that slapping is an installed package and it knows how to import this regardless of the fact that these two are in completely separate directories. Again, we're able to do this because we made our project into a package and then installed that package into the current environment. Okay, now we can get to the libraries that actually do the testing. I'm talking about pytest, mypy and flake8 for the purposes of this video. That means, you guessed it, more configuration files. First up, I made another requirements file. This one with just the development requirements. And those are kind of separate from the normal requirements of the project. Because if you just want to use this, you would only need the request library. But if you want to run the tests, then you need all this other stuff. That's why I put them in different files. So, I just added these lines to the setup.cfg file. This section is how you indicate that a Python package has been type hinted. So, the slapping package is type hinted. This py.typed needs to be just a blank file right next to the init of the package. Then here, we have some configuration for the flake8 program that we're going to be using. So, this is kind of annoying. But some programs want their configuration in the cfg file, some programs want it in the toml file, some programs want their own configuration file. And there's no one right answer. Some allow both, some prefer one over the other. For each program, you just have to look at its documentation and see where you can put its configuration. I think the community is moving to pushing more and more of the configuration into the toml file and then just leaving metadata in the cfg file. But for now, this is just the state of affairs. So, our flake8 configuration is going to be here. But our configuration for pytest and mypy is going to be in the toml file. So, I'll just go to the toml file and then paste those in. I'm not going to go over all the configuration options and what they mean. I think they're pretty self-explanatory. Of course, we just added some dev dependencies to our project like pytest. So, we do need to go ahead and install everything from the requirements.txt Now that we have those installed, we can go ahead and run mypi on our source directory and see that we have no issues. We can run our linter on our source directory and see that we have no issues. And we can run pytest and we already told pytest and its configuration where the test is. So, with mypy and flake8, they're pretty self-explanatory. You just run them and they check your code and tell you if something's wrong. But with pytest, there's actually a little bit more to it. Namely, you have to know how to use it. It's a library and it helps you write tests. So, let's just go through some of the basic principles of how pytest works and what are its features. Essentially, it just looks through your tests directory which we specified in the configuration. And looks for any module that starts with test underscore. And then within those modules, it looks for any function that starts with test underscore. And then it assumes that those are tests and runs them. In this case, this is the only test that it found. But we actually have a bunch of different test cases in this one test function. So, I split it up into three separate functions based off of the behavior that they're testing. Even though these are all morally testing, what happens when there are many slaps happening. There's still a lot of different cases that are going on in this one function. If this first case were to fail, this test wouldn't run any of the rest of them. For these cases, pytest allows you to parametrize your tests. This mark parametrized decorator allows you to feed a bunch of different test cases into a single test function. So, I specify what are the arguments that I'm going to be passing in, in this case the input and the expected answer and then those values that I pass in appear as the arguments in the function. So, I'm going to have a test input of ll. So, if you slap the like button twice, then the expected answer should be that it ends up with the empty state. You pass in a list of tuples that have all the different pairs of test input and expected. And then each one of those becomes its own test. So, with this setup, it's the same tests as before. But if one of them fails, it's still going to run all the rest of them. You can also use this decorator to skip a test. For instance, here I have a test for a regex slap pattern where if you do any sequence of likes and dislikes followed by two dislikes and a like, then you end up in the like state. But of course, I haven't implemented this feature. So, I'm just going to skip this test. You can also use a related function skip if to skip tests based off of things like is the operating system Windows versus Mac. If you have a test that you know is failing and just for some reason don't want it to fail the build, then you can mark it with this xfail, expected fail, then it'll still run the test but it won't count as a failure for the purposes of did the build complete or not. However, an expected fail is not what you should use if you're expecting an exception. If I wanted to test whether I get a specific exception, there's a separate mechanism for doing that. So here, we have a test case where we're testing an invalid slap. The valid slaps are like and dislike. You can slap the like button or the dislike button. But here I'm trying to x. The way to tell pytest that you are expecting a specific exception is by using a with statement and saying pytest raises and then the name of the error that you're expecting. This test will now fail if a value error is not raised. The last extremely important feature of pytest is fixtures. Fixtures are what you use if a bunch of tests require a certain amount of setup that they all share in common. Imagine that, you have a bunch of tests that require a database connection. It wouldn't be good practice to copy and paste the setup code for the database connection across all of the tests. Instead, you can use a fixture that gets passed to your test. By convention, if you create a file named conftest.py and create a fixture in that file, it'll be available for all of your tests. You can create fixtures just right next to the tests. But if you want them to be available everywhere then put them in conftest. Fixtures are good because they allow you to avoid a lot of boilerplate setup and tear down code for tests. And also, they can have different scopes. So, here I use the fixture decorator which marks this db_conn function as a fixture. What that means is, if I have a test function that has db_conn as an argument name, such as this one, then what it's going to do is call this function and depending on whether it returns or yields something, it will take the either returned or yielded value and put that in as the argument to the function. So, in this case we can create a database url and then create a connection and then yield that connection to any test that wants it. You should use yield instead of return if you have anything that requires tear-down code. In this case, the connection presumably needs to be closed after all the tests are done and that's going to happen at the end of the width block. So, in this file, where I have db_conn as an argument, what's going to happen is it's going to call this create the connection and then pass it as an argument here to this function or any other function that has this db con as an argument. The default behavior for a fixture is that it will run the fixture code for every single test. But if you have something very expensive to create like a database connection, then you might want to share that same database connection amongst all the tests. The way you can do that is by changing the scope. So, if you say scope equals session, then it will only run this function once and then cache the value and pass the same database connection to all of your different tests. Fixtures can also depend on other fixtures. In this case, I'm making a fixture called capture_std_out that depends on a built-in fixture called monkeypatch. Suppose that, I want to test something like something gets printed to standard out when I run this function. Well, if I actually called the function and it printed to standard out, that's gone. I have no handle on that. A way I could get around this is by changing what the system standard out write function does. What if instead of actually writing to standard out, I had to just append to a string in a buffer. Well, then my test would be really easy. I could just pass the buffer around and check to see the value of the buffer at the end of the test. Now, what monkeypatch allows me to do is change the right attribute of the real system.stdout to my fake write function. What makes monkeypatch so convenient is that everything is automatically undone and put back the way that it was before at the end of the test. So, I haven't just messed up my standard out and pytest itself can still print say the test results. Then I could use my capture_stdout fixture like this. So, I make an argument with that name to my test function. So, it runs all of the code to monkeypatch standard out. Then I just call something that I know prints to standard out like the print function and then I go ahead and check what's in the buffer. So, fixtures can be extremely useful for setting up before test and tearing down after a test. Test can depend on fixtures and fixtures can depend on fixtures and there are a lot of really useful ones already built in. So, at this point, I can run pytest and see that I get all of my passing tests one skipped and two expected failures. One thing I haven't mentioned yet is this down here. This is called the coverage report. This is printed out because we put a --cov in the configuration. You can see that my tests cover 100% of the source code. What this means is that all of my tests combined touched every single line of executable code in my source. If this number is less than 100, then that means some part of your code is untested. Of course, that doesn't make it wrong and that doesn't mean that there is a bug. And having 100 test coverage doesn't mean that there's not a bug but it's just an indicator. If you look into the pytest coverage options, you can even get a printout with html and it'll show you exactly which lines are hit and which lines aren't. So, suppose that, at this point, all of our tests are running. mypy passes, pytest passes, flake8 passes. Everything is good. And everything essentially is good in our current virtual environment. That's what just running those commands outright does. But, how do we know that everything is still going to work in a fresh environment or using a different version of Python? Of course, to do all this, we need yet another configuration file. This one is called tox.ini Tox allows you to create a bunch of different virtual environments that are completely fresh and install your package into those virtual environments and then automatically run your tests or mypy or whatever else you want in that new fresh environment. These first four are just built-in versions of Python that tox already knows about. Their configuration goes in this test end block here. So, you see, I installed the dev requirements and then run a pyest command in any of those situations. These last two environments are not built into tox. You can define your own. So here, I define my own flake8 and mypy environments. And their configuration goes in these blocks. Since flake8 is not a version of Python, it's just a command that I want to run, I still have to specify what version of Python is the base environment in. So, let's just say that I'm going to be running my mypy and flake8 in a Python 3.6 environment. Then I just install whatever dependencies and then run the command that I want to run. Once I have this configuration and pip install tox, then it's as simple as just running the tox command with no arguments. You can see right now. It's creating the package. It's installing setup tools and wheel. It's creating the py36 environment. It's installing the dependencies for my project in that environment. And then, it's going to go ahead and run the tests. I'll go ahead and probably speed through this portion. Okay. And it finally finished and as you can see everything passed. so py36, py37, py38, py39, flake8 and mypy everything passed. So, we get a nice little smiley face. But if there's one thing to take away from this, is that that took a long time. That took way longer than just running any of the tests individually. So, why did it take so long? Well the reason is because, when I just run pytest by itself, I already have my virtual environment installed. But when I run tox, it's going to create a new virtual environment and then install everything into that. It's going to download from pip. It's going to install everything and do it that way. So, it could take much much longer to run through tox than if you just run pytest by itself. In my own development cycle, I run pytest all the time. That's very quick and very easy. It's only when I'm thinking about getting ready to commit and push, that I will run tox locally on my own machine. If everything passes in all the different environments, then it's probably safe to commit and push. So, tox allows me to run all of my tests in a bunch of different environments. But it's all still just on my own machine. How do I gain confidence that these tests are still going to pass on someone else's machine? This is finally where the automated part of automated testing comes in. I'm going to use Github actions to automatically run tox every time I push to my main branch. The way that you configure Github actions is with, you guessed it, another configuration file. You need to create a .github folder in your project at the top level and then a workflows folder within the .github folder and then a yml file within the workflows folder. You can have many different files with many different actions. We're just going to put all of our testing stuff in one. Give the workflow a good name. This is the name that's going to show up in the badge that we put in the readme. Here, we're using on to specify that we want this workflow to run every time there's a push or a pull request. This workflow is just going to have one job that runs. The test job. We use this matrix strategy syntax to basically create all the different combinations of environments that we want to use. So, we have strategy, matrix. Under os, we'll have ubuntu and windows and Python version will have 3.6, 3.7, 3.8, 3.9. Now, all of these things are pre-configured Github options. I can't just put in ubuntu 1.0.0 or something like that. It needs to be something that's specifically supported by Github. You can find a link to all the supported options in the Github actions documentation. But most likely, you should just be able to copy paste from this project to get what you want. Note here that, the matrix strategy tries every combination of each of these things. So, I have two os's and four Python versions that means this is going to create eight test runs. Then you just define what are all the steps of your workflow. So, we check out our repository. Then we set up the version of Python with the given Python version from the matrix. Then we install all the dependencies. So, we upgrade pip and then pip install tox and this tox-gh-actions. Then we run tox. Now I haven't said anything about tox-gh-actions before. Let me go over that. Basically, Github already provides its own set of actions that sort of mimic what tox does. And that would be fine if all you wanted to do was run your tests when you push to Github. But if we want to run our tests locally, then we kind of require tox. Because we can't run Github actions locally. So, someone created this talks-gh-actions package which basically makes Github actions and tox work together as well as they possibly could. Basically, you just have to give Github away to correspond your tox environments with the Github environments. So, we go back to our tox.ini file and then add this section specific to Github actions. All you have to do is list out the names of the Github Python version and what tox environments that corresponds to. So, for Github 3.6, we want to run our py36 and our flake8 and mypy. And 3.7, we just do the py37. 3.8 py38. 3.9 py39. And hopefully, if I haven't made any typos, then everything should be set up. Let's go ahead and commit our code and try a push. So, here I am at the Github repository and now you'll see this little orange dot. This little orange dot means that our actions are running. So, if I click on that, I see all these actions. Let's go into the details. As you can see, all the tests are running. In fact, it appears that they're all running concurrently which is good. It's not going to take as long. It's running the test with tox. It's going through the flak 8. And a few of them have already started passing. So, everything looks like it's running correctly. Let's just examine some of the output. Let's go into the tox. We see the same thing. 14 items, 11 passed, one skips, two expected fails. And it looks like everything is passed just the same as it passed locally on my machine. So, now I can have confidence that all this stuff works in Ubuntu and Windows and all these different versions of Python. Back to the main page of the repository, I now see the check mark that the last set of tests has completed successfully. But if I look at the readme, there's no badge here telling everyone how cool I am. So, let's see how to do that. So, here I am in the readme and this is all the code that you need to add in order to add a little badge with the check mark that tells you that your build is passing. Basically, Github just provides everything for you. You just need to navigate to the correct link. So, we have github.com your username or organization or whatever then the project name, then actions workflows and then the name of the yml file that you used and then badge.svg So, the workflow just finished and now you can see that we have this test passing badge. Just for fun, let's go break a test and then see what it looks like when it's broken. So, we'll get rid of the expected failure for the divide_by_zero and then that should definitely cause a failing case. So now, everything completed and we can see that we had three failures  and then Github will just cancel them if it notices that they're all failing. So, three of them failed and then it canceled the rest. And we can just go into the logs and we can see the zero division error. Now when we're on the front of the page, we see the red x showing that it's failed and we have a test failing here. And that's all there is to it. Feel free to look at this repository yourself. The link is in the description. Thank you to everyone that made it to the end for watching. If you did, don't forget to slap that like button an odd number of times. And while you're at it, go ahead and slap subscribe too. I'd like to give a huge shout out to my newest exponential tier patron on patreon. DRAGOS CRINTEA I'm sorry. I don't know how to pronounce your name. That was the best I could come up with. I hope you understand. I really appreciate the support. And for everyone else, please consider becoming a patron on Patreon. It really helps me out. Thanks again for watching.