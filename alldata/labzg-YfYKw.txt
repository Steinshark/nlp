it's not the most glamorous work but a large part of software development is about setting up connections between different systems that's all the more true these days we got the rise of the internet giving us vastly more data to deal with and then you've got the popularity of microservices and cloud services giving us even more systems that want to be connected together you have to be able to do that connection work to build anything of any real size I suppose could hanker back for the old days when there was just one big database at the center of our world but even then we had data integration problems we just solved it in a more ad hoc way custom connection software each time it's not the way forwards the way forwards and we are gradually getting better at this in the industry is to build reusable tools for connecting arbitrary system a with arbitary system B now some of those Solutions I really like but they are admittedly quite big like Kafka red panda that kind of thing there's quite an upfront investment some do an excellent job with a very specific approach I'm thinking of things like deum if it fits your use case fantastic if it doesn't then you got to keep looking but my ears pricked up recently when someone recommended I add to my list benthos as a kind of lightweight way of getting some kind of connection up and running really quickly something I could add to my toolbox as a bread and butter tool that was more formal and more reliable than a shell script but was a similar investment of my time to get something working so joining me today is Ashley Jeffs Ash is the creator of benthos and it's a project that started at his day job where he was creating a lot of data pipelines and the project went open source and got more and more popular until he hit that dream that some of us have his open- source project became his day job we talk about about the how that happened and how that Journey unfolded but mostly we talk about benthos the tool and what it can do for you the design Sweet Spot it's aiming for what it wants to be and what it doesn't want to be we're going to get into that but before we do quick aside I have to say the benthos project has as its mascot a blobfish and when we recorded this ash was sitting next to an adorable stuffed toy blobfish and I couldn't resist mentioning it but I did break a rule of radio in doing so I'm talking about something you can't see so if you're listening to this on the audio only version please imagine a man sitting next to a Melancholy pink stuffed fish there's a sentence you don't hear every day if you have that vision in mind we can get started I'm your host Chris Jenkins this is developer voices and today's voice is Ashley Jeff's [Music] joining me today is Ashley Jeffs Ash how you doing I'm good thanks how are you very well very well nice to see uh the company logo in the bottom corner yeah this is a custom crochet from a fan did not marketing spend is still zero as befits an open source project but it's it's nice that you've got crocheting fans out there that's a very specific crossover of use or family thereof I think actually in this particular case I do a bit crushy where I'm trying oh yeah okay well that's that's definitely a topic for a different podcast craft voices but for now I thought so we're going to talk about benthos and I thought the way we get into that was something I actually use dayto day I have a script that goes to YouTube's API and grab some YouTube data as you might expect given what I do and it does a bit of paing on it and shoves it either into CFA or a SQL database Python and I want to ask you have I done it the hard way uh so the F the first thing I would ask is do you really care if it fails is it is it the sort of thing that if it fails you'll just run it again because you're running it as a CLI or is it on some server and you'd rather not think about it so I do run it manually I wish it were run in an automated fashion but then I'd have to worry about failure more than I do ah okay so yeah that's exactly where I come in usually cuz I think if if people see that they've got a particular script or like a a use case that just does some simple like plumbing and they come to me and they say hey can we do this with benthos if they're happy and they don't really care if it fails and they're quite happy to run it manually I will usually say not unless you're trying to learn what benthos is um because it's it's just another tool if you had a script and you're happy with python then you know why why Rock the but but it's it's when you've got some sort of like plumbing system that you could it's not necessarily a streaming application as it currently is but in your case you would probably want that to be almost like a stream where it's polling on some interval um and then spewing the data through and you you don't have to think about it it's just it's just running automatically so I would cons some people would say that's like a batch job I would just consider it a stream because it's hands off you're not you're not hitting anything manually you're not you're not maintaining anything manually and the problem with doing that is the question of what happens if it can't send data to your database what happens if it can't send data to Kafka what happens if the Transformations fail what happens if it can't hit the API all those things um if you want a nice answer for uh looking after all of those aspects then yeah that's when you want to use something more um stream process e that has already got a nice answer for all those questions and kind of forces you to deal with them um so yeah th those are the times when I would say say yes it's worth learning a new tool and one that's kind of like in the streaming space I guess or you know like a a workflow processor or some some general data engineering tool um that's now mostly config not necessarily script or code um yeah I certainly would like it to run for the data to just magically show up more often in my case but it also in Cas is where things like um the output is just unavailable for a period of time if that happens at 3: in the morning you don't want to get woken up by some alert so you want it to already have in its own um mode of operation some answer to dealing with that which obviously in the Stream processing space is usually just back pressure and alerting if you've enabled it uh but like an opin thing but ideally you want to be able to wake up and see that oh there were some issues last night but it's just it just carried on like it just fixed itself it's it's sorted and I can see that in the metrics or logs or whatever you didn't you didn't have to do anything uh it just it just resolved itself but in cases where it can't resolve itself like your database is just broken um you know you wake up and maybe there's some logs there that tell you hey you've got to fix this thing and while I wait for you to do that I've just stop myself from from operating for a bit uh that sort of stuff yeah yeah okay so I want to get into how you do that and what your design choices are but I thought before we did that one big interesting design choice you have up front is to limit the scope of what this does it's not trying to do all databased data system to Data System processing under the sun why and what's the scope um I think B well from the initial um conception of the project it's because I didn't I I had a lot of uh engineering problems where I worked that were basically in the single message transform space where you're doing like enrichments and you're hitting external services and then you're aggregating the results into like a single payload that thing gets um pushed along whereas typical data engineering tools are all usually catered around windowing systems so they fit the the mental model of it's basically a database but it's a streaming database um and I didn't want any of that stuff I didn't care about any of that stuff I just wanted systems that you can compose that will do enrichments and brokering you know reading from multiple sources writing to multiple syns and you know do doing sort of what what would be uh chores in the streaming World usually people would just make bespoke tools for this sort of stuff and then they would use like the bigger tools like flank and you know SQL over stream whatever product you're going to pick for that for like the the big tasks um but I wanted a solution that I could just keep redeploying with config that's going to solve the the what I consider to be the boring stuff traditionally they the things that you just like throw away to like an engineering team and say hey build a tool that's going to read data from Kafka and then hit our sentiment analysis tool that's owned by the data scientists and then you know remap the uh data to to fit some of the schema and then then you know dump it in elastic search or some database or something like that I don't want to have to write that same program over and over and over again because I was at a point in my career where I know that that's dangerous uh you know dealing with like writing the same streaming application over and over again but with different code every time you're gonna hit edge cases because delivery guarantees are actually super complicated people um and you know there are these edge cases that it's not necessarily if I'm writing code and throwing it over the wall I don't necessarily care if the operations team have a bad night sleep um but that's terrible but that does happen but sometimes they get so angry that they'll make it my problem and then you know I do have to think about those things so you know I I was in a realm uh at a company where we just had loads and loads of streaming tools that were doing all kinds of different things and I wanted something that was just going to solve the operational uh side of things so delivery guarantees uh good behavior around recovery and and hitting issues and then the idea is that the the bit that I would build is the ability to compose these simpler broken down problems and there's nothing stopping you from adding complex stuff on top like there's no reason why you can't have windowing algorithm implemented within benthos um and in fact there is one there's a there's a very basic um windower but the point is that's not that's not what you're necessarily deploying every time you use it so you can you can have the really simple use cases um and start from there very very simple tool and then it only reaches the complexity that your use case has essentially when you're like adding stuff to the config right so this is making me think I mean there used to be this old thing in Pearl right that was the job of pearl to make hard things possible and easy things trivial and you're on of make the easy stuff trivial end of the design Spectrum yeah defin 100% yeah I mean the the initial use cases I had for benthos were the most trivial uh almost like obnoxiously simple use cases for stream processing like imagine like we're just migrating from you know Kafka to Nats or something like that where you're just making a bridge uh maybe some buffering um and then the idea was you make that really really simple uh especially to express in config everybody hates yaml so you want to minimize the amount of yaml they have to write and then the idea is the every piece of functionality that you add on top of that so obviously you can get more complex with different brokering patterns you get more complex with processing patterns um and error handling and swim laning all these things but all of those features are introduced as just blocks of config that you can add uh but you don't have to learn about it you could you could use benthos as as a user for for years and not have any idea that any of this stuff exists and you definitely don't have to do any operational steps in order to enable things that you're not going to use so you don't have to worry about discs uh and and distance any stuff like that it's it's stateless um and just memory based essentially and it's only if you were going to opt into something more advanced that you then have to deal with the implications of that and yeah there definitely been a day one goal because it reminded me I mean it seems pretty straightforward to say it reminded me a bit of um Docker containers right you set up this thing you set up that thing and you describe another bit that connects the two together and hopefully you're done yeah I mean all of these sorts of tools um were kind of up and coming when I was conceiving of the the general project Ben so I mean one of the things that did really drive the uh the way that it operates is containerization the idea that you're just going to have this one thing it's portable you can deploy it and also the idea that you can just deploy one of them um because you know back in those days if you wanted to test Kafka but you wanted to use containerization that literally wasn't a container there was no image for running Kafka you had to to like use all these hacky weird um custom builds and then it's like multiple containers you having to work out the networking for all these things and it was a nightmare from from like a de if you just want to run this stuff just to play with it as a you know developer researching these tools it was an absolute nightmare um so yeah in the Forefront of my mind the whole time when I'm you know building these new tools is the idea of like what does it look like for somebody to explore this tool what does it like for them to do a Hello World uh test and yeah basically if you can just do a one liner uh you've got a thing running and then you move on from there um that's the that's the high level goal is is you can get started with the very very Basics it works and then you you know you kind of dig deeper into it right that explain because I um I tried it out quickly because it's I without sounding like I'm pitching your stuff but I tried it out you do like benthos create input SL processing layer SL output and it'll create you a config file yeah and I just guessed I thought I'd put you to the test and I was like input file slash I don't know I said JQ SL SQL I think and it did just work created me a config file so well done on the the developer experience at least the initial stuff but how's it done how have you implemented this uh well the whole thing's in go um and that was basically just cuz I was having fun with go at the time um I was a C++ developer primarily and then um I kind of had this so a lot of the tools I was building these bbok streaming tools I mentioned they were all in C++ basically um and we were I was part of the team that was managing all these different things and there was definitely this belief of like it has to be C++ for this to to run as well as it does it has to be written in C++ and you know I didn't necessarily um question that myself you know I I just figured it probably ought to be proven um so I took one of the services that was essentially just a bridge with a with a buffer so you know it was actually reading from zero and Q to Kafka and vice versa um and then doing a bit of disc buffering on top of that with memory map files and I thought okay well how how much is the damage if I if I wrote this in go and I did it in a really cheeky way I'm using go channels because I'm lazy and I don't want to like have all this custom stuff I'm just going to use the basic Primitives what does that look like for performance and it didn't run as as memory efficiently it didn't it didn't run as fast but it was well within I mean it was something like an extra 10% um of time and CPU results on top and that's with like no optimizations no thought process of like trying to make this fast it was just what's the easiest for me to maintain um and then yeah from that point onwards it was like okay well I guess I guess I'm never writing C++ again then um just I tried to basically double down on this tool okay but how did you um how do you implement it because you've got it's one of those classic integration things your biggest problem I'm guessing is two biggest problems are reliability and pluggability because you want to support All Things to All Things totally reliably let's start with All Things to All Things how do you do that okay so the the it's evolved over time um but essentially the internal uh representation of an input the inputs are the more complicated ones uh so the the internal model of that is it's a thing that creates messages by some means it could be you know pulling stuff over a network or it could be making stuff up um but essentially it doesn't really matter that's part of the plug-in implementation that you have so the kafa one will obviously be reading CFA partitions um and the that's one will be reading NS messages the file one will be reading a file by some scanner so either lines or whatever ever um and then the idea is that as it's as it's parti it's creating these messages um it return essentially introduces them into a benthos pipeline as what I call a transaction um and what that is is a mechanism that Associates a given payload of data or more it could be more than one it could it could already be a batch um directly from the source uh it Associates it with a mechanism um to acknowledge that payload of data so from Kafka it would be a mechanism that ensures the uh partition is um marked with a given offset uh for a message um obviously that gets more complicated if you want to process messages out of order and you want to make sure you're not um marking offsets that technically haven't finished yet um but essentially that's all encapsulated in the in the acknowledgement mechanism and that's abstracted as just basically a function um the that's associated with the payload and then it gets pushed through a benthos pipeline using go channels um the channels mechanisms those are things that you don't really touch if you're if you're developing a plug-in like if you're developing the Kafka plugin you don't have to worry about the channels you're just defining how a message is uh formed and how a an acknowledgement is established um and you know what to do if if the message is rejected as well uh because that will be different depending on the input some inputs have a sense of a knack that you can push upstream and then some of them don't so you would just it wouldn't make sense to just drop the data because just because it got rejected so what you'd have to do is You' have to make sure it gets reintroduced into the pipeline um and that essentially that channel mechanism that ends up becoming the the uh the lower level I guess you call it representation of an input that can then get hooked up to any number of layers uh let's call them so the obvious layer is the output layer uh which receives traffic see it receives these transactions over a channel and then for every transaction it will try and deliver the data potentially multiple at the same time so there could be like a maximum in Flight that an output has configured for itself or a user has configured um so you could have like 200 messages in flight at any given time and then what its job is uh at the plug-in level if you are writing a kafa plugin you're just writing a definition of you receive a message how do you serialize that in to the data that gets sent to Kafka in this case you're just getting the raw bites and some record headers um and then you either return an error or you don't it either succeeds or it doesn't um you can add a bit of complexity with batching so for example you could Define how to rather than deliver individual messages you might want to represent how to deliver a batch of messages and you be benefit from performance there um especially in the world of CF you might want to send a block of messages uh and then what you can also do is you can translate an error that comes back you can break it down by messages of the batch and then you can return like an a benthos representation of here's a failure that happened for this batch for these given messages so if you're able to some inputs won't be able to but if you're able to the for whoever formed this batch um if you able to then break it down into just these indexes and retry the ones that failed and not the others go ahead and do it otherwise R the whole thing um and then once you've got those basic abstractions you can then form the high level ones so brokering patterns in benthos you can have like fan out uh sequential round robin um you can have switches for swim laning all that kind of stuff okay those are abstractions around the channels um so they're they're able to do uh pretty much real- time flow control um and it's it's nice for me as a developer to to benefit from like go channels for that sort of stuff because it it helps you when dealing with all the nasties all the edge cases such as back pressure retries having multiple things in fight at the same time um you know all all those uh nasty stream processing problems you can basically solve with go channels which is doesn't make it trivial um you're not going to solve it overnight but it makes it a lot easier to both write and reason about once you've done that yeah what's the word tractable doesn't make it trivial but it makes it tractable right got a chance of solving it in a sane way I'll thumbs up that okay so um I'm trying to stack the things I want to dive into here so the first is in order to get into how we handle errors there must be statefulness right you can if you've got a cfer input your progress through the topic will be stored on the broker so you don't worry about that but if you're reading through a file your progress through the file file isn't going to track that for you so Ben must be stateful so um with the specific file input uh we I don't track any of that stuff so basically the way that the file input works is it's not a streamed input um which means if you read it and then you crash the service and you run it again it will just read the whole file again um people have been asking for like watch uh watch mechanisms and things like that so you can like read it gradually but again this is one of those problems that I've just figured I don't want to solve that uh I'm not I'm not writing a log aggregator um for example you can obviously process the logs once they've been written into CAF or something but the the problem set around doing something like watching a file um for the delivery guarantees perspective if you want to do it properly there's a huge amount that you got to implement for that so I figured I'll leave that to the other tools that specialize in that sort of stuff um so the file input and benthos is basically just how can you do like almost like a batch job um in which case if you restart it uh after a crash you'll just run the batch again uh pretty much as you would with a with a normal batch tool um okay so then that leads into delivery guarantees so yeah so delivery guarantees obviously within benthos um the goal is at least once uh as as like a core Foundation you don't you don't have to do anything special and it will do it but the obvious caveat to that is that if your inputs and outputs don't support at least once delivery guarantees and obviously benthos doesn't um so if you're if you're writing data over UDP stream then you know you can't guarantee that the data's gone anywhere and similar if you're reading data from standard in you can't guarantee uh because there's no guarantee that data that's been consumed by benthos has also been delivered by benthos it just doesn't exist um that's that's basically one thing that I'm I'm quite happy to just s of leave out there you document it in the input that you can't really have an expectation of strong delivery guarantees with these things but the things that you do expect to Li guarantees on you don't have to think about it it'll just work um and that's really where my focus is right this is again delineating where the where this tool begins and ends so let's talk a bit more about what happens when it does go wrong is it just drop the world and start again or is it can you do something more sophisticated than that so it'll depend on your config uh but the you've got you've got different options in the streaming person world right you've got um you've got reject uh so you can you can Knack a message so let's say you're reading from sqs just to get or get a range of Q systems into this conversation but um imagine it's got a system Upstream where you could you know you have like a uh uh a q like a reject Q um a dead letter Q is one I'm looking for um and you don't want to just keep retrying a message internally in benthos Forever uh if if it's a bad payload right so essentially the default behavior of benthos is something reads a message it goes through whatever processing it gets to the output layer uh processing Cannot drop data so error hand handling in the processing space so say like mapping filtering um all that stuff if errors occur there there's a different mechanism for handling that the data itself always travels through benthos so if you don't handle your processing errors messages that aren't processed will be delivered and you'll have to deal with it yourself uh it doesn't drop data under any circumstances because in my opinion it's easier to deal with oh we've got some weird messages appearing in our Kafka topic we must have done something wrong than 10 months later oh we've actually been dropping 10% of our messages because yeah some issue um so processing is kind of like a separate topic which we'll have to dig into um but assuming you know the data makes its way to the output layer um we attempt to deliver it if that fails the default behavior of benthos is to reject the transaction that comes from the input layer so if the input layer is um gats or rabit and Q whatever it will Knack the message um if that's not the case so saying cfal land doesn't make any sense you can't Knack a message because that would mean that that offset is just done yeah you're going to lose that data if you don't reread it um so what happens instead is Kafka will enforce a an internal retry and it will never acknowledge it will never um store an offset that is still in the process of of being handled so if you imagine um you've read a message that's uh too big to be delivered it will reach the output layer the output layer will reject it what'll happen is it'll get knacked um and then the cfer input will say yeah but that doesn't exist so I'm going to pass it back through the the processing pipeline um and what you'll get is back pressure because the output layer will eventually stop delivering data uh if it's not going anywhere so you'll um it'll try its best if there's some data that's in like a retry Loop and you've got like Maxum flight of greater than one it'll attempt to continue to deliver traffic and you'll see like error logs and Metric telling you that there's data that's being not delivered um but eventually it's going to grind to a halt um and the B pressure is obviously important because you don't want to be retrying an indefinite number of messages so you have to have some some number at which messages being retried and and blocking the pipeline is going to stop the whole thing from consuming the input will then stop being asked to deliver data so it will then stop and then what you'll get is benthos Will effectively slowly grind down to a halt as more and more data doesn't get delivered and then you as the operator of this pipeline at your leisure um can come along and figure out okay well why is data entering this retry Loop what processes do we need to add like maybe a filter that just you can just drop the data if you want to but it has to be explicit so if messages are this size just delete it I don't care and then you rerun benos with that new config and it will reach those same offsets because it didn't commit them so it'll reach the same offsets reread that data drop it and then it flows um like a Happy Fish um in freshwater and the the idea is that every possible Edge case fits some model similar to that where the worst case scenario is you have a task as an operations person to you know adjust this config or expand this config in order to deal with edge cases that you haven't anticipated but the idea is that the data isn't just gone it doesn't it doesn't just disappear we don't just move on and forget about it we make you deal with it but you don't have to deal with it straight away you don't get like an alert like oh my God this whole thing's dying because the data is still in CFA right it still exists somewhere you're using Q systems that have delivery guarantees which means it's persisted on disc somewhere hopefully more discs than one and you don't need to panic like then the stream processing world doesn't need to panic there's no need for urgency in any um form other than syn right yeah exactly you don't there's no reason to to wake somebody up at 3:00 a.m. if as long as you can process the backlog so as long as as long as you don't end up in a situation where you can't catch back up again um but obviously you know if you're in a situation that's that tight then yes I would strongly consider it a an advantage to to get ahead of some of these issues before they might arise rather than just relying on benthos to to hold hold your hand through it right but presumably I could set up like a fallback that just sent it to a pager system that texted me as soon as there was any the the default output obviously if you've only given it one output it's only got one place it could possibly Ru that data to and if it can't then it it will just apply back pressure but if you've given it a full back um so there's there's a bunch of different programming patterns but one of them is if the first output fails try this one and try this one and try this one and because of the whole composure of of benthos you can add processors specifically to outputs themselves so you can have like a full an output of deliver the data to CFA right and then you can have a fullback output that's also Kafka but there's a processor on it that says if if if the payload looks a bit dodgy just send this metadata instead um so you still move on you don't retry the data and the system doesn't apply any back pressure but what you've got is you've got a record somewhere that it doesn't have to be the same topic it could be a different topic like dead I literally just send it to a dead letter que that's annotated with extra metadata yeah exactly so the the pattern is there for dealing that and then obviously you can have more fullback outputs after that as well um so you know you could you could just write to standard out uh you could you could write Devol if you wanted to or you could just delete the data obviously because because you can just put processes in there as well so you can also like have have processes and just like delete the data or you know send an HTP request somewhere or as you said like you could you could hook it up to alerting um if you get to that point and then there's there's other brokering patterns so unfortunately we're going to have to have this whole conversation again because as soon as you add um a brokering pattern like fan out for example the error handling has to look completely different right because there's Now new edge cases so in the world that I described where if if you've got two outputs Kafka and uh say Nats um and the kafa is failing consistently either because it's offline or or some of networking issue uh but the other one isn't you don't want to have a situation where a message travels through the pipeline gets to both gets to both outputs that it's rooted for manages to be delivered to one but not the other and then it's retried again and again and again and again in like a fast loop uh because then you're going to flood gats with duplicate data yeah and you're still not delivering anything to cfus so when you add an output broker that's a fan out um by default it will isolate the retries to the output itself so for example if you if you did rout a message to Nats and Kafka and Kafka fails what the broker will do is it will keep it attempting the message at Kafka and it won't Knack the message um it'll just keep trying to make sure that we don't do that busy Loop instead we have like a soft Loop happening at the N level and obviously you still get the logs and the metrics and stuff but by default it's not going to enter that busy Loop but if you do want the busy Loop you can have the busy Loop by just adding a bit more config on top uh that just essentially would force The Knack um because I mean you might you might want to hook it up to uh like Rabbid mq input and still have it delivered to a dead let at the input level Upstream even if it did get to deliver to Nats but not Kafka um but yeah that's brokering there's like a bunch more patterns as well okay but I mean this is all stuff you could do with more sophisticated tools I guess the edge here is that it's fairly easy to set up these patterns yes I mean config in benthos is you know talking like 20 lines of yaml to have that broken pattern I just described with some processing on top you could have that in like 20 lines of yaml config and then you got the metrics logs uh all the observability that you need um plus it's portable plus it doesn't need any access to the dis uh plus you know the operational Simplicity is stacked uh massively in its favor uh but yeah you're not getting the the like the bigger fish like Flink for example that it's obviously going to do way more advanced super massive use cases if your if your needs are that like if you need these super Advanced window and algorithms and super efficiency on you know transferring and storing all that data then you know sure you're going to need to reach for something else but if your use case is I just want to read some kafa enrich it there's like 20 HTTP services that are like this interconnected network of things I need to hit I just want to store that in a file that my data scientists can edit um you know they can change oh it's not a post it's a get like you know they might want to do some trivial change uh or you know actually the payload slightly different now it's it's capital instead of all lowercase for this field they can just go into a yaml file and modify that submit it as a as a PO request to you and you can just you know click it approve um rather than them having to modify your code yeah yeah much rather tell a data scientist to edit a AML file then uh updates and CFA streams Java yeah and we don't we also don't leave you completely uh out in the woods as well I mean it's yaml obviously people do have issues with yaml but you got linter which is very nice there's also um bunch of Dev tools for for building benthos yaml configs and like holding your hand through the whole process uh there's like an explicit schema as well so I mean if you for example you can use Q if you've heard of Q CU um goine project it's basically a a um better configuration system I'm going to say uh but essentially it's it's more advanced and it's it's much more explicit um so you could use that we generate a q schema um and we also generate a Json schema if you want to use that to to help you build your configs out that sort of stuff okay then let's talk about the stuff around it like that so operationally what have you got for monitoring and that kind of thing so there there's obviously logs um I don't like logs personally I've never I've never really liked uh logs as a as a way of monitoring a service so um that obviously highlights specific issues if they occur you can look at the log and it'll describe what exactly happened but there's metrics for throughput um and latency and and all the stuff that you you would consider important in a stream processing system every component that you add to a benthos config will also have its individual metrics so for example if you've got three processors are mapping um some JQ uh or you know some some HTTP hit you're going to hit some service they will all have individual metrics by label that you can dig into with dashboard so if you want to specifically monitor the errors um hit by your HTTP um service request then you can have a specific thing for that um there's also um uh what do they call it now the distributor tracing stuff so I've got open Telemetry support for for dist Trac so you canuse and all that stuff um that is actually really cool um because you can you can literally look at an entire journey of a message through a benthos config so if you've got a massive complicated benthos config you can literally see uh a picture of um its Journey but personally I don't really use it I just use the metrics um because obviously I know what the metric should look like and if if it doesn't look like that then I want an alert straight away um but then also so there's no there's no like formal um alerting uh system in benthos what you would do is you just hook it up as part of your it's like an output for your config so like you described you could have a fullback that's just hit you with an alert directly uh so you could have like a page or an email or whatever or you know a slack message um and you know things like that can just be glued into your config as if it was just any other destination because at the end of the day it needs to have the same delivery guarantees right if you've got a message that's failed getting an alert because it's failed is probably just as important as delivering the data itself in terms of delivery guarantees so the idea that you might just not get an alert because it was kind of hooked up as like a second class citizen of the pipeline that's like if you think about it that's not really that good a thing that's that's obviously something that you would probably want to address if you could um so yeah we just I just kind of treat that as like a just any other uh component um so the big three of the logging metrics and distribute tracing um where does the the metrics is how do you access that is that is that like does it come with a web gooey or oh there's lots of options so you can have um you can have Prometheus uh scrape it you can send it to Stats d uh there's influx DB there's Cloud watch metrics um basically for those options you're just it's just a config block right so you're just saying instead of the default of Prometheus um send it to Cloud watch this address and it's like a few lines of your config um because by default it's it's Prometheus and you scrape an endpoint that benos hosts um but you can also if you want to do things locally and you don't like reading Prometheus metrics and you don't want to hook up an actual uh metric endpoint you can just have it spit out Json um and you can do that two ways you can have it so that you can scrape an endpoint and get Json formed metrics so like you can see the counter go up every time you refresh the page but also you can add um the ability to log the metrics if you wanted to so you can run benthos as like a a one-off job say it's like a batch job or something and it runs to a file and then writes it to like Kafka or whatever and then what it does at the end just as it exits it will spit out like a a Json block of of metrics for the run like the latencies and and things like that which is pretty cool I didn't add that that was somebody else who contributed that which I think is pretty cool okay um that leads me in contributing right so from the list I've got you support fairly large number of inputs and outputs and a surprising number of different processing layers are you writing all those yourself are people contributing them is there a plug-in mechanism what's the deal there so I would say about half would probably me uh and then probably the other half of people just coming in and and just adding stuff that they want um and then there's a smaller number of people who are dedicated and and they add things because they think other people want it it's kind of like a mini version of me um and the way it works is that so there's been different generations of the plug-in API um to kind of reach the right level of abstraction because obviously I'm trying to make the config for benthos simple for people using benthos but also then there's the the exact opposite end of the um Dev Spectrum where I want the developers building plugins to also have a fairly easy experience and it's the same philosophy of if you've got a really simple component that works basically like all the others you should just be able to write essentially just a function uh for how to deliver a given piece of data or you know consumer piece of data or process piece of data and the plug-in apis are essentially designed so that you can you can Define the configuration spec so what does the configuration look like that includes things like default values for Fields whether things are optional whe the fields are Advanced because you want to be able to generate nice documentation um so C you essentially Define what the configuration for a component looks like and then you define you know the the uh thing that you want to do with that configuration as like usually just a nice a nice function um or you know like a struct that implements a certain interface um and the idea is that if you've got a more advanced user case that needs a little bit more control for performance reasons or just because of its functionality so I mean if you going to implement a broker for example needs to be a little bit more to it than just a function um for those you you would use a sort of more advanced API that sort of builds upon the other one and then more advanced one if you want to opt into other functionality IDE yeah yeah yeah so and most of those are internal so I've got like a public API um and that's the one that most plug-in authors will use and that that allows you to have your own custom build of benthos with your own custom plugins and I would actually say that a huge chunk of benthos users have own custom build with their own plugins in um and the idea is that they're first class citizens so you can generate documentation and it will be the exact same benthos website as the official dos with your plugins it there um and it's all the same thing um and you know the create Tool uh that we talked about and linting and all that stuff works the exact same with their plugins as it does for everybody else and you can obviously contribute official benthos plugins that way as well so it's the same API so somebody can write their own private plugin and then decide later actually the world needs this okay and they can come and uh basically just copy paste it as as a PR um and then obviously there internal ones let me think how that plays out so I'm working at a bank trying to connect super ancient uh Mainframe to some Modern SQL database let's say so I write is it presumably some go code does it have to be go implements your function it doesn't have to be it's a lot easier um but we've got there's like a bunch of bunch of options so go is the best I would say for for just really huging into the uh the apis for the config specs and all that stuff um but if you want to you can just execute a subprocess uh and and just oh okay essentially read that um not particularly good for delivery guarantees because you're kind of reading off a just a stream of bites rather than like a back and forth protocol um you can also just hit an a API so I mean if you want to you can just run your like a sidecast service that exposes an HTTP stream endpoint and then use benthos to consume that there's also some web assembly stuff in there so right now I think I'm pretty sure I should probably know this I've only I've only added in web assembly processors so you can you can like Define a web assembly uh thing in whatever language you want and then execute that as like a processor eventually I would like to have it so that you could Define an input or an output with that as well but the web assembly uh experience is a bit uh confused right now I'll say it doesn't really match the ethos you definitely right but eventually I could write these things in Rust I guess as my go-to web assembly thing at the moment that's the goal I think that's that's kind of like the dream one of all these like hip uh web assembly native languages the rusts and things you can stick them in there you won't get better performance on anything but you know you can still do a hip language go still counts as hip doesn't it no I think go is business suits now oh is it that okay fair enough um and so these private plugins I'm doing this kind of sounded like you were saying I need to maintain my own Fork of benthos it's not dynamically loaded it's not technically a fork so you would have a it would be a custom build so it's a go build um and then what you do is you basically import penthos as like a library and you toine your plugin and it like registers you can have like an isolated environment of plugins or you just have like a global one and then you basically call a function that's I think it literally is just run CLI and it will basically execute benthos as a as a CLI so you're essentially running the official benthos or if you want to get a little bit clever um there's like a an API for building benthos streams in code and you can have as many as you want so you could like Define a bunch of plugins and then you can you can execute like multiple streams in the same binary and stuff like that and you're essentially running benthos programmatically with that which I do know quite a few organizations do because they they don't just want benthos they want to kind of uh nest benthos in their own kind of ecosystem so they want like custom inputs and outputs and things as well um that they might want tighter control over the rather than just like plugins if that Mak sense yeah I can totally see that so I end up writing probably a bit of go with a very thi main function in most cases yeah the main function is three lines okay gotcha yeah okay what you've you've led into this if we're running multiple benthos processes within that custom main function potentially we get into the issue of um kind of clustering I mean if I if I want to do some processing that's too large for a single machine do I still use benthos or do I need to break out into the meteor tools uh there's no reason why you can't basically it depends on the input so if you've got Kafka for example uh the default behavior um is if you run multiple instances then they'll they'll have a consumer group so the partitions get distributed um so even if youfa yeah yeah exactly so it's it's essentially determined by the the sources um and you know obviously the majority of the ones in the streaming ecosystem it just you just get it for free um so I mean it's it's definitely been asked for people will come in and say like hey are you going to add any like coordination between penoses and I always just kind of think well why what are you doing with that is as long as you can fan the workloads out like what else is there there's no State there's there's nothing happening yeah yeah um if you care about because obviously if you care if you're doing some sort of window because I said like Ben has like a window capability is there right um so if you if you needed to make sure that your uh your window messages are all um of a given type um or like given group say um you're already dealing with that in CFA by the partitioning schema so you're you're keying messages that are of a group of a window by the key in the first place so you know you've already solved that problem and it's the same with you know storage you know persistence you've solved that with kfka or Nats or rabit and Q um you know that's an issue that you've already had to deal with so you know why make somebody deal with that again by having benos instances have to coordinate because then you got to work out well how am I going to enable that just use the The Source like use use your input that's that's what it's for that's what it's built for yeah and why make Ash write it again yeah exactly why give me extra work that's fair enough okay so another thing um that leads into you talked a bit about window so you've talked about enrichment I'm trying to get the boundary lines here so you've got enrichment some sometimes that implies joining different inputs and sometimes it doesn't but when it does it implies statefulness so what kinds of enrichment can I do in a stateless world uh so well the easy one the easy pattern um and if I talk about this for for what 30 minutes you won't bother me about the more complicated stuff the easy one is you've just got a single payload that is just the world right so I mean a tweet and you want to enrich it with what's the language what's the sentiment uh who's it mentioning sorry an x uh and you know all that stuff maybe you've got like a data science team they've deployed a bunch of services that will do that sort of stuff so this is kind of like benthos is bread and butter um because what I did is I I replaced A system that was very um stateful and handling all these relationships between these things you have to do the language detection first you have to do the sentiment analysis afterwards you have to do this afterwards and then there there like this massive dependency graph of um enrichments and in benthos world the you also have to negotiate each API differently so imagine each team who's made some enrichment will do it differently it might be a different company might be some completely out of your control um so the way all this stuff works in bentos is it's composed so we've got an HTTP um thing which just does a request whatever the contents of the message is it will be sent and then whatever comes back replaces the message um but then you can compose that within what's called a branch and the branch will describe a way of transforming the current message into something new and then you do any number of processes so one of them will be your request and then there is a mapping afterwards that describes how to merge that back into the original payload so you don't you don't lose the original contents of the data and you don't have to send everything out which is a big big problem for efficiency if you've got this massive payload and you have to send it out to all these different Services you obviously don't want that you want to just create like a subset of the message and then what comes back you're going to just form it back into the the the new payload um so there's kind of like this abstraction of uh mapping do the enrichment and then map it back um and then what you can do is you can take that that block and you can compose that so if your use case is complicated you've got this big network of of things you have to hit um we've got what's called a workflow processor very loaded term uh but in this case what it means is you've got a bunch of of these branches these enrichments that you want to execute and they essentially have like a dependency graph so you have to you want to do maximum parallelism so if you got a bunch of services you can just hit straight away you want to just hit those all in parallel and then aggregate the results and then whatever depended on those you'll do those and so on and so forth um and you can either do that automatically by allowing benos to analyze these mappings um so it knows what you're using as a reference to the enrich ments and it knows what you're mapping back into the payload so it can kind of like build a best um attempt to work out what the dependency graph is or you can just make it explicit you can just add in a a list of these ones parallel these ones parallel these ones parallel um and then it will it will execute those things in a streamed fashion so you know it's reading CFA data for example it might be processing I don't know 24 messages at the same time because that's something you can tune is how many messages get processed in parallel and then each one of those will enter this workflow uh execution little mini engine you've configured um and then all these parallel requests are happening and then you can make it a little bit you can add a little bit on top of that to be efficient around batching so for example you can create micro batches at the input level and then send those batches to your workflow and then the branches themselves can all have custom Behavior as to whether they send out individual requests or if they create a batch so so say like a Json array or lined limited messages or whatever um whatever the individual protocol is of those Services your uh like little encompassed um enrichment config can basically choose how it handles uh batches just for a little bit more efficiency than um just leaving it all into like single message uh interactions um so that's the simplest one then obviously it gets it gets more so basically what I've done there is I've taken systems that I've already seen in the past um that have essentially a flipped mentality of um messages come through and we're going to hit all these services and then each each um service is essentially a stage in a workflow stream um so you know the output of one will be then stored back into Kafka potentially and then reprocess stored back into kafa and then process again stored back into kafa and it's slow moving pipeline um and you've got you know potentially persisted data multiple multiple times um and what I've done is I've kind of like flipped that on its head and just said well okay if if realistically we're only talking like minutes to do all of these requests you could just do that in memory like there's no reason why you can't as long as it fits in memory and you know that flow isn't going to change because the the speed at which you can hit all those endpoints doesn't magically change because you've persisted to the disc over and over again so if you can realistically just do this in memory um why not and you know the idea is that you've you've then just made this composed config that it's not NE I wouldn't call it stateful technically it is because it's got state in memory it's got a message in memory but if everything crashes and it starts back up again it just does the same the last message that didn't make it all the way through gets repr process little bit of duplicate effort you're not going to notice it unless there's something really wrong that's not an issue for most people uh operating at a pipeline and if it is an issue you can do the same pattern as before you could do the you know stream um one after the other approach because it's all composed um so there's the more complicated thing which is joins and that is that's essentially something that at the beginning I didn't really care too much for because in in my mind like windowing systems that these data engineering uh products were were putting together um was kind of like a separate thing to something else that our team was doing so something that I was doing as part of my career was doing data joins through caching so you might have like a m cached instance or reddis or something and what you would do is if you've got multiple streams that you need to join um you read all of these different Q systems and you essentially populate the bits that you need from them into these caches and then you choose one of them as like the canonical stream and that's the one that you flow through and you basically hit all these caches almost exactly the same way that I just described all the enrichments where each cache represents a bit of the stream that you need to join it with some other topic or some other Q system or it doesn't really matter what it is but essentially you need to acquire piece of information based on some signature from the data and then whatever comes back you then want to merge it back into the the the new payload that's being formed and then eventually you either make it all the way through or you don't in which case then the interesting Behavior comes along where maybe you put that into a dead letter ke that's got a delay on it and um maybe that's teared I don't know like as far as you want to go um you essentially make sure that payload is going to be retried uh if we couldn't find all the information that we needed and what that essentially represents is your window uh because you're obviously going to put a cap on how much time you're willing to wait for a payload to reach all of its um other pieces of information in the pool um you know maybe a day let's say that means your window is now a day and rather than having conceptual window that actually exists where it's all one weird thing that's been sort of like designed to to work on a dis or S3 or whatever this conceptual thing uh what you have instead is you just have a bunch of memcached instances with some info in it um that's not going to be anywhere near as efficient for some payloads um so there's there's like efficiencies that you can definitely benefit from by not using that approach but if that's not the case and it's literally just there's a blob of data with a key uh from all these different topics and I just want one overall blob of data based on that key you you can get away with it you can you can have massive throughput uh huge volume data pipelines where um you have complex joins and you're not doing anything interesting there's nothing there's nothing going on there so your your operational job as as the person deploying all these Services is to make sure that these caches are good so you know obviously need to make sure they've got enough retention the discs are there and you know either there's um backup uh or you know you've got some other uh some other system in place for for recovering that data if you need to mechanism for repopulating the cash if it crashes exactly so you might have some mechanism for doing a backfill um if you find that some of those caches have died in which case you also need to have a policy for like how long we need to know how long that's going to take like realistic because we might never catch up so stuff like that you need to have like an answer for um but as long as you're comfortable with that sort of stuff then you now have a very very simple pipeline it's an extremely simple config for the actual busy work that benthos is doing and you know that benthos if if it crashes Midway through a job it just restarts and it picks back up where it was um the data in Kafka you need to make sure that that's you know replicated and persisted you had to do that anyway um but that's essentially where your data lives that's that's its real location until it's been delivered somewhere safely you're at least once delivery guarantee is existing CFA that's just the the reality if if your CFA clusters all die and all the discs are gone your data is gone and that was always going to be the case um so that's the bit that you worry about in terms of like data retention the caches need to be alive uh and obviously Kafka is what refills them um if something goes wrong that's probably a procedure that's going to be manual right you're going to have to make sure that you're able to manually kick off a back fill in which case you'll have some procedure that the operations team needs to know but in my mind that works out a lot simpler operationally then you've also got this other tool that's doing this complex aggregation on disk that also has state that you might need to recover because now you need to understand what that looks like what does the recovery look like for that how do you make sure it's backed up how do you you know do all these other things um mcash is obviously lossy AF um so you know the the idea is that you don't have any pretense that that's a safe place to keep your data forever the the knowledge is always there that it's Kafka that is the the real source of of persistence yeah I can also see some argument here for um as long as configuring these pipelines up is easy there's an argument for having it you know I can get you this join put together as a proof of concept within an hour and then it's going to take a Sprint to do it in CFA streams or something to make it more or fault tolerant um there's tremendous value in being able to ship it within the hour right there there's a a lot of people who use benthos for like proof of Concepts and then um what happens is they they have a bit of fun with it and then I get them and then they they stay with it there so maybe we should talk about that next um because this is I mean you're unemployed employed unemployed this is your technically I'm self-employed but I'm I'm not a very attentive boss let's say what what's what's your life as a programmer working on benthos tell me about that H so I feel like um this kind of this kind of situation because basically I live off ad hoc like support contracts and stuff which was basically just me um doing doing stuff to live until I figure out what I actually want to do and I've just been doing that now for like four years um but then I also the majority of my income now is sponsors um but that's that's an extremely lucky situation uh I definitely would not be able to just reproduce um but essentially that just means I can I can figure out what I want to work on at any given time um and with that freedom um the thing is like in in a situation you've got like an open source project and you are trying to keep it going that's the number one concern and the concern I don't have is taking over the planet um it doesn't need to be the most popular thing out there it doesn't need to it doesn't need to be any anywhere near close um for me to essentially get my goal which is to live off keeping this thing going um essentially my job is to attend the co-base obviously that's like the obvious one that everybody knows I have to write some code every now and then but you're also evangelizing a little bit so that in my mind that doesn't mean marketing and finding new people um it's developers that do that for me uh I just make sure they're happy enough to spread the word and my evangelism is making sure that the documentation is good and fun um the uh various support channels that we have are active so we have you know lots of people in our uh various chats we've got Discord and slack um and you know if as soon as a question pops up like we one of us is usually on it within a couple minutes um and then you know you make make done videos and you go on people's podcasts and stuff like that but essentially the way that my day-to-day looks is whatever I feel like doing is what I'll do because a lot of the I just I like a bit of variation I like a bit of context switching personally so if I if I wake up and I just don't feel like coding one day there's obviously a lot of not coding stuff I need to get done yeah like you know documentation testing uh videos you know all those things and what you're doing is you're basically switching roles so I might be like a product guy one day and I'll just have like a three-hour shower where I'm thinking of you know but what does the next three years look like and how are we going to get there and you all that stuff and then you know maybe the next day I just want to draw stupid blobfish and you know crochet them or crochet them and I'm just basically I'm just doing dumb artwork that one day I might go oh that's perfect for this thing and then I'll like put it in my blog post and I don't have to worry about oh I need some graphics for this it looks a bit boring for a lot of us this is the dream right this is the programming dream any tips 100% I I feel like I feel like open source is one of the things where like everybody can dabble in it a little bit and then we all have this like pipe dream of like oh one day I'll make money from this and it's definitely great uh for a lot of reasons but also if you don't have the self-control to do certain things then yeah it ends up becoming a nightmare because what'll end up happening you basically become a CEO but it's for a company that doesn't exist right so you can easily like get yourself stuck in a situation where it's like you're basically working for free now on stuff that you don't enjoy so keeping the joy is the fundamental bit because you are going to be working for free and then just kind of like hoping that you can live off it by you know some means um and if you if you feel like you're stuck and you're you know you're doing stuff with no compensation and you know oh my God I need to get a real job at some point um you're going to freak out and you're not going to enjoy it anymore uh so yeah it's basically an aspect of you have to keep this Joy going you almost have to you're you're not really your own boss you're more like your own uh I don't know Guru or something but you're trying you're trying to get into your own head of like how can keep myself enjoying this thing so that when it's when it feels a bit tough like there's obviously days where you don't want to deal with people or you don't want to fix this bug or you don't want to handle this PO request or whatever um that you're going need to kind of like get into that head space uh but then yeah the main trick is don't don't burn out and that that means you've got to be able to like jump and I think most most people would not enjoy getting to that stage like once you've got a lot of users and you need to be doing a lot you have a lot of spinning plates a lot of those plates are things that people don't want to bother with and it it ends up not being as fun um but yeah I mean obviously if you if you can make it work then yeah it's great it's like a a dog chasing your own tail every day I'm like distracted and you know doing whatever I want but also you know kind of being led by the world is essentially telling me what I need to do at any given point that's nice because when you got a large user base you're not like living in a cave disconnected from the world you've got these people anchoring you and keeping you saying I guess yeah yeah definitely so we have like regular Community calls as well so we can actually see each other's faces um on the very rare occasion we actually physically meet in a real world space but that's obviously very difficult to coordinate um but a lot less lonely once you've got a big enough community that people can be around a lot um and not just like five minutes every month when they've got a question yeah okay that's cool that's cool I um I hope it continues for a long time full of joy I have to ask you there's one more topic I have to cover and it's a little bit out of order because we normally end on the lighter stuff but let's go to something harder the processing layer right I know you support you've got quite a few options the different ways you can write processing including orc excellent choice for the old school kids uh you've got JQ in there I know you've got a few others um but you've also got your own language for that right y you're developing what is it blob tell me blob blob blang yeah blob blang so why yeah so I didn't have it for a long time I didn't want to I didn't want to build a custom mapping language um and you can obviously fit there's like brilliant go libraries for for JQ James path orc all these things um and brilliant people behind them as well so I I wanted to get those in um because it it unblocks people you know you can do like arithmetic and all which was was a blocker at the time when I brought that in um and obviously a lot of people are used to JQ and things uh there's like a JavaScript processor around now for people who want to do that um eventually I would like to have a python one if we can um I don't want to limit people's choices as to what mapping languages they use and I I kind of felt like it wasn't really my job to solve mapping or anything either um the problem we had though is there's a lot of mapping that is um very very uh complicated and messy for like big nested structures the there's definitely a justification for having a a language that specializes in that stuff um I luckily I didn't really have to design that because there was um there was a language called idml um which is open source it's very very small project uh but it's a very very cool language um that was essentially designed specifically for mapping you know it's either Jason's Json or it could could be anything it's just structured data really and idea is that you can really dig into the nested horrible um like massive structures that people are used to with a lot of enrichments um you know where you've got like a raise of tokens or something like that where it's just these really gnarly like deeply nested objects that you might need to zip up with something else and all this other stuff um so I kind of had it in my head that I was going to eventually support idml itself um but it was really difficult to get it involved in in in go because it's basically jvm to go so it was it was pretty pretty messy experience but then the other side of things is I didn't just want to have this extra mapping language I also wanted to have it sort of like a native component within benthos so that we can do clever stuff with it so the fact that blob blang is is essentially native to benthos is how so the branches I was describing where we can infer the dependency graph of enrichments the reason why that's possible is because the map to translate to and from an enrichment is blob blang and I can analyze blob blang to see what are we depending on um right as part of the data transformation and what do we create at the end of it when we're merging it back into the new object so the fact that I can analyze that means I can then infer what's the dependency graph because I can see where where does certain Fields come from and who uses those fields that sort of stuff um but also it it was apparent that I I can obviously support JQ to an extent and I can support orc to an extent but having a mapping language that I actually wrote makes it a lot easier for me to support people with really complex use cases because I can immediately see oh you can fix this you do this so from from the support aspect it's been a massive benefit because um you know somebody can could be coming in with like a different use they might have Orc in their config but the fact that I can just give them some blob blang that does something really gnarly and complicated but it specifically fixes their thing um kind of frees me up then I'm I'm kind of unblocked on this this particular question that's interesting because there are so many places in the design of this where you've solved the problem by saying that's someone else's problem push that out to a different system that takes care of it this is a place where that's reversed and you said I'm going to bring that particular problem in house yeah exactly and it's it's definitely a scenario where you've you You' kind of made a compromise here where you're going to have this thing that you have to support indefinitely um and it's not trivial making a mapping language yeah um I took a lot of inspiration from idml so I didn't have to design it completely from scratch and because I've kind of had a lot of time to mess around with use cases and stuff I knew roughly what it had to look like in terms of like uh error handling and flow control all that stuff um so I wasn't coming at it completely from scratch but it was literally just writing language from scratch uh I initially thought I was going to put it out there and it'd be like a niche feature within benthos and then I would probably just let it sit as that almost like it's it's just the way of careering fields and that would be it um and then I was I was just going to see how people received it because I don't want to force people to use a a new language um because that's obviously the benefit of having all these different processes and stuff um but people just used it like people went with it and used it so I thought okay well I'll support it then I'll I'll you know build it up and it's it's got its own plug-in API so you can write your own blob blang functions just the same as you write um other plug-in types and you can also use it as its own Library there there's a few few organizations that are using it as their kind of translation language but again yeah yeah exactly so they've basically just got this thing running as like a library in their own applications um and it's it's not really something that I'm like putting out there as like its own its own uh project because I don't have the resources I don't have the energy to do that and I don't I don't have the energy to kind of convince people that this is you know this awesome language that does all these things um I'd rather just leave it out there and and if people want to use it they can use it but it it solves the benos problem which is it's a lot easier for me to support people use it it's kind of like native to language so it kind of ticks a few boxes that I didn't have checked until I made it um and it's not it's not a massive thing to to maintain it's not um it's not to in complete or anything like that so it's it's like fairly simple as far as languages go it's easy for me to keep keep on top of okay okay so if I want and if other people want to get started with this um where should I start uh go to benthos dodev and there is a few options on the website so you can either do a getting started um guide uh by just going to the docs or there's a video uh where I I talk about it if you want to see more of me um then you can go go down the video route I tend to find people there's like different categories of people some people need a video to to feel engaged in something and then some people hate videos yeah um so we we've just got all of them if you go to ben. Dev you can find the exact thing that you need some people just want to jump straight in if you go to benthos Dev there's like five config examples on the on the front page for various things so if you if you literally just want to copy paste a config run it and then start from there and and play with it that's probably what I would have done to be honest um then you know you can just go ahead and do that um it's easy to install because it's go so you can either use um you can either just download the binary or you can uh use home brew and things like that um be Dev it's go I'm assuming then it's Windows Mac and Linux yeah yeah cool MIT license yep I think that's all the headlines then I I think I think I'm GNA I've got the rest of the afternoon free I think I'm going to spend it trying to um convert my python script and end where we began oh good yeah you can send me the the script and I'll give it a I'll give it a burial thanks very much Ash great pleasure talking to you thank you for having me thank you Ash and I hope you'll be sending me a toy blobfish when those go into mass production one day as we said if you want to check out benthos it's benthos dodev but actually since you now know what it does the place I would start is there's a page that lists all the input sources it supports and all the processor types and all the outputs and that very quickly tell you what it could do for you today so I put a link to that it'll be the second link in the show notes that takes you straight to that list before you click that link if you've made it this far please click like And subscribe and rate and all those buttons and the algorithms will then make sure we see each other again soon oh and if you happen to be listening to this on Spotify on the mobile app please consider rating developer voices I don't know why but you can only rate podcasts on Spotify on the mobile version so it's an oddly specific feature that I have to make an oddly specific request about from time to time and you know I've been thinking about it I bet there is some weird internal reason why it's set up that way so if you work for Spotify and you can get permission to talk about it please get in touch and come on the show I would love to do an episode about the difficult realities of writing software at a company the size of Spotify with the user base and the number of platforms that Spotify has I think that'll be fun all that said I think it's time to go I've been your host Chris Jenkins this has been developer voices with Ashley Jeffs thanks for listening