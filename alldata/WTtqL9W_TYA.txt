um I'm Philip Schubert I'm a static analysis researcher at Pottermore University and at the end of this talk you should be able to implement your own static analyzes using llvm and also you should be able to develop your own optimizations believe it or not so let's see how that works out but first of all who said Philip truet guy and why am I here actually so um this is Milford University the University of the city of bielefeld the city that doesn't exist for the German friends here International people maybe have already heard about that otherwise ask your German colleagues but anyhow this is the place where I got spoiled because a few years ago I studied bioinformatics and the algorithms involved in solving bioinformatical problems typically have a very bad asymptotic run time so you have o of N squared on a good day and typically it's much worse right so that means it's hugely important to craft a really optimal implementation of your algorithm to really squeeze out the last bits of your processor right and what language can be used to achieve that you're right it's not C plus plus it's c because that's the language I learned in our operating systems class it's still the case that in at most universities there are no C plus courses and we should probably try to change that in the future and um so I implemented a project for my Master's thesis in in C and it really worked out and yeah and now now see right and but I thought okay there had to be more right and then I saw all your crazy videos on meeting CPP YouTube channel and cppcons YouTube channel and all that good stuff and then I got interested and excited about C plus which is why I started a PhD on static program analysis mostly for C and C plus plus really so today we are going to talk about compilers and static analysis because this is really where the fun begins this is all the hard and difficult bits here really so by show of hands who has this book at home all right by show of hands two of you users see Clan compiler in your daily software development cling format Clint ID clang static analyzer okay that should be a bit more because it's free it's open source it's out there go try and check it out but anyhow if you read that book you will notice that static data flow analysis is only a small part of one chapter so for compiler guys yeah data flow analysis is a thing we need to use it in order to find some interesting program properties which we can then use to build some cool optimizations right and here you see on the Toyota plate here it even says data flow analysis so this will be a major topic of this talk so why static analysis you may ask okay let's have a look on applications and techniques of course we have compilers right compute interesting properties to build cool optimizations to make your program run faster bug finding nowadays right that's also a huge thing vulnerability detection finding syntax violations or even detecting violations of coding standards by tools of like client ID for instance right how can we achieve that we have multiple techniques in static program analysis we have abstract interpretation we have pattern matching for instance that's what Clank tidy uses for the most parts right so it builds the AST of your compilation unit then using um matte charts you can specify AST patterns that then the tool finds on your ASD of the compilation unit and then this may find or help finding code smells and bugs and potentially also vulnerabilities you have type and effect systems right you all know and love it in C plus you can really use the type system and make it a type save language if you wish to use it in that way where snc for the most Parts if you declare variable as an integer variable you mainly say Okay I want four bytes of memory on most modern modern machines at least and improve systems I won't go into details about that symbolic execution that's what the clan static analyzer implements that's a technique that analyzes all program paths of your program your path by path really can be a bit on the expensive side so that's why it's typically only really used for individual compilation units and not whole program although it could be used for whole program as well but it mostly never terminate really and then we have data flow analysis that's again the main topic of the stock so as you can see static analysis is really useful for a lot of things right so and in this talk I'll try to show you so but but what does it do right how does it work so in static program analysis it's most of the times exactly like that so you have your program P that you wish to analyze and for that program P you wish to find an interesting property right that can be a sign that indicates a BARC right so a certain variable may be null a null pointer right so you could also have as a property a certain variable at a certain point in your program carries some sensitive material that you don't wish to have leaked to the outside world and properties like that and then you have an analysis which aims at exactly that it tries to show that the property fire holds at some statement of your program P right that's the idea and you have a corresponding picture such that you never forget this idea here really uh that's how you put it together right so for bug detection there's a recipe for compile optimization it looks a bit different but let me talk about bug detection here so first of all of course you need to find and understand the bug that you're interested in and that you wish to find in an automated manner right you can only really find what you're looking for using static analysis unless you have some implementation flaws in aesthetic analysis of course and then what you do is you write an analysis that finds that kind of bug and then you run the analysis and if it's a bug or vulnerability you fix it as a developer otherwise you need to improve the analysis probably right and how come for bug findings that we need to check manually the findings and then take action and then fix it if it's a vulnerability or otherwise improve the analysis we'll come back to that later because in compiler optimizations you don't do that right so you run your optimizations in an automated Manner and you don't wish to check if the optimization produced the right results so we'll come back to that while that is so how it works is you pass your functions that you wish to analyze you build the control flow of that function and then you conduct the analysis on that control flow graph in an intra procedural setting if you wish to analyze only one function at a time in the whole program analysis of course you would pass the whole program you would build an inter procedural control for graph that is a control flow graph with additional core graph information and then you would conduct your analysis on the icfg the inter procedural control graph so but in the intra procedural case where you only wish to analyze one function at a time how could you do it so let's talk data flow analysis 101 let's talk about the the monotone framework which has been available since yeah almost forever at least for decades right so how does it work so the essential idea is that you have to get right and then you really know what's going on is the following you left your data flow effects D that encode the property you're really interested in right can be what variables carry constant values what variables may be tainted that should not lead to the outside world because they contains sensitive material potentially so that is data flow facts of Interest and then what you do is you capture the interactions of the instructions of your target program or Target function with the data flow effects that essentially describe the properties you're interested in of the program State here right and you do that by applying flow functions a flow function describes how an instruction interact acts with data flow facts and as a result the flow function will give you modified set of data flow effects or potentially modified set of data flow effects D Prime and this is the whole idea and if you write your own analysis you need to implement the flow functions you as an analysis writer really need to implement the flow functions and describe what are the individual effects of the individual instructions of your program and this is then sometimes also called environment right so you have an input set and an output set you have an instruction and you have a flow function that says what's instruction does to your facts of Interest right and that's pretty much all you need to know um the rest can be solved by a generic data flow solvers you just need to understand this so let's have an example let's conduct a constant propagation to perform some optimization constant propagation of course aims at finding constant variables and their and the values that are currently carrying at a certain point in the program so this should be our Target program and I hope the font size is large enough but I believe so um uh so the first thing as I already mentioned is we construct a control for graph here right so that's pretty simple here right and then what we do is we propagate what we are interested in and we are interested in constant variables and the values they are currently carrying at the beginning we don't know anything about the program right because we didn't look at any statement or any instruction just yet and then basically what the underlying solve of the monotone framework does it it now checks out each and every statement and applies the flow function that describes what this instruction does to our data flow effects of Interest and so the solver looks at the first statement and what does it see so we now play Flow function here right so what happens we declare an integer variable X which we initialize to 42. that means no matter what we now before that instruction we now after that instruction we have a constant integer variable X and it carries the value 42. so we add that to our set of data flow facts and then we can continue propagating those sets through the control flow graph we check out the next instruction and same thing right we declare a variable y we initialize it to 13 and we know okay no matter what holds before that statement after that statement we definitely know that Y is constant and carries the value of 13. and X isn't modified here by that statement at all so we just keep it as identity right and then an interesting thing happens I mean you know aesthetic analysis called Static happens at compile time so we can't really evaluate the predicates of Branch instructions right of the ifm so static analysis just assumes that both branches can be taken at the same time so to say so we propagate information along both branches here and then we capture the effects of the other statements here like this addition here we also like we capture that and we also have here another assignment and we also capture that here in the sets of data flow effects and then we have another interesting point in the program because here we have a control flow merge point and in contrast to a symbolic execution which really analyzes path by path on their own static data flow analysis merges early so at this control flow merge Point static analysis information yeah must be brought together before we can continue propagating them through the program and it depends really on on your analysis how the merge operation should look like for a constant propagation it should be something like a set intersection I believe right because you want to be sure that a variable is constant a lot along both branches really otherwise you don't wish to perform an optimization I believe I think that is only reasonable so let's perform a merge so X is constant and carry C value of 9043 along both branches so that stays as is and for y it's constant in each branch but it also carries a different value in each branch so we need to like do something about that and in this case we just say okay we we say the value of y is then top and top is I mean we have an underlying lattice I won't go into detail details here just this is a pragmatic approach here but top means can be anything I can't tell statically right and so then I continue propagating it here and then I'm done in this example otherwise the generic data flow solves the monotone framework would otherwise use a worklist algorithm and it would reapply flow functions for each Devi statement and instruction unless you have a stable solution and these sets no longer change and then you have reached your fixed point and this is your solution this is the information that you can then use to build an optimization so what do we know it's a return statement here return X we do now X is constant and it carries this value so maybe let's just um let's just use it and let's just return the integer literal right away and also we have no other real uses of X which are observable to a use of that program so we can completely eliminate all the instructions that involve X right away we still need y though because we print it here and the value cannot be decided statically at this point in the program so we still need to leave that b we can however um Whoopsie Daisy we can optimize the branching here a bit and make it a bit easier as you see on the right hand side so that would be a prime example of course for data flow analysis so but how does it look in llvm as most of you are familiar with llvm so in static data flow analysis if you wish to write your own analysis you don't really want to analyze source code because it's hugely complex typically you can if you have a simpler task and C plus here is a million ways in which you can solve it I mean at least syntactically and they all boil down to the same thing essentially so you don't wish to have that variability here so you bring it in some different form which you analyze which is much easier to analyze and what you can use is lovm's intermediate representation that's a special compiler representation that the compile internally uses as an analysis Target and then it also conducts optimizations on that intermediate representation and it's much simpler to analyze really if you wish to do it in an automated manner so this is some llvmir in fact that's the IR for the program I just showed you on the previous slide so it looks a bit like assembly but it's still a bit more high level so we still have functions such as the main function here and we have axi and RV we have some basic blocks here some entry basic block we have the if Branch the else branch and then the the final basic block and as you can see we have different instructions doing different things right so we have at the beginning some aloka instructions which just allocate stack memory so you can think of in a local as just a variable declaration then we have store instructions which store values to memory locations then we have load instructions which retrieve values from memory locations we have comparisons we have conditional jumps as you can see here and then we also have unconditional jumps and all the good stuff and we have function calls hey that would be the C out overloaded left shift thingy right so probably should have used print or something right but this is how it looks like and also you can see here that the compiler is not really the compiler doesn't know member functions everything is transformed into three functions where the first parameter is the the this pointer basically which is also why you pass the global variable here because you see how it variable as the first parameter um anyway so this is what we analyze because we only have I mean by now llvm only has like 65 different instructions and most of them are really Irrelevant for for day-to-day analyzes right so you just need to analyze some few instruction types here to check out the llvmir for your own program or compilation units you can just produce it using Dash uh and then it will show you or you could also use just compilex Pro log Explorer and choose a clan compile and then emit llvm emit as also works and probably you should also use no discount value names because then the compiler tries to keep the original program's value names variable names as much as possible otherwise it was would just enumerate and come up with names for things so and um yeah okay how does it help so we still need to analyze this intermediate representation now so llvm provides apis for IR inspection and the types that it provides to do that are built in a highly kind of hierarchical manner so let me show you so at the highest level we kind of have an llvm context type which does memory management and all the nasty stuff you don't want to see as a static analysis developer you don't have to be concerned with what it does so just believe me you just need to have it an instance of it and then it takes care of the difficult bits then you have the module type and you can think of a module instance of a module is something that holds the IR for a complete translation unit for instance that can be then accessed in form of a modular so it carries Global values Global variables it carries functions and all the good stuff then for functions you also of course have function types right a function in turn comprises basic blocks and instructions instructions can use operators we saw the at the addition operator on the last slide and then you also have that special value type and value is quite interesting because it's located quite it's located very high up in the type hierarchy so an instruction is a value a basic block is a value a function is a value and this allows you really to write high level interfaces which accept values as or pointer to values as parameters and then wherever necessary you can obtain the details by casting casting around oh Dynamic cards are expensive you might say you may say but llvm implements its own closed rttdi system um and you can then use Dynamic cast here and you give it a point huh and you give it the target type you're interested in and then it will return to you either null pointer if it couldn't be casted or it gives it gives you a valid pointer to the now downcasted thingy then you can also check for is uh under the hood it's all integers right so each of those types is associated with a special integer value and then instances of those types carry that integer value and those Dynamic tasks checks here and is a check that's just an integer comparison so that's very very cheap and it's important that this is cheap because we need it for analysis right so I'll show you on the next slide but as I was saying this allows to really to implement high level interfaces so a flow through function a flow function basically if you wish to for whatever reason Implement your own monotone framework could look like that right it receives as a parameter an instruction pointer at an input set and the result should then be the output set such that this function describes what the instruction does to your data flow effects so details when necessary um and this API is so ingenious that it not only allows your code inspection but also it allows for transformation in code generation even and we will see that on some slides later on so it really feels like magic at times and it's super easy to use so I I I'm not sure what people did before llvm occurred all of a sudden and so on and with this in with with this information that we now obtained here nothing stops you from automatically inspect your code so you can write code like that using llvm you just have a simple Main as I was saying you need that context here so you need some context variable and then you can just ask everyone please pass me that llvmir file sometimes also called llvm bit code file right and then it will deposit and create a module for that and it will return to your unique pointer to that very module then it's probably a good idea to verify if that module is all right and nothing got messed up for some reason you can also check the debug information that may be attached may or may not be attached to that llvm IR or Bitcoin file and then you can inspect the code really right so you can have a bunch of for Loops it's not the best coding style here but I just want to show some simple example here so you can just iterate over all functions in that very module and then you can print the name of the functions that you are just looking at and then for each function you can iterate the basic blocks right and then for each basic block you can iterate its instructions and then you can use the rtti system of llvm to check what kind of instruction actually is so you can ask okay is that instruction I'm just looking at is it in a local instruction and if yes it will give you the pointer to the local construction otherwise it will give you null and then I don't know for a show off purposes here you can just print something so as you can see llvm also has its own like streams which are better than seeing out and what we have in the standard and you can also check if an instruction is a coil base a function coil so to say and then you can basically check if it's a direct function call or indirect function call to a pointer to a function pointer or a virtual member function so it's really I mean it's just a few lines of code and then you can analyze your your own code and now the question is is it too good to be true and if I ask that question the answer is probably yes so let's see let's have a look at Magic initialization so you can write such code and if you compile it with any compiler I know it will throw a warning at you it will say okay you are using the variable I here and it's uninitialized so please do something about it right I hope we can all see that right away and then you fix your code and write code like that and then every compiler I know is perfectly fine with that doesn't report anything so um yeah why is that really the problem is here we are messing around with addresses all of a sudden right so we are taking ice address and pass it to a pro Edward to another subroutine to Magic init function and point us are difficult beasts I will elaborate on that more in the next few slides so we need points to information all of a sudden and also in order for an analysis to find those issues it would need to analyze a cross-function boundaries it would now also need to analyze the magic init function and while analyzing main jump into magic in it continue analysis there and then jump back at some point and compilers typically don't do that so no warning so a realistic analysis requires much more information especially analyzes for bug finding or vulnerability detection you need information on type hierarchy on top hierarchies you need information yeah on on on points too uh pointer relationships you need call graph information and depending on the complexity of your data flow analysis which you which you actually wish to solve you may even need results of other data flow analyzes right so this example on the right hand side should showcase that so here in order for instance to research that function call to F I mean we have multiple implementations of f we first of all need points of information in order to find that s is actually pointing to an allocated type duper and then we would also need the re-table information to see okay the DuPont implementation of f is actually being called here such that we can give that information to the underlying analysis right such that they continue continuous analysis in the right implementation of f right that would be an indirect function call here and oh no he's using a raw pointer yeah just for this simple example here all right so um can we do better in practice so how is it done what what you do to conduct inter procedural analysis and practice what can you do um I wanted to show to you the IDE framework the inter procedural distributive environments framework that's still considered state-of-the-art although it's quite some years now in age so let's see same program as before but how does it work in ide we also still need the control flow graph for obvious reasons and now what we are doing is instead of like propagating sets through the control flow graph we are now transforming the data flow problem into a graph reachability problem and we're constructing a so-called exploded supergraph that must be awesome right if it's called exploded super graph so we have a special effect a special bunch of nodes which are in literature often called Lambda facts right and they hold everywhere so everywhere in the program the Lambda effect holds it's like some special thing to make that whole concept work and then we just propagate that node through the program link it all together right and then we can do a reachability check for any given statement so we can check here at this return statement the node here represents Lambda is it reachable from Lambda yeah it can be reached here okay so this Factor holds now and then you also in in the same manner you encode your other pieces of information you're interested in really so for the variable X you would also just generate a new node and make it reachable from Lambda by just drawing an edge and as if that wouldn't be awesome enough what you can now do in ide you can also use the edges and you can perform computations on those edges and you can use those computations in in Lambda calculus so using Lambda functions you would now be able to specify computations or modifications made to your variables and here we're just saying okay we have set constant Lambda function which always returns 42 to denote okay this variable I mean is reachable so it's constant and its value would be 42. we continue propagation and I left out Whoopsie Daisy I left out all the identity functions here for the most bits and pieces and we also have some we also have some further modifications here so the 9001 we would also the addition we would also add as a Lambda function and here's the plus 9001 and the other Branch put also modulus Lambda function right so in same we would do for a y and then continue propagation and then we are done and if we now wish to kind of obtain excess value as the return statement here we would just conduct the reachability check so we would now go to the node in the graph which um in the r which represents X which would be this one here and then we do a reachability check and while performing the reachability check we just collect all the Lambda functions compose them potentially also merge them at this merge point and which gives you which gives us an expression here that can then be evaluated to the value of 9043. so and then we know X is constant and Carries this very value so Isn't that cool and what's even more cool that is a really fancy part here once you analyze the complete procedure you can build summaries right you can just compose everything on the way to this node here and then you just have one huge function which is an often called jump function which describes the whole effects of this procedure on the variable X NY and Lambda right so we can build summaries that's the ingenious thing here for IDE so as you saw it's flow sensitive it's intra procedural we would be able to also propagate across functions and it's context sensitive and it's even infinitely context sensitive by Design because we built summaries and once we analyze the function and this function is called again in some different context we can just plug in the summary and don't need to reanalyze and all the context sensitive bits and pieces are encoded in those Lambda functions so you can also apply those summaries in every single context it doesn't really matter right which makes it blazingly fast so depending on the complexity of your analysis of course you can do a whole program analysis in minutes if you wish to compute a more complicated property it can also take a few days so you need to design your analysis in a smart way right it's complex um I mean I just overall simplified here a bit probably so the actual implementation is like 3 000 lines of complex crazy complex C plus plus code and it only really works for distributive problems I don't expect you to know what that means but what I do say is it it's yeah it prevents us from analyzing using IDE a certain class of analysis problems for instance pointer analysis we cannot solve using idea and memory is also a Thing If you conduct called program analysis because he exploded supergraphs that is being constructed can get very large so keep that in mind and I find that this shows a few things and so we should probably talk about compiler people and static analysis people because the incentives are different so on the one hand we have optimizations and on on the other hand we have mostly bug finding and and stuff like that right so what are the different objectives here so in compile optimizations I mean we all experience long compile times in C plus plus right if you need to compile that very large libraries that you are using have a hard time right should probably become a small car or something or at least grab a cup of coffee so speed is an issue it must compile fast and this is why we can't integrate like two heavyweight analyzes in the compiler also compiler analysis must be sound right because you build optimizations based on the aesthetic analysis and so what what does sound mean it means like whenever something is unknown you need to use a safe over approximation say you have a function fool which receives as an inter which receives as an argument a reference to an integer and then you call that Foo function but for the analysis the full function is only available as a declaration so you really can't see what it does the analysis has to assume that whatever you pass in is no longer const after that function call when you conduct a constant analysis say and also all the global variables could be potentially modified so you need to save over approximation approximations in order to obtain correct results for bug finding vulnerability detection it's a bit different because the other goal is you need to make it precise and into a procedural to make it precise and soundness is typically thrown out of the window immediately because when you wish to build a sound inter-procedural analysis you would need to over approximate so much that the results at the end of the day are so imprecise that you can't fail any you can't say anything really so it could be like the result of a concept propagation would be yeah everything is associated with top now so I can't decide statically and that that doesn't help very much right but even if you're unsound and use under approximations so you just assume okay whenever I don't know a function because it's only available as a declaration I just assume it can't do anything bad to my analysis um that's what we do and Bug finding vulnerability detection so um analysis challenges still it is the internal fight for precise Point information it all boils down to that thing um it's not distributive so you can't use IDE or weighted push down systems or push down systems or whatnot right and it will be hugely expensive if we try to compute in a context sensitive and inter-procedural setup but that is exactly what we would like to have if you wish to compute that you would need to use something which is called the call string approach which is basically the monotone framework on steroids so whenever you propagate data flow information into a call Target you attach a call string to it such that you now once you analyze the quality Where to return otherwise you have no clue where to return you need to return to all possible return sites which increases in Precision in Maxi results unusable at the end of the day so and yeah you need that and that's a problem and Analysis I mean pointer analysis is an undecidable problem like most other types of analyzes and that's a huge problem makes it really difficult and you may Wonder okay restrict doesn't look too bad after all right because it was exactly introduced for this purpose at least in some or with our C specification it was part of the language correct me if I'm wrong here um but the idea was okay if the compiler can't compute it really hit the compiler and we do some arithmetic here through indirection using pointers right and what do we do what do we need to do in the first example so we know we need to conduct a load right and and store five to variable a we need another load to store six into B and then b and a could Alias they could point to the same memory location we can't tell this this is why for the addition we need once again to load from a to have to be sure to have the correct value before we do our addition and then return the result and we can use of course the restrict keyword here to say okay those two variables do not Alias and then the compiler can omit the second load of a basically and if you have really a program where you have hot loops and and you have functions like that that you are calling on hot Loops that can make a huge difference but really be aware that that is also dangerous of a comment on that later on so what are further challenges scalability of all program analysis is still an issue so you can design analysis that are able to finish in minutes even for million line programs but it highly depends on the whole setup and and what kind of data flow domain you choose and and that you're using to model your effects of Interest so it can also run for a few days if you don't be careful analysis Precision noise to use for information ratio I mean if you run an analysis over the Etsy Nike build and then developers come back to the office next morning and you throw 50 000 potential bucks and vulnerabilities at them that's the static analysis found they are not happy about that they're probably just throw their notebook out of the window and leaves the office again so or never use that tool again so that is still an issue and then complexity of course most problems in Civic analysis are undecidable so this is not only hard in theory it's even harder in practice if you need to implement those Concepts because you must craft implementations that are as efficient as possible in order to make the best of those techniques and for that reason we started a static analysis project which is called phaser it's on GitHub go check it out and in that project we implemented concepts of static analysis that are too expensive to be integrated into the compiler so and don't blame me I mean I started developing that framework when I was still learning C plus plus so some parts look probably very nasty but we are we have grown a user base by now and we are continuously improving on the project so if you want to join just drop me an email we could probably use your help but no and now I want to show you how to write an llvm based analysis and I want to show you that this is really nothing dangerous yeah and then nothing too complicated let's have a look how you write an analysis pass in llvm so we wish to write an analysis that finds all direct function calls to fool a given given a certain Target program and so an analysis implementation just corresponds to a class implementation so you implement your class called site finder analysis this is the name I come up with I came up with you use a mixing pattern to mix in some llvm infrastructure such that the llvm analysis infrastructure and Optimizer can understand what this class is and is meant to be but just using the mix and better that's relatively relatively easy and then essentially you are specifying what your result type is and since we wish to find all direct function calls to Foo we just have a vector a set Vector of called sites here as a result and then we have one function that does the analysis it always has to be named like that and it always must be static and it returns our result types that we just described you're using that using directive and you obtain as an analysis writer you get a reference to the module that you should analyze and you also get a reference to the module analysis manager and recall on one of the last slides I said okay more complex analysis sometimes need additional information from helper analyzes and the modular analysis manager is the thing you can ask for additional help information if you really need them we don't need them here so we just ignore that parameter and yeah then we just iterate the functions of the module we iterate the basic blocks and we iterate the instructions and we check if the instruction is a card site that calls a function and if that is a direct function call and the function that is being called is named fool then we put it in our result vector and return it at the end of this analysis right so only very few lines very simple so I'm based on this simple analysis let's now conduct an optimization that that uses the information of the analysis path and I mean it's pretty much the same an optimizational lvm is just a class implementation you mix in again some information that the llvm infrastructure provides and then the optimizer needs to to understand what you're trying to do here and then it's only also just an implementation of the Run function here this function always needs to be quite run you get a reference to the module under analysis and again you get a reference to the module analysis manager and this time we need the module analysis manager because in order to conduct our transformation here we need the information from the analysis that we just wrote so and we can just ask here please give me the results of that analysis and then we get a vector of all those called sites that comprise direct function points to fool and in our transformation it's not not much of an optimization but it's a transformation in any way anyways um we try to replace those function calls to full with function calls to bar so let's do that let's apply the transformation so we just iterate all the call side sites that the analysis found and then we can use the llvm API to generate new intermediate representation so we can now also use code generation so we can ask llvm llvm constantin.get colon colon get and it is a that is a factory function which now produces a new constant integer which we initialize with the replacement counter which sits at one at the beginning right and we also say okay we wish to have that integer of a 32-bit size and since we also pass as a parameter the context the lifetime and all that memory management that you don't really wish to do that is handled by the llvm context as I mentioned earlier then we also create and create is also a factory function we create the new call site which calls to the replacement function bar and there's a parameter we give to that bar function the constant integer that we just created so we have a function called bar and then it receives a literal here in that case namely the replacement counter transitively and then we can use it and replace the old call site by the new call site that we just constructed and then increment the replacement counter so that each call site gets a an incremented counter really and then at the very end we need we need to turn something and we need to return the preserved analyzes because the transformation path modifies IR we need to tell the path manager that calls us let on what we modified and what analyzes that may run previously we just broke right so you can now in detail specify what other analysis may need to be recomputed because you just heavily modified the IR right here we are lazy we just said okay I'm modifying everything so please do recompute everything and then it's up to the optimizer infrastructure to schedule like what happens in a sensible manner right that's also a hard problem compilers but I won't go into detail here so now we can put the pieces together I mean that is pretty much the same left hand side as before so we just pass the module that we wish to modify then we set up a bit of pass infrastructure right so pass Builder module analysis manager we register our analysis and then we register our transformation pass the call site replacer maybe it's also a good idea to add a verifier at the very end just to make sure that we didn't mess up anything and then we can apply it and run that infrastructure on our Target module and then we can just print the modified module to the command line for instance just to Showcase how it could be done all right so do developers really know better than the compiler we also wanted to talk about that unfortunately at some point the compiler could need your help as the restrictive show show us here so yeah because we are lacking in compilers at least I mean in theory you can have your own very precise pointer analysis which are very expensive but can't be integrated into the compiler but what the compiler currently provides is not very good so you are lacking precise points information and especially you're lacking into a procedural points information which led to this restrict keyword here and this would be the assembly that is generated and as I promised before you really save a load here and yeah you don't need to reload from a but as always I mean if you lie to the compiler and you pass in something that actually does alias the code is just wrong right so you need to be very careful if you really have to use it yeah measure it if it makes sense but and you may say Okay restrict is not a c plus keyword at least not an official one yeah but compiler writers don't care right you have all your fancy compile internal keywords underscore underscore and then on I or things blow up that you can use here instead right so every compiler now implements that expression does it help um so let's have a look let's just have a normal version of the factorial function and then just call it with a integer literal here and the llvmir that is generated is the following so we have code for the factorial function and then the function column main is optimized so you don't need const expert in this in that case and also even before const Expo existed compilers did that right so it's nothing like oh yeah we turn the whole world over and now everything is better so it worked previously um it's more or a kind of thing of expressing intent right so to to to tell other developers what your code is supposed to be or can potentially do um there's another case where you can where you can see what what what what the compiler may do if you say something is in a context for a function so let's have this case here and if you have the context per version of the program it will produce you the main and it will return the result directly and it will completely eliminate the factorial definition because in order to be used as a const expert function the function definition has to be available in the module that you are use that you wish to use it in right so that factorial function cannot be used in any other module and the compiler here was smart enough to figure out that this was the only called site here and it's no needed it's no longer needed for any other call site or say a let alone another module so it can be completely eliminated and makes the code a bit smaller more compact compact of course if you start messing around with function pointers also right or similar things compiler may not be able to do that right so keep that in mind but potentially it gives new opportunities for optimizations no except same thing if the compiler can see your code and if the information that need to be computed are not too complex such that it can't do it it automatically knows if your functioning throws or doesn't throw so why using no except again it's for expressing intent right so in the ordinary case in the good old times you would write code like that for instance and let's assume we only know about the Declarations of Foo and bar we don't know their definitions and if you look at the corresponding llvm intermediate representation you'll see that the compiler needs to invoke every function call in in the try block what is invoke invoke is similar to a call ordinary llvmir call instruction with the only difference being you have an exception Handler attached to it right so you have your your ordinary destination they would continue if if that function that is being called like Foo for instance if that is executed normally and you don't have the exceptional case you would continue at the invoke continue label and if that function throws you go to the unwind this unwind label here which you see percent L pet here that's just the name for the basic block which is then executed which contains the exception Handler same holes for the bar function right it could potentially throw so we need to invoke it and also at the exception Handler so but you can also say no except even though the function May throw of course then just stood terminate is being called right so when termination is an acceptable response you can also just at the no accept keyword right and it may help the compiler to kind of like up I mean it gives room for internal optimizations I should say right same situation here so we only have Declarations of Foo and bar and on the left hand side we have the same usage pattern so we still are calling fluent bind the try block and the llvmir now looks as follows so the full function May throw I mean we don't know the compiler doesn't know because we don't know its definition so it still needs to be invoked bound the other hand is marked as no except so and here you can really see it's just only called so no exceptional is registered or whatnot right and if you again if you lie to the compiler the terminate is being called and then yeah I mean it's your business right it's end of the game so it gives opportunities to the compiler and also it's like for expressing intent so let's like draw some first conclusions here static analysis is pretty cool and compilers are awesome I'd say so and compilers as you saw heavily Reliant static analysis information but some tasks are statically undecidable and unfortunately any interesting general program analysis undecidable but I mean we should still do it right we can still compute lots of useful information that the compiler then can use to conduct optimizations or we can use for bug finding vulnerability detection so now how static analysis works and write your code accordingly so do now what it means if you have procedure boundaries at certain points in the program do now the implications that using pointers and Global variables yeah bring with them so do now what what an analysis can compute for you and what it can't do for you and then help the compiler to figure it out by writing code that expresses intent right that's that's the best advice I can give so an example here I teach a C plus plus course as well at parabon University and there's a project at the end of the semester and one tiny bit of the algorithm that the students need to implement is finding the maximum of four values right and as it turns out you can do that in multiple different ways so I see code like that from the students which is yeah I mean it does what it does right but yeah it's also crazy then you have the most efficient solution right so you can get away with just three comparisons that's also perfectly fine and then you can just be lazy and cry to the standard STL library right algorithm and so now let's have a look at the assembly actually so this is O2 optimized with the most recent clang so I mean even for the left hand side the compiler even the optimizer couldn't figure it out and just generated like some 28 lines of assembly which do many unnecessary things and for the code in the middle so the Max Smart implementation the max lazy implementation this is the assembly being generated so sometimes it's also okay to be lazy to be to just Express what you want your program to do such that the compile Optimizer can see through it and so you don't need to be necessarily smart so to conclude uh keep in mind how static analysis works now its limits now what it can compute and now the things it has trouble with such that you can write code that aids the static analysis and the compiler such that it can produce the kind of optimal code really right so also don't be naive but also don't be smart so somewhere in the middle ground is a good thing I'd say and if you're interested in static analysis and those kind of things talk to us I mean I already talked about the phaser project where we Implement stuff that's too expensive to be put into the compiler we really need your help right it's crazy complicated right and the more people we can get on board and have interest interested in those kind of things see better so just drop me an email if you're interested and otherwise I don't know I am happy to take any questions or I stick around for a few more minutes [Applause] first of all thanks for for the talk um you mentioned in your talk about the problem with scalability and I would like to know what's the bottleneck there and you also mentioned about memory uh would like to know what's the problem is it density it is irregular accesses it is low operational intensities yeah yeah um scalability the worst case runtime of the IDE algorithm is n times D Squared where n is the set of instructions of your programs that you wish to analyze and D is the domain of the data flow effects and it's cubed the domain of the data flow effects that what in essence what that tells you is the more data flow effects you need to create while conducting an analysis the slower it gets and if you design your domains in a clever way you don't need to generate very many data flow facts and then you can have really fast analyzes right but if you need an analysis that is also let's say a field sensitive and can distinguish individual Fields you typically need to generate much more data flow effects and then it falls apart at some point right that's the main issue and and still if you wish to have a precise analysis you also need pointer information to resolve like indirect calls to build a precise call graph right that's also a huge pain point and then in terms of space yeah a space of the exploded supergraph for the idea algorithm for instance is n times n times d as simple as that that is the maximum number of data flow effects generated times the instructions of your program and this can also like blow up and it really heavily depends on how the how do you design your data flow effects how do they look like how many of them do you have to generate so there are lots and lots of design decisions that you can make and you need to be aware what kind of consequences those decisions have hope that answers your questions I'm still around yeah thanks for the talk I wanted to mess with it that this is very timely uh so my question is um when you do optimizations obviously it matters which Hardware you Target and often there is like you know as far as I know there are after they'll all be answered as a native pass exactly but um like the native then you often need to reach like let's say you're on Roll groups like depending on the hardware would you target you will enroll Loops differently exactly uh how does so what kind of information does the IR generation know about the specific Hardware it takes or nothing at all so IR llvmir is just really an intermediate representation that some of the majority of analyzes and optimizations are performed on and then as you're mentioning correctly when the actual machine code is generated from the then optimized IR first optimizations may be applied that require information on specific hardware and those kinds of information are really like oftentimes hard-coded right so what kind of loop unrolling should be done for what kind of architecture and so on that's a whole different story really complex okay so if you take which algorithm I don't remember but if there's an algorithm that if you flip certain switches will be vectorized and you don't flip the switches you don't get factorization yeah that obviously needs to know about the switch so is it done after they are generation then yes [Laughter] thank you for the talk um I have a few questions from our online audience um first of all download image of and I'm very sorry if I butchered the name asked how stable is llvm's API does it change much or [Music] um it changes from major release to Major release but only typically at very small places so whenever you have a bunch of analyzes up your sleeve you already implemented them and then you upgrade your llvm typically there are only few minor places where you need to like replace one function called by a different one but it's it's not that frightening so it's all manageable oh thank you another question by the same question is does functional error handling so is it expected or versus exceptions make optimizations analyzes harder or easier um I'd say to the compiler it doesn't matter much it really doesn't matter much I mean at the end of the day uh it's all translated I mean in C plus plus you are you have always those crazy syntaxes right and then but first of all llvm IR is produced which is much simpler looking and then it really doesn't matter what kind of concept you use it typically even boils down to the same ir and then the analysis do their magic the best thing to compute useful information thanks and one last question by Julian Kent and sorry again messed up the name uh what are some examples of analysis it would be viable if we gave double or 10 times the computational budget to compile it pardon me once again what are some examples of analysis that would be viable if we gave double or maybe 10 times the computational budget yeah that would be definitely uh points to analysis then you would finally be able to put precise points to analysis into your compiler currently I mean llvm most for the most part at least assumes your point information is rubbish anyways so but that could be improved and I talked to compile Engineers from a major company which I don't know if I can mention the name right here but anyhow um they estimated that an improvement in points to analysis points to information could speed up all of their applications across the whole company from one to ten percent and even one percent would I mean that would safety salary costs of four for thousands of programmers and in software Engineers probably so yeah there's lots in it but I mean it's complicated and at the end of the day it's an undecidable problem so we have to work around and be smart about it all right thank you welcome yeah yeah thanks for the talk um so passing by ref reference is effectively the same as passing by Point yes yeah so if you pass by construf then you run into all the issues that you have about like potentially aliasing yeah so is it then in principle better pass by value and move into the values is that actually going to be if you want to help the compiler is that a good idea um first of all the compiler doesn't really care about const I mean that's only for to protect developers because the compiler always has to assume you can const cast it away somewhere so yeah um in some places it's if I mean if the object that you try to pass into a function is small you should really do it by value such that the compiler can like forget about points to information that really can make a difference I don't have a good example here up my sleeve but maybe I could craft something right um yeah that that is definitely a thing um moving stuff around then move Constructors are being used right and in move Constructors what do you do for the most Parts you are like pointing to now different things and then it's the same story so you can't really get away with that all right so no more questions I think we are done have a good time [Applause]