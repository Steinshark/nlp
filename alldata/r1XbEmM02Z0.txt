we are talking about a harm formal definitions of harm it's obviously related to AI systems even though the concept of harm has been around for a very long time you think of Hippocratic Oaths that the doctors still take to this day in particular it says I Will Do no harm right to my patients so the philosophers had this concept in front of them and they wanted to Define it for for a long time and they essentially gave up on it so there is a paper from as recent as 2012. it's quite recent and philosophy right and computer science it would be the dinosaur area 2012 it's just not not relevant anymore but in philosophy it's practically yesterday so it's paper by broadly prominent philosopher who says harm is a from Christian jumble we don't know what to do with it we should just do away with harm and replace it by other more well-behaved Notions right but ignoring it doesn't make it go away unfortunately right and also now also for example we are facing an imminent I think rise of autonomous cars on the road so who is exactly going to be in short and against what so it's very difficult very difficult question so we really have to urgently Define harm and and to quantify harm to Define who is harming whom so when we have the characters driven by a person Pluto then it's easy right so Pluto is careless and there is an accident with another car Pluto was driving so his insurance company has to has to pay to the injured one but let's imagine that there are two autonomous cars now this car there's a person inside in this car there's also a person inside but they're not driving there's almost cars are driving and let's now imagine so we they're driving on the road and there is a safety fence right this one on the border of the road so so this autonomous car is driving Bob Bob is actually completely um distracted watching the previous computer file video quite a while ago now I did a video on it and then the stationary car can be there for multiple reasons that may be broken down out of charge Etc so now it has uh three options one of them is to alert Bob who he is distracted by the fascinating computer file video another one to do nothing to just crash into the stationary car right and this will be terrible so Bob will die and the person inside the stationary car will die as well and so it is to crash into the safety fence which means that Bob will be mildly injured based on the current proposition of how to deal with people who sit inside autonomous cars they are not going to be considered drivers or responsible for any behavior of autonomous cars because how could they they are completely distracted by the computer of all videos they're not even looking at their own the car is not going to suddenly seizing control and somehow alerting the crash so the car chooses the option of crashing into the safety fence so Bob is mild injured and now Bob wants to sue the manufacturers of the autonomous car or the software company who provide the provided the algorithms um saying well I didn't expect that right I was injured um you you wronged me um and the car manufacturers say well actually this was the best possible outcome so how can we formalize all this story and to get something to get to some conclusion so first of all we are putting all this discussion in the framework of causality of course so first of all we would say that a harmed B if in particular the action of a is somehow caused the result because I mean it in a very broad sense so it is well defined but we're not going into this formal definitions it had some influence on the outcome right it's not completely relevant it's not the fact that that the current latest uh snowing in Siberia right so it's it is actually relevant to the current situation um so okay so that's one two um there is another thing that the uh that a or you know in this case the car could have done that would would have avoided um the this outcome right it would have caused the another outcome maybe it's not a deal but it's another outcome and this outcome would have been better um and but they caused I mean that maybe some other things should have changed as well but we can imagine a very similar scenario which is slightly different in which the car makes a different decision and it is a better outcome for Bob uh now what we add to this so this up up to now it's more or less standard definition of causality we have two things first is the utility function which actually quantifies how much harm was done so in this case we can quantify the harm that was done to Bob injuring him and contrasted with harm that could have been done to Bob he's had the car crashed into the stationary car and that Bob would have been dead right so this is a maximal harm I guess that you can cause uh to Bob um versus the utility of not harming bulb at all which is what what he expected so that's one ah and two is the default value so why do I need the default value we need the default value the default value is somehow the lowest possible utility which is yet still not harm for that imagine the case with a birthday present right so you invite a friend over to a birthday party friend comes over um and brings you a 10 pound 10 pounds bottle of wine you're kind of a bit disappointed because you expected a fabulous present in fact you've been hinting to your friend that you expect this amazing Brandy 40 years Brandy that you saw in the shop for 200 pounds but you have no idea anyway 200 pounds so yeah you are disappointed right but you wouldn't say that your friend harmed you I mean it's still above the default your friend could have come empty-handed so okay so we have the default right which is kind of the threshold which is below the default is harm what is above the default is no term so you're not expressly happy but you cannot say that your friend harmed you in this situation remember the autonomous car chose to to crash into the safety fence so first of all what is the default so Bob is saying the default is that I expected to get to where I'm going to my destination um healthy right I left healthy I expected to get healthy what is this enjoy um okay so first of all we can you know we can accept it now second of all uh if we talk about default and utility we have to check whether this default is achievable whether they actually exist something that the car could have done uh with slight modifications of possible values of other factors that would have resulted in a better outcome so in this particular case the the default right because he wants to to arrive on harm and let's say that we are not thinking about the case where the Bob's car would have driven straight towards the stationary car and then the stationary car teleported somewhere else and then continue driving this is not possible we're not considering this we're not considering Bob's car suddenly becoming a Batmobile but mobile and flying over no I mean we are reasonable people we are considering reasonable changes so okay so in this case I think we can safely say that the default was unachievable and actually any other possible outcome so another possible outcome would have been to do nothing to continue driving in the Lord Bob or to crash into the current front um and effectively both of them would have resulted in a crush and popping that so there is nothing that the car would have done um to make this station better for Bob right so this whole discussion leads to um the decision that Bob was actually not harmed by the decision of the car right so that's the concept of harm now in order to actually decide how much of the insurance premium the company is going to pay uh we need to be able to quantify our so far we hadn't Quantified anything we just decided that the car didn't harm Bob right according to these assumptions into this defaults right let's imagine Alice yeah that's just looking well it's not going to a restaurant and having a meal and then this is Alice waiter that's also a table not looking very happy you'll see why so Alice had a nice meal and she got a bill of a hundred dollars right so let's say we're there in the US hundred dollars as we all know us is a tipping country so now the even I think they put options for tip underneath so tips so it will be like 20 is this written 25 is that's a kind of pre-calculate the tip assuming that you will keep 20 at least so um next the waiter expects the tip of 20 which is going to be oops twenty dollars okay the problem is that the restaurant only accepts tips in cash and Alice didn't go by the ATM this morning or there was no ATM on the way to the restaurant she only has five dollars cash in her wallet and she paid the the bill with the credit card okay so she gives the way to the whole five dollars the waiter is very unhappy as we can see but can we claim that Alice harmed the waiter actually well first of all it depends on the default I think we can again agree that default would be twenty dollars because the waiter has no idea right I have to know this because I'm going to America soon so it's very important very important yes when 20 is expected unfortunately yes waiter expects twenty dollars now is it achievable if we say well it's not achievable because Alice only has five dollars what what is he gonna do give her watch and the earrings there's nothing she can do and then we'll say well twenty dollars is unachievable the maximum that's achievable is five and she gave five dollars so therefore there's no harm she's gonna have to go out to the ATM get some money out come back and give it to the waiter yes exactly so another way to look at it is to say no twenty dollars is entirely achievable she should say to the waiter um I'm leaving my coat and my bag here I'll be right back go out to the ATM withdraw the money and come back and leave twenty dollars so if to if it's achievable then she actually should have given him twenty dollars and therefore she haunt him at fifty dollars right it would take utility the same as um as the dollar amount okay so so this is the Dilemma of a chewable versus not a chewable um and by the way what happens if Alice decides that she needs to keep some money in her wallet she doesn't want to update completely and gives the waiter one dollar tip so in this case I think we can agree that she harmed the waiter regardless of whether ATM is close by or not um but the amount of harm is different so if we say that the maximum that's achievable is five dollars than to have the weighted four dollars right five minus one if we say that the default is achievable then of course you have the waiter at 19 because that's going to be 20 minus one why is harm better than utility or more interested is just utility because here I think what we've done is more or less we subtracted utilities right um so let's talk about um a doctor doctor's dilemma so we have a patient let's take another I think this one is looking sufficiently miserable so this is a patient and this is a doctor right so the patient is ill and the doctor has a dilemma of whether to to give the patient medication or to do a surgery okay so medication will keep the patient stable let's say that if the patient is healthy that's going to be utility one right which is the best if the patient is ill but not dead it's going to be half and that is going to be zero so medication will keep the patient stable at 0.5 surgery will cure the patient completely except sometimes it it fails right well no sometimes surgeries don't end well maybe the patient is allergic to Anesthesia where you say um possible causes that are not not um under the doctor's control okay so it will cure the patient or it will kill the patient with some small probability p I don't think we're all used to it if we are going to the hospital for even some minor procedure we always have to sign a consent form that says there is small probability of um etc etc and then some horrible things that could happen and let's say that P is small so what is the expected utility for medication from the surgery let's say that the physician is only basing their decision on on utility so medication always guaranteed results 0.5 right so the expectation so here is 0.5 that's that's the result of opening steering medication for the short term the result is 1 minus P it's a 1 minus p is the probability of the successful surgery okay so it's multiplied by one plus P multiplied by zero so it's one minus B so the expected to take is 1 minus P now if p is small as we assumed as it usually happens with surges otherwise it wouldn't be and they're going than ever then of course surgery wins right one minus p is bigger than 0.5 right but that's utility but let's now tokens in terms of harm so the patient has been ill for a while they're kind of used to this unpleasant model unpleasant feeling for them the default is 0.5 the default is how they are feeling right now and now treating them with medication doesn't result in anything lower than the default right so it's all going to harm them just going to stabilize this this current condition in the contrary the surgery so with the probability y minus P it's going to completely cure them so this is no harm so this is above harm but with probability p it's going to actually result in patient deaths which has the utility zero which is 0.5 below to the default right so now we have the expected harm from the surgery 0.5 VP and the expected harmful medication is zero hmm so now I will suddenly perform medication so this is just different ways of looking at the same thing right it is different ways of looking at the same thing yet it solves quite a lot of um moral dilemmas how does this play into compute science then so so this uh is very relevant to AI to AI systems autonomous systems the adoption of autonomous systems is not going to happen unless we figure all this out uh it's not going to happen unless we figure out explanations which is something that I talked about in my previous video it's not going to happen until we figure out harm and it's not going to happen until we figure out fairness I think so for this particular subject of harm the insurance companies are going to block the regulation that allows the autonomous cars on the road or rather they will refuse to ensure that autonomous cars which will block the manufacturers of the autonomous cars for blazing them on the road uninsured until all this is figured out if we Kill Bill and we give his organs to those people so five people will be healthy one person is going to be that so what happens if I cover this part this is probably no longer a panda right this is not about that