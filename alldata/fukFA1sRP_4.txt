speeding up V8 Heap snapshot so if you've never worked with Heap snapshots a they're really really slow but if you've never actually worked with their underlying format it's it's quite amazing uh for for Netflix I built um some some Heap snapshot stuff uh that worked in inside of this I helped build it I I did work with it I did a lot of memory stuff and so this is near and dear to my heart so I'm very excited about this this post has been authored by Jose dapina with contributions from Jason Williams Bloomberg Ashley Claymore Bloomberg Rob Palmer Bloomberg Joey Chung engalia and uh Google that's how good I am at saying names I'm not good okay I hope you know that in this post uh V8 snapshots I will talk about some performance problems found by Bloomberg engineers and how we fix them to make JavaScript memory analysis faster than ever love this the problem Bloomberg engineers we're working to diagnose on diagnosing a memory leak in JavaScript application it was falling uh without it was failing with out of memory errors for the rest of the application the V8 Heap limit was configured around a 1400 megabytes that's a lot normally a v8's garbage collector should be able to keep the Heap under uh usage under that limit so the failures indicated that there's likely a leak correct uh the common technique to debug routine memory leaks uh scenario like the one capture let's see ah a common technique to debug a routine memory leak scenario like this is to capture a heapsnop shirt uh Heap snapshot first then load in with the uh this memory tab find out what is consuming the most memory by inspecting various summaries and object attributes uh in the devtools UI the heaps not shot can be taken in the memory tab for node.js applications yes what you told they're missing one thing I think they didn't mean to say it this way but you take one snapshot you keep doing the operation for a long time you take another snapshot you keep doing the operation for a long time you take another snapshot and then you can run diffs on those heaps and you'll see that you know like there's one thing that keeps on growing while the other ones aren't uh they wanted to capture several snapshot at different points in the application's life so that the devtools memory viewer could be used to show the difference between the heaps at different times the problem was capturing a single full-size uh a single full size 500 megabyte snapshot was taking over 30 minutes that doesn't make any sense I I've used this thing all the time why was this thing taking 30 minutes what it was the slowness in the memory analysis workflow that needed uh to solve what what narrowing the problem is there some difference between this and what you get is there some difference between what node does in this I I assume it's the same like it's literally the same call inside the V8 system then Bloomberg Engineers started investing investigating the issue using V8 parameters as described in this post node.js and V8 have some nice command line parameters that can help with these these options were used to create the Heap snapshots simplify the reproduction and improve observability Max old space don't you love that it's called Max old space size whenever I read the phrase Max old old space size I constantly think that this is like the old way to do it but old I believe refers to the generational garbage collection right this limits the Heap to 100 megabytes and helps reproduce the issue much faster keep snapshot Heap snapshot near Heap limit 10. this is a node.js specific command line welcome I forgot to turn off alarm I love you hey shh not now not now this is a node.js specific command line parameter that tells node.js to generate a snapshot each time it comes close to running out of memory it is configured to generate up to 10 snapshots in total this prevents thrashing where the memory starved program spends a long time producing more snapshots than needed awesome enable uh I don't know extended stack walking uh this allows uh tools such as etw and WPA and xperf to see the JS tax which means um which has been called in V8 oh nice okay perfect uh enter interpreted frame native stack this leg is used in combination with tools like etw WPA and xperf to see the native stack when profiling oh perfect Okay cool so you can see both Stacks right here uh when the size of the V8 Heap is approaching the limit V8 forces the garbage collection to reduce the memory usage it also notifies them better about the Heap snapshot near Heap limit flag in the node.js Genera let's see in node.js oh whoopsies something reads wrong here it also notifies the embedder about this the heaps not the Heap snapshot near limit flag in node.js generates a new Heap snapshot upon notification it just turns out it's me in the test case the memory usage decreases but after several iterations garbage collection ultimately cannot free enough space and so the application terminated with an out of memory errand um as we call it they took recordings using Windows performance analyzer see below in order to narrow down the issue this reveals that most CPU time was being spent within the V8 Heap Explorer specifically it took around 30 minutes just to walk through the Heap to visit each node in the collected and collect the name this didn't seem to make much sense yeah because I would assume it would take the same amount of time then to garbage collect that long why would recording the name of each property take so long this is when I asked to take a look quantifying the problem the first step was adding support to VA to better understand where time is spent during the capture of Heap snapshots the capture process itself is split into two phases a generation then serialization we landed this patch Upstream to introduce a new command in the flag profile Heap snapshot to V8 which enables logging of both generation and serialization times using the flag we learned something some interesting things first we could observe the exact amount of time V8 was spending on generating each snapshot in our reduced test case the first took five minutes the second took eight minutes and each subsequent snapshot kept taking longer and longer nearly all of this time we uh was spent in the generation phase okay okay this allowed us to quantify the time spent on Snapshot generation with a trivial overhead which helped us to isolate and identify similar slowdowns in other widely used JavaScript applications in particular eslint on typescript so we know the problem was not app specific furthermore we found the problem happened on both windows and Linux the problem was also not platform specific okay okay we're getting something good here we're getting something good here well let's let's do some cooking first optimization improves string storage uh hashing to identify what was causing excessive delay I profiled uh the failing script using Windows performance toolkit when I opened the recording with Windows performance analyzer does he work on Windows gross what I found was this string storage git entry one third of the samples was spent in git entity okay or git entry uh all right what goes on here compute hash interesting interesting we literally just compute a hash and we just do a name lookup okay because this was run with a release build the information of the inline function uh calls were folded into Strings get entry to figure out exactly how much time was uh the inline functions calls or we're taking I added the source line number column to the breakdown and found that most the time was spent on line 182 called compute string hash is this starting to look like rolling your own map is that what I'm seeing right now are we looking at like a roll your own map problem here classic issue um all right so looks like uh they really like that one okay so over 30 of the snapshot generation time was spent on compute string hash but why well let's first talk about string storages or strings storage its purpose is to store a unique copy of all strings that will be used in the Heap snapshot for fast access and avoiding duplicates this class uses a hash map backed by an array where collisions are handled by storing elements in the next free location of the array okay so standard hashmap pretty much I started to suspect that the problem could be caused by collisions yep that's where I would expect which could lead to Long searches in the array so I added exhaustive logs to see generated uh hash keys and on inserts and see how far it was between the expected position calculated from the hash key and the actual position of the entry ended up uh ended up in due to collisions the log the things were not right the offset of Many Items was over 20. and in the worst case the order of thousands so this sounds like what we have right here this sounds like we got ourselves a little bit of a collision here right what is all this happening why are we doing that I know would I okay I use Windows on stream I use Windows on stream come on yep let's see part of the problem was caused by numeric strings especially strings for a wide range of consecutive numbers the hash key algorithm had two implementations one from four numeric strings and another for other strings while the string hash function was quite classical the implementation for the numeric strings would uh would basically return the value of the number prefixed by the number of digits nice the original hash kbit uh K value bits 24 mask we're gonna shift this thing all the way over minus one a bunch of okay yep very beautiful so that would be uh 23 ones right no that's 24 ones numeric string length shift that over or together with numeric and mask okay I I guess I don't understand the problem here but I I um original hash okay what is X yeah I guess they're they're somewhat close Okay it takes a tenfold it takes a 10 order magnitude to shift this large key uh this function was problematic some of the examples of the problems with this hash function once we inserted a string whose hash key value was small uh was a small number we would run into collisions when we tried to store another number in that location and there would be similar collisions if we tried to store subsequent numbers uh consecutively so is the storage Arena just right here is that what they're saying the storage arena is right here because I would assume they'd do this whole like this whole thing modulo the size of the Maps underlying storage area and so this would still map to a different location than this which would map to a different location this but that must not be the case all right once we've inserted a string whose hash key value was a small number we would run into a collisions when we tried to store another number in that location and there would be a similar collisions if we tried to store subsequent numbers uh consecutively yep uh or even worse if there were already a lot of consecutive numbers stored in a map and we wanted to insert a string whose hash key value was in that range we had to move the entry along the occupied locations ah yes yes I guess it does create these big block problems yeah what did I do to fix it the problem comes mostly from numbers represented as strings that would fall in consecutive positions I modified the hash function so we would relocate the resulting hash value two bits to the left the old multiply by four situation people yeah so it just it just simply yeah I was about to say just simply hops by four right so each pair of consecutive numbers uh would uh we would introduce three free positions between uh in between these modifications was chosen because empirical testing across several work sets showed that it worked best for minimizing collisions this hashing fix has landed in V8 isn't it sometimes emotionally painful to see that like something as simple as this can literally change the performance of a system massively it doesn't leave a lot of free room between them but it does leave a lot of free room between them you know what I mean like it both doesn't and does assuming that collisions are somewhat rare this leads 75 you know it's it's a 4X the amount of space in between so collisions should happen a lot lost less then but if you had a collision at all with a bunch of consecutive numbers you'd have to travel that entire consecutive number right so it makes sense in the end but still wild after fixing the hashing we re-profiled and found further optimization opportunity that would reduce significant part of the overhead when generating a heap snapshot for each function in the Heap VA tries to record its start position in a pair of line uh and column numbers this information can be used to buy the dev tools to display a link to the source code of the function during usual compilation however V8 Only Stores the start position of each function in the form of a linear offset from the beginning of the script oh no oh yes please tell me please tell me to calculate the line and column numbers based on the linear offset V8 needs to Traverse the whole script and record where the line breaks are this calculation can be very expensive uh what did I call it did I call it that is it called LS on this one oh it is oh it is look do you like that little look at this warning I'm doing right here look how cool that little warning is here's the funny part uh if I open up utils whoopsies uh you didn't I call you what did I call it here uh bite literally doing the exact same thing given a position I have to hand it the source plus a bite position and walk the whole damn thing look at that I'm doing the exact same thing oh the feels ah but I did I I mean I did test mine and I was able to get it was pretty fast I I mean obviously I'm not doing what they're doing and I'm not calculating every piece of memory I have very few warnings but nonetheless very funny normally after V8 uh finishes calculating the offsets of line breaks in a script it caches them in a newly allocated array attached to the script unfortunately the snapshot implementing cannot modify the Heap when traversing it so the newly Crea the newly calculated line information cannot be cached let's go the solution before generating the Heap snapshot we now iterate over all the scripts in VA contacts to compute and cache the offsets of the line breaks as this is not done when we Traverse the Heap for Heap snapshot generation uh it is still possible to modify the Heap and store this uh Source line positions as a cache okay the fix for caching of the line breaks offset has also landed in V8 did we make it fast after enabling both fixes we repo profiled both of our fixes only affect snapshot generation time so as expected snapshot serialization times were unaffected when operating on JS programs containing development development generating generating time is 50 faster production.js generating times is 90 faster that should make sense because obviously there's in fact did you know that if you take a node.js program when you Minify it you just get better performance right it's wild it's because you got to remember that there is this like whole parser and interpreter blah to the blah blah blah uh why was there a massive difference between production and development code the production code is optimized by bundling and minification so there are fewer JS files and these files tend to be large it takes longer to calculate Source line positions for these large files so they benefited the most when we can cache The Source position and avoid repeating calculations the optimization we were validated on both windows and Linux Target environment for the particular challenging problem originally faced by the Bloomberg Engineers the total end-to-end Time Capture 100 megabyte snapshot was reduced uh a painful 10 minutes down to a very pleasant six seconds ah that is awesome that is great well it's not it's it's not six seconds to write a hundred megabytes it's six seconds to find everything that's in memory gift it all and then write it to a file right so you gotta remember the writing to the file Parts probably really fast right that's probably you don't even notice it fast it's all the other stuff yeah it's a noticeable Improvement you know I wonder how they I wonder what uh or how they felt upon releasing the gains probably felt pretty good uh the optimizations are generic wins that we expect to be widely applicable to anyone performing memory debugging on V8 node.js and chromium these wins were shipped in V8 11. what's next first it should it would be useful for node to accept the new come on node get your together node uh flake and node options yes in some use cases users cannot control the command line options passed to node.js directly and to have a con and have to configure them through the environment variable node options today node.js filters V8 command line options uh set in the environment variable people and only allows a known subset which could make it harder to test new V8 flags on node.js as what yeah okay well just work with them I'm sure you can get this going right uh today let's information accuracy and snapshots can be improved further today each script source code line information is stored uh representation the V8 Heap itself and that's a problem because we want to measure the heat precisely without the performance measurement overhead affecting the subject we are observing yep it's true that means when you do this you take all that extra information you add I mean really you're probably adding 50k worth of data Maybe maybe 100K who knows but it's probably nothing comparatively to the crappy amount of objects you've created in react ideally we would store the cache of line information outside the VA Heap in order to make the Heap snapshots information accurate finally now that we've improved the generation phase the biggest cost is the serialization phase further analysis May reveal new optimization opportunities and serialization absolutely very cool awesome uh awesome job Engineers love it great times this was great Jose thanks for posting this appreciate this this was super cool it's I mean I think at the end of the day it's always emotionally painful to discover the uh to discover some of the like solutions to bad performance sometimes they're as stupid as that right cash Collision in a map is just like a huge problem and look at it go right uh I also think that it's uh hilarious that the other one is don't do work you don't have to do memoization can be effective so when you're doing this right here A whole bunch memoization makes sense right because you can just do like a really fast like B tree lookup or some sort of some sort of ranged binary search that's amazing right so I like it I like it a lot this is beautiful thank you very much hope they're not using Json they are using Json they're using Json uh because the Heap snapshot is like a just a giant Json blob uh unless if it's a different Heap snapshot these days uh what's the link for the article it's on the VA Dev blog I'll link it in the video of course but great great this was great the name the V8 again