We've built artificial that, on specific tasks, There's AI that can play chess But since the introduction to the general public a couple years ago, there's been more talk about artificial and that describes the idea that there's AI that can perform on a wide variety of tasks, And people who think about AGI if we reach that level Right now, there's people coming out and saying &quot;The AI that we're building that it poses a threat to civilization." And they're going &quot;Maybe you need to regulate us.&quot; Now normally when an industry they don't say it poses and that it needs to be limited, And I think there's two main reasons. One is if your technology is so powerful that it can destroy civilization, there's an awful lot of money And what better way to put some money with you than to warn that your tool The other is that the idea is truly a cinematic concept. We've all seen those movies. And it's kind of entertaining to think with tools that we're actually able In fact, it's so entertaining from the real problems already happening The more we think the less time we spend thinking or the fact that there's AI right now whether or not people and we know it's racially biased. But are we anywhere close Some people think so. Elon Musk said that we'll I think he posted this a few weeks ago. But like at the same time Google put out their eye search tool so you don't have to click on a link, and it's not going super well. [&quot;How many rocks should I eat?&quot;] [&quot;... at least a single serving Please don't eat rocks. (Laughter) Now of course these tools But if we're going to achieve AGI or if they're even going to fundamentally we need to be in a place on a sharp upward trajectory And that may be one path. But there's also the possibility is that these tools what they're capable of doing, and the future is incremental So to understand the AI future, we need to look at all the hype and see what's technically possible. And we also need to think about where and where are the areas that we don't. So if we want to realize the one main challenge These algorithms are wrong all the time, And Google actually came out and said, after these bad search results that they don't know I use ChatGPT every day. I write a newsletter that summarizes and so I download that data, ChatGPT helps me write a summary. And it makes me much more efficient But I have to correct it every day because it misunderstands something, it takes out the context. And so because of that, I can't just rely on it And this reliability is really important. Now a subpart of reliability a great technical term for the fact a lot of the time. I did this in my newsletter. I said, ChatGPT are there any If so, give me the quotes. And it produced these three that didn't sound anything on these message boards. And I went back to the data, It just made it up out of thin air. And you may have seen this I asked it to give me a close up That's a hallucination (Laughter) We have to solve if this AI is going And I don't think it's a solvable problem. With the way this technology works, we're going to have it taken but there's no technical reason Because generative AI When you ask it a question, it's creating that answer when you ask. It's not like a search engine that goes And so because its job I don't know that we're going to be able and then not make up other stuff. That's not what it's trained to do, and we're very far from achieving that. And in fact, there are spaces One space that there's is in the legal area where they hope it will help Some people have found out the hard way that they should not write and send them to federal court, because it just makes up And that's a really fast way and to get your case thrown out. Now there are legal that advertise hallucination-free generative AI. And I was really dubious about this. And researchers at Stanford actually and they found the best-performing still hallucinates 17 percent of the time. So like on one hand, it's a great scientific achievement that we can pose basically any query to, and 60 or 70 or maybe even it gives us a reasonable answer. But if we're going to rely and they're wrong there's no model And that kind of leads us into how do we make these tools that useful? Because even if you don't believe me and think we're going to solve we're going to solve the tools still need And there's two things One is lots more data and two is the technology So where are we going to get that data? Because they've kind of taken And if we were to find twice as much that doesn't mean they're going I don't know if there's and it's compounded by the fact that one way the generative AI is at producing low-quality That's bots on social media, and these SEO pages but have a lot of ads And if the AI starts training we know from decades of AI research It's like the digital version (Laughter) Let's say we solve the data problem. You still have to get And we've seen 50 billion dollars invested in improving generative AI. And that's resulted in three So that's not sustainable. But of course it's early, right? Companies may find ways to start But is it going to be valuable enough to justify the tens and maybe of hardware that needs to be bought to make these models get better? I don't think so. And we can kind of start looking And it leads us to think about where Because one place is that AI is going to take Lots of people are telling us and people are worried about it. And I think there's a fundamental So imagine this scenario. We have a company, and they can afford to employ And if we were to give those engineers which is something it's pretty good at, let's say they're twice as efficient. That's a big overestimate, So in that case, the company They could fire one because the other one can do or they already could afford two of them, and now they're twice as efficient, so they're bringing in more money. So why not keep both of them The only way this math fails that it's not worth it. But that would be like to do one person's worth of work. So that sounds really expensive. And practically, there are already open-source that are low-cost, that companies Now they don't perform as well but if they're half as good wouldn't you take those over the one to do one person's work? Of course you would. And so even if we solve reliability, we make the models better, the fact that there are cheap suggests that companies hundreds of millions of dollars There are areas that we need Because if we look at AI now, there are lots of problems I've been building artificial and one thing we know is that if we train AI on human data, the AI adopts human biases, and we have not been able to fix that. We've seen those biases start and the gut reaction is always, well, let's just put in some guardrails to stop But one, that never fixes the bias And two, the guardrails themselves So Google has an AI image generator, and they tried to put guardrails in place And it turned out it made it wrong. This is a request for a picture of the signing of the Declaration And it's a great picture, And so in trying to stop the bias, we end up creating We haven't been able to solve And if we're thinking replacing human decision makers and we can't solve this problem, that's a thing that we should worry about and demand solutions to before it's just widely adopted And I think there's one final thing which is our human intelligence is not defined At its core, it's defined by our ability Our ability to have emotional responses, to take our past and integrate it and creatively come up with new things. And that's something that artificial nor will it ever be capable of doing. It may be able to imitate it and give us a cheap facsimile and empathy and creativity. But it can't do those And that's why I'm not really worried But if you come away from this and right now you're worried about humanity being destroyed the one thing to remember is, despite what the movies have told you, if it gets really bad, we still can always just turn it off. (Laughter) Thank you. (Applause)