February 18, 2017. At the Cloudflare HQ work as the weekend approached. Morale was high and no to happen. The engineers were excited to go home operational issues. Suddenly 4:11 pm Pacific time the Googlers working in Project Zero, a security Cloudflare's system. He immediately reached out urgent like this and first contact was made alarming details of the report were made clear You may have seen Cloudflare's DDOS mitigation service before. This is built on top of their primary product, a Content delivery Network, or CDN. CDNs came into existence in the 1990s They're kind of like distribution centers. Amazon middle of the United States that every delivery across the country and they a store or should I say Similarly it makes no sense to deliver internet single centralized source. A CDN will have with edge servers that cache content from the a particular website, the request is directed to likely already cached. It was here that Cloudflare cookies, keys, and other sensitive customer data. where to look, plenty of useful information could requests, IP addresses, responses, passwords, and who actors could have already compromised thousands did not self-detect this issue, as a third party leakage like this can come with hefty consequences: importantly of all it degrades customer trust. no revenue. No revenue, no Taco Tuesdays. To make regularly index and cache websites so this leaked 4:40 PM. Now this was serious business. Everyone even some cross-company action with the Google that the occurrence of this bug seemed to correlate which was also immediately suspected as it had a recent deployment to partially migrate to a new HTML parser. Either way every feature that engineers immediately flipped what they called the using the feature. By 5:22 PST about an hour had been disabled worldwide. However, the the Atlantic, the London team had joined the the Friday night debugging and rethinking life. 8:24 PM PST, four hours in, another two features were found to be problematic: automatic HTTP rewrites was shut down immediately with its global kill. But server-side excludes with such an old feature that it predated the practice of deploying with global could release a patch for this feature to allow for implementation and deployment. Alternatively and deploy a single proper fix. But the root cause on the global kill for server-side excludes and As the night progressed streets outside the London, the engineers were more than ready to patch to turn off server-side excludes was finally to be done. Cached data from search engines still root cause, reoccurrence was still within the realm Well, edge servers contain software to perform all This was the clear common denominator among the and modified the returned HTML content in some way. in the returned webpage if the requester source IP similar, it can automatically hide content wrapped Automatic HTTP rewrites would simply rewrite any HTTPS. Furthermore these three features all used engineers however found nothing suspicious It wasn't until the next few days Now, Cloudflare had originally been using a parser migrate to something simpler and more maintainable. software that the bug took root. Ragel is a that works through defining finite state machines actions based on the match results. You can think state and transfer to different states based on which matches consecutive numbers and letters. In C here using the double percent signs and then it It's actually fairly readable and concise So the HTML web page consumed by the Ragel parser is represented by a series of data buffers code. Each time the Ragel parser is invoked to pointers initialized to the beginning and end the buffer and pe to tell when the buffer has been they wanted to parse were HTML attributes Taking a look at the Ragel code, this script_consume_attribute machine will try to match this regular expression: attribute characters Then we have a few actions, this is an entering the machine. It simply logs that the script is which is performed when the machine completes equivalent to p-- and will move the script_tag_parse machine it proceeds to jump to characters that the attribute machine would have There is also a local error action, which match. There's a log here for failure and then it Going back to the success case, after exiting back continue until the end of the buffer is reached. the buffer? Well if the data pointer p is equal reached the end of the buffer. So it turns out unfinished attribute at the very end of a web occur when the data pointer p is equal to data now at risk of parsing undefined heap memory. Aw man, the pre-increment causes p to skip over But wait, this is a bug in the old parser which has data all this time? No. So it was actually migration back to the buffer override we were talking about unfinished tag could just be due to the rest of action will not be invoked. The error action is very last buffer, as there is no more data at that example, the unfinished attribute is at the very last possible buffer. However the key here is that would always receive an extra dummy last buffer just did. This meant that for a website that ended in the second to last buffer, and the error action empty, the parser would also not be invoked again. After the and the empty last buffer was no longer present the unfinished tag to be in the last buffer and cleaned up the empty last buffer before passing can only occur when a customer enables features So what can we learn from this failure? Well compatibility. No matter how dumb the behavior of time and you change it, something is definitely to maintain backwards compatibility. Obviously the ability for Windows to run 32-bit programs, more accurately, not adding the extra dummy buffer be overlooked. And it was not just this but also a of input that in combination caused the data leak. of interlocking components each with millions inevitably be bugs in all software. So what can fuzzing generated code to search for pointer malformed web pages. There are also various memory can likely also have been caught by static code are best practices. The coding standards for experimentation, I don't think it is possible It's possible to under-run the buffer by spamming overrunning impossible. There's no Ragel command Ragel iterates the data pointer forward naturally has reached the data end. This points to Cloudflare code rather than the Ragel code itself, something Two days later, pointer checks to detect memory engineers determined it was safe enough to Cloudflare then worked with the various affected websites. In terms of overall impact, were quite a few conditions that needed to claims that there is no evidence of the bug 0.6 percent of cloudflare websites ended with than 18 million times. It is reasonable to say one of the features which could trigger this Had this exploit falling into the wrong Cloudflare is so much bigger, there