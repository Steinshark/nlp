At this sprawling lab at Google's Silicon Valley running workloads for Google Cloud's millions of Here, for example, is the very first Trillium system with 256 chips in it for racks. Or for YouTube, or the world's most dominant search And what is Trillium? Trillium is our latest generation TPU. It'll be public later this year. Instead, they're running tests on its very own all, search. And of course, video, YouTube ads. Everything Google does has been powered in many ways Now, TPUs are used to train AI models like Google's own .. and in some big news, Apple's AI, too. Apple, actually, we found out yesterday they disclosed in a The world sort of has this fundamental belief that all Nvidia. But Google took its own path here. And yet, despite being the birthplace of some Google's fallen behind in the AI race. But it was the first major cloud provider to do custom It was ten years ago, almost to the day, where we in terms of a particular application, voice hardware. In the years since, Amazon, Microsoft and Meta have Here we're turning on the chips and the boards for the specification debugging any issues that might come up, And no media has been inside here before. First time? Yep. We went to Google headquarters for an exclusive look to ask why and how. Google's betting big on the expensive, complex It all started in 2014, when a group at Google recognition features, Google would need to double the Amin Vahdat, now the head of custom cloud chips, A number of leads at the company asked the question, with Google via voice for just 30s a day? And how much compute power would we need to support We realized that we could build custom hardware, not Processing Units in this case to support that much, In fact, a factor of 100 more efficiently than it What is a Tensor Processing Unit? And did you guys coin that term? We did. We coined the term Tensor Processing Unit. We believe that it was certainly the first large scale There's a whole gamut of qualification and validation You're really trying to make sure the design has know, at volume, at scale. Principal engineer Andy Swing, who ended up leaving There's actually four chips inside there. It's connected to. Actually, two of those are connected to a host machine And then all these colorful cables are actually one large supercomputer. Google data centers still rely heavily on chip giants CPUs, and Nvidia for Graphics Processing Units. GPUs. Google makes a different category of chips circuits, which are more efficient because they're Google's best known for its AI-focused ASIC, the TPU. But it also makes ASICs to power YouTube, called VCUs, And just like Apple, Google also makes custom chips The G4 powers the new, fully AI enabled pixel nine, Pro two. But the TPU is what set Google apart because its kind. So the AI cloud era has completely reordered the way And this silicon differentiation, the TPU itself may went from the third cloud to being seen truly on two clouds for its AI prowess. Amazon Web Services announced its first cloud AI chip, came out. Microsoft's first custom AI chip, Maya, In order to stay differentiated, to stay competitive, dependent on any supply chain, partner or provider, more in-house. According to Newman's team's research, Google TPUs market share and Amazon comes in second at 21%. In 2017, a group of eight Google researchers wrote the the underpinnings of today's generative AI craze. The invention, Vahdat says, was made possible by TPUs. The transformer computation is expensive, and if we purpose compute, maybe we wouldn't have imagined it. Maybe no one would have imagined it. But it was really the availability of TPUs that algorithms like this, but we could run them Still, Google has faced criticism for some botched AI, and its chatbot, Gemini came out more than a year Dozens and dozens of customers are leveraging Gemini out there, whether it's Deutsche Bank, Estee Lauder McDonald's, if you like, and others. Was Gemini was trained and is served externally entirely on Back in 2018, Google expanded the focus of TPUs from Version two was actually a pod that connected 256 TPUs Now version five is in production, which connects The real magic of this TPU system is that you actually dynamically. And so you can build small or as large of With version two in 2018, Google also made its TPUs leading chips like Nvidia's GPUs, which are still used If you're using GPUs, they're more programmable, they're more flexible, The AI boom has sent Nvidia's stock through the roof, cap in June, surpassing Google's parent company position as the world's most valuable public company. Being candid, these specialty AI accelerators aren't powerful as Nvidia's platform, and that is what the can anyone play in that space? Now that we know Apple's using Google's TPUs to train full AI capabilities on iPhones and Macs next year. They were renting chips from Google for about two bucks chips to train their AI models. So they didn't even need Nvidia. All the market pull is coming from Nvidia, but longer And when they want to just do AI things, they may be homegrown piece of AI dedicated silicon. But developing alternatives to Nvidia's hugely It's expensive. You need a lot of scale. And so it's not something that everybody can do. But these hyperscalers, they've got the scale and the But the process is so complex and costly that even the Since the very first TPU ten years ago, Google's helps Meta design its AI chips. Broadcom says it's spent more than $3 billion on R&amp;D AI chips, they're very complex. There's lots of things on there. So Google brings the compute. Broadcom does all the peripheral stuff. They do like the I/O and the SerDes and all of the They also do the packaging. Then the final design is sent off to be manufactured at the world's largest chip maker, Taiwan Semiconductor world's most advanced semiconductors. Do you have any safeguards in place should the worst Taiwan? Yeah, it's an important question. And it's certainly something that we prepare for and But we're hopeful that actually it's not something I think the entire world is at the same risk. It's not unique to Google. It's not unique to Amazon. It's not unique to Apple. It's not unique to Nvidia. If Taiwan is not given the appropriate support. If it deals with unexpected end of day circumstances, one of these companies, it's going to set back the That's why the White House is handing out $52 billion US, with the biggest portions going to Intel, TSMC and Intel and TSMC are putting a lot of their own money into this I'm heartened to see that. But I mean, it's going to take a long time to to So let's let's hope that it doesn't need to be Risk aside, Google just made another big chip move, Axion, will be available by the end of the year. Now we're able to bring in that last piece of the And so a lot of our internal services, whether it's and more running are running on Axion. But Google is late to the CPU game. Amazon launched its processor Graviton in 2018. Alibaba launched its own server chip in 2021, and Why didn't you do it sooner? Our focus has been on where we can deliver the most starting with the TPU, our video coding units, our We really thought that the time was now starting a Again, these things are a number of years in the CPUs. I don't fault Google for pacing out the launch of Axion delayed fashion. This wasn't as differentiated. It's not as differentiated. To me, it is more of a supply game. It's more of a margin and vertical integration game Whereas the TPU was truly differentiated. Six generations, ten years of experience. All these processors from non chipmakers, including more customizable, power efficient alternative that's from Intel and AMD. Power efficiency is crucial because by 2027, AI power every year as a small country. With TPUs, the ability to customize greatly boosts This is our second generation optical circuit switch. So our large TPU supercomputers are actually optically It allows us to dynamically link together collections job that's running. This is developed all in-house by Power is a huge thing now, and you know any anything costs and save power, I think you're going to do. Google's latest environmental report showed emissions partly due to data center growth for powering AI. Without having the efficiency of these chips, the We remain committed to actually driving these numbers 24/7, driving it towards zero. Training and running AI also takes a massive amount of That's why with the third generation of TPU, Google servers that uses far less water, and that's also We have four chips, and these are our liquid cooling There's essentially a cold plate here that has little puts it into the water, and that comes back out. Despite challenges from geopolitics to power and water, tools, but to making its own chips to handle the I've never seen anything like this, and no sign of it I think it's fair to say that we really can't predict five years, and hardware is going to play a really