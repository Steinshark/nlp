Hello, everyone. This presentation will be about the story between RealityCapture and also Mels Studios. Unfortunately, Mels Studios won't be joining us today, but they will be here virtually. Before we start, we would like to introduce ourselves. So my name is Tomas Misura. I am a technical product manager at Capturing Reality at Epic Games, and I'm here with? I'm Silvia Tuha, product specialist at Capturing Reality Epic Games. So yeah. Special thanks. So this is the crew from Mels Studio. Huge shout out to them for organizing everything and also the production volume and their generosity while I was in Montreal. I also like to have some special thanks to Overhe4d, which helped us with some post-production while I was really busy doing some other work. So what's the story like? What we were trying to accomplish? So one of the use cases that is quite common and I see it a lot in production is basically when people are trying to build some stats, it could cost a lot to build these stats. After the shoot is over, they usually scrap it. They destroy these stats. What we could do is to preserve these stats and use it later for virtual production. You could also resell these to other studios. There are many things that you can do with it. But first, we need to create a digital twin out of that. So I'm now going to show you a video that we accomplished together with Mels to tell you more about that. [VIDEO PLAYBACK] - Hello. I am Tomas Misura. I am a technical product manager at Capturing Reality team at Epic Games, and I'm here to digitize the replica of a metro station that is Mels Studios. - The thing is Mels acquired one to one replica of a subway station and a metro car, which served as set pieces on several movies. But set pieces are not resistant to time, and that's why we wanted to keep a digital copy of it, so we can use it as a virtual background on our stage. - So when there is any shooting, let's say a film or a series, there are a lot of assets created for that series. Some of these assets can be quite huge, such as this metro station. And usually when the shoot is over, these assets are destroyed. These assets can cost millions of dollars, so what we want to prove is that we can digitize these assets and then we can reuse these for other shoots. - That's where RealityCapture comes in to create the digital twin. Now we can even shoot a scene inside of the practical set while at the same time have another scene with other actors shoot another scene inside of the virtual production studio, because we have exactly the same environment. - The size of the station is really gigantic. That is the main reason why it's hard to do it. But at the same time, it's a great asset for learning. So I actually used that asset mainly to teach people from Mels how to digitize such assets. - And since we have it as a 3D asset, it means that we can imported inside of Unreal Engine and basically think about some modularity in the space. So we can cut some wall, duplicate some platform, and we have a bigger station. We can also add new props, change the lighting, and basically have the full control makes us able to meet every production need. - The greatest challenge about photogrammetry is to avoid any kind of reflection, shadows, flares, and glares. So you have to keep your lighting as soft as possible with the maximum output to reach a stop like T8 to 11. That's a lot of light. At Mels we do virtual production, but our core business is studios and equipment rental. All that light is available here, so when Tomas asked me, can you enhance the studio light? I told him, well, what's your dream setup? - And this is where Mels' team was so amazing. So we got everything changed in a matter of minutes, as I liked, because we need a lot of light. - At one time we even brought five 18k ARRIMAX and we projected to gigantic 20 by 20 bounces to scatter all of this light all around the studio and get a really diffuse light without too much shadows. And then we even brought six sky pans to light the interior of the metro station. And then we clipped three Astera, which are tube of light, to a scissor lift. So we were able to laser scan at different heights while keeping the same exposure. - Nowadays everybody can scan a 3D mesh with their phone, even us. We use the RealityScan app to scan, on the go, object that we needed to add to our scene. But Tomas really showed us that for a big set like this, we really have to combine two different techniques. Photogrammetry technique and also the laser scanning technique. - The laser scan is having some issue with black surfaces where photogrammetry doesn't, and photogrammetry have some issue with white surfaces with no texture where the laser scan is not having any trouble with that. So by combining the two techniques, we utilize their strengths to cancel out their weaknesses. - That's why I'm trying to connect these two technologies, and RealityCapture is the software where you can ingest both at the same time. So any imagery and, at the same time, the laser scans themselves. [END PLAYBACK] Yeah. Thank you for listening. I'm just now going to recap what you see in the video, and also I'm going to talk more about the process how this was accomplished. There is a process in that, and I would like to share it with everybody so even other studios can basically start doing this. Why would you like to create something like a digital twin? As I mentioned at the beginning, it's basically mainly that. So you start with a set. There is a movie. In the next slide I will basically show you a circle that I came up with which could be basically repeated for each and every studio. But to sum it up, why would you like to do it is basically to preserve your work before you basically scrap the set. It's really great to have the database. It could be used for J&amp;AI companies. It could be also used for other studios that are having difficulties finding some set. And since studios need to destroy their sets because it just takes a lot of space in their studios, yeah, that's why you want to preserve it. And RealityCapture can accomplish that really easily because you can get to microns of accuracy. You can get textures. You can get models. You can get anything, and you just need to save it to your drive or publish it to FAB. So how much does this particular set, for instance, cost? It was around $1 million. And after shoot is over, you scrap it. So this is what I came up with. This is actually like a circle, as I mentioned. So you have a new set. Then you could start with set skinning. So set skinning could be something like the capture, the planning, the scouting process. That's what I call set skinning. Then I have a different step, which is called the RealityCapture step where you are trying to create the 3D models with textures, and that is actually what you do in RealityCapture. So here you can create the digital twin. The next step is basically virtual production in UE5. So obviously when your asset is ready, you can ingest it to UE5. And then, of course, you can even preserve it for later use if you don't need for that particular virtual production and preserve it on FAB. You can sell it to other companies. You can also do whatever you want with that. So that's the process. Then you can set-- you have a new set. Then you repeat with that. So now I'm going to talk about this process more. So the first is the planning. Yeah. You already seen this in the video, but with Mels it was kind of hard to do the planning because the planning just took two days. I didn't know dimensions of the New York station that they built in the studio, and it was really hard to do it. Yeah. I mean, you can see the dimensions of that. So they had a really large environment. They had the metro card. They had the metro station. There are different materials. And yeah, I had to come up with a plan, what equipment we need, what lighting we need. And I only had one backpack, but it was great to use Mels because they have basically everything inside that. So they have lighting. They have cameras. I could basically rent anything I want, so it was also a little bit of improvisation. I was also having cross-polarization set up in my backpack with a Sony camera and cross-polarization light, cross-polarized light, so it was really good. This is just a simple list that I was basically using. In the end of the list there is also scissor lift. I didn't have that in the backpack. But yeah. Basically that was also one particular thing which I really liked, because I was able to get to different heights. I could use even laser scanner on a scissor lift. I really love that scenario. We were also using dulling spray, scanning sprays, because photogrammetry don't really like shiny surfaces and also translucent ones. So we could do that. There are some sprays that will actually vaporize in minutes, so we were using these to not damage the set. I was also using my iPad because for laser scanning it's really good to have a preview. And I had an app which was basically just controlling me and I'd seen where I already was while doing the laser scanning. And it was also good too for teaching, basically, Mels because, for instance, the metro station was scanned just by Mels' crew while we were doing this exercise. I was also having a bunch of lenses. I'm a prime lenses person, but you can definitely use zooms. Yeah. I also brought Sony camera. So this is the workflow that I was mentioning that I want to go through today. Hopefully you can read it correctly. If not, I will read it for you. So the first process is the capturing and rigging. Yeah. I would say that this is the most important part of the whole process, because how reality capture works is basically garbage in, garbage out. The better inputs you have, the better outputs you have. Software won't really make miracles if you don't do proper capture. So that's the first one. What is really nice about reality capture is that you can ingest any imagery. It could be even a screenshot from your screen. It could be DSLR image. It could be a phone image. It could be a rig. But you can also ingest laser scanners and then combine them automatically inside RealityCapture, which is really powerful. So yeah, this is basically capture and rigging. I would also say that the really important part is the actual prep. We did some rigging, too. We had, for instance, the Dolly, which was basically done in minutes by Mels' crew. I also had a running setup, which was basically just remote shuttering from the ground and also moving everything. The Dolly was basically there on the ground, but I also had the ramp which was around the train, and we were going left to right to mimic the drone. We couldn't really fly the drone inside the studio. That's why we came up with that setup. So that was pretty nice. I'm just going to repeat the video again. We were using also these large lights, and this is the laser scanner on the scissor lift. Yeah. This was actually the ramp that we mentioned. [INAUDIBLE] sprays for scanning. And we also did need a lot of storage. What I want to say, you never have enough of storage on these sets, so I would actually bring a server. Pre-processing. So after you have everything done, what you should do is to really calibrate the images. In RealityCapture, you don't really have any color management, which is actually a good thing because you can do your color management on images. You could actually export your images as linear, and then RealityCapture will not touch it anyhow. So you could be doing basically 32 floats in linear. RealityCapture won't touch that. And then you can color grade them when you export from RealityCapture. So it's simple as that. We were also using these targets, which are basically used as scale, because these are automatically detected by RealityCapture. But what is good is that we were using in RealityCapture also these laser scanners, which will scale your scene automatically. So you are getting perfect scale which you can use in any program, such as Unreal, and it will be automatically scaled even without using these markers. I just want to point out, if you are not using laser scanners, I really advise you to have something like this. For instance, on this marker you know that from one point to another it's 25 centimeters. And you can even load the distance constraint to RealityCapture, which is pretty nice in pipelines because, yeah, you just need these two points and this whole scene will be basically scaled. I forgot to mention that I was using also these X-Rite color checkers. These are pretty nice because they can give you some amount of color accuracy. If you want to be perfect then, of course, there are different solutions. But this is good enough for most productions. Then we were also doing pre-processing for laser scans, but for that, I would like to hand it over to Silvia. Thank you, Tomas. So in this step, we are using the pre-processing for laser scans, which is another input that you can upload to RealityCapture. This step really depends on the manufacturer that you have chosen for the laser scanner. That's because they also provide you with the software where you can register those scans. So you are putting those cloud to cloud-- those compositions together, and you are pretty much just trying to overlap them on top of each other. So what you can see now on the video is that we are using a top view which is really good for seeing the walls and edges, because this is something you can then use for the overlapping. Then we will continue with RealityCapture. So here we will start with alignment. This is really, really crucial step of the 3D modeling. And what is here happening is that RealityCapture will automatically generate alignment of all the inputs that you have uploaded to your project. This can be images and laser scans. And what is this process about is that the software will detect all the features on the images and in the next stage it will match those features together. And the end product is a sparse point cloud which is representing the type points of the features that were found on the images and the camera positions. So you have this kind of representation of the cameras how the images were captured in a 3D space. The next stage we are moving to the meshing. So in this step, RealityCapture will calculate a complete dense 3D mesh and a 3D point cloud of the preferred quality. The end result is watertight mesh. That is pretty much just a collection of 3D points in a space representing the features on the input. And then the second end product is the mesh that is just the connection of the points from the point cloud forming triangles that are collectively defining the mesh or the 3D surface of the model. Next step in RealityCapture workflow is simplification and cleanup, which is another toolset that RealityCapture is providing you with. So you can then check the topology of your mesh and then simplify or optimize the mesh. The simplification is because you won't be able to use those two big meshes because you can end up with triangles, like in billions, and you don't actually need this for next processing. So you will just try to decimate this to something that is preserving the level of detail but is more compatible with third party softwares. Now I will hand it back to Tomas. Yeah. Thank you. So as you can see in the video, I just want to point out that this is an automatic process. There is one trick in that. So what you can do is basically you can select the largest connected component, which is basically the station in this scenario. Then you can invert the selection. So you can just filter out any points that are just hanging there in the air. It's really powerful. Not many people know about that so I just wanted to have some tip in here. Yeah. The next step would be splitting into logical parts. There is also a tool in RealityCapture that provides you with the power to do that. It's called cut by box. But for this particular job, we're actually using a different software. So we end up with splitting it into these logical parts. You had stairs. You had slopes. You had walls, ground, things like that. So we can be basically moving anything we like. We can copy it over. This was actually pretty much enough for this production. By the way, this station was also used in Scream 6. I don't know if I mentioned that. After that, you actually want to do some geometric cleanup or retopology because, yes, photogrammetry is amazing in the terms of granularity and the accuracy that you could get, but also there could be some noise around and maybe something to tweak. There could be some non-manifolds, sneaky geometry. So we do that, and we do that a lot. We do use other softwares to actually fix the geometry. So as you did see, there were some triangles that would need to be fixed. And yeah, this is something that you need to do, and I'm completely open to say that. In some productions you also need to basically get rid of some objects, and this is basically what we were doing for these stairs. This also allowed us to use different materials for certain parts. So yeah. This is the truth. After you have all these logical parts done, you can do basically UV unwrap for your main model. This is, by the way, automatic process in RealityCapture. You have multiple options. You can, for instance, do UV unwrap for the whole part in a single 8K, 16K texture, or you could do also a different type, which is basically, for instance, fixed texel size. Which will say to RealityCapture, do how many textures you want because I want to have full fidelity, full accuracy. And this is pretty powerful, but you could also end up with 8K textures. So sometimes it could be too much. But you have a toggle to turn it or tune it a little bit down, if you like, because there is also a custom functionality. So during the unwrap, you just say how many textures you want. You can also fill your unwrap with color checker to see how seams look like. And then you just hit the texture button and it's done. After this is done, you want to basically texture bake all these textures from your main model or, yes, main model to your logical parts that you split already. This is basically the next step. So in this video, you can see how I just loaded these slopes and I created another unwrapping RealityCapture, and I reproject it or texture bake to these slopes. Also, I did the same and repeatedly for all the other assets that were basically separated. It's pretty powerful because by using reprojection tool, you're also getting free normal map and free displacement map. So as you know, in the RealityCapture, just by hitting the high detail meshing, you are getting a lot of triangles. It could be even billions. It's not really that usable, having billions of triangles, but what you could do is you can texture bake from this amount of detail and just use it in a normal displacement map. So you have that fidelity, you have that precision from that billions and billions of triangles, but you are just using it in texture maps. So I just wanted to point it out. Maybe out of curiosity you may ask how many images were used in this particular data set. For station it was around 4,000 images with 42 laser scan positions, and for the metro car it was around 7,000 images and around 40 scan positions. Then of course, the next step could be the lighting and retouching. Since we were using a lot of diffuse light and also cross-polarization setup, we didn't need to do the lighting. But we did need to do some texture retouching because of some reflections. It sometimes happens. So since this asset was mostly an exercise for Mels, we were also doing the retouching for these. You can use any tool for that. This is a widely used workflow. I just want to show you how that could look like. So you are using basically like a Clone Stamp tool in Photoshop and just repairing the texture. Now, after everything is done, you just export it from RealityCapture in form of APX, OBJ, whatever you like, and yeah. Start with lighting. Start with compositing and also sequencer, or if you just want to go real time and play with whatever you want, you can use the full power of Unreal Engine with these assets. We are also generating roughness maps, by the way. So that's why it looks pretty good. So now what I want to showcase-- and that was one of the use cases why we were even doing that- is to prove that you won't see what is real and what is Unreal. [VIDEO PLAYBACK] - With traditional modeling techniques, it's really hard to deliver realism. You can provide realism in terms of models, but still, it's really hard to provide you with all the granularity that the object can have. Photogrammetry will provide you with the toolset to create a real digital replica with all the fine details that are in the subject so artists don't need to think about them. - And because it is photogrammetry, it's not like it's any metro station. It is the exact same metro station. And that is extremely valuable in virtual production because you can cut from one place to its own virtual replica, and it's really useful for reshoots, close ups, insert, additional shooting, or even VFX in post-production. - RealityCapture is, in my opinion, the ultimate tool, because with this it can create the ultimate fidelity of precision of assets and I don't think you will actually see any difference now what is real and what is Unreal. You can put your asset on LED volume, and in the end you won't see any difference. [END PLAYBACK] Yeah. So basically all the background that you've seen in these videos that we were showing was just virtual production. So we were just using that set we scanned to showcase that you won't even notice that it was not a physical scan. So yeah. I really hope you liked this presentation. I'm here to answer your questions. [APPLAUSE]