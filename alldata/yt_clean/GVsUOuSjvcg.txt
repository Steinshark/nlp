- For hundreds of years, analog computers were the most predicting eclipses, tides, Then, with the advent of digital computers took off. Now, virtually every But today, a perfect storm of for a resurgence of analog technology. This is an analog computer, and by connecting these I can program it to solve a whole range of differential equations. For example, this setup a damped mass oscillating on a spring. So on the oscilloscope, you of the mass over time. And I can vary the damping, or the spring constant, or the mass, and we can and duration of the oscillations change. Now what makes this an analog computer is that there are no Instead, there's actually up and down exactly The electrical circuitry is an analog for the physical problem, it just takes place much faster. Now, if I change the I can program this computer to solve other differential equations, like the Lorenz system, which is a basic model of Now the Lorenz system is of the first discovered examples of chaos. And here, you can see the Lorenz attractor with its beautiful butterfly shape. And on this analog computer, I can change the parameters and see their effects in real time. So these examples illustrate some of the advantages of analog computers. They are incredibly and they can complete a Plus, they don't take much power to do it. With a digital computer, if you wanna add two eight-bit numbers, you need around 50 transistors, whereas with an analog computer, you can add two currents, just by connecting two wires. With a digital computer you need on the order of 1,000 transistors all switching zeros and ones, whereas with an analog computer, you can pass a current through a resistor, and then the voltage across this resistor will be I times R. So effectively, you have multiplied two numbers together. But analog computers also For one thing, they are not general-purpose I mean, you're not gonna run And also, since the inputs I can't input exact values. So if I try to repeat I'm never going to get Plus, think about There's always gonna be some variation in the exact value of components, like resistors or capacitors. So as a general rule of thumb, you can expect about a 1% error. So when you think of analog computers, you can think powerful, but also single-purpose, And if those sound like deal-breakers, it's because they probably are. I think these are the major reasons why analog computers fell out of favor as soon as digital Now, here's why analog computers (computers beeping) It all starts with - [Narrator] A machine and to move objects. - AI isn't new. The term was coined back in 1956. In 1958, Cornell University psychologist, Frank Rosenblatt, built the perceptron, designed to mimic how So here's a basic model of how An individual neuron so its level of activation as a one or a zero. The input to one neuron is the output from a bunch other neurons, but the strength of these connections between neurons varies, so each one can be given Some connections are excitatory, so they have positive weights, while others are inhibitory, so they have negative weights. And the way to figure out whether a particular neuron fires, is to take the activation and multiply by its weight, and then add these all together. If their sum is greater than then the neuron fires, but if it's less than that, As input, Rosenblatt's arranged in a square grid, to capture a 20 by 20-pixel image. You can think of each with its activation being Although strictly speaking, the activation should we can let it take any All of these neurons are connected to a single output neuron, each via its own adjustable weight. So to see if the output neuron will fire, you multiply the activation and add them together. This is essentially a vector dot product. If the answer is larger than and if not, it doesn't. Now the goal of the perceptron was to reliably distinguish like a rectangle and a circle. For example, the output neuron could always fire when presented with a circle, but never when presented with a rectangle. To achieve this, the that is, shown a series and rectangles, and have its We can visualize the weights as an image, since there's a unique weight Initially, Rosenblatt set If the perceptron's output is correct, for example, here it's shown a rectangle and the output neuron doesn't fire, no change is made to the weights. But if it's wrong, then The algorithm for updating the weights is remarkably simple. Here, the output neuron didn't because it was shown a circle. So to modify the weights, you simply add the input If the output neuron like here, when shown a rectangle, well, then you subtract from the weights, and you keep doing this until the perceptron correctly identifies all the training images. It was shown that this so long as it's possible into distinct groups. (footsteps thumping) The perceptron was between different shapes, or between different letters. And according to Rosenblatt, it could even tell the He said the machine was capable of what amounts to original thought, and the media lapped it up. The &quot;New York Times&quot; called the perceptron &quot;the embryo of an electronic computer that the Navy expects will see, write, reproduce itself, and be conscious of its existence.&quot; - [Narrator] After training it's given new faces it has never seen, and is able to successfully It has learned. - In reality, the perceptron in what it could do. It could not, in fact, This and other critiques were raised in a book by MIT giants, And that led to a bust period for artificial neural It's known as the first AI winter. Rosenblatt did not survive this winter. He drowned while sailing in Chesapeake Bay on his 43rd birthday. (mellow upbeat music) - [Narrator] The NAV Lab modified so that researchers or computers can control the vehicle - [Derek] In the 1980s, when researchers at of the first self-driving cars. The vehicle was steered by an artificial neural It was similar to the perceptron, except it had a hidden between the input and output. As input, ALVINN received of the road ahead. Here, I'm showing them as 60 by 64 pixels. But each of these input via an adjustable weight to a These were each connected So to go from one layer of you perform a matrix multiplication: the input activation times the weights. The output neuron with determines the steering angle. To train the neural net, a human drove the vehicle, providing the correct steering angle for a given input image. All the weights in the through the training so that ALVINN's output of the human driver. The method for adjusting the weights is called backpropagation, which I won't go into here, but Welch Labs has a great series on this, which I'll link to in the description. Again, you can visualize the weights for the four hidden neurons as images. The weights are initially but as training progresses, the computer learns to pick You can see the road markings Simultaneously, the output onto the human steering angle. The computer drove the of around one or two kilometers per hour. It was limited by the speed at which the computer could Despite these advances, artificial neural networks still struggled with seemingly simple tasks, like telling apart cats and dogs. And no one knew whether hardware or software was the weak link. I mean, did we have a good we just needed more computer power? Or, did we have the wrong idea about how to make intelligence So artificial intelligence in the 1990s. By the mid 2000s, most AI researchers were But one researcher, Fei-Fei Li, thought maybe there was Maybe these artificial neural networks just needed more data to train on. So she planned to map out From 2006 to 2009, she created ImageNet, a database of 1.2 million which at the time, was the largest labeled image And from 2010 to 2017, ImageNet ran an annual contest: the ImageNet Large Scale where software programs and classify images. Images were classified into including 90 different dog breeds. A neural network competing would have an output each corresponding to a category of object that could appear in the image. If the image contains, then the output neuron should have the highest activation. Unsurprisingly, it turned One way to judge the performance of an AI is to see how often the five do not include the correct category. This is the so-called top-5 error rate. In 2010, the best performer of 28.2%, meaning that the correct answer was not In 2011, the error rate of a substantial improvement. But the next year, an artificial neural network from the University of blew away the competition with a top-5 error rate of just 16.4%. What set AlexNet apart The network consisted of eight layers, and in total, 500,000 neurons. To train AlexNet, 60 million weights and biases using the training database. Because of all the big processing a single image individual math operations. So training was computationally intensive. The team managed it by graphical processing units, which are traditionally used So they're specialized for The AlexNet paper is a blockbuster. It's now been cited over 100,000 times, and it identifies the as key to its success. It takes a lot of computation but the improvement in With others following their lead, the top-5 error rate on the ImageNet competition plummeted in the years that followed, That is better than human performance. The neural network that achieved this had 100 layers of neurons. So the future is clear: We will see ever increasing demand for ever larger neural networks. And this is a problem for several reasons: One is energy consumption. Training a neural network of electricity similar of three households. Another issue is the so-called Virtually every modern digital computer stores data in memory, and then accesses it as needed over a bus. When performing the huge by deep neural networks, most of the time and energy goes into fetching those weight values rather than actually doing the computation. And finally, there are the For decades, the number of transistors on a chip has been doubling but now the size of a transistor is approaching the size of an atom. So there are some fundamental to further miniaturization. So this is the perfect Digital computers are Meanwhile, neural networks and a lot of what they do boils down to a single task: matrix multiplication. Best of all, neural networks of digital computers. Whether the neural net the image contains a chicken, it doesn't really matter, So slight variability in components or conditions can be tolerated. (upbeat rock music) I went to an analog called Mythic AI. Here, they're creating analog And they demonstrated - Oh, there you go. See, it's getting you. Yeah. - The biggest use case is If your friend is in a different, they're at their house you can actually render each So it needs to really and then render it in the VR world. - So, hang on, is this - Yeah, this is a very This is depth estimation It's just taking this scene, and then it's doing a heat map. So if it's bright, it means it's close. And if it's far away, it makes it black. - [Derek] Now all these on digital computers, but here, the matrix multiplication in the analog domain. To make this possible, Mythic has repurposed Normally these are used as memory to store either a one or a zero. If you apply a large positive electrons tunnel up through and become trapped on the floating gate. Remove the voltage, and the electrons can for decades, preventing the And that's how you can store You can read out the stored value by applying a small voltage. If there are electrons no current flows, so that's a zero. If there aren't electrons, then current does flow, and that's a one. Now Mythic's idea is to use these cells not as on/off switches, They do this by putting a on each floating gate, The greater the number of electrons, the higher the resistance of the channel. When you later apply a small voltage, the current that flows But you can also think of this where conductance is just So a single flash cell can be used to multiply two values together, So to use this to run an well they first write all the as each cell's conductance. Then, they input the activation values as the voltage on the cells. And the resulting current is the product of voltage times conductance, which is activation times weight. The cells are wired together in such a way that the current from each completing the matrix multiplication. (light music) - So this is our first product. This can do 25 trillion - [Derek] 25 trillion. - Yep, 25 trillion math in this little chip here, burning about three watts of power. - [Derek] How does it - The newer digital from 25 to 100 trillion but they are big, thousand-dollar systems that are spitting out 50 - [Derek] Obviously this isn't like an apples apples comparison, right? - No, it's not apples to apples. I mean, training those algorithms, you need big hardware like this. You can just do all sorts but if you specifically and you wanna deploy 'em, You can imagine them in security cameras, autonomous systems, inspection equipment for manufacturing. Every time they make a Frito-Lay chip, they inspect it with a camera, and the bad Fritos get blown But they're using artificial intelligence to spot which Fritos are good and bad. - Some have proposed in smart home speakers, solely to listen for the wake They would use a lot less and reliably turn on the But you still have to deal - So for one of the popular networks, there would be 50 sequences of matrix multiplies that you're doing. Now, if you did that entirely by the time it gets to the output, it's just so distorted that you don't have any result at all. So you convert it from the analog domain, back to the digital domain, send it to the next processing block, and then you convert it into And that allows you to - You know, when Rosenblatt up his perceptron, he used a digital IBM computer. Finding it too slow, he built a custom analog computer, complete with variable resistors and little motors to drive them. Ultimately, his idea of neural networks turned out to be right. Maybe he was right about analog, too. Now, I can't say whether off the way digital did last century, but they do seem to be better suited to a lot of the tasks to perform today, which is a little bit funny because I always thought of digital as the optimal way of Everything from music to pictures, to video has all gone But maybe in a 100 years, we will look back on digital, not not as the end point but as a starting point. Our brains are digital in that a neuron either but they're also analog in that thinking takes place So maybe what we need to achieve true artificial intelligence, machines that think like (gentle music) Hey, I learned a lot much of it by playing with You know, trying things out for yourself is really the best way to learn, and you can do that with this Brilliant is a website and app that gets you thinking deeply by engaging you in problem-solving. They have a great course where you can test how It gives you an excellent intuition about how neural networks can and it also allows you to of good training data and hidden layers to understand why more sophisticated neural networks work better. What I love about Brilliant is it tests your knowledge as you go. The lessons are highly interactive, and they get progressively And if you get stuck, there For viewers of this video, Brilliant is offering the first 200 people 20% off an annual premium subscription. Just go to brilliant.org/veritasium. I will put that link So I wanna thank Brilliant and I wanna thank you for watching.