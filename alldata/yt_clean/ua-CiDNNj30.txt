Welcome to Data Science: An Introduction. do in this course is We are going to have of the field of Data Science. Now, some people things like: Data and think about piles of of that Science and think about people working not for me. I'm not really a technical person here's the important thing to know. While the technical aspects of Data Science the so much a technical discipline, but creative. that is because in Data Science you use tools math But you use those to work creatively than one way to solve a problem or answer Because the goal, no matter how you go about what makes Data Science unique, compared to to all of your data, even when it doesn't and paradigms you're trying to be much more you want to do that is because everything everything can give you additional understanding and so in this course what we are trying to Science and how you can use it and so now get ready to get going with Data Science. And we're going to begin this course by defining going to be doing it in kind of a funny way. the demand for data science. So, let's take in a few ways. I am going to give you some is that data science is coding, math, and working definition. But, if you want to be a definition. That data science is the analysis would fit into standard analytic approaches. science is inclusive analysis. It includes you have, in order to get the most insightful Now, you may say to yourself, &quot;Wait... that's show you a few things. First off, let's take the Sexiest Job of the 21st Century.&quot; And Review. So, this is an authoritative source that data science is sexy! Now, again, you think so.&quot; Oh yeah, it's sexy. And the reason has rare qualities, and second it has high The rare qualities are that data science takes and value in the data. Those are important, high demand. Well, the reason it's in high into what's going on around you and critically, a huge thing in business settings. Now, let Let's take a look at a few other sources. published a very well-known paper, and you that webpage, this is what's going to come this one, the executive summary. It's a PDF up, you will find this page. And let's take here, I'm going to zoom in on those. The first next few years for somewhere between 140 and So, this means actual practicing data scientists. as high is 1.5 million more data-savvy managers data in the United States. Now, that's people but have to understand it, who have to speak of this particular course, is to help people scientists learn to understand what they can to get there. Let's take a look at another URL for it and that will bring you to this people hired in 2014.&quot; And take a look at data mining, very closely related to data one in Australia, and Brazil, and Canada, and South Africa, and the United Arab Emirates, you need a little more, let's take a look year, 2016, and it's about the &quot;25 Best Jobs here, it's data scientist. And we can zoom to be 1,700 job openings, with a median base opportunities and job scores. So, if you want you can reach is that data science pays. And for instance, here's a list of the top ten News. We have physicians (or doctors), dentists, scientist to this list, using data from O'Reilly.com, third with an average total salary (not the compensation) of about $144,000 a year. That's all this? First off, we learn that there is we learn that there is a critical need for data scientists; and for Generalists, the can be done. And of course, excellent pay. a compelling career alternative and a way Back here in data science, we're going to by looking at something that's really well Diagram. Now if you want to, you can think of data science?&quot; Well, we're going to first up with this. And if you want to see the original what Drew said is that data science is made circles because it is the intersection that's or computer programming, or as he calls it: or mathematics, or quantitative abilities or intimate familiarity with a particular education, or science, or something like that. is data science. So it's the combination of knowledge. Now, let's say a little more about because it helps you gather and prepare the novel sources and is not necessarily ready formats. And so coding is important because the data from the sources to put it into your are important; for instance, there is statistical are R and Python. Two open-source free programming is general-purpose, but well adapted to data. too. The most common language there is SQL, for Structured Query Language, because that's line interface, or if you are on a Mac, people there is Bash, which actually stands for Bourne-again regex, or regular expressions. While there a small little field), it's sort of like super-powered for you to both find the data and reformat your analysis. Now, let's say a few things like a little bit of probability, some algebra, procedure). Those things are important. And is going to help you choose the appropriate data that you have. And probably even more problems when things don't go as expected. with new data in new ways, you are probably to understand the mechanics of what is going the third element of the data science Venn Think of it as expertise in the field that need to know about the goals of that field, that people come across. And it's important to be able to implement them well. Data science something. And your familiarity with a particular much easier and more impactful when you implement back to our Venn Diagram here just for a moment. intersections of two circles at a time. At right is traditional research. And on the &quot;the danger zone.&quot; Let me talk about each ML. Now, you think about machine learning or statistical programming and mathematics, these are referred to as &quot;black box&quot; models. necessarily have to know what it means or of crunch through it all and it will give but machine learning is considered slightly involve the particular applications in a specific This is where you have math or statistics intensive domain knowledge but without the with that because the data that you use in It comes in rows and columns, and is typically Doesn't mean your life is easy, because now in the methods and the designing of the project very heavy intellectual cognitive work, but finally, there is what Conway called, &quot;the of coding and domain knowledge, but without to happen, and that is probably true. On the what are called &quot;word counts,&quot; where you take and you count how many times a word appears very important things. And also, drawing maps and maybe even across time. You don't necessarily insightful and helpful. So, let's think about from here. First, is coding. You can have and business. So, you get the three things the people come from a programming background. statistics. And you can get statisticians That's less common, but it does happen. And science from a particular domain. And these code and do numbers. And they are the least data science. And so in sum, here is what up Data Science. Second, diverse skills and in data science. And third, there are many different things that need to happen. We'll next step in our data science introduction about the Data Science Pathway. So I like on a major project, you have got to do one In data science, you can take the various general categories. First, there are the steps data prep. Third, there's the actual modeling And there are several steps within each of First, let's talk about planning. The first define the goals of your project so you know you know when you are done. Second, you need data from several different sources; you might have different people. Which gets us to the so they can work together productively. If who is going to do what and how their work to state the obvious, you need to schedule and you can finish in a reasonable amount are taking like food prep and getting the you need to get the data. And it can from formats. You need to clean the data and, the part of any data science project. And that from a lot of different places. You also want it looks like, how many people are in each are like, what is associated with what. And means choosing variables to include, choosing to the data you need to do. And of course from one to the other. The third group is where you actually want to create the statistical analysis or you might do a neural network. model, you have to validate the model. You might do it really with a very small replication model. So, once you know that the model is much does it tell you? And then finally, you there may be variables you want to throw out; You may want to, again, transform some of easier to interpret and apply. And that gets And that's follow up. And once you have created Because it is usually work that is being done a third party. But you need to take the insights way with other people. You also need to deploy to accomplish something. So, for instance, you may be developing a recommendation engine might buy this.&quot; You need to actually stick way that you expected it to. Then you need times, the data that you worked on is not change when you get out in the real world to see how well your model is working. And the assets, document what you have, and make the analysis or develop off of it in the future. consider the data science pathway. And in First, data science isn't just a technical planning and presenting and implementing are knowing how it works in a particular field, skills matter as well. And then, as you got things to do. And if you go one step at a you will ultimately be more productive in our definition of data science by looking The way that different people can contribute thing, and it's nice to be able to say that a single goal. So, let's talk about some of they contribute to the projects. First off, people who focus on the back end hardware. that runs them. This is what makes data science software developers, or database administrators. of the work. Next, you can also have people who focus on computer science and mathematics, as a way of processing very large amounts data products. So, a thing that tells you might know these friends,&quot; or provides ways and those often involve a huge amount of very researchers; these are people who focus on physics, or genetics, or whatever. And these and they can use some of the procedures and people like the big data researchers, but in the data science realm, you will find analysts. tasks of running a business. So for instance, or they might pull data from a SQL database. good for business. So, analysts are key to may not be, exactly be Data Science proper, with is going to be pretty structured. Nevertheless, And then, speaking of business. You have the who organize and run businesses. These people questions that can be answered with the data. and the efforts and the resources of others. coding, they must speak data; they must know how to implement it. You can also have entrepreneurs. starting their own little social network, needs data and business skills. And truthfully, the way. Usually because they are doing it have in data science something known as &quot;the who can do everything at an expert level. they may not actually exist. I will have more we can sum up what we got out of this video is diverse. There's a lot of different people for their work, and they bring in different approaches. Also, they tend to work in very in a very different place from a business from an academic researcher. But, all of them and make it a richer field. The last thing where I am trying to define data science, idea here is that data science has many different experts in each one of them. Now, you have, Also, you have what feels like design, or And the question, of course, is: &quot;who can things at the level that we need?&quot; Well, that's it before), it's the unicorn. And just like creature with magical abilities. In data science, Data Scientist with universal abilities. The there are really no unicorns (animals), and data science. Really, there are just people. projects even though we don't have this one So let's take a hypothetical case, just for people. Here is my fictional person Otto, good coding, but has limited analytic or statistical abilities... So, here we have five things to work, they all have to happen at least, take his coding ability, he is almost there. he can do that. And then, business, eh, alright. see here is, in only one of these five areas hand, let's pair him up with somebody else. business training, has good tech skills, but on the same thing that we saw, there is coding, not so much. Business, good. And projects, we can make a team. So let's take our two put together their abilities. Now, I actually to accommodate the both of them. But our criterion in order to do the project competently. And past eight. Statistics is past eight. Graphics projects, they are too. So when we combine that we need for everything. Or to put it by team, and that makes it possible to do usually can't do data science on your own. people need people, and in data science you and make collective unicorns, so you can get and you can get the things done that you want. data science, it can be helpful to look at Probably the most informative is with Big often confused. It makes me think of situations but not the same. Like we have here in the problem stems from the fact that data science with them. So, for instance, Venn number one already. We have three circles and we have expertise, that put together get data science. is for Big Data. It also has three circles. velocity of data, and the extreme variety you get Big Data. Now, we can also combine we call Big Data and Data Science. This time left and Data Science on the right. And the Science, which actually is a real term. But, it kind of helps to look at how you can have looking at Big Data without Data Science. the volume or velocity or variety of data So, we are just looking at the left side of only works if you have Big Data without all velocity, and variety for it to count as Big fit into a standard machine is probably Big here of things that might count as Big Data, learning, where you can have very large data very much domain expertise, so that may not an enormous amount of data and it's actually much sophistication in terms of quantitative not data science. On the other hand, to do at least two skills. You are going to need to have some sort of quantitative skills as Data? That's the right side of this diagram. talking about data with just one of the three or variety, but singly. So for instance, genetics comes in very set structure and it tends to volume and it is a very challenging thing but it may or may not count as Big Data. Similarly, coming in very quickly, but you are not necessarily in it. That is a lot of velocity, and it is Science, the full skill set, but it may not where you have enormous variety in the data are coming in. Again, very difficult to deal may or may not count as Big Data, depending Now, if you want to combine the two, we can we are looking right here at the middle. This velocity, and variety in your data and truthfully, to need the full Data Science skill set. You and math, and you are going to have to have variety you are dealing with, but taken all in sum, here is what we get. Big Data is not Now, there is common ground, and a lot of data science and vice versa, but they are there is the shared middle ground of Big Data Another important contrast you can make in it with coding or computer programming. Now, machines and you are trying to talk to that you can think of coding as just giving task lot like a recipe when you're cooking. You and then maybe you have if/then logic, and simple example, if you are programming in in quotes, &quot;Hello, world!&quot; will put the words it some instructions and it gave you some and data gets a little more complicated. So, you take a book or a whole collection of books, there are in there. Now, this is a conceptually math and statistics are not vital. But to in the face of variability and uncertainty you need data science. It might help to compare trades. So for instance, there are tools for there are tools that are specific for data from the IEEE of the top ten programming languages down to Shell. And some of these are also and R and SQL are used for data science, but So, let's, in fact, take a look at a different and you see that things move around a little is there, but for me what is the most interesting which would never be considered programming, tool for data science. And that is one of computer programming with data science. In equal to coding. They are different things. tools and they share some practices specifically is one very big difference in that statistics, between general purpose programming and data science and we are contrasting with some fields, and think they are the same thing is data there is a lot in common, but we can talk each. And we also get into the issue of definitionalism define it differently, even when there is helps to take a look at some of the things here about statistics. Put a little circle borrow a term from Steven J. Gould, we can So, you think of them as separate fields that to do with each other. But, you know, that if we go back to the Data Science Venn Diagram, in the top corner. So, now what do we do? sense to say these are totally separate areas, they share procedures, maybe data science like this. But, if data science were just it would follow that all data scientists would that's just not so. Say, for instance, we superstars in the field. We go to a rather 7 Most Powerful Data Scientists&quot; from Forbes.com. URL. There's actually more than seven people, Let's check their degrees, see what their people on this list, we have five degrees engineering, and one each in biology, economics, And so that tells us, of course, these major statisticians. Only one of them has formal next question. Where do these two fields, they seem like they should have a lot in common, we can look at the training. Most data scientists Also, in practice, things like machine learning are not shared, generally, with most of statistics. then there is the important issue of context. settings than statisticians. Specifically, settings where they are trying to get recommendation will make them money. So, maybe instead of we can think of it more as these two fields but they do different things in different they overlap, they have analysis in common distinct. So, in sum: what we can say here data and they analyze it. But the people in and they tend to function with different goals to be conceptually distinct fields despite grasp on data science, there is one more contrast data science and business intelligence, or is data in real life; it's very, very applied internal operations, on market competitors, as opposed to just sitting in the bar and science is involved with this, except, you using apps that already exist. And the statistics they tend to be counts and percentages and is simple; it just does its one job there the focus in business intelligence is on domain It's simple, it's effective and it provides with business intelligence is what are called like this; it is a collection of charts and quick overview of what is going on in your may, let's say, look down their nose upon very well designed and you can learn a huge information from dashboards. So really, where the connection between data science and business to BI in terms of setting it up. Identifying framework for something like a dashboard or science can be used to extend it. Data science and the easy data, to get the questions that require really sometimes data that is hard is an interesting interaction here that goes can learn a lot about design from good business encourage anybody in data science to look In sum: business intelligence, or BI, is very the data and sets up the form for business a lot about usability and accessibility from worth taking a close look. Data science has it is important to consider some ethical issues, in your data science projects. And for that who gave us the Hippocratic Oath of Do No the important ethical issues, very briefly, privacy. That data tells you a lot about people If you have private information about people, their addresses, their credit scores, their and you shouldn't share that information unless one of the reasons this presents a special see later, a lot of the sources that are used If you scrape data from a website or from to do that. But it was originally created is something that really falls upon the analyst is anonymity. One of the interesting things people in data. If you have a little bit of four different points in time, you have about You look at things like HIPAA, that's the Act. Before HIPAA, it was really easy to identify has become much more difficult to identify for really people's well-being. And then also, client, a company, and they give you their You may know who the people are, they are may not be there, but major efforts to make is even if you do know who they are, that of the data. Next, there is the issue about Now, just because something is on the web, Scraping data from websites is a very common You can get data from web pages, from PDFs, number of things. But, again the assumption use it is not true. You always need to check for you to access that particular data. Next, and the idea here is that when you go through and prepare for analysis, you have created people and you have to be concerned about especially if the data is not anonymous and an additional burden to place on the analyst the data is safe and cannot be broken into things like a person who is on their project drive. You have to find ways to make sure lot of possibilities, it's tricky, but it Now, two other things that come up in terms in these conversations. Number one is potential or the formulas that are used in data science and the data that they get. And so, the idea something that is associated with, for instance, you might unintentionally be building in those title nine, you are not supposed to. You might being aware of it, and an algorithm has this can place confidence in it without realizing that may happen in real life. Another issue analyses are limited simplifications. They because of this, you still need humans in The problem is when people run an algorithm and they say, &quot;this must be true,&quot; and treat truth, when in fact, if the data were biased if the sampling was not representative, you wrong path with too much confidence in your order when doing data science work. In sum: also has significant risks involved in the can't be neutral, that you have to look at preferences, prejudices, and biases of the is that no matter what, good judgment is always project. Data Science is a field that is strongly In this section of videos, we're going to are used in data science. Now, just as a quick of technical and that can cause some people a non-technical overview. The technical hands really important to remember that tech is or the ability to find meaning in your data, And so, we want to focus primarily on insight further that goal. Now, there's a few general with an overview for each of these. The first how to get the data that goes into data science, coding. That again is computer programming and analyze the data. After that, a tiny bit data science methods that really form the the statistical methods that are frequently as applied to data science. And then there of methods for finding clusters in the data, outcomes. And even across these five things, they are basically still friendly. Really, of the overviews. In sum: we need to remember science is greater than tech, it is more than while important to data science is still simply in discussing data science methods is to look that is used in data science. You can think go into your analyses. Now, you have got a in data science. You can use existing data, can scrape web data, or you can make data. in a non-technical manner. For right now, This is data that already is at hand and it a company, it might be your company records. many governments and many scientific organizations then there is also third party data. This but it exists and it is very easy to plug that stands for Application Programming Interface, computer applications to communicate directly computer programs. It is the most common way about it is it allows you to import that data you are using to analyze the data. Next is to use data that is on the web, but they don't is usually data that's in HTML web tables either with using specialized applications language, like R or Python, and write the is to make data. And this lets you get exactly you can get what you need. You can do something you can do experiments. There is a lot of training in terms of how to gather quality because no matter what method you use for this one little aphorism you may have heard of GIGO: that actually stands for &quot;Garbage bad data that you are feeding into your system, any real insights out of it. Consequently, or methods for measuring and the meaning - exactly ways you can do this. For instance, you can about KPIs, which means Key Performance Indicators, which is a way of describing the goals that also talk about, in a measurement sense, classification in a little more detail in a later movie. data sourcing is important because you need The nice thing is there's many possible methods, for data science. But no matter what you do, meaning of the data so you can get the most next step we need to talk about in data science you a very brief non-technical overview of you are going to get in there and you are domain and make the data jump when you need about the Data Science Venn Diagram at the And while we often think about sort of people it is more important to remember when we talk what we are really talking about here is any in the ways that you need to perform the procedures out of your data. Now, there are three very here on datalab. The first is apps; these working with data. The second is data; or formats for web data, I will mention those languages that give you full control over with the data. Let's take a look at each one spreadsheets, like Excel or Google Sheets. a majority of the world. There are specialized or SPSS, it is a very common statistical package and one of my favorites, JASP, which is a I think is a lot easier to use and replicate choices. Now, in terms of web data, it is and XML, and JSON, and other formats that those are the things that you are going to when you get your data. And then there are most common, along with Python; general purpose data use. There's SQL, the structured query like C, C++, and Java, which are used more there is Bash, the most common command line will talk about all of these in other courses are just tools. They are only one part of to the end, and the end, the goal is insight. and then simply choose the tools that help most important thing. So, in sum, here's a Remember your questions need to drive the will just mention that a few tools is usually and R. And then, the most important thing and even your data to match the goal, so you data. The next step in our discussion of data to give a very brief overview of the math thing to remember is that math really forms If you go back to the Data Science Venn Diagram, but really it's math and stats, or quantitative math part right here. And probably the most to do what you need to do? Or to put it another you have got a computer to do it? Well, I to rely on just the computer, but it is helpful Here they are: number one, you need to know your question, you have your data and you make an informed choice. That's not terribly when things don't work right. Sometimes you you can get a negative adjusted R2; that's know the mathematics that go into calculating impossible can work. Or, you are trying to and you get a rotation that won't convert. algorithm that's happening, and why that won't interestingly, some procedures, some math firing up the computer. And I'll show you that can be the case. Now, fundamentally there data science as, for instance, chemistry is grammar is to writing. The idea here is that any chemistry, but if you know some chemistry dancer without know kinesiology, but it is writer without having an explicit knowledge difference. The same thing is true of data some of the foundational information. So, you need for data science? Well, there's a you need some elementary algebra. That is, do some linear or matrix algebra because that And you can also have systems of linear equations all at once. It is a tricky thing to do, in is actually easier to do by hand sometimes. You can get some big O, which has to do with sort of how fast it works. Probability theory is a way of getting what is called a posterior for answering some fundamental questions in can help you make informed choices when planning help you find the problems and fix them when to look under the hood that makes a difference. like systems of linear equations, that can you can do with a computer. So, you can save ahead more quickly toward your goal of insight. and its methods without a little bit of statistics. overview here of how things work in data science. an attempt to find order in chaos, find patterns to see the forest and the trees. Now, let's recently had math and stats here in the top about stats, in particular. What you are trying You can have exploratory graphics, because to see things. You can have exploratory statistics, can have descriptive statistics, which are about when they took a statistics class in inference. I've got smoke here because you movement by looking at patterns in smoke. information from samples and infer something one source to another. One common version version is estimations, sometimes called Confidence but all of these let you go beyond the data one interesting thing about statistics is some of the details and arranging things just like feature selection and that's picking and there are problems that can come up that of those in later videos. There's also the model you have to see if it is actually accurate. have a holdout sample and do that, or you choice of estimators that you use; how you in your model. And then there's ways of assessing these are issues that I'll address briefly greater length. Now, I do want to mention this &quot;beware the trolls.&quot; There are people do things exactly the way they say to do it, data is junk and you've lost all your time. here is don't listen to that. You can make to go ahead and do an analysis that is still things to think about in this is this wonderful it says, &quot;All models or all statistical models question isn't whether you're technically purity, but whether you have something that Box. And I like to think of it basically as yourself&quot; flag, and just take pride in what are people who may be criticizing it. Go ahead, So, in sum: statistics allow you to explore things about the population. There is a lot But no matter what you do, the goal is useful will find something meaningful and useful and projects. Let's finish our data science of Machine Learning. Now, I've got to admit people start thinking something like, &quot;the world.&quot; That's not what it is. Instead, let's and in the intersection at the top between it's commonly called, just ML. The goal of space so you can, for instance, you can take here), and then you can reduce the dimensionality. set and try to find the most essential parts to find clusters within the data; like goes You can also look for anomalies or unusual if we go back to categories again, I talked logistic regression or k-nearest neighbors, or Decision Trees or SVM, which is Support Any of those will help you find the patterns similar cases next to each other, and get about these groups. Also, a major element going to point your way down the road. The is linear regression, multiple regression. used for modeling count or frequency data. where you create several models and you take put them together to get an overall more reliable these in a little more detail in later courses, know that these things exist, and that's what So, in sum: machine learning can be used to outcomes. And there's a lot of choices, many as I said with statistics, and I'll also say the goal is not that &quot;I'm going to do an artificial useful insight into your data. Machine learning helps you get that insight that you need. the role in data science of technical things. to the practice, and the first thing I want idea here is that you want to be able to lead to tell a data-driven story, and that's the science. Now, another way to think about this you're trying to do is solve for value. You're trying to solve for value. The trouble is but they need to remember that analysis is to think of it this way: that analysis times that's multiplicative, not additive, and so to, analysis times story equals value. Well, zero value because, as you recall, anything go back to this and say what we really want that we can maximize the value that results the overall goal here. The analysis, the tools, that goal. So, let's talk about goals. For are trying to accomplish something that's or the explanation you give about your project for a client that has a specific question a professional responsibility to answer those know whether you said yes or no and they know here is the fact the client isn't you and here, simply covering your face doesn't make a few psychological abstractions. You have about being vain, I'm talking about the idea understand what you know. That's not true; the first place. And so you have to put it they understand, and you're going to have do that. Also, there's the idea of false consensus; And again, that's not true, otherwise, they that they are going to come from a different and interpretation. You're going to have to is the idea about anchoring. When you give as an anchor, and then they adjust away from over on their heads, watch out for giving you absolutely need to. But most importantly, and you, you need to have clarity and explain about the answers. When you are explaining to start in a very simple procedure: state your answer to that question, and if you need top to bottom, so you're trying to make it the answer is, and make it really easy to how you did this all. Most of the time it just want to know what the answer is and that terms of discussing processes or the technical something to keep in mind. The process here breaking something apart. This, by the way, individual component. Analysis means to take exercise in simplification. You're taking of the data, and you're boiling it down and the needs of your client. Now, let's go to here, who said, &quot;Everything should be made true in presenting your analysis. Or, if you Ludwig Mies van der Rohe, who said, &quot;Less originally said that, but Mies van der Rohe of putting a principle that comes from my they talk about being minimally sufficient. If you're in commerce you know about a minimal within analysis here, the minimal viable analysis. a presentation, more charts, less text, great. that doesn't need to be in there. Generally, are hard to read. And then, one more time again. Charts, tables can usually carry the here. I'm going to give a very famous dataset stairs at Berkeley, but it gives the idea off and distant. Here's the data; this is over 40 years ago. The idea is that men and at the University of California Berkeley. men who applied were admitted, that's their of women were admitted when they applied. it actually led to a lawsuit, it was a major was find out, &quot;well which programs are responsible set of results. If you break the applications A through F), six different programs. What male applicants on the left female applicants women actually got accepted at a higher rate, true for D, and the same is true for F. And and it is something that requires explanation. is known as Simpson's Paradox. But here is department level. And in fact, as we saw in bias in favor of women. And the problem is programs with lower acceptance rates. Now, &quot;nothing is going on, nothing to complain the story a little bit early. There are other a data-driven story, this is stuff that you want to ask, &quot;why do the programs vary in rates differ from one program to the other? And you might want to look at things like the promotional strategies, how they advertise look at the kinds of prior education the students to look at funding level for each of the programs. more questions, maybe some more answers, and of this to provide a comprehensive overview say this: stories give value to data analysis. sure that you are addressing your client's' principle here is be minimally sufficient. you need to, but otherwise be concise and discussing data science and communicating that can be used productively to accomplish segue here, you look at a game controller. object, but remember: game controllers exist the game and to do it as effectively as possible. Same way data is for doing. Now, that's a figures. This is William James, the father philosophy. And he has this wonderful quote, always for the sake of my doing.&quot; And the your data is for the sake of your doing. So, insight in how you should proceed. What you one of my other favorite cultural heroes, said, &quot;We're lost, but we're making good time.&quot; not make up for lack of direction. You need reach the particular goal. And your analysis your analysis, you're going to try to point The goal is usually to direct some kind of And that the analysis should be able to guide you want to do is, you want to be able to next steps; tell them what they need to do those recommendations with the data and your tell them exactly what they need to do. Make within their range of capability. And that Now, that being said, there is one really here, and that's the difference between correlation your data gives you correlation; you know client doesn't simply want to know what's something. Because if they are going to do to produce a particular result. So, really, is what you have in the data, to the causation, a few ways to do that. One is experimental trials. Now, that's theoretically the simplest in the real world. There are quasi-experiments, of methods. They use non-randomized data, ways to get an estimate of causal inference. this is research-based theory and domain-specific get to rely on your client's information. especially if they have greater domain expertise are the social factors that affect your data. We've looked at it lots of times. It has got a fourth circle to this Venn diagram, and social understanding is also important, critical that idea, and I do think that it's important out. There are a few kinds of social understanding. You want to make sure that your recommendations Also, that your recommendations are consistent is what we do,&quot; but, &quot;This is really who we context, sort of the competitive environment working in. As well as the social context; but even more often within the organization. within the client's organization. And you as you can to make it so that your recommendations in sum: data science is goal focused, and client you need to give specific next steps from the data. And in doing so, be aware of that gives you the best opportunity of getting When you're working in data science and trying graphics can be an enormously helpful tool. a picture for the benefit of your client. can be a couple of different goals; it depends There's the general category of exploratory as the analyst. And for exploratory graphics, you get very simple graphics. This is a base more sophisticated and this is done in ggplot2. histograms, or you can make it a different apart into small multiples. But in each case, analyst understanding the data. These are very well-labeled, and they are usually for as a result of that. On the other hand, presentation client, those need clarity and they need a of those characteristics very briefly. Clarity can go wrong in graphics. Number one is color. or false dimensions are nearly always a distraction. is interaction. We think of interactive graphics you run the risk of people getting distracted with it. Going, like, &quot;Ooh, I press here it So actually, it may be important to not have of animation. Flat, static graphics can often distractions in them. Let me give you a quick is a chart that I made. I made it in Excel, I've seen in graphics submitted to me when here I have seen in real life, just not necessarily a little bit, so we can see the full badness on here. We've got a scale here that starts cover the range of the data. We've got this access lines on the walls. We come down here; order, instead of the more logical higher represented as cones, which are difficult by the colors and the textures. You know, grad degrees doesn't even make it to the floor is cut off at the top at 28%. This, by the this kind of stuff and it drives me crazy. exact same data, this is it right here. It it's as clean as possible. And this is better it communicates clearly. There's no distractions, the point across so much faster. And I can previously about salaries for incomes. I have If I want to draw attention to it, I have I can put a number next to it to explain it. going on. We don't even have to get fancy. note and I drew a bar chart of some real data as well, that there is something terribly about creating narrative flow in your presentation charts from my most cited academic paper, of Empirical Research on the Psychological it as mediation for juvenile crimes, mostly really it's about fourteen bar charts with you can see there's a flow. The charts are the criminal justice system was fair. The bars on the right are offenders. And for each in restorative justice, so more victim-offender the right are people who went through standard usually means plea bargaining. Anyhow, it's restorative justice bar is higher; people also felt that they had an opportunity to might think it's fair. They also felt the In fact, if you go to court on the offenders, the offenders themselves making the judgment. And again, this is actually a simple thing difference. In fact, one of the reasons there of court preceding, the offender very rarely to qualify this a little bit because a bunch no injuries or accidents. Well, when we take can go to whether a person is satisfied with restorative justice. Whether the victim is are a little bit different. And whether they over a two to one difference. And then finally you see a big difference there. And so what very very simple to read, and they kind of and then detailing it a little bit more. There's there's nothing animated, there's nothing It's easy, but it follows a story and it tells be your major goal with the presentation graphics. for presenting, are not the same as the graphics needs and they have different goals. But no graphics and be focused in what you're trying that gives different level of perspective a client's questions and to give them the confidence in your analysis. The final element to talk about is reproducible research. And to be able to play that song again. And the rarely &quot;one and done;&quot; rather they tend to and they tend to adapt to these circumstances things here, probably, if you want to summarize There's a few reasons for this. You may have own analyses. You may be doing another project studies. More likely you'll have to hand it they're going to have to be able to understand issue in both scientific and economic research show that you did things in a responsible that's for clients funding agencies, regulators, Now, you may be familiar with the concept with the concept of open data science; that's just let you know there is something called And it meets three times a year in different to open data science using both open data, around them. One thing that can make this Science Framework, which is at OSF.io. It's with an annotation on how you got through the research transparent, which is what we the Association for Psychological Science practices, where they are strongly encouraging permissible and to absolutely share their a way of getting rigorous intellectual honesty of this is to archive your data, make that And what you want to do here is, you want totally raw before you did anything with it your final clean dataset. Along with that, used in the process and analyzed the data. or Python, that's really simple. If you used files, and then that can be done that way. liberally and explain yourself. Now, part because you are not just this lone person with other people and you need to explain to explain the choices, the consequences of and try it over again. This also works into You want to do a few things here. Number one; formats like a CSV or Comma Separated Values If you stored it in the proprietary SPSS.sav somebody tries to use it later and they can't place all of your files in a secure, accessible best choices. And then the code, you may want package like Packrat for R or Virtual Environment packages that you use; that there are always get updated and it gets broken. This is a have will always work. Overall, you can think and a neat way to do that is to put your narrative lab book or you can also do digital books. using Python, is Jupyter with a &quot;y&quot; there notebooks. So, here's a screenshot of one you have text, you have the graphics. If you something called RMarkdown. Which works in Markdown and you can annotate it. You can And so for instance, here's an R analysis left and you see the markdown version on the little bit of code here, this title and this displayed as this formatted heading, as this R output right there. It's a great way to have the option of uploading the document online document that can be made accessible if you want to go see it, you can go to this to let you write that one down yourself. But, your work and archive the information in a choices, say what you did, show how you did so it will work in other situations for other how you do it, make sure you share your narrative can see that your conclusions are justifiable, several times when talking about data science, that it's important to give people next steps. If you're wondering what to do after having I can give you a few ideas. Number one, maybe in R or Python; we have courses for those. one of the most important things that you and maybe some math that goes along with it. learning. All of these will get you up and can also try looking at data sourcing, finding no matter what happens try to keep it in context. to marketing, and sports, and health, and number of other things. And we will have courses those. You may also want to start getting One of the best conferences that you can go times a year around the globe. There's also times a year around the world. Then there's or tapestryconference.com, which is about a one-day conference about data stories that sourcing applications that's available for with actual data, a great choice is to go competitions, which actually have cash rewards. with there to find out how they work and compare once you are feeling comfortable with that, some service; datakind.org is the premier service. They do major projects around the things you can do; there's an annual event be sponsoring twice-a-year data charrettes, Utah area to work with the local nonprofits you to remember this one thing: data science that everybody needs to learn to do in some data is a fundamental ability and everybody data intelligently and sensitively. Or, to Thanks so much for joining me in this introductory forward to seeing you in the other courses I'm Barton Poulson and in this course, we're for Data Needed. The idea here is that no So, instead of leaving it at that we're going measuring and evaluating data and methods for creating new, custom data. Take those same time, we'll do all of this still at an because the technical hands-on stuff will let's talk data. For data sourcing, the first And within that category, we're going to talk need to know what your target is if you want particular reasons for this. First off, data do something as opposed to simply understand academic practitioner. Also, your goal needs the goals can guide your effort. So, you want so you know when you get there. Also, goals can prevent frustration; they know what you're to get there. And finally, the goals and the because they help you use your time well. can move ahead with something, and that makes more productive. And when we talk about this success in your particular project or domain. can include things like sales, or click-through include scores on tests; it can include graduation include things like housing and jobs. In research, that you're to better understand. So, whatever for success and you're going to need to know metrics or ways of measuring. Now again, there are business metrics, there are key performance (that's an acronym), and there's also the about each of those for just a second now. If you're in the commercial world there are obvious one is sales revenue; are you making are you getting sales. Also, there's the issue potential customers because that, then, in also the issue of customer value or lifetime of customers, but they all have a lot of revenue overall profitability of your current system. do with, you know, losing and gaining new any of these are potential ways for defining metrics, there are others, but these are some something called a key performance indicator he's got a few ways of describing them, he Number one should be nonfinancial, not just be associated with it or that measures the should be timely, for instance, weekly, daily, should have a CEO focus, so the senior management decisions that affect how the organization everybody in the organization, everybody knows They should be team-based, so teams can take of the KPIs. They should have significant should affect more than one important outcome, or improved manufacturing time and fewer defects. side, that means there's fewer possibilities people for sort of exploiting the system. for Specific, Measurable, Assignable to a can actually do it with the resources you when it can get done). So, whenever you form of these criteria and that's a way of saying for the success of our organization. Now, goals, multiple possible endpoints. And the it's easy to focus on one goal if you're just trying to maximize graduation rate. There's difficult when you have to focus on many things these goals may conflict. The things that And so when that happens, you actually need of optimization, you need to optimize. And have enough data; you can do mathematical efforts to pursue one goal and the other goal summary and let me finish with this. In sum, awareness of how well your organization is goals. There are many different methods available towards those things. The trick, however, reach multiple goals simultaneously, which When talking about data sourcing and measurement, accuracy of your measurements. The idea here all your ideas; you don't want to waste effort. fashion is to make a classification table. about, for instance, positive results, negative at the top here. The middle two columns here your house is on fire, or whether a sale occurs, So, that's whether a particular thing is actually the test or the indicator suggests that the have these combinations of true positives; really is, and false positives; where the then below that true negatives, where the and then false negatives, where the test says fact the event occurring. And then you start of events present or absent, then the row table what you get is four kinds of accuracy, accuracy using different standards. And they positive predictive value, and negative predictive of them works. Sensitivity can be expressed ring? You want that to happen. And so, that's and dividing that by the total number of alarms. and the event present means there's a fire; a fire. Specificity, on the other hand, is a fire, does the alarm stay quiet? This is negatives to total absent events, where there's that's what you want. Now, those are looking rows. So, the first one there is positive and we flip around the order a little bit. a fire? So, now you're looking at the true of positives. Total number of positives is because there was a fire. And negative predictive does that in fact mean that there is no fire? and dividing it by total negatives, the time to maximize that so the true negatives account want the true positives to account for all numbers on all of these going from zero percent one as much as you can. So, in sum, from these a different focus for each one. But, the same positives and true negatives and avoid the this is one of way of putting numbers on, Now data sourcing may seem like a very quantitative measurement. But, I want measure one important of measurement. The idea here really, is that own goals, and they're going their own ways. that don't always coincide with each other, instance, when you're trying to define your want to look at things like, for instance, model, the way they conduct their business, its identity and its reason to be. And if to their business model, that can actually and people tend to get freaked out in that there may be laws, policies, and common practices, may limit the ways the goals can be met. Now, idea is you can'tjust do anything you want, you make your recommendations, maybe you'll still behaving legally and ethically, but Next, is the environment. And the idea here that company here is trying to reach a goal, there, but probably even more significantly This is really a recognition of office politics. based on your analysis, you need to understand into the office and things are going to further of another. And in order for your recommendations play out well in the office. That's something your recommendations. Finally, there's the people is that any reward system, any reward will generally game the system. This happens you need to get at least 80 percent, or you their numbers appear to be eighty percent. executive compensation systems, it looks a it happens in an enormous number of situations, exploitation and gaming. Now, don't think, can still do really wonderful assessment, these particular issues and be sensitive to as you make your recommendations. So, in sum, the way you meet those goals. There are limits goals and how, really, what the goal should to reach those goals please be sensitive to will adapt their behavior to meet the goals. likely to be implemented the way you meant can happen with your goals. When it comes thing is to get data. But the easiest way existing data. Think of it as going to the right there at hand. Now, there's a few different you can get open data, and you can get third-party proprietary, public, and purchased data; the about each of these a little bit more. So, in your organization. What's nice about that, there and the format may be appropriate for you are using. If you're fortunate, there's it's in-house people just kind of throw it And there's the issue of quality control. you need to pay attention with in-house, because under which people gathered the data and how There's also an issue of restrictions; there you may not be allowed to use, or you may the results with other people. So, these are going to use in-house data, in terms of how projects. Specifically, there are a few pros easy, and free. Hopefully it's standardized; this study is still there. And you might have for you to do an individual level analysis. simply may not exist, maybe it's just not and of course, the quality may be uncertain. to pay more attention to when you're using data like going to the library and getting available, consists of things like government from a number of sources. Let me show you so you know where they are and that they exist. the US. That is the, as it says right here, Or, you may have a state level one. For instance, a great source of more regional information. the European Union open data portal. And then so the UN has unicef.org/statistics for their Health Organization has the global health are private organizations that work in the Center, which shares a lot of its data sets to use APIs to access a huge amount of the time span. And then two of the mother loads, data which is a wonderful thing. And then datasets. So, if you needed a data set that place that you would go to get it. Now, there's data. First, is that you can get very valuable to gather and to process. And you can get groups of people and so on. And often, the There are, however, a few cons. Sometimes you only get people who have internet access, the meaning of the data is not clear or it A potential problem is that sometimes you are doing proprietary research, well, it's can create a crimp with some of your clients. and confidentiality and in public data that there and you are going to have to work at option is to use data from a third-party, DaaS. You can also call them data brokers. give you an enormous amount of data on many time and effort, by actually doing some of things like consumer behaviors and preferences, do marketing identity and finances, there's brokers around, here's a few of them. Acxiom data. There's also Nielsen which provides there's another organization Datasift, that's wide range of choices, but these are some data brokers, there's some pros and there's save you a lot of time and effort. It can can be hard to get from open data. Open data give you information about specific consumers. about things like credit scores and marital or smokes. Now, the con is this, number 1 a huge service; it provides a lot of benefit need to validate it, you still need to double and that it works in with what you want. And use of third-party data is distasteful to as you're making your choices. So, in sum, obviously data science needs data and there's and Public and Purchased. But no matter what to quality and to the meaning and the usability projects. When it comes to data sourcing, what are called APIs. Now, I like to think mermaids. If you're familiar with the love says, &quot;I have heard the mermaids singing, to adapt that to say, &quot;APIs have heard apps Now, more specifically when we talk about called Application Programming Interface, to talk to each other. Its most important you to get web data. It allows your program the data, bring it back in almost as though thing. Now, the most common version of APIs stands for Representational State Transfer. the world wide web and it allows you to access transfer protocol. They, you know, run the data that you usually get its in JSON format, The nice thing about that is that's human Then you can take that information and you the nice thing about REST APIs is that they're any programming language can call a REST API, it needs to with it. Now, there are a few first is what are called Social APIs; these So, for instance, the most common one is Facebook; a big one and FourSquare as well and then popular ones. And then there are also what visual data, so for instance, Google Maps that accesses YouTube on a particular website information. Pinterest for photos, and Flickr common APIs and you can program your computer and sites and integrate it into your own website there's a few different ways you can do this. language, you can do it in Python, also you line interface, and there's a ton of other an API one way or another. Now, I'd like to to open up a script in RStudio and then I'm from a webpage. Let me go to RStudio and show in RStudio that allows me to do some data a package called JSON Lite, I'm going to load couple of websites. I'm going to getting historical to be getting it from Ergast.com. Now, if to my browser right now. And this is what so what you're doing for an API, is you're address it includes the information you want. And if I want to get information about 1957 I can skip over to that for a second, and here, but it is all labeled and it is clear go back to R. And so what I'm going to do here, in R, and then I'm going to use the it into R. And which it has now done. And what's happened. I've got this sort of mess R. And then I'm going to get just the structure right here and you can see that it's a list within each one of the lists. And what I'm list to a data frame. I went through the list located, you have to use this big long statement Let me zoom in on that again. There they are. names for that bit of the data frame. So, And then what I'm going to do is, I'm going going to select some variables and put them this is what I get. I will zoom in on that this data set that I pulled from 1957, are drivers ever, and other people who competed using this API call in R, a very simple thing in a structured format, and do a very simple learned from all this. First off, APIs make structure, they call it for you, and then you to analyze. And they are one of the best data science. When you're looking for data, scraping. And what that means is pulling information data is hiding in the open; it's there, you way to get that data. Now, when you're dealing different formats. You can get HTML text from rows and columns that appear on webpages. scrape data from all sorts of data from images very important qualification before we say and privacy. Just because something is on it out. Information gets copyrighted, and this is stuff that's publicly available, and your own analyses. Now, if you want to scrape one, is to use apps that are developed for my favorites. It is both a webpage, that's There's also ScraperWiki. There's an application in Google Sheets, which I will demonstrate to use an app or if you want to do something code your scraper. You can do it directly PHP. Now, what you're going to do is you're webpage. If you're looking for HTML text, text from webpages, similar to how a reader on the webpage to identify what's the important and h1 for header one, and p for paragraph, from HTML tables, although this is a physical This also uses HTML table tags, that is like data, that's the cell. The trick is when you're sometimes you just have to find that through of how this works. Let's take a look at this I'm going to go to the web right now and show Iron Chef America. And if you scroll down bunch of text here, we have got our table have a table that lists the winners, the statistics pull that from this webpage into another program easy way to do this with Google Sheets. All and in cell A1 of that Google Sheet, we paste give the webpage and then you say that you stuff in quotes, and the index number for to figure out this was table number 2. So, this works. Here I have a Google Sheet and this; if I come here to this cell, and I simply sort of magically propagates into the sheet, now I can, for instance, save this as a CSV And so this is one way that I'm scraping the API, but I just used a very simple, one-link was a HTML table. You can also scrape data a native PDF, I call that a text PDF, or a native PDFs, it looks for text elements; again text. And you can deal with Raster images, the lines, and that's what makes them infinitely you can deal with tabular data, but you probably Wiki, or Tabula in order to get that. And and audio. Getting images is easy; you can And then if you want to read data from them, country, you can go through it, but you will through the image pixel-by-pixel to read the statistical program. Now, that's my very brief if the data you are trying to get at doesn't and you can write code in a language like sensitive to issues of copyright and privacy, instead, you make an analysis that can be step in data sourcing is making data. And new data. I like to think of this as, you're de novo,&quot; new data. So, can't find the data simple solution is, do it yourself. And we're used for doing that. Now, these strategies role. Are you passive and simply observing active where you play a role in creating the the &quot;Q/Q question,&quot; and that is, are you going are you going to get qualitative data, which as well as things like photos and videos and the data? Do you want to get it online, or other choices than these, but these are some you look at those, you get a few possible say more about those. Another one is surveys. one is experiments, although I actually want The first one is laboratory experiments, and the information or an experience for the participants their reactions. It doesn't necessarily mean the situation. And then there's also A/B testing. more variations on a webpage. It's a very, actually very useful for optimizing websites. make sure you can get exactly what you need. And if you can't find it somewhere, then make methods, each of which have their own strengths about each of those in the following sections. making new data that I want to talk about the most common, but because it's the one basically an interview is nothing more than of people. And, the fundamental question is, or something else? Well, there's a few good with a new topic and you don't know what people's so you need something very open-ended. Number you don't know how they will react in particular three: something's going on with the current need to find what's going on, and you need where you get past you're existing categories methods for getting that data. If you want when you don't want to constrain responses. one very basic choice, and that's whether structured interview, you have a predetermined questions in the same order. It gives a lot open-ended. And then you can also have what's is a whole lot more like a conversation where talking to - your questions arise in response interview can be different for each person done in person, but not surprisingly, they Now, a couple of things to keep in mind about can range from just a few minutes to several a special skill that usually requires specific necessarily the hard part. The really tricky interviews by far is analyzing the answers categories and the dimensions that you need about interviews is that they allow you to in sum: interviews are best for new situations can be time-consuming, and they also require but also to analyze the highly qualitative step in data sourcing and making data is surveys. just ask. That's the easy way. And you want real question is, do you know your topic and answers? To know what the range of their answers are going to be important. If you do, then as there were a few dimensions for interviews, can do what is called a closed-ended survey; where you give people just particular options, survey, where you have the same questions in a free-form response. You can so surveys or over the mail or phone or however. And doing surveys. Some really common applications or at the very simple end there is Google is Typeform. There is a lot more choices, how you can get data from online participants surveys is, they are really easy to do, they easy to send out to large groups of people. other hand, the same way that they are easy The problem is that the questions you ask, they can be loaded and the response scales think this particular way&quot; and the person exactly what you are trying to get at. So, that the meaning is clear, unambiguous, and respond, is very clear and they know where of the things about people behaving badly during election time; like we are in right like a survey, but really what it is is a for social media campaigns or I am going to agree with me. A push poll is one that is answer to the questions. This is considered a research point of view. Just hang up on of research ethics, you do need to do other wording, in the response options, and also those can push your responses off one way that it is happening. So, in sum, let's say quickly, on the other hand, it requires familiarity So, you know, sort of, what to expect. And bias to make sure that your answers are going are really concerned about understanding. you are making data is Card Sorting. Now, in academic research, but in web research, of it as what you are trying to do is like trying to build a mental model of people's how do people organize information intuitively? that you are doing online? Now, the basic of little topics and you write each one on with like three by five cards, or there are version of it. Then what you do is you give and the people sort those cards. So, they topics over here and so on. And then you take to calculate what is called, dissimilarity the difference between various topics. And things are structured. Now, there are two There are generative and there's evaluative. respondents create their own sets, their own they like. And this might be used, for instance, be looking for one kind of information next to put that together on the website, so they if you've already created a website, then is where you have a fixed number or fixed way you have set up your menus already. And put the cards into these various categories that your hierarchical structure makes sense or evaluative, what you end up with when you of visualization called a Dendrogram. That here is actually a hundred and fifty data Iris data, that's what's going on here. And and then splits it in pieces and pieces and observations, well actually, individual-level off into two or three groups or whatever is the entire collection of similarity or dissimilarity that you had people sort. Now, I will just card sorting, which makes your life infinitely is really hard. You can use something like These are some of the most common choices. about card sorting in this extremely brief you to see intuitive organization of information physical cards or you can also have digital you are done, you actually get this hierarchical is structured and related to each other. When making data, sometimes you can't get what got to take the hard way. And you can do what course, when I mention laboratory experiments doctor Frankenstein in his lab, but lab experiments little more like this. Nearly every experiment and pencil one with people in a well-lighted Now, the reason you do a lab experiment is And this is the single most theoretically what makes an experiment an experiment is in experiments with manipulations. Now, people think that you are coercing people and messing are manipulating the situation; you are causing people or for one situation than another. see how people react to those different variations. you are going to want to have focused research, variation at a time. And it is usually hypothesis-driven; have done enough background research to say, situation and this way to the other.&quot; A key almost always have random assignment regardless in your study, you randomly assign them to is it balances out the pre-existing differences care of confounds and artifacts. The things between groups that provide alternate explanations assignment and you have a large enough group are basically minimized. Now, some places in this version are for instance are eye tracking bring people in front of a computer and you looking. That's how we know for instance that of web pages. Another very common place is my field, psychology. And in all of these, is considered the gold standard for reliable On the other hand, while it is a wonderful how that works. Number 1, experimentation It is not a simple thing to pick up. Two, and labor intensive. I have known some that can be very expensive. So, what that all means done enough background research and you need important to get really reliable cause and for experimentation. In sum, laboratory experimentation causality or assessing causality. That's because randomization. On the other hand, it can be when considering whether you need to do an it. There's one final procedure I want to New Data. It's a form of experimentation and common in the web world. So, for instance, homepage and you're got these various elements way, when I did this that this woman is actually was kind of weird; I have never seen that entire layout, how things are organized and by variations on A/B testing by Amazon. Here's element like what's the headline or what are do you word something and you create multiple B, why you call it A/B testing. Then when these visitors to one version or another, And then you compare the response rates on second. And then, once you have enough data, set that one solid and then you go on to something are a lot of different outcomes you can look a page, you can actually do mouse tracking you can also look at shopping cart value or of these contribute through A/B testing to to make your website as effective as it can is something that you are going to do a lot. fact, I have seen one person say that what testing. Kind of cute, but it does give you Now, if you want some software to do A/B testing, and VWO, which stands for Visual Web Optimizer. are especially common and when you get the testing to compare the differences or really But you may want to adjust the parameters a little too soon and the information is not sum, here is what we can say about A/B testing. it is done online, which makes it really easy you to optimize the design of your website And it can be done as a series of continual sure that you're accomplishing what you want people as possible. The very last thing I is to talk about the next steps. And probably just sit there. I want you to go and see what data sources. And if it helps, check with you what you need to do your project, then here is get what you need and get going. Thanks Welcome to &quot;Coding in Data Science&quot;. I'm Bart series of videos is we're going to take a So, I am inviting you to know your tools, is to know their proper place. Now, I mention talk about data tools, they talk about it science, as though they were the same set. second that is not really the case. Data tools data science is made up of a lot more than like, business knowledge, it includes the social factors and so there's much more than you will need at least a few tools and so that you can use in data science if it works the basic things. #1 is spreadsheets, it is how they play an important role in data science. there is Tableau public, which is free, and something called Tableau server. Tableau is and I'm convinced for most people provides though while it is not a tool, I do need to you have to be able to navigate that when can talk about some of the essential tools language R, which is specifically for data, Python, which has been well adapted to data. SQL for structured query language. Then if other things that you can do. There are the and Java, which are very frequently used to of high level production code is going to line interface language Bash, which is very data. And then there's the, sort of wild card We'll talk about all of these in separate that you can use, don't forget the 80/20 rule. idea here is that you are going to get a lot of things. And I'm going to show you a little different tools and we'll call them A through less and it kind of tapers down to, you have of stuff that you need. Now, instead of looking cumulative effectiveness. How much are you Well, the first ones right here at 60% where 20% from B and it goes up and then you add smaller pieces and by the time you get to from your ten tools combined. The important the 2nd tool, that is two out of ten, that's up example, you have got 80% of your output. that's a fictional example of the Pareto Principle, approximately like that. And so, you don't don't have to learn how to do everything in tools that will be most productive and specifically say these three things. Number 1, coding or programs and computers. Coding is important, collection of tools that's used in it. And tools to use and what you need to learn and to get a lot of bang from a small set of tools. be most useful for you in conducting your discussion of Coding and Data Science, I actually I want to talk about applications or programs manipulate data. And we are going to begin We're going to do the rows and columns and you need spreadsheets. Now, you may be saying know what I'm fancy, I'm working in my big on.&quot; But, you know what, you too fancy people, reasons for this. Most importantly, spreadsheets a lot of circumstances; there are a few reasons everywhere, they're ubiquitous, they're installed everybody uses them. They probably have more and so it's a very common format. Importantly, of your clients are going to be using spreadsheets dollar companies that keep all of their data them, you need to know how to manipulate that of what you're doing, spreadsheets are specifically of the lingua franca or the universal interchange take it from one program to another. And then, really easy to use. And if you want a second ranking. There's a survey of data mining experts, are the tools they most use in their own work. the list, and in fact, what's interesting of the major big data fancy tools. And so, toolkit for data analyst. Now, since we're let's talk about some of the things you can really good for data browsing. You really which isn't true if you are doing something sorting data, sort by this column then this for rearranging columns and cells and moving replacing and seeing what happens so you know really good for formatting, especially conditional switching the rows and the columns, they make changes. Now it's true if you're a big fancy but for everybody else in the world spreadsheets to do it. You can make pivot tables, that way, in a very intuitive way. And they're for consumption. Now, when you're working you need to be aware of: they are really flexible, when you are working in data science, you called Tidy Data. That's a term I borrowed in the R world. Tidy Data is for transferring rules here that undo some of the flexibility you want to do is have a column be equivalent variables, they are the same thing. And then, cases. That you have one sheet per file, and individual, then organization, then state flexibility that's inherent in spreadsheets, from one program to another. Let me show you If you have downloaded the files for this Let me go to Excel and show you how it works. you get is totally fictional data here that of several products at two locations, like And this is the way spreadsheets often appear; stuff arranged in a way that makes it easy got totals here, with formulas putting them well for the person who made it. And then, month right here and then we have another all for first quarter of 2014. We have got formatting and changes and if we come to the that eventually loads; it's not a good graphic, often find. So, this is the stuff that, while use, you can't feed this into R or Python, to do with it. And so, you need to go through this involves is undoing some of the stuff. tidy. Here we have a single column for date, the site, so we have two locations A and B, things that are sold and how many were sold would want the data laid out exactly like series, you will do something vaguely similar going to collapse it even further. Let me have done is, I have created a new column so, by the way, what this means is that we got over a thousand rows. Come back up to that now it's in a format that's really easy makes it tidy and you can re-manipulate it So, let's sum up our little presentation here, you are, no matter what you are doing in data for that is that spreadsheets are often the in mind though, that is as you are moving tidy data or well-formatted data is going analytical programmer language of choice. and specifically the applications that can more than almost anything else, and that's not familiar with these, these are visualization data, the most important thing you can do work with it from there. And in fact, I'm might be all that they really need. It will to work constructively with data. So, let's Now, there are a few different versions of and Tableau Server, and these are the paid lot of money, unless you work for a nonprofit for free. Which is a beautiful thing. What the paid version, but we are looking for something here and go to products and we have got these We click on that, it brings us to this page. one that has what we want, it's the free version save files locally to your computer, which it saves them to the web in a public form. can get an immensely powerful application a lot of people, which is why people are willing And again, if you work for a nonprofit you I am going to show you how things work in you can work with personally. The first thing And so, you put in your email address, you on. It is a pretty big download. And once up the application. And here I am in Tableau By the way, you also need to create an account to see it. I will show you what that looks thing right here and the first thing you need I'm going to bring in an Excel file. Now, you will see that there is this one right fact, it is the one that I used in talking course. I'm going to select that one and I'm like bringing in Excel because it's got all This one works better with it, but what I'm data. By the way, you see that it put them tidy data and I'm going to drag it over to And now what it does is it shows me a version can do here. You can rename it, I like that things that you can do here. I'm going to one. Now, I've got the data set right here, to a worksheet. That's where you actually one. Okay. This is a drag and drop interface. going to pull the bits and pieces of information flexibility here. I'm going to show you two sales of my fictional ballpark items. So, going to put that as the field that we are down right here and this is our total sales. time. So, let me take item right here, and right up here into rows. Those will be my total of each of the items. Fine, that's really put that here in columns to spread it across. don't want to do that, I want to have three click right here and I can choose a different not going to help because I only have one I'm going to come down to week. Actually, gets enormously complicated, so that's no I've got a lot of numbers there, but what going to come over here and click on this seeing the information, except it lost items. up into this graph to say this is a row for by week for each of my items. That's great. in the site, the place that it sold. So, I'm right over here. And now you see I've got the different sites. I'm going to color the I'm going to grab site and drag it onto color. sites. And this makes it a lot easier to tell other cool stuff you can do. One of the things and I can tell it to put an average line through Now we have the average for each line. That's get a little bit of a forecast right here. here. I will get this out of the way for a few weeks, and that's a really convenient, organizations that might be all that they here is the absolute basic operation of Tableau, of visualizations and manipulate the data so much to it and we'll show that in another one last thing about Tableau Public, and that and save it, it's going to ask me to sign asks me how I want to save this, same name hit save. And then that opens up a web browser, see here's my account and my profile. Here's that I need there; I'm going to edit just I'm going to leave its name just like that. I wanted. I can allow people to download the that there so you can download it if you need this thing that says show the different sheets and also it's published online and people is an incredible tool for creating interactive menus, and you can rearrange things, and you way of presenting information, and as I said this may be as much as they need to get really And so I strongly recommend that you take the paid desktop version or the public version compelling and insightful visualizations out their first experience of &quot;Coding and Data I think of SPSS and the first thing that comes though this looks more like Harry Potter. SPSS comes from Statistical Package for the about it now, they act like it doesn't stand social science research which is generally and that's where I first learned how to use webpage ibm.com/spss. If you type that in, you to IBM's main webpage. Now, IBM didn't 16, and it was very briefly known as PASW briefly and now it's back to SPSS, which is desktop program; it's pretty big, it does used in a lot of academic research. It's also even some medical research. And the thing but has drop-down menus to make your life programming languages that you can use. Now, a student you can get a cheap version, otherwise it one way or another, when you open it up showing SPSS version 22, now it's currently is, in anything other than software packaging, feel like we should be on 17.3, as opposed small that anything you learn from the early there is a lot of backwards and forwards compatibility, I have practically doesn't matter. You get you don't want to see it anymore you can get And this is our main interface. It looks a you have a separate pane for looking at variable for output and then an optional one for something works by first opening up a data set. SPSS they are not easy to get to and they are really me go to where they are. In my mac I go to to the folder IBM, to SPSS, to statistics, I have to say I want the ones that are in files are the actual data files, there are kind of file and then we have a different versions of it. I'm going to open up a file data set in SPSS format. And if you don't it really doesn't matter for now. By the way, be really really slow when it opens. It also, of buggy and crashes. So, when you work with your work constantly. And also, being patient is a data set that just shows addresses and This, I don't even know if this is real information, you do point and click analyses, which is to come up here and I am going to say, for a- I'm going to use what is called a legacy So, I simply click values. Put that right of it and click ok. This is going to open version of it, so I'm going to make that bigger. window and it has a navigation pane here on from, and it saves the command here, and then, we see most of the houses were right around $400,000. I have a mean of $256,000, a standard is 94 houses in the data set. Fine, that's to do some analyses, let me go back to the come here to analyze and I can do descriptive Explore. And I'll take the purchase price get a whole bunch just by default. I'm going window. Once again made it tiny. And so, now and I've got a bunch of information. A stem way of checking for outliers. And so this You can export this information as images, you can do it as a pdf or a PowerPoint. There's everything that's on here. Now, I just want life so much easier in SPSS. You see right it's actually saying graph, and then histogram, we've got this little command right here. in SPSS, and that's something you kind of but there's a very simple way to do this. up something called a Syntax file. I'm going window that's a programming window, it's for I did a moment ago. I'll go back to analyze and explore, my information is still there. it up with drop-down menus and point and click, is, it takes the code that creates that command this is just a text file. It saves it as .spss, anything. And what's beautiful about this you can even take this into Word and do a to replicate the analyses. And so for me, Syntax you don't know the true power of it way of operating it. Anyhow, this is my extremely say is that it is a very common program, kind you a lot more power and options and you can commands as well to automate your work and I want to take a look at one more application JASP. This is a new application, not very but with an amazing promise. You can basically you know what, we love free. But, JASP is it's intuitive, and it makes analyses replicable, So, take that all together, you know, we're before we move on, you just may be asking has emphatically denied that it stands for as it may, we will just go ahead and call to it by going to jasp-stats.org. And let's program, they say a low fat alternative to of doing statistics. You're going to want it even comes in Linux format, which is beautiful. are updating regularly. If you're on Mac, an easy thing to install and it makes a lot way to do analyses. When you open up JASP, blank interface, but it's really easy to get over here to file and you can even choose one called Big 5 that's personality factors. to work with. Let me scroll this over here and let's do some quick analyses with these. we can pick a few variables. Now, if you're much the same and the output looks a lot the what I want and it immediately pops up over I can get core tiles, I can get the median. all you have to do is click on it and they and you can modify these things a little bit, Let's see if I can drag that down and if I I went a little too far on that one. Anyhow, hide this, I can collapse that and I can go neat though is when I navigate away, so I page, we are back to the data here. But if one right here, it immediately brings up the it some more if I want. Say I want skewness an amazing thing and then I can come back can come down to the plots expand those and that made them. It's an amazingly easy and really nice thing about JASP and that is that well through a program called osf.io. That its web address osf.io. So, let's take a quick science framework website and it's a wonderful open, transparent, accessible, accountable, say enough nice things about it. What's neat can create your own area and I've got one instance, here's the datalab page in open created a version of this JASP analysis and my JASP analysis in JASP and I'll show you go back to JASP. When we're here we can come saved this file to the desktop. Click on desktop, this with all the other files, DS03_2_4_JASP, going to open up a new window and you see I did a lot more analyses. I've got these Come down here, I did a linear regression. the commands that produce it as well as the that, but I did do some confidence intervals way to work with all this. I'll click back go away and so I've got my output here in option of saving it to OSF, in fact if you be able to go to the page where you can see let's take a look. This is that page, there's you see here is the same analysis that I conducted, with people or if you want to show things Everything is right there, this is a static of downloading the original file and working I'm really enthusiastic about JASP and about rapidly. I see it really as an open source and I think it is going to make data science strongly recommend you give JASP a close look. and Data Science&quot; the applications part of choices. And I'll have to admit it gets kind many choices. Now, in addition to the spreadsheets, already talked about, there's so much more things that I'm aware of and I'm sure I've other people like really well, but these are but interesting ones. Number one, in terms is an extremely common analytical program, actually the first program that I learned to use and it can be expensive, but there's also has something called the SAS University and it's slightly reduced in what it does, in a virtual machine which makes it an enormous if it's something that you want to do. SAS it not so extraordinarily expensive and that Think a little bit of Tableau, how we saw you can drag things around, it's really wonderful expensive. Another very common choice among use Minitab. Now, for mathematical people, Mathematica itself, but it is really more hand, Wolfram; who makes Mathematica, is also people don't think of this a stats application Wolfram Alpha is an incredibly capable and you can do amazing things in this, including and so it's worth taking a little closer look the data that you need so Wolfram Alpha is that are more specifically geared towards regular, you know, little t tests and stuff KNIME and Orange and those are all really where you drag notes onto a screen and you things run through. All three of them are them work in pretty similar manners. There's and this is unusual because it's browser based, though you can't download a whole lot, it very friendly, very accessible program. Then for free on your own computer, there's one open for all, it's kind of a cheesy title, a web page straight out of 1990 is Past 3, hand does do very general stuff, it runs on thing and it's free, but it is relatively one that's near and dear to my heart is a but it costs like $6 or $12 a year, it's really basic statistics and for learning, I used And then if you're deeply wedded to Excel you can purchase add-ons like XLSTAT, which the Excel environment itself. That's a lot is don't get overwhelmed. There's a lot of of them. Really the important question is you're working on? Here's a few things you is functionality, does it actually do what You don't need everything that a program can do, people probably use five percent of what's these programs are a lot easier to use than ones that are easier to use, I like them, I need custom stuff&quot;. But I'm willing to bet anything custom. Also, the existence of a you come across problems and don't know how do a search for an answer and have enough who have put answers up and discuss these programs are very substantial communities and it is to you to decide how important it is the issue of cost. Many of these programs cheap, some of them run some sort of premium So, you don't buy them unless somebody else things that you want to keep in mind when Also, let's mention this; don't forget the most of the stuff that you need to do with maybe three, will probably be all that you range of every possible tool. Find something with and really try to extract as much value discussion of available applications for coding are tools, they don't drive you, you use them. of your applications and the way that you is to remember, what works for you, may work with it, if it's not the questions you address, works for you and the projects that you're tools, for working in data science. When you're important things you can do is be able to data you're going to be working with HTML. is what makes the World Wide Web go 'round. - and if you've never dealt with web pages are just text. It is just a text document, the document and a web browser knows what way. So, for instance, some of the tags, they and you have an angle bracket and then the the body, the main part of your text, and body to let the computer know that you are p for paragraphs. H1 is for header one and table data or the cell in a table and you it looks like just go to this document: DS03_3_1_HTML.txt. depending on what text editor you open this I've opened it up in TextMate and so it actually typed this manually; I just typed it all in is, I have an empty header, but that sort body is, and then I have some text. li is a link to a webpage, then I have a small table. when displayed as a web page, just go up here same document, but now it is in a browser know this is very fundamental stuff, but the going to be extracting data from the web, is encoded in the web, and it is going to web page. Now, I will mention something that, use CSS to define the appearance of a document. and CSS gives the appearance. And that stands to worry about that right now because we're you have the key to being able to read web data science project. So, in sum; first, the web pages that are there. HTML defines the the page. And you need to learn how to navigate data from the web pages for your data science Science&quot; when you're working with web data like to think of this as the part of web data thyself&quot;. XML stands for eXtensible Markup data. What that means is that tags define piece of information is. But, unlike HTML, want. And so you have this enormous flexibility it so the computer can read it. Now, there's XML files. Number one is in web data. HTML they're feeding data into it, then that will Microsoft Office files, if you have .docx a version of XML that's used to create these information that has all of your artists, that's all stored in an XML file. And then programs can be saved as XML as a way of representing And for XML, tags use opening and closing the major difference is that you're free to thinking about iTunes, you can define a tag in genre to begin that information, and then to let it know you're done with that piece or you can do it for rating, or you can do you want and you put the information in between of how this works. I'm going to show you a at ergast.com and API, and this is a website One racing. Let's go to this webpage and take are at Ergast.com, and it's the API for Formula of the 1957 season in Formula One racing. in each race, and how they finished and so in a web page. If you want to see what it XML onto the end of this: .XML. I've done one. And as you see, it's only this bit that same because the web page is structuring XML it looks like in its raw format, just do an page source. At least that's how it works page. And you can see we have tags here. It obviously, these are not standard HTML tags. particular dataset. But we begin with one. we close it using the backslash right there. knows how to read it, which is exactly, this a really good way of displaying data and its web. You can actually use what is called an to access this XML data and it pulls it in with it really easy. What's even more interesting it between different formats, because it's dealing with. So for example, one it's really value files (that's the spreadsheet format) are; what piece of information goes in each HTML documents to XML because you can think sort of a subset of the much freer XML. And comma separated value, to XML and vice versa. the structure is made clear to the programs we can say. Number one, XML is semi-structured to tell the computer what the piece of information want them to be. And, XML is very common for the format XML/HTML/CSV so on and so forth. forth which gives you a lot of flexibility you need for your own analysis. The last thing and web data is something called JSON. And is better. Now, what JSON stands for is JavaScript to be one word. And what it is, is that like you have tags that define the data, so the is, but like XML the tags can vary freely. and JSON. So XML is a Markup Language (that's to the text; it lets the computer know what you to make comments in the document, and so you can actually put information there context. JSON, on the other hand, is specifically got that special focus. And the structure; know it directly represents objects and arrays that works really well with the programs that shorter than XML because it does not require that with XML, but that's not typically how JSON is basically taking XML's place in web a lot of things, but JSON is slowly replacing between the three by going back to the example One car races in 1957 from ergast.com. You we will navigate to the others from that. just type in without the .XML or .JSON or races in 1957. And we saw earlier that if it looks exactly the same. That's because default. But, if you were to right click on get this instead, and you can see the structure. opening tag and a closing tag and some extra .JSON what you really get is this jumbled is a lot of structure to this. So, what I copy all of this data, then I'm going to go you can do here, and it's a cute phrase. It's it look structured so it's easier to read. Print JSON, and now you can see hierarchical is that the JSON tags only have tags at the a colon, then it gives the piece of information the next one. And this is a lot more similar like R or Python. It is also more compact. but this is one of the reasons that JSON is And as you may have guessed, it's really easy to convert between XML, JSON, CSV, etc. You in and you get the other version out. There of situations, they are just interchangeable. JSON is semi-structured data, where there but you define the tags however you want. and because it reflects the structure of the easy. Also, because it's relatively compact as the container for data on web pages. If Science&quot; and the languages that are used, that is, according to many standards, R is example, take a look at this chart. This is experts of the software they use in doing R is first, and in fact that's important because in hand with R for Data Science. But R sees this particular list. Now there's a few reasons and it's open source, both of which make things for vector operations. That means it's able having to write 'for' loops to go through. you know that would be kind of disastrous R has a fabulous community behind it. It's Google it, you're going to end up in a place examples of what you need. And probably most packages that add capabilities to R. Essentially, with R, you actually have a choice of interfaces. how you get your results. R comes with it's You can do that, or if you are on a Mac or Terminal through the command line. If you've up. There is also a very popular development actually the one I use and the one I will new competitor is Jupyter, which is very commonly there. It works in a browser window, even and Jupyter there's pluses and minus to each to each one of them. But no matter which interface of code in order to get the commands. Some there are some advantages to that in terms the transparency of your commands. So for the commands in R. You can enter them into one line at a time and that's called an interactive and pieces selectively and that makes your if you are familiar with programming other a little weird. It has an idiosyncratic model. it is a different approach, and so it takes in different languages. Now, once you do your going to get is graphs in a separate window. output in the console, and no matter what So that makes it portable, you can do it in I like to think of this: here's our box of going to get. The beauty of R is in the packages Now there are two sources of packages for stands for the Comprehensive R Archive Network, does is takes the 7,000 different packages topics that they call task views. And for they have datasets that come along with the and you can even have vignettes where they interface is called Crantastic! And the exclamation crantastic.org. And what this is, is an alternative something you like in Crantastic! and you But the nice thing about Crantastic! is it shows how recently they were updated, and sort of the latest and greatest. Now from a few things about R: Number one, according and it's a command line interface. You're a strength and a challenge for some people. and thousands of packages of additional code make it possible to do nearly anything in talking about &quot;Coding and Data Science&quot; and about Python. Now, Python the snakes is a and that's its beauty. If we go back to the experts, you see that Python's there and it's about that, is that on this list, Python is It's the only one that can be theoretically you want. That gives it some special powers are very specific to data science work. The it's general purpose. It's also really easy computer, Python is built into it. Also, Python of thousands of people involved, and also actually has 70 or 80,000 packages, but in still thousands available that give it some to know about Python. First, is about versions. in wide circulation: there's 2.x; so that Version 2 and version 3 are similar, but they this: there are some compatibility issues the other. And consequently, most people have this leads to is that many people still use I use, I'm using 2.x because so many of the that in mind. Now let me say a few things does come with its own Interactive Development You can also run it from the Terminal, or have. A very common and a very good choice for programming and it was originally called lot of the time when people are talking about is this Python in Jupyter and the two are neat things you can do, there are two companies: made special distributions of Python with to make it very easy to work with data. I the one that I use, a lot of other people it's going to get you up and running. And you use, all of them are command line. You're strength to that but, it can be intimidating commands of Python, we have some examples to remember is that it's a text interface. of people because it is very often a first purpose programming. And there are a lot of it very powerful for data science work. So, loves Jupyter, and Jupyter is the browser-based you access it through a web browser that makes in data science. There's a few reasons for text output and you can use what's called You can get inline graphics for the graphics you did it. It's also really easy to organize, in Jupyter. Which makes it a strong contender programming. Another one of the beautiful thousands of packages available. In Python, name PyPI. Which is for the Python Package packages and 7 or 8,000 of those are for data-specific get to be very familiar with are NumPy and in general; Matplotlib and a development of and graphics. Pandas is the main package for learning, almost nothing beats scikit-learn. Python, I will be using all of these as a for working with data. In sum we can say a very familiar to millions of people and that languages we use for data science on a frequent purpose. Which means it can be used for a And it gets its power, like R does, from having expand its capabilities especially in terms in Data Science,&quot; one of the languages that think data science, is Sequel or SQL. SQL &quot;why do we want to work in SQL?&quot; Well, to who apparently explained why he robbed banks is.&quot; The reason we would with SQL in data Let's take another look at our ranking of and there's SQL. Third on the list, and also tool. Other tools, for instance, get much has been around for a while as very very capable. will notice that I am saying Sequel even though is a language, not an application. There's be used in different applications. Primarily, databases. And those are special ways of storing can put things together, you can join them and then what you usually do is then export of choice. The big word here is RDBMS - Relational you will usually see SQL as a query language Management System, there are a few very common have some money to spend, there's Oracle database In the open source world, two very common say Sequel, when it's here you generally say both open source, free versions of the language; for you to working with your databases and thing about them, no matter what you do, databases tables. Each table has rows and columns and abstraction or measurement, which means you and then it can refer to lots of other tables. and up to date. When you are looking into Management System, you get to choose in part GUI. Some of those include SQL Developer and choices. And there are a lot of other choices graphical interfaces for working with these So really, any command line interface, and programming tool is going to be able to do command deck of your ship and think of a few working with SQL. There are just a handful to go. There is the Select command, where include. From: says what tables are you going of specifying conditions, and then Order By: together. This works because usually when out the information. You want to select it, are going to do is you are going to send the analysis, like R or Python or whatever. In one, as a language it's generally associated efficient and well-structured ways of storing be very useful when working with databases. really a handful. Five, 10 commands will probably Then once the data is organized, the data for analysis. When you talk about coding in the groups of languages that come up most powerful applications and very frequently In data science, the place where you will The absolute fundamental layer that makes C and C++. C is from the '60s, C++ is from usage, and their major advantage is that they're used as the benchmark for how fast is a language. them really well suited to production-level really neat is that in certain situations, then you can actually use C code in R or other is based on C++, it's major contribution was idea that you were going to be able to develop and different environments. Because of that, language overall against all tech situations. like I said, when time is of the essence, the job accomplished quickly, and it has to probably going to use. The people who are engineers. The engineers and the software of the algorithms in data science or the back and the entire structure that makes analysis are actually analyzing the data, typically elements. They don't usually touch C or C++, to the high-level languages like R or Python. bedrock in the back end of data and data science. they are very reliable. On the other hand, reserved for the engineers who are working makes the rest of the analysis possible. I of &quot;Coding in Data Sciences&quot; and the languages that's called Bash. Bash really is a great are still being used actively and productively it's almost like typing on your typewriter. typing out code through a command line interface computers practically goes back to the typewriter you even had a monitor, you would type out of paper. The important thing to know about interacting. It's not a language, because For instance, it is important to talk about a shell is a language or something that wraps language, that is the interaction level for that aren't really human-friendly. On Mac which is short for Bourne Again Shell. On But whatever you do there actually are a lot C shell; which is why I have a seashell right Interactive Shell, and a whole bunch of other Linux and PowerShell is the most common on computer at the command line level. There's You have a prompt of some kind, in Bash, it's command here. Then, the other thing is you how much you can get done with a one-liner so one feeds into the other. You can run more you call a text document that has a bunch elaborate analyses done. Now, we have our and what these are, are specific programs thrives on &quot;Do one thing, and do it very well.&quot; for Bash. Number one, is the Built-ins. These and so you're able to use it anytime by simply are: cat, which is for catenate; that's to is it's own interpreted language, but it's line. By the way, the name 'Awk' comes from Then there's grep, which is for Global search a way of searching for information. And then and its main use is to transform text. You 4 utilities. A few more are head &amp; tail, display Sort &amp; uniq, which sort and count the number is for word count, and printf which formats while you can get a huge amount of work done there are also a wide range of installable. can add to Bash, or whatever programming language that have been recently developed are jq: object notation data from the web. And then JSON to csv format, which is what a lot of with. There's Rio which allows you to run programming language R in the command line This is a command line tool that allows you through the command line. Normally, you do servers remote. It's an amazingly useful program in the command line is an enormous benefit. have all these opportunities, all these different And there's still an active element of utilities being in one sense as old as the dinosaurs, well evolved and well suited to its purpose the built-in and the installable are fast thing and they do it very, very well. And of very active development of command line data science. One critical task when you are the things that you are looking for, and Regex a wonderful way to do that. You can think needles in haystacks. Now, Regex tends to an example. As something that's designed to and it specifies what can go in the beginning, got a certain number of letters and numbers, end. And so, this is a special kind of code expressions, or regex, are really a form of specifying what needs to be where, what can write both specific patterns; say I only want like the email validator that I showed you. search pattern, your little wild card thing, those cases, then you export them into another of how it can work. What I've done is taken texts to Emma and to Pygmalion, two books the command. Grep ^l.ve *.txt - so what I'm that start with 'l', then they can have followed by 've', and then the .txt means folder. And what it found were lines that so on. Now in terms of the actual nuts and certain elements. There are literals, and mean. You type the letter 'l', you're looking which specify, for instance, things need to code that give representations. Now, there this character is used as a variable, but to a placeholder. Then you have the entire have the target string, the thing that it few very short examples. ^ this is the caret. a circonflexe. What that means, you're looking you are searching. For example, you can have that begins with capital M. For instance the you have iMac, it's a capital M, but it's false, it won't find that. The $ means you string. So for example: ing$ that will find but it won't find the word 'flings' because dot, the period, simply means that we are So, for example, you can write 'at.'. And 'a', a 't', and then one letter after doesn't have anything after the 'at'. And how it can work. Obviously, it gets more complicated these bits and elements. Now, one interesting this as a game. I love this website, it's And what it does is brings up lists of words; expression in the top, that matches all the on the right. And uses the fewest characters great way of learning how to do regular expressions going to get you the data you need for your help you find the right data for your project, Now, on the other hand, they are cryptic, the same time, it's like a puzzle and it can see how you can find what you need. I want Data Science&quot; and we'll wrap up this course steps you can take for working in data science. tools and you want to start working with those that I've said at another time. Data tools but don't make the mistake of thinking that same thing as actually conducted data science. enthusiastic and they get a little carried really is this: Data Tools are an important is much bigger than just the tools. Now, speaking you can use, and that you might want to get just Apps, specific built applications Excel getting the data from clients or doing some wonderful for interactive data visualization. with both of those. In terms of code, it's or ideally to learn both. Ideally because utilities, it's a great idea to work with regular expression or regex. You can actually expressions. So they can have a very wide requires some sort of domain expertise. You're or intimate understanding of a particular what constitutes workable answers and the go through all of this, you don't need to you don't need everything. You don't need you don't need every approach. Instead remember, style. But no matter what you do, remember an end. Instead, you want to focus on the it is. And I can tell you really, the goal of your data to make informed choices. In always meaning. And so with that, I strongly in data science and start finding meaning &quot;Mathematics in Data Science&quot;. I'm Barton Mathematics matters for data science. Now, and &quot;Computers can do it, I don't need to need math I am just here to do my work&quot;. Well, That is if you want to be a data scientist, to talk about some of the basic elements of and how they apply to data science. There science. #1, it allows you to know which procedures in a way that is the most informative and of math, then you know what to do when things values or things won't compute, and that makes thing is that some mathematical procedures by actually firing up the computer. And so to have at least a grounding in Mathematics Now probably the most important thing to start algebra I want to mention. The first is elementary is Linear or matrix algebra which looks more computers to actually do the calculations. of Linear Equations where you have multiple to solve. Now there's more math than just in this course. Calculus, a little bit of and complexity of operations. A little bit Bayes or Bayes theorem which is used for getting you interpret the results of an analysis. to demonstrate the procedures by hand, of the real world, but we are dealing with simple the most important thing to remember is that math, really You can do it! And so, in sum: off, you do need some math to do good data helps you choose the right procedures, and or you can use software computers to do the of the role of &quot;Mathematics and Data Science&quot;, elements. And in data science nothing is more I'd like to begin this with really just a the first book on algebra was written in 820 was called &quot;The Compendious Book on Calculation was called this, which if you transliterate right here. That's the algebra, which means comes from and for our concerns, there are to talk about. There's Elementary Algebra, of linear equations. We'll talk about each into context, let's take an example here of from a survey of the salary of people employed of it. The salary was equal to a constant, started with and to that you added years, how many hours they worked per week. And that exact there's also some error to throw into has. Now, if you want to abbreviate this, + Y + B + H + E, although it's more common go through this equation very quickly. The y the variable y for person i, &quot;i&quot; stands outcome y for person i. This letter here, or the average, that's why it has a zero, But right next to it we have a coefficient sub 1 for the first variable and then we have i means its the score on that variable for we do the same thing for variables 2 and 3, with an i for the error term for person i, was their actual score. Now, I'm going to see how they can be applied to data science. First off, Algebra is vital to data science. get a single outcome, do a lot of other manipulations. one case at at time. Especially when you're for Data Science&quot; foundations is to look at algebra. And depending on your background, to think welcome to the Matrix. Because it's dealing with matrices . Now, let's go back salary. Where salary is equal to a constant error, okay that's a way to write it out in form, it's going to look like this. Now before to talk about a few new words, maybe you're and this means a single number. And then a of numbers that can be treated as a collection. a matrix consists of many rows and columns. of that by the way is matrices and the thing Now let's take a look at a very simple example of matrix algebra or Linear Algebra. Where variables. So over here on the left, we have 1 and 2. And we put it into the square brackets Here on the far left, it's a vector because is a matrix, that has here on the top, the X1 is for variable 1, X2 is for variable 2 it's for person 1. Below that, are the scores here, in another vertical column are the regression are using. And then finally, we've got a tiny terms for cases 1 and 2. Now, even though to run through the procedure, so I'm going to take two fictional people. This will be We'll say that she's 28 years old and we'll a 4 on a scale of 5, and that she works 50 Our second fictional person, we'll call him and he has moderate bargaining skills 3 out of $84,000.00. And so if we are trying to representation that we had here, with our Greek symbols. And we will replace those variables Sophie, our first person. So why don't we the result here. Sophie's salary is $118,000.00 to get that. The first thing here is the intercept. sort of the starting point, and then we get years over 18. She's 28 so that's 10 years is bargaining skills. She's got a 4 out of By the way, these are real coefficients from And then finally hours per week. For each up, and get a predicted value for her but you may be saying that's pretty messed up, in the equation including she might be the going to make a lot more. And then we do a neat about matrix algebra or Linear Algebra here are these bolded variables. That stand instance; this Y, a bold Y stands for the the entire matrix of values that each person all of the regression coefficients and then error terms. And so it's a really super compact of data and coefficients that you use in predicting computers use matrices. They like to do linear simpler because you can put it all in there compact notation and it allows you to manipulate And that's that major benefit of learning Our next step in &quot;Mathematics for Data Science And maybe you are familiar with this, but there are times, when you actually have many all simultaneously. And what makes this really Specifically that means X depends on Y, but about this, is it's actually pretty easy to matrix algebra to do it. So let's take a little you have a company and that you've sold 1,000 around naked like they are in this picture sold for $5. You made a total of $5,900.00 at each price?&quot; Now, if you were keeping our this little bit of information. And to show going to start with this. We know that sales total cases sold. And for revenue, we know $20 and another number times $5, that it all we can figure out the rest. Let's start with isolate the values. I am going to do that then I can take that and I can subtract it, Normally I solve for x, but I solve for y, to revenue. We know from earlier that our $5,900.00 total. Now what we are going to are going to replace it with the equation that through and we get $20,000.00 minus $20y these two because they are on the same thing. $20,000.00 from both sides. So there it is, then I get it over on the right side. And $14, 100.00. Well, then I divide both sides get y equals 940. Okay, so that's one of our We have x plus y equals 1,000. We take the the equation, then we can solve for x. Just We get x is equal to 60. So, let's put it What this tells us is that 60 cases were sold at $5 each. Now, what's interesting about We're going to draw it. So, I'm going to graph we had. This one predicts sales, this one in the economical form for creating graphs. so we're going to solve both of these for is on the left, we subtract that. Then we something we can graph. Then we do the same way through, that gets rid of that and then from each side. And what we are left with we can graph. So this first line, this indicates 1000, but we rearranged it to y is equal to we have here. And then we have another line, originally written as $20.00 times x plus that to y equals minus 4x plus 1,180. That's is right here at the intersection. There's of cases sold at $20.00 and 940 as the number the solution of the joint equations. It's equations. So in sum, systems of linear equations unique solutions. And in many cases, it's with linear algebra when you use software our discussion of &quot;Mathematics for Data Science&quot; we want to talk about is Calculus. And I'm here. The reason I'm showing you pictures Latin for stone, as in a stone used for tallying. of stones and they would use it to count sheep formalized in the 1,600s simultaneously, independently And there are 3 reasons why Calculus is important of the procedures we do. Things like least they use Calculus in getting those answers. that changes over time. If you are measuring then you have to use Calculus. Calculus is especially when you're optimizing. Which is Also, it is important to keep in mind, there Calculus, which talks about rates of change Calculus of change. The second kind of Calculus are trying to calculate the quantity of something It's also known as the Calculus of Accumulation. we're going to focus on differential Calculus. going to do y equals x2 a very simple one calculate things like the slope. Let's take middle of the red dot. X is equal to minus to get the y value, all we got to do is take us 4. So that's pretty easy. So the coordinates 4 on the y. Here's a harder question. &quot;What Well, it's actually a little tricky because part on it. But we can get the answer by getting are several different ways of writing this, And let's start by this, what we are going part, so that we have x2 . And you see that come over here and we put that same value here. And then we can do a little bit of subtraction. ignore that then then you get 2x. That is derivative of x2 is 2x. That means, the slope let's go back to the curve we had a moment x minus 2, and so the slope is equal to 2x, it and we get minus 4. So that is the slope if we choose a different point? Let's say the slope is equal to 2x so that's 2 times hand, you might be saying to yourself &quot;And that this is important and what it is, is the decisions. And if that seems a little them to make more money. And I'm going to right now in sum, let's say this. Calculus foundation of statistics and it forms the In our discussion about Mathematics and data to talk about right here is calculus and how of this, in other words, as the place where or something. Now if you remember this graph that shows this curve here and we have the 2x. And so when x is equal to 3, the slope comes into play. Calculus makes it possible And if you want to think of something a little by the way that's Cupid and Psyche. Let's assume you've created a dating service and for it that will maximize your revenue. So, First off, let's say that subscriptions, annual can charge that for a dating service. And week. On the other hand, based on your previous have some data that suggests that for each you will get 3 more sales. Also, because its more easier right now and assume there is it works, but we'll do it for now. And I'm by hand. Now, let's go back to price first. subscription price and you're going to subtract I'm giving D. So, one discount is $5.00, two have a little bit of data about sales, that per week and that you will add 3 more for we're going to do here is we are going to do that the first thing we have to do is get $500.00, is the current annual subscription to do is, is we are going to get the y intercept well we take the $500 we subtract that from is equal to minus $500.00. Divide both sides to 100. That is, when d is equal to 100, x y intercept, but to get that we have to substitute to 100, and the intercept is equal to 180 per week and then we take the three and we 3 times 100,[1] is equal to 300 add those y intercept in our equation, so when we've expected sale is 480. Of course that's not for finding the slope of the line. So now the change in y on the y axis divided by the looking at sales; we get our 180 new subscriptions and we take our information on price. $500.00 and then we take the 3d and the $5d and those by minus 5, and that's just minus 0.6. So to minus 0.6. And so what we have from this is equal to 480 because that is the y intercept price. So, this isn't the final thing. Now there's another stage to this. Revenue is did you sell and how much did it cost. Well, If we take sales and we put it in as a function a moment ago, then we do a little bit of multiplication times the price minus 0.6 times the price. What we're going to do now is we're going that we talked about. Well, the derivative of the x, the derivative is simply 480 and to what we did with the curve. And what we times the price. This is the derivative of zero now, and just in case you are wondering. is going to give us the place when y is at have to invert the shape. When we are trying at the very tippy top of the curve, because so what we're going to do is solve for zero. to find out when is that equal to zero? Well, and we divide by minus 1.2 on each side. And we've been charging $500.00 a week, but this $400.00 instead. And if you want to find out 480 and if you want to know what the sales take the 480 which is the hypothetical y intercept our actual price of $400.00, multiply that, 240 total. So, that would be 240 new subscriptions is 180 new subscriptions per week at $500.00 is $90,000.00 per year, I know it sounds really the formula for maximum value is 240 times And so the improvement is just a ratio of is equal to 1.07. And what that means is a to get a 7% increase in their business simply revenue. So, let's summarize what we found $500.00 year to $400.00 per year, assuming you can increase sales by 33%; that's more total revenue by 7%. And so we can optimize and it has to do with that little bit of calculus sum, calculus can be used to find the minima It allows for optimization and that in turn Our next topic in &quot;Mathematics and Data Principals&quot;, wondering what Big O is all about, it is about does it take to do a particular operation. to be really precise, the growth rate of a add elements is called its Order. That's why O gives the rate of how things grow as the is there can be really surprising differences. kinds of growth rates or Big O. First off, the spot, you can get stuff done right away. order. That's something that takes the same an email out to 10,000 people just hit one the number of people, the number of operations, from that is Logarithmic, where you take the of that and you can see it's increased, but off really quickly. So an example is finding Next, one up from that, now this looks like not a big change. This is a linear function, time. So if you have 50 operations, you have it takes 50 units of space. So, find an item be linear time. Then we have the functions a lunch because it's going to take a while. You take the number of items and you multiply example of this is called a fast Fourier transform, sound or anything that sort of is over time. have 30 elements your way up there at the of time, or 100 units of space or whatever But really, that's nothing compared to the going to be camping out you may as well go You square the number of elements, you see That's Quadratic growth. And so multiplying two numbers that have 10 digit numbers it's take a long time. Even more extreme is this the power to the number of items you have. not even go all the way to the top. That's doesn't draw it when it goes above my upper this is a really demanding kind of thing, for what's called the Travelling Salesman an example of exponential rate of growth. is sort of catastrophic is Factorial. You that to the exclamation point Factorial, and it basically goes straight up. You have any to be hugely demanding. And for instance if Problem, that's trying to find the solution huge amount of time. And you know before something turn to stone and wish you'd never even started. not only do something's take longer than others, more variable than others. So for instance, to sort, there are different kinds of sort is something called an insertion sort. And It's O of n, that's not bad. On the other huge difference between the two. Selection and the average is quadratic. It's always a long time, but at least you know how long of something like an insertion sort. So in #1, You need to know that certain functions thing applies to making demands on a computer's vary in their demands. Also, some are inconsistent. slow or difficult the others. Probably the the demands of what you are doing. That you possible solution or you know, your company be mindful of that so you can use your time time that you need it. A really important and one of its foundational principles is comes in intuitively for a lot of people is sports outcomes. And really the fundamental That gets at the heart of Probability. Now We've got our friend, Albert Einstein here work this way. Probabilities range from zero percent chance. When you put P, then in parenthesis is in parenthesis. So P(A), means the Probability B. When you take all of the probabilities Space. And that's why we have S and that all % of the possibilities. Also you can talk to say the probability of not A is equal to have to add up. So, let's take a look at something is really important in statistics. A conditional if something else is true. You write it this line is called a Pipe and it's read as assuming the probability of A given B, is the probability for instance, what's the probability if something's caret given this picture. Now, the place that people is the probability of type one and we'll mention at some other point. But I do probabilities because it does not always work by talking about adding probabilities. Let's say you want to find the probabilities of adding the probabilities of the two events. of event A and you add the probability of something, you may have to subtract this little between the two of them. On the other hand occur together, then that's equal to zero. you get back to the original probabilities. I've created my super simple sample space 5 circles on the bottom and I've got a couple we want to find the probability of a square but we have to adjust for the overlap between out of the 10 are squares and over here on of 10. Let's go back to our formula here and the B to S and R for square and red. Now we that something is a square. Well, we go back 5 squares out of 10 shapes total. So we do up the probability of something red in our two of them on the far right are red. That's Now, the trick is the overlap between these both square and red, because we don't want Let's go back to our sample space and we are the squares on top and there's the things overlap and this is our little overlapping of those, one out of 10. So we come back here, we just do the addition and subtraction here. that means is, there is a 60% chance of an at it right here. We have 6 shapes outlined that lines up with the mathematical one we for Probabilities. Now the idea here is you of two things occurring together, simultaneously. to multiply the probabilities. And we can are asking about A and B occurring together, probability of A times the probability of it just a little bit because you can have bit, and so you actually need to expand it of B given A. Again, that's that vertical are independent and they never co-occur, or then it just reduces to the probability of But let's go and take a look at our sample of each kind, and then two that are red. And of something being square or red, now we are square and red. Now, I know we can eyeball the math. The first thing we need to do, is 5 on the top and the ones that are red, and the ones that are both square and red, yes at the top right. But let's do the numbers for square and red, we get the probability so we do 5/10, reduce this to .5. And then it's a square. So, we only need to look at them, and one of them is red. So that's 1 those two numbers; .5 times .2, and what you our total sample space is red squares. And yeah there's one out of 10. So, that just So, that's our short presentation on probabilities Probability is not always intuitive. And also in a lot of situations, but they may not work arithmetic of Probability can surprise people it so you can get a more accurate conclusion discussion of &quot;Mathematics and Data Science&quot; called Bayes' theorem. And if you're familiar you can think of Bayes' theorem as the flip in terms of intersections. So for instance, give you the probability of the data; that's a known hypothesis is true, this will give chance. The trick is, most people actually of the hypothesis given the data. And unfortunately, many circumstances. On the other hand, there's this is our guy right here. Reverend Thomas He developed a method for getting what he prior probabilities. And test information overall to get the posterior or after the to how this works: You start with the probability what you get from the likelihood of the data. test. To that, you need to add the probability That's called the prior or the prior probability. the data, that's called the marginal probability. way to get the probability of the hypothesis Now, if you want to write it as an equation, is equal to likelihood times prior divided like this; the probability of H given D, the that's the posterior probability. Is equal that the likelihood, multiplied by the probability of the data overall. But this is a lot easier let's go this example here. Let's say we have people and we are looking at a medical condition. we got this group up here that represents of all people. And that what we say, is we of them will test positive, so they're marked far left people with the disease who test And so if the test catches 90% of the people let's look at it this way. Let me ask y0u for a disease, then what is the probability want a hint, I'm going to give you one. It's information I gave you before and we've got a conditional probability, they test positive. in the big white area below, 'of all people'. ever test positive, do we ever get false positives positives. And so let's say our people without way they should. But of the people who don't those are false positives. And so if you really positive do you have the disease?&quot;, here's of people with the disease who test positive look at it this way. So here's our information. darker red box, those are the people who have good. Then we have 6.7% of the entire group, test positive. So we want to do, we want the have the disease and test positive and then And that bottom part is made up of two things. disease and test positive, and the people Now we can take our numbers and start plugging positive that's 29.7% of the total population right here. That's fine, but we also need the disease and test positive; of the total to rearrange, we add those two numbers on bit of division. And the number we get is test result still only means a probability is advertised at having 90% accuracy, well 82% chance you have the disease. Now that's this: what if the numbers change? For instance, Here's what we originally had. Let's move much less common. And so now what we do, we who have the disease and test positive. And who don't have the disease, we are going to positives. Again, compared to the entire population are going to go back to our formula here in get 4.5% right there, and right there. And positives that's 9.5%. Well, we rearrange and when we divide that, we get 32.1%. Here's test result; you get a positive test result, of having the disease. That's ? less than tell, that's a really big difference. And it answers the questions that people want depending on the base rate of the thing you this. Bayes theorem allows you to answer the what's the probability that I have the disease. if I have the disease. They want to know whether need to have prior probabilities, you need to know how many people get positive test information and run them through it can change of what you're dealing with dramatically. and Data Science&quot; and the data principles you can do afterwards. Probably the most important a long time ago but now it's a good time to of the principles you've used before. The way in data science. So, things like Algebra O and Probability. All of those are important least a working understanding of each. You need to understand the principles of your projects. There are two reasons for that very if a procedure will actually answer your question. Will it give you the insight that you need? what to do when things go wrong. Things don't up, you got impossible results or things just about the procedure and enough about the mathematics and respond appropriately. And to repeat myself on in data science, no matter what tool you're on your goal. And in case you can't remember meaning. Welcome to &quot;Statistics in Data Science&quot;. be doing in this course is talking about some the unseen. To infer what's there, even when surprised. If you remember the data science we have math up here at the top right corner, of this Venn Diagram, it's full name was math in case it's not completely obvious about the idea is this; counting is easy. It's easy document, it's easy to say how many people of the country. Counting is easy, but summarizing of the problem is there's no such thing as depend on the purposes that you're dealing couple of pairs of words and try to summarize three words. In a word or two, how is a souffle different from a Pine tree? Or how is Baseball different from opera? It really depends on goals and it depends on the shared knowledge. and then there's the matter of generalization. three concerti by Antonio Vivaldi, and do all of his music? Now, I actually chose Vivaldi you could, he said he didn't write 500 concertos take something more real world like politics. US, can you then accurately predict the behavior voters in the US, and that's a matter of generalization. of with inferential statistics. Now there statistics and all of them are described to you're working on. There are descriptive statistics, the inferential procedure Hypothesis testing about each of those in more depth. There are some of the things I'm going to discuss in that's different from estimation. Different which variables are the most important in that arise when trying to model data and the this all, the most important thing to remember to serve a particular purpose. And there's world that says all models are wrong. All because they are not exact depictions, they from George Box. And so the question is, you're because in that case you just wouldn't do better off not doing your analysis than not in sum, we can say three things: #1, you want and to generalize from one group to another &quot;one true answer&quot; with data, you got to be the shared knowledge. And no matter what your guide you in your decisions. The first thing is the principles of exploring data and this overview. So we like to think of it like this, exploring and seeing what's in the world. you want to see what your dataset is like. so you can do a valid analysis with your procedure. want to listen to your data. Something's not then you're going to have to pay attention help you do that. Now, there are two general exploration, so you use graphs and pictures reason you want to do this is that graphics really good, in fact the best to get the overall is numerical exploration. I make it very clear, first, then do the numerical part. Now you precision, this is also an opportunity to do some transformations, move things around see how that effects the results, see how part. They are very quick and simple plots bar charts, histograms and scatterplots, very to understand the variables in your dataset. the graphical method, you can do things like logarithm of your numbers. You can do Empirical use robust methods. And I'll talk about all right now, I can sum it up this way. The purpose your data. And also you want to explore your before you build statistical models. And all listen carefully so that you can find hidden As we move in our discussion of &quot;Statistics thing we can do is Exploratory Graphics. In Yogi Berra, &quot;You can see a lot by just looking&quot;. to baseball. Now, there's a few reasons you get a feel for the data. I mean, what's it strange things going on. Also it allows you your data match the requirements of the analytical for anomalies like outliers and unusual distributions If something unusual is happening in the data, a different angle or do a deeper analysis. of reasons. #1, is they are very information It's our single, highest bandwidth way of to check for shape and gaps and outliers. you want to and the first is with programs programming language R, the general purpose amount in JavaScript, especially D3JS. Or for exploratory analysis, that includes Tableau and even Excel is a good way to do this. And who's the father of Exploratory Data Analysis, it's all hand graphics and actually it's a process for doing these graphics. We start And so you'll get something like this, the when you are dealing with categories and you are in each category. The nice thing about Put them in descending order and may be have Horizontal could be nice to make the labels profiles of the United States, this is real and conventional, a smaller amount in the common of the United States is relaxed and called a box and whiskers plot. This is when that's measured and you can say how far apart it also shows outliers. So for instance this Utah at 5 standard deviations above the national to see that there. Also, it's a nice way to are on proximately similar scales. Next, if to want to do a histogram. Again, quantitative And these let you see the shape of a distribution three histograms of google searches on Data And you can see, mostly for the part normally you've done one variable, or the univariate at a time. That is bivariate distributions to do this is with grouped plots. You can I have here is grouped box plots. I have my United States and I'm showing how they rank As you can see, the relaxed and creative are go to the lowest and that's kind of how that association between a categorical variable and a quantitative outcome, which is what also do a Scatterplot. That's where you have for here is, is it a straight line? Is it strength of association. How closely do the we have here in the middle. And this is an across the bottom, so more open as you go can see is there is a strong downhill association. open are also the least agreeable, so we're And then finally, you're going to want to distributions. Now, one big question here not 3D. So, what I have here is a 3D Scatterplot the left, I have FIFA which is for professional searches for the NFL and on the right I have what's neat about this is you can click and kind of fun, you kind of spin around and it And this particular version, I'm using plotly and see, let me see if I can get the floor and see where it ranks on each of these characteristics. control and once it stops moving, it's not worked with are just kind of nightmares. They So, here's the deal. 3D graphics, like the being shown in 2D, they have to be in motion fundamentally they are hard to read and confusing. clusters in 3 dimensions, we didn't see that them like the plague. What you do want to the variables, you might want to use a matrix many quantitative variables, you can use markers it to be much clearer than 3D. So here, I NBA, NFL, MLB for Major League Baseball and you can see the scatterplots, you can get a much easier chart to read and you can get display. So the questions you're trying to what you need? Do you have the variables that need? Are there clumps or gaps in the distributions? are really far out from everybody else, spikes in the data? Are there mistakes in coding, there impossible combinations? And these kinds that really kind of puts it there in front graphical exploration of data. It's a critical want to start. And you want to use the quick plots are really easy to make and they're with the graphical exploration, then you can the data through numbers. The next step in statistics or numerical exploration of data. you do visualization, then you do the numerical #1, you are still exploring the data. You're exploration. This might be an opportunity parameters as opposed to theoretically based and explore the effect of manipulating the variables. Also, it's an opportunity to check the same general results if you test under talk about things like Robust Statistics, we'll start with Robust Statistics. This by And the idea with robust statistics is that varies in unpredictable ways you still get of statistics, it's an entire category, that's and other abnormalities in the data. So let's that I created. The median, which is the dark I am going to look at two different kinds the Winsorized Mean. With the Trimmed mean, the top and the bottom and you just throw Winsorized, you take those and you move those Now the 0% is exactly the same as the regular or move in 5%, the mean shifts a little bit. 25%, now we are throwing away 50% of our data. we get a trimmed mean of 1.03 and a winsorized 50%, that actually means we are leaving just we get 1.01. What's interesting is how close data left, and so that's an interesting example data, even when you have things like strong And that's like pulling marbles repeatedly them back in and trying again. That's an empirical you get 20% red marbles, sometimes you get are several versions for this, they go by And the basic principle of resampling is also have more to say about validation later. And Here's our caterpillars in the process of here, is that you take a difficult data set There's no jumps in it, and something that on the full dataset. So you can fix skewed a curved line, you can fix that. And probably something called Tukey's ladder of powers. of exploratory data analysis. He talked a ladder, starting at the bottom with the -1, it works, this distribution over here is a and as you start to move in one direction square root you see how it moves the distribution you get to the end then you get to this minus it way way, way over. If you go the other it pushes it down in the one direction and move it around in ways that allow you to, back to a more centrally distributed distribution. you can use in the numerical distribution or numerical exploration allows you to get allows you to check the stability, see how distributions and so on. And perhaps most modelling of your data. As a final step of to talk about something that's not usually statistics. I like to think of it this way. tell a story. More specifically, you're trying statistics, you can think of it as trying of data. Using a few numbers to stand in for consistent with the advice we get from good Simplify. If you can tell your story with data, go for it. So there's a few different to describe the center of your distribution a single number, use that. # 2, if you can the spread or the dispersion of the variability. the distribution. Let me say more about each center. We have the center of our rings here. center or location or central tendency of and there's the mean. Now, there are many, going to get you most of the way. Let's talk a little dataset here on a scale from 1 to There's a one, and another one, and another then we have a score way over at 9 and another this is the distribution. This is actually most commonly occurring score or the most each of these go, we have more ones than anything 4 times and nothing else comes close to that. is looking for the score that is at the center have 8 scores, so we have to get one group of four, this really big one because it's place on the number line that splits those here at one and a half. Now the mean is going people understand means in general. It's the where M for the mean is equal to the sum of by N (the number of scores). You can also like this where that's sigma - a capital sigma N. And with our little dataset, that works plus two plus two plus nine plus eleven. Add how many scores there are. Well that reduces If you go back to our little chart here, 3.5 any scores really exactly right there. That's by its outliers, it follows the extreme scores. a visual analogy, is that if this number were the balance point or the fulcrum would be If somebody weighs more they got to sit in to sit further out, and that's how the mean and cons of each of these. Mode is easy to other hand, it may not be close to what appears splits the data into two same size groups, pretty easy to deal with but unfortunately, any statistics after that. And finally the it's the most effective by outliers and skewness it is the most useful statistically and so there's the issue of spread, spread your tail that are pretty common also. There's the range, and there's variance and standard deviation. The Range is simply the maximum score minus 11 minus 1, which is equal to 10, so we have chart. It's just that line on the bottom from 10. The interquartile range which is actually distance between the Q3; which is the third quartile score. If you're not familiar with score and the 25th percentile score. Really some of the some of the data. So let's go going to do, we are going to throw away the greyed out now, and then we are going to throw there. Then we are going to get the range by the fact that I have this big gap between quartiles do something with that gap. So if to do an interpolation process and it will down to one for the first quartile, so not how it works usually. If you want to write range is equal to Q3 minus Q1, and in our of course is equal to just 2.75 and there or variability or dispersion, is two related These are little harder to explain and a little at least the easiest formula, is this: the sigma that's the sum, X minus M; that's how take that deviation there and you square it, divide by the number. So the variance is, I'll try to show you that graphically. So there at 3 and a half. Let's go to one of and if we make a square, that's 1.5 points a similar square for the other score too. to be 2.5 squared and it's going to be that squares for each one of our 8 points. The to be huge and go off the page, so I'm not those squares you add up the area and you for the variance, but now let me show the measure. It's closely related to this, specifically Now, there's a catch here. The formulas for slightly different for populations and samples they give similar answers, not identical but say over 30 or 50, then it's really going do a little pro and con of these three things. only uses two numbers the high and the low, numbers. And if they're outliers, then you've Range the IQR, is really good for skewed data either end, so that's nice. And the variance the least intuitive and they are the most the most useful because they feed into so science. Finally, let's talk a little bit have symmetrical or skew distribution, unimodal, there's a lot of variations. Let me show you distribution, pretty easy. They're the same pyramid shape is an example of a symmetrical where most of the scores are on one end and skewed distribution where most of the scores the high end. This is unimodal, our same pyramid kind of one hump in the data. That's contrasted modes, and that usually happens when you have There is also uniform distribution where every distributions where people tend to pile up middle. And so there's a lot of different shape of the distribution to help you understand and like the standard deviation and put those you use this script of statistics that allows story and tell it succinctly. You want to the spread of the data, the shape of the data. they can exercise really undue influence on better understand your data and prepare you in Data Science&quot;, one of the really big topics with just a general discussion of inferential with a joke, you may have seen this before the world. 1) Those you can extrapolate from because the other group is the people who from incomplete data or inferring from incomplete difference between populations and samples. every possible case in your group of interest. it might be whatever. But it represents everybody interested in. And the thing with the population it has it's mean and standard deviation and because you generally use those in doing your of being all of the data are just some of with error. You sample one group and you calculate you do it the second time, and it's that variability little tricky. Now, also in inference there which is short for hypothesis testing and This is where you assume a null hypothesis you calculate the probability of getting the is true. And if that value is small, usually which says really nothings happen and you The other most common version is Estimation. intervals. That's not the only version of is where you sample data to estimate a population mean to try to infer what the population mean you have to calculate your values and you work with a certain level of confidence. Now, concept of sampling error. I have a colleague what percentage are red, and you get them talk about this, a population of numbers. population of the numbers 1 through 10. And from those numbers randomly, with replacement. a one and I put it back, I might get the one which actually may sound a little bit weird, behind inference. And here are the samples I got a 3, 1, 5, and 7. Interestingly, that sample is 4, 4, 3, 6 and 10. So you can see the 2, the 5, 7, or 8 or 9. The third sample way at the ends there. And then my fourth were drawn at random from the exact same population, That's the sampling variability or the sampling trickier. And let's just say again, why the because inferential methods like testing and sampling variation to get a clear picture say this about Inferential Statistics. You and as you try to interpret it, you have to ways of doing that. And the most common approaches of parameter values. The next step in our Hypothesis Testing. A very common procedure of it as put your money where your mouth is out testing their plane. Now the basic idea start out with a question. You start out with of X occurring by chance, if randomness or explanation? Well, the response is this, if when nothing's happening is low, then you Okay, there's a few things I can say about research, say for instance in the social sciences, can be really helpful in medical diagnostics, does a person have a particular disease. And go/no go decision, which might be made for school district or implementing a particular to make a yes/no. Hypothesis testing might to have hypotheses to do hypothesis testing. the null hypothesis. And what that is in larger, is no systematic effect between groups, there's error is the only explanation for any observed with HA, which is the alternative hypothesis. effect, that there is in fact a correlation difference between two groups, that this variable take a look at the simplest version of this is a null distribution. This is a bell curve, Which shows z-scores in relative frequency, regions of rejection. And so I've actually and the lowest 2.5%. What's funny about this looks like 0. It's actually infinite and asymptotic. leaves 95% in the middle. Now, the idea is a score for you data and you see where it think of that as you have to go down one path And you have to decide to whether to retain or reject it and decide no I don't think it's You can get a false positive, and this is effect, but it's really randomness. And so you can see a little down hill association has a true correlation of zero. And I just about 20 rounds, but it looks negative but about false positives is; that's conditional a false positive is if you actually conclude the highly descriptive name of a Type I error, or a 5% risk if you reject the null hypothesis, a false negative. This is when the data looks a relationship. So for instance, this scatterplot but in fact this came from two variables that association. Again, I randomly sampled from look pretty flat. And a false negative is can only get a false negative if you get a also called a Type II error and this is a several elements of your testing framework, I do have to mention one thing, big security Testing; there's a few. #1, it's really easy if you get a statistically significant result, And that's not true because it's confounded that don't really matter. Also, a lot of other of a null effect or even a nil effect, that be, in certain situations can be an absurd There's also bias from the use of cutoff. have problems where you have cases that would It would have switched on the dichotomous lot of people say, it just answers the wrong what's the probability of getting this data about. They want it the other way, which is I'll say more about that later. That being ingrained, very useful in a lot of questions So in sum, let me say this. Hypothesis Testing the default in many fields. And I argue it of the well substantiated critiques. We'll discussing Estimation. Now as opposed to Hypothesis give you a number, give you a value. Not just for a parameter that you're trying to get. looking at something from a different way. Intervals. Now, the important thing to remember You're still using sample data and trying population. The difference here, is instead focus on likely values for the population related to Hypothesis Testing, sometimes seen how that works in later videos. Now, I like any sample statistic and there's a few different and Bootstrap versions, that's why I got the of randomly sample from the data, in an effort also have central versus noncentral Confidence going to deal with those. Now, there are three choose a confidence level. Anywhere from say, than zero and it can't be 100%. Choose something it does, is it gives you a range a high and the more confident you want to be, the wider and your low estimates. Now, there's a fundamental trade off between accuracy; which means you're contains the true population value. And the There's a tradeoff between accuracy and what's means a narrow interval, as a small range emphasize is this is independent of accuracy, or both. In fact, let me show you how this situation, I've got a variable that goes from at 50. If you think of this in terms of percentages if you're on the left or the right of 50%. at 55 to say that that's our theoretical true a distribution that shows possible values here is it's not accurate, because it's centered on 45 as opposed to 55. And it's not precise, to almost 80. So, this situation the data one. This is accurate because it's centered really spread out and you see that about 40% side of 50%; might lead you to reach the wrong the nightmare situation. This is when you not accurate; it's wrong. And this leads you of what's going on and you're going to totally is this: you have an accurate estimate where close to the true population value and it's can see that about 95% of it is on the correct see all four of them here at once, we have ones on the top, the accurate ones on the so that's a way of comparing it. But, no matter interval. Now, the statistically accurate this: you would say the 95% confidence interval just kind of taking the output from your computer Interpretation of this goes like this: there between 5.8 and 7.2. Well, in most statistical to bayesian you can't do that. That implies how people see it. Instead, a better interpretation selected samples will contain the population with a little demonstration. This is where with a mean of 55 and I got 20 different samples. sample and I charted the high and the low. population value. And you can see of these it. If you look at sample #1 on the far left; like it made it, sample 20 on the far right, missed it completely, that sample #2, which always just one out of twenty, I actually because it gave me either zero or 3, or 1 what I was looking for here,. But this is say a few things about this. There are somethings The first is the confidence level, or CL. The more certain you have to be, you're going Second, the Standard Deviation or larger standard thing that you are studying is inherently of the range is going to be more variable the sample size. This one goes the other way. The more observations you have, the more precise can show you each of these things graphically. where I am simply changing the confidence and as you can see, it gets much bigger as As the sample standard deviation increases gets a lot bigger. And then we have sample it at each point. And you can see how the as we go through. And so, let's say this to Intervals which are the most common version And the variation in the data is explicitly argue that they are more informative, because value is likely, but they give you a sense that's one reason why people will argue that in any statistical analysis. As we continue we need to talk about some of the choices some of the effects that these things have. is different methods for estimating parameters. of measuring stick or standard are you going common. This is called OLS, which is actually a very common approach, it's used in a lot the sum of squared errors, and it's characterized Best Linear Unbiased Estimator. Let me show here of an association between two variables. distance to stop from about the '20's I a straight regression line right through it. Linear Unbiased Estimate, but the way that the Residuals. If you take each data point to the regression line, because the regression that value on the X axis. Those are the residuals. Residual. You square those and you add them line here will have the smallest sum of the line you can run through it. Now, another And this is when you choose parameters that kind of weird, but I can demonstrate it, and always find the best, I like to think of it looking around them, trying hard to find something, Let me give a very simple example of how this parameters that maximize the likelihood of I've got three possibilities. I've got my blue which is a little more centered and green identical, except they have different means, one that is highest where the dotted line we are doing is changing the mean, and we then the blue one is the one that has the On the other hand, we could give them all their standard deviations instead and so they the red distribution is highest at the dotted Or if you want to, you can vary both the mean And here green gets the slight advantage. because obviously you would just want to center is when you have many variables in your dataset. values that can maximize the association between works with this. The third approach which A Posteriori. This is a Bayesian approach it adds the prior distribution and then it process. What happens, by the way is stronger estimate and that might mean for example larger a greater influence on the posterior estimate is that all three of these methods all connect they connect. The ordinary least squares, when it has normally distributed error terms. Maximum A Posteriori or MAP, with a uniform way, ordinary least squares or OLS is a special likelihood or ML, is a special case of Maximum we can put it into set notation. OLS is a are connections between these three methods just sum it up briefly this way. The standards choices and they determine which parameters Several methods exist and there's obviously many are closely related and under certain it comes down to exactly what are your purposes with the data that you have to give you the The next step we want to consider in our &quot;Statistics to make. Has to do with Measures of fit or have and the model that you create. Now, turns this and one big question is how close is between the model and reality. Well, there's first one has what's called R2. That's kind of determination. There's a variation; adjusted of variables. Then there's minus 2LL, which of variations. The Akaike Information Criterion or BIC. Then there's also Chi-Squared, it's it's actually c and it's chi-squared. And First off is R2, this is the squared multiple And what it does is it compares the variance it looks like the total variance of that and made your prediction. The scores on squared next is -2 Log-likelihood that's the likelihood And what this does is compares the fit of set, than the larger set overall. This approach you have a binary outcome. And in general, Now, as I mentioned there are some variations chocolate. The -2 log likelihood there's the Bayesian Information Criterion (BIC) and what of predictors. Because obviously you're going going to get a really good fit. But you're where your model is tailored to specifically generalize well. These both attempt to reduce again. It's actually a lower case Greek c, examining the deviations between two datasets. and the expected values or the model you create, Now, I'll just mention when I go into the these are some of the most common standards, in sum, there are many different ways to assess your data. And the choices effect the model, for throwing in too many variables relative a quantitative or binary outcome? Those things as always, my standing advice is keep your to fit best with your analytical strategy your data. The &quot;Statistics and Data Science&quot; the most important is going to be feature in your model. It's sort of like confronting to choose what matters most. Trying to get feature selection is to select the best features variables and simplify the statistical model overfitting or getting a model that works well with other data. The major problem here has to do with the relationship between the it to you graphically here. Imagine here for the variability in our outcome variable; we're predictors. So we've got Predictor # 1 over that's nice. Then we've got predictor #2 here, but it's also overlaps with Predictor 1. And 3, which overlaps with both of them. And the and the outcome variable. Now, there's a few pretty common. So for instance, there's the and regression equations, there's standardized regression. There are also, there's newer of the association between the predictors. there's Dominance Analysis, and there are are many other choices in both the common are worth taking a special look at. First, the simplest method, because most statistical for each predictor and they will put little is you're looking at the p-values; the probabilities next to it, which sometimes give it the name through a large output of data, just looking a problematic approach for a lot of reasons. and it inflates false positives. Say you have an alpha or a false positive of 5%. You end false positive in there. That's distorted sample anything can become statistically significant. problematic approach. Slightly better approach coefficients and this is where you put all standardized from zero and then to either of 1. The trick is though, they're still in separate them because those coefficients are as a whole. So, one way to try and get around Where you look at the variables in sequence, that'll allow you to do that. You can put them in blocks and look at how the equation in fit in each step. The problem with a stepwise the risk of overfitting which again is a bad And so, to deal with this, there is a whole include commonality analysis, which provides contributions of each variable. Well, that's it just moves the problem of disentanglement off then you were as far as I can tell. There's subset of Predictors. Again, sounds really combinatorial explosion. If you have 50 variables have millions of variables, with 50 variables, you're not going to finish that in your lifetime. standard errors and perform inferential statistics something that's even more recent than these weights. And what that does is creates a set each other, basing them off of the originals can predict the outcome without the multicollinear It then rescales the coefficients back to Then from that it assigns relative importance predictor variable. Now, despite this very that resemble dominance analysis. It's actually plug in your information and it does it for with a problem multicollinearity and trying variables. In sum, let's say this. What you're most useful variables to include into your reduce the noise and distractions in your to have to confront the ever present problem the predictors in your model with several step in our discussion of &quot;Statistics and problems in modeling. And I like to think the rock and the hard place and this is where include things like Non-Normality, Non-Linearity, talk about each of these. Let's begin with like to deal with nice symmetrical, unimodal sometimes you get really skewed distribution they happen pretty often, they're a problem gets thrown off tremendously when they have they assume the symmetry and the unimodal way of dealing with this as I've mentioned the logarithm, try something else. But another if you have a bimodal distribution, maybe that got mixed together and you may need to a little bit more. Next is Non-Linearity. we like to put straight lines through things But sometimes the data is curved and this here, but a straight line doesn't work with of many procedures especially regression. one or both of the variables in the equation the relationship between the two of them. include curvature like squares and cubed values, of multicollinearity, which I've mentioned predictors, or rather the predictors themselves is, this can distort the coefficients you it turns out are less affected by this than might be to simply try and use fewer variables. need all of them. And there are empirical perfectly legitimate to use your own domain To use your theory to choose among the variables the problem we have here, is something called combinations of variables or categories grow something about this before. If you have 4 then you have 16 combinations, fine you can doable. If you have 20 variables with five you have 95 trillion combinations, that's computer. A couple of ways of dealing with your own understanding of the domain to choose potential to inform. You know what you're is, there are data driven approaches. You Carlo model to explore the range of possibilities of each and every single one of your 95 trillion explosion is the curse of dimensionality. things that may only occur in higher dimensions until you have these unusual combinations. but the project of analysis is simplification. different things. You can try to reduce. Mostly your data. Reduce the number of dimensions trying to project the data onto a lower dimensional of a 3D object. There's a lot of different methods. And the same method here, a Markov a wide range of possibilities. Finally, there a big problem. Missing data tends to distort group that's missing. And so when you're dealing check for patterns and missingness, you create a variable is missing and then you see if variables. If there's not strong patterns, put in the mean or the median, you can do Imputation, a lot of different choices. And have to talk about in a more technically oriented problems that can come up during modeling, assumptions at every step. Make sure that check for the effects of outliers, check for what you have and use your analysis, use data the theory and the meaning of things in your of dealing with these problems. As we continue that are Made&quot;, one important consideration that as you are doing your analysis, are you you create through regression or whatever you've optimized it there. But, will it work is the question of Generalizability, also are trying to apply in other situations, and work in other situations. Now, there are a trying to get some sort of generalizability. Replication. Then there's something called I'll discuss each one of these very briefly and the idea here is you want to get what give you the probability value for the data with an assumption about the hypothesis. But by combining it with special kind of data the data. And that is the purpose of Bayes Another way of finding out how well things That is, do the study again. It's considered The question is whether you need an exact similar in certain respects. You can argue do is when you do a replication then you actually is the first study can serve as the Bayesian you can actually use meta-analysis or Bayesian of them. Then there's hold out validation. on one part of the data and you test it on in separate baskets. The trick is that you to do these two steps separately. On the other competitions, as a way of having a sort of a model. Finally, I'll mention just one more the same data for training and for testing of it, and the idea is that you're not using through and weaving the results together. one case at a time, also called LOO. There's number at each point. There's k-fold where groups and you leave out one and you develop And there's repeated random subsampling, where of those can be used to develop the model and then cycle through to see how well it in sum, I can say this about validation. You how well your model holds up from the data Because that is what you are really trying validity of your analysis and your reasoning utility of your results. To finish up our and the choices that are involved, I want but more an attitude. And that's DIY, that's really you just need to get started. Remember everybody has data. Everybody works with data so is Data Science. And really, my overall of people think you have to be this cutting true, there's a lot of active development stuff. The trick however is, the software lags. It'll show up first in programs like in a point click program that could be years. edge developments don't really make much of They may in certain edge cases, but usually say analyst beware. You don't have to necessarily so you don't have to wait for the cutting to pay attention to what you are doing. A &quot;Know your goal&quot;. Why are you doing this study? hoping to get out of it? Try to match your on the usability; will you get something out with. Then, as I've mentioned with that Bayesian Remember that priors and posteriors are different Now, I want to mention something that's really the trolls. You will encounter critics, people grumpy and really just intimidating. And they do stuff because you're going to do it wrong. the critics can be wrong. Yes, you'll make tell you how many times I have to write my want it to do. But in analysis, nothing is I've mentioned this before, everything signifies. The trick is that meaning might not be what have to listen carefully and I just want to your listening carefully. In sum, let's say is not is your analysis perfect, but can you fundamentally, data is democratic. So, I'm and that is just jump write in and get started. &quot;Statistics and Data Science&quot;, I want to give Mostly I want to give a little piece of advice Kirk Whalum. And he says there's &quot;There's something you can do to try things differently it also works when you're dealing with data. datalabb.cc that you might want to look at. overviews on things like machine learning, encourage you to take a look at those as well, the field. There are also however, many practical statistical procedures I've covered and you and other programs. But whatever you're doing, writers in mind, and that is &quot;Write what you and analyze and delve into what you know. and the Venn Diagram, we've talked about the part on the bottom. Domain expertise is just to work with computer coding and the ability skills. But also, remember this. You don't have to be perfect. The most important thing Thanks for joining me and good luck!