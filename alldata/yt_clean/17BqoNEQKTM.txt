Intro from the previous benchmark, where I compared to ensure a fair comparison for both databases. In the first test, we'll measure the latency We'll also evaluate the throughput of each can execute per second. Since most databases we'll monitor the number of disk operations disk usage to see how efficiently Finally, we'll measure the CPU Test Design to simulate a real use case for an e-commerce The first table is customer, which stores holds the inventory of the online store. We also items before purchasing them. The cart_item table the customer makes a purchase, everything from along with the related items in order_item. When a user opens the online store, we with an INSERT query. We link the customer to the and initialize the cart's total value to zero. cart, we perform another INSERT query. This using the cart_id and product_id as foreign When the customer adds a second product to instead of hardcoding the values in the Next, we need to update the cart's total value the sum of the values of both products. and pays for the items, we create an order. to the order table. As each product is added the product table to reflect the new inventory. table, again using the appropriate foreign keys. Once all items are transferred to the order, we After that, we delete the second item as well. and DELETE operations. In a real online store, you'd use transactions to group multiple For the second test, based on feedback from the Although it's still a simple join, it involves The query selects a customer and displays the we generate a random ID for each query execution. Now, I want to share the improvements I I received. First, I'll standardize all These instances are specifically designed For this video, I used a large instance with I may scale up to xlarge or even bigger I really like these instances, and we also distributed databases like Cassandra. The not only fast but also included On the other hand, if you use generic EC2 File System) volumes with unlimited storage or that, with these options, you're not only paying costs for disk operations and throughput. This can Database Configurations the same latest versions as in the previous In the previous benchmark I mostly used default for production use. Based on this feedback--and For PostgreSQL, I used these new pgtune to optimize the configuration For MySQL, I used the MySQLTuner Perl on the recommendations it provided With these optimizations, I believe the databases Tables mention that when you create a table with foreign for each foreign key column in the database. you need to manually create those indexes. was higher in the previous video. MySQL had while PostgreSQL didn't have that overhead. latency during data insertion. But when reading Source Code database/sql interface to run the tests. First, follow exactly the same test workflow for both that I use the RETURNING keyword in PostgreSQL create. Since MySQL doesn't support this, I use You can find the full database schema and the repository. Alright, let's go ahead and run the test. but as always, I'll compress it to you can see that all INSERT, UPDATE, and DELETE The insert latency between the two databases is with updates and deletes, where MySQL takes at You can also notice that MySQL performs many and its CPU usage is significantly higher. In memory usage should be close to 80 or 90% of the as shown in this video, they don't utilize much of the VM. Maybe if I ran it for a week or so, When the load reaches around 200 queries per to rise, along with other queries. MySQL can no longer process new Let's see how much further PostgreSQL can go. per second, when it also starts to degrade and usage jumps from 30% to 100% in just a few which only reached 40 connections per connection count to 300 for both databases, so the Now, let me open each graph First, we have the queries per second graph. Then the update latency graph, where you can see Updates are significantly slower in MySQL. Next is the disk write Then the disk usage graph. And finally, the memory usage graph. in insert, update, and delete performance are case that I implemented for the benchmark. Alright, let's go ahead and run the second test only measure the latency for the SELECT statement, second, connection pool size, and memory usage. still there, but it's much smaller compared to remains consistent. At around 6,000 queries per a few more clients to generate additional load. As inserts, updates, or deletes, and both databases However, at around 12,500 queries and started failing some queries. we found PostgreSQL's limit as well. for the entire test duration. Next is the CPU usage graph. Then we have the connection pool size connections per application, and they are And finally, here's the memory usage graph. operations and need to perform a lot PostgreSQL is the clear winner. However, if your MySQL might also be worth considering. databases perform with big data--at least 1 or 2 PostgreSQL seems to be the better choice. you might find interesting. Thanks for