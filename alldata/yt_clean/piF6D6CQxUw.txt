The amount of random access memory - RAM - in That's the part that directly interacts with the say, another terabyte or so of memory. But the human brain - have? And can we even measure question we should ask. However much memory the memory in a computer has a physical location. To have to know the binary address associated with comes down to turning on just the right wires Now imagine a different kind of memory. its binary address, how about we could specify if we provide an incomplete version of the memory, with the right software your computer can already on its basic level. The point of this video is also known as *associative memory*, is a kind With that it'll become clear that it memory capacity in networks of neurons in The biggest difference might be: a fixed location, but as we'll see, the memories Computer memory is measured in bits, binary such bits can represent anything from letters them as patterns of this kind, like these 64 bits always find that there's a piece missing from the how do I get to a memory once it's saved, say It's only when we retrieve it that it becomes I'm glossing over tons of technical detail here, address. And this binary address really eventually that are either turned on or off. Each piece of only be retrieved by knowing its address. How the is really the meat of programming and is another fact that memories are matched to addresses and Contrast this with what we believe about the like a CPU and there aren't any addresses. Rather, independent units called neurons. In this video, activity of networks of neurons by introducing a named after the author of this 1982 paper. more generally this video aims to be a lesson the art of the essential. This is a neuron. this before. But it sometimes pays to remember to understand things about the brain. The reason it integrates electrical signals from other it broadcasts that activity back to the network. there's electrical signals coming in from other Then the synapses act as multipliers on these activity of the neuron is based on the sum of the it's fine to apply some function after computing once we turn this into a network, connecting the This is a special type of neural meaning that there are back and forth connections remember that any such edge is actually two edges Okay there's details here that we need to do with memory? Well it needs to be somewhere of an associative memory, which is the ability Let's try a definition of memory that's Let a memory system be a system that, a configuration, it has the ability to return from earlier actually has this property if Our network seems different though. So let's get lives that fall under our definition of memory and plastic bottle. If it's crushed, in other words to its earlier state, which in that sense could be not arbitrary. I actually do think that networks a neural network is a system with a pattern of we could construct our network such that it would that state over time if it was perturbed, then This is a network of 64 neurons that I this pattern of 8x8 binary pixels. So what's going we need to take the following steps. Remember I the activity of the network changes over time. And we actually imprint memories into the network? And the neurons. Finally, we need to understand if and The crucial ingredient of our network really activity changes over time. By activity we mean is described by a number and that this number is time moves forward in discrete steps. Furthermore, we'll assume that activity means that the inactive, say minus one, and active, say 16 minus or plus ones at any given time, What actually happens if we Imagine folding out the time dimension in space. update its state according to the input The rest of the neurons stay the But hold on you, might ask, how and why update only one neuron at a time? Well update all neurons at once but the issue here a global updating signal, almost like a clock, It's slightly more realistic, although not asynchronously. Okay and for the other question Well it's remarkably simple. It's a weighted sum meaning that each state is multiplied by the And since the connections in that sense &quot;weigh&quot; call them &quot;weights&quot;. But then of course to ensure we'll make use of that function I Those of you familiar with linear algebra the dot product between the vector of neuron This is a network of 64 neurons and I'm just that it has memorized this pattern. Starting running the equations I just described, selecting activity - and wait let's just make this a little this intriguing property that it gravitates well, it ends up with the anti-memory in some certain symmetry in the network that we will get it doesn't change anymore. The memory of the network. So can we be sure that things start to get complicated when there is more to that. but for the simple case of just a single the memory or its anti-memory or, as an edge case, networks are just vectors that evolve in time and some configuration of weights, there are stable and we have seen, although not proved, that the That leaves the following question: how do we make words, how can we design the stable states of the the network, which turns out to be a matrix with example, the state of this neuron is determined by this might seem impossible, especially if we wish So I'm just going to tell you the Given a desired memory state with, say, eight We will just say, seemingly out of nowhere, that is determined by the product of their states in algebra: this means that the matrix is an Except for the diagonal which we set to 0 since we of it this way: our whole approach was in some simple parts. So there really should be a way for them independent of the rest of the network. and it is exceedingly plausible because remember synapses in actual neurons also would have no way And so why the multiplication? Well here comes and plus one. If you map out all four combinations weight will have positive sign whenever the if they disagree. And this should make sense its state onto other neurons and a negative weight It's almost like the neurons behave like charged can see it in action, and that's - how do we store We do it by computing the outer products for all for each and then we average those matrices. This Now however things start to become There's no way to guarantee that all The memories will start talking to each other, and super interesting at the same time. Here's converges to one of the memory states, yes, in between, a merging of memories, which I And with this we are finally making some beginning of the video: how much memory do you giving some number of bytes, but now the question how many memory patterns can we store in a they are stable states of the network? And the The original paper showed that this model has only of stable states grows as a linear function of assumption in this graph, too, which is that all of pictures like this is totally not the case. I images to really show what this network can do Realistically, that means that this model is maybe given all the violence we did to these networks of this video wasn't to convince you that this although in a follow-up video, I want to tell actually useful for deep neural networks. What to warn you of false comparisons. Networks and why should they? But secondly, it's to show thin line between complexity and simplicity, differently than otherwise we would have. You static, but I'm hoping now you're willing to the invisible stable states of you