Hi everyone, welcome to part five of the Azure Master class V2. And in this module we're going to dive into storage, understanding what are the different types of storage requirement we may have and what are the services available in Azure to actually go ahead and meet those requirements. But then also what's special about storage, when I think about using it with a virtual machine and what I say virtual machine realize many other services build on top of that virtual machine. Foundational block, virtual machine, scale sets, AKS, node pools, app services, database services. Many of those build on this humble virtual machine. So when we think about the storage available with a VM, that applies to many other types of service. And then what are some of the storage tools? Now if I think about it. They're all very different requirements we have different workloads have needs for or sometimes it's ephemeral, it's just temporary. Maybe it's a cache of some kind. There's data stored in a durable, IE long-term survives, reboots, power off, etc. But maybe I have some local cache of it to improve performance. But I don't need to store it in a durable service as a power off type way because I can repopulate that data. When I start up so sometimes we need ephemeral storage. It's not that long term durable. Sometimes hey I need it durable. I need to make sure it's always retained no matter what that type of event is. The types of data we might have vary greatly as well. For example, if we think for a second around, let's just jump over to the whiteboard. So I could think about what are some of our types of data. And there are three key types that most things fall into. We're used to the idea of structured. And we typically associate the idea of structured with. Well, there's some schema, there is some well defined blueprint of. These are the columns. This is the type of data can be stored in each column, integer, a string, whatever that might be a date. And then the table consists of number of those columns and it has entities records within it. And this is when we think of a database. So we have this very structured type data that's very useful when I want to be able to interact and do queries against it, types of transactions. Then I also have things just like media files, a document. A PDF. Images, videos, audio tracks, blobs, binary, large objects. These have no fixed structure, so we'd say they're unstructured. So I need services to be able to handle just, hey, I want to store some stuff. You don't need to understand what it is, but I need to be able to store it. And then we have almost an in between. We have things like XML, JSON and we think of these as semi structured. They're not adhering to some very strict schema in the traditional sense, but they do have some kind of structure. They might have some kind of entity, some kind of hierarchy. Typically they're self describing in terms of the actual data, the attributes they have. So we have these different types of data available to us and depending on what we're doing, we need different services to actually go ahead and support those. So the different capabilities are needed across and to support the different types of data based on those applications. And there really isn't one better type of storage. Hey if I want to store big media files, well I want an unstructured data store. If I have records about customers and sell orders, hey I want a structured data, I want to database. If I have some configuration type files that's probably a JSON, hey I want semi structured so it's not there. Is one better than the others? They are used for different scenarios. So let's think about the idea of, well, Azure storage. There is a base Azure storage account service. Azure does not use traditional storage most of the time. There aren't big storage area networks that maybe the EMC. They're netapp sans we're used to in data centers. I say mostly because there is an exception service that's available, which we will talk about. Instead, there's this three tiered architecture model based around storage stamps. Now you hear me use the word stamp or cluster. Azure divides up its hardware into racks with servers in them that are managed by certain controllers, and we can think about certain groups. Those racks make up a cluster. It could be a compute cluster that runs VMS. It could be a storage cluster that provides storage services. So we're focused on the idea of a storage stamp storage cluster. So this uses this idea of layers, says a streaming layer, a partition layer and a front end layer. Now you do not have to understand the details of this. I stress this only so you understand it's not using some sand. It's built around an architecture that's designed for these huge hyperscale clouds, hyperscale performance type scenarios. And the goal of it really is that we have this stream layer. They're the bits on the disk that's responsible for distributing and replicating data across servers within the stamp. So it's durable. It's never one copy of your data. A minimum. There's three when we talk about Azure storage. Then we have the partition layer, and this understands higher level abstractions like a BLOB, a table, a queue. It gives you this scalable namespace into which you can interact, then the front end responsible for actually taking those requests. They're stateless servers. They take the request, they might look up account names, they do some authentication, and then they route it through to the partition layer to actually go and get those bits. DNS is used for all of the namespaces. So when you go and look at a storage account, what you'll see always is these endpoint names. In fact, if we super quickly go and look at a. And just the browser just jump over to this quickly. So if I was to go and look at, hey, I'll look at my favorite storage account. I use nearly every demo and if we go down to my settings, one of the things we see is under settings we have endpoints and we'll see. Azure Storage account supports many types of service and I'll see there's a different DNS name for each of the types of service. Now what you're seeing common? Is the name of the storage account. That's why storage account names have to be unique across all of Azure. And then you'll alwaysseethis.corewindows.net at the end, but then in the middle you'll see the service type so I can see BLOB, but then if I keep scrolling down well I can see one for files. I can see one for Q. And I'll see one for table as well. So we see these entries. So there's the DNS name. For each unique service. Within the storage accounts, all of the ones that are available, hey, I'm going to see those. Data is actually replicated in two ways if I've enabled geographical redundancy. So at minimum it's always replicated within the stamp I within the region I create my storage account. So this is synchronous. There's never a risk I'm going to lose data through some unplanned failure, and it could be synchronous because it's within a fairly low latency window. So this is done at the stream layer. And it makes sure, hey, there's a change and update a write operation. There's three copies of that operation. So that's within the cluster. I can also turn on Geo redundancy, so it's replicated to the paired region. We talked about this in resiliency hundreds of miles away, but that's asynchronous. And then there's three copies stored in that other region as well. So that doesn't the partition layer when I'm sending between stamps into stamp, that's done up there. So now I can think about, well, what is that unit? So we have a storage account. This is the top level namespace for our storage services. You saw that in the DNS name, you saw it was, hey, the name of the storage account and then the service. And that core.windows.net. There are different options we have. A storage account always lives in a region. Now again, if I've turned on that Geo redundancy, there'll be a replica to the paired region, but that primary where it's normally available from, that's going to be in the region in which I create the storage account. There are different types of storage account. Really this divides up into a general purpose V2 type storage account which can support many many different types of BLOB and queue and table and file, which is all the types of service or it's a premium type when it's premium, it's SSD based, it's higher performance, lower latency, it will support one of the types of service, and we'll see this if we actually go and look to go and create the storage account. So if I go back to the idea of. Just hey, let's look at my storage accounts and I want to say hey, I want to go and create one. Well, firstly, as always we have to create it in a resource group, but the name if I try to just do. Store for example, it's already taken. Remember these have to be globally unique and it can't be for example uppercase characters, it's only lowercase and numbers between 3 and 24 characters. And then I have to pick the region so it exists in a particular region. So I'm just going to make this savill. Savill or text or one, I'm not going to actually create this. But what you'll see is this idea of a performance. Standard lets me use all the types of service and it's going to be this general purpose V2. Now there is a V1. There is no reason I would want to use a V1 anymore, so why would it's not even available as an easy option in the portal. I could do things through I think the command line or a template, but you just don't want it. So this is hey standard. I might use many types of BLOB and queue and table or. I can say well, premium, but if I select premium, notice now I get this extra drop down and if it's premium it's one of the types. It could be block blobs, it could be SMB or NFS file shares, or it could be page blobs. So it's no longer multi types of service. When I pick premium it becomes very, very specialized in terms of what it will actually support. But again if I go standard that all goes away. Obviously it's gonna cost more money. So if I do go and use, for example, that idea of premium, I get the better service, it's going to cost me more and maybe the way it's provisioned actually differs as well and we will talk about that. Now I can have different types of resiliency. This is the redundancy of my data. Now, this affects a number of different things. It might impact my ability to interact. I get read access to the data. There's a certain service level agreement about being able to actually interact. Then there's a very different SLA about the durability. I'm making sure no bits, no ones, and zeros get lost. The durability is going to be much, much higher. Then the SLA to be able to interact with the service, because you could think there could be problems with those front end servers, there could be issues with DNS that's going to impact my ability to get and talk to the data, but the actual bits on disk are still safe, so there's very different Slas. So if I went and looked for example at durability, just super quickly. Durability is about all. How safe is the chance of losing any bits of data? And even the lowest LRS is 11 nines. And then as you start going into GRS and ZRS, well things just get higher and higher. Then it comes 12 nines with ZRS. And then GRS, so what's the chance of the primary region and the secondary region going down as well? It's going to be even higher. So it's 16 nines I you're not going to lose there's, I don't think there's ever been a loss of data in Azure. But that's the durability, the guarantee if you will that I'm not going to lose that. But the SLA would be very, very different. The actual ability to interact will be lower than that. I mean, we can actually go and look at what the SLA of the storage service is. But you can see, hey, what's the chance of getting read access? Well, hey, look, just for regular tiers. It's 4 nines. It's lower if it's called an archive, so that's totally different. The ability to interact is lower. As we would expect then, the assurance that the data really is safe on the disk. So understand there's a big difference between those things, but we do have control of that resiliency. What is the redundancy of the data? So let's jump over to our whiteboard for a second. So. What I want to think about here is just a whiteboard over I didn't open. There we go. We create inner region. Now remember when we think about a region. A region is actually made-up of many physical data centers. And we often talked about availability zones, and I'm just gonna pretend for this instance, this region supports availability zones. So there's a group of data centers that make up each AZ with independent power calling networking. So I can say, hey, this is AZ one, AZ two, and AZ 3. Now, one of the types of data that we could have is. We'll start with. LRS locally redundant storage. With Los, there were three copies of the data, but they're all within the same storage stamp. Said my 3 copies. So potentially. If that whole data center went up in flames, I could I could lose my data. But more likely what could happen is there's some problem with power or calling and I can't get to it for a period of time. Then I can think about there's ZRS. So if zone redundant storage, there's still 3 copies. But now those 3 copies are distributed over the three AZ's in that region, so now there's an increased ability to get to it, even if 1A has a particular problem. And then we have the idea of GRS. So we've GRS. Let's use a different color range. This is Geo globally redundant storage. Now remember. Regions are paired. So here this is region 2. And this is the paired region. Of Region 1, so these are pairs, which means they're typically hundreds of miles apart. And region two it also has. Multiple data centers. I'm just going to join its AZ one because for the purpose of what we're talking about, if I do GRS well, there's three copies of the data in the same storage stamp. And then it's asynchronously replicated to a storage stamp in the paired region where there were three copies in the same storage stamp there as well. Now when we say the same storage stamp, remember the storage stamp is made-up of fork domains, racks, so it is distributed over different racks in the same cluster. So there's still a certain amount of blast radius protection there, but they're within that same building so to speak. And the next thing we can do is combine. So I can say GZRS. So we have GZRS. It's zone redundant so it's spread over the three AZ's. And it's then asynchronously copied, but on the other side it's still in a single storage stamp. So those are the key combinations we have. Now one of the optional things I can turn on is when I have the geos or the GZRS there is the ability to say hey I want read access RA so it's is optional. I don't have to have this. And if I turn this on, what this now lets me do is there's going to be a second set of endpoints for the BLOB, the queue, the table service is not files, files I can't do read access to the replica, there'll be a second set of endpoints so my application could actually go and read. From this one. Now there's actually a failover. Then the endpoints the DNS changes to point to this copy of the data. But ordinarily, hey, it's pointing here. But if I do create it or change it to be this read access type of geos or GZRS, then I get a second set of endpoints over here. And if we go and look quickly at the portal for a second. Let's go over here. So let's close those down. If we go back and look at our storage account, our favorite friend Savill Tech and if we went back and looked at firstly the configuration. So it's a general purpose V2 and if I look at my redundancy. I have chosen that read access Geo redundant storage and you can see it's showing me how your primary is South Central US. And your secondary is North Central US, so those are the paired regions. And if I go ahead and look at the end points because it is that read access, I have secondary endpoints. And all you'll see what it does is on the name. It's the name of the account secondary. So I could actually go and connect to that and you will see that for BLOB you don't see it for files, no, it's files has no secondary. But you will see it for queue and you will see it for tables and I will see it for Data Lake as well, which we're going to talk about. So I can turn option on that read access for that type of storage if that's what I want to do. So we have those capabilities available to us really at any time and I can control. A lot of that actual failover and I can see the current state of where we are. So for example, if I go and look at that redundancy again, well, this is actually not a good account. I've got certain features turned on that blocks me having as much control. But if I was to look at this other storage account and go to redundancy where I don't have some of those features, one of the things you'll see I can do here. Is prepare for failover. I have control of performing the failover and what it was going to show me when was the last asynchronous replication. OI can see hey your last sync time is now. If I initiate the failover by typing yes in this box down here, potentially I would lose any data that was changed after this time. And they tell me what exactly is going to happen. It's going to switch to an LRS and then I could convert it back to GRS so you can switch accounts between them. So notice here it's giving me options. Hey, I could switch it from goes to Los. I could enable read access GRS. It's blanking out all of the zone ones. So the way it works today is the zone ones. You can convert them, it's a support request. There is even a self triggered option that is available to you. But it is a different beast. It's a different process today because of the way the data then has to get distributed over the zones instead of just that local storage stamp. It's not easy. It's just add an async replica and remove it. Then take the three copies and distribute them over the three availability zones both directions so it can be done. But it's going to be a conversation to actually make that happen. OK. So as we go back. So we do have a different job abilities now. Those are the paired regions. I have no control. There's nothing else I can do about these. Well, there is something I can actually do for my block blobs. Now I've not talked about the types of service yet, but we are going to cover these in some detail. But for one particular type, block BLOB, which is very, very commonly used, you're going to see this as probably one of the key services we ever leverage. We actually have a different replication option. So one of the things I can do with block BLOB is thing called object level replication. I could still have this going. But I can do something extra. I can do object level and if I think of the storage account, so I've created a storage account. I'm blobs, live in containers. So I might have for example a container one in the same storage account and a container two. Remember also that the storage account lives in a particular region, it's a region 1. And I want to point out this is block blobs only block BLOB. Now what object level replication lets me do is hey look there's another region region 2 and another region. Region 3 and I created a storage account in this region storage account 2. Storage account three etc etc. I created a container, container a, container B. I can configure a container level. Hey I want you. To replicate to this, but I want this other container to replicate to here. It's not having to be the paired regions. I have complete control of this now, so this is a very powerful capability for block BLOB that I can now break away from those paired regions and I can essentially just have my own replication however I want. So if we go and quickly look at this again. Let's go over here. If I go back to my SavillTech account. You'll see it as object replication. Right here. And all I do is I specify, well I want to create replication rules. So as part of the rules I could filter this. I could say hey I only want to filter things that are on from a particular source container. With maybe this in the name I have full control, but I'm going to say hey where is the destination. I'm storage account, which one do I want to use? So I might say, hey, I'll do the Savill Tech 10 there and then all I have to do is say, well. Where? From what container? From? So I might say from my source container to the target container. I pick the wrong one, whatever, I'll pick images. So it's a target container. I could add a filter to the prefix. Maybe the container has to be, I don't know, images. Slash or and start with the letter. AI have a lot of control over this. And then once I created this it would start replicating that content. And I can also change. Hey, is it only new stuff I add? Is everything or is it some custom configuration based on some date time? So this would now create a replication rule that as content errors created or that it exists already. It would now start copying it to this target container in. This storage account does not care about the fact that it is or isn't the paired region, so that is a nice option available to me. So we have a lot of different redundancy options, either native use the paired region or with block BLOB only. I can have a container level going anywhere I want. It's only to one. Each container can go to one other container. I can't do multiple chains of replication or multiple targets, it's a one to one. There's different API supported, so there's different ways to interact. There's restful APIs, it could be for something like files I can do SMB, I can do NFS for BLOB. There's different APIs exposed depending on which services I exposed. So there's different ways to interact. We're going to talk more about this when we talk about the actual services. There were two account keys. And again, we'll come back to this. We talked before about governance, about control plane. And the data plane. So no matter what my rights are on the control plane, those APIs, those protocols don't care about that. To interact at the data plane level, one option is I can use these account keys, there's always two of them. On the storage account, there's two of them because the idea is that with two. I could well I could be using one for my application. And then if it's exposed in some way, I could switch to use the other key and then rotate the primary key so I can go and look at my access keys here. Now this is not actually showing them, but I could show the key. And I could use that to connect and do anything I want, and then if it is compromised in some way, there's always this nifty hey rotate key. So while that's rotating, I would shift and use key two. So the reason there's two is that I could switch and rotate the other one. So that's really the whole goal around having those multiple keys. With our new options around data plane role based access control. Pretty good. Try and steer clear of those and we will. Go that more detail, but realize they are there. Treat them with huge respect. Guard them away or disable them completely, which we will do. Now the actual capacity, the maximum amount I can store, the performance I get, the throughput I get is going to vary completely on what is the performance tier. Is it that general purpose V2 standard or is it one of those premiums? And in the Azure subscription and limits page, which we showed in previous modules and is in the link below. It will go through what those actually are. So if we actually quickly looked at this, so this is that page. We're going to come back to this over and over again. But for example here it's talking about storage limits. Talks about maximum storage account capacity. For example, how many requests can I get per second? Well, what's the maximum ingress for a general purpose V2? And then if you keep going down, you'll see all the different types of capabilities based on all the different types of account, the premium ones I might actually do. So if you're curious about what they are, you can go and look now for some of them. The performance is going to directly scale with the capacity. So it's going to be, hey, how big do I provision? Now it doesn't care much I'm using it. What is it provision for? And that will be my performance. And different services behave in different ways about that. But realize hey, it's definitely going to vary. There's also different access tiers, so if we think, for example, our interactions with data are different. I might have the idea that there's certain data. I'm constantly communicating with, so if I think tiers for a second, different access tiers. Now. Remember they were the premium account types and that general purpose. The tier availability varies greatly, so if I think for OK so I've got access tiers, then there's also the idea of the account type. And we're really focused on is a general purpose V2. So there is the idea that, hey, I'm premium, but if I'm premium, I'm just premium. There's no other type of tearing available to me. I'm just a premium account type, but if I'm a general purpose V2. Then I have different access tiers available to me. Now at the top we have hot. And when I talk about, I guess at the top I should be specific. These are focused on BLOB. Because also Azure files, it has some different tiers as well available. To me that's a different type of service in the storage account. So what is data I'm constantly. Doing transactions against. I'm constantly reading it, writing to it. And the reason you have these, if it's hot, I pay more for the storage of the data, but I payless for the transactions. So this is the most cost optimal way for me to use it. Hey, I'll pay more to store it, but I don't have to pay as much when I actually interact with it. Then there might be data that hey I need to have immediately available to me, but I don't intend to interact with it very often. So our pay less to store it, but I'll pay more for the transactions against it. So that's the point of call. And then we actually have archive. And a key point of archive is this is offline. When I move it to archive, I no longer have a media access to it. If I want to be able to read the data, I have to hydrate it back into cool or hot to actually get access to it again. So this is still drawable, it's resilient, but there is no instant access and it may take many hours. There's a priority mode. Well, I think it's an hour depending on the size, but it has to rehydrate this back and I can absolutely move. Data blobs between the tiers. But realize this direction from archive to this is ours. There was a priority mode where I think, again, I think it's an hour based on the size, but it's a fairly long amount of time. When we think of files, so Azure files has the same idea of hot. End call. Should these blue again? But it actually has something above even hot. I don't know how we draw that. It has transaction optimized. And once again, the whole logic of this is simply I set this at the share level. So with the BLOB, this is per BLOB in one general purpose storage account in one container. One BLOB could be hot, one BLOB could be cool, one BLOB could be archive. We've files. It's at the share, so I configure the share. As one of these. So there's a big difference between those. When I think about those actual types of options available. So we have these there and if we looked at the pricing, so we jump over for a second. We we can see that in place. This is BLOB storage pricing. And what you'll see straight away is, well, this is the amount you pay to store it. So obviously premium is always going to be high. Because premium is this super low latency, very very high performance. But hey, hot is kind of two cents per GB, call is only one cents per GB, and archive is just dirt cheap. And was at 100% basically. Was that 10% temp percent per GB? So it's very, very cheap, but again, I don't have instant access to it, but that's the price to store it. But then the actual operations against it will for premium, they're very cheap. Some of them I don't pay anything. The heart. Well, it costs. A bit more, but it's still pretty cheap. But for court costs more. Because I'm paying less for the capacity storage of it. And then archive will archive actually I bring back. So there's still of the operations I do to move it around, but I have to actually hydrate it back to get it from archive. And then I have a cost associated as well. We'll talk more about this. But just realized there is difference and we have those access tiers so that you can optimize. How I store the data because you're have different requirements. So for the data I hey I'm working with right now I'll probably put that in hot. For the data, I must have access to, but I don't interact with very often. How I move that to call, I'll spend less money. The data maybe I have to legally keep, but I don't intend to access, or if I do, I can wait a day. Hey, I can move that to archive. So there's always to optimize how I'm spending my money. That's the whole point of all of these is how can I really optimize what I'm spending money on? That's everything around these different services available to me. So that's our goal, OK. So that's the basic of the storage account, and we saw those in action when we tried to create one. There's monitoring, there's logging icon enable on those things. Just like everything else. I actually get some really nice data just for a regular storage account supposed to jump over here really quickly. If we again close these things down. If I just go and look at the storage account. You'll see. Hey I have insights just available to me. I can get nice information about hey transactions by the type of storage I can see availability, used, capacity. Failures, performance. There's alerts, I can create these different metrics. I can diagnose they're diagnostic settings as always. I could send this to a log analytics workspace or a storage account. I have a lot of controls. Over these different types of service and I guess while I'm in here, if I was to go ahead and look at my containers. And I think maybe images. Notice I've got just images, image files in this container, so images of the container. But notice I've got a bunch of different files. But some of them are hot. Some of them are call. One of them is archive. And I can go and view these. Files if it's in hot and cold. So in hot hey, if I did edit I can actually see the content of it. I could do the same for the call file, but I'm paying a bit more now for that transaction against it. But if I try and do it for the archive. It can't. It's not available. I would have to bring it back, which could take time, and there's certain limits on how long things have to stay in archive before I could actually go ahead and do that. So money. Everyone likes money. Standard performance is consumption based, so general purpose V2 I pay. The amount of data I'm storing. I pay for the amount of transactions and that's going to be common across all of them. But that's the key point. Hey, I pay for what I actually consume, and now that that amount will vary. Is it hot? Is it cold? Is it archive? But it's always the amount I'm using. For premium, it's different. For premium, it may be based on the amount of space you provision. And the reason for this, again, if I think about there's really multiple dimensions I care about, there's yes capacity, but in addition to capacity, there's also performance. So if I think about the idea of, OK, great, there are these things, but we always have this idea of capacity. And then we add things like IOPS and throughput. How many operations can I do well depending on the size of the operations, what's the amount of throughput, megabytes per second supported and what we see is, hey, as my capacity goes up. My amount of IOPS and throughput goes up as well, so it's possible in the premium SKUs I might pick a certain size, not because I need to store that much data. But I need that much performance. So when I look at these services like files premium. And Page blob premium. When I create that share with files premium, I say the size, that's the provision size. I pay for that provision size. Doesn't matter how little I might write to it because I'm getting the benefit of the performance of that size. So sometimes for different services and check the pricing calculator to make sure you understand this. You might have a provisioned capacity, doesn't care how much you've actually written because maybe you created it at a certain size because I need this IOPS or this throughput. So that's why there can be these differences. Between them, and that's very common for files. Hey SMB, I need a certain performance for my application, it's using that file based protocol. Hey, my page blobs, this is storing a disk. I need to have huge IOPS or high huge throughput. It's not about the amount of data, it's the performance I care about. So those, well, they're actually done based on the provisioned size. Block blobs always pay as you go Data Lake. Data Lake builds on block BLOB. That's going to be the same thing. Realize. Managed disks, which we use for our virtual machines and some other workloads. They're always based on the provision size. If it's a standard hard disk, a premium SSD, a standard SSD and ultra disk. I pay for the size of the disk, not the amount of data I write to it, and then sometimes I pay additionally or differently based on the performance. Premium SSD V2, ultra. I individually tuned the IOPS and throughput I want dynamically at any time. And also don't forget about those operations. Don't forget if I if I turn on GRS, remember it has to copy the data to the other region. So I pay for those operations as well. So if we go and look, hopefully this is a link to the pricing calculator. I've messed up my links. There we go. So when I think about the options available, to me this is the pricing, OK, don't forget it's not just the amount of data you're writing. What is the amount of operations, write operations, read operations you're performing? That's going to be important thing to understand. Additionally. And there's small print. Additional cross region data transfer network charges may apply. For each storage access. Well, that's basically saying is, hey, if you've got GRS turned on, if I write a bunch of data. What it has to then replicate that over the network? I'm going to pay network egress charges. It's going outside of the region. I'm going to pay network egress, so make sure you understand all of those things apply. So when I go into the pricing calculator and I start plugging in my numbers. Especially if you're a very high operational workload. Don't neglect the operations, don't neglect those network things, because hey, that's going to come into play as well. So what are the storage account services I can actually use? So BLOB, BLOB is this this? All powerful one we always talk about. There are different types of blog. There's block, BLOB. Where the BLOB is made-up of blocks that are chained together. This is very common if I had media files, if I had PDFs, if I do some random stuff, I'm going to store it in a block BLOB. And block BLOB we build on top of for things like the Azure Data Lake Storage Gen. 2, which adds a true hierarchical namespace which we're going to talk about in the next slide. Page BLOB. Page BLOB is really tuned around pages instead of blocks. That's very good for random access. Hey, I need to just jump around anywhere in this file. If I'm a disk for an operating system, I'm random. I need a page BLOB. If I'm creating a log file, for example, all I need to do is append to the end of it. So that's why an append BLOB is for. We have different types for the different scenarios I might want. And again, if my storage account is a general purpose V2, I can create. Three of them. So to drill down. And just think about these for a second. So if I start over here. So I create a storage account and I'm going to say again, for right now this is a general purpose V2 so I can do all of them. So I create a storage account. Because if it again was premium, it would be a particular, it would be files, it would be block BLOB, it would be page BLOB, and that's it. That's all I could create now. But if this nice general purpose B2, I can create any of them. The first service is BLOB. Now we've blocked the way I organize my content is I create containers. Like have multiple containers. And into those I create my different types of BLOB so it could be block. It could be page. And it can be append. So all three of those can be mixed in. They go into a container. Now, one thing I do want to stress with. These. It's flat, there is no hierarchy, although it might look like there were directories. You can go and create it and it looks like there's a file system there is not. That file system you see is a virtual directory. It's made-up of the name. The name has the folder name structure in it. So operations when I'm listing a folder actually has to go and recurse and check all the names of the files. See well, would it show in this virtual directory it's trying to create and show for you. So. Realize that's really important in terms of performance and types of operations. You might do that, although it might look like, for example my block BLOB has this nice hierarchy of stuff. It isn't. It's all flat. So if I did a rename operation or moved it between a folder, that's a copy and delete operation. It's very very expensive. Which is why one of the things you can actually turn on on top of block is thing called a hierarchical. This is optional hierarchical namespace. Azure Data Lake Storage Gen. 2. That creates it as a true file system, POSIX style. ACLs. I can actually move things. I'm truly recursing through actual directories. Very very efficient. And then once I turn on things like hierarchical name space, then I could turn on access using things like the NFS protocol. I can turn on things like the SFTP protocol, but that has to have that hierarchical namespace to enable me to do that. So these will options that when I create my storage. Account. Jump over here. We'll see as available. So if I go back to storage accounts and do create for a second. In my advanced tab. You got this option to enable hierarchical namespace. So this then makes it instead of just regular BLOB. It makes it that hierarchical namespace with true directories, POSIX style ACL's, all of that goodness and then. Actually enable SFTP access I could enable NFS V3 and it's telling me hey look if you want this. You have to turn on the hierarchical namespace. I need true folders to be able to offer these services. I can't do it these virtual directory things. Once you've done that, so I've got a data lake over here, for example SFTP. That's a very common secure FTP service. I will now actually go and hey up I add local users. So this is not integrating with things like Azure AD. Today I actually have to go and create a local user which each has a password or a key pair. And they're going to go and authenticate. I give them access to a particular container and I give them certain permissions so I can stay at Home directory, but I can natively now just enable SFTP on BLOB. It's just an option I can do providing I have that hierarchical namespace, as can I do with things like NFS. So what you can kind of see happening here? Is additional protocols are being added just as part of BLOB. They're adding more and more things to it. So there's the BLOB rest, but there's also hey NFS, hey there's SFTP. Hey, when I turn on things like this data lake over here, well this turns on even more types of APIs. There's things like the DFS API, which can then talk to the abfs drivers that's used by things. Hadoop. So why block is extremely powerful, extremely flexible in what I can do with that. Obviously page we talked about, the whole big deal of age is it's this random types of operation and append. As the name really suggests, I'm just adding to the end of it. Now one of the nice things we can do. Across these there were some the features not to spend a huge amount of time, so things like BLOB index tags. And this enables me to add metadata and we saw this when we talked about governance, those role based access controls. So I can just add metadata to blobs. That could then be used maybe for inventory purposes maybe I want to use it as part of that attribute based access control as we saw before. I'll just show it quick just in case you didn't watch, but I recommend watch the governance session because that's where we actually showed granting permissions based on this. But if I go and look. At something. I don't even know which one. But I have this ability to have BLOB index tags, which is just this key value data that I can store along with the BLOB. And then there's different things I can do as part of that. I think the key is 128 characters, the value can be 256 characters and their maximums, and I think I have 10 index tags per block. So it's a nice way to maybe add just metadata. Maybe I'll use it as part of some lifecycle management quickly finding things. It's just a cool thing. I can do and what I'm in here. Overtime. I might get a lot of blobs, so the other thing I can do. Is I have this BLOB inventory. And I create a rule and what I say is, hey, maybe it's what types of blobs I care about. As you can see over here, I could restrict it. I could just, hey, do I care about blobs or do I care about containers? What do I want to include as part of that? I could absolutely. Add different prefix matches or only want certain ones. How do I want to export it? How often do I want to export it? And what this will do is then just in the background it will go. And create an inventory file for us O if I go to my inventory container that it created. I can see how you for January every day. It goes and creates a set of files. Now you'll notice in my instance it's only giving me data. 4. Really, the last three months? That's not the default. The default is we'll just keep it forever, so we'll just start consuming more and more space. But what I created was a lifecycle management rule, which we're going to talk about, that goes and deletes anything over 90 days. So remember, if you use this, you want to make sure you've got something to actually go and clean it up. So we have things like BLOB index tags, and I also have BLOB inventory. If I want something to go and periodically, hey, just create a catalog of what I've got that I can then go and consume. As needed. So that's BLOB and that again is that super, super powerful. I mean, it's just such a common service we're going to use then we also have. Tables. So table is all about key value pairs. It might look like when you look at a table, then it's a schema, it's a database, but it isn't. Every single entity I create in a table can have its own columns which whatever values it wants, it doesn't have to adhere to any set thing. So we have table. Which of these key values I'm going to come back to these we have queues, which is messages. It's first in, first out. But it's not guaranteed. It should be, but it's not a guaranteed first in first out. If you need a guarantee first in first out, then it's saying like service bus, service bus can guarantee that first in first out. And then we also have files. So this is going to be an SMB or an NFS 4.1 share. So there's different things we can do. So if we go back and look at how these are structured. So yes, I can have blobs. Blobs are fantastic, but I can also have a table. At a table. We have entities. So these are these key value pairs. Then I can have queue. On a queue we put messages. These are fairly small pieces of data. Normally it might be used to drive something else, an event driven type compute. Hey I put a message on a queue and then it goes and triggers something else to then actually go and happen. For example what would be very common imagine BLOB and imagine this queue I could think about. Well maybe I have some application. When it actually does is. It's a BLOB. He writes an image, so it writes image one. But then what it also does is to the queue it just says hey I wrote image one and then there's some other function. That is event driven. So it sees, hey, a message is being created on the queue, it takes the message off. So this is the event. Oh well, image one, I'll go and read image one off the BLOB and then go and do something about it. So it's very common that I'll use the multiple things together. I want you to go and do something. I'm going to dump a message and then I might go and do something else with that. That's super common thing. And then we have files. These are file based protocols. So what I do is I create a share. Now when I create that share it can be SMB. Or it cannot be both? It can't offer both the file based protocols. When I create the share, I'm going to create it as SMB or NFS? SMB it's 2.1, 3.X, so I can do encryption. There's NFS, it's version 4.1. There were different capabilities, ones like create the share, obviously into that share I then create folders, I create files. The things we're used to. There's a number of capabilities we have as part of that, that we're used to. For example, I can have snapshots, different points in time, version of a file share. Then on Windows for example, if I go to the previous versions tab, I would see those snapshots to enable you to go back to those different points in time. Let's actually have a quick look at each of these now, one of the nice. Things today is. When I'm over here. You can actually see well, I can see I've got a file share, I can see I've got a queue, I can see I've got a table. But there's also this storage browser. This lets me go and actually interact with those things in a very rich way. So hey, I can go and look at my tables. There it is. And shrink this over a bit. I can actually see the data and although it again it looks like it's a schema, it also is a schema that defined hobby and children and car and age. That is not true. When I add each entity. It's showing me. Once have been added before, but I could add any property I want, I could add a new one. Steak cook. And I'll set that to. Well done, whatever I want to do. And I don't want these ones. I'll just ditch all of this. The only thing I have to have is a partition key, which is how it actually shards it distributes it, and then a row key and the partition key with the row key have to be unique. So those are the unique properties. But now it's everything else I could get rid of. There is not a schema, it's a set of keys and values that I have complete control over. Then we also have. Cues and a queue is just, hey, I can add a message. Test 1. And then there's an action I can dig you. A message? You want to dequeue it? Yep. And then it dequeued the message. Obviously an app would normally read the content of it. There's the files again, I would interact with SMB with NFS. I have the BLOB containers as well, and I could go and look at the content and interact, upload, etc. There's also a desktop. Type utility icon downloads. There's also going to talk about at the end storage Explorer. Here I'm using the tool I'm talking to the same. Storage account I see the same data. But this is I think it's a HTML5 based app. I'm saying all that same stuff, so different ways I can actually interact, but they each again have their different use cases. Based on what am I trying to achieve, what do I need? What I'll pick the right type of service. Hey I've got today some windows based file share. I want to move this workload to the cloud. Hey, I'll create an Azure files SMB. Hey I've got a bunch of Linux workloads today that need some shared area. Hey I'll create a SharePoint as NFS. I need a very very basic key value store hey Azure tables. There were rich like Cosmos DB as a much richer key value Nosql type store. But maybe this does what I need it to do. Again, it's all about the features I need and the cost of the service. Let's optimize and use what I need to actually attain the requirements of the application. I don't want to waste money. Let's do enough that hey I get those supports. So it's really that the key point about those are the services just on a regular storage account like it all of these great things, which is why storage accounts really are just so, so powerful. But I mentioned the idea of a data leak. Many, many times in any data flow today. Extract, transform, load sources coming in. I've got a data pipeline that maybe transforms it, scrubs it, sends it somewhere else. Storage has got really cheap. And rather than getting data to come in, transform it and then store it. It's becoming more and more popular today to as it comes in that story in its raw format straight away. Because maybe in the future I want to transform it in a different way. Maybe in the future I want some different type of analysis done. So the Azure Data Lake storage Gen. 2 builds on top of BLOB, it provides a true data lake. Now remember blog has got this massive, basically infinite scale. So by building the Azure Data Lake Storage Gen. 2, there was a Gen. one which was its own thing that's gone. But building it on BLOB I can use hey the BLOB API. But then I can also use. The the DFS APIs I can use, the abfs drivers I can hook in very without doing anything, Hadoop, all of those types of platforms. And what this does is it gives it the true hierarchical namespace. Remember BLOB, there is no structure. If you see folders, it's virtual. The folder is part of the name of the BLOB. When I do a move between folders, it's copying the entire contents of the BLOB with a new name and then deleting the old one. It's actually very very expensive. Very very slow. Atlassian 2 has a true directory structure. That's the key part. I can actually rename it, I can actually move it, and it's just a move operation. It's a rename operation and I can store anything. I can sort of structured, unstructured, semi structured, anything I want in here. It supports POSIX style apples. So there's accounts I can create within here um, or I can use Azure AD data placed RBAC so I have a choice so I can actually go in and create. That's more maybe how I'm used to. We have a date like so use the POSIX axles or hey, I can still use the Azure AD. I still have those capabilities available to me and I would think about if my workload is primarily analytics, I probably want to use data like. If it's more, hey I have a bunch of media files. Want to store it? Well then maybe I'll use BLOB. So that's really that that key guidance around there. So if we just want to keep our picture up to date. So I drew BLOB and I drew this idea of. Sure we have this structure, but the whole point of this is what this gives me. Is this data lake? That's the key part. And so yes, with the block BLOB I still have the regular BLOB API for example that I can interact with, but now also what I can do with this hierarchical namespace once it's enabled. Now I'm adding this DFS API which. I have these special abfs drivers that call. The dfsa API. So my Hadoop, my typical workloads that user date like are just going to work. I don't have to do anything. And again, the whole point is my data pipeline. That initial raw data I just dump in the data lake. Whatever format it's in, I'll dump it in there and then I'll go and transform it and send it to a synapse or a SQL or cosmos, whatever it is. And some services today will even be able to interact with the data while it's still in the data. Like if I had park files or Avro format files, I can actually connect to read it in place in the data. I can use it in maybe my analytics software. So this is a hugely, hugely powerful thing. So that that service is there. I can host a website on BLOB. Now this is only static content, it has to be pre rendered. There's no server side CPU processing available to this. But if I had HTML, images, cascading style sheet files, JavaScript embedded in the HTML, I've pre rendered it with something. Hey I just enable on the account level. Say I want to offer website, it creates a dollar web folder. I just. Put the stuff in. It's going to have a URL. I can hide that URL in my own DNS. I can create a scene name. I can have an alias to this name and hey. It's just there now. What has come along since this is called Azure static web apps. I just, I think Web apps I think is just better. I just started web app. Web apps uses the content delivery network. It's more geographically distributed and available. It ties into things like Azure functions really nicely. If I do want some server side processing, there's a bunch of that's even just free. But it does exist. It's there. It's just an option in my storage account. So if I jump back to my nice and basic storage account. You can see. If I can find where that option is gone. Static website, so there we go. Data management. Static website. And if I turn this on, it's going to give me the URL. So this is the endpoint icon. Go to to actually go and see my site. I've got this enabled. It's telling me all the files you need to put in here. I could change it but index dot HTML and if there's an error error dot HTML. Once it's enabled there's now a container. Called dollar web. So in this container I would put my website. So I created an index file, an error file, images. And now if I go to the URL. Witness the power. Of the worst website in the world. This is Savilltech logo I created when I think it was about 18 years old. It just spins around but I'm hosting that from my storage account. I'm not paying any extra for other it's just storage. Obviously there would be the transactions, but I'm not really hosting anything else. If I had a Super basic website, hey I could just use this and again I could hire this URL by creating a vanity Uri, my own DNS, just an alias to that, but again, yes, it's there. But I do think that the Azure static web apps just better in the probably every way, so I probably wouldn't use this at a large scale anymore. I'd prefer to use the Azure static web apps, but we're to make sure you knew it was there. OK, so how do I control who has permissions to things? Azure AD integration is probably the best option and we'll see this now for pretty much all of the services for BLOB, for queue, for data if we go and look at our storage account. And if I actually jump over to the roles that are available. Remember, I don't care about control plane. Control plane is pretty much useless to me other than I could go and get the access key. But let's pretend we don't know that. But what we see is a whole bunch. Now it's a bit bigger. We'll see a BLOB. Data. Data contributor, data owner? Data reader. We'll also see. Q data. Table data. So there are RBAC roles available, and let's just pick one of these. That have data actions, so this one can only add a message and the power of this is now because it's the Azure AD role based access control. Remember one of the things from our governance session is, well firstly I can do conditions. So the data plane like we had over here, I could say well yeah you've got a certain role, but well only. If the BLOB index tag. Project matches your Azure AD attribute tag of primary project. So I can actually go ahead and add conditions on those permissions. But what is really nice about this is with the Azure AD. Well. Think of other Azure resources. So if I had the idea for example of a compute resource. That's just running in Azure. Well, it has a managed identity, it has an Azure AD account. They don't have to go and storing code somewhere, it's just native to the resource. So if I think about the different services, I'm just going to think of controlling access. So how can I manage this? So. Access options. What I really? Don't want to do is use that key, so ideally I'm going to use Azure AD. Data plane. Role based access control, but I can create custom roles as well and I can do this for BLOB. Q and table. For files, I can give a permission to access the share, but I can't do anything on the actual files and folders within the share by the SMB. There are different things we're going to talk about that in a second, so ideally if everything is available to me and I have a choice, this is absolutely. My happy. Yay, this is what I want to use. If I can use the data plane RBAC, that's what I want to do. Very, very powerful. Lots of options there. What if I can't? And, and again, remember that managed identity. It could be given those permissions to some compute resource. Could just natively within its identity be given access to some storage service. Well, then we have shared access signatures. This is not as bad as just using the access key. There's two types, there's service as an account, SAS. Account SAS, as the name suggests applies at the accounts to at account level. So it could be given access to any of the services, BLOB, queue, table, whatever service SAS well it's specific and I create it at a particular service level, so it could be for files or for queue or for table. The benefit of service though. Sorry, and these are signed with the account key. So if I rotate the account key, it would invalidate the SAS that was signed with that account key. Otherwise I can't revoke it, but there is a concept of a policy based shared access signature for service. Account um can only use ad hoc. There's no concept of a policy at the ad hoc level. So what this looks like? Is OK, so I could use Azure ID is one of the options. We have the access key. We have two of them. Remember we have the access keys. I do not want to use these. If I have these stored, I'm using this as part of my app as a bad thing to do. I don't want to use them, but maybe what I could use is a shared access signature and remember the access key is used to sign the shared access signature. There's the account level SAS. Which I can give permission for any of the different services within the account or there is the service which is going to be one of BLOB or queue or table. If I use service I can also at the for example the container level create a policy that could then be used to create a policy. Types of access signature or both of them can create ad hoc types. Ad hoc is just. I select the settings and it's signed by the key. This means I can't revoke it. There's no way to revoke or change this. Unless I rotate the access key, which will then validate the signature, it won't work anymore. We've policy because it's built off of a certain policy. By changed the policy it changes what this can do. And then with these I can for example, I can time bomb it, there can be a time limit, I might do, IP limits, I can obviously restrict the operations. It can actually perform. I can. I can disable the access key. If I disable the access key. Well. I can't use shared access signatures. Remember, the access key is used to sign the shared access signatures. If I turn off access keys, I've got nothing to sign the shared access signatures with anymore. So although yes, I may want to turn off access keys from a security perspective, realize that also means I can't use these more granular shared access signatures. I can only use the Azure AD RBAC anymore, so just realize that. A trade off you're making. And we can see these. So here if I look at my storage account. One of the things we'll see. Is the idea of a shared access signature so this is at the account level. So this is going to be an account SAS and notice I can pick there for any service. BLOB, file, queue, table, one of the resource types? What are the permissions I'm going to allow? What are the different types of permissions? What is the expiry? When is it valid? From what IP addressing? And then hey, what key are you using to sign it? And I can just go ahead and do that and to give you an example, I guess, um, it's going to container. Images. If I select an image. One of the things I could do is an image level. I could generate a SAS. Now because this is within BLOB, this is now a service type going to sign it with the account key. I'm not using a policy. This is valid for what's this like 8 hours by default? I'm not going to restrict anything, but I'm just going to generate the ass. And there is. So if I copy all of this. And then just paste it in. There. Picture of Ollie when he was much much younger, but it just worked because this URL contains. It's got the signature all the way at the end of it now. One of the things that happens when you show this is people panic. They're like, wait, you just sent the checks, the signature in plain text over the Internet. Anyone could get that and get access to it. That is not how the Internet works. So bit of a digression, but yes it's a URL. It's added to the end of the request, but when you talk to a secure site you don't send the URL over the Internet say hey I want this page. Well, not first if this is me. And then this is some service. This could be a storage account, it could be savilltech.com. It's all the same. The first thing you do is when I type in savilltech.com, but I actually have to convert savilltech.com to an IP address. So my machine goes and talks to DNS and says hey what's the IP address of this service and it sends me back the IP. Then I go and talk to the IP of the service and there's kind of this 3 way handshake and establish a TCP session. Then I have to establish an TLS. And there's a, I think it's a four sets of packets, the kind of a client Hello, server hello, you agree? The whole authentication and what level of TLS you support, what encryption algorithms you're going to use, all of that stuff happens, but the end result is I establish a TLS. Session and then over this TLS do I send an actual request which would be the actual URL? So it's never sent plaintext. I established all of that stuff first before I ever actually send anything, and we'll see that like in this browser right here. If I super quickly just typed in that Savill Techcom. I played around with this recently because I was bored. So if I look at this very very plain looking site right here, well actually what's happening is let's go. If I was to right click somewhere and do inspect, you get a whole bunch of troubleshooting stuff. But if I add an element and do security or maybe it's there already so. They got security. It will show you. So from right here. Trying again. It's showing you hey look, you're using TLS 1.2 Diffie Hellman with RSA. Advanced Encryption standard 256 with GCM like you've established this secure connection before you did anything else. And so that's when it actually would send that request over the URL. So you're never sending that shared access to mature, just over plain text to web. That's not a concern you actually need to think about. Now, what I would say though is you need to be super, super careful that you don't put these into code. And that is that you see. So never. In code you're seeing GitHub, it will go and warn you. It will tell you if you have to use these. What maybe I have is some process generates the SAS, it then goes and writes it the site. Maybe like an Azure key vault as a secret and then my application. Can read it and then it might go and use it to actually go and talk to BLOB. For example, this is going to read the SAS, use the SAS, but it's never stored it in the configuration file or an environmental setting or anything like that. As a key vault, remember I can have RBAC permissions, so this could have a managed identity and then the permission on the seat group could be. Hey, I can read the secret for this managed identity one. There's a bunch of things I can do to lock it down, but never ever end up with those in just a config file. It's a terrible place to be. So there's different types of capabilities, different types of shared access signatures, and of course Azure files actually has different types of integration. Now it's got three options available to us, so it's really grown. The probably most common one you will see today is it can hook into Azure directory domain services. So you're existing Active Directory you have the storage account gets an account created in your regular ad, and now my users just connect to the SMB share and the apples would be applied and maintained. So that's probably the easiest one for most companies. I can use Azure AD domain services, but then I have to go and deploy that and if I already have adds I probably don't care or don't want to do that. The other thing I can actually do is Azure AD Now does support Kerberos for limited scenarios, but one of them is access to Azure files I have to set up on my VM. So then I'm actually using Azure AD to give me a Kerberos session ticket to authenticate. To the Azure falls share. So there's different things depending on the scenario. Again, there's there's often the case there's not a right or a wrong particularly, but there are a lot of options and I just have a slide so it's in the notes. Don't panic, you'll never actually sending this plain text over the Internet. TCP TLS is established then it sends a particular request. OK, so encryption, this is super important. We can think about encryption always. It's always encrypted at rest. I cannot create a storage account without encryption anymore. It used to be an option, it's impossible, it's always encrypted. I think it's 256 bit AES. It's in the documentation exactly what it's using, but it FIPS 140-2 compliant. It's everything is encrypted. You can choose customer managed key. So with many services today, the database services, the storage services, I can actually say I want to use a customer managed key. So instead of a Microsoft managed key where they maintain the key encryption key that's actually used to encrypt the data encryption key that encrypt the bits on disk. I want to be responsible. I want to be able to revoke it. I want to be able to pick when I rotate that key. I want complete control. So you would put it in an Azure key vault as a key and just at the storage account I can say hey I want to use my own key. So if I was to go and let's close this down. If I just go and look at my encryption, I could change it from Microsoft managed. I want to change it to a customer managed. It's just going to ask me for which key Vault or I could enter Uri. Bracket select the key vault I have. And I could pick an existing key or I could create a new key. If I. Create a new key or it doesn't matter which one. Ideally don't pick a specific version. And the reason I say that is one of the nice things now with storage accounts and if I'm using managed disks and disk encryption set. If I don't say the version of the key. It would just use the latest one and then what happens if I rotate the key? It will automatically detect that and justice start using it. On an ideal world, don't bother about picking a particular version unless you need to. For some reason it's better just not specify the version. And now when I rotate it in Azure Key Vault it will just automatically pick it up and Azure Key Vault. I can now define policies to automatically rotate keys so I can actually be completely hands off. I could just create a policy and key vault. They rotate every 90 days. It would rotate every 90 days and the storage account would just pick it up and start using it. So I can have complete control over that if I actually want to. So there's a lot of things that I can have there. I can have cross tenant customer managed key again for storage accounts and disk encryption sets. We've managed disks so this would be really useful if I was a SAS provider software as a service. And I had multiple customers, but it may be the customer wants to maintain control of the key that's used to encrypt their data in my service. So with this, the key stays in their key vault, they give me permission to it and I encrypt the storage account that's used to store their data under my subscription. But they still maintain control of that key. So I can have a cross tenant CMK. And again it's predominantly a requirement for if I'm some SAS service and the customer wants to maintain. Control with the key. I can also do encryption scopes, So what encryption scopes let me do is instead of encrypting the entire storage account. I still have the entire storage account level encryption. But maybe I have some particular set of containers. And again this might be a sad scenario. I've different customers I want to do different encryption. I can create encryption scopes so I can create multiple encryption scopes with either Microsoft managed it just needs to be a different key or customer managed I need to control it. So I create multiple encryption scopes. And then what I can do is add a container level when I create the container. I could specify a different encryption scope. So that's going to use a different key for the encryption of everything within that container, and I think even an individual BLOB level if I'm not already applying an encryption scope at the container level when I upload a file. Uh, yeah. I can add a BLOB level, select a different encryption scope. Says lets me have a completely different set of encryption than that of the storage account. If I did have those scenarios where hey, there's different customers, maybe different sensitivities, whatever the requirement is, I have some need. To separate those out so I can do that and obviously encryption over the network I can enforce as part of the configuration of the storage account. I can require only encrypted type communications. So at the storage account configuration like many things. If we go and look over here. Well, while we're in here, one of the things I can do in configuration, and one of the things I've done is. I have somewhere. Well firstly when I can see it secure transfer, so I could set that to enabled. I could only use HTTPS for the APIs, I could only use SMB 3 so it's encrypted, but then also. Allow storage account key. So I could disable this so I could say hey I don't want to use that account key. But remember if I disable the storage account key I also cannot use shared access signatures. So just to be super careful that sure it might be a great idea, you want to force data plane role based access control actually on those things. Fantastic. But realize you're also killing off shared access signatures so just be aware of that. Send networking. So standard features apply to storage accounts, private endpoints, service endpoints, IP based firewalls and I'll walk through these quickly. But also there are resource instance rules. So what this lets me do is on the firewall of the storage account I can say you know what I want to let in only. Um. Particular types of service, for example from Azure. Or it might even be only a particular instance of this particular type of service. So I've got my storage account. And like nearly every service in Azure, it has a native firewall. And I can do the regular things, I can do an IP based restriction. But I can also do a service. Endpoint. And what are service endpoint lets me do is if I think about my virtual network in Azure. So I have a virtual network. The virtual network is made-up of multiple subnets, so it says Subnet 1. I can enable on subnet 1. Service endpoint for storage. Well that lets me do is now this subnet becomes a known entity to storage account type services. So now I could say on the firewall, hey I want to let in. Only subnet 1. There's even things I can do called service endpoint. Policies. So we've service endpoint policy at the subnet level. I can say, hey, you're actually only allowed to talk to storage account one and storage account 2. The point of that is data exfiltration. If I'm running some service in here, I can't go and connect to my production storage account and also some dodgy storage account I've set up and copy the data. So setting up a service endpoint policy on the subnet stop someone exfiltrating copying the data somewhere else. So service endpoints let me know and enable specific subnets to talk to it. Private endpoints. Remember a private endpoint is a feature that hey. I create just an IP address in a certain district, different subnet. Doesn't have to be I don't think, but private endpoint. It's just an IP address, endpoint 1. And that points to a specific instance of a service, so storage account one. So now when I talk to this IP address, I'm actually talking to storage account one. So that's a way to again lock it down. And this could be used by any connected network as well, but then we also have resource instance rules. What that does is I can now say hey actually. I want you to also let in. You'll see this on the firewall properties. And I've got SQL instance 1. Um, let that in. But only SQL. One or any SQL in my subscription or different capabilities are available, but they're all configurations I have on the firewall to actually let me go and lock that down and control it. Let's jump over super quickly. So here if we jump over. And we'll actually look at our networking. So right now let's access from everything, but if I do enabled from selected. What you'll see I can do. Is we have to IP based, yeah iPad service endpoints, but we have this resource instances where I could pick hey a particular type of service. And I could say, well, all of them in the same Azure AD tenant, all in my sub, all in my resource group or a particular named one I want. So I have a lot of control. Actually over that use. So that is a. A very powerful, nice thing we have. Available to lock down that type of access. OK, so. That's how I can control and maybe make more efficient. Service endpoints also provide a more optimal path. Private endpoints also are more optimal path. Even if I'm accessing the public endpoint, although it's Internet facing, it doesn't bounce out to the Internet and come back. The Microsoft routing is more efficient than that. It's still going to be an optimal route, but service endpoints make it even more optimal. So lifecycle management is a really cool feature in the. Think of those access tiers. Hot, cool, archive, maybe even delete stuff. How do I do that? I am I manually going to go and look at my account and be like, OK, well it's it's the 1st of the month. I should probably go and go and delete a bunch of stuff I know I don't really need any more. That's a pretty painful, measurable thing. Maybe I've got versions. I want to go and delete versions older those log files. Hey, over 90 days go and delete it. So what life cycle management lets me do is I can actually say, hey, I want to have rules over here. That will automatically go and do various things. So move delete based on when it was last modified, when it was last access. I can define them for my BLOB type resources. Now there's no tiers in premium, so the premium it would be delete stuff. That's basically what I could do. But for general purpose I can actually go and move things between it. Now if it does move things between tiers, this does not change its last access time. So my whole point here is we find it on my picture again somewhere. Yeah, so we had this. So rather than me manually going and changing those things. Well. Life cycle management. Based on rules I define can go and say hey, it's been more than 90 days since you've accessed, let's go and move you to call may. It's been more than a certain amount of time. I'm just going to delete you so I can enable all of those things through that lifecycle management. So let's go and see one. And I'm actually got this in action, which is why you saw certain behavior in my. So if I look at my account again. Remember, I only had three months of inventory. The reason for that is I have lifecycle management and I've got 2. But this rule right here. Says. Well, firstly I'm limiting it. I'm not applying it to everything, I'm applying a filter. I'm only doing it for block blobs. I'm only doing it for bass. I'm not looking at for snapshots or versions. And my role is. If it's more than 90 days old it was created, I want to delete it. Now I could do very different things again I could do things like hey I want to add a tier O if it was last accessed or created or modified, then hey look, move it to cool, move it to archive, move it to core and move back if accessed. All different things I can do. Automatically through these rules. I don't want that and then I applied a filter. That the file name has to start with inventory. Remember there's no real folders. This looks like I'm saying, hey if it's in the folder inventory, which I guess is the net effect, but really it's just the prefix of the BLOB name. I could say only blobs in inventory that start with letter A well I just do slash a because that's exactly how it's working. So these are super powerful also versions, so I could apply it to only apply to versions for example. And here what I want to do is, again, I'm deleting versions of blobs that are over a certain number of days old. Very, very powerful thing. So if you think about, hey, I don't want to just have things accruing forever and getting bigger and bigger. One of the rules we think about in good architecture is a certain amount of steady state. I want to make sure some service doesn't just infinitely grow and consume resources, because that's when problems will happen. So lifecycle management can help me a optimize cost by moving things that haven't been access for 90 days. Probably should be cool. I'll pay less for the storage, bit more for the operations, but that's a better deal for me. Hey, I've had this stuff for longer than I need. Let's delete it. There may be regulatory requirements to say, hey, this is create over a year ago. I want it to lead. I should not store this stuff last cycle. Management so it's not just even maybe about optimizing money might help me make sure hey, I'm staying healthy by not growing too much. I stay in a fill up my desk and crash my app, but also meet regulatory requirements that I may actually have. So super powerful feature. They are some native protections available. They think obviously there's backup services and there are different capabilities around backups. When I think of blog, particularly when we have a backup service, it's really an orchestrator to use blobs native capabilities. Now it had snapshots, I've done this in great. That ain't great because it's well, it's a bit sad. It's a bit long in the tooth. I don't want to use it anymore. A snapshot was a particular point in time. I could take the snapshot, I could do a recurrence and it would still the delta of it and I could go back to that. Particular snapshot but. And it was a a point in time we don't view it was incremental storage, but there were better features now. There's block versioning. So, BLOB versioning, let's think about. This is block BLOB when I when I'm talking about this. So what is a BLOB? Ohh really? We say the word BLOB and we say block BLOB. That gives away really what it is, a block BLOB. Is. A set of blocks. That's a block BLOB. A set of variable size blocks. OK, and when I create a block BLOB, I commit the sequence of blocks that make up the block BLOB. Fantastic. This block here. And someone made a change. So when they make a change is they basically create a new version, the block with a new data in it, and they do the commit. So the block BLOB actually becomes that. This essentially now is disconnected, but what BLOB versioning does is it keeps this. But now I could go back in time. It's going to maintain the old version of the block. Now, is that a block level? So this was a 50 megabyte block, but only a few 100K have changed. It still has to keep the entire block, but it's not having to keep all of the blocks again. It's only having to store the delta block, the blocks that have changed. But versioning would now let me actually go back and put this block back in my view at this particular point of time. So this feature versioning. Well, that's great. I can now go to previous point in time versions of the blobs and I can configure that individually. I can just turn on versioning. But how do I know? Which blobs were at different times? Now I could look at the BLOB and it will show me previous versions, but that's kind of hard to manage at a large scale. So the next feature we get, I'll actually go through these in one go. Is was an idea of a change feed. So hey there's transactions are happening. This is a transaction log of anything changing on the BLOB or the blobs metadata. And then? There's also soft delete. If I enable this, if I delete a BLOB, it actually doesn't delete it. It keeps it for a configurable retention. I think it's 1 to 365 days. So I could go and undelete it in that time. That past that time it will actually get shredded and it's gone. But this gives me saying good point in time restore. So all of these features together, if I have versioning, if I have the change feed so then I've got a transactional log of every change and when it happened and I add to that soft delete. So even if I delete a BLOB it's not actually deleted, I can undelete it. Well, if I put all three of those together. I get point in time restore. And that's the replacement for snapshots. I don't want to have one particular point in time. I can go back to point time itself. I go out to any point in time. I can figure this with a certain number of days that I want to be able to do this for. So then obviously all of these have to be that or longer. If I say this is 30 days, I can't have soft delete for 15, it has to be a longer duration to support that. But now once I've turned that on, I can just go back to any previous point in time and restore that. So this is a really nice feature when I think about, hey, I want to protect. So this gives me protection from things like. Hey, yeah, a logical corruption. Some delete was made, someone didn't update and they didn't mean to. This is still stored on the same set of infrastructure as the primary, so this is not a protection from certain types of hardware failure. It's also local. Now there are some locks I can put on things, but it's trying to go and delete things. Depending on the locks I've put in place, maybe they could still remove the history, but again, I can do certain amounts of immutability so they can't do that. But you want to say what am I actually trying to protect from to work out? Is this the right solution for me or not? OK. So then we have Azure file sync. I talked about Azure files might be a great solution that had an on premises file server and I moved it to the cloud. But I may still have the on premises file server, it may not have gone anywhere. So Azure file Sync is a way that. I can synchronize the content from my on premises file server to my Azure file share, but more than that I can have multiple. File servers. So there's a single Azure file share as part of a synchronized group SYNC group. I can have up to 100. Windows Server. Fall shares as part of this group. And then it replicate via the cloud endpoint. But I can also have tiring it's a based on a certain maybe capacity threshold. Hey, I'm full on Prem, they've got 20% disk space left. Stop storing it locally, I'll pull it down from the cloud if someone tries to access it or hey, it's not been accessed for 90 days, stop storing it locally. I'll pull it down if someone tries to access it. So what this looks like? Is with my SMB, so it's my SMB so I can think in the cloud. Great. I've got my Azure file share, so it's Azure files I'm going talk about. Hey we have the cloud. But still on premises. I've got hey, I've got a file share here on this server. In this location I'd be another location over here with its file share. And what I'm going to do is there's an agent I install, so there's kind of an Azure file sync agent piece. And then I create in the cloud. A sync group. And into this sync group I add the Azure file. Share and then I add multiple SMB file shares and what they're going to do is they're going to synchronize. We have the Azure file share. They're not going directly to each other. If this makes a change, it synchronizes it up to Azure files and then it synchronizes it down to this one. There's a certain amount of triggering, can detect through journals when there's a change made, there's a certain delay of the replication. It does maintain Ackles, so if these have got any Ackles, hey, these are replicated up here as well and they'll get replicated back down. So it does keep that. And if I'm using things like the adds integration and I connect to this share, they would be enforced if I use the Azure AD cobot. So I'm still going to get those. ACL's maintained as well, but then what I can add to this is that tiring. So I can say, well hey, a certain threshold of this space. Don't store the file locally. It's always going to be stored up here. And then if the user tries to access the file, it has to pull it down. There'd be a delay, a lag as it downloads the bits again, but then it would present it to the user. Hey, a file has not been accessed for end days. Stop saving it locally, delete it again. I'll pull it down if I actually need it. Azure File Sync is a really nice capability I have. When I still have these file shares that hey, I want to synchronize them via this I get the redundancy. And that is a copy in the cloud. I can do things like the Azure files has snapshots to get previous point in time which the clients would then be able to use. I get all of those features available. Hey if this has a failure of the site I can go and connect to this to go and get the data. So this really opens up a lot of nice scenarios for those workloads. And of course just the cloud tearing, hey, I've got finite space here. This is pretty much infinite. This is that large file share. I can make these really, really big as well. So there's a lot of things that this solves for me. Now I lied. Um, early on I said Azure doesn't use sans. Normally for Azure storage, but there's also is Azure net app files. This is a first part is when Azure offering using net app filers, so it's using the net app appliances which gives you a certain amount of great compatibility with what you might have net app on Prem, there's certain amounts of replication things I can do also just from a performance perspective, they have different performance tiers that I can leverage. Standard, Premium, Ultra which we're going to draw out, they support SMB and NFS. It's a file. These protocol, but they can do dual. On a single volume I create, I could actually connect to it using SMB and NFS. They do NFS encryption by integrate with Kerberos. It has the best talk to an Active Directory instance, but they have those capabilities. It connects and takes over a subnet of a virtual network, so it has to have a subnet. When I create a volume, I tell it which delegated subnet it's going to offer its addresses to, but then anything that can get to those IP addresses. Could use it now as his own sets of replication so I can configure the replication. It's not just the Azure paired regions, they have their own sets of replication that they support and I get to pick that. So at a volume level, yes, there's the Azure regional pairs. Then they have nonstandard pairs. So hey, actually I don't want to use the Azure pairings, I've got my own ones I want to do so I have that ability. So I think therefore of this service and how this differs. So now we have go totally run out of colors. It's purple, so if we have Azure. Net app files. There's there's a hierarchy of the components that I use as part of this, so the first thing I do is I create an account. So my account. Well that's bound to a specific region. So my count lives in a region and then within the account I create N number so I can have multiple capacity pulls. And as the name really suggests, the capacity pull for its attributes, it has a certain capacity. But then it also has a certain service level. Now it was service levels. There's three of them. I can think there's standard. Standard, I think is 16 megabytes per second. Per one terabyte. Provisioned. Then there's premium. Which is 64 megabytes, one terabyte. And then there's ultra. Which is 128 and obviously they're more expensive, they're ultra is more expensive than premium which is more expensive than standard. So I create capacity poles and then in the capacity pause I can create volumes. So if volume is a certain size. It is a certain protocol. This is where it can be SMB or NFS or dual. Now you get certain flexibility. Um. In the NFS version, but I think if you do draw it locks I think to 4.1 so that you have a certain locking to that, but then it's which subnet is it in? So when I with my virtual network I've got a vnet. I created a certain subnet that I as part of this I say hey, I'm delegating. For Azure net app files and F. And so then I link it to that particular delegated subnet. So that's part of the configuration I'm going to do to actually enable that. So that's how I can think about those processes running together. So such an app files another option, and again, I might use that because hey, I need a different type of performance characteristic than what Azure files may offer me. It may be certain compatibility with some net app software I'm using already. It might be certain features like the NFS encryption. There's just it's all about hey, I have options available to me. I'm going to pick the one that really just makes the most sense. So this was those Azure storage and Azure net app files. There's services I have available to me. But there's then there's disks. Now disks used to just be page blobs in a storage account. I would see them. I would have to worry about or. What's the maximum throughput of my storage account? How many page blobs should I create in there before that becomes the throttling? Managed disks does away with that. It abstracts away the storage account. Storage account is still there, but I don't see it anymore. I only would see it if I need to do some operation like copying it, in which case it would generate a shared. Access signature temporarily, which would show me the storage account that's kind of there behind the scenes. So I don't see these. Because it's abstracted, I don't worry about limits of a storage account anymore. I say what are the type of managed disk I want? This is the characteristics of performance I want. Disks and snapshots are now just native Azure resource manager resources. It's just a first party resource like a VM. Now, they've all many, many different tiers. So before I start talking about the tiers for a second, so let's just come back up for a second and think about this. So now my idea is I'm going to create. A managed disk. And the whole point is, yes, there is still. A storage account. There is still. A page BLOB. But it is completely abstracted away. I do not see it. I do not know it's there. It's invisible to me. Yes, it's there. I don't know anything about it. All I do is I focus on this really nice managed disk thing. Now this managed disk has a certain SKU, which is go back to my dark blue. It does have a certain SKU. So as part of that skew, it's got different performance levels. It has certain capacity. Now this is a provision capacity by pay for the size of the disk, not the amount of data I write to it. Then it has a Max shares property. So when I think of the SKU. There's standard hard disk drive and SSD. There's premium. SSD and there's Ultra, and there's actually an SSD V1 and a V2. We talked before about the idea of this capacity gets bigger, the performance gets bigger as well and we'll see these when we go and look at the options. So if I was to go and look at this page. Come on, wake up. Here we go. What we'll see here on the managed disk. It depending on the size. So disk size gets bigger. Well, so too does the IOPS, so too does the throw up. So these IOPS and throughput are getting bigger. As the disk gets bigger, so we can see that happening right there. There is also the concept of bursting. So up to a certain size. I get a certain amount of just free bursting. I accrue a bucket of extra IOPS and throughput so I can burst above my provisioned normal amount. Or if I've accrued the credit, I can actually burst up to this for a short period of time and that's free. It's just, hey, if I've accrued the money, sorry the the IOPS and the throughput, I just get that if it's over a certain size, so the P30, so 1 tebibyte. I don't get it free anymore. I can go up to 30,000 so I get it's much bigger burst, but I pay for that. If I want to use this, I have to enable it and I pay for those and that's all covered in the bursting. But premium SSD V2, so that was regular premium SSD V1, premium SSD V2. It doesn't have any of that. With premium SSD V2, I pick a size, then I can separately pick the IOPS and the throughput, and I pay individually for the additional IOPS and additional throughput I want from what is the capacity. That's exactly the same as ultra disk. Ultra disk. Also, hey, I individually pay for its IOPS and throughput, I get a certain amount of base IOPS and throughput. It differs based on the size. That's a bit different from premium SSD B2, but the key point is. For all of the standard hard disk drive for the standard premium SSD B2. These the IOPS and the throughput. The performance scales linearly with the capacity. For the V2 of the premium SSD and the ultra they have separate. And dynamic so I can change these even while the disk is connected. So they have separate little dials that I can change it anytime. For IOPS and throughput. So now I'm not just, hey, I have to make this bigger if I want a high performance and premium message DV, one has a performance tier. So separately from the capacity I could make the disk bigger without actually making it bigger. To get the performance of what the bigger disk would be, it starts to get very very confusing. I've got a whole separate set of videos on this, but essentially for premium SSD V1 I could change its performance tier to that of a bigger disk. It doesn't resize the disk because you can't shrink a disk. It will give you the performance of what would be the bigger disk, and I pay for the cost of what would be the bigger disk without actually changing the capacity of the disk. So manage disks. This is what we really think about for everything related to virtual machines. Which are then used by many many other services. Surprise is based on that provision capacity, not the consumed capacity. I can make it bigger so I can dynamically resize these. Not Ultra premium V2. But for others it can be connected. It can't be the OS disk, but if it's a data disk to an additional disk, I can make it bigger. I can never shrink them. I can never shrink a disk. Even if I disconnect it. I can't shrink it because inside there's a volume of data spread around. How do I'm not going to truncate some data? If I wanted to shrink a disk, I would have to create a new disk of a smaller size, copy the content, and then delete the old one. I can never shrink. I can only make bigger, but I could dynamically expand while it's connected a data disk. Then inside the OS would go and increase the size of the volume to use that new space that would suddenly show up. There is no GRS option. There's no globally redundant replication for disk. In reality, this was never a good thing anyway. For a disk, I might have multiple disks that make up my workload. If I'm trusting Azure to the asynchronously replicate my disks, how do I know that at the same point in time? If one is a few seconds behind, it's going to be in a really bad place. It's better if I need replication to do something inside the guest potentially, and that's what Azure site recovery does. It has an ingest agent that can actually creates changes, so pause any changes, flush things out to disk, do an app consistent snapshot. Or maybe it's just some crash consistency, but it understands it's inside the OS that these disks are part of the same thing and do a snapshot together so there's better ways to manage disk can never geographically replicated if I want. That do it inside the OS or maybe at the app level. Hey, let the SQL database do that. We talked about that in the governance and the resiliency session. Even ZRS is not available for certain types of service as well. Availability sets we're using less and less, but one of the things I can do we've managed disks is I can do an alignment. And what that would do is if availability sets remember puts each VM on different fault domain. What it will do is fault domain 0 use this storage cluster for the disk, storage for fault domain 1 user different storage cluster for fault domain 2 user different storage cluster. And what that does is remove a point of failure that the storage cluster was common between the three different fault domains. So each. Sets of managed disks would be a different storage cluster for each of the fault domains in your availability set. There was a Max shares property. So what Max shares does is I create a shared disk. I can actually have a disk that multiple VMS can connect to. So this would give me. Maybe I'll have some sort of cluster configuration I have. I could now actually have multiple VMS connecting to the same managed disk. It would appear as exports scuzzy persistent reservations. I could use it for my Windows cluster for example. I can use it for Linux. Clusters. Must be in the same availability zone if I'm using availability zones. But then hey, it's just going to show up and the number of machines that can connect varies. For most of the premium V1 standard, it's based on the size. If it's ultra premium SSD V2, then you just pick the number. I think it's up to 15 or something. So is the shared Azure managed disk documentation. So Ultra disks, premium disks. Ohh you go. So yeah, premium SSD and standard SSD. The number of machines that can connect depends on the size of the disk. But for ultra disk and premium SSD V2. You configure it between one and 15. So that would be the number of machines that could actually go ahead and connect to it. And just like a regular storage account. Uh, I can have customer managed key for the encryption now. It's not applied to the managed disk directly. The way this works is I create something called a disk encryption set. The disk encryption set is configured. To use a certain key in Azure Key Vault. And then I. Add a disk into the disk encryption set, so we'll then use that configuration for its encryption. So multiple disks we put in the same disk encryption set, but now I'm using my key that I've configured for that so that is available. If I want to have my own encryption. OK. Virtual machine storage. A VM is provisioned on a certain host. It has an OS disk which is typically durable, IE it's backed by a managed disk which is backed by drawable storage. 3 copies at minimum, but it can be ephemeral if I think about the host that the VM gets created on. So I drew the idea of hey, I managed disk here, but then let's expand this out. So let's actually now say OK, there's a VM. Well, the VM does not sit on magical cloud. The VM is running on a host, so there's a big box. And when my VM starts, it gets provisioned onto a certain host, and that host has a certain amount of local storage as well. This is basically always SSD. Now, the old original AV1 series VMS, the nodes had hard disk drives. It's always SSDs now. So the point is ordinarily, well, our OS disk used by our virtual machine, we care about it, we care about its state, we want to make sure it's always durable, and so for the OS. Disk. We use a managed disk. It's a pet. We care about it. But there's some types of workloads where we don't care. We can just recreate it every time we start it up. If it's a VM scale set it's creating from some template. It's an AKS node pool that's being created from some template. We're auto scaling. We're constantly creating, deleting these things. I don't need it to be durable. I don't want to pay for this. There's this local disk. This local disk has a certain amount of space set aside for caching purposes, for when I connect to premium storage, there's a certain amount of space set aside for my temp. So many VMS actually get a temp disk. So if this was for example my C drive, it's Windows, obviously Linux, I can use as well. Also what I see. Is for example AD drive that is not D for data. That's naming is unfortunate. It's. Remember, if I shut this VM down, this all disappears. This is totally ephemeral. It's temporary. I could lose this at any moment. In fact if I even go and look at a virtual machine. So this is my VM. It's easy. In Azure, we see I have a C drive on my C drive is absolutely running. On a managed disk. Then you have this temporary disk that's using that temporary space. It even has a warning on it. Data loss warning so because it's not durable. They put things like the page file on it. If I was a database I could put a cache file on it, tempdb for example. But that data loss warning says hey look. You could lose this. This is a temporary disk, subject to loss. No way to recover it abandoned. Hope all ye that enter. So. You would see this disc on. There are some skews that don't actually have a temporary disk and you'll see it called out. When you go and look at the VM and we'll talk about this, we talk about VMS, you would actually see it called out as, hey, it's not supporting. Those, but for most of the time, hey, it does actually have the idea of. A temporary disk, but. If I am one of these work clothes that don't care. Many times I can use either my some of my temp space or my cache space to store my OS disk. So it's using an ephemeral OS drive. There's actually benefits there because this is SSD super low latency, very very high performance. It's not durable, I don't care. It's thing happens to this box and it just create another one. So I can have the option of using ephemeral OS disk depending on the size of the VM, the size of the cache and the temp space available to you. Can I fit the OS disk in there? Many times I could do a firewall so if I am a stateless or I don't care about the state in the OS disk. This is a a fantastic option to do this. I can obviously add multiple other disks as well, so I might have data disks I want and then I can do well. What's the type of caching? Is it read, write? And maybe I don't want any caching if it's a database disk, so I can configure those various options available for me. One of the things I guess you should point out. All of these connections are using the characteristics of storage for the virtual machine, but realize the VM also has a network interface card and that has its own characteristics of performance. If I'm going and connecting to some other service, SMB and NFS, iSCSI, any of those, those connections are not going over it's storage characteristics, those are going over it's network. Limits. So just realize it's very very different managed disks of it's storage characteristics. Sometimes these hosts actually have Nvme, there are certain. NVME types of VM's in the host they have again it's non durable to have amazingly high performance that I use for some caching type mechanism potentially. Or hydrate it and warm it up when I start up. But I've got some other mechanism trying to replicate it between a cluster of boxes. So even if this failed my data is still durable because it's being replicated to so many other boxes, but if I use a file based or any network it's going over the network. Capabilities of the virtual machine, it's not going over the storage capabilities of the virtual machine. So just important you have that distinction in mind. And that's really the key point, I think this all gets mapped out, but so I can use the scratch drive, I don't care about it. This SSD based that temporary drive I can add additional data disk. The number of disk I can add depend on the VM. They support different numbers of data disks. I can dynamically add these, I can dynamically remove these. I can hot add and remove data disks. As I talked about, some of them have Nvme disks and there are limits. There's limits on the VM, the VM would have storage limits, the VM will have network limits. The managed disk has limits. So it's really important to understand both of those sets of limits because if I add too many disks to a VM. Well, maybe the disks capabilities far exceed that the VM, so I won't get the performance I'm expecting because the VM is the bottleneck. Likewise, maybe I just have one really really big disk. And the VM can do far more, but the disk is now the bottleneck. So when I think of storage performance, I have to look at both the VM and the storage to see where is the bottleneck and one of them is going to. It's not going to be infinite. The VM limit will get hit first or the storage limit will get hit first. The question is, before I hit that limit, am I meeting my requirement of the workload? And again, maybe I'm hitting the storage limit my VM, or if it's like an SMB or NFS, maybe I'm hitting the network limit to my VM. Or maybe I'm hitting the limit of my storage account. I'm hitting the limit of my Azure net app files, in which case I would look at the metrics available for the VM, look at the metrics available for the storage service and work out hey, is it IOPS? Is it through port bits per second? What's happening to work out where are those things not meeting what I need? Some of the VM's have burst capabilities as well. Uh, just a quick note. This is not a huge problem anymore. I can add lots of disks, but now with the premium disk with the ultra disks, I can have a single disk with just ridiculous amounts of performance. But there was some scenario where I couldn't meet the requirements through a desk. Obviously I can add multiple disks and basically just going to join them together. So in Windows I could use storage spaces. What I don't want to use the raid set because remember, a raid set adds redundancy. Every one of my disks already has three copies I don't want to waste. CPU cycles, working out redundant bits or parity bits. There's no point in doing that. I never want to do a raid set. If anything I could do a simple. I just want a basic hey, join the things together. Maybe I don't have to join together at all. Things like SQL Server can have multiple disks and it can break up the workload over them, so I could look at that as another option. Obviously I'll get the combined amount of IOPS if I do multiple disks, but again, I keep coming back to with premium SSD V2 with ultra working independently, change the IOPS and the throughput. I think this is becoming a smaller and smaller issue. A final word, so there are tools. I showed you Azure Storage Explorer already, it's free. It's really nice that I can use Azure AD based authentication and I kind of showed this one of the storage account with that RBAC. It might use the storage account key by default or I can use the Azure AD based authentication. So if I just jump back for a second. If I go and look at my storage account. When I was going looking at content. One of the things you would see is it would tell me how am I accessing it. So when I'm looking at this right now, because I have permission, it's using the access key. But I could say actually I want to switch to using Azure AD user account. I still have the access because I've data plane based permissions. Now if I turned off the access key, it would have to use Azure AD account and that data plane access. But there is obviously a difference between those and so the HTML5 tool, this Azure Storage Explorer, it can use both. I can give it a shared access signature, I can give it the account key, it works with all the different types of workload. It's really fantastic tool. And it supports server side copying. So Azure supports asynchronous server size if I were to copy a lot of data. From one service to another service, rather than it bringing it down to me as the client and then copying it back up, it just says hey you on the server side, copy it between them. The data doesn't bounce by my high latencies slow machine, you just does it on the Azure side. There's a job I can inquire on the state of it. When I copy things with Azure Storage Explorer it does exactly that. It does a server side asynchronous copy. There's also AZ copy this great little command line tool, but super powerful. Again, it can do the server side copying of data blobs files to and from between storage accounts. I can set a certain amount of concurrency to optimize the throughput multiple threads going on it can do a sync mode so it will just do a delta based set of changes to the data that's happening. So this is a really nice utility as well and also sometimes. I just don't want to copy the data up to Azure or from Azure over the network. I'm going to doing a huge scale. So one thing I can do is import export. So for BLOB file disks I can take my disks. It encrypts them BitLocker and I send it to an Azure data center or vice versa, they can send me my data so it's an offline transfer or. So that's 2 1/2 or 3 1/2. Or I can actually use Azure databox disk where Azure provides the disks. They'll send me the disks, I'll copy my data, then they'll let import them into my storage account. If the volume of data is so high, disks and just not going to cut it. And there's thing called Azure data box. Azure Databox is this BIG4U appliance that they're shipped to you, you're connected to your network and then using file based protocols like SMB, I'll copy my data to it, it can I set it up in advance with multiple storage accounts? It's a I think it's 700 or sorry, 80 terabytes of usable space on this thing. I ship it back now, copy it into my storage account if I have a really really lot of data. There's a data box heavy which is literally heavy. It has to ship via freight Courier. It's PS500 and it supports 770 terabytes of usable data. So I can get a lot of data moved with this. It's very long lead time. It's going to take like a week to ship. Then you have to find a place for it in your data center, copy the data, send it back. But there are options available for offline data movement. And a final word. I don't think about data governance. Data is probably our most important asset today. So with my data, where is my PII? Where is my data around this project? Where is the data I need to care about? What is the lineage, the history of this data? So we have this super important asset. I need to know where it is. I need to classify it. I might need to write protect it, I might need to. Whatever there is things I need to do about this, I have to make sure I'm compliant. So Microsoft Per view provides a data governance service. Of course, the entire hybrid data state. It can be an Azure, it can be an other clouds, it can be on premises. There's a concept that I create a data map, so in place you can go and understand the data, discover the data, apply automatic classifications to it. I can create my own custom. Massive classifications through regex expressions. Got a whole video on rejects if you want to care about that. And then? I have certain scale units for how many operations that have fast it can consume this that can create a data catalog of the data it's discovered so I can go and query it and find out information about it. I can get insight on well, what is my compliance state? What where is my sensitive data? What is my classifications? I can apply policies to make sure I'm protected at scale. I can even do data sharing, so this is a really cool capability. But in my storage account I can. Both BLOB and Data Lake essentially mirror the metadata to storage account in a partner organization. And. They're just accessing it against their local storage account. The data is still in my storage account. I can revoke it at any time, but they pay for their operations. I pay for my operations and the capacity, but I can actually share the data in, for example, my data lake or just BLOB with a partner without actually having to copy the data, so. Microsoft purview is just a very, very powerful thing. Now I know we covered a huge amount of things. As always, if we go and look at everything, there's just a huge amount. But. It's just nice to understand the options available to us. I think there's this very rich sets of services that we're going to use those different scenarios. Hey, is it blobs scenario, is it a table, queue, files and then files? Hey, I can get this great integration with things on premises. How do I think about the security around them? Different resiliencies based on what I need. Hey, VMS, managed disks, there's a lot of capabilities that just native to the environment. But I hope that was useful. That obviously concludes this part of the master class and I hope to see you next video. Take care.