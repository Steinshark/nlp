Hi everyone, welcome to part 10 of the Azure Master Class V2 and in this module we're exploring monitoring and security to things that are actually pretty intertwined. I need to make sure I'm monitoring, I'm need to make sure I've got great insight to help ensure I can achieve the security I need for my environment. So we're going to explore a few different areas where look at Azure monitor which is really our insight and our entry point to most things. Monitoring in Azure and even beyond, but we'll see how that then leads into aspects of security. We'll look at some of the security solutions and then finish off with two key things we've mentioned in other modules, specifically managed identity and then Azure Key Vault. So that's our goals for this module. So I think about monitoring in Azure. It is a shared responsibility model. That's an important thing to understand. Remember we talked about IAS and Paas and software as a service. We're depending on which of those services we're using, the responsibilities shift. But for any of those things, and it's kind of think about this, you'll often hear about the idea of racing. So if I think about RACI, because even if I'm not responsible. So the R for responsible for some aspect of a service, maybe I'm accountable. So I'm actually still ultimately will be held accountable for making sure we meet that service, although maybe I'm not doing the actual job, I need to make sure this requirement is regulatory need is being met. Maybe I'm consulted, so maybe I give input, I give guidance on something, but I'm not actually responsible. I'm not actually held accountable for it and eyes maybe I'm just informed it could potentially be for some type of service. I'm the end business user. And I just need to know if there are things going on. But we have these different aspects of responsible, accountable, consulted, informed for all of our different types of services. And when I think of monitoring, even if I'm not maybe responsible for a particular aspect of the service, maybe I still want to be informed. So I still do monitoring. So I know about if there is some change to the performance, is there is some outage, maybe I am being held accountable. So I need to monitor these things to understand. And my meeting the SLA that my architecture needs to meet O even if I'm not responsible for a service. It's the responsibility maybe of Azure. I may still be accountable for it. I may still want to be informed around it. So we still think of this even if I'm not responsible for some part of a service. Now with that said. As I move U from infrastructure of the service like VMS to platform as a service to software as a service. I am responsible for less and less, but that's still again may not change what I want to be informed about, what I may still want to monitor. So I know and my meeting some SLA. Is there some service outage that is going to impact my overall application I'm offering to my customers? So I have to have a plan. I have to understand what is important to my service. What do I need to know about? And then make sure I'm monitoring for those components. OI want to think about service health, I want to think about availability and that Azure monitor is going to be that primary conduit into all the things I want to know about. So what I have to think about is for any service I have. What are the things that matter? And what level of detail do I need for all the different scenarios so we have to think about? It's not just I'll. Each general health being what if there's a problem, if I need to troubleshoot, what data would I need in troubleshooting environments? What data might I need to be able to feed in as part of a security solution? So I have to think of all the different layers to make sure I'm not missing something. And again, monitoring and observability is going to feed into DevOps. It's going to feed into security. So what's first? Well, we always think today. Identity. From a security world, the network is not really the security perimeter anymore. It's shifted to the identity. Many services we use don't live in our network anymore, they're in the cloud somewhere. They're used from any device anywhere, so trying to use the network really doesn't work very well, especially in a zero trust type scenario. I don't want to just trust how you're on this network, your VPNs, you're good to go. We want to constantly evaluate and check. So my Azure Active Directory. That's our first point. Remember Azure AD is the identity provider for our Azure services. So step one. Is my Azure Active Directory now. There are many different types of monitoring we can do against Azure AD. There are some obvious ones. There were things like signs. And signs obviously can help track what are we getting some weird combination. We're getting some new anomalous type behavior. The signings are out of our regular pattern. They're from locations we don't normally see, some monitoring the sign INS attempts. So these ones that fail, that will give us some indication of, hey, maybe there's some compromise trying to occur. There's audits so I can go and see how your thing is changing and it's probably easier to just go. And look at this so if we quickly jump over. If I was to go and look at my Azure Active Directory. Now Scroll down, you'll notice straight away. I can see. Well, there's sign in logs, there's audit logs, there's provisioning logs. But there's even more than this. If I'm doing identity protection, there would be log specific to that. And if we go and look at our diagnostic settings now, diagnostic settings, we're going to see time and time again on all of the services and we're going to come back to this. But if I was to add a diagnostic setting now it shows me all the different categories of log and here you can see I can break it down even further. Sure, there's the audit logs, there's the sign in logs. Then we can see things like noninteractive user sign-in logs, service principal sign in log, some service principle used by an application, by a service, managed identity, ADFS sign-in log, risky users coming from that, identity protection, user risk events. There's all these other types of logs that I may care about, and yes, I can view them in the portal, for example, but maybe I actually want to get them to some other type of solution. So what we're going to see throughout all of this is, yes, there might be some native way to view them, but it's probably also going to be hey, I want to send them to something else. Now for these logs, we often think about retention, the ability to go back and view them at some other time from a retention perspective. I can think it's seven days. Retention or 30 days if I have the premium? Azure AD attached to my tenant, so I have an Azure AD premium P1 license and there are some other combinations as well. But if I have premium, it generally adds additional days to that. But remember as we saw, I can always take some of these logs and send them somewhere else as well. So that's our identity. Then we actually get into Azure Resources and if we remember the top level will construct for where I can provision resources. Well, that's inner subscription. Now the key type of log that I think of with a subscription, this is going to be our activity lock. Now the activity log is all about control plane activities that are to resources within that subscription. This is not data plane. This is not. I updated a table. This is not I created a new BLOB. This is hey, I created a resource, I'm modified a resource, I deleted a resource, I regenerated some access key for resource anything down at the control plane. Will register and be saved into this activity log. Now there are other things as well, for example, there's service health. Things that have impacted some resource that I have within my subscription. And these these are maintained for 90 days. And we can go and see these as well. So once again if we jump over super quickly. If I was to now just go and look at a subscription in my environment, let's look at this one and look at the activity log. Well, I will see things related to the control plane over example here you can see hey I started a virtual machine an hour ago and I can see who did it, I can see details. About what happened there. So see the exact time so 349. GMT I could see the JSON around. That's all the details. I could also see change history. So I get that detail of everything. So old values, new values on that control plane. Because it started, it gets some additional information around it, but I get all of the data. That I need to see and once again. I can export the activity logs and we see this familiar diagnostic settings and once again I can pick what do I care about. So we can say, yes, administrative categories. We also see things like the service health, so wanted to send out service health alerts recommendations and I get this familiar set of destinations, which again we're going to come back to. So we have these built-in logs for our subscription. Once again, we saw there's a certain amount of built-in retention, but if we want to, hey, we can send them somewhere else. Again, the idea being maybe I need to keep them for longer than that. Maybe I have some other system that's going to perform analysis against them to help me detect variances, help me detect signs of intrusion and some vulnerability. So that's what we have this capability to actually. Send them somewhere else as well. Maybe it just natively does what I want, maybe it maybe it doesn't. So the whole point is all of these logs right here. This is giving me those control plane activities now within the subscription, then we have actual resources. Could be a storage account. Now this is all about the Azure resource manager. These are resources in ARM. And these VM a Kubernetes cluster, but there were various resources. Now those resources, they have their own sets of logs, they have their own metrics. So metrics is some numeric value that's in a time series, so it's some number, maybe CPU utilization at a certain time increment. So it's a time series database time moves on this counter values, it's always numeric. Is on as well. Now this is where it starts to get interesting. I'm always going to have metrics. And those are just available and those metrics get sent to Azure monitor metrics, a time series database. So they are, they're there for me. There were also logs. However, these don't actually get generated until I do something so to get the logs I have to configure. Because there is no native storage to handle text based logs for the various types of logs that will come out of resources and these logs will vary by the type of resource, I can imagine a database has going to have very different types of logs it might generate compared to a virtual machine. So I have to say, hey, I want these logs and we're going to explore that in a second. So I can get logs. But they don't get generated by default. There's no built in automatic storage to actually have those. Now for the metrics, these are going to get stored for 93 days. Is that worst case scenario of three months. If there were, don't think there's ever is 3 months in a row that have 31 days, maybe there is but 93 days. So I have three months of metrics that I can go back and look at. That's really the key point about this. And I guess this would be a good time to look at the diagnostic settings. So once again, let's look at one of these. So if I was to go and look at a virtual machine for example. Let's go and look at my demo VM. So come on virtual machines demo VM. I don't have to do anything special, but under monitoring I just see metrics. Now the exact metrics you will see will again vary by resource. A Kubernetes cluster will have very different metrics from a VM will have very different metrics from a storage account. But for example, if I was to just go and look at this VM, I'm looking at the virtual machine host. So it's things that it's getting from really the fabric. It's not looking inside the guest operating system. But that is an option as well, which I'd get then counters around logical disk space, things that it has to ask the OS about. But I can pick a metric, so for example maybe it's something interesting percentage CPU. And it shows me, it shows me that percentage CPU use for that virtual machine. O those are just standard, those are just built in for me. Then I have diagnostic settings. And then, hey, I can set up a diagnostic storage account, for example, to send some of this data to and that's going to gather certain additional metrics for that. Let's go and look at something a bit more standard. Let's look at a cosmos DB. So I want to show the diagnostic settings. Already saw it twice, some resources behave a little bit differently. But now if I look at my diagnostics settings, notice logs, the logs are very different for a cosmos DB database. Hey data plane request, Mongo request query, runtime statistics and ensure at the end it does have some metrics I can send as well. What I'm really interested is in the destination details. We always have these essentially these three core options and then also there's the ability to integrate now with some partner solutions. The partner solutions are, as the name suggests, made by some ISV, some independent software vendor. And they have some solution that can work with those logs that would then give us some feature. Maybe it's a SIM solution, so it wants to log so it can monitor them and give data around it. So if we think, what are those three options? Why do I have this log analytics workspace, storage account and event hub? Where do they differ? Where would I use them? So I can think about well. This is just the ones we've explored so far and we already saw there were metrics and logs were available to be sent. So what we have now is this idea of diagnostic settings that are pretty much the same for all of them. Now this is optional, I don't have to do this, but if I want the logs from resources, this is the only way I can go and turn those on. So these diagnostic settings and this is applies to really all of these different options. And I've got 3 core targets and then we had those partner solutions ones. So I have a storage account. I had event hub. And then I had a little bit of space for this one and I had a log analytics workspace. Now, log analytics, you'll also hear called Azure monitor logs. It's the same thing. It's still log analytics workspace underneath. So we may just hear it called Azure monitor logs, but it's a log analytics workspace. So these three options available to us. And if I think about what my possible use cases are of why I want to send logs. Well, maybe I'm sending them just to keep them. I have to keep these logs for a huge amount of time. I don't really have any intention of having to interact with them, I just have to keep them. In which case storage is a very attractive option because for storage while it's can be long term. And it's cheap. It's just creating blobs, so it's very, very cheap. But it's not super useful to interact with. If I actually wanted to go and start searching for things or running aggregations, this is not a great option. But if I just have to destroy it and then maybe I'll bring it back into saying else later on, hey, that works. Event Hub remembers a publish subscribe, so I'm publishing the data. And then something else can subscribe and read those records off. So this would be very useful if I had some kind of external SIM. So I had some solution, some incident management, event management solution. That I need to send it the data to pretty much in real time and it's then going to do some work. So this publish subscribe let's multiple systems read off and get those records. Now log analytics, we're going to come back to this in more detail. This is very useful, yes, for the storage, but it's more about the fact that once it's stored there. I can do very, very rich. Analytics against it so I can then analyze. I have this whole kusto query language to which I can run queries and get information about this. And so many solutions. If I actually want to maybe get some insight into the data in Azure, we'll send it to a log analytics workspace, because then not only am I storing it, I'm then actually have the ability to run queries against it to get insight against it. There's machine learning that builds on top and looks at, well, what's the regular type of interaction and numbers we see now? I can see saying that falls outside of that normal behavior. And we'd also have and I should write this on the board as well, there are those third party. ISV solutions. If you've enabled those, many of those will use. The event hub. But rather than you having to go and think about their preconfigured for you, that's really the idea around those. Additionally. We can set these up from diagnostic settings. But another thing I could do. Let's say as a company I have a requirement that I need everything sent to a log analytics workspace to make sure we've got some central solution that's gathering that data. Maybe that's doing that monitoring as the logs. Maybe I have to send it to an external event hub. Or I can also configure this rather than going into diagnostic settings per resource and going to Azure monitor and do bulk diagnostic settings configuration. But I could also configure this. Via Azure policy. So if you go and look at Azure policy, you'll see a whole number of different policies just related to diagnostic settings. Turning on a send to a log analytics workspace, sending to an event hub, sending to a storage account. So that could be a very powerful solution if I need to do this on a bigger scale. OK, so we have the resources. Realize some resources, they actually have data inside the resource, and we started to look at operating systems for a virtual machine. There's a whole set of different data that I may want to get from inside the VM as well. So if we now think OK inside the resource. Well, let's think about. The guest operating system. So we'll go over to this color now. So we've in the resource and it's going to be different types. Remember maybe it's an OS. Now once again I could think about for an operating system they are going to be metrics. But there can also be logs. Maybe it's taking syslog, maybe it's taking some of the event logs from Windows. Maybe I'm taking IIS logs. There's different types of logs that I make care about. And so I may want to grab these as well. Now, previously there were multiple solutions to how we would do this. There was the log analytics agent, there was a diagnostics extension which would take a certain data and actually send it into metrics itself. There was Telegraph to get diagnostic information and metrics for Linux. They're still there today, but they're all going away. They're all being collapsed down into a single Azure monitor agent. So all of this is powered by something called the Azure monitor agent. Again, today, you may still hear about log analytics agent, you may hear about Telegraph, you may hear about diagnostics extension. They're all collapsing into the Azure monitor agent today. There are still some solutions that use the older components depending on what OS I'm using and where I am in my migration journey. There are things like the dependency agent that used to depend on the log analytics agent, but now it's moving to AMA as well. So depending on what I'm trying to do, I may still see those older terms but realize it's definitely moving to that Azure monitor agent. And one of the huge benefits of the AMA. And I've got a whole separate video on this. It's not just it collapsed it to one agent previously. The configuration of these because I have to think about, well how do I configure this? How do I tell the agent? What to capture? What logs to capture, what metrics you care about. And I had to reset a workspace before or I did it via an extension before. It was very messy. Now what we have is the essential data collection rules that I define. I tell it what I want data to be collected, I tell it which resources should get this role and I'm done. So I have the essential data collection rules that power the Azure monitor agent. And what this will do is hey, it will then send it. The Azure monitor agent will send it to. Log analytics workspace, but it can also send the metrics element as custom metrics. To metrics, so there's a custom type of metric as well. So there were ones that are native to the system. Then there's custom where we can add additional time series, so numbers into the metrics and we'll see why that's a benefit later on. But it's all about the visibility, but also the speed in which I can react to changes in those numbers and if they're stateful or not. Is it just going to understand is it triggered some threshold or not, or is it just going to fire every 5 minutes when I go and check it? There were some pretty big differences around those. So for the OS, this Azure monitor agent works for Windows, it works for Linux. It works for virtual machines. It works for virtual machine scale sets. If I'm using Azure arc, I can leverage it there as well. And so the whole point of this is, it's an extension that will show inside the VM. It uses a component in the VM, but it's going to give me now insight inside that guest operating system as well. And again, we have those data collection rules. Let's see if I can find one super quick. So if we went to just Azure monitor. What we now see is these nice data collection rules. Now some of these would just get set U for me when I turn certain things on it will just go and create a data collection rule. In case you can see some of these hey VM insights, it just went and set this U for some of the resources. But here are some ones that I manually configured. So I did one for Linux, I did one for Windows. And you can configure what it is you actually care about O what are the data sources you want. So notice I could do Windows event logs as well. I could also capture IIS logs. I could capture custom text logs if I've set up a collection endpoint for those. Now this is because this is set U as a Windows Data collection rule if I was to go and look at a Linux. Data collection rule. We notice now it shows hey Linux syslog. Instead of those Windows event logs, OI tell it which things I care about and so for me I configured some performance counters, but there are basic ones that are custom ones so I can add other things I care about. I tell it where I want it to go to. So I set up Azure monitor metrics, so it's going to take time series database and then you target, well which resources do you want these rules to target. So it's all centrally configured, there's just going to go and give me that information. Into those areas. So this is a really powerful now capability that we have to make sure I'm capturing what I want. And I'm doing it just in one place. I'm not going to a bunch of different log analytics workspaces and trying to find, OK, where was this bit, where was wherever. And then all this goes here, this goes over there. It's just a nice central point that I can manage all of that from. Now. Operating systems are one type of resource, but there are many many others I might also think about. Well, there's Kubernetes so that Azure Kubernetes service, so with Kubernetes. Well, there's also different types of collection I might want to do now. You'll see things like container insights. So we've container insights, it's using now a pod which is running an Azure monitor agent that's going to go and send data to hey, my log analytics workspace. Percuba Inators is interesting that it also has something called Prometheus. It has these Prometheus metrics. This is a standard, so Prometheus is all about this. Think CNCF standardized set of solutions and ways in which you interact to both scrape the metrics and then show the metrics. There's a whole separate type of query, a prom QL. Grafana is a great third party to then visualize this data that it's capturing so for these Prometheus metrics. And we have. There's now a native Azure Prometheus solution. Rather than me having to set UA Prometheus server to go and capture and store these logs, I talked about a log analytics workspace. There's a new type called an Azure monitor. Workspace. At time of recording this workspace is only used. By Prometheus metrics. So if I configure this for my Kubernetes environment. I create an Azure monitor workspace. It's different from a log analytics workspace. It's an Azure monitor workspace and then those scraped Prometheus metrics from my Kubernetes will be sent to this Azure monitor workspace, where I can then visualize that. It's also managed grafana as a great way to then show that data. Now one of the really nice things I will say I'm going to focus on the Kubernetes. You're going to hear me say insights. Insights is all about providing a curated set of workbooks. I ways to interact and query the data. Dashboards so ways to view the data. And it will autoconfigure. The metrics it has to capture and the logs it has to capture. So the audit really have to maybe even know what I'm doing, but it's going to give me a great starting point to go and see what's going on now. One of the challenges in the past is I pay for log analytics or I pay for the maybe I should write this in as I'm saying it. So dollars are important. So what I pay for for my log analytics workspace, I pay for the data ingested, so I pay for the ingestion and then I pay for the retention if it goes beyond the default that's included, which I think it's normally 30 days and sometimes if I think I turn on Sentinel get 90 days but I pay for their data ingested. Well if you consider Kubernetes may have a lot going on, there's the pods, the nodes, the deployments. Services. This is now configurable. In two ways I can configure. Both the interval. So it used to be every minute, now it can be between one and 30 minutes. If you think about it logically, if I was capturing data every minute, well there's X amount of data being ingested that I pay for. If I change that to every 5 minutes, was now 1/5 of the data because it's now only capturing those metrics every 5 minutes. Now, obviously I lose some fidelity, but it's now 1/5 of the data and 1/5 of the cost. But I can also control the namespaces. So in namespace is a particular grouping of information about some aspect. Could be pods, could be the nodes, could be the cluster. If there's certain aspects I just don't care about, I can add and remove the namespaces I want. O again, if I took out a whole namespace, that's less data, that's less cost. But then certain dashboards workbooks won't work because the data it needs to function will not be there anymore. But I get that option for the configuration to control the cost. I can also think, well, there's my custom applications. So hey I go and have my application that I write. This could be a J2E application, this could be a net application. Once again, there are metrics and there are logs. And these are captured through application insights. So I can enable application insights and then these will actually just go and get collected and guess where it sends them to. Azure monitor locks that I can then go and for applications App Insights adds a lot of value. It goes beyond just hey the performance and if there's a problem it can give me user experience behavior. There's actually components that can run on the client side as well on the web server to show what the users normally take this route through the dialogues they normally give up here. So I know hey I need to give some attention to that component. So a different aspects of what I can do and configure through that. There is a another option, so for log analytics I can also configure what color to use for this one use this one. There is also a rest API. And the rest API essentially means what I can send anything. So for the Restful API. I might have some data that doesn't have an agent, but I want to send that to a log analytics workspace so I can do that with the rest API. Now. There's actually a way I can send this to metrics as well, so I'm focusing on the logs, but there's also an API to send it to the custom metrics, so I could also send things up to the metrics atomizer metrics time series database. But from the logging perspective, the way this is work, it's a HTTP data collector. It's expecting a JSON payload which will have the data that then we'll get stored as an entry in a table in my log analytics workspace. Now if my data isn't JSON, one of the things I could do is let's say CSV or XML I could write. I don't an Azure function use some serverless technology that would parse the source format like the XML or CSV. So it would deserialize it into just a regular object and then serialize it into JSON. IE take the object and convert it into a texture representation, serialize it in JSON and then I could bring it into my log analytics workspace. But also by doing that maybe I could compress it, I could deduplicate whatever I need to do. To change how I'm actually ingesting that data into my solution. Now if I think about this log analytics workspaces, look at these options. Again, storage cheap, store it for long amount of time, but it doesn't give me any way to really interact with it. It's just there I need to keep it. If I actually had to do something, I'd have to bring it into another solution, event hub. This is going to subscribe. To a certain topic and then it's going to get those records, but there's stores it and lets me interact. Now. Look at it's workspace when we talk about those retentions. The maximum, so it's configurable so it has a 2. Year maximum. But it is configurable. Retention. Because remember, I pay on 2 dimensions. I pay, yes for the data ingested, but I also pay for the retention once it goes past just the default retention that's included as part of it. So I'd want to be super careful that hey, I don't just say let's keep everything for two years and it's like, why do I have this big bill? Was because I said two years retention and I'm paying for that. So once again it comes back to I don't monitor things for the sake of monitoring it. Hey, I love monitoring. I'm monitor because at some point I'm going to want some insight or be able to go back and look at, well, why did I have this problem? Why did I see this thing? So we configure our monitoring to capture the data and we set the retention for what is the interactions I know I'm going to need to be able to do. How long do I want to be able to go look back in time? That's what I would configure the retention for. So it is important to understand that. Now, because the log analytics can get pretty pricey, we want to be really careful with this. There's actually now some options we have around log analytics and how it stores the data. So the default when I just bring data into log analytics workspace. It stores it and something called an analytics. Log. Now these are the priciest, but it gives me all of the rich KQL, the acousto query language, the ways I can interact and ask it things. That's how it's stored, so it's stored as an analytics log. But maybe I don't need all of that power. Maybe I don't need to do these fantastic aggregations and these cross table queries. I just have a fairly basic set of data coming in that I just need to get values from just from within that table and I want to try and do it much much cheaper. So there's also now. Basic locks. Now, basic logs are cheaper, but I pay a small amount of money for when I run a query against it. The types of queries I can run are greatly reduced. There's no summarization, there's no aggregation. Think I'm searching for a particular record. That's really what I'm doing. I'm searching for some particular thing. But these are designed for these very high volume, very verbose logs that I might use for debugging, troubleshooting, auditing. But I'm not using them for bigger analytics. I'm not using them for auditing purposes. I can use these for some of the built in logs and for my own custom logs, for example data collection rules. I could use a basic table O. Again, they're cheaper is why I would do this, but I lose some capabilities. These get stored for eight days. It is not configurable. 8 days. It's like, hmm OK, that's that's limited, so I'll write in limited search. Let me see query, it's a matter of time. Certain built in tables can use this now and I talked about container insights earlier. Well it can actually leverage this. So if we jump over, let's look at one of my log analytics workspaces for a second. So if I search for log log. Analytics. So I've got my workspaces. South Central. And one of the nice things now is if you go to settings, there's actually tables. So I have a little interface to actually go and interact with the schema, see what tables I have. And one of the things you will see here is it will tell you the plan for the tables. The plan is the table type. Notice there's also showing me. The retention say this 90, this is the workspace default OK so 30. And I can go and see these are all analytics. Tables here. But if I scroll up and look at container log V2O, container log V2 is the new type of container insights. And if I click the little three dots and manage table, this one will actually let me change it to basic. OI could change the type to basic. So they actually change container insights to. They are not send the logs cross table, they only send to this single table. Which means I don't need cross table queries, which means hey basic logs would actually work for what I need. So I could go and change now this to basic. And notice it's saying interactive retention. Is hard coded to 8 days. It can't be less, it can't be more. It is 8 days. But also notice there's this. Well, the total attention. Is configurable and it's set to 30. So it's showing me, well, there's this archive period of 22 days, and if I change the total attention period to longer. Just that archive. Is going to go up so. I'll closeout that and you'll see this for all of them, even the ones that are analytics. If I managed a table. I could change the total retention period to longer and then we see this rchive will. Go UO what what's going on here? So the whole idea of this is once again, how can I? Control my cost a bit more. So the analytics logs and the basic logs I can interact with analytics logs, full interaction, basic logs, little bit of interaction just within the table. But then maybe there's times that I need to just keep it for longer again because those may be regulatory requirements, I don't anticipate having to interact with it or maybe it be very rarely, but I need to keep it. So then we also now have the option. To have archive logs. And as you saw, what happens here is I set a retention. For the analytics or the basic log and then the total time I say I want to keep it for the additional time it goes and writes it to. Archive logs. Now I cannot query this directly, it's cheaper, but I can't query it. If I then want to query data from archive logs, I have to do one of two things. So to actually then interact, I need to. So many colors so to get it back and doing something useful. I can either perform a search job, there's this particular search job and what this will do is it will run here and it will bring the data back into a new table in my analytics table and I think it has_SRHC to show you hey it's a search result. Yeah _SRCH table will be created here or I can do a restore. Hey just restore this set of data. From archive logs and that will create a_RT table in the analytics log. So if I and I have to pay for that and I have to pay for the data to be brought back as well. So there's costs associated with interacting and bringing things back from the archive logs. But hey, I don't really anticipate it. I need to keep it for seven years. This is a very attractive option because this can actually be retained for up to. Seven years. Seven years, the popular number in banking, have to keep this for 7777 years all over the place. Now that search job I talked about, I can actually run against basic logs as well, so I might sometimes need to bring things back into my analytics so I can also run that search job against basic. I don't have to if it's just a simple query. Remember, I can run limited queries directly against the basic logs and just get the results if I had to perform some richer type of search. I can run a search job against basics and those results would then get stored in my analytics lock so that is available to me. So those are some of the real key types of interactions that we'll see and what that's really doing is getting that data. Into either the Azure monitor metrics time series database or primarily for interactions we want to do the log analytics workspace, but hey, I could also send it to an event hub or storage. One thing I'll finish with on this picture is some solutions, as you saw, don't use diagnostic settings, they just send it directly to a log analytics workspace. Well, what if in those scenarios I actually wanted the data in event Hub? I wanted to send it to an external solution which want to store it in storage. There's two different things I could do. There's a capability to export. I can set up an export job from my log analytics workspace at a table level. So if I set this U what it's going to do is everything in a particular table, so it's at a table level. There's no filter in, it will be everything, so I could set up an export. And it will send it to. Event hub. Or storage account O every single. Record in the table that I set this up for. It will send it. It's pretty much in real time. I think there's five minute increments in the files it creates in the storage account, but this will just push it to the event hub if I want to actually filter it. Or then I could write something. So for example, I could write a logic app, some serverless function, then I could define what records I want to take. And then hey, from here I could send it to there. Well, I could send it to a storage account so built in. No filtering everything in the table that I set up the export for. Or if I want to only send certain things, hey write some server list, could be logic app, could be Azure function and then it will be responsible for actually then sending that on. So I have those choices, but we have those 3 core things built in to send data. OK. So what we actually do with the data? So we might visualize it. One of the common things we want to do is we're sending it because we want some way to see the data in the environment. so I'm thinking of dashboards. So I could think about, well, OK. We've got this over here. I want to create some dashboards. I want to create a workbook. Maybe I want some ability to actually share it with people. Maybe I want to do some deeper analysis of the various issues. But maybe I also want to be able to react. Hey, there's a certain loggers come in, which is a bad sign. Maybe the metrics has gone past some threshold uncomfortable with hey, I need to alert something that's busier than it should be, so there's different things I might need to do. With different solutions, all of these different aspects as. You can imagine O if I think for a second about those options so I'm going to I guess draw back O hey we add the storage where the event hub we had log analytics workspace. But remember we also had this Azure monitor metrics time series database that was just native to the platform. So if we take a step back. Let's throw out those things. So we have the activity log. If the subscription. Which also remember includes the service. Health. We had the time series metrics. And then I can also think about and graduate over here so I can fit things in. We had the Azure monitor logs, so I'm just going to write logs, but that's a log analytics workspace and those logs are broken down into separate tables. Which are particular aspects, different namespaces for different types of service that we're leveraging. So in terms of hey, I want information, I want to know what's going on in the system. I want to be able to visualize things. Well. The first place we might just go is from all of these. I could create a dashboard. A dashboard actually is great that, well, I can take all of these insights into here. So from a dashboard I could read in things. Obviously metrics can be fed into here I can feed in data from log analytics workspace. So I'm writing KQL queries. That will then give me some set of results that I can show in here I can get things from my activity log. And then I could have bits of data, I can get little pictures, I could have some chart. I can also bring in information from the resource graph. So the resource graph and we'll talk about this in the final part of the master class. But it's. A way to interact a huge scale with information about the resources in my environment O other than having to talk to the RM API and say hey list out the VMS, I can use a KQL query and say hey select from VMS and it's really really fast and it doesn't go against my quota of the interactions I can have with the rest API. So it's really fast. It's really huge scale it supports. I could also run queries against the Azure resource manager. So the regular arm API, maybe. There's things I want to know about in here. And one of the nice things about dashboards is, well, I can pin elements of things I look at very easily to a dashboard and dashboards are very easy to share. With other people. So I can go and create a dashboard and let's actually go and look. At a dashboard. So I can pin from activity logs if we just go, this is the home screen. But then we've got this little dashboard icon here and I might think about creating dashboards for different workloads so I can create lots of dashboards. Now it says an Azure AD audit dashboard in Azure AD sign-in dashboard, I might create one for my application A which has things pinned from its load balancer and it's database and it's virtual machines, whatever is applicable to this single pane of glass. This is what it's really designed around. I've got a single pane of glass that I want to show things for. So that's why I think about as a dashboard. But I can pin things from the activity log from metrics if we edit. Notice there's a gallery. So look, I can add a metrics chart, I can add a resource group, I could add clock, I can add markdown language. I could add Azure AD tasks map security metrics. Resource graph single value tile. Availability tests, videos, arm data resource graph, ARM actions. So it can actually do things, but most of the times when you're looking at some resource just cancel this notice. I could rearrange, I could put anything I want in here if I was just looking at let's say a VM. 2nd. I'm looking at some aspect of it so. She's just. Monitoring. Now it's this pin icon. If I select the pin icon. They'll ask me which dashboard I want to pin this to. I could create a new One South. It's super easy to go and see things I care about and then go and add it to the dashboard. So dashboard is really powerful. Then we also have the idea of a workbook. So if we Scroll down. So this is obviously just regular Azure monitor. We have these insights, but then we have workbooks. There's a lot of workbooks just built in. Now the point of a workbook is, this is a document like flow. It's a collaborative authoring experience, it's interactive reporting. If I select some element of the workbook, it will update other elements in the workbook. So it's a very different type solution. And let's actually go and look. These are all of the workbooks kind of that are native, but if I was to look at a VM for example. Let's go over here. Look at my virtual machines if I selected a VM. 32 things I could look at here. Firstly it was show me workbooks relevant to a VM. So here's a VM metrics workbook and you can see it's showing me information. But one of the things I can do here, hey, look, there's alerts. But I can edit the workbook. I could go and add my own things. I could edit the things that are built in. I have a lot of control over this. So. I can add components to it, so if I look at this add at the bottom here, I can add text parameters, queries, metrics, create a new group of information. So what book is saying hey there I I want to actually interact and maybe get a richer set of data from this particular solution. There's also things like grafana. So I mentioned before, so we have dashboards. So think a single pane of glass. I have workbooks. Which are more interactive investigation to what I'm doing? And this will really come from. I can create them from scratch, or there's a lot of templates built in that I can use, and then even those templates I can fully customize. There's also things like grafana. Grafana is very popular, especially with containers. There's an metrics of an Azure monitor. Metrics. Pug in is available. So from grafana, I can now easily show the data that's in that Azure monitor metrics database. But it again, it works super powerful with Prometheus for my containers to visualize all of that all up data and solution like Power BI. Power BI is a very rich solution for analyzing and showing data well. I could use power BI with all of these as well. So there's a lot of solutions available to me. But you may wonder, well, how do I get, how do I get started? Well, you saw me say insights a few times. So. The recommendation is to start with insights. Now what insights is doing is it's pulling data from the activity log. From metrics and log analytics. Now what this means is when I turn insights on for a resource it's going to set U those diagnostic settings and send things to log analytics workspace. Which means I'm going to pay for that log analytics workspace data. So you want to be careful but it gives you a huge amount of great insight. Now remember things like containers. I can a change it to the basic type table because it all writes to 1 tables. That would save me money but I can also now configure that. Interval of how often it sends it, and I can also exclude certain namespaces, but the huge benefit of this is it's this fantastic, it's curated. It's created and designed by the people that create. The resource so Azure Cooperative Service, for example, they created container insights. Now it does also provide workbooks. So it's going to give you these nice visualizations, but then they'll workbooks that I could go and customize and there's insights for all of the key types of resource. So if we jump over again. If I just looked at my virtual machine, I turned insights on for this VM. And if I select insights. Well, I can see performance information. So I can see the logical disk performance. So logical disk it's getting that from within the guest operating system. Hey I can see CPU utilization, available memory. Logical disk IOPS, logical disk megabytes per second. Lots of very, very nice information. I can also go and look at a map. So this would actually go and show me I've not got this turned on. This would actually show me connections it's doing to other types of resource and if I went back to Azure monitor. Notice all the different categories of insights it has over here. So applications, VMS, storage, accounts, containers, network, SQL, Cosmos DB, Key vault. I mean the list goes on, it has these massive amounts of information. I don't actually know what other resources are I have turned on for insights. Ohh OK so my cosmos DB. So if my cosmos DB hey look an overview page OK, I can get some basic information, but then I would see information about the throughput, the request, the storage, the availability, the latency and it has workbooks. So I could then take that insight and take it to another level and the key part of a workbook, especially when it's talking to a log analytics workspace if I edit it. So I can see these different aspects of what it's showing me here. But depending on the type of way it's working, where it's getting the data from, we've got throughput, for example, what I'll get for some of them. Is I could actually see the query of what it's doing. So when I'm actually editing the component. Edit. So here I'm editing this particular one. I could see what it's getting the data from. But trying to think where this is here. The advanced editor. I would be able to see the kusto query language query it's actually using to get the data. So if there were some really nice insight you're seeing and it's using log analytics workspace as part of the editing, I would see the kusto query language that it's using to get that data. So it's a nice way to learn some of the acoustic query language and get those capabilities for myself. So you can play around the workbooks and get some nice insight. Now in addition, to start with insights. Obviously once it's in Azure monitor logs, I can absolutely just at my machine. I can use KQL. Accuse you for my application. I can use it for other things. I can go and get nice information about what's happening in the environment. But the chances are. I want to know when there are issues. I want to know if there's a problem going on in the environment. And so we have these different sources of information, and there's the activity log, there's metrics and there's logs. They're the real key input points if I want to know about something. And there's different types of things I need to know. Account has gone over a certain value. My CPU is over 90%. But it's also going to be, hey, I'm seeing throttling logs. I want to know about those. Or if I get more than 3 logs in 5 minutes, I want to know about those. And so if I think about. Something telling me I need alerting. So the way we do alerting. Is we create alert rules. Now alert rule can take in. Events from the activity log, which includes the service health, it can take in metrics. Now metrics are a little bit special here, we call them a fast. Metric. Pipeline. That's really a fancy way of saying when some metric happens, we can respond to belly less than 60 seconds. So it's it's a fast interaction. But I can also interact with locks. Now the interaction with locks would normally not be metrics. Metrics is far faster. The only reason I would use metrics against log analytics table would be maybe some really complex set of queries I'm doing, but it's really not typical. What's going to be more typical here is I'm running a log query. And I'm interested in one of two things. I'm interesting in the result. Value. Or I'm interested in the number of results. Our result value would be find me the log. OK, I've got the log. What is the log say? Or. Hey did I get any matches? And if I did how many hey I got 3 threshold. Error messages in 5 minutes? OK, I've broke three. I'll need to be told about that fact. Have a huge thing about the log queries is they are interval based. So I'm telling it hey, run this over this time period, they are not stateful. And what I mean by that is if I run the query and it found me through records and it's going to fire off an alert, if I run the query again 10 minutes later and that's still within the time range, it will fire again. So I'll just constantly keep getting. This alerting, whereas when I'm using something like the metrics these are stateful. So hey, it's flipped. It's gone past this. It won't just keep alerting me to that fact. Now one of the nice things I can do because of the benefit of the metrics. I can take. Time series data that maybe it was only sent to the log tables. I couldn't send it to metrics, although that's getting less and less these days. But I can send data. From a log table. To the Azure monitor metrics as custom data, so I can have a scheduled. Query raw. And that scheduled query wall rule would take time series I numbers from log analytics and send it as custom metrics. Which I could then use. The metric type of input into the alert rule, which then would be stateful, I would get that maybe the faster metric pipeline. So there's benefits to this, whereas there's a lag, I mean there's lags involved with log tables anyway, but this will have some nice capabilities to bring that in and get that stateful benefit. So different ways I can come into. The roles. So if I was to jump over. And we could start looking at building one of these. So if I look at Azure monitor and I go to alert, now I'm building this from scratch. If you were to go and look at let's say. It took a VM. I can build a lot for this particular resource directly here, and for some of them now it will even recommend alerts. So now it's saying, hey, do you want to turn on? Some recommended alert rules. I could just create a regular one. If I enable the recommended, it will show me ones that it thinks we should care about. And I can just click enable and it's just going to send an e-mail basically. But I could configure some other things or use an Action Group, which we're going to see, and they're telling me it's going to cost you some money you pay for each of these. Ones, but it's basically $0.10 and alert when it's a time series metric. So hey, there's seven of these it's going to add. It's going to cost me $0.70 a month. So I could just turn them all on there or I canmore centrally, maybe at a broader level, I want to turn on some alerting O when I go and create a brand new alert, so I create an alert rule. Or I can tell it. Or what are the types of resource I care about and I can drill down. Now I might say I want to do it about virtual machines, so I could drop down to here and just select virtual machines. And it would show me the resource groups that have virtual machines. Where do I have? I could select all of them for example. So notice there were two. That's kind of weird. There we go. It's not updating very well. Come on. See in here no. So if I just search for demo. It's kind of weird. There's two in here, there we go. There's the first virtual machines, so if I select, I could select multiple. I could select it at the subscription level so I could cover multiple resources. But if I just select demo VM for example. And what is my condition? So we have signal types. Now we can see here we have the activity log. We have resource health, we have metrics. And then we have the log. So we have those key types we talked about. Which ones are available will depend on the type of resource. But if for example I was right, now show me all of them so I could do a custom log search, which is obviously going to be of type log search. Resource health would obviously be resource health, and then metrics retries is actually most of them. So if we look at all the different metrics, again, we could search. But if I search for a percentage CPU. Now, one of the things to look at here, because it's a time series type, so it's a number. I pick a threshold of when I want it to Alert me O as the average, maximum, minimum, total. How often am I checking? How far do I look back? I have all these configurations and I could set an absolute number. OI might say hey if it's higher than 80. And it's showing me well historically where I am. And then where my threshold I've configured or I can use machine learning if I hit dynamic? I don't enter a value anymore. Instead, I tell it how aggressive do I want to be? So I could say high. Sensitivity, which means it's not a lot me earlier if it's just even slightly outside the norm. Low hey, I only care if it's like wildly outside what you typically see. Or I could set as medium. To have these different options to configure and it's telling me the cost. So to use the dynamic it costs a little bit more because obviously using that machine learning to go and see what's normal in my environment, but I go and set U this alert rule. And if I look at the advanced options, hey, I can set additional numbers of violations. And the key part here is I'm going to skip actions. Now notice there is an actions ability, I don't want to set that here. Instead, under details I can set a severity. It's important. A whole name and a role description. And again, there's a few things to enable upon creation, automatically resolve alerts. Solved. And then I'm setting U the alert rule. So what this would essentially do at this point is. File and alert. So I've created the alert rule. Based on these things. And the output of that is going to be OK, create an alert. So it's fired. There's been an event, although it's always red, so we're going to have alert. Attention over here and it has things like that severity and obviously the resource it's impacting. Etcetera. Now there's a lot. So that alert rule is going to drive the alert. This would show U. In for example, there's a dashboard, so these will show up as a list on my dashboard. There's a built-in just alert screen. I could pin it to my own custom dashboards. I could remember. I can feed this into other things to go and trigger something else as we're going to see, but it's just going to show up as an alert. OK, now one of the things it will also do when I'm thinking about these dashboards and displaying the alerts, it does have smart groups. And what smart groups mean is, if there's like lots of alerts we've got fired, all about the same type of alert, the same resource, rather than showing me 50 alerts, it will understand they're really about the same thing and it will group them together. So that helps me solve that problem. OK, this is alerted, but now I actually want to do something. Or to do something. What we're going to create is something called an Action Group. So the next component then is an Action Group. And then Action Group as the name suggests does stuff. So this could send an SMS message, it could e-mail, it could interact with an ITSM service management solution, it could call an Azure function so it's saying server list, which then go and do anything you wanted at all. It could go and write to event hub because some other services subscribed and would see that I could call a web hook. To some Restful API that could do anything as Azure automation. There's a whole bunch of stuff it can do, so if we jump over and go and look at those. So firstly I've got my little rules. So I've got a whole bunch of different ones detecting different things. But then if I go back and now look at action groups, well these are combinations of things I want to do. If I do create. I have to give it resource group. There's notifications. So hey, I can e-mail. The ARM manager role. Or particular people I specify so if I select the second one. Hey, who do I want to e-mail? Who do I want to send an SMS to? Who don't want to ping their Azure mobile app on? Who don't want to call? Obviously calling is fairly aggressive. I can do any of those I want. And or actions. So call an automation runbook, call an Azure function, call event hub, call ITM, call a logic, A call a secure web, call a web hook. So I create these action groups which are just combinations of actions. That's a fairly easy thing. Now one of the things you saw was that when I was creating the alert role. I did have the ability to not only generate the alert, but I could also. Specify an Action Group. It does let me do that. The challenge with that is I'm then defining the relationships between every single alert I create. Which if I then want to change it, it's cumbersome to do later on. So we have say that can sit in between. In between these I can create something called. A lot. Processing. Rules. And what's nice about this is this is triggering from the alerts but I can assign a certain scopes. So hey this particular alert processing rule only applies to alerts that for resources in this resource group I can also add filters. Hey it only applies for resources in this resource group if the severity is high and then it. Can call the Action Group. So now splitting apart what generates the alert from them? What should happen based on different types of alert. But in addition to calling it. Or. I can actually suppress. And you might be saying, what does that mean? So normally an Action Group would have got fired, but would be some other alert processing. Well or because I had this direct link, but maybe there were some scenarios I don't want it to fire, maybe there's some one time event. Do you know it's Christmas Day? Normally I get. Alerted if there's just a warning there's Christmas Day. I just don't care if it's critical, sure you can bother me on Christmas Day, otherwise I don't wanna know. So I could have one time special event or I can actually set these up as recurring. Because I might think. Or maybe there's maintenance windows, so I know. At certain times I might get maintenance window. Would be common used for this? I know I'm going to get alerts when I'm doing my maintenance. I don't want them. So I might say, hey, suppress certain types of. Actions that would have happened in these times using an alert processing role. So it's about hey alert rule. Fires the alert of certain categories severities. Then alert processing rules will look at the alerts that have been generated and decide what to do based on scope and filters. And then it can call an Action Group to do something or even say, hey, I want to suppress certain things for certain time periods because there are other things going on right now. And so if we go and look at one of these, so I've got my action groups, but then I can also look at the alert processing rules. I'm going to see I've got two, so I've got one. For look if it's this. Management. So if it's this resource group. And the severity is 0123 or four, then it's running an Action Group. So I didn't have to have anything defined. There could be loads of different types of alert rules that would generate these severities for resources in this resource group. But you know, I have a resource group owner, they're the ones that she can notify for anything in the resource group. That I also created one. Where its filter is for the whole subscription and it's saying look anything. That's just the severity 4. Suppress it. I don't want that to fire, so whereas normally it would have emailed me this would override it and it will not. And I've got a whole separate video about these, but this is really nice when you create one of these. You can tell it whatever the scope is. And you're all settings, you're telling it, hey, are you suppressing or do you want to do something? And I can go and set up all of the other details for that. So those are all of the components that we have and this is key like you want to spend a bit of time getting these set U. Because now. It will alert you when there's something going on in the environment as opposed to me having to try and draw through and understand what's going on. And it might take me time, let it do the hard work for me. So this is a really important set of work to get done and you can see there's an entire flow, so I have to make sure I've got the right configurations to capture the data into the systems. And then I go and set up the alerting around that data. Now if it's actually one of the metrics that's just there by default, so I can do. If it's just number based, that's almost out-of-the-box, and it's even recommending alerts for lots of types of resources that you should turn on. But for some of those richer behaviours, hey, I need to get diagnostic settings to send it to a log analytics workspace and then I can do queries and results against that. Again, there's going to be a little bit of a lag because there's an ingestion time to get things into log analytics. Workspace metrics is much, much faster because it's got that fast metrics pipeline. So that's a big chunk of the monitoring and I could spend a lot more time to wrap monitoring, but hey, there's only so many hours in the day and you take my kids bowling today, so. Then we can start to think about. Security. So from a security perspective, we have to be preemptive. We can't wait for the bad thing to happen. We want the think ahead of how do I minimize my attack surface? How do I make sure I have good visibility into my environment? So there were some core roles which I'm about to talk about. Make things only have what they absolutely need, only when they absolutely need them. You'll hear about just enough administration, just the permissions you need, just in time. Only have those permissions when you need it. You have to elevate up to get them. Things like permissions management, things like privilege identity management help do that just in time. Make sure you're patched. Make sure you have antimalware, make sure you have firewalls. Make sure you're restricting the type of traffic, like good hygiene so you want to minimize your responsibilities. Like as you're not patching the OS, you're not doing the firewall for the OS, so if I can use PAS, there's less I'm responsible for. But good security hygiene? Will solve a lot of the problems ahead of time. Get that right first. If you think about making sure your operating system is patched, that's seems stupidly obvious, but unfortunately it's ignored a lot of the time like that. Hygiene is critical, so if I was to think. Actually I'll leave it red so if I think the security. We think about lots of advanced solutions and fantastic dashboards. Do you know if I can just have good hygiene? It's like 90% of the battle. Like patching is so obvious, but so companies don't do it if there's a critical patch released. I should be thinking about less than one week to get that rolled out. Some of the bigger attacks I think about the wanna cry. They'd been a patch out for a long time. Companies hadn't deployed it. If it's non critical. One month they might say. Why am I saying a week? Why not a day? I still wanna go through a good, safe deployment practice. There could be something in that fix that breaks my application. I'm not going to push it out to everything. I still want to go through the idea of piloting and then a gradual rollout to make sure there was some impact. I know about it and don't take down my company. But it needs to be accelerated. This should not be waiting a month. There's a critical thing. Get it out there. Protect your environment. Make sure you're encrypting stuff. Encrypting stuff at rest. So hey, my laptops have got BitLocker on them in Azure. All of your resources are encrypted anyway. Maybe I want to control that encryption with a customer managed key. Bring my own key and my key vault in transit. So make sure you've got TLS turned on. I can use things like Azure policy for storage account to say I could only support encrypted communications. Make sure you're doing just enough administration and justice in time. Don't give people a whole bunch of high privileges. Make sure you're using things like MFA or even better, password lists. Just get the basics right. And we also think when I think security, there's a whole shift left. What that means is I'm introducing security as early as possible. I don't think I have deployed my app now. I should put a firewall in front of it as I'm developing my code. As I'm checking code in, maybe there are solutions that are checking the dependencies how you're using a dependency that has a vulnerability. Here's a pull request to use a dependency that's got that issue resolved. Maybe I'm converting my code to data to go and look for things I'm doing that would introduce some. Attack point to me as I go through my creating a container image. Well, that image is scanned for vulnerabilities and it's rescanned for vulnerabilities because new vulnerabilities get found. As I'm pushing out my code to a test environment, there are test performed against the environment to make sure I've got good configurations. I'm using good templates to make sure I have firewalls turned on. I'm using policy to enforce those things. Security, security, security. I'm thinking layers of protection. Yes, the OS has a firewall, but I'm also putting a web application firewall in front of my app gateway or my Azure front door. I've got distributed denial of service protection in my environment, so I think all these layers of protection all of the time. But I try, I don't think security at the end. It's introduced all the way at the start, so I bring it in as early as I possibly can. Now obviously a huge part of what we just talked about monitoring. If I'm not paying attention, if I don't have good observability into the environment, it's very, very hard to know if there's a problem and then obviously, yes, we do think about threat protection. Because there were still advanced attackers out there. That even if I have good hygiene, there were maybe some attack surface leftover. I still need protection from different types of threat. All the great policies in the world. Someone brings that USB key in, it just plugs it in or does thing silly. Hey, now it's doing this labor movement, but I can minimize that if I have a lot of these other good hygiene if I have this just enough, if I limit the network communications to only the required ports that have to be there, there are still things I can do to minimize that attack surface. So shift left on security. And then we have to monitor, we have to apply those different intelligent solutions. So we don't want to bring up a term zero trust. We always that should be talk took, always took, about layers of protection. But. Zero trust focuses on three key tenants now. We still want all those different layers. I still think about identities, critical and yes or my different perimeter networks and my resources and everything is still layers of security. We have that onion. Zero trust is these three tenants, and it's really about verify explicitly. Use least privilege and assume breach. We have these three things. So I think this is critical enough. I actually want to draw this out so we think. Doing right again. Zero trust. So we verify. We never assume hey you because you're on this network, you're good to go. Every aspect of every interaction we constantly revalidate the identity. Could be a user, could be a service, principle, a machine, everything. This includes the device. If I'm connected from a device, what is the health of that device? Maybe if it's a user's personal device. We're doing application level management and the application is in its own sandbox and the security of that are we within certain policies? I think about least privilege. And there are many dimensions to least privilege. But this helps me reduce lateral movement an attacker will get on a machine. And they will look for some credential or something else, they can move laterally to other machines which have some other credential and they move, they want to move around. So the lower the permissions that some resource has, the harder it is to do some of that movement and elevate U. The whole point is to get into an environment and elevate U my permissions to I'm a domain admin and then I can do anything I want. Overtime most people accumulate permissions because they go from project to project to project. So I want to think about good life cycle management, think about access reviews. Do you still need these permissions? Should you still be in this group? Should you still have this role? Think least privilege. So I think this micro segmentation of communications, I can only talk to this machine and this machine on this pulp. So this is not just least privilege in terms of users. This could also be around network. And actually I don't like even just users. It could be users, application identities, managed identity service principles, the network, only having the ports open that I actually need to do the job I want, good controls and policy on everything that I have. And justice assume breach. There used to be a joke about there. What was it? Two types of companies, those that have been breached and those that don't know they've been breached, like just assume you've been breached. So what this means is. I need that monitoring. I need constant signals coming in, so I want signals telemetry constantly coming in, because what I want to be able to do is to constantly make decisions. And I've decisions will they're going to come from having policy defined. So this could be things like conditional access. So in Azure AD what I use conditional access to look at different aspects, the users risk the service principal's risk. The type of resource they're trying to access, how strong was the authentication coming in? And then based on that decision. We're then going to enforce. Some type of act access to that and. We're constantly monitoring to bring signals back in O we don't trust anything. We assume there's been a breach. We want constant signals from every aspect of our environment, the network, the device is the resources, the users, the Internet. Looking for known bad things? We want all these telemetry and signals coming in. So the more signals we have, the more intelligence we have, the better the decisions we can make. And that's a key part to this. And we keep that idea of that least privilege. We're constantly reevaluating what are you really need. And that's, that's what you can have, but you don't need anything beyond what you need to do your job at this particular moment in time. O this is a key tenant to everything we're going to do. So now I want to talk about. How we implement some of this, and specifically Microsoft Defender for Cloud or Microsoft Defender for UMM, as we'll see is the answer. So there's a basic. There is a basic offering that's completely free. It works cross cloud, so Azure, AWS, Google Cloud platform, hybrid. This is giving me cloud security and posture management. It really services this in a nice way through sync with the secure score. Now if I look at this. Let's jump over here. This shows me the plan options and the key part is there's both a free and there's a defender CSPM plan and we're looking at the free option right now looking at this. But notice even the free Azure, AWS, Google Cloud platform and on premises, so hybrid now if it's not Azure. It's focusing on using the Azure ARC agent to actually get the information so it can drive and. Make these recommendations, but the whole point of this being free and let's look at the free one for a second. So if I go to my defender. So straight we've got a security posture. I do not have a paid plan. But it's showing me OK, some recommendations, my regulatory requirements, offerings, workload, protections, inventory. But if I go and look around, there's recommendations. So I have a secure score. My secure score is not very good, 34%. And based on their secure score, it's giving me things I could do. That would have the biggest potential impact on my score O hey, enable MFA. Secure management, remediate vulnerabilities O if I'm starting off. I could come and look at my secure score and then I could go OK well. These are the things that give me the highest improvements in my score, so these are things I should probably start with. So. Defender for cloud. If you've been around for a long time, this was Azure Security Center before, but now it's defender for cloud because as you saw, it's not Azure. Yes, it's Azure as well, but it's also way beyond that. So we have this nice basic option. Which is free. But even though it's free, it's giving us that cloud security posture management and it's a huge driver for this is it's being driven by Azure policy. So behind the scenes it creates an initiative using Azure policy. And we saw that when we talked about helping our compliance and governance solution. So this is being powered by Azure policy. So it's creating a whole bunch of policies, wrapping them up in initiative and using it, which is why we need Azure arc if it's not Azure. So it gives us something to use those policies. There's also some agent this capabilities. But this is powered by Azure policy. And as we talked about, yes, it runs absolutely on Azure. But it also runs. It's going to draw the others in a different color. That's kind of petty, but also on Amazon Web Services, Google Cloud platform, and I guess hybrid, I should join a different color and hybrid. So this could be outside of a cloud platform. And once again, these are powered by ARC. If it's not Azure, it's arc. Driven. So it uses that art capability. I have the arch agent installed that then enables policy to be used, which can then gather the signals to make these recommendations. And you might actually wonder, why is this free? Like why would they give this away for free? This cloud security posture management piece, that's that security hygiene. And once again, if you get that right, you're so far down the path to be successful. So they're giving away free because in a lot of ways this is the most important bit. Get this right, you're defeating a lot of the bad stuff out there, so they give this away free to try and get you in as good a possible place you can be. So just without spending any money, I want to go through that secure score. I want to go through the recommendations. I can connect to my other clouds, get the information in and try and at least get a good security hygiene perspective. Follow the recommendations, get my secure score up, and I'm in a much better place. But then obviously there is an enhanced version. So if we look at the enhanced. This I pay for. Say to us about the enhanced feature, so basic as we talked about is that cloud solution posture management? Then enhanced. Well, it's actually interesting because when I get to the enhanced side. There's different aspects to it. So one of the I can enable this defender CSPM plan that's going to give me far more industry standards, more regulatory compliance offerings out there. There's things like the cloud security Explorer that's we run queries against the graph. So I could see these plans. See if I was to look for example at my environmental settings. I could actually two things I'll show here. Firstly, if I just select a management group for a second. It will show me I could go and enable. More standards so I can turn on all these other regulatory requirements. That I might need to adhere to. And once again these are going to use policy huge massive initiatives that it's scanning many, many different things to enable these to work. ISO 2701 HIPAA High trust. So I'll go and turn these on. But then within a particular. Subscription. This is where I can turn on the defender plan. So here I could go and turn on. CSPM. So I've got CSPM free right now, but then there's actually defender CSPM. Now notice, I think pretty much all of them have a free trial, so you can try them free for 30 days. But you'll see there were more defender for servers, Defender for app service, defender for databases, and there's different types of database. There's SQL, there's Cosmos, there's Postgres, storage containers, key vault, arm DNS. So I can pick the ones that I want to enable. Based on the types of resource I actually have. So depending on which ones I turn on here. That's going to influence which services, so there's defender CSPM, which gives me that richer set of capabilities around different regulatory requirements. That explorer. There's an attack path analysis tool, but also then see defender for blank. And as we saw there a huge number of those solutions. So if we have the free version. Then we also then have the enhanced. Now was it the enhanced I pay money for, but that gives me those additional regulatory. Capabilities that need to track against. There's that explorer to go and run queries against my overall security status. There's monitoring of attack paths. There's different types of workload protections. So then that was defender for cloud, then we have change my color. Defender. for. Pretty much everything. There are things like servers. Defender for servers, defender for servers introduces things like just in time protection where it will only open up a port, RDP or SSH or winrm for a time window of when I need it. It's going to add things like app controls, dynamic app controls. These are the apps we typically see. We'll whitelist these, we'll stop other ones. Network hardening. There's a whole list of things. That I get. There were ones around are containers for example, and the key point, all of these are threat protections and the point is that they have different capabilities based on the type of resource and the things we would care about and be attacked about for those types of resources. So containers, well, I'm going to look for vulnerabilities in the Kubernetes environment, but I'm also going to go and scan the registries where I have my images and look inside the images and are there. Their abilities that their problems inside those, and there's just a massive number of these solutions I think are linked to it from there. So how does it collect the data? But if I looked on this left side? So defender for servers different capabilities assessments, defender for containers. Database protections. And there's SQL. There's open source like Postgres. And now there's Cosmos DB, app services, storage, key vault, Arm DNS, defender. For DevOps. We think about shifting left. That helps us do that huge shift left. So these are the things that hey. I'm using Azure security, always kind of gets pushed to the side. Until there's a problem and then it's ohh what do we do? What do we do? Where did this attack come from? So yes, security hygiene is huge and very, very important. But it's critical to understand what are the resources I have and yes, get that fantastic hygiene in place that's going to stop most, most attackers. You're not being targeted by some expert in a room with a ton of monitors or some state attacked. It's a script kiddie that downloaded this script and they're looking for easy things. That good hygiene will protect you from that. Now again, the targeted attacks. That's when I need more than that, the script kitties. We'll probably get beaten by this part, but when you start getting into the more targeted attacks, I need monitoring, I need the threat protection, so I need to have these things in my company. And the best type of threat protection is not really some generic solution. It's knows what to look at and what is the possible types of attack surface for different types of service. So servers, containers, DNS, arm as a control plane, databases, key vault. Let's look for the signals and understand. Signals that are most pertinent to that type of resource to help me have the best threat protection from those. So that's what all those defender solutions are giving me. And then Azure Sentinel, so Azure Sentinel. Is there to help ingest data to get all of those signals in from all of the different types of signal systems. Then add intelligence on top of it. So help give me good alerting help create incidents based on what it's seeing. So Sentinel is both a SIM and a sore. So a SIM is just a security incident and event management tool, hey. I'm detecting there's something happening I'm helping you manage and investigate. That event. A saw is about security orchestration automated response. I let's do something, don't just tell me. Help me do something to automatically. Don't wait for me to see it. Let's do some things. And it also has extended detection and response XDR Tye capabilities. So the whole goal of Sentinel is. It's built on log analytics workspace, so it's going to bring in. It uses the native log analytics workspace connectors, but also adds a whole number of its own, so it goes beyond what log analytics workspace can do in terms of getting data from different types of systems. So it has those additional connectors. It has ways I can alert, but it also has ways I can go and hunt. So look for signs of problems. Playbooks to actually automatically respond. Notebooks for deeper investigation into problems. You Jupiter notebook. So it's using those capabilities to go and scan and look around to find the problems. Just regular workbooks and for more standardized basic viewing of what's going on. So has all these capabilities. But it has that automation, and there's automation through playbooks, which are built on. Logic apps. But it also now has. Maybe it's some simpler thing I'm doing. I don't want to have to create a whole logic a there were some basic things that just has built into it. So if I think. About Azure Sentinel. So I can get the universe color as well. So we have Sentinel. It's built on top of. Azure monitor logs or log analytics workspace. But the whole point here is yes, there were connectors. There was just native log analytics workspace, but there's also additional connectors that's now coming in with this as well. So I can get data from just huge different types of systems, my network devices, yes, things like syslog that goes without saying, but just massive types of ingestion from all these different things and then what it's applying on top of this is things like machine learning. So it's not just running some basic rule, it's learning what we normally see and feeds it back in to then go and create, hey those incidents. And when we do that hunting and all those other things, so it's sitting. On top of this and so if we go and look. It's a super quickly. So if I search for sent now I don't have anything fancy set U. This is my little subscription here, but again you can see it's built on a workspace. It's got a nice little overview page where it share any instance I have. Data received that's coming in from a different connectors. My analytics rules that I have. There's a nice content hub now. So the content hub we're used to focus on this data connectors. We should bring in data from a particular system, but now through the content hub we just say, well, what is the service. That we want to monitor. And it will then bring in the connectors we need. For that OI can just search for something I need. And it would then add the connectors I need to do that. He would go and set up for example the diagnostic settings if it's an Azure resource to go and send it to me, but here you can see the various connectors. I've got things like Azure Active Directory. Connected. But really, use the content hub, so this is about getting the data in. Once the data is in, well then it will start creating incidents when something's happened, when it's found some issue based on all of those feeds of data coming in and then alerting you to that and it will then guide you through resolutions and what you should do. But they were workbooks. That will help guide me through. Hey if there is something happening, it's great for visualization high level view. I don't need to be a coder to do this. Then I get the notebooks which again is built on those Jupiter notebooks. It's using Python. I can apply machine learning through these are more complex tasks, could be a chain of tasks, could be some procedural set of controls I need to use. But I can build those in hunting. Is a set of queries, so it's using. Remember it's log analytics workspace underneath, so it's a whole set of KQL queries that I can run. That might give me some sign of some problem happening. And it will tell me the different connectors I've acquired. It's moaning I don't have the right connectors, for example because I've not got connected to DNS, so I won't have the events in my log analytics workspace that this is looking for. But it's showing me those things. If you're used to the idea of, again, log analytics and logic apps, so we'll see. For example, I have automation. So I can have my automation rules, so these are some simpler things I can do. So if I create automation rule I don't have to create a whole logic a I can just say hey. If this some instant is created or an incident is updated or an alert is created. If it contains some text then just perform some simple action notice. I can run a playbook or I could just hey I want to change the status, change severity, change an owner or I can still go and create a playbook based on an incident happening and alert happening and entity trigger. I can go and look at entities. So an entity for example could be a user. So user entity, behavior analytics, I could look at a certain user, I could look at a certain host, I could look at a certain IP address if I'm used to the Mitre attack framework. So this is the whole adversarial tactics, techniques and common knowledge. It was. Show me. Where I have. Protections against that. If I close that, if I've got mitre attack. These are all the different areas and I can go and see where I might have things that address. The different types of attack in the environment. So really centralized, bringing together just fantastic insight and intelligence across all those different signals. Now I've got a whole different video on this which I would recommend to get more detail on, but. This is the. Solution in Microsoft for my SIM and my saw and don't think it's just Azure, so I think I might have said Azure Sentinel. Obviously it's Microsoft Central because those feeds can come from everything. This can come from AWS, it can come from Google Cloud. It's not just respected to Azure on premises, things can feed into it as well. So two other things I did want to talk about just briefly. So managed identity and the idea around Azure Key Vault because when I start to think about. Security and reducing the chance of having some secret written somewhere. Managed identity is huge. So if I think. What is this doing? Where possible, I don't want to have to have a password or even a certificate in some application configuration file or stored somewhere. It's easy to get published to GitHub accidentally or leaked. Managed identity ties the credential to the resource. Many many services, pretty much all the services support this now. And I can have system assigned or user assigned. So. We've got some space. This looks like a space. So I have a resource. Now let's say this resource is called Resource one. Could be a VM, could be a container host, it could be an app service. And then obviously we have Azure AD. And then I have some process, some a running inside, and what it wants to do is it wants to go and communicate to some other resource. Maybe it's database, maybe it's a storage account, doesn't really matter. But this is resource two and resource two let's say has data plane role based access control. So I've got some role based access control and there's maybe a data reader role. I'm gonna say data owner. I don't know. That this app needs permission to for here. Now, in the past I would create a service principle. They'd create a secret and then that SQL would have to get stored or accessible somewhere. They might say, oh, put it in key vault. How would I authenticate to key vault to get the secret out? I'd have to store credential to connect key Vault. There's chicken and egg problem so. What we can do is we use managed identity. Now there's two types. There is a system assigned managed identity. So I can say hey I want to turn on system assigned. System assigned is a one to one relationship. This managed identity can only be used for this resource and what it's going to do up here is it's creating a managed identity basically and identity called resource one. Now, the exact mechanism this will work varies. Is it a VM? Is it an app service? Is it a pod? But this application. Wants to be able to talk to this resource. It needs a token. Doesn't have any stored and So what the application is going to do is their application has the ability to talk to some endpoint. Now if it was a VM, it would be the instance metadata service, it was an app service, then there's a rest endpoint, but it's basically saying, hey, I need a token, so token please. Whatever this buddy site is, or sidecar, whatever that is, it goes and talks to Azure AD. Shows it is this resource. This creates the token that it gives the passes to it. Now I can use that token so the R back on this would be OK resource one. Has that role and is now going to have a token for this, which it can now go. And use and it's all happy, it passes and it can access the resource. So the system assigned managed identity is fantastic when it's one resource needs a certain set of permissions on a whole bunch of resources. Now there's also the ability to create something called a user assigned managed identity. Then this is a separate resource, so it's a user assigned managed identity one. And the whole point of this is this one identity can be used by N number of resources. So it's one to N. This would be useful. Imagine I had many resources. The all needed the same set of permissions to a whole set of other resources rather than create 3 or 10 system assigned managed identities and all ten had to be given these permissions. Well now I could say hey you know what, for this group of resources instead of giving you a system assigned, and I kind of have both, I'm assigning you how you can use user assigned managed identity one. Then it would be able to get a token as that identity and then they could all use it. And use assignment identity, one could all be given that certain permission. So if we were to go and look at this, just a super simple demo so that demo VM I had earlier on. If I go to identity, we can see I've turned on the system assigned managed identity. I could also give it user assign which I've also done, so it's also got a user assigned and then I can grant it roles. Now in this example the roles I've given it. Is we've actually got a few roles, but it can. It's got a data plane storage BLOB data reader on this storage account, but it can also access certain secrets. And from key vault. So what I'm going to do is. This is that virtual machine. And right now looking at the test folder. And it's empty. So test folder is nothing in it if I now go and look at some code. I'm going to connect. To Azure and I didn't give it a credential, I said identity, that means used my default managed identity. Now I'm going to create a storage context. And I'm going to try and get a BLOB. And hopefully what will show U is that will work. Because it had the data plane permissions to. That particular storage account, so it create the context using the managed identity. And now it's going and connecting to it and actually getting that BLOB. And then that file should show U. No such host is known so it's errored on me. Oh I know why I didn't change the. Ohh interesting. This is the wrong O. We have fixed this on the family. I don't know why that is that file. So that's the wrong storage account. Storage account is that name. It's. I copied the wrong file over. So if we quickly jump over again. The reason it failed is it's the wrong storage account name. And then also the pictures in that storage account if we go and look at it will be different than that name. But this, this is good. You can see it's not set up so images and we'll just take this picture only and Eddie in the cereal and again that managed identity if we look at the access control while I'm in here anyway. We'll see it has BLOB data reader. So it's managed identity has BLOB data reader, so it should be able to get to this, so we've changed the BLOB we're going to try and grab. To that one. We want all those again. And it got it. So now if we look. There we go. So using the managed identity. It actually enabled us to go and connect and there my dogs eating some cereal. And also what I have in here is the next one is actually connecting to Azure Key Vault. Remember we also had permission to key Vault. But I probably not updated this one either, so this would also actually be wrong. I don't know if I bothered to go and change all these as well. But so there's a particular secret maybe I want to read. So if I looked at my Azure key vault, because this would be that chicken and egg, maybe there is a secret I need, but how do I access the key vault? So if I looked at my key vault for example. I've got two, so this would be my key vault name over here. So we change this name. So this upper value. And I need to get a certain secret and let's say if we look at my secrets. Secret two. We'll look at secret 2. So I'm gonna just query for secret 2. So we now if I invoke the web request. I've got my token. And run the query. So role assignment failed. And do not have permission. So interest is I don't have permission to secret too. Let's try secret one. And that one worked. So notice how the role based access control works and I can see OK the value. Is password super secure there? But you can see that how powerful then that data plane control is. So clearly what I did is I only had permission to one of the secrets. So secret 2. Access control role assignments. If we go and look here. At like Key Vault reader. So Miss Secret one the user assign has it but not the machine. Whereas if I look at secret one, I'm expecting I will see the demo VM. Has that secrets. Yep, user role. So sometimes it's nice when things fail, we can actually go and see that well based access control is really taking effect for those particular resources. So my whole point of the managed identity is I don't have to store any configuration anywhere, it's just native to the resource. And then anything running inside the resource can act as the identity and get access to things without me having to store anything. And then you saw. Key vault. So we like, OK, so that was weird. I was looking at a secret key vault. Sometimes I still have to have a secret, sometimes I still have to have a key. Sometimes I have to have a certificate. This is the place to store it, so it supports key, certificates, secrets. There's a rotation policy now for keys and I guess it's just quickly explain what these things are. So here was a certain type of resource, but we also have Azure Key Vault. We support three types of thing. Secrets. So what is the difference? So a secret I can read and write it? Password. I need to store it. I need to be able to get it back out keys. Keys I can import into it, I can generate them inside it, and I can perform cryptographic operations. I cannot export it. Cannot get the key out. And then we have certificates. Those already about distribution. And the life cycle management. So at these three different types of things. So keys are commonly used with bring your own key, customized key on anything, data at rest. This is where that keyword gets stored. And one of the nice things is keys shouldn't just last forever. So what I can have? I can do auto rotation policy. So I could say hey. Every 18 months, rotate the key and then a lot of services will automatically see there's a new version of the key because I have versions and then start using it for that encryption of the data. And there's two different ways that we can do the Azure Key vault security. There's an access policy which is really the old type access policy was these roles have these permissions for these types of resource in the key vault. I couldn't be granular, it couldn't be different people could have different permissions to different secrets. Like what you saw? So I'd have to create different key vaults for any different combination of permissions. Whereas the new RBAC access model as you saw at a per object level can have different permissions. So this is the one you're basically going to use. You want to use the more based access control. I can use Azure policy for governance to say hey I need these particular configurations, I need these settings applied and it could even be an event source for event grid. So hey something happens go and trigger these actions. So I rotate a key, go and do these other configurations to really make it take effect. So if I do have a secret, use Azure Key vault. If I have a key, if I have a cert and remember I can use them together. So if I do have to have permissions so it supports use the RBAC mode, what I could absolutely do is you saw is I could say hey secret one. Well, for sequel one. The managed identity. You have secrets, user. So there's no chicken and egg problem. I don't have to have a credential to authenticate to the key vault, I use the managed identity. To authenticate the key vault to then go and get the thing that I couldn't natively do. Most things support Azure AD based authentication for the data plane, but if it didn't, if it had to use a secret, if it had to use a access signature or an access key, store the key in here. Control the permissions to the key using the managed identity of the resource. Nowhere is there any kind of actual token stored in a config file that gets actually put on GitHub so. I need to go and get the secret. I'm using my managed identity and that token because it has permissions to the resource. And that was it. So that was all security and monitoring. Dive in. As always, I hope that was useful. As always, we covered a lot. But I think it's really important to understand the types of data we can get. It's important to understand what we can do with that data, bringing them together from monitoring for alerting, feeding into security, get that good hygiene first. If we're patching, if we've got the firewall, if we've got the antivirus, we've got good configuration, we've got good policy. It's 90% of the battle. And then we can alley the monitoring. Then we apply the threat protection to give us that complete protection. That was it. There's always the next video. Take care.