In an unmarked office building in Austin, Texas. Come on in. There are two small rooms where a handful tiny tweezers in hand. They're designing two types of microchips made to When we get this, what do we do? We test it. First thing that we do, we test it. But these chips aren't coming from Nvidia, AMD or any and market milestones since ChatGPT burst on the scene I'm here inside Amazon's Austin, Texas chip lab, where those from Intel, Nvidia and other giants. And it's actually a way for them to save money and buyers of data center chips in the world. CEO Adam Selipsky told CNBC that the chips that we saw and more for the AI boom. The entire world would like more chips for doing that's GPUs or whether that's Amazon's own chips that a better position than anybody else on Earth to supply to want. Amazon Web Services is the world's biggest cloud retail giant with an operating income of $5.4 billion Although that number has been down year over year for 70% of Amazon's overall 7.7 billion Q2 operating needs for the huge undertaking that is custom silicon, eventually propel Amazon to the center of all the Many of our AWS customers have terabytes or petabytes AWS, and they know that that data is going to help models that they're using to power their generative AI And yet others have acted faster and invested more to Think Microsoft's reported $13 billion investment in Google's chatbot Bard, followed by its $300 million AWS's profit margins have historically been far higher narrowing. And although AWS's growth is still Amazon is not used to chasing markets. Amazon is used to creating markets and I think for the on the back foot and they are they are working to play CNBC sat down with top AWS executives and analysts to plans to make strides in generative AI to catch Google too. We end up with a package part like this. And this is an actual machine learning accelerator Annapurna Labs on it. In 2015, Amazon bought Israeli startup Annapurna Labs In July, we went to AWS's Annapurna location in Austin with lab director Rami Sinno. AWS also designs chips in Silicon Valley, Canada and made by chip manufacturers like TSMC in Taiwan. AWS quietly started production of custom Silicon back Nitro, now the highest volume chip with more than 20 server. Then at AWS's big annual customer conference its ARM-based server chip Graviton to rival x86 CPUs That's probably high single digit to maybe 10% of total of those are going to be Amazon. So on the CPU side, they've done quite well. We're into our third generation of our graviton chip speed and cost efficiency and power for very general After announcing Graviton in 2018, announced its first Product Matt Wood showed us the two AI chips it has This big one here is called Trainium, and this small It's called Inferentia. Inferentia. Amazon's first AI chip launched in 2019. Which we're on our second generation of which allows throughput, low latency machine learning inference, a prompt into your generative AI model, that's where With Inferentia, you can get about four times more latency by using Inferentia than anything else Trainium came on the market in 2021. All right. So this is a packaged part. And then let me show you the other side. What you see here is all the interfaces. Machine learning breaks down into these two different So you train the machine learning models and then you And so we see a lot of customers that are interested own generative AI models. And so that's where Trainium really, really helps. Trainium provides about 50% improvement in terms of performance relative to any other way of training But for now, Nvidia's GPUs are still king when it comes AWS itself just launched new AI acceleration hardware H100s. Accelerating performance by up to 6x and reducing 40% as compared to EC2 P4 instances. Nvidia chips have a massive software ecosystem that's nobody else has. The big winner from AI right now is That seems clear. Still, Amazon is not the only non chip giant getting Apple has its M series of chips and a couple years cloud tensor processing units or TPUs. Nobody's at the same scale as Google. Google has been deploying this for like eight years. My assumption is all of the hyperscalers, whether own accelerators and many are also working on their But when it comes to custom chips, Microsoft is lagging Microsoft has yet to announce the Athena AI chip it's I think the true differentiation is the technical guess what? Microsoft does not have Trainium or Generative AI is the current craze, but Amazon was learning with dozens of services, long before it made Late 1990s, we were the first one to actually leverage to reinvent recommendation engines, and we leveraged better product search and then automating leveraging FCs or fulfillment centers to help products ship reinventing completely new customer experiences with But when OpenAI launched ChatGPT in November 2022, AI headlines, followed by Google's Bard in February. Two months later, Amazon announced its own large service to help developers enhance software using I think ChatGPT and Microsoft rollout of their caught a lot of the market participants flat footed. Amazon is trying to educate the market in order to a couple of months. Let's rewind the clock even before ChatGPT. It's not like after that happened. Suddenly we hurried and came up with a plan because quick of time. If anything, it actually accelerated actually move forward with generative AI deployments. Meta also recently announced its own LLM, Llama 2. The open source ChatGPT rival is available on Now a leaked internal email shows Amazon CEO Andy team that's building out expansive large language instead of building a ChatGPT competitor. So if you look at the Bedrock strategy that they are fact that enterprises might not necessarily be Bedrock gives AWS customers access to LLMs made by AI21 and Amazon's own Titan. Titan is actually a family of foundational models. We have text-based models which are great for advertising, chat bots, those sorts of things. And then we have an embedding model which is great for Amazon says its AI products are being used by numerous HSBC. In the Q2 earnings call, it said a very AI and the 20+ machine learning services it offers. We don't believe that one model is going to rule the We want our customers to have the state of the art they are going to pick the right tool for the right One of Amazon's new AI offerings is AWS HealthScribe, a automatically draft patient visit summaries and more. Another big tool in the AWS AI stack is CodeWhisperer. CodeWhisperer generates code recommendations from information. Participants who use CodeWhisperer were likely to complete their tasks successfully, and they average. Last year, Microsoft also reported productivity boosts And then there's SageMaker, Amazon's machine learning Autodesk, for instance. They were able to leverage generative foundational In one example, it was 45% lighter for a particular In June, AWS also announced a $100 million generative We have so many customers who are saying, I want to do that means for them in the context of their own And so we're going to bring in solutions architects and data scientists to work with them one on one. When companies are choosing between Amazon, Google and choose Bedrock because they're already familiar with of data. If you took the data that we have in Amazon S3 that's them up one on top of another, it would take you all International Space Station and almost all the way And that is a lot of data. At the end of the day, Amazon does not need to win Amazon already has a really strong cloud install base. All they need to do is to figure out how to enable creation motions using generative AI. So how many AWS customers are actually using it for We have over 100,000 customers today that are using have standardized on our machine learning service, their own custom models. But in reality, that's not a big percentage of AWS's Although most aren't tapping into it for AI yet, that What we are not seeing is enterprises saying, 'Oh wait let's just go out and let's switch our infrastructure not happening because at the end of the day, even if Amazon customer, chances are you're likely going to How quickly can these companies move to develop these is driven by starting first on the data they have in using compute and machine learning tools that we Imagine you're cooking dinner and you're using a new It is a lot faster to start with ingredients that you prepared to go ahead and put together the recipe than familiar with them and then learn how you're going to Right? That's what AWS customers are doing. They have all the different ingredients that they are And whether that's storage or compute or it's a SageMaker and Amazon Bedrock, and they're putting it And as generative AI continues to accelerate, all the these tools responsibly and securely. I can't tell you how many Fortune 500 companies I've So with our approach to generative AI and our Bedrock anything you do, any model you use through Bedrock virtual private cloud environment. It'll be encrypted, it'll have the same AWS access Selipsky joined six other AI players at the White House secure. There are open problems that still need to be solved, regulated industries in financial services, health We still do not have any well thought out regulatory protection, private information protection and There's also national security concerns. The Biden administration has proposed new rules that cloud providers like Amazon and Microsoft to seek computing services using AI chips. But for now, there's no slowdown in sight for the chips needed to power them. And that race is just beginning. So let's say that we're three steps into a race and we Who's behind? How do the runners look?' But then you and then you realize it's the wrong question to ask, The real question is what's going to happen at the end