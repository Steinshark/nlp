study on Ashish Vaswani and his coauthors' This paper is a major turning point in deep which was introduced in this paper, is now in natural language processing and beyond. Introduction such as language modeling and machine translation, memories, and gated recurrent neural networks approaches. handled by sequence to sequence architeccture. transforms a given sequence of elements, such another sequence. Sequence-to-Sequence models The Encoder takes the input sequence and maps That abstract vector is fed into the Decoder output sequence can be in another language. Decoder of the Sequence-to-Sequence model In 2015, Bahdanau and coauthors proposed a Since then, attention mechanisms have become modeling and transduction models in a wide of dependencies regardless of their distance However, since RNNs were used in these architectures, of sequential computation that recurrent neural from taking advantage of parallel processing This paper introduces transformer, a new deep sequential processing and hence, has a lot Model Details transduction models have an encoder-decoder The Transformer follows this overall architecture. stacked self-attention and point-wise, fully right halves of this Figure, respectively. it in greater detail. Here is the transformer encoder. Let's start As you can see in the figure on the left, embeddings. The transformer architecture was So, the inputs are series of words. Of course, can only work with numbers. So, we should This is what this embedding layer do. and then represent each word by its number into a one-hot vector. But this is actually representing the words. This clever way is Word embeddings are a method of representing words have similar vectors. For example, we vector with And although "Cat" and "Car" would we want their vectors to be relatively different, in totally different contexts. is beyond the scope of this video, but if computerphile's excellent video on the subject. The embedding layer in transformer architecture, for each word in the sentence. Then it's passed next stage is positional encoding. The purpose about positions before feeding the embeddings I'll remind you once again that the Transformer translation in mind. So, I'll use sentences understanding natural language correctly. consider the position of each word, they are of exact same words. the same at all. Actually, the fate of more 'Thanos' comes first or last in that sentence! vital that our models have a sense of positioning. Recurrent Neural Networks inherently take a sentence word by word in a sequential manner. the backbone How can we add some information regarding word order, now that we're removing RNNs for the benefit of parallel computation and increased speed? to add positional information by using the vector that contains information on position The positional encoding should satisfy the It should output a unique encoding for each Distance between any two time-steps should lengths. without any efforts. Its values should be And It must be deterministic. to each time-step linearly. That is, the first given "2", and so on. time step and Consistent distance between However, the problem with this technique is huge, but our model could also be confronted in training. Furthermore, our model may not would limit our model's generalization. to each time-step within the range 0 to 1, the last. One of the issues it will create are present within a given range. To put it not have a consistent meaning across sentences. way for positional encoding that achieves In this method, instead of adding a single we create a d-dimensional vector that contains sentence. Because word embeddings have 512 will also be 512 dimensions, and the positional well. authors used a sinusoidal function. Keep in even and one for odd dimensions. hypothesized it would allow the model to easily for any fixed offset k, Positional encoding linear function of Positional encoding at We can also imagine the positional embedding pt as for each frequency wk. positional encoding in transformer architecture, blog post on the topic, link to which is included The two components that I described so far, are actually doing the pre processing of data. the next part, which is where we do the REAL times. next step. encoder's next element. Let's start with the attention&quot; mechanism. does the pronoun "it&quot; refer to? Is it "the It is easy for us to infer that it is pointing an algorithm to do so. these details about sentences. As you can word &quot;it,&quot; part of the attention process was of it into the encoding of &quot;it.&quot; Let's look at the process of encoding a single First of all, we should create three different each vector with a weight matrix, which is three copies of the input embedding will be vectors. so the result of their multiplication has we can choose non-square matrices. The authors that the 512 dimensional input embedding was after matrix multiplication. "score". Say we're calculating the self-attention is "Thinking". We need to score each word we encode a word at a specific location, the on other parts of the input sentence. square root of the dimension of the key vectors. score value was divided by 8. operation. Softmax normalizes the scores so This softmax score determines how much each the word at this position will have the highest to attend to another word that is relevant The next step is where finally, the value vector with its corresponding softmax score. words we want to focus on while fading out small numbers. up the weighted value vectors. This produces the first word. The resulting vector is neural network. In the actual implementation, form for faster processing. So let's look of the calculation on the word level. of individual words are stacked on top of So, instead of vectors, we end up with query, The entire process of calculating the output of using matrices instead of vectors. That's on multi-head attention. with d-dimensional keys, values and queries, project the queries, keys and values h times In this work, Vaswani and coulegues employed With multi-headed attention, we maintain separate head resulting in different query, key and input embedding X by the WQ, WK and WV matrices On each of these projected versions of queries, function in parallel, yielding output values. The feed-forward layer is not expecting eight (a vector for each word). So we need a way matrix. then multiple them by an additional weights This wraps up everything on multi head self read more about it, check out Jay Alammar's a lot of his material in these slides. You descriptions. and norm layer. In this layer, we first calculate which we just calculated, and the input embedding The aggregate is then subjected to layer normalization. why we should normalize our data? Let's Normalization is good for your model. It reduces features and doesn't allow weights to explode certain range. with gradient descent with non-normalized There are more then one way to perform normalization, the main difference between these normalization variance in order to normalize our data. the right, the batch norm. batch, and for each feature in these sentences, will be used to normalize the data in that For example, here we have a batch of 2 senteces: you can see that each sentence is displayed contains the sum of input embeddings and the In batch norm, we take one feature and calculate And then normalize the data so that the average Of course, we should repeat this for other In the layer norm, we take the average and sentence, instead. sentences. two sentences are from the same batch. we simply use all of the features in every And again, after normalization, we'll have 1. to be used in Recurrent neural networks because on the mini-batch size and it is not clear Transformer architecture chose it as their it performs exceptionally well, especially In addition to attention sub-layers, each a fully connected feed-forward network, which identically. This consists of two linear transformations role and purpose is to process the output fit the input for the next attention layer. a more detailed look in their paper "Transformer They found out that the feed forward networks some linguistic patterns that might be one performance in NLP tasks. lower layers often capture shallow patterns, ones. comprehend these patterns better. Several network of the transformer model appears to of sentences and try to figure out why this Shallow patterns are the ones that come from sentences shown in the first row of this chart Semantic patterns, on the other hand, are looking at the its words. In the second row with base or bases, but also are related to words. in the Feed Forward network, are pure semantic chart are all about tv shows, and the model as "episode", "season" and "NBC". add and norm layer that is just like the one So that's it about the encoder. But before I want to point out 2 more things. connections. These connections send the output of the next layer. stacks several identical encoder blocks on the number of stacked encoder units was 6. First of all, we should point out the different When our model is in the training phase, we and the decoder is in the training phase. translate sentences, we are in the test mode. translate sentences from English to French. We first feed the English sentence into the The encoder has to process the entire sentence we will feed to the decoder. produced so far, should be fed into it. But yet, we simply give it a "Start of the Sentence" Based on the encoder output and the initial then chooses the word that is most likely Now that we have our first output token, we of the sentence" token and the French word to find the best translation using the encoder We repeat For every word output would be the "end of the sentence" As you can see, the encoder uses parallel the decoder takes its time and produces output This time, we were fortunate, and our translation an improper word for translation in one of knowing, and the result would not be as excellent In the training phase, on the other hand, by experts. We use these "target" sentences it would produce the same translation if the To do so, we give the entire input sentence the "start of sentence" token to the decoder, so that it tries to predict what the next Of course, at the beginning, our model will in the back propagation process, it becomes As you see, in order to Train the model, instead as an input, we use the ground truth. Please keep in mind that during the Train utilize parallel computing to speed up their the transformer model, instead of only a portion target sentence to the decoder during the done in the next slides as we go over the The embedding and positional encoding steps encoder. So, we skip them. first attention layer in the decoder. This encoder, but it also masks the input. What As previously said, we'd like to provide during the training phase so that our model The decoder gets the encoder output and full an output that is as much similar to the ground access to every word in the target sentence, This way, The decoder knows what it's next to generalize. we use masking. For example, when the decoder we should mask every word from index 4 until The self attention mechanism used in the decoder main difference is that in this self attention Mask is a matrix like this. It's an upper above its main diagonal are minus infinity. cause the softmax to assign zero probability weight on these words will be zero. our value matrix will prevent our model from yet. procedure, I recommend you watch Lennart Svensson's in the description. multi-head attention. So these calculations and value matrices. through another add and norm layer, which Next step is the second multi-head attention that encoder produced. mechanism uses 3 inputs in order to produce Let's see that for this particular self values come from. takes the entire input sentence and produces copies of it by linear transformation. One the other will be the value for our self-attention the last ingredient for self-attention, which takes the target sentence and in the first Let's say we are currently working on the target sentence are both masked. block will be used as the query. just like before. You may have noticed that gets its values from encoder. The only influence has is that is contributes in the calculation dimension of current attention layer is the If you wanna learn more details about this another video from Lennart Svensson's channel, After this second attention layer, comes yet The rest of decoder is very straightforward. like the one in encoder. Then another add and then a SoftMax layer. SoftMax layer will Don't forget that just like the encoder, connects each sublayer to the add and norm And just like the encoder, several decoder In the original implementation of the paper, Finally, as you see here, the key and value to each and every decoder block. now let's see how does this complex architecture Results of the transformer model: a base model, and This table shows some of the diffrences between model contains the mind blowing number of This chart shows the performance of transformer models. german translation that even the base model In English to French translation, the big the art BLEU score of 41.8. outperforming at less than a quarter of the training cost To evaluate if the Transformer can generalize on English constituency parsing. the sentences by breaking them down into sub-phrases belong to a specific category of grammar like they trained a 4-layer transformer with model portion of the Penn Treebank, about 40K training for this setting. setting, using the Berkley Parser corpora of 32K tokens model in English constituency parsing task. tuning, this model performs surprisingly good parsing. The Transformer architecture was introduced model based purely on attention, with multi-headed most typically employed in encoder-decoder For translation tasks, the Transformer not architectures basedon recurrent or convolutional them. It achieved the state of the art status translation tasks, and In the former task all previously reported ensembles.