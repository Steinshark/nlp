Omri Suissa: I am very happy to welcome Henry Swissa, who is Omri Suissa: a postdoc and a visiting scientist in the conversational AI lab at BSI which I had, and he, we are working busily on all kinds of research projects, and he has. Omri Suissa: I've heard this talk, I think, 3 times, and it's a very good talk introduction with efficient Llm, so, and he takes this very seriously. He's practiced it innumerable times. So it'll be a flawless performance. Go for it. Thank you. Omri Suissa: Right? So yeah, let's start So Omri Suissa: today's talk is about efficiency in the context of Llm, and it's an introduction talk. And by the end of the talk I hope you will be able to run and train Llm. With less resources. So this is the promise of this talk before we dive in Omri Suissa: working Omri Suissa: right? So a bit about myself. I'm a postdoc. And as Shekha said in the air Conversation Lab, we're working in the lab on fascinating research, on increasing supervision of multimodal embeddings, which is Omri Suissa: super interesting if you care about that. My Phd. Was on question, answering on multiple data structure and small models. I also served as a Vp. R. And D at Clemesh that works in the Aa. Space, and have about 2 decades of R&amp;D experience to enterprises mainly all this to say that you're more than welcome to engage with me on these topics now, afterwards Omri Suissa: I'm always happy to to talk about that. Omri Suissa: Alright. Omri Suissa: So Omri Suissa: today, talk is about efficiency in Llms. Right? So what are we talking about? When we actually said efficiency in the context of Llm, the 1st thing that comes into mind is latency right? We want Llm to be fast. Omri Suissa: The second thing that comes into mind is usually resources. We want to use less. Gpus Omri Suissa: Gpus cost a lot, as you probably know, and we want to train model or inference model with as minimal gpus and minimal resources as possible. And the 3rd thing that comes into mind, I guess, is data we want to need as minimal data as possible in order to train our model, because data curation is also very hard and expensive process. Omri Suissa: The second question we need to ask ourselves is, when do we want this efficiency to happen? So one place is inference. We want a model to generate output faster, and the or with stress resistance, and so on. And the other place is doing fine tuning or training. Omri Suissa: Now, most of the methods are going to show you today are treating only one of those processes only inference or training. Note both. Some of them treat both, but most of them only treat once, and some of them are even undermining the other process. That means we can have a model that is very efficient, doing fine tuning, but going to be less efficient in inference, and vice versa. So you need to keep in mind where where exactly you want this efficiency to happen when you talk about efficiency? Omri Suissa: You know. Omri Suissa: So before we go into solutions, let's talk about the problem. So why do we even talk about this efficiency in Llms. And the root cause is the scaling works. Omri Suissa: I guess you all heard about scaling laws, right? Omri Suissa: The combination between how much resources we have, how much data and the size of the model. Omri Suissa: So in general, if I'm simplifying that a bit, the the larger the model, assuming that you have the data and resources, it's going to get a better accuracy and Omri Suissa: simple demonstration I found online is this visualization in size between Gpt-three and Gpt-four in the context of number of parameters. So the larger the model, we get better accuracy. But we get less efficiency. Omri Suissa: Now there is room to talk about other architecture except transformers. That may be less computational, sensitive to size. But this talk is about how to make transformers more efficient. Right? So I assume, by the way that you all know, understand transformers, architecture, basic training and inference processes and no python. But Omri Suissa: that's it. Right? Yeah. Omri Suissa: And by the way, you can even buy online T-shirts of scale is only for Agi. So if there is a meme about that, it must be true. Omri Suissa: all right. Omri Suissa: So before we talk about how we handle with this old phenomena of scale as much as you can. Let's understand how we actually measure model size and how it affects our resources. That means how much resources we're going to need depending on the model parameters. Omri Suissa: Let me Omri Suissa: at the pointer. Omri Suissa: So let's say that we have a 0 point 3 billion model parameters. I mean, we have 1.3 billion parameters. We have, all those parameters are stored in a 60 bit floats, and our secret sync is a 512 size. Omri Suissa: So let's see what going to be the estimate size of that model. And I'm emphasizing it's an estimation. It's like a back of the envelope. Calculation. It's not exact. Omri Suissa: but it's enough to understand what we are facing here. Omri Suissa: So 1 GB going to be for cuda, and there's nothing we can do about that unless we walk in Nvidia. But let's take that as a Omri Suissa: for granted. 2.6 GB going to be our model weights. And that's the size of our float times the size of our parameters. And we have 1.3 billion parameters. So that's going to be 2.6 GB, another 2.6 GB going to be our gradients when we train the model. Omri Suissa: and that's going to be the size of our floats times the size of our trainable parameters, and here we assume that we train the entire model. We will not assume that later on. But here's an assumption. So it's another 2.6. Omri Suissa: Now, the hidden States, which are a bunch of different things like the embeddings, the position encoding the tension matrices, and so on. It takes another about 1 GB per sample. It's very hard to calculate that I didn't find even one, let's say reliable calculation all back of the envelope. Omri Suissa: calculations, but it generally related to the length of the model. And and you can I put a paper here. Omri Suissa: The talks about that. If if you want to read it later, of course I will post the presentation. So Omri Suissa: let's explain it in more details and optimizers takes most modern optimizers, for example, like Adam, takes about 2 times the size of the trainable parameters. Sometimes this goes out notice, but in Adam, in order to calculate the learning rate, needs to store 2 copies of the gradients. And that's been another 5.2 GB. So in total, if we sum everything. Omri Suissa: We're gonna have a model of about 12.4 GB just to emphasize on the estimation here when you upload this model Omri Suissa: to to your Gpu memory. It's going to take about in real life about 11 and some. So it's it's the same Omri Suissa: same skill. And you get the idea. Now, this is for Omri Suissa: bed size equals one Omri Suissa: in real life. We use bit size of 64, 128256, and so on. And, moreover, this is in 1.3 billion parameters model, which is quite small in today's terms. Omri Suissa: For example, I guess I guess you all heard about llama. Omri Suissa: you know, by Facebook. So it comes from sizes, from 7 billion to 405 billion. Omri Suissa: The 4 and 5 billion parameter model Omri Suissa: needs more than 3 TB of Gpu memory in order to to upload the memory to run. So it to train. So yeah, it's it's a big problem. Omri Suissa: So we have 4 variables here. Omri Suissa: We got the float size. We get the number of parameters. We got, the number of trainable parameters and the length of the model. Right? So keep in mind. We're going to come back to that later on. Omri Suissa: So we know our challenge. What are our solutions? Omri Suissa: So this is introduction to the topic, so I'll try to cover as many methods as possible to give you a picture of a broad picture of of this space. But if you want me to focus on something more specific, please let me know. Omri Suissa: So when we talk about improving efficiency in inference. Omri Suissa: we have Omri Suissa: 2 types of Omri Suissa: generally 2 types of method method that improve memory and method, that focus on latency. Of course, that Omri Suissa: reduction of memory, improve latency, but that to say that latency focused method not always improve memory. I will explain that later on. Omri Suissa: and we're going to cover fine-tuned training. We're going to circle back to some of the method we see inference and talk about parameter, efficient tuning, and all these sub method of that Omri Suissa: right? Omri Suissa: So let's dive in into an inference before I start Omri Suissa: any question about that Omri Suissa: right? Good so the 1st method I want to discuss is knowledge distillation. Omri Suissa: Here we assume that we already have a very good model. Omri Suissa: Right? For example, let's call it Gpt. 6, and it's large model, and it's cost a lot of money to run it, and it takes time to to output. Omri Suissa: Now we want to create a smaller model. Let's call it Gpt. 6 Mini. Omri Suissa: That will have the same similar accuracy to the large model, but will be much more efficient to infer, to run. Omri Suissa: So we want to mimic Omri Suissa: the output of our model of a larger model with what's Omri Suissa: we call a student model. So we have a teacher model Omri Suissa: which large model and a student model. Omri Suissa: Now, the assumption here is that while we need a lot of weights during training. Once the model learns the underlying distribution of of our task or data, then we don't need as much as weights during inference. Omri Suissa: So we have 2, main, 3, 3 main options. How to do that. Omri Suissa: The 1st option is response-based installation. Here we take the tissue model as a black box. We don't care about the teacher model internals. We just take the output of the teacher model. We take the output of this student model. Omri Suissa: and we combine between our task loss, which could be a classification, for example, and a distillation loss, a distillation, loss is a loss that computes the distance between the output of the 2 models. It could be L. 2 loss. We could convert the probabilities into soft tables Omri Suissa: and then use cross entropy, for example, and so on, and the idea that the student model will not only learn how to perform the task, but it will also learn how to mimic the larger model. Omri Suissa: Right? Omri Suissa: Yeah, make sense. Right? The second option we can do is feature based installation. Omri Suissa: Here we need to understand the internals of the teacher model. It's no longer a black box, and what we try to match with the distillation loss is components within the network. For example, layer by layer we can match the 1st layer output to be the same or similar to the 1st light output of the of the student model. The second layer output of the teacher model to similar to the second output, and so on. Omri Suissa: So the assumption here Omri Suissa: that if we give those signals in the loss, it will be easier to the student to learn better than just see the end response. Right now, when we say components, it doesn't have to be layers necessary. It could be also attention matrices. It could be a group of layers, and so on. So the idea here is to give signals during the Omri Suissa: during the train, during the inference not only Omri Suissa: at the end. By the way, we I didn't say at the beginning we freeze the teacher model. We only train the assert model. Omri Suissa: Yeah. Omri Suissa: And the 3rd option is relationless distillation. Here. We don't care to map between each output or each component within the student teacher model. We care to Omri Suissa: reduce the distance between the relations between outputs of the teacher model and the relations between the output of the student model. So we're trying to capture here a relational knowledge that exists Omri Suissa: between how the teacher predicts Omri Suissa: output is prediction. Omri Suissa: All right? Omri Suissa: Any questions. Omri Suissa: I guess you could combine the 3 approaches also a great hybrid. Omri Suissa: Yeah. Well, you can. You can look at the response. Sorry. The question was that if you can combine the approaches, you can look at response based distillation as one particular case of feature base. You can see the response of the feature so you can combine them. The relation distillation both. I don't see. I didn't see a paper that combined in theory it could work. But I didn't see research. Say that Omri Suissa: that doesn't work. Omri Suissa: I don't know. Yeah. Omri Suissa: Can you give some intuition. I mean, maybe you're not going to cover them Omri Suissa: steeping. But Omri Suissa: you know, with feature-based installation, you're using information not available in the training data. Omri Suissa: whatever whatever you're Omri Suissa: in response phase you said, you're trying to use a loss that combines both the output of the model and performance of the task. Omri Suissa: But that presumes that you still have access to some kind of label training data or something to do that. Omri Suissa: And so there's a question of like, well, how much should I just train a smaller model on the task itself? Not even worry about the teacher. Omri Suissa: you know. Like, yeah, what? What's actually happening to make an improvement. Omri Suissa: All right. So the the question was Omri Suissa: all right. The question was about how, if if I sorry for simplified it a bit, but how the distillation loss actually improve the the outcome of the student compared to just train it on the classification laws. Omri Suissa: So the intuition here or the concept here is that there is so much you can learn from the data itself, because we're training the student model right on our data set. But there are things that you cannot learn from the data itself that can only be learned from the teacher model. It could be reasoning features. Omri Suissa: I'm way simplifying that. But it could be reasoning features that the student model could not get alone unless you see examples. So you can see as the teacher model as a way to produce Omri Suissa: a more nuanced output of how how the output should be. Omri Suissa: So I mean to be specific. Is the presumption here that the teacher model is. Omri Suissa: you know, open AI, it had access to the full Internet. And your. Omri Suissa: yeah, exactly. Exactly. We assume that this model saw everything, and it's always correct. Of course it's not true. I mean, it may be wrong, but this is the assumption and our model. See our data, and more prone to errors. Yeah, this is the general assumption here. So it's kind of like an indirect fine-tuning. Omri Suissa: Yeah, you can call it that Omri Suissa: exactly. Omri Suissa: Or even if it saw the same data. Omri Suissa: the assumption is that the larger model Omri Suissa: picks up things that the smaller model does not right. And therefore it's still worth doing. Yeah, definitely, it could work on the same data, but usually from what I saw in real life, you use like a large model to train on the Internet and try to small train on a small data center. Omri Suissa: All right. So if you go back to our formula, then what are we actually affecting here? Omri Suissa: Comparing from the teacher model to the student model. We reduce the Omri Suissa: the parameters. We reduce the trainable parameters and reduce the length of the model because everything is smaller. Omri Suissa: Right? So I want to go. It's supposed to be a very light talk, so I'll not go line by line on call, but just to give you examples of open source framework. You can use today that already implement those methods and how you can implement yourself. So I guess you all know hugging face, and they have a distillation trainer. Omri Suissa: which is a class. You can just send the teacher model and the serial model, and it will do response based installation for you so super easy to use. And if you want to implement it yourself, it's also very simple concept. Omri Suissa: You can have a large model that will be on Eval. We're not going to train that. A distillation model would be on train, and then we can just convert the large model output to soft labels and the distillation Omri Suissa: we can combine the our training loss, our task loss and our distillation loss will be some metric between those 2 labels. Again, like cross entropy or whatever. So it's raised to implement. But you don't have to. It's already exists in open source community. Omri Suissa: All right. The next method I want to talk about is quantization. Quantization is actually a form of compression. Omri Suissa: We want to compress our model and to do so, we want to reduce the size of the floats, because most of our data is actually floats right? So the size of numbers is 2 to the power of that size of a number. For example, if it's float 32, Omri Suissa: the range will be 2 to the power of 32. If it's the int. 8, the range will be 2 to the power of 8. Right? So we can reduce this range. And how we can do that. Omri Suissa: we can simply scale it. Or it's simple as that Omri Suissa: we take the ranges of 2 numbers, we get a scaling factor, and then we can divide our number with a scaling factor. And we want to keep 0 0 because it have a special meaning, and we can just simply scale it. So if we scale all our weights, we just reduce dramatically the size of the model. And it's linear. So if we do, from 64 bits to 16 bits, you can see the reduction is Omri Suissa: it's linear and dramatic. Omri Suissa: However. Omri Suissa: when we do that, we also introduce errors. Omri Suissa: like most compressions, and how we can measure this error if we take our matrices, and we quantize them and dequantize them back. We can calculate the difference between them. And this is going to be our quantization error. Right? So Omri Suissa: what have what caused quantization error mainly outliers. If we have most of our values in our weights have similar distribution. But we have some outliers when we scale them. We're gonna completely raise this distinction between different weights. Omri Suissa: So what we can do, we can just clip the range. Omri Suissa: for example, say that everything above 5 is 5. Everything below minus 5 is 5, and so on. And then you can see that the distribution will be much more similar to the original distribution. So we just solved the problem. But we actually introduce another problem. How do we select Omri Suissa: the clipping value? How do we select the mean? And Max volumes? Omri Suissa: So in order to solve that Omri Suissa: there are 2 main methods. Omri Suissa: One is doing that with a collaboration data set. Omri Suissa: That means we have our 2 distributions. We want to minimize, make them as similar as possible. So we have a calibration data set, we run the calibration data set, we run a model on the calibration data set. And we see the actual values Omri Suissa: of the tensors. And now we know what is the right mean and Max to use, we can have different techniques to select outliers. But it's a startup procedure. But the problem is, even though it's a very lightweight, a simple method. The problem is that it may fit to our calibration data set. And that's not going to fit to any Omri Suissa: other data we're going to meet in the future. So another technique is called dynamic quantization that we actually quantize the weights beforehand. But we don't quantize the activations. We keep them in high precision. Float 32, for example, and then only in runtime. We see the mean and Max, of what's going through our layers, and then we quantize Omri Suissa: in one time. So it's done lay over there. So this is an example, what I said at the beginning Omri Suissa: of a technique that will reduce memory, but it will increase latency. Omri Suissa: because we now have another action to do within every layer to to quantize in runtime. Omri Suissa: So it's a trade-off Omri Suissa: wait. Omri Suissa: So if you go back to our formula now, we're going to influence or reduce the float size Omri Suissa: right and by that reducing the total model size. Omri Suissa: So there are many quantization methods. Some suggest that we will use different scaling Omri Suissa: per different groups of weights. Omri Suissa: The research suggests that we use different precisions. That means not just one precision to all weights, but different precisions. You can just ignore or change the weights of weights that are specifically more prone to high quantization error. That means it's the same idea as creating dynamic quantization, but Omri Suissa: only on specific specific ways that are presented as an outlier. You can do it. Row wise. That means channel wise instead of layer of column wise, you can ignore quantization of about 1% of the weights that are more important. Omri Suissa: It's another topic how you select what is more important. But you can do that. And recently there was a paper that a lot of noise in the community of two-bit quantization, which is super extreme now, and by all means there are so many other methods. But just to show you how much this research is current and evolving Omri Suissa: this paper is from like 6 weeks ago. Omri Suissa: And it's obviously already implemented in the open source community. So yeah, so how you do that? So torch actually, introduced dynamic quantization Omri Suissa: super that you could give a model, you could give a type. And and it's quantized activation during training. And if you took a bird model small model Omri Suissa: in today's world, it's already small. You can see the output dramatic reduction in in memory, size, and in latency, but small degradation in accuracy. Omri Suissa: And if you want to implement that yourself, then again. Just do the scaling. We get our parameters. We get the mean to get the Max. We can scale that. And then we can just clamp. That means trim based on scaling Omri Suissa: sorry questions about quantization. Omri Suissa: Right? This makes sense Omri Suissa: good. All right. So another thing we can do is we can just speed up calculations by pressing the to go button on my computer. No just kidding, but we can have ways to calculate the matrices much more efficiently. 1st of all, we can do. Cache caching is a simple method Omri Suissa: in transformers. We are doing the same calculation over and over again, specifically in the K and V, and in the attention mechanism. So we can just catch that. Of course, we're going to increase memory Omri Suissa: on the on. But we're going to reduce latency. We can do approximation of attention. Omri Suissa: That means lowering approximation. And I'm going to talk about it more in detail when I talk about lower on permanent efficiency. But we can just take our attention, decompose it, and use an approximation of the result, and a very famous method called flash attention. Omri Suissa: which actually prevents the materialization of the matrices into memory. That means that when we do matrices multiplication, instead of doing the uploading, the entire matrices to memory, we do something that calls online softmax. That means incrementally calculate softmax and only uploading to memory block by block of the attention matrix. Omri Suissa: I'm not going too deep because it's really depend on how how it works. But it's very easy to use. Omri Suissa: and it has 100% accuracy. I mean no accutation, because it's doing the same calculation as Softmax would do. Omri Suissa: but only block by block, with attention to the different memories that we have Omri Suissa: on the Gpu. Omri Suissa: So how can you use it? Omri Suissa: We have attention permutation in in Omri Suissa: transformers. You can just name the attention we need. We already have attention 3 session 3, and you can use cache Omri Suissa: when you generate Omri Suissa: will again increase memory to the cache size, but improve speed. Omri Suissa: and this should improve Omri Suissa: 2 to 4 times this, based on transformers measurements. The writers of flesh. Attention said it could even improve by 7 times. Omri Suissa: but they measured 2 to 4 times and reduced memory, of course. Omri Suissa: sorry any questions about that Omri Suissa: right? Good. Omri Suissa: So Omri Suissa: The next method. Oh, sorry. Omri Suissa: The next method is speculative decoding. This could be the last method regarding inference. Omri Suissa: and this model going to actually requires use much more memory. So it's not efficient in a sense of memory, of Gpu. But it's it's efficient in in a sense of latency. Omri Suissa: The idea behind speculative decoding is that we can ask a small model. Let's call it a draft model to draft a response and then validate that with a larger model. Omri Suissa: So how can you do it? Let's say that you want to generate a 10 token sequence right? Like a 10 token sentence. So we ask, so we can use the larger model to run 10 times right. This is the Omri Suissa: standard approach. A generative approach, run 10 times and generate 10 tokens right. So this is very slow, so we can use a draft model to run 10 times and generate those 10 tokens right which we much better. And then in parallel. Omri Suissa: we can use the target model to validate the response. And what do I mean by validating? I mean giving the the large model all the sequence and subsequences in parallel that it will take the time of one run. It's in a in a method called Batch processing, I guess you know, and all that. So Omri Suissa: invalid, we mean that the probability that the large model would assign to every token is going to be equal or large, than the probability of the student model. Omri Suissa: Right? So we could call. We call it an agreement. The student, the teacher. Sorry, agree with the student. Omri Suissa: So let's say, we have these 10 tokens, and we run the draft model 10 times, and we now have 10 draft tokens, and we validate that with the teacher model. And it's going to take us one in terms of latency times. One time we're going to do it in parallel. And let's say the teacher agrees with the 1st 9 tokens, but this sorry agrees with the phase 8 tokens and disagree with the 9th token. Omri Suissa: All right, so we don't know what going to be the 10th token. We don't know that because the 10th token is dependent on the prediction of the 9th token right. But we know that the 8 1st tokens are correct, and we also have the 9 token, because we know what the teacher predicts that contradict the student. And now we can run one more time, the teacher model or the student model depending how much more we want to generate and do it again and again. So we just traded Omri Suissa: 10 times of running the teacher model with 10 times of running the student model, and only 2 times of running teacher model. Omri Suissa: And when you talk about latency, it's dramatic, because the gap between using those models in real life is usually very dramatic. Omri Suissa: Right? Omri Suissa: And yeah, super simple. When you use transformers, you can just give an assisted model, and it will do the drafting and and use it Omri Suissa: as as a draft model. Omri Suissa: So yeah, 2 to 3 times. Omri Suissa: So let now let's move to how we efficient fine tune or train our model. Omri Suissa: And the 1st thing I want to discuss is circle back actually to knowledge installation. Omri Suissa: And there are methods of knowledge, installation that affect training or fine-tuning eventually affect inference, but also affect training, fine-tuning. So the 1st one is Omri Suissa: self distillation. Self-distellation is the idea Omri Suissa: that while we train the teacher model, unlike the inference, when we have a teacher model. Omri Suissa: now, when we train the teacher model, we're going to distill itself Omri Suissa: to a student model. Omri Suissa: And the idea is we do that every once in a while. So let's say that we do it every epoch. So after the 1st epoch the teacher was going to distill itself, and we're going to call that student generation number one. Omri Suissa: And then, after the second epoch, the teacher model distill itself. And you're going to call it student generation number 2, and so on. So think about student generation number one. It has the knowledge, the still knowledge of what the teacher learns after one epoch, right and student generation number 5, for example, have the still knowledge of what the teacher knowledge understood of the 5 epochs, but it also could forget stuff, right? It's not like a continuous improvement. Omri Suissa: So there's no better student but what we assume that in inference. In real time. We're going to take all these student models, and we're going to average their output and going to be similar at least as like the teacher model. So we're going to trade here one large model Omri Suissa: with multiple, smaller models which are much more efficient in resources, and we can run them in parallel. So the overall latency going to be like, run run of the student model. Omri Suissa: Another method to do that. Omri Suissa: It's called online distillation online distillation. We don't have a teacher model. Omri Suissa: We only have student models. Let's say we have a data set, and we want to train it on a small model. So instead of just training one model in the data set in parallel, we're going to train several small models. Omri Suissa: and we're going to have a distillation loss that we're going to use at the end of each epoch or batch, depending how you want to implement that that will distill Omri Suissa: from the better student to the rest of the students. So students could be, for example, the same model with different hyperparameters or different models with different architecture. Right? Doesn't matter, but you can think about it as a learning group of students that advise each other while they are learning. Omri Suissa: so the better model at the end of each epoch will distill its knowledge to the Omri Suissa: less accurate students. Right? So those 2 concepts are the same cost of distillation. But doing when we train our models. Omri Suissa: and again, we affect the size of the length and the size of the number of parameters. Omri Suissa: of course. Yeah. Omri Suissa: So going back to self distillation, users add up. Omri Suissa: But has anybody suggested a weighted edition? Because. Omri Suissa: yeah, yeah, there's several different methods to do that. And and yeah, definitely, you can weight them based on the time based on which epoch they've been, and you can weight them based on the average accuracy of each student. And so on. So yeah, there are different methods to this. Omri Suissa: And the question was, if there is a waiting mechanism, we can add to that. Omri Suissa: So going back to quantization, we can have what we call quantization aware training, which is basically making our model more robust. Omri Suissa: 2 quantization errors. It's not replacing the static or dynamic quantization that we discussed earlier. It's just going to make it more robust. How we do that during training Omri Suissa: we for after each layer, or if each calculations we do, we quantize and dequantize back Omri Suissa: our our weights. So as we do that, we actually introduce here Omri Suissa: quantization error. Right Omri Suissa: now, since this is during training, we actually using our loss function to make the model more robust because those errors will propagate to our loss function at the end, and will will be fixed back when we do the back propagation. So by doing that our model become more robust to those 2 techniques of quantization discussed earlier. Omri Suissa: Fix it. Yeah. Omri Suissa: no, it just means scaling back. But maybe I think noise could be a good idea. I didn't see a paper talking about that, but it mainly means scaling that back. Omri Suissa: alright. So we're gonna Omri Suissa: again impact the Omri Suissa: well in training. We don't see that. But afterwards we're going to pick the the float size when we do it in runtime. Omri Suissa: So the next group of of Omri Suissa: of techniques to improve fine tuning. It's called parameter. Efficient fine tuning. There are many different methods to do that. But you can categorize them into 4 main method additive. That means we're going to add something to our network selective. We're going to select which part we're going to train, and which not, and reparmatization, which we're going to calculate our parameters in a different way. And of course there are hybrid method which can combine some of them. Omri Suissa: so here we're going to inference only on the trainable parameters. Omri Suissa: That means we, the size of the model, will be the same, but the number of trainable partners will be less. Therefore the training would be much more efficient. And I want to focus. This is only relevant to fine tuning. That means we assume we already have a Omri Suissa: pre trained model that will train on language and so on. And we just want to fine tune that on our data set, for example, we have this llama 405 GB, 400 million parameter sorry model. But we don't have this 10 billion dollar cluster like Facebook have. And we want to find that on data. So how we find that more efficiently Omri Suissa: so additive means, we're going to add something. Omri Suissa: So the basic assumption that we're going to freeze all weights Omri Suissa: except something. So adapters is a concept. When we add layers fully connected layers after each transformer block. That means we're not going to teach the model the attention mechanism and so on. We're going to teach that our task. We're going to use those additions of feedforward to learn how to adapt the model output of each layer to our task. Omri Suissa: Right? So we're only going to train those fully connected networks that are very small instead of train. The entire transformer Omri Suissa: prompt tuning is another technique which is more specifically to Nlp because it's assumptions. But Omri Suissa: we assume that we could add to a prompt special tokens, as we can learn only them. Now, the concept of a prompt here is not the logical or textual, prompt like. There's a whole bunch of works on prompt engineering like how to prove the prompt Omri Suissa: from the language point of view. Here we treat prompt after they were decoded. That means the prompt is the 1st layer matrices of K. And V. In the attention mechanism. And we add those trainable parameters, and we only train them. So we Omri Suissa: adjust the prompt Omri Suissa: or just the input to fit the network instead of adjusting the network to fit the input Omri Suissa: and the last concept is I to the power? 3. And of course, by the way, I put all the papers on the presentation, so you can later, if you want to review them. And they did interesting research when they found strategic places within the transformer architecture to put activations or matrices of their attention, to put scaling weights that we only train them. Omri Suissa: So it's a different places. But the idea that Omri Suissa: these are the more crucial places, if we want to steer the model to our task without training everything. Omri Suissa: So this is the concept of adding things to a network. But and training all only those things. Omri Suissa: The selective approach Omri Suissa: is that we freeze all the weights, but only train some part of it, for example, Bitfit suggested. We only train the biases. That's it. I know it sounds weird, but they show it works at least on small scales. Yeah. Omri Suissa: just like that. Freeze and configure is a method or a bunch of methods that say, let's freeze part of the weights or part of the layers, and so on, and train only only some of them. A very common method is to freeze the lower layers and only train the upper layers of a model. Omri Suissa: but you can also do it within a layer or within group of neurons, and so on. You need to have some metric, how to select those neurons. A very popular metric is to do a calibration data set like run the network through a calibration data set and see which neurons affect more the output than others. But there are different methods from my experience Omri Suissa: in the industry, not in the Academia. Those methods work less than the adaptive adaptive is much more effective, and repromatization is even more effective. Omri Suissa: But Omri Suissa: again, again, the concept is valid, and and the walk show that it's working. Omri Suissa: Now, reparmatization means we want to calculate all parameters differently. So Omri Suissa: the most famous method, I guess, is lower rank adaptation, and the idea here that we're going to decompose our matrices, the large, vast matrices of the attention mechanism into smaller matrices, and by decomposing them it will be much easier to calculate them, so I guess it's easy to see what is more easy to Omri Suissa: come up with this 5 by 5 matrix to train that or train these 2, 5 by one and one by 5 matrix. Of course this is easier. And of course we're going to add noise because it's an approximation. Omri Suissa: but it dramatically reduced the amount of calculation we need the memory we need, and so on, because we only upload those into memory, etc. And then, in inference, we can combine the Omri Suissa: weight matrix to the lower metric. So if Omri Suissa: sort of more formal, this is the weights of the pre-trained model. Omri Suissa: and in full fine tuning. We're trying to learn some change of that weight. And then this is our fine tune model. Right? So in Laura we have the weight of the model. And you're trying to learn 2 matrices that their multiplication will be the same size of the original change weights. Omri Suissa: and this Omri Suissa: dramatically improve memory consumption as you can imagine, and there is a concept of ranking as an IP parameter. That means how much we want to decompose. Omri Suissa: The smaller the matrices, the less memory we need. But the more error we're going to introduce, of course, so the rank of the Laura is a super important high parameters in that sense. And, of course, as you can imagine, based on that, people suggest as many different things, so you can take that to be more and more nuanced, and Corona, for example, suggested to decompose the matrices in another way, and there are many matrices and many, many methods to Omri Suissa: to do it in in different concepts. But the idea in the end is just train small matrices that we have the same, the same size of the what we'll do in in fine tuning. Omri Suissa: So yeah, already implemented in a library called Pft, which peft, which is parameter, efficient tuning, fine tuning. And you can just have a lower configuration the rank is the port part, the other things come into play. But this is the main idea. Omri Suissa: and if you want to implement it yourself. Omri Suissa: It's fairly easy. You just need to have the input and output dimensions and the rank that you care about. So we can create an age matrix, which is the input times size on the rank and the rank on the output dimensions. That means if we multiply them, we're going to have the input times the output in size. And we're going to have some sort of Omri Suissa: of a hyperparameter for waiting. And then in forward, we could just multiply the input with the A and B, which should be the same size, would be the same size as the original weight matrix. If we do full fine tuning Omri Suissa: alright questions about the Omri Suissa: Laura. Oh, I didn't say that Omri Suissa: there is Q. Laura. Omri Suissa: which is Laura, and we quantize the matrices as well. So an example of how you can combine methods. Omri Suissa: Yeah. Omri Suissa: any question on that. Omri Suissa: Alright great. So final notes efficiency is both engineering and theory. Omri Suissa: It's it's exactly when Omri Suissa: computer science and data science myths. I think they're a place for both. Some of the most advanced that we have in efficiency comes from very good engineering without training a theory at all, and some of them are more theoretical in nature. There is a lot of room to improve by all means. Omri Suissa: All the methods I presented does not mean that we have efficient Llms. We don't have efficient Llms. We just have better inefficient Llms. So if you have ideas or want to research that it's a very interesting domain. There are many methods you cover. Of course, as this is an overview, and the devil is in the details. I mean, some methods can be combined. Omri Suissa: and I put here some names here. You can read off them if you like. That combine very well, for example, efficient tuning methods, and so on. But some undermine each other. So you need to be careful if you try to combine those methods. One thing about reading papers, or I guess you know benchmarks only good for benchmarks. I saw a paper that Omri Suissa: present a very good on the backmark, but in real life it didn't work well, and so on. So my advice is not just to pick the best papers. As is increased, the best memory decrease the best memory, and so on. Try it on your data. Omri Suissa: And of course, this is just an overview. So I simplified stuff to get the main point. So I put all the papers in there. So we can dive in. Omri Suissa: Yeah, that's it. I have a few more minutes of questions. If you have. Omri Suissa: Yeah. Omri Suissa: this is excellent talk. Thank you. I'm wondering if Omri Suissa: the Omri Suissa: like this literature, like moral compression, knowledge. Omri Suissa: distillation. Omri Suissa: If this literature Omri Suissa: talks or overlaps at all with the Omri Suissa: traditional kind of Omri Suissa: compression, literature from computer science. As you know, this is kind of in some ways a form. Omri Suissa: So how much overlap are they? 2 different to share ideas? Omri Suissa: The short answer is that I don't know the longer answer that quantization is very much inspired by the same concept that we have a compression. But I'm not sure. I'm not sure if someone did this reference. I Omri Suissa: it just Omri Suissa: any other questions. Omri Suissa: Anyway, it was visited in some places. I was just thinking Omri Suissa: there wasn't a lot of Omri Suissa: application on the basis of interpretability or explainability. Omri Suissa: You can imagine with language models. Omri Suissa: I mean, if you think back to like deep dream or something they're like, Oh, I'm trying to figure out this node needs this and therefore great. So you know, I'm just wondering, is there any Omri Suissa: active? Omri Suissa: Yeah, so yeah, there is work on that. The question was about if we can use explainability in some sense to select which Omri Suissa: layers or neurons want to compress. So yeah, there is work on that, both in knowledge distillation that we can use distillation loss only for specific layers on specific areas, mainly with a calibration data set. Then you run them, and you see which terms more affect the accuracy. And in quantization there is a method. Actually, I think I put in this paper that suggests. Omri Suissa: let me see. Omri Suissa: there's a paper suggests that Omri Suissa: yeah. Omri Suissa: activation, aware weights quantization that suggests to find some metric of which are the most important weights, and don't point as average Omri Suissa: keeping in high precision. Now, what is more important, and what's explainability? Here is the hardest part. So there are ways to do explainability, but it's still not that accurate. So Omri Suissa: you know, envision. It's much easier to figure out that there are certain directions you're talking about accuracy as a percentage. But it also matters what your residuals actually are. Omri Suissa: Well, exactly. It's hard to figure out what they are. I mean, they'll work on. That entropic did a very extensive work on their model. They present that they try to figure out part of the network what they meaning, if they're reasoning, or even Omri Suissa: attending to specific topics or stuff like that. So but I think it's we are very early stage of of explainability Omri Suissa: to effectively use that in such methods. Omri Suissa: Yeah. Omri Suissa: I hope it will be in the future. Omri Suissa: Right? Omri Suissa: Thank you. Omri Suissa: Thanks. Ben Sapirstein: Thank you. Omarine. Omri Suissa: There were any questions on that.