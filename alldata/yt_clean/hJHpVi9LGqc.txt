One commonly asked question online I see is I've set up this Proxmox hypervisor, now how do I set up a basic file share for it to work like a NAS? And in this video I'm going to take a look at multiple different ways to set it up as a NAS, as well as some other things you might want to think about if you're using your Proxmox server as a NAS and put some to potential hardware considerations you might want to make. Proxmox is a great hypervisor or OS that runs other OS's but it doesn't have any built-in NAS functionality. There's no built-in way to set up a file share or GUI for doing that in Proxmox. So all your file sharing is going to be done by either programs installed on a Proxmox host, in a VM, or in a container and there are a lot of pros and cons I'm going to talk about in a minute. The first thing I want to talk about is does it make sense to make it so your compute box and your storage box is on the same piece of hardware? There's definitely a lot of advantages of having it on the same piece of physical hardware including having less physical space, typically less power consumption, and less cost. But there might be a few other things you might want to think about. The first thing is typical NAS storage I think of is relatively low I.O bandwidth and typically best suited for mechanical hard drives because they're cheap and allow a lot of data to be stored cheaply. But VMs you typically want to have high speed I.O where SSDs work best because you don't want to be dealing with slow VMs running on hard drives your likely want to make sure your hard work is capable of storing multiple SSDs for your VMs and multiple mechanical hard drives for all your data. But this really depends on your use case. If you need a high performance NAS you likely want to go all SSD for all the storage and if you're going with a relatively small NAS, maybe less than a terabyte, going all SSD is great as it just makes things work faster. So it really does depend on your environment but I think combining it makes sense in a lot of smaller environments. But those definitely times where having two boxes might make sense because it can often be hard to find a box that has the exact processor and RAM you want for compute applications as well as having all the drive bays you can want for storage applications. Let me know in the comments below if you have specific questions you want to ask or if you want me to go in more detail about hardware recommendations for NAS or NAS plus VM combined use cases. The next thing I want to talk about is RAID. RAID is very commonly used in NAS storage as it gives a few advantages which make it easier to work with. Some of the big advantages I see of using RAID on a NAS box includes redundancy in most RAID levels so a drive can fail without any effective operation of the NAS share. It also typically gives you more speed than a single disk will give you which is great and it also makes it easier to manage your data. So instead of having one network share for each of your drives you have a single large network share that's spread across drives under the hood. This can make it a lot easier for users to work with because they just see one large share and there's multiple drives under the hood. Now when it comes to setting up hardware and software to use your multiple drives in RAID there's a few ways to do this. Some systems might have hardware RAID cards kind of like this one can be used as and that means that multiple drives will go through the hardware RAID card and be presented as one drive to the OS. This might be your best option if your system comes with this included in the hardware and they can have some advantages like additional battery backed caching for high speed I.O. but they seem to be kind of fading away especially in the small home server or small business server market. And Proxmox luckly has ZFS built in which is a pretty good software RAID solution which in addition to doing just RAID also has a lot of other cool features like being able to do snapshots, being able to read checksums of data to make sure there's no corruption, easy migration of ZFS send, and a lot more other features I'm not going to go into right now. And luckily Proxmox has a pretty decent GUI for managing it. If I go on my host right here under it I can go under discs and then ZFS and I can see for example my different ZFS pools go into detail and see everything here that's going on on my pool and I can also go under create ZFS and create a new pool here. And this GUI is relatively similar to what you can do in the installer which makes it a great way to set up a mirrored boot drive on Proxmux or set up another different RAID array on your Proxmux system using ZFS. But ZFS has some limitations. For example if I have a parity array or RAID Z array in ZFS terminology I can't just add a 6th drive to a 5 drive array for example. It also doesn't work well with mixed drive sizes. Other storage systems work a lot better in those configurations and luckily you can still use them on top of Proxmox by using hardware passthrough Gonna talk a little bit about this more in the VM system but using a VM you can run something like un-RAID or Windows or storage spaces or anything else you could pretty much want to on top of Proxmox and use that storage subsystem. I also want to take a quick note to talk about the different protocols that are used for typical NAS file sharing. Typically they're going to be either SMB or NFS. SMB or SAMBA is a Windows based file protocol which is able to use on pretty much any client system including Windows, MacOS, Linux on the desktop and even most phones, TVs and other types of devices typically work quite well with using SMB or SAMBA shares. It does have some interesting issues at times with Linux because it's built upon Windows permissions and how Windows does files rather than Linux on files. But for client OS's it works pretty well and you can integrate SAMBA with Active Directory if you want to. The other main file server is NFS, so Network File System. This is a kind of Linux or Unix based system that follows the Linux and Unix based protocols and typically is what I'd use if I'm using it to mount on another hypervisor for example. I typically use SAMBA for client to desktop OS's and NFS for other hypervisor Linux uses. In my basic small home performance settings they're effectively the same, no some minor differences but if you're working off normal drives on a relatively low speed network they will be effectively the same from what I've seen. With that out of the way, let's first talk about how you can actually set up your network share to make it work. And the first method I'm going to talk about is just using the Debian under the hood that powers Proxmox. So Proxmox is built upon Debian Linux which allows you to use all the standard Debian Linux tools. The big thing to note with doing this method is it kind of breaks the rule of a hypervisor should only host VMs and containers and not be used for other tasks on the host. Some of the disadvantages can include making it so you can accidentally do something wrong on the hypervisor and take out all of the VMs and containers or shut down the system making it offline for a period of time. It also means that if you're backing up your system and VMs on it, it isn't included with VM backups so it's something else you have to think about. And it also means you have to take note of it and manage it in the command line instead of in the Proxmox GUI which can make it easy to forget if you need to migrate to a new piece of hardware later on. So make sure you take note and make sure you're aware of these disadvantages before going forward with this method. But with that out of the way, for the first thing I typically like to start with is just googling &quot;Debian Samba Share&quot;. This is a great place to get started and often has a lot more details of how to cut up one of these. So there's a lot of great guides out there that other people have posted and typically the steps will involve doing the &quot;App install SMB Samba Tools&quot; which is what's going to actually share the files over the network. So let's actually open my terminal connection and run that now. So I'm going to run &quot;App install Samba&quot; which is what's actually the server on the system. And then the next thing I'm going to have to do is I'm going to have to edit my config file where it's going to tell it what files I should share and how it should share over the network. I will mention there's a lot of Windows sharing specifics like Active Directory domains and work groups and other things. Not going to get into them now. And if you're working as a basic home server where you're just mounting a network share, you probably don't need to touch these. And now I've gone and edited my Samba config file with the name of the share, the path that I want to share, and a quick comment that it's able to be viewed by the clients. The basic configuration here is pretty much all you could need to do. But as I look at this page here, there's a lot more options that I can get into that I'm not going to get into in this video. Let me know if you want me to make a specific video going over Samba shares and all the different potential options and things you might want to think about when setting up a Samba share in the command like this directly. So I'm going to exit out of this. And then there's a couple more things I'll have to do before I can actually mount the share on a client. First thing is I'm going to have to make my user actually have a password. So even though I set a Linux password and making this user EW, I am going to do SMB password dash A to add a user EW. And it's going to give me a different password prompt for my Windows Samba password within Linux that I'm going to use when I map the Samba share different than my password. If I want to SSH in or login as that user directly in a Linux system. And then what I got to do is I'm going to do system control restart SMBD, which is the name of the service that the Samba share uses. Once that's restarted, it applies those config file changes. And let's see if I'm able to mount that on the system. So on a Mac, I'm going to use the command K option. Now I map my network share and I could see all of my files, but I couldn't write anything to it. So a few things to check if you have that problem. First of all is the configuration. One thing that you might want to change in it, and I did here is change the right will flag to yes, because by default, it's going to be set to read only looking at the man page for Samba. The next thing to be aware of is there's two sets of permissions. Does the Samba share permissions, which typically I set to be pretty lenient to let everyone read and write data, and then the share permissions and both have to let you access the file. The share file permissions can be edited using Linux file permissions and the commit and that share permissions can be edited here in the Samba configuration share. So once I added writable and gave my user permission using change mod, then I can go make new untitled folders in the share and access all the data. Setting up a Samba server using the command line, editing config files and editing permissions can get complicated pretty quickly, which is why it's really nice to have a little web GUI, especially if you're new to setting it up and makes it a lot easier to see what the options are. And if you want to do that, doing it in a container or VM allows you to have web gooeys and makes it simpler and also gives you the option to use the command line if you want, as well as the other main advantage, which is your Samba or file share is isolated from the rest of the Proxmox OS. So if something goes wrong, your VM or container won't be able to break out and break the rest of the VM in most circumstances. So let's talk about why you'd want to use a VM or container and what the differences are. The main advantage of a container is that it's lighter weight because it shares the kernel with the host Proxmox operating system. It uses very little memory disk space for the OS and CPU. But with a larger system, the difference isn't that big and you're not saving that much. So it likely won't matter performance wise for a lot of use cases. But containers have some other advantages like being able to map a file share on the host of the system to it, which VMs can't do, but VMs have their own advantages like being able to do direct block device pass through or PCIe pass through to do it. If you're trying to just share a specific path or make a virtual disk to share using Samba, I think setting up a container makes a lot of sense. But if you want to use NFS servers, which need a kernel server, which requires editing app armor profiles, there is a Proxmox forum page that goes over all the steps you need to make. But if you don't want to worry about making those setting up a VM means it just works without thinking about it. A VM might be a better option for you. And if you want to run RAID or alternate operating systems, a VM is going to be the only way to go. Let's talk a little bit about more in detail about setting up a file server or a Samba server within a container. A container gives you a couple of options. The first option I see is setting up a Debian OS and just editing the same files. I can go on here and I can add the same S&amp;B config file, mount it on my Mac, and it just works It shows all the files and I can create a virtual disk for it to be able to use. The next big advantage that using a container gives you is that it's able to map a drive from the host onto it. So if I take a look at the wiki page here, I can see I can set up this map point on the container config file and map an existing directory on the host with data on it and make it accessible to the container. The nice part about doing this is it can already have data and it's just accessible to the container to do it. And I can also map the same path to multiple containers if I want a shared space that multiple containers can access without having to configure the containers at all. The disadvantages this point on the host isn't backed up like a container disk would be. So you need to think of an alternate backup solution if you're backing up Proxmox VMs or containers so that your data in that file share is being backed up as well or back it up via some alternate method. I would typically recommend if you're creating a blank disk, just go under your container here, go under add and say mount point and just create a blank mount point. So that way it's easy to back up that if you're backing up the whole container or migrate it to a new host if you want to move the data around. And then that way the RAID would be managed via ZFS or hardware RAID on the Proxmox system and it'll just have a single large disk that you're sharing. The next thing I want to point out is the turnkey file server container. And this is a really nice way to be able to have it so that you can just set it all up in a web interface while still having a lightweight container. I did a video focusing on this up here so I'll link it up if you want to take a look at a full video of all the details. But the great thing is I can see a little web interface like this and see all the same parameters I edited in the config file, but presented to me here so I don't have to remember is it writable, is it read only, what is the parameter name and edit that config file. This can make it a lot easier to set up and still essentially does the same thing under the hood as setting up manually in Debian or another OS within a container would allow you to do. Now let's jump over to setting up VMs and using VMs to do file sharing. The big advantage I see with doing a VM as a file server is you can set up whatever RAID configuration and OS you want, which means off the shelf RAID or NAS focused OSs like Unraid and TrueNAS work just fine and give you access to their full web interface as well as their own RAID solution. Running Unraid on top of Proxmox means I can use all of Proxmox's interface here, which I typically prefer to Unraid when it comes to running VMs, while having all of Unraid's disk storage methods which work really well with mixed disk sizes and can be a nice web interface for managing that. The simplest way to make it so that your host OS is able to ask us the individual disks is go under add and then hard disk and then edit this in the config file. So instead of having a virtual disk on the host it's accessing, it can access a device like slash dev slash SDA for example, or STP. And this gives it full access to the whole disk itself, but the problem is it doesn't pass through smart data. So you have to manage your smart data or health data about the drive within the Proxmox web interface instead of inside Unraid or TrueNAS for example. The next thing to be aware of is hot swap doesn't really work like this. So if you want hot swap and smart data, you have to do PCIe pass through. And if you want PCIe pass through to work correctly, what you typically want is to have a separate device with PCIe pass through, a system that is capable of doing PCIe pass through, which typically means your CPU has to support VTD or AMD IOMMU, and then editing your system so that it has the editing grub files. Typically all these steps are going to be in the GPU pass through. So look through those guides if you want more details about how to edit the config files. Once that's done, you can go to your system, go under add PCIe device and give it the device that you want to pass through. Once that's completed, your system should boot up and have access to all of the disks provided by that PCIe device and hot swap. A few things to note though, make sure that this HBA isn't also hosting your Proxmox boot drive. You'll want to think about this when setting up the system. Using NVMe drives can be a great way to do this as it's a separate device that can't be plugged into the controller, but you can't have a SATA drive that Proxmox boots off plugged into an HBA that you also pass through to a VM. I've also seen some weird circumstances where you're able to pass through and the disks show up, but it's not really stable and crashes after a bit, so I just copy 100 gigs of data to it and make sure it looks correct before throwing it into prod. I also want to point out that you don't have to do disk pass through. You can just give it a single large disk like you would with a container or on the host directly and have it access all of its files in there and have the RAID or disk managed by Proxmox on its host using ZFS or hardware RAID or a different solution. I use this a lot when I need a basic little file server setup for some circumstance, so for example, I've set up TrueNAS as a simple way to set up a time machine share to backup my Mac instead of having to set up the Samba shares directly on my main NAS. And the also advantage of setting it up using virtual disks on the host means I can back it up like I could with any other VM and pass all the data. If you're doing disk pass through, you will not be able to backup the disks with Proxmox backups using PBS and you have to back it up using an alternate method. Snapshots also won't work with those drives and you won't be able to easily live migrate it to a new host. And once I've gone over the different steps for setting up my VM and installed an OS, I can get something like this. This is a standard TrueNAS CLI interface on the host and I can open the web page here and get the same familiar interface that you would if you installed TrueNAS on the host, but with the advantage of being able to use Proxmox for VMs, which I typically find better than TrueNAS in my opinion for managing VMs, backups, clusters, and other things. But it's a great way to be able to combine the advantages of TrueNAS and being able to easily set up a share with all of its permissions and settings being easily accessible in a web interface along with Proxmox and all of its advantages of being able to easily manage VMs, clusters of systems, backup using PBS, and other advantages. Hopefully this video is interesting or useful if you're looking at setting up a NAS on top of Proxmox and gave you some more things to think about. Let me know if there's any of these topics that you'd like me to go more in detail about because I'd love to go into more detail about these topics that I kind of brushed over in this overview video when it comes to a NAS on top of Proxmox. And thanks for watching.