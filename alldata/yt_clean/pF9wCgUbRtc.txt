We are going shopping for a new concept Keep your hands free because we are going You guessed it. The topic for today is B&amp;G foods. B&amp;G foods is a feature extraction technique to convert text into numbers, and it's exactly what it sounds like. A collection of different words. A great use case for B&amp;G foods For example, about the latest news, maybe some interesting messages and perhaps a few spammy content. Saying that you have won a lottery Bag of words and the frequency in each of these emails Which of these would be spam? So today we are going to be looking at what bag of words We will be looking at the pros certain applications, and also modifications that we can use to improve our bag of words algorithm. Like I said, bag of words is a feature all of your different texts are converted into numbers. After all, numbers is what our machine I like to think of Bag of Words Let's think of the different words And each word represents a kernel. Or rather, The cool thing about Bag of Words is it can also be applied to visual elements, which is bag of visual words. Let's say, for instance, And yes, this is how I draw a cat, but you can break down this image of a cat into multiple different key features. You could have an ear, you could have whiskers, a body, legs and a tail. And each of these different elements computer vision techniques, So you can use bag of words, but also on visual words, which is images. Next, let's take a look for different sentences, Common MLP tasks text classification. Let's say for example, spam or not, you could have your email and depending on what So this is an example Another example could be that of document similarity where perhaps you want to compare and check Or maybe you have a particular query, like the type you put in a search engine, and you want to find the most relevant documents. Both of these examples text Use bag of words in the back end. Now let's take an example of two sentences and see how we can convert the text into features on numbers for our machine To understand. Consider two sentences. Sentence number one I think. Therefore, I am. And sentence number two. I love learning. Python. Now that we have our two examples what we are going to begin with or a dictionary, set up in all of the given documents. In our case, here are only two sentences But let's take a look at all So we have AI as a unique word. Think. Therefore. AI has already been covered over here, I'm going to the next sentence. AI is also included here. Love learning Python one? That's 12345677. Words are seven. Unique words our vocabulary Let's look at what of words representation and what we are constructing over here So here are our documents. We consider our first document. And these are the different terms So going over the first sentence She look at the count of the word And you try to see how many times So I have used a total of two times. Think once. Therefore once. once. And in our first sentence, love which is why they get a score of zero. Doing the same technique I appeals a total of one time. Think therefore and are absent they get zero and love learning in Python, which is why they get one. So what you're seeing over here that represent the first sentence. So we have now taken words a feature representation. That is we have numbers over here, learning models used to understand. And similarly this is the feature representation Now that we've seen what bag of words looks like the pros are kind of obvious. It's simple, which is how you saw it. You count the number of times particular to that particular position It's easy, which is what we did over here. And it's explainable as opposed to certain other algorithms that maybe are not as intuitive. Unfortunately, as with all things in life. There are going to be pros Next, we'll take a look and see if we can modify it Let's look at some of the drawbacks The first one being a compound word. Think about words like AI, artificial intelligence, or New York. In a simplistic bag of words approach. You break down artificial as two separate words with no correlation That would apply to New York as well, one word and York is another word. In this case, that exists between the two words, Let's look at another example. Perhaps kick. And baking. Maybe racing as well. Given these three words cake baking and racing, cake and baking in the same context, in the same documents as opposed to cake and racing. Well, of course, a new sport called cake racing, But let's hope it doesn't. In this case, our Bag of words to associate the correlations which might pose a problem Let's look at another drawback of Polyphemus words. Consider the word biting. Looking at if I'm talking about The animal. Maybe there's another word that's content or content. It could mean either of the two, hard to see which is which. Another drawback that exists is that we lose the order associated Like I mentioned, Bag of words is nothing with each of the kernels And when you shake that bag, you lose As far as the order of the words Let's say, for example, I have a sentence that says flight San Francisco, Mumbai from unto. What does this mean? Am I trying to fly from San Francisco to Mumbai? Am I trying to fly the other way around from Mumbai to San Francisco? It's hard to tell when we have only the bag of words available. Last but not the least is the problem of sparsity in our bag of words approach. We look at each of the unique words and denote the presence given a large number of documents. You could have a very, Yet in each of the sentences, there could be maybe only three words that actually are present This leads to the problem of sparsity. Since our matrix are our vectors, in the sense because they're denoted by zeros, This could also pose a challenge Fear not though. Despite these drawbacks, Let's take a look improve our bag of words. Approach. Our first modification is n grams. Instead of looking you can now look at a combination of words For example, being the phrase we don't break it but now we look at the presence and denote how many times Similarly, for New York, of New and York right after each other and denote the number of counts In this case, since our words are made up, and is equal to two, is equal to three and is equal to five, In which case you would look at, you would look at three words So maybe it is Python artificial intelligence. And any time you would count the number of times And given the occurrence in here. Another modification that we can do is text normalization. Text normalization refers to certain that you can do before Model. A good example for in which case you're in the hope of getting back to its base Consider the words coding coded codes and code. When you start removing You can try to get to its base word, which is called in this case. This is a way to reduce or reduce your dictionary words, and hopefully that will help An important concept is Tf-Idf or term frequency. Inverse document frequency. You can think of Tf-Idf as a weight or a score or perhaps even a feature scaling TF is the term frequency, or the number of times Let's say the words votes. President. Governments Probably has something to do with maybe elections So higher the term frequency higher is the score That makes sense with inverse document However, That that particular word occurs in. And if that word occurs or a huge proportion of documents, So the more number of documents the IDF score and the lower This may seem a little counterintuitive, It's opposite of the term frequency. But I give you un and some. But what's it? Don't but they're used to create As you can imagine, in an English language with English language in it, Perhaps, maybe even the most frequently In that case, we do not want these words to have a high tfidf score, which is where the IDF component lowers As these scores or the sentence of the documents. Let's take a look at some applications of tf IDF. Let's consider document classification as an example. Perhaps you have a company and a product and you have a support channel for them certain concerns, complaints Maybe you have a chat associated with your customers and you could use the bag of Words to understand which of the teams are associated with the problem Maybe you have a building team or an onboarding team. Or a trial team. Or maybe it's a documentation issue. Looking at the vocabulary that is present, that is looking of what is entailed in the customer chat You will then be able to identify and the appropriate team Another example of bag of words is what to make. You might have heard of a to. These are word embeddings that exist in an n dimensional space. Your words are represented as vectors in this n dimensional space. For example, king and queen are two words, and the closer that means they are more In this case, king and queen as you would find documents or sentences appear together. Maybe you have another word swim, but you wouldn't really associate swim as much as you would So swim would be further away This is called and it does use bag of words as a back end Another example where bag of words comes in You could look at the collection of words and understand Maybe words like happy, joy, excited, or words that are negative, frustrated, angry, hate, terrible. And depending on the bag of words to identify with sentiment You could even take this further that helps to keep speech. So you would look at the negative sentiments and maybe extended with other words, or other discrimination forms, you distinguish these annoying Now that you have this concept in the bag, I hope this helps you understand processing and encourages you to continue your journey If you like this video and want to see If you have any questions or want to share please leave a comment below.