This beginner's DevOps course is your first It is taught by the CEO and co founder of LayerCI. The goal of this course will be for regular to learn fundamental DevOps concepts so that We'll also be talking about DevOps beyond that will primarily be talking DevOps is a methodology that helps better by continuously integrating user feedback. And if you google DevOps, and you look for it really helps understand how DevOps of thinking about software developed. much like things would be developed in a and then the output, you'd have a product that But since the advent of the Internet, it's become really easy to launch things and get product instead of making a new version of the upgrade instead of requiring you to buy, you old games, like some city would require you to buy formalized by DevOps, the sections are planning, to build. And you work with your team to make some you code them. So developers on your team will released. And of course, they're built. So for bundle it into JavaScript that a user's browser releases for various different versions that and versions that run in the browser. So you So testing is both automatic and manual. as continuous integration. And manual testing And then after it's tested, and you know, the it's released. And continuous deployment automatically after a change is known to be good. In larger teams. There's, you know, popular tools in later talks. But the core idea is you want to users in a way that they don't notice if there's you might show it to a few percentage of show it broadly. Again, a company like Facebook, users complain, they'll get hundreds of millions deploying means it's released to your users for a the internet. For a CD ROM, you know, you bundle for a mobile release, you'd build the artifact the App Store would review it and then publish And then you operate it. Operating making sure that enough resources exist for configuring things dealing with on monitoring. So as your users use your software, jobs and create posts on your forums, you want And then finally, you take all this feedback, So the planning stage takes all the user feedback deployments, teams learned about deploying to build out new features, solve bugs, and make the architecture. And then just continues in the our company uses DevOps, or our company is transformed. They mean that instead of taking a which is then shipped, it's a continuous cycle Scrum cycles, usually, and producing software had some say in producing DevOps engineering is the methodology, which is something that maybe there's a subfield of DevOps engineering, You know when they say DevOps, and mean when they say DevOps. So if a job you know, they're not asking for code. They're mostly asking for someone that So the three pillars of DevOps engineering, our and application performance management. And we'll pull request automation helps developers build their proposed change is good, faster. Deployment that users don't complain. Again, Facebook has just threw their code out into the void, there'd be hundreds of millions of complaints. And around making sure that things are healthy. So waking someone up, if you know, the site goes things if there's a problem. And we'll get into The first pillar, which I mentioned has primarily to do with the developer feedback by proposing these atomic sets of changes they're full featured on their own, First, it's like if a developer proposes a pull is good. And as far as they can tell, the change what they have to do is get through some gates. their goal is to make sure that developers can or not. So for example, if you're working on a adds a typo, that's something that can easily a typo gate that says no change may go in if it sure that developers get automatic feedback about as of 2021, usually they mean Git. So Git is and it helps developers make these sorts of request is usually reviewed by at least one other where the other programmer will tell you about whether there's architectural problems, scaling automated. But that process of review can also be And DevOps automation can help with things like from all of these other automations that we'll Usually, an engineering manager or product manager will get feedback. So if you create a new button, designed the button, and you'd like the product Both give feedback because if the button is mobile, responsive, those are all problems that be great if the original merge request fulfilled proposed. And so usually, non technical people So what can be automated for a DevOps engineer, you can automate things like automated test running, per change ephemeral notifications through reviewers, getting the And the end goal of all this automation is that and get it merged the same day they propose the because it means that critical bugs can be very needing a special process. And it also means that they can propose changes once they get through there isn't additional special gates that proper gates and automations have been set up, without having to ask everyone in the company. or not. By virtue of passing the tests and passing good. And if a problem does arise, a new gate future problems don't occur. The second pillar is 2000s, the founder of Stack Overflow, places Can important question for a development organization, The efficiency of the build process isn't the other goals include the deployment strategies, I show a feature to one user at a time starting downtime. If you have to shut off your turning on the new version, the visitors that will notice downtime. So there's clever And finally, rolling back versions It's easy to overcomplicate the planets. Many for building and distributing releases. is finding the appropriate deployment tools to And then ideal world there should be little shelf solutions like Spinnaker and harness are Finally, application performance management, even errors. There's a famous case where a user put a Overflow. And they brought down Stack Overflow, because Stack Overflow hadn't deployed their code whitespace. So a bunch of space characters at the with the simplest things like just a messaging production and are only uncovered by users. So metrics like how long it's taking for requests all of those key health metrics are being of the requests to the landing page are suddenly be notified automatically, instead of an engineer Logging. So as a program executes, it will information about the state of things. like, you know, a user visited the website with address? What was their username, what resource for fulfilling that access. So if they had to was slow, it's useful to be able to say, the user fulfilled slowly. But the request was fulfilled slowly. So mapping these requests all the way down monitoring. So again, I mentioned metrics the logs and metrics, how slower things how if there's a bunch of load, you might decide the number of servers, so add more web servers if there's errors, you might want to automatically And if there's a downtime, you might want to call and take care of the downtime. And they can drop And that's alerting. So alerting is when fault the metrics, some number of requests or to slow to notice degraded performance, someone should be a new product, shouldn't dive into DevOps, talked about our end goals for really large developers that add automation So a new startup with no users building a website. pillars two and three are essentially useless a downtime won't be noticed by anyone. It doesn't need to run automated tests, a useful stack for five, or sell or our product where you can get developers. But that's about as far as you care for every proposed change. And you can play around whether it's good or not a team building an app much more sensitive to downtime. So test coverage On logging and log aggregation error and code Cove for automated test running, there's known for mobile testing, and for alerting keeps track of who should be notified if there's you might assign someone to be the person for the day. If there's a downtime, they And a social media app like Reddit might be using a large combination of things so Elasticsearch LogStash, Kibana is a popular pingdom will check whether certain launchdarkly lets you add feature flags. So some group of users or not, should the new landing in terraform, lets you automate the deployment things that need to run on the servers, terraform ensure that the right things And the conclusion of all of this is that DevOps cognizant of its three pillars, customers will you know, things will go down, things And so it's really important to keep the engineering organization, or if you're being need to automate very much. However, as the it's more and more important to automate DevOps Very good morning to code review automation. Let's vital baseline information for when we talk about automation topic. So test driven development is before the code is written. And, you know, we're terms of coffeemakers. So enjoy this picture test driven development spun around for a long the idea is simple, but it requires knowledge of So, historically, common words in software and unit test have roots in factories building building coffeemakers, you would test that So unit tests, ensure individual Does the heater work? Does the tank hold components work together? Does the system end to end tests? ensure Does the coffee maker brew a cup of coffee acceptance tests after being launched, sent to they confused with the button layout or breaking All of these tests have software analogies, it's to diagnose a problem. But it's also useful to Because even if every individual component works water with its heater, that's going to be That's really the idea for testing. But which is the methodology built on top of 10 or 20 years. Most developers that aren't using they'll choose something to work on. Based on our the developers would find something they build it, so they'd write code, and they'd it. So they've read small scripts that made If you're making a function that adds two numbers, four. And that would be a good indication So steps one and three, as it turns out, are essentially codify the specification, what is heat up in five seconds. So write a test for that. so write a test for that, and so on. test driven three to flip this process. So first, developers the tests before reading the code. So they the specification isn't satisfied. And then they wrote in step two are satisfied. So they might coffeemaker succeeded, and then build the cheapest And the end result is the same. So the software specifications. But it's significantly If you write the tests first because you know what which things are important to work on and which So this is a very quick video to discuss about continuous integration, which is really the So we've talked about testing, where developers continues working way off into the future years us into our discussion of ci, which is really in a DevOps context. And ci stands for continuous pushing small changes to a central repository verified by automated computer software that So we've gone over what tests are. So let's Well, ci is really the first step in automating single developer is making a program That developer makes the original program releases Now, imagine that developer has And they go back to the old code, and they I've become a better programmer since a year here. But that's really how development works. they have to read and understand the bad code way to be confident making changes to that legacy ci improves developer speed, because new changes about breaking existing functionality. As long problems in the software are much less likely to automatically. As long as you get those check core features of your application will continue development process? First, let's talk about the development teams use. So first, developers the files that are most current, the ones they'll branch off of it. So they'll make their feature independently of all of the other to the various components. So this feature makes And then on that branch, they'll push it back like GitHub, git lab or Bitbucket. And then be configured on the repository side, it'll run And then the results of those pull request. And the pull request is the it into the central repository that users will and you put it at the end, and all of the And so this commit is now the one and the next time is a deployment, the features And the best part is it doesn't cost you git lab and Bitbucket. Most have generous security and access control, you know, permissions ci providers like layer ci, GitHub actions, git Their ci, you know, is really made for people consider. But if you're really early on in your which ci provider to use. Of course, there's one it's that ci is a vital tool, it's really most pull request automation schemes. Because tests regardless. And so if you don't run the things without realizing that they're breaking And following best practices like feature scale a developer team. With just ci, one to 10 developers. And at some point in other pull request automation topics, like talk a lot about theory. But let's to round out our understanding of Let's look at what setting up ci torey This is the live chat example. It's That's used as a demo repository throughout layer this open source version of slack, we'd like to so that in the pull requests tab, we'd be In particular, let's say a developer In the main website, After you log in the top bar in sidebar purple, perhaps the customer has requested if we asked a developer on our team to make this and edit the color. In this case, there are two colors to change. If the developer opened this pull request, it'd Without a CI system, all we and the description of the commit. So we can see changed these color values. But it's very hard especially hard to understand whether this will especially for changes that are less For this request, if I was asked to review it, developer machine, run the script locally, and the developer to set up a screen sharing session, of these add a lot of friction to the development changes without needing any involvement But continuous integration helps. Continuous comprehensive tests, so that if something it says right in the pull request. Let's close this change for now, and look at And this repository. One of the services is called It contains several configurations. interact with the page with a fake browser. For example, this test enters and then logs in and then ensures this test goes to the message area enters a has actually been submitted that it's With enough end to end tests, you can be reasonably confident continues working. So we'd like to run these tests every To do so we'll have to set up the server to run after every pull To do that, let's set up layers here. For our use cases, it's easy to just We can now install it onto our GitHub repository. And now, it's listed here. This means that we've successfully installed However, nothing will happen yet. we need to set up a configuration start the whole stack and then run Let's do that now. Because our Let's use the Docker compose Here, we're going to install Docker, which more about containers versus virtual Install Docker compose, which is again a way of these these concepts will become We copy the repository files into the test runner. We build all of the services, we start all of Let's skip the blank for now we'll talk about that And after all the services Luckily, I've already pre set up a script So to recap, what this configuration will do is install the necessary software, in copy the repository files, start them all locally within the test So now that we've installed all we have to do is add this configuration, So let's click Add File, we'll name it layer file. This is and other ci providers will have we'll copy our configuration. And we'll commit the file. So now that we've set up ci, we can see that And that dot turns into a checkmark This means that every time a developer pushes look at a success metric. Namely, whether they won't have to run the test themselves. original developer has actually So let's go back to our original in production from blue to purple. Here, we're going to make our But because we've configured a CI provider are running automatically directly Now, when our developer asks us for a review, whether the change has negatively affected our configured Cypress and later ci to check that know that for this change, even though many files work, which gives us a degree of confidence that So we can look at the file change for our And then we can view what the CI is doing. we'll see that the tests are in progress of been built and started within the CI runner. it's tested that you can post chat messages the landing page loads, and So now within our pull request view, we'll be that all of the relevant ci checks have passed. other source code management platforms that automate that all ci checks must pass before developers are never reviewing code that's so And you don't only have to run end to end tests other versions of tests, which we talked about I'm happy with the change, I've reviewed I can merge it with a great deal more confidence That's it for setting up ci and an applied setting. Let's get back to Continuing on the topic of testing let's talk about code coverage. So code coverage The tests for a code base are, you might think common bugs and to really check all of the put a number on it. Unless you're measuring graph looks like from a popular tool. and the color represents how many tests are of the file is tested. And bright red means be a priority of a file that should either So let's say you're taking it's relatively large at 100,000 lines of code. Over the years, it's been adopted by a couple and add features without harming those users. So which we discussed earlier. But they weren't So there's a mismatch of libraries and naming which tests are testing which files And before you write any new features, how sensitive certain parts of the codebase are to tests, you'll be much less scared to make changes than if there's a part of the code that doesn't shines. You've got a complicated code base that tests are written so that things aren't broken code of this whole series, let's look at this So this is a very simple It takes a number. And it defines a few variables. It loops up to And then every 50 elements, it pushes So this whole function is 10 lines of really, there's three kinds of lines in a like these closing ones that don't actually constructs for the programmers benefit. make sense to test these because how would There's logic lines like this one, which I mean that these lines, if you remove them, And those branch lines like this one, So for loops, and if statements in programming are commands that run, so this if would run this line. And if it didn't evaluate to the three kinds of lines are syntactical ones that effects, and the branch ones that And code coverage is usually defined of the non syntax lines which are executed by So again, consider this test. If you expect the function and you manually calculate what the this would be a unit test for your function. But since you're only executing it on the input of at least 50 to execute, wouldn't run. So you'd it would execute and this line, which five out of six lines, and the deed that would be 83% test us most of the way to understanding our related concept is called branch coverage. So measures groups of lines, in our example, the main branch. There's the body of the for The if statement body. So here at this line will only execute if i is less than n. so here or these lines to execute. And this line will And so branch coverage would be how many evaluated to true by a test. So you'd like are tested. And this is useful because if this will always execute. So treating them both as really mean as much as taking the bodies of these And if you measured the test with branch coverage, you'd see the two of the three branches So when should you care about this line coverage scenario where you've inherited an existing code situations. In general, you should measure the following are true. Your product has users. by bugs, in which case, it's important to work with your team to improve the code coverage with developers that aren't immediately that you're bringing them into your code base, like form up internship. So they can immediately And you'd like them to be able to make changes Or, if you're working on a very large code Your code coverage analysis can complement test previous talk, to make sure that everyone on the and that the things they make So it's a common mistake in code review automation enough users, if you force developers to get unit tests for every function, it's going to that users will actually notice. Remember So the only thing that users care about is the or if you have a product that doesn't have very it to measure or optimized for branch coverage. And by writing unit tests, and other types of developers are solidifying the implementations If you build a feature, and it ends up not it's always a better idea to throw out if a developer builds a feature, and writes many feature, they'll be much more likely not to throw And they'll feel a sense of sunk cost in having So it's important not to over optimize for it's a subjective idea. But really, you'll notice So organizationally, there are some The first one is useful when you inherit a code not decrease. This one is one of the easiest ones taking over an existing code base, as I mentioned. never decrease. If the current code has 75% of 40 lines of code, at least 30 of those lines code coverage would be less than 75% 30 out of 40. As with most code coverage policies, this will because things will be better tested at the will have to make some complicated tests, testing infrastructure. So features will be policy. And if you enforce it unfortunate side effect of this policy is such as integrations will be less So developers are incentivized by ship features quickly to make many features harder to test, because they require internet those features will be harder to make and less likely to make them regardless of So it might be useful to have an exemptions policy If your organization decides to go for this Another useful policy is code owners for test to keep code well tested, it's often beneficial to means that developers can change implementation changes. So the tests define what success means the tests for a new implementation would need in GitHub, with an engineering manager, a GitHub that spec dot j s is a common JavaScript testing, username means if, if there's a file called code manager will need to approve any change, which So if you're working in a large code base or if you're hiring interns, or contractors, or if you're afraid that they'll have a bad experience, it might be in your team's best interest to And at the time of writing, these the open source world code coverage, So we've talked about testing, and we've talked really like the initial things that are set up But the problem is that it requires the developers are probably busy building make tests or improve test coverage. which is something that approximates testing, but linters are programs that look at a program source common feature of pull request automation, because production, obvious here in quotes. So an example program, the very simple one, you should it defines a variable var x equals five, after the open bracket, which is generally second variable and defines it with the same name you know, wouldn't be called good code, a code And then it says while x is less than 100, on this line, and it messes up the end Det. These and then it closes the function. Finally, you x isn't incremented in the body of the while without running any environment or looking tell that this loop will run forever, and that's So much of this feedback could be automated, a set a variable in an inner scope that has the could be applied to each proposed change, so that leaving code style comments. tools that relevantly, another class of code review It's easy for coder bureaus to waste time pointing camel case versus pothole case. These you know, your customers don't care what case serve to cause resentment and missed If a review takes an extra couple hours couple hours that the programmer could have been So engineering organizations should eventually But in most cases, just starting with which is open source and available at These guides often come with linter Stay stylistically similar, and some programming style guides and automation, like Pep make it easy for developers using those an organizational thing you can do for code Instead of blocking at the code review stage, if feedback, it might be better for code reviewers So they'd say knit full colon This is great. Because it allows the reviewers to merge something with a few pieces of feedback a small piece of refactoring that could be Once the style guide is adopted, it's possible to follow the Style Guide, which tools are called which we use at layer ci, a command such as are the one that comes with go to clean up So we'd use the Janu, find command, and we'd exec go format on them. And this format them with their style guides And of course, if your ci system is running tests the code could be automatically limited as a human reviewer to tell them whether the code is it's cheap and convenient to run linting and So an easy solution is you get another it's a good start to make lint act the same as code isn't listed properly, and then the developer needing to talk to another human or wasting their And a CI configuration that might look like this. So copy the project files, run the linting the the whole pipeline would fail. This approach the linter is a perfectly reasonable response to automation like this can improve the development reviewers from having to give style feedback at like it passes all of the linters the reviewer might still leave some feedback for commits getting to production because of small A better long term solution is which is common idea that happens all over in this specific example, it might look like run, yes, lint with the dash dash fix flag, which again, goes through all of the source So this would go through all of the source files. rules, and it would fix any stylistic an additional, you know set of file changes And it would create a new branch with a suffix So that developer pushes unlimited code, the bot everything and it would create a new branch so the reviewer could simply merge the Linton And then we failed the pipeline with lint can't be merged. But the limited one, assuming could be merged. So we'd have two branches, the the code reviewer would look at the one that or not, like the logic of the commit was good. could merge this branch instead of This branch would be the same as the original So some examples of limiters JavaScript the standard as a 2021 is Eastlands. piland and flake eight. c++ is much more CPP lens from the Google style guide mentioned which acts somewhat like a linter. Although beyond that, Java has checkstyle and find bugs, for languages like Java. And Ruby has broke and Java, JavaScript C sharp, and many other which is a popular static analysis larger enterprises. But it has an open source 10 developer teams that would And finally, it was a startup called startup to startup. And they're doing all sorts Static Analysis is just the practice of and finding bugs. So I'd encourage So in comparison to most other code automation Any team with more than one developer To catch obvious bugs like infinite loops. Just able to tell you whether there was a common Automatic linting comes standard has many code how to configure their code editors to use has set up in the CI automation, so that the code to get this feedback, they can get the And then teams working on earlier products. Instead of relying on a test a static analysis to find common bugs infinite loops or stylistic problems. has many users get feedback without Right. So that's it for linting and Let's finish up our discussion of about ephemeral environments, which are comes to doing code reviews and helping ephemeral environments are temporary environments entire application. Generally, for every feature or automatically on every commit using DevOps Temporary environments are overtaking traditional review experience. Because these environments not just developers, but the product, people in that set up a developer environment or asking to So for a more concrete example, let's on a website. So they're changing, you know, some component of the website. And they'd So a code reviewer would look at the the visual ramifications of that change within the code review view itself, the reviewer I'll zoom in. So within GitHub, this is what the the code change, but also a button to view the button, it wakes up a version of the website so that the reviewer can actually see whether the changes is visually In general, ephemeral environments environments and staging environments. replaced by formal environments in Benefits of ephemeral environments. an ephemeral environment workflow is that is that Developers can review the results of changes feedback on the code change itself. Additionally, collaborators such as designers as easily So You could post a slack message like this give me feedback instead of needing to to get the other person to The hardest part of setting up ephemeral So dealing with things like by their nature, you know, ephemeral They're isolated from production environments, and A reviewer should be able to delete a resource know, deleting a user still works without fear So in the early implementation of ephemeral API servers with read only permissions to a might have an Iam role that has read only access able to sign up to the service, for example, The end goal should be to have a fresh So every time a developer the new database specifically for their an ideal ephemeral database has so it contains representative anonymized data. identifiable information must be scrubbed It should be undoable. So if in the course of the database to its original state. This is also which we'll get into later. the database should use the schema currently know if something's working with an old version of problems uncovered by formal environments or Another hard problem to solve with ephemeral create them and when you destroy them? The classic to the lifecycle of an ephemeral environments. So environment for them, keep it running 24. Seven The biggest factor to consider there is cost. So it's 10% cheaper, and you have 30 open pull So you know, that's an expensive developer tool. Another approach is to create a chat ops bot that branch with a specific timeout. So for example, issue description that could create an environment This requires the environment to be provisioned And it's again, hard to tell when to delete these. The best approach is to create an So similar to the PR workflow, but There's only a few providers that do this. So one turn on and off environments. And the other So as users use the environments, and layer You can automate this yourself with memory this might be something that's better And back to that idea of continuous staging. The idea is to merge staging ephemeral is kind of layer ci itself primarily sells to our powerful and easier to create. They approach and integration pipelines. So if you can set up then it's relatively easy to run tests because entire back end. At its logical conclusion, this and ephemeral environments form a single of the requirements for everything. And then that but also how the review environment but also the If you're going to make them about a month, or a month of time So if your production environment has many databases, it'll be relatively difficult Large companies like Facebook have set But they haven't hired developer teams, So if you're a smaller company, you might want to instead of making it yourself up to And to avoid having to micromanage it's easiest to use a hosted provider. If Some popular choices are for cell netlify. But the only choices available right now are layer options available in many source code platforms But it's not really truly an ephemeral explore all of those options So that was ephemeral environments. code review automation. Because pull request automation is let's do another applied tutorial here. In this example, we'll be setting up we talked about before using a hosted So because we've already set up ci for this which is our CI configuration. However, many ci can set up ephemeral environments, which to evaluate the changes live as a reviewer. Again, from blue to purple. And we'd like someone to be looking at the test results, but also by looking you might see, in this case, it's actually very Let's create a new file. Here, we'll make another layer file so that they run in parallel. And we'll we'll say expose website. And we'll expose the website running inside the directive, but many other providers have And let's jump right to So here, our code reviewer would as you can see, those are Let's look at the actual graph to understand better what's going on. Here The main layer file has again built all of And now it's running our Cypress tests But after the Cypress tests run, we'll have a the first and that second environment will have So let's see that here. The snapshot is done being taken of the tests, which means that the ephemeral And now you can see that it's built a staging So in our actual pull request, now that all we can click the femoral environment button. As soon as it appears. We could click the main layer file details, we can click the services and we can click View website. And pipeline which we initially set up to run our to the web server inside. So here specifically for this test, and we can see And we can evaluate this change so we can And that in the test channel, it's This means that you don't need 100% test coverage For every pull request, you'll be able to and then wake up that environment And now that we're satisfied with the environment And from now on, all changes which edit so the reviewer can check that things work, manager might be able to check that supposed to do. That's it for formal Welcome to DevOps Academy deployments. And this And primarily, when you talk about And you're talking about containers. And So let's talk about the difference between those People talk about DevOps deployments, they're a large portion of all And containers are really only defined in So with all that in mind, let's So what Linux really helps you do is take care it takes care of memory. So programs need memory And since you only have a finite amount of it, will get which sections of memory, so which ram Linux also takes care of processors. So Linux will make sure that the right amount If you're, if you've ever run very computationally your browser gets laggy. That's because So if you're running production that every program is getting its fair share Because disk, so Linux takes the files of all you might have multiple disks, and you might you might even have disks shared across networks. that the right files are on the right disks, And finally, there's devices. So there's things like GPU. So for and things like network cards, which Linux needs to take these individual resources five processes trying to connect to the internet Linux needs to make sure that the right upstream. And the responses are sent So in a diagram, this is So here we have three programs, Chrome, running in Linux. So this is assuming you have And here you have the four shared resources. So some of the CPU time to Chrome. And it'll also And similarly for all the other shared resources. So this is great, but there's too much programs know about each other. So if you home calling file dot txt, but another program could delete or read that that means those programs can communicate between So, for example, programmers often use a popular programming language. And One is Python two, and one is Python three, but user bin Python is a Python two executable, and then that program would error because you'd be However, some of your programs might need Python So here, there's cross talk between programs in they expect there to be different files there. So They need different versions And so you can't really run Similarly, two web servers might listen to connect to them. So if you're running to be open, the first one will start saying that Port 80 is already used these is really where virtual machines and containers files and ports between programs so that So here, if you are running your it would look remarkably similar. but it would be running within a container. which would then allocate the container Now, this might not make sense yet. But let's talk in a container like this. So what happens when The big change is that each version of shared resources The container running chrome might create While the container running notepad could they get different copies so that they couldn't talk amongst each Similarly, if you had two web servers running would be able to open port 80 in their container, their container. So you can have two programs listening on port 80. But really, they'd So in Linux containers work by creating that groups shared resources together. If within a Docker container, they'd still would not see the other processes, the So within the container, if you ran PS, au x, and VSD UX is how you see the running commands that means that there was 10 processes visible so within the container up here, if you said how 10 processes. But within Linux itself, you'd see from this container. So the containers are kind processes where the processes can't see the files network ports outside of the container. They So essentially, what's happening is the programs Python, in our example before and instead of the contents of another file. So the container if you were using Docker for your containers would Docker overlay Fs one user lib Python, which is So each container would have This little deception allows programs to run in with different files for each container. One two executable, and one container could have So if that's how containers work, then Well, VMs are very similar to someone running an older video game on So the idea for containers Within the container they don't really container. They see files, but the files within the real Linux installation. The one level below that. So pretty produce fake The VM equivalent of Docker It's the program which is in charge of creating it corresponds to an instance The hypervisor might lie to the VM and say attached and then has 50 gigabytes of capacity. it would instead go to a file, it wouldn't go to This file. So when the VM itself is writing to which is very similar to the deception of had. But there's some practical differences. The them to run other operating systems, such as configurations, you can emulate a gamecube, in containers, it's the processes that are being thing that would run within Linux itself. But generally doesn't know it's not talking to the drive, for example, those rights are sent to So when the process writes to the that operating system sends the right to but that drive goes through Linux, Various benchmarks show that CPUs and VMs are usually use 50 to 100% more storage, because they would need. Duplicate containers don't need all of And finally, VMs use about 200 megabytes more containers don't need all of the operating system So VMs use more memory, they're So given these performance benefits, it looks And in most cases they are However, there's a few cases where VMs are a better If you run in untrusted, so user supplied they can't escape a container. This but it's long been a contentious point. Virtual if you're running untrusted code, usually it's If you're running a Windows or Mac OS script, another operating system, you'd need to use a an old video game that doesn't run in Linux, you'd need to VM. And vice versa, if you're if you'd like to run a Linux program in Windows, And finally, you can emulate hardware So if you're testing that your graphics card that it would give and then test that the So that's the big difference And these are really the two So let's go into actual deployment strategies Let's keep talking about deployments. So rolling strategies. And we'll talk about the pros and cons section. Rolling deployments themselves work sending traffic to the new version, to make sure version and repeating that until all versions of I realize I've said version many times. So let's look at pictures that will This is the myrn app. myrn stands for the user's web browser connects to where front end is the stuff that the user provide connections to the database. So if If you're just viewing the landing page, Let's say your app has enough traffic that users How would you push a new version of the This is where rolling deployments come in. The looks like this. So you create an instance of until it's up. So you keep trying to connect to And then you delete an old version and If any instances of the old version still And our myrn example, we'd initially version and one instance of the new Until we had three instances of the new version So here, all of the versions are the back end. And we add a new version of the new back end. And keep repeating that. So as time goes on, the red loops of this, the only ones remaining are red added the red one, remove the pink one. And we're So what are the benefits of rolling deployments well supported. Rolling deployments are relatively they're natively supported. And several for example, Kubernetes helps you with also supports rolling deployments. They don't have huge bursts. So in another if you had three versions of the back end, you version, and then you turn off the old three. the amount of things running, which might of servers, for example. It's also not to limit the amount of connections. So if you the database, now that might be too much load And really, deployments are easily reverted. If in the course of an upgrade, you notice the rolling deployment by just going in the you can go in the opposite direction as characteristic of deploying The downsides of rolling deployments are they and you're replacing one at a time, and it to replace all of the versions, which This can be mitigated by increasing the number of which is sometimes called a burst The other problem is API compatibility, which So if you add a new version of an API endpoint to then since you're not switching them both at your back end serving a request for version two of so there'd be errors visible to the user for mitigated with complicated routing techniques, compatible. So make version two of the front end So rolling deployments are relatively simple If your users mind when there's to deploy using a rolling deployment strategy. that services can consume both the old version contract was violated, users might see errors talk more about deployment strategies. Another deployment strategy people To set up a bluegreen. Deployment teams consistently deployed, and which services will I'll explain a little bit more of what I mean by be a shared resource, multiple versions of the and a standard deployment would generally In our mern example, all of the new versions of them would be So in a bluegreen deployment strategy, where this is what that would look like. So there's application where each is a fully standalone database, and the database is not part of blue bluegreen deployments are so called because they one named green out of convention. If the current we deploy the new version to green to ensure that the new version of the app After we're confident that the new we'd move over production load from blue to green, and then repeat the cycle in We started with the users being sent to blue, then we're investigating that version two works. will route users to version two. And can shut it off and replace it with version three, make sure it works, and then switch user traffic side, bluegreen deployments are conceptually you just have to create two identical production the other, which is relatively simple with They're also quite powerful, longer running in the old version of the application after So if a user has an established and you've switched everyone else over to blue, it was doing. So if you're watching video, and which might take minutes that can continue bluegreen deployments can be extended to many There's a few notable drawbacks to bluegreen for example, to revert a change, because the tasks and unavailable to switch to. So if you switch over to version two, you realize push version three very quickly, which addresses only place you could deploy version three. So you transfer load between the clusters, if resources and load is transferred all at once the allocated to serve the surge of requests, because And finally, if one cluster modifies the table in the database, it may affect the other So here's some common extensions to they're very extensible. And many teams set up to improve stability and deployment velocity. The deployments, which I call rainbow deployments, but Instead of only having two clusters, some so blue, green, red, yellow, so on. This is useful If you're working on a distributed web scraper, you might need your clusters to to ensure things continue working as you'd keep all of the clusters that are doing something like video encoding for long that's in the middle of encoding a long video In a regular deployment of clusters of their long running jobs are done processing. Some teams rely heavily on manual QA and often building desktop or mobile apps, which So if example.com is being routed to the blue a new version of the application to the green with this setup, the new version of the app could very environment that will soon become production. because they're happening in production with access to the code base. So for a game, you might the new release available and have your QA testers ahead, you can point the game client to the new another useful add on to bluegreen deployments. canary deployment. So if the new version of your the UI, it might be ill advised to push them of users. So if even 1% of their users complained of feedback. The changes may break users workflows to user feedback. So in the context of a bluegreen extension which routes maybe 5% of user traffic to those users don't have negative feedback before was version one green was version two, we'd have We'd wait to see if anyone on version two to version two. And we'd shut off version So bluegreen deployments are powerful that works well with teams that With strategy only really starts being where there's many services being Alright, let's keep talking about deployment. continuous deployment can sound daunting, but in many cases. Let's take a look back at So, in our README, we've helpfully added this little line, which is If we look at our hosted version, slug, which is is still purple, despite having changed The reason that it's still purple is we haven't pushed the new version of And oftentimes, requiring human intervention product skill. So to deploy, let's run and then let's talk about how to automate So here, we'll use a terminal and simply This developer computer comes with otherwise it would be difficult to which needed the ability to Here we can see that it's we'll talk more about how to Now, if we refresh the page, we can see that the deployment has which is blue, so it's picked up the color So continuous deployment, we'd like it to run on merges to the main branch, we don't before they've been reviewed. For that, We could write this configuration but let's write it in the API directory for now. So we'll create another layer file will inherit from the testing layer file to make sure that the deployment And will only run the deployment However, if it is the main branch, and use that secret for our SSH key and Let's do that now. And then hopefully gives us the What's appose? For now we're exposing the SSH key which is within the CI process itself. One other thing we need to do is change This is required for SSH, but it might not So now that we have our SSH key within the CI are passed, all we have to do is copy our command All this configuration does is wait for tests to and then use the SSH key to deploy Let's create a new pull request with We can see that as before the ephemeral built. But this API service is also being contains our continuous deployment process. actually looks like Instructure. so here we can see that the application has been built successfully, and is being started, just as in their running our continuous integration, but not our And we can see the tests are running. As usual, the test running process requires starting a fake browser. And then after the tests pass, we'll see that the deployment process runs. So the graph, these are usually called So here, we can see that the step was skipped because which is exactly what we wanted. However, we'll create a new merge branch on the main Here. And because this is the main branch, the deployment process itself will be running. We're simply loading the environment And here, we can see that the deployment needing to run it as an individual developer, And this idea of deploying automatically from So let's work through that whole process clear on the deployment automation side of things. So let's change the color, again, for the it's visible if a change gets pushed correctly. And again, we'll change the two colors. And we'll create a new pull request. And now our reviewer will have a lot of good or not. So the reviewer will be So they'll see all we've we'll be able to look at the CI process itself. So they'll be able to see that tests are running. So in particular, that the application they'll be able to look at within minutes of me creating this new change. And if they approve the change, it'll be shown process will only take about a minute, with the One by one, these steps should become green. Again, this is the base. This is the continuous deployment process. And this is the administrators of GitHub might mark as can the commit be merged and shown to users. so take a look at the ephemeral environment just to We can see that from the thermal rose ish red, perhaps this is the color is correct. And the test has passed and that the functionality of the application After we merge it be the end to end test deploy If we take a look at that, we'll see that because we the deployment is already running. in And in a short period of time, the production our application running on it. So here And the snapshot is being taken. So everything is our website, it is now the shade of red that ci CD, ephemeral environment pipeline Alright, so let's talk more about talked about deployment strategies. But that's strategies help you reduce downtime and deploy But another key consideration for deployment is your containers or VMs. So that if there's a large So let's say you're building a CI system, this hits close to home because I have your users would push code, you'd against that code. And you'd see bursts of And you'd see significantly less For a peak load of 10,000 concurrency runs, However, at night, outside of peak hours, you wouldn't really need all 10,000 So your usage might look like this, which your lowest point is maybe 500. Runners required, So you need 20 times more workers from the highest In an ideal world, you'd be able to create or hours, you'd be able to create new ones, and then That's the idea for auto scaling. It's only possible to create and destroy At their enormous scale, it's possible hour leases. The most popular technology is AWS easy to spot instances, which act exactly you provision them for short periods of time. is Kubernetes horizontal pod auto scaling, which Kubernetes out of the box, you can just assume you'll get auto scaling if you configure if you're using Microsoft's as your as your cloud VMs. And for containers. If you're using AWS, And if you're using Google Cloud, there's scaling is usually discussed on the timeline concept of auto scaling and took it to its limit, quickly started and use them on the timeline For example, a web server might not need to Instead, it could be spun up specifically for that That's exactly the idea for serverless. provisioning resources as the required and doing serverless is primarily used for services you wouldn't run something like a CI job But you might run something like a auto scaling is primarily used for services likely run a CI job within an auto scaled VM or As a 2021, the distinction between the models is becoming popular. And they often run for upwards containers, but they're created and turned off in Within a few years, it's likely will converge into a single unified interface. So I'm excited about that. That's that's And that ends our discussion of auto scaling Another key concept in deployment A database might be at one IP address, so 10 dot while the web server would be at And they'd have to discover each other, because and the database might have those get even more complicated as you add more services. Again, let's consider the myrn app So you have a web browser, the user themselves is front end, and they're connecting And your back end is connecting to a database. Here, there's three services that need to be example.com corresponds to the front end, back end. And the back end needs to land on the So the backend needs to know the IP address and know the IP address and port of the backend and everything is manually configured the back end names within DNS Domain Name System, it's the internet. And the back end is configured So your DNS configuration, this is the CloudFlare would look like this. So if the user visits And if they visit api.example.com, So this is all manually configured. And we've And then within the backend, we'd which is a dictionary of key value pairs that you'd say connect to the environment and then connect to Port 27017, And then when you're starting the backend, you just have to specify the IP This configuration is completely fine. For It's relatively secure, and it doesn't over simple configuration. Most products could launch But you'll know that you need to start caring you see one of the following. So you need zero like this if you want to do rolling deployments. arrows point to, you can't automatically change deployment strategy. For example, if you have get hard to remember where they all are. And if if you have a developer environment, a staging production environments that all have different set the IP addresses all over the place. So let's illustrative of the broader problem. Before which are another crucial system design and deployment is simple. As we've seen, you start wait till they're up, and then you shut off the happens in both rolling and bluegreen deployments. in DNS itself. If our rolling deployments that wouldn't work very well, for various time to propagate users in other countries than to see the new IP address, and they'd still The solution is to add a web server that acts we'd be able to change where it points to So web servers like these are called crucial for setting up zero downtime So taking our myrn app and adding this level instead connect to the reverse proxy. So the user The DNS system would respond with, oh, it's here, the IP address of the reverse proxy. And then and send it to the appropriate, they view on what the user asked to connect to. And then So if you're running a deployments, the proxy could choose which of v1 or v2 users request to, and that would just And then after your deployment is done, you proxy could route traffic entirely diversion service IPS in a hash table. So implicitly, we assumed that our reverse proxy would be able which is exactly the statements of service tell our reverse proxy where the front IP address of version two of our back end, it When the new versions come online, they can update the value for the key back end and And then the reverse proxy could watch for changes For a very concrete example, which is about set of videos, let's look nginx is a very popular reverse proxy. And it's And it lets you define where various host names reverse proxy, again, in the picture, this connecting to your website, but they'd for example, COMM And nginx would take their So nginx just has to learn where And that's what this configuration would take this key from this file, and then use conf D, which reads from a hash and then send the user there. So all you need is a key value store, which has a front end version. And then to run your rolling check that it was alive, and then you just change And then constantly would pick up that of version two of the front which would change the arrow to the That was a lot to deal with. So let's back up a bit, all you'd need to set the key in the hash table for and then make your back end do the And that way, when the new version of the in the table. And then nginx would start routing This is what proxy passing means and nginx. So you see this proxy pass directive. It's just an illustrative point of if you were The most common thing used in industry So DNS, we thought of before as the to propagate changes across the And that's the industry standard. So let's talk about DNS a little bit. The idea for DNS is just to map host names to the global DNS system will first map at the time of this video 104 dot 217 9.86 and computers connected to the internet. And you where those addresses are. So this is saying for And usually when people mentioned So visiting websites on the internet. However, as It would be ideal if in our nginx configuration, end and then have front end resolve That way we wouldn't have to change That's exactly how DNS based you configure your services to query a server So instead of saying MongoDB, full colon MongoDB full colon slash slash in the key value pairs in Of course, it's not trivial to deploy though there are popular options like core DNS, the most likely thing you do is use a cloud So the end result you'd the user's web browser would connect to nginx. the DNS provider, where is the API right this is the IP address, given the deployments, this is what we currently want users to visit this would correspond to version And then the proxy would the request to be fulfilled, and then we go So the conclusion of all of this is that service foundational building block for these deployment general, if you configure service discovery so DNS based in a Kubernetes cluster, for example, have microservices that talk to each other, connect a MongoDB app, and then deal with where to MongoDB at MongoDB colon slash slash Mongo. configure where that Mongo always points to the the application logic from the deployment team build faster, and you'll be able to deploy Let's go on to the next and final pillar, There aren't that many general topics so this section will be a little bit shorter. in the DevOps Academy. But just let's talk about two core concepts. And it's a way of collecting and tagging into a single dashboard that can easily be built out in an application performance management application performance management is the part built and deployed. And you need to make so they have enough resources allocated to In most production deployments, there are many Google, a single search might hit five different got unexpected search results, that might mean And log aggregation helps companies like they built a single dashboard where they can something, your search will get a unique ID through a different service, that service will This is the essence of a good efficiently collect logs from everywhere that In the case of a fault. Again, this is our a back end and the front end, and the If the user told us, the page turned we would be hard pressed to diagnose the user would need to manually send us the logs in the other three services. Let's log aggregation stack named after its three If we installed it in our burn app, we'd get three would connect to our front end and back end. The services, the browser, the front end, the back And then the way that these three components work, the components of ALK Elasticsearch Log Services send logs to LogStash. LogStash the application. For example, the web browser. log this visitor access this page at this Those logs would be sent to LogStash, which would user did thing a time, it would extract the time include those all as tags. So the message that you could search them easily. You could say, But LogStash doesn't store things itself. which is efficient database for querying text. and cabana is a web server that connects as the DevOps person or other people view the logs in production So you as the administrator would connect to logs matching whatever you wanted. You could say, errors. And cabana would say Elastic Search error. And then Elasticsearch would return and LogStash would have been samples If you visited a web page, this might And it might be processed into an object like time format. That's the same for all messages service which service submitted the log, and you'd And the processor, LogStash itself would often in the browser can catch errors and send them to like century that might be better suited a production problem? Well, let's say a user says with elk setup, we'd have to go to cabana, enter that would show us the logs that corresponded server error returning 1234567. And we'd see that and we'd see what time that blog was time in that log. And we could look in the backend. And then we could see a better And we'd be able to repeat this process what actually caused the problem for the user. The final piece of the puzzle is ensuring As logs can contain sensitive information like users can access them. You wouldn't want to way of authenticating. My favorite way of doing our friend nginx and then have the auth request So in our back end, we could which simply returns a successful status. If and there an admin, it would return a successful an unauthorized status. And then we could videos, to have these location blocks, the slash And then we could make sure that if this was slash because with this auth request directive, if administrator, they wouldn't be able to access by a company called elastic and they have a paid which facilitates this as well. So you can go for or the paid version of the application. As an aside, you can use log So in your ci pipelines, where you you can repurpose your log aggregation stack while the tests run. If your end to end test you're starting your logging stack. You're add an extra step which queries Elastic Search for There are no logs, that printed error. if there's an error going on, that error might So this adds a free extra check to your ci stack. And there's a few examples of log aggregation Kibana, which we talked about, there's fluent D data dog, which is very commonly used at larger log DNA, which is another hosted offering. facilities like AWS, cloudwatch logs. So log in production. It's relatively simple to install and it makes diagnosing and triage problems That's it for log aggregation. Plus topic we're going to talk metrics are simply data points that tell can see on the screen, things like CPU usage, are all important production metrics that you first tool to set up for production monitoring, both indispensable for finding production faults, Log aggregation primarily deals with text, metric aggregation deals with numbers. How It's frighteningly difficult to understand Netflix, for example, measures to monitor the health of their production being able to automatically notify the necessary Let's keep looking at open source implementations Prometheus is a tool originally deployed at servers. And this is what it looks like. Similarly things like nodes would send how much disk usage long services are taking ALC itself would parse And then promethease figures out what get services video. And then it takes those and it stores it what Elasticsearch is for text, and then that And then finally, there's a front end. So other thing you might want to do is if there's something you might want to connect a pager duty message with Twilio, beyond call engineer But you might also want to query And that's what prom qL is used for. So grafana And it's common way of viewing these and you can make API's. And there's many The diagram above is daunting, but it's quite for log aggregation frameworks. There's four key database actually stores the measurements, So the sorts of metrics we collect. Well, metrics are important based on what your But here's a few ideas for what you So request fulfillment times, these are very getting overloaded, or if a newly pushed The format times are often parsed out of or taken out of a field in a database. For a website or REST API, a common request for web sites and rest API's. A common request That way slow web pages could be A related metric that is very indicative of a huge spike in requests per second, it's systems will have trouble scaling. Watching mitigate attacks like denial of service attacks, which are when attackers sent many malicious The last common metric across many Here's a few examples. So the database size terabytes of disk for your database and your 1.5 increase the amount of disk available for the web server memory. So if your web server doing a lot of processing, it might require it would crash and your users wouldn't Network throughput. So if you're downloading you can saturate your network. And that And a final one is TLS certificate expiry TLS certificates to see whether the browser internally. And these cause problems if they're So for example, Google Voice had an outage in when their TLS certificates would expire. So production faults very rarely look There are often a gradual ramp, and then eventually everything breaks. portal statistics into something actionable. A website fully load their landing page to notice issue. So with cuartel analysis, you'd split How long did the slowest 1% of users take? How long did the slowest 25% of users take. are logged in and logged out, just by visiting that the web page is very slow. But users of requests bucket, and you'd see that those Or when example stackoverflow.com itself was page was taking a long time to respond was published to stack overflow. For metrics There's Prometheus and grafana. As we not only log aggregation, but metrics aggregation maybe the old reliable option. And there's of this. There's AWS cloudwatch metrics, Google That's it for application performance