Ninety percent of the world's data was generated the amount of data being generated exploded social media, and various digital themselves facing a massive volume of To address this challenge, the Big Data refers to extremely large and complex traditional methods. Organizations across the of data and derive useful insights from it. In 2006, a group of engineers at Yahoo developed were inspired by Google's MapReduce and Google way of data processing called distributed machine, we can use multiple computers to get each machine in a cluster will get some part of on all of this data, and in the end, we will There are two main key components of Hadoop. which is like the giant storage system for into multiple chunks and stores all of this part of Hadoop is called MapReduce, which is a together. MapReduce helps in processing all your data into multiple chunks and process them to solve a very large puzzle. Each person in and in the end, we put everything So, with Hadoop, we have two things: which is used for storing our data across multiple all of this data in parallel. It allowed volumes of data. But here's the thing, although there were a few limitations. One of the relied on storing data on disk, which made it would store the data onto the and then store that data again through a disk. Another issue with Hadoop was that it processed for one process to complete before submitting group of friends to complete their puzzles So, there was a need to process all of this Apache Spark comes into the picture. In 2009, Berkeley, developed Apache Spark as behind the development of Apache Spark was is where they introduced the powerful concept RDD is the backbone of Apache Spark. It allows data access and processing. Instead of reading Spark processes the entire data in just memory. Access Memory) stored inside our computer. And 100 times faster than Hadoop. Yes, you heard it Spark also gave the ability to write code in Java, and Scala. So, you can applications in your preferred language Apache Spark became very famous because and process it efficiently. Here are the different most important parts of the Spark ecosystem is across multiple computers and ensures everything Spark SQL. So, if you want to write SQL queries using Spark. Then there is Spark Streaming. If in Google Maps or Uber, you can easily do that have MLlib. MLlib is used for training large-scale With all of these components working together, Apache Spark became a powerful tool for in any company, you will see Apache Now, let's understand the basic architecture a standalone computer is generally or anything else. But when you you can't do that on a single computer. You need tasks so that you can combine the output at just take ten computers and start processing coordinate work across all of these different Apache Spark manages and coordinates a cluster of computers. It has something called a it is called a Spark application. Whenever we which grants resources to all applications In a Spark application, we have two processes and the executor processes. and the executor processes are like workers. The of all the information about the Apache Spark input from the user. So, whenever we submit it goes through the Apache Spark application done, divides our work into smaller tasks, and it is basically the boss or a manager who properly. The driver process is the heart of the everything runs smoothly and allocates the provide. Executor processes are the ones that assigned by the driver process and report back Now, let's talk about how Apache Spark executes our code in Apache Spark, the first thing It is basically making the connection with the with any of these languages: Python, Scala, begin writing your Spark application, the first You can perform simple tasks, such by writing just a few lines of code. For one column containing a thousand rows with values you create a data frame. A data frame is simply similar to MS Excel. The concept of a data frame concept available in Python and R. In Python, the in Spark, the data frame is distributed across data is executed in parallel, you need to divide partitioning. You can have a single partition while writing the code. All of these things are basically the instructions that tell Apache Spark For example, let's say you want to find can use the filter transformation function to if we run this code, we will not get the once you run the code, you get the output that. Spark uses lazy evaluation. It waits and then it generates the proper plan allows Spark to calculate your entire To actually execute the transformation There are multiple actions available in Apache which gives us the total number of records and Spark will run the entire transformation Here's an example to understand all of these we need to do is import the Spark session. You can import SparkSession. This creates the entry point you can use the sparkSession.builder.create so that you can import the dataset have all the details available, such Now, let's see if we have this dataset called a simple function called spark.read.csv. If you will print the entire data from the CSV file. As sex, smoker, date, time, and size. All of this print the type of this particular file, you will Now, you can create a temporary view on top createOrReplaceTempView, it will create a table of it. For example, you can run the query SELECT spark.sql, you can easily run this particular SQL did was import the data, convert the data into a The same thing can be done to convert this Spark want to apply any Pandas function, you can also do to understand lazy evaluation, where you are just once we run this particular statement, Spark does action to be performed. The action over here then it will run this entire thing, and This is called a transformation and this is the action that you were talking You can go to the Spark documentation and functions available, and for each function, I hope you understood everything about this code. If you want to do an entire data you can watch the video mentioned in the understanding of how a data engineering That's all from this video. let me know in the comments, and I'll