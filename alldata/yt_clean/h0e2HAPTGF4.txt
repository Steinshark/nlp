The following content is Commons license. Your support will help continue to offer high quality To make a donation or to from hundreds of MIT courses, at ocw.mit.edu. ERIC GRIMSON: OK. Welcome back. You know, it's that we're all kind of doing this. So let me see if I can get a few that two weeks from Should be worth at least a Professor Guttag is smiling. He likes that idea. You're almost there. What are we doing for the We're talking about And I just want to was the idea of I have Case of a spring where I put displacements. And regression was giving to fit that data. And In some cases it was easy. We knew, for example, it was We found the best line In some cases, we said to actually let us explore would fit it, whether a some higher order thing. So we'll be using that to That's a nice segue into lectures, the last big which is machine learning. And I'm going to argue, you can an example of learning. But it has many of want to talk about when we So as always, there's Chapter 22 of the book gives and it will follow And I want to start what we're going to do. And I'm going to as I'm sure you're aware, I've listed just five that all focus on And that doesn't where learning is So natural language processing, computer vision heavily on machine learning. And you'll see those in So we're not going to into three lectures. But what we are going to do We're going to start by talking of machine learning. The idea of having examples, and representing those you measure distances and use the notion and group similar of doing machine learning. And we're going to of two different standard One, we call Example we're is something called and the second class, Classification works we would call labeled data. I know labels on my going to use that to that I can learn, and when I don't have labeled data. And we'll see what that But we're going to give Unless Professor Guttag we're probably not going to sophisticated machine like convolutional neural things you'll read But you're going to behind those, by looking talk about learning algorithms. Before I do it, I want just how prevalent this is. And I'm going to admit I started working in AI in a pretty simple thing to do. And it's been over 40 years, the change. And if you think about it, just AlphaGo, machine learning based a world-class level Go player. Chess has already been conquered Go now belongs to computers. Best Go players in the I'm sure many of Any recommendation Amazon, pick your favorite, uses to suggest things for you. And in fact, you've probably The ads that pop coming from a machine looking at your preferences. Scary thought. Drug discovery, character does character recognition of a machine learning algorithm behind it. You probably don't It's actually an MIT it's a hedge fund in New York. They heavily use AI and And two years ago, their I wish I'd invested in the fund. I don't have the kinds but that's an impressive return. 56% return on your Last year they didn't but they do extremely well using Siri. Another great MIT that does computer vision learning component that is and will be used in It will do things like if you're closing too fast which is going to because I drive And it would be Face recognition. Facebook uses this, do to both detect IBM Watson-- cancer diagnosis. These are all just learning being used everywhere. And it really is. I've only picked nine. So what is it? I'm going to make an You're now used to that. I'm going to claim argue that almost every computer But the level of learning So if you think back to we showed you Newton's method And you could argue, but you could argue something about how to In fact, you could generalize But it really didn't learn. I really had to program it. All right. Think about last week when we Now it starts to feel like a learning algorithm. Because what did we do? We gave you a set mass displacement data points. And then we showed you how fit a curve to that data point. And it was, in some sense, that it could then use In other situations. And that's getting we would like when we learning algorithm. We'd like to have program that something that it can then Now it's been a problem in And I love this quote. It's from a gentleman 1959 is the quote his definition of is the field of study the ability to learn without And I think many he wrote the first such program. It learned from experience. In his case, it played checkers. Kind of shows you how But we started with checkers, But it played checkers. It beat national level it learned to by watching how it did in games to change what it thought Samuel did a bunch I just highlighted one. You may see in a he invented what's called is a really useful But the idea is, how can without being And one way to to think about the difference program and what we would algorithm. Normal programming, I there's such a thing but if you think of what's the process? I write a program that so that it can then some appropriate output. And the square root finder I wrote code for using Newton and then it gave me the I'll give you the square root. But if you think about it was a little different. And in fact, in a machine the idea is that I'm going I'm going to give it examples of labels on data, of different classes of things. And what I want is, given that characterization I wanted that machine to actually produce a program that I can new information about things. And that creates, if you where I can have the learn the program to solve some other problem. That would be really And as I suggested, that is a simple version of that. It learned a model for the use to label any other or predict what I would see in as I changed the masses. So that's the kind of idea If we want to learn ask, so how do you learn? And how should a computer learn? Well, for you as a human, there This is the boring one. This is the old style Memorize facts. Memorize as many facts as you on the final exam as opposed to some other This is, if you think way an example of declarative Memorize as many as you can. Have Wikipedia in Better way to learn is to new information from old. And if you think gets closer to what we ways to deduce new things. Now, in the first that in when we wrote that But what we'd like in is to have much more like We're interested in to write programs that can from implicit So not something like that comparison of but actually implicit and have the algorithm figure and use those to can use to infer new about string it is you're trying to do. OK. So the idea then, that we're going going to give the data, some observations. We did that last time with We're going to then to figure out, how do write a program, a system about the process that And then from able to use that to make we haven't seen before. So again, I want to If you think about it, the I gave you a set of relative to mass displacements. For different masses, how I then inferred something In the first case, I but let me figure out what What's the spring constant And based on that result, I could use to predict So it's got all of those an inference engine, to use that to make But that's a very simple So the more common going to use as is, when I give you those examples have some some features and some labels. For each example, is a particular kind of thing. This other one is And what I want to how to do inference on So it's not just, what's the it's actually a label. And I'm going to use one I'm a big New if you're not, my apologies. But I'm going to use So I'm going to show I'm going to give you a set of The label is the And the data, well, it We're going to use But what we want how would we come up with the implicit pattern of how the kind of position And then come up that will predict the We'll do the draft Where do we want them to play? That's the paradigm. Set of observations, potentially Think about how do we do And then how do we use that What we're going going to see multiple is that that in one of two very broad ways. The first one is called And in that case, I give you as part I have a label on it. I know the kind of thing it is. And what I'm going do I find a rule that would with unseen input based It's supervised because I Second kind, if the obvious other one In that case, I'm just going to But I don't know the labels I'm going to just are the natural ways together into different models. And in some cases, I may know In some cases, I may the best grouping I can find. OK. What I'm going to do today I was expecting cheers for that, Not a lot of code. What I'm going to do the intuitions behind And I&quot;m going to start with my So here are some data points And I've got two I've got receivers, And each one is just labeled by and the weight in pounds. OK? Five of each. If I plot those on a this is what I get. OK? No big deal. What am I trying to do? I'm trying to learn, are that distinguish the two And in the unlabeled are just a set of examples. So what I want to makes two players similar can I separate this natural groups. Similar is a distance measure. It says how do I take or features going to decide how And in the unlabeled case, the if I know that there are in this case, I'm going two different groups there-- how could I decide how together so that all the are close to each other, all are close to each other, and There are many ways to do it. I'm going to show you one. It's a very standard way, and If all I know is that I'm going to start two examples as my exemplars. Pick them at random. Actually at random is not great. I don't want to pick too I'm going to try and But I pick two examples And for all the other I say which one What I'm going to try with the property between all of the examples The average distance is small. And see if I can gets the average distance as small as possible. This algorithm works by clustering all the other put it in the group to which Once I've got going to find the median Not mean, but median, what's And treat those as exemplars And I'll just do it either or until I don't get any So it's clustering And we'll come back to So here's what would If I just did this there's the natural And it kind of makes sense. All right? These three are and again, it's They're all down here. These seven are at There's a natural If I were to do it based This is what my with as the best meaning that these four, are close together. These six are close together. But it's not nearly as clean. And that's part of the is how do I find If I use both get that, which was actually Those three cluster together. in terms of just Those seven are near each other. There's a nice, natural And in fact, that This line is the between the centers Meaning, any point is the same distance to as it is to that group. And so any new example, I would say gets that label, gets that label. In a second, we'll at how do we measure but the idea here I want to find groupings and far apart from Now suppose I actually knew These are the receivers. Those are the linemen. And for those of you you can figure it out, right? Those are the two tight ends. They are much bigger. I think that's Bennett and a big Patriots fan. But those are tight ends, and it's going to but there are the labels. Now what I want to do is say, of knowing the labels, how And that's kind of easy to see. Basic idea, in this got labeled groups space, what I want to do is divides that space. Now subsurface is a fancy word. It says, in the I want to know if I can find a single line, with one label from all the We'll see that, if the this is easy to But in some cases, more complicated because may be very close And that's going that you saw last lecture. I want to avoid overfitting. I don't want to create a to separate things. And so we may have to labeled things, if And as you already with the labeled data, right there. Anybody over 280 pounds is Anybody under 280 pounds is OK. So I've got two different about doing this labeling. I'm going to come back to Now suppose I add I want to label new instances. Now these are actually players These are running backs. But I say, all I know about I get these two new data points. I'd like to know, are a receiver or a linemen? And there's the data So if I go back to oh you notice one of the issues. So there are my linemen, the the two black dots are And notice right here. It's going to be really examples from one another. They are so close to each other. And that's going to we have to trade off. But if I think about using with unlabeled data, there Now you see, oh, I've got This new example I would like a receiver than a lineman. But that one there, unclear. Almost exactly lies between those two clusters. And I would either say, I or I want to say, you know what? As I know, maybe there Maybe there are three. And I want to classify So I'll come back to that. On the other hand, if I there was my dividing line. This is really easy. Both of those new below the dividing line. They are clearly categorize as being than they are like linemen. And I know it's a If you don't like football, But you get the use the data in a labeled to come up with different So what we're going and 1/2 lectures is write code to learn that way We're going to learn models That's the case where I don't by simply trying to find ways nearby, and then use the to new data. And we're going to learn models and seeing how do we best come with a line or a plane or a from one group, from With the acknowledgment that we don't want to create a And as a consequence, to have to make some we call false positives But the resulting classifier by just deciding where to that separating line. So here's what you're going and 1/2 lectures. Every machine learning method We need to decide what's and how are we going to evaluate We've already seen We need to decide to represent each instance I happened to choose height and But I might have been better or, I don't know, arm How do I figure out what And associated with that, between those features? How do I decide what's Maybe it should be different, in for example. I need to make that decision. And those are the going to show you examples of Starting next week, is going to show you how you building more detailed versions measuring similarities to find want to minimize to decide what And then what is the best to use to learn that model. So let's start talking I've got a set of I need to decide what is it useful to use when I close to another thing or not. And one of the problems it would be really easy. Features don't always I'm going to belabor but why did I pick Because it was easy to find. You know, if you work for the is the thing that you really what's the right feature? It's probably some other So you, as a designer, are the features I want to use. That quote, by the of the great statisticians I think captures it well. So feature engineering, comes down to deciding I want to measure in that vector and how do I decide So John, and Ana, and I this term really easy at the beginning of the we've taught this We've got data John, thousands of students, Let's just build a that takes a set of data and You don't have to have to go through because we'll just Wouldn't that be nice? Make our job a little easier, like that idea. But I could think about Now why am I telling I was trying to see if I I saw a couple of them there. But think about the features. What I measure? Actually, I'll put this on What would he measure? Well, GPA is probably not a You do well in other likely to do well in this class. I'm going to use this Prior programming experience but it is not a Those of you who haven't in this class, you can still But it's an indication that languages. On the other hand, I don't So I don't think the month the astrological sign has probably anything to do I doubt that eye color with how well you'd program. You get the idea. Some features Now I could just throw all the machine learning algorithm to keep from those it doesn't. But I remind you of that If I do that, that it will find some month, eye color, and GPA. And that's going to that we really don't like. By the way, in case I can assure you in the dean of does not use machine He actually looks at a because it's not easy to yet. All right. So what this says is how do we pick the features. And mostly, what is to maximize something called Maximize those features that and remove the ones that don't. So I want to show you might think about this. I want to label reptiles. I want to come up with a are they a reptile or not. And I give you a single example. With a single example, But from this example, I know it has scales, it's it has no legs, So I could say my model I'm not certain. I don't have enough data yet. But if I give you and it also happens have scales, poisonous, There is my model, right? Perfectly reasonable or a machine learning do it says, if all of these are OK? And now I give you Ah. It's a reptile. But it doesn't fit the model. And in particular, and it's not poisonous. So I've got to refine the model. Or the algorithm has And this, I want to remind you, So I started out This doesn't fit. So probably what I I'm going to look at scales. I'm going to look I'm going to look at legs. That captures all Again, if you think about all three of them OK. Now I give you another example-- chicken. I don't think it's a reptile. In fact, I'm pretty And it nicely still Because, while it has scales, it's not cold blooded, So it is a negative example Sounds good. And now I'll give It's a reptile. And oh fudge, right? It doesn't satisfy the model. Because while it does have it has legs. I'm almost done But you see the point. Again, I've got to think And I could by Let's make it a little more cold blooded, 0 or four legs-- I'm going to say it's a reptile. I'll give you the dart frog. Not a reptile, And that's nice because So it's an example outside says no scales, but happens to have four legs. It's not a reptile. That's good. And then I give you-- I have to give you I mean, there has to Oh come on. At least grown at There has to be a python here. And I give you And now I am in trouble. Because look at scales, look I can't separate them. On those features, to come up with a way say that the python is a And so there's no easy And probably my best to just two features, And basically say, scales and it's cold blooded, If it doesn't have I'm going to say It won't be perfect. It's going to incorrectly But I've made a design And the design choice is that What that means is to be any instance of something going to call a reptile. I may have some false positives. So I did that the wrong way. A false negative that's not a reptile I'm going I may have some false I may have a few things label as a reptile. And in particular, to be an instance of that. This trade off of false is something that we worry Because there's no perfect to separate out the data. And if you think back to my Patriots, that running back so close together in there was no way I'm going to And I just have to how many false positives do I want to tolerate. Once I've figured out what then I have to decide How do I compare I'm going to say vector be multiple dimensions to it. How do I decide how Because I want to use the how to group things together that separates things apart. So one of the things I have I also have to And finally, I how to weigh relative importance in the feature vector. Some may be more valuable than And I want to show you So let's go back to my animals. I started off with a had five dimensions to it. It was egg-laying, cold I forget what the other one So one of the ways I is saying I've got four binary feature associated And one way to learn to separate is to measure the distance and use that distance to and what's not. And as we've said be used to cluster things or to separates them. So here's a simple way to do it. For each of these examples, be 1, false be 0. So the first four And the last one is And now I could say, all right. How do I measure or anything else, but these Here, we're going called the Minkowski Metric Given two vectors we basically take of the difference between of the vector, raise it to and take the p-th route of that. So let's do the two If p is equal to 1, I just between each component, add It's called the The one you've seen more, if p is equal to 2, this is It's the sum of the of the components. Take the square root. Take the square root it have certain That's the Euclidean distance. So now if I want to measure here's the question. Is this circle closer to the Unfortunately, I put But it differs, depending Right? Euclidean distance, well, so it's about 2.8. And that's three. So in terms of just standard we would say that these two Manhattan distance, Because you can only walk along Manhattan distance say this is one, two, This is one, two, And under Manhattan this pairing is closer Now you're used to We're going to use that. But this is going when we think about how between these different pieces. So typically, we'll We're going to see Manhattan So if I go back to my three a gross slide, isn't it? But there we go-- rattlesnake, boa There is the representation. I can ask, what's the In the handout for today, of code that would do that. And if I actually run actually, a nice are the distances between those I'm going to come back to them. But you can see the reasonably close to each other. Whereas, the dart frog is a Nice, right? That's a nice separation a difference between these two. OK. Now I throw in the alligator. Sounds like a Dungeons I throw in the alligator, and I And I don't get nearly as nice as before, the two snakes But it says that the dart are much closer, under than either of them And to remind you, right, snakes I would like to be close away from the frog. Because I'm trying to So what happened here? Well, this is a place where is going to be important. Because in fact, the alligator in three features. And only in two features from, But one of those features And there, while the difference is here it can be between 0 and 4. So that is weighing the distance The legs dimension is How would I fix this? This is actually, I would to use Manhattan distance. Why should I think in the number of legs or the is more important than Why should I think that Euclidean-wise makes sense? They are really completely And in fact, I'm but if I ran Manhattan it would get the alligator exactly because it differs only The other way I be to say I'm letting too with the difference So let's just make Either it doesn't have Run the same classification. And now you see the are all close to each other. Whereas the dart frog, not but there's a pretty natural using that number between them. What's my point? Choice of features matters. Throwing too many give us some overfitting. And in particular, that I want on those And you, as a designer have a lot of influence in how So feature engineering How you pick the is going to be important. OK. The last piece of going to look at some examples features associated with them. We're going to, in some in other cases not. And we know how now to measure distances between them. John. JOHN GUTTAG: You to say weights of features. You intended to say ERIC GRIMSON: Sorry. The scales and not No, I did. I take that back. I did not mean to say I meant to say the is going to be important here. Thank you, for the You're absolutely right. JOHN GUTTAG: Weights, we as we'll see next time. ERIC GRIMSON: And next time why we're going to So rephrase it. Block that out of your mind. We're going to talk about as being important here. And we already said at two different labeled and unlabeled, And I want to just two examples of that. How we would think about and we'll look at them As we look at it, you the things that are How do I measure distance What's the right What is the right set of And then, what constraints do In the case of do I decide how many Because I can give you a really If I give you 100 examples, Every example is Distance is really good. It's really close to itself, of labeling things on it. So I have to think decide how many clusters, of that separating service? How do I basically avoid which I don't want to have? So just to remind seen a little version of This is a standard way to we had on an earlier slide. If I want to cluster I start by saying how many Pick an example I take as For every other example put it to the closest cluster. Once I've got those, find the And that led to that separation. Now once I've got it, And in fact, I should Those two clusters came without Once I put the like to validate, how well And that example there is It's too close. So that's a natural place to with three clusters? That's what I get. I like the that. All right? That has a really The fact that the algorithm is irrelevant. There's a nice grouping of five. There's a nice grouping of four. And there's a nice grouping And in fact, if I looked between examples in it is much tighter And so that leads to, then, look for four clusters? Question, please. AUDIENCE: Is that overlap not an issue? ERIC GRIMSON: Yes. The question is, is the overlap a problem? No. I just drew it you see where those pieces are. But in fact, if you like, Those three points are than they are to that center. So the fact that they It's just the way I I should really circles, but as some little OK? Having done three, I could Well, those points down are an example where hard to separate them out. And I don't want to overfit. Because the only way is going to be to come up with which I don't like. All right? Let me finish with showing from the other direction. Which is, suppose I give So again, the goal associated with each example. They're going to have But I also know the label And I want to learn way to come up with a rule that and assign them to A number of ways to do this. You can simply say I'm looking will separate those examples. In my football case that the best line that which turns out to be easy. I might look for a more And we're going to see where maybe it's a that separates them out. Because there's not just one As before, I want to be careful. If I make it too get a really good separator, And you're going I'm going to just There's a third to almost the same called k nearest neighbors. And the idea here is I've And what I'm going to do say find the k, say the five And take a vote. If 3 out of 5 or 4 out of 5 are the same, I'm going to And if I have less going to leave it And that's a nice way about how to learn them. And let me just finish by Now I won't use football I'll use a different example. I'm going to give I think this is But these are a set of with their preference. They tend to vote Republican. They tend to vote Democrat. And the two categories are they live from Boston. Whether those are relevant but they are just two things I'm And I'd like to say, to separate those two classes? I'm going to keep I'm going to use half So if this is my can say what's the best I don't know about best, This solid line has the are on one side. Everything on the other but there are some Republicans I can't find a line that as I did with the But there is a decent Here's another candidate. That dash line has the you've got-- boy, I don't John, right-- but you've got almost It seems perfectly appropriate. One Democrat, but there's a And on the left side, But most of the Democrats are All right? The fact that left with distance from Boston is But it has a nice punch to it. JOHN GUTTAG: Relevant, ERIC GRIMSON: But Thank you. All right. So now the question is, How do I decide And I'm simply very quickly, some examples. First one is to look at what's What does that mean? It says for this, one of the solid line. Here are the predictions, of whether they would Democrat or Republican. And here is the actual label. Same thing for the dashed line. And that diagonal is the correctly labeled results. Right? It correctly, in gets all of the correct It gets half of the But it has some where but it labels it as a Democrat. That, we'd like to And in fact, it leads called the accuracy. Which is, just to we say that these Meaning, I labeled it as being These are true negatives. I label it as not being an And then these are I labeled it as being an and these are the I labeled it as not being And an easy way to measure it over all of the labels. The true positives and the ones I got right. And in that case, both models So which one is better? Well, I should validate that. And I'm going to by looking at other data. We could also ask, with less training error? This is only getting 70% right. Not great. Well, here is a more And this is where worried about overfitting. Now what I've done, with a sequence of lines So everything above this is a Republican. Everything below this line, So I'm avoiding that one. I'm avoiding that one. I'm still capturing And in this case, I get 12 true and only 5 false positives. And that's kind of nice. You can see the 5. It's those five red It's accuracy is 0.833. And now, if I apply that to the It has an accuracy of about 0.6. I could use this idea to try come up with a better model. And you're going to There could be other ways And I want to use this Another good measure we use is Value which is how many true of all the things I And in this solid model, I can get values about 0.57. The complex model on the And then the testing And finally, two other sensitivity and specificity. Sensitivity basically did I correctly find. And specificity did I correctly reject. And I show you this where the trade-off comes in. If sensitivity is how label out of those labeled and incorrectly how many them did as being the kind that I want? I can make sensitivity 1. Label everything is the Great. Everything is correct. But the specificity will be 0. Because I'll have a bunch of I could make the specificity Say nothing as an instance. True negatives goes to 1, and but my sensitivity goes to 0. I've got a trade-off. As I think about the machine and my choice of I'm going to see I can increase specificity at versa. And you'll see a nice technique Curve that gives you a sense of And with that, we'll We'll take your if you don't mind, because But we'll see you next will show you examples of this.