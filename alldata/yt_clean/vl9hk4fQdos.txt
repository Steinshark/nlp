- Hi everyone, welcome to my I will talk about how we can analyze these big data infrastructures from an offensive point of view. Before starting, let me My name is Sheila, I at Dreamlab Technologies, I'm an Offensive Security Specialist with several years of experience. And in the last time I had focus on security in Cloud Environment, Cloud Native, Big Data and related stuff. Okay, so let's go to the important things. There are some key concepts before jumping into the security part. Probably the first when talking about big data is the challenge of storing and the technology that Although that's correct, there are many others of great importance that make up the ecosystem. When we design big data architectures, we must think about how the from the source to the storage, if the data requires some kind and how the information So the different processes are divided into four main layers that comprise the big data stack. We have the Data Ingestion, that is the transport of the information from the different origins The storage itself, the Data Processing layer, because the most common is that later needs some kind of processing. And finally, the Data Access layer, basically how users will access And let's add one more layer here that is not part of the big data stack, but we have this layer in all The Cluster Management So for each of these layers, there is a wide variety of technologies that can be implemented is hugely big. These ones are just a few For example, Hadoop for the storage. Spark and Storm for processing. Impala, Presto, Drill for Flume, Sqoop for data ingestion. Zookeeper for management, for example. So when we analyze an entire we can actually find many interacting with each other, that they meet different functions, according to the layer of the stack, where they are located, right? So let's see an example of a Here we have two different and another one in any both are running in that are serving different applications. And we want to store and analyze the logs of these applications. So we will use Fluent Bit to collect all the application logs. Bring them to Kafka for the first cloud and stream them using Flume and Kinesis to run on-prem, Hadoop cluster. So within the Hadoop cluster, the first component that is Spark Structure Streaming. This one will take care of the information before dumping it into the Hadoop filesystem. So once we have our information here, we want to access it. So for that we could Hive and Presto, or instead of Presto we could use Impala, Grid or any other technology against Hadoop, right? And if we are developing our own software to visualize the information, we will probably have an API talking to the Presto coordinator And finally, we have the management layer to find Apache's Zookeeper, to centralize the configuration And also, an administration or a centralized lock system So this is an example of a and how the components So back to security, the question is how we can analyze these complex infrastructures. I would like to propose where the analysis of, is based of, of the different layers Because I think that the good way to analyze big data analyze the security of the In this way we can make sure the stages that the to go through, right? So from now on, I will explain that I've found throughout this research for each of the layers. Okay, so let's start with Zookeeper, as I said, is a widely use tool to of the different technologies Its architecture is pretty simple. It runs a service on all let's say a cluster one of the nodes and So when that happens Zookeeper the change across all the nodes. So, with this kind of node of the cluster, we will find the ports 2181 and 7888 open. Because these ports belongs to Zookeeper, are opened by Zookeeper, basically. So the ports 2181 is, is the port that accepts Should we be able to connect to it? Well, according to the of Ambari, a tool that is widely used for deploying on-prem big data clusters, Disable the firewall. for installing big data clusters. So we can probably connect to Zookeeper. How should we do it? We can download the Zookeeper client from the official website. Then it's just about running this command, specifying the node IP So once we connect, if we run the help command we can execute on relevant znodes. The znodes, or Zookeeper that Zookeeper organizes in So with the ls and get commands, we can browse this hierarchical structure. We can find very interesting the configuration of all the components that make up the cluster. Like Hadoop, Hive, HBase, And of course we could We can also create new delete configurations. These actually will be a Some components might go down because, Zookeeper, for example, is commonly used to manage the Hadoop high availability. So if we delete everything, the cluster might run into troubles. So I wont run demos of these because it's a pretty simple attack. But is actually quite impactful. So what about Ambari, open source tool to install And it has a dashboard from whose default credential But if they were changed, absolutely really to check. Ambari uses a Postgres database and information about the cluster. And in the default installation process, the Ambari wizard asks you for this network but it the default credential for the database. So we could simply connect to using these default credentials. They are user: ambari, password: bigdata and explore this Ambari database. We will find here two tables. The user.authentication and users one. So if we want to get the username and authentication key at once, we need to do this inner join query between those two tables. The authentication key is salted hash. So the best thing that update the key for the I log in to the Ambari source code to find Ambari salted hash. Here we have the hash So now we can run an update query. And once done we can log with the admin/admin credentials. Well, I know that if this but it's actually worth to check because Ambari controls the whole cluster. If you can access this password, you can do whatever you And as the default installation process doesn't ask for these you can most likely Good. So the important thing is to analyze the security and monitoring tools, right? So let's now talk about the storage layer. First, first of all, it is, it's good to understand how Hadoop works. It has a masters layer architecture the HDF, that means Hadoop and Yarn. So the HDF has two main components. The namenode that saves the metadata of the files stored in the And the datanode that and runs in this slave nodes, right. And, on the other hand, Yarn consists of two components, as well. The resource manager It controls all the processing resources in the Hadoop cluster. And the node manager that takes care of tracking on the slave node, among other tasks. But basically, what we have that is the Hadoop filesystem, is where the cluster And then Yarn is a service for the processing jobs that are executed over Basically it's stored. So when it comes to the storage layer, we are interested in the So let's hear how we could Hadoop disposes an IPC port of 8020, that we should find So if we can connect to it, we could execute Hadoop commands However, this is not as simple So managing to do this is a There are four configuration to perform operations over And if we take a look at we can say that they have dozens of configuration parameters. So when I saw that, I and I don't have access to these files, how can I compromise the So a part of this research was to find among those dozens of parameters which ones are, a how we can get them remotely that Hadoop itself discloses by default. So I will explain now how we can manually craft these files, one-by-one. Let's start by the core-site XML file. The only information we is the namespace. This is pretty easy to find, a dashboard on the It's a pretty high port. We can actually see it So as you can see here, we So I will hide my target cluster. And that's all we need for this file. Then we need to craft the hdfs-site file. It's necessary to know the from the previous file. And we also need the namenodes So we could have one, We need to provide the ID in these files. Where can we have these information? From the same dashboard? We have the namespace and the DNS, right? So we just need to access this Remember that this is on port 50070. Another alternative is to Port 50075. And then we can see all So the next file is the mapper-site one. Here we need the DNS of the namenode that hosted mapreduce-jobhistory. We can try to access the ports 19088 on the namenode. If we can see this dashboard then that's the namenode We already know its DNS Right. So. Finally, we need to Again, we need the namenode and DNS. In this case, the one that So we can try to access the board 8088. And as we see this dashboard And here we can get its DNS, of course. So all these dashboard and don't require any authentication. But if for some reason we cannot see them, we can try to get these through Zookeeper with the Because Zookeeper also has Cool. So once we have the the next step is to start and provide it with those files to perform the remote communication. As I didn't want to start I build these Docker It's pretty comfortable. You should need to to match the version of So from now on, this is going to be our running on the attacker machine, right? Good. So let's run and We can create the config we have crafted before. And you also need to copy inside this folder. And another thing I did to write the result of this namenode DNS. You can actually use the IP but for some reason I had Okay, so we are ready to go. Just pass to Hadoop this config directory, and you can execute for So voila, we can see the from our remote attacker machine. But before jumping into a demo of this, I would like to mention that most likely we will need to impersonate HDF users. For example, if I tried using the root user, I cannot. So we need to impersonate within the Hadoop filesystem. That means one of these ones. Fortunately, that's very easy to do. We just need to set these with the Hadoop username before That will allow us to also will allow us to delete So we could wipe out the Okay. So let's see a demo of this. Here, I have my files, the I also have the HDF file. This is, has more And also the namenodes. For the namenodes, I need to specify the DNS. I have two namenodes in this case, so I need to specify the And in the last property add for this specific cluster. The mapred-site has the DNS for the mapreduce.jobhistory address, the namenode that has this resource. For the yarn-site, I had for the resourcemanager node. So once we have those files, we are just ready to go an we over the remote filesystem. If we check the help for we can find a super common to move, copy, delete, To impersonate we need to as we saw before. And here or we can modify files or delete Good. Good. So let's now talk and how we can abuse Yarn in this case. So back to the Hadoop Yarn task schedules So these jobs execute So our mission here is to remotely submitting that executes our code or a command that we want to execute Basically I achieve our remote We can use the Hadoop IPC that we are, we were using in the previous attack. It's just necessary to our yarn-site file. We need to add the This path used to be the default path in Hadoop installation. So it should not be difficult In the example, here, we for installation using Then these other properties optional. It will specify the in the Hadoop filesystem. It might useful for us of our remote code execution, And something I would like to mention that I didn't say before. If you can access these under the /conf, we can find all the But you cannot just We still need to manually craft the files the way we were doing it. However, if something here you might find what's missing. For example, here we have the, the path that we are looking for for the, the property we have to set in this case. Good. So okay. So now we have improve our Yarn file and we can submit the The question is, what Here our Hortonworks That is enough for us to achieve the remote code execution that we want. It had only three Java files. Because Yarn applications but there are a lot of to include and use. So it might not be so easy to develop a native Yarn application, but we can use this one for our purpose. It takes as parameter, on the cluster nodes. And the number of instances, which is basically, on will be executed, right? So we will clone this repository in our Hadoop hacking container and proceed to compile We need to edit the pom.xml file and change the Hadoop version to match the version of our target. This is really important. Otherwise, this is not going to work. So once we do that, we can compile the application using mvn. Good. So the next step is to into the remote Hadoop filesystem. We can do it using the And after that, we are ready to go. In this way, we can submit Passing as parameter the and the number of instances. Here, example, I have over three nodes. And we are going to, to It's important to take note of it, but it's even more important because that's means that successfully. And now what, where can we It's what we are interested in, right? Well, we can use this command we got in the previous step. And the output is going We have executed this So we have three different Of course, we can change for any other, right? So let's see a demo of this. Here, I have improved the Yarn file to have the path I need to add. I have my simple yarn And I already uploaded it So remember, you can just upload the jar to And now with this command, we have to specify the and the command that we want to execute, and the number of instances, the nodes and the remote path. So with these commands, we are going to get our So now we need to use this application ID. In my case, I need to move the output from one directory to other one. Just to allow yarn to find the, the output in the next command. It might be not necessary for you. So with a yarn command, we can just get the output So we are going to see the We have the hostname output for hadoop2, and hadoop3. Good. So let me show you one more. I submit one more application before to dump a file of the nodes. In this case, the /etc/password file. So here we can see the password file for the three nodes, as well. So basically you can change these and execute whatever command you want. So that's pretty easy to use. Yep. It's also, should be quite simple to change these Yarn perhaps a more complex command. Just keep in mind that any in the application master file, as we can see here inside and and also in the client file, right? So for example, if we like a reverse shell on the But keep in mind that this is a shell that it starts finished. So we may need to use other alternatives, like backdooring the crontab for example. So you can execute this command and then back through the crontab, and then you will have your Sorry, on every node of the cluster. Good. I can't help but talk about Spark is super popular, for processing data, as well. It's generally installed on top of Hadoop and developers make data For example, in Python using PySpark. Because it's easier than developing a native application for Yarn. And also Spark has other So as we can see here, Spark has its own IPC We can submit a Spark on the cluster through this port. Is easier than we can. And This small code will to execute the hostname We should simply need Spark master IP address, our own IP address, to receive and the command itself. And then we should run this We don't need anything But I am going to talk in depth about this because there is already a talk a hundred percent dedicated to Spark. This was given at Defcon, last year. So I actually recommend The speaker explains how to via Spark IPC. That is the equivalent So keep in mind that the be present to the cluster. While Yarn will always be So it's good to know how to via Yarn and also via Spark. As we have the possibility to Awesome. So let's take a look If you remember from our big at the beginning of this and such data is ingested to our cluster using data ingestion technologies. There are several ones. We have some design for and Spark Structured Streaming, And then others like Sqoop, For example, from one data Or from one database to So from a security point of view, we need to make sure that these channels, that the information go to the storage are secure, right? Otherwise an attacker might and ingest malicious data. Let's see how this could happen. This is how Spark Streaming or a Spark Structured Streaming works. It's a variant of Spark that ingest data, and also process it into the Hadoop filesystem. So it's like two components in one. So Spark Structure Streaming or streaming can works with technologies to pull or receive the data. And also has the possibility from TCP sockets. And that could be pretty dangerous. Here we have an example when the streaming input It basically binds a port to the machine. So abuse this is super easy. We can use Netcat or a favorite tool, and just send data over What happens to the data that we ingested will depend on the Most likely we will crash the application because we may be ingesting bytes that the application Or a byte might end up That's also likely. So it's important to that are waiting for data to be ingested cannot be reached by an attacker, right? And regarding Hadoop, as I said, It's commonly used to insert information from different SQL databases into Hadoop. Analyzing a Sqoop server by default on port 12000. We can get the Sqoop software using this query, but there is not so much And honestly, it's quite easier to, um... abuse this using the Sqoop client. So something important client version of the server. For example, this already is 1.99.7, we should download that from this website. Good. So what can we do? Well, we could, for example, from that database that into the target Hadoop filesystem. That takes some steps. We have to connect to create some links. This is provide Sqoop with to the malicious database and And then we have to create a Sqoop job specifying that we want to ingest data from this database link to this So this is quite easier to So let's see video demonstrate. So here I have my Sqoop client, and I connect to the remote Sqoop server. These are the connectors We need to create a link the remote attacker database. So I will specify the mySQL driver and the remote address of the database, some credentials to access with. And then most of the So I will just create it. And also we need to create Here we have to specify two parameters. The first one is the remote In this case, it's in the ports 9000, but it's going to be most likely And the Conf directory is a It's a remote path. That by def... It's going to be the Hadoop So now I hear on it, I specify on it, the path of DEXO machine, but it's going to be most likely lines, /etc/hadoop/conf. So good now, so now we the links we have to create a job and the job we are going to specify that we want to inject data from to the target HDF. So we need to specify we are going to ingest and then most of the So I'll, I leave it blank. So once we create the job. Ah, also, here we have to and that's also important. That's the remote directory So, right. Now we have our We just need to start it. This is all we are going to but to show you that we actually I will login to the remote machine that has the Hadoop filesystem, just to show that the in the filesystem. Here we have the hacking Hadoop and the hello, blahblah, malicious data that was in my remote mySQL and I ingested it into this group, into their Hadoop filesystem via Sqoop. So keep in mind that you but you can also, um... export data because Sqoop allows you So you can do this in a reverse from the Hadoop filesystem into for example. Good. So finally, let's talk a little bit Back to our architecture example, we saw that it's possible to for data access. In this example, we are using but there are many others. And when it comes to Hive and HBase, these are HDF based storage technologies, but they also provide interfaces For example, Presto to create information and So this technology expose that can be abused by an attacker if they For example, Hive exposes where we can get interesting information and also an idea how the data The same for HBase. And regarding Presto I found this tedious login form where a password is not allowed. It's quite curious but you cannot enter a password. I know that you can set but by default, it seems to be this way. So you can write admin And there is a doc for, that show some information being executed against the cluster. Good. So as I said, this technology It's common to find at least a JDBC one. For example, in Hive, we and there are different to connect to it. Like SQRL, for example, or even Hadoop includes Beeline. We can connect to the remote Hive servers just specifying the remote address. If no authentication is there's usually nothing by default. And Hive has its own commands. We need to know them to With show databases, we can see the databases in the cluster. Select one and show its tables. And then we have syntaxes to insert, update, delete Good. So I running out of time. So let us provide some Many attacks that we were based on exposed interfaces. And there are many dashboard So, if they are not be being used, we should either remove them using a firewall, for example. If some components need without a firewall in the middle, then we should secure The firewall has to be present despite the official documentation I believe that we can to be allowed in our infrastructure and design a good firewall policy rules. Do remember also to change Implement any kind of authentication being used. Hadoop support authentication for the HDF. It's actually possible to and authorization in that we have seen. But we have to do it. Because by default, there Finally, remember that in there are many different technologies communicating with each other. So make sure that those in a secure way. Good. So in the next to put some more resources about the practical implementation So for today, that's all. Thank you for watching my talk and here's my contact information in Please feel free to reach me out. Thank you so much. Bye, bye.