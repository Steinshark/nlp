If you think about us humans, the data for us. time have learned about patterns So given that that's how nature has that the machines that we built just because of that technique. can't leave the building of these AI We need more people in our society, communicators, journalists, interested users of the technology, or people who are just willing to to make sense of why machines learn. that we can point out that, hang on. the way we think we are reasoning. that what's happening right now is very sophisticated pattern matching. We are interviewing the author by Anil Ananthaswamy. July the 17th on his way to India. 12 hours and I invited him to Unfortunately, I thought he was going to be here the and I had to get my good friend airport, take him over to the studio So I'm going to rerecord the I mean, you know, unfortunately, very, very pleased that I managed studio even if I wasn't there. What is this all about? pedagogical history of the field. underlying Lying mathematics machine learning. You should look at some of the He's really, really good. I enjoyed reading it. Pretty cool eh? conversation with Anil. My name is Anil Ananthaswamy. I trained as a computer and I did my bachelor's in India and Washington in Seattle. for a few years before I started And at some point, I love science and writing, could actually become a science So I went back to school, came to London to do a internship was with them for six months eventually led to a staff position. uh, became physics news editor, and uh, and, you know, wrote for Uh, and while I was doing that, I started working on my books, Edge of Physics. uh, book on cosmology and And each chapter is essentially, where I go to some really like the Atacama Desert in Chile, in peak winter, uh, the way to the South Pole. essentially extreme physics. The Man Who Wasn't There. human sense of self. who am I? theology and philosophy. that question from the perspective of The third book was Through Two Doors of uh, uh, it's essentially the called the double slit experiment, uh, experiment to explain with understanding the world. illustrative of what's happening So it's really a story about quantum but told through the lens of one that experiment done over 200 years. um, you know, It's called Why Machines Learn, that underpins modern artificial What inspired you to write about machine learning? you find extremely exquisite? or cosmology or neuroscience. something I could do. it was more about understanding But over the last few years, more about machine learning. given that I used to, you know, would write stories about machine engineer part of me woke up like, get this desire to actually get coding to actually understand this So about five years ago, Knight Science Journalism Fellowship. I decided to teach myself coding So 20 years after I had stopped doing back to, you know, the Computer sat with teenagers and taught myself and started building some very Well, 1 or 2 small things that I And as part of that exploration of system, a deep neural network interested in understanding the kind basic theory behind machine learning. Covid happened. and I spent a good six seven months by myself, both in Boston and Um, listening to all these machine again, teaching myself essentially. I started realizing that the learning is quite beautiful. me woke up saying, oh, these ideas to to my readers. this book came about. which is essentially really mathematical principles that underlie And regarding, you know, mathematics of machine learning? you know, machine learning is, calculus and linear algebra and What's particularly elegant And, um, I'm not talking about those Um, the beauty and elegance that I about machine learning had to do proofs that I encountered. if you go back to 1959, when the being designed, there is a there convergence theorem and its proof. uh, proof just based on, And it was while listening to a students in Cornell that I kind of, with the subject. this is something I really need something wonderful, uh, So the perceptron convergence proof really lovely and elegant about the with a caveat that, you know, things What I might find beautiful and else's cup of tea, but, Um, there's there's also, uh, uh, kernel methods, which is this you take data that exists in low high dimensions, into a much, possibly even infinite dimensional these kernel methods, on the mathematics that needs to But the computations that are dimensional space. there is a function or a kernel data into high dimensional space. algorithm is functioning in that But the actual computation happens And that that whole process of pushing it into high dimensions, in those high dimensional spaces, any computation in the high Uh, it's really lovely when you It's, uh, quite beautiful and So there were a lot of ideas like my research, that almost made it of things about which to write. disciplines do you find So for me, when I when I wrote you know, people who have maybe or first year undergraduate level want to learn something about So we're not talking of people who but it's basically people who need more depth than is possible if you So for that kind of audience, really need to kind of get come to trigonometry, um, linear algebra, basics of probability and statistics theory, it's not a whole lot. together, you kind of get a very why they do the things they do. seem quite empirical. foundations do you think are I think it's true that modern AI which is essentially based on deep there is a lot of empirical People are just building things this way or that way without algorithms work the way they do. understand why these systems are are, I think the answers to those figuring out the mathematical Right now, the way the field is, empirical evidence about, you know, And we're still struggling to formulation that can explain why or for that matter, Because until we know, you know, machines, from the perspective of the put upper and lower bounds on what How does your book showcase the You know, of machine learning I mean, if you ask anybody today, people on the street, they will And yes, you know, made a big splash. technology called deep neural But, uh, that's not, you know, learning goes back a long way. that has happened. You know, we I mentioned earlier that of neural networks, of artificial in the late 1950s, early 1960s. uh, single layer neural networks, artificial neurons. designed were enough to train networks to do some task. soon that if you had more than one and the output, this layer that And if you had more than one you could not use the algorithms And so and these single layer even though you could train them, So, you know, people had kind of given up on neural are not going to be very useful. research didn't stop. things that were happening that were So for instance, uh, also in the was analyzed mathematically and algorithm. That was really popular. with the using Bayes theorem and you know, Probably my favorite non neural algorithm is the support vector Support vector machines came about in the pre neural network era for a These algorithms are uh algorithms to some classification problem. part of the algorithm the kernel You know, this idea of taking data and projecting it into higher margins in the higher dimensions, in the lower dimensions. margin classifiers and kernel vector machines really powerful. stuff that one can talk about that late 1950s and early 1960s when the And, you know, the last decade or come back in full force. Right. the intervening history also, concepts that underlie those other understanding what is happening of how they represent data, in terms of manipulating the data. the algorithms and concepts that I had two hats on when I was things to put in the book. probably the most important criterion useful for demonstrating some for instance, the k nearest neighbor important for understanding how data, and how these vectors, you know, dimensional space. vectors is what determines how And, you know, using the k nearest give the reader a whole, uh, data gets converted into vectors and dimensional Commercial spaces, right? making sure that every algorithm some key aspect of something developing an overall picture of Um, again, this is subjective. you know, writer could have chosen could still make the case that that of the mathematical concepts. needed to address a particular set had my writers rat hat on. Right. making me choose algorithms which So to to make the story engaging So it was not enough that there these algorithms, but that the themselves had a story to tell. I could tell a story about them. believe that we understand things understanding is anchored in stories. algorithms that had key mathematical you know, substantial stories, What are some of the basic be grappled with in order to get I would say, um, calculus? Nothing very fancy. depending on whether you're going building these systems versus this math to understand what's you know, doing research or going If you're using the method to just are doing what they're doing, really need a whole lot of it. the, you know, concept of vectors and these vectors and matrices. it's it's not very complicated stuff. about the basics of probability You need to understand Bayes theorem, Um, and again, these are not bit of optimization theory. word optimization theory, techniques that we need to understand uh, essentially learning. techniques for optimizing their, And so, yeah, it's not a whole at least for people who want to so to say, as as you put it, build these systems and if you then your mathematical chops have Can you explain the bias variance Yeah, the bias variance trade And the basic idea here is that when model to learn patterns that exist if the model is too simple, you know, categorizing the simplicity or the the number of tunable parameters. different knobs that you can turn So if the if the model has too few data and it's being asked to figure that exist in the data, if the model then it's going to underfit the data. figuring out what those patterns are. that are underfitting the data But then you can you can start by again by here by complexity. I'm using the number of Um, and as you keep increasing there comes a point where the If the if the data has a lot of actually going to fit all the noise. a simple model might have drawn that you have. basically draw a very squiggly curve, every data point that you have. Um, so you essentially end up So and when you have a complex you are in the high variance regime. model is doing on training data, when you're given a training the training data? the risk of, uh, It's it's making a fair amount of But as the complexity of the moving towards high variance, really well until it overfits it. you basically you basically now making on the training data. is that there is a certain from the machine. certain amount of data. And when you test the machine this held out test data, bias side, you will still make a And then as the model gets more the error that you're making on But then at some point when the overfit the training data, test data starts to rise again. curve that is just going, you know, which is the risk of training error. is kind of bowl shaped. to a minimum and then it starts And that's essentially the bias You you want your models to be you're making a low enough error but you're also your error on And and that's the trade off. And you don't want your model to What is the role of learning models? chapter in your book, So this, uh, bias variance curve as you're making the model more more and more parametrized in the in the model. Are increasing. networks, what has been noticed that the model has far outstrips standard machine learning theory, curve that we just talked about overparameterize, as your number much larger than the instances of overfit the training data. where you're overfitting, make on your test data should, And it turns out that that's sort of We don't have a good theory for And Deep learning systems. flouting some of the accepted norms So even though they have their well on the held out test data. inability to generalize or the that they have is actually low. to generalize despite being over And the honest answer is we And the reason why in my book I systems terra incognita. not a term I came up with. researchers that I was talking If you have the I just mentioned, the standard machine learning systems the standard bias variance curve. As it happens, your training data your test error, you know, reaches the training error reaches zero. learning system is said to have But then what they notice is the test error starts falling again. curve now, We don't really know why the or in this particular case, the deep neural network behaves in part of the bias variance curve. terra incognita, basically because How does your book address the statistical principles underlying this crazy world that we live in deep learning models? understanding of the apparent even though they are heavily The empirical data is certainly mathematical theory to explain We don't know the answer to that. reconciles the two. standard machine learning theory, this is how machines should work. machines that learn should work. empirical results that we have they are not behaving the same way. essentially sets the sets this up as, not a profound mystery. what's happening, but really, the the Um, about why that's the case. reconciles them. of explaining what the situation we are we have literally entered with these deep neural networks. self-supervised learning? just train a model on the data I think self-supervised learning machine learning because until then, the other type of learning, which is had to annotate the data and tell And then, you know, by the fact that we need human And that's very, very expensive. large data sets, um, that the restricted purely because of cost. annotate data and give labels to the kinds of things machines learn trying to match, you know, patterns Supplied labels is a very It's learning something very So for instance, and a bunch of images of dogs that And the machine learning system this is an image of a cow, It might just pick up the fact that fields, so it might completely ignore As long as it sees some grass. that's the image of a of a cow. indoors or whatever. it might pick up in order to match data to human supplied labels, uh, It might be doing exactly the wrong that are not particularly useful. interesting breakthrough because technique relies on this idea that Humans don't have to label it as Humans are not involved in the mix. Let's say you take an image and portion of the image. you feed the masked image to the it to predict the entire image, Um, you implicitly know what that you had it on the input side. machine to complete the entire portion in the beginning, It's going to come up with some But you know what the right solution actual input in the first place. that, oh, you've made an error and I'm going to tune your parameters so your prediction the next time around, and over again, until the machine image and generate the full image. about the image that maybe wouldn't learning, because here there's no It's actually trying to understand structure of the image itself. with language, the kinds of things You take you take a sentence and you and ask it to predict the last word. the beginning. is because you had that sentence And you know, you take the amount of of the model in such a way that if ask it to predict the same missing make an error again, but you know, And you do this over and over gets it right. sentence on the internet, it has learned the statistical And so then after that, no matter you know a word, it knows how to So the amazing part about it can be easily automated. intervention here. learning some very sophisticated inherent in the data. supervised or unsupervised? The these are words that come And he has very authoritatively not be supervised. not even implying explicitly will be unsupervised. that supervised learning requires that humans have to label the data, And that's just not going to be You can do it for small data sets, But really, to keep scaling up But also the kinds of things that learns is very different from So there's a richness to the, uh, in self-supervised systems. philosophical reason to think be self-supervised is, is that, humans, you know, nobody has sat Our brains, over evolutionary time, exist in the natural world and you know, the body, do its thing, you know, predators, towards prey. All these things, uh, are have And yes, of course, stages of a child, you know, parents some form of supervised learning. humans learn, much of what we Have much of what we learn even as we grow So given that that's how nature has that the machines that we build just because of that technique. work so well given the complexity Well, again, this is one of those, have empirical evidence that Uh, exactly why it works so networks is still an open question. that suggests that the reason works is because it acts as an I can never say that word properly. And the reason, uh, is automatically, uh, or as part pruning the number of parameters, that it doesn't overfit and hence But there has also been work that will still find the optimal solution without stochastic gradient descent. something particular about a stochastic gradient descent that So again, the honest answer here And uh, we know it works. even when it shouldn't. thing to be doing, It's of course, very efficient. gradient. Gradient descent? efficacy are still, uh, not clear. dimensionality? something like the k nearest You take what that algorithm does and plots them in, you know, uh, So let's say we have a, you know, ten by ten images of cats and Uh, And you can imagine each pixel as, then you know that pixel has a So each image can be turned into a And that vector can be plotted So you know, And what will happen more or less cats will end up in one region and all vectors representing dogs of that high dimensional space. image that you don't know whether image into a vector and then you plot space and see, oh, is it closer If that thing is closer to dogs, If you if it's closer to cats, This procedure depends on this are alike are near each other. or vectors representing similar this high dimensional space. which if, let's say it's a dog, dimensional space, should be close, Now the funny one. you move into higher and higher let's say let's say the image was, So now you're operating in, a million elements. dimensional space. that similar things are closer in then things that are not similar. you start moving into higher and And that is the curse of You, the very metric that you starts falling apart, because in everything is just as far away So the notion of similarity that they're close to each other And that, in a sense, And as your data starts becoming you cannot use some of these of similarity by just using some Can you explain the context of and why do you think it's a and challenging to explain? garnered more attention than it I mean, the term seems to suggest that's happening, and it refers to models like ChatGPT started getting demonstrating behaviors that And in essence, It's basically saying that if there's asked a smaller model like GPT but then you built a larger model Nothing fundamentally changed in the underlying architecture of There's nothing different about Everything is the same. models have been scaled up, they have seen more data. mathematics underlying their architecture, uh, you know, that hasn't changed. you take the same problem that It could not solve it. 3.5 or GPT four and it solves it. called emergent behavior. you're making something bigger. You know, of course, these They've seen more data. more sophisticated pattern matching. more sophisticated correlations So it's not surprising that that the smaller models couldn't. behavior that cannot be explained. suggest something mysterious, you use the word emergence, it simply as saying that, okay, smaller model couldn't, uh, do. observed in a larger model, If emergence is simply the fact as you make the model bigger, uh, data and it just has a larger able to process the data in ways If you just look at it that way, skeptical about. would be the case. to imply something that is yes, there are aspects of why worked out mathematically, but, you know, mystery around it, It's it's not like that. ability in a large language model. ability that emerges. note is that we build GPT two, which and then we build GPT three, And when we test GPT three, wasn't present in GPT two. transition, that something just But the fact is that we didn't build, So GPT three has ten times more, We didn't build models that were, you We just went from, you know, parameters to something that has But if you had built the intermediate behavior, you probably would have not this sudden step change that So in that sense, again, sense that it just appears suddenly. How do deep learning models I think we have to be really careful um, human cognition, human cognition There are, uh, models that Um, that model, for instance, human auditory system, And they are the best models we happening in the brain. exact models. what's happening in the brain. the behaviors that we see in our human brain or other primate brains. the exact mechanisms that are in our brains? Absolutely not. most of these deep learning models The, you know, and the information just flows There is no recurrence. in the 10th layer, the outputs, 10th layer don't feed back to the nine, eight, seven and earlier. has to move forward. 12th and so on. They are numerous. connections probably outnumber So there's a lot of feedback And, you know, have this kind of recurrence. learning models seem to be to what they lack very obvious So they can't be, you know, exact. exactly what's happening, right now, and they are definitely might be processing information. machine learning models? invariance and, um, permutation So inductive priors are essentially incorporate into the architecture of ideas we have about how certain kinds For example, if you take things they were inspired by what we system or the primate visual system. that there's a certain hierarchy, visual system processes You know, there's a there's a certain that has to do with identifying So for instance, if I'm looking at a, system is identifying the edges, the texture before it puts it all Um, and but this is happening in, There's also invariance built So for instance, if there is an system that edge can, you know, And it should still be, you know, capable of detecting that, uh, should be able to, you know, still be So there's rotational invariance, And we've taken these ideas that we the the animal visual system And designs of deep neural networks. neural networks came about. priors so to say. about what these networks should the architecture of the system. this where we we already build think we need in order to make architecture of the system. algorithm and its history? probably one of those, uh, personally found quite elegant and And it's also a very significant learning and deep neural networks The basic idea behind backpropagation Again, if you go back to the late single layer neural networks. an input, it produced an output. the network made an error by looking and what it does you you calculate you just modify the strengths of the weights of the neurons. and those algorithms worked as long The moment you put another layer the so-called hidden layer, The algorithm couldn't work anymore. to do was every time the network made that it made on its prediction, and this problem of credit assignment. that error that the network has made the weights of the network, right. then it's easy to take that loss of the single layer. layer, it was very hard to figure out you know, move backwards from the stage and allocate to each weight the error that the network made. Frank Rosenblatt, who came up with he was aware of. Principles of Neurodynamics. look, the moment we have a then you're going to have this your errors from the output side so that every weight in your He just didn't know how to do it. Also in the 1960s, there were, you engineers who were building, uh, the trajectory of rockets. Arthur Bryson, I think, uh, so the Um, they had some form of this even though it wasn't called that, systems that could help control the you know, going in space. came up with the a use of the chain the Kelly Bryson algorithm better. of slowly falling into place. there was a Japanese researcher, who also figured out some aspects Again, none of these were, uh, the kind of bits and pieces were And, you know, there's a there's Jurgen Schmidhuber website that mentions, for instance, uh, I think it would have been 1970, um, for efficient back propagation. his PhD at Harvard, uh, develops uh, sort of version of the modern his PhD thesis, which had more It wasn't really addressing So all of this stuff was happening. happens in 1986, when Rumelhart, their paper just for 3 or 4 page propagation algorithm. being talked of specifically for neural networks with hidden layers. And they also not not only did they but they also pointed out that if multilayer neural networks, They learn about the data. they call feature learning or They could identify what kinds are learning because you use So finally, in 1986, I think people there's this formal thing, uh, a lot of the credit is given to, Geoff Hinton, because, uh, as one of the main people behind But even he would say that, look, he would be the guy getting all Hinton also acknowledges that this algorithm that they were it all together and made it, uh, network community. decades. And if they do reason, and how do you think their reasoning If you think of reasoning as what we learn something about how to solve Uh, not only do we learn how to of abstracting the principles and then we are able to transfer symbolic language, like mathematics about or solve problems in some And that kind of, uh, learning models are doing, right? essentially very, very, matching machines. in data that might even miss, So they are very good at that. large class of problems that can pattern matching machine. Right. correlations between inputs and statistical correlations, ah, solving a large class of problems. happening with these machines. ask them if these questions are machine to really deep into its correlations that exist in the data, It will seem like reasoning when but it's not reasoning in the in the Nonetheless, depending on where constitutes reasoning, right? but only in a very limited sense. These machines right now, essentially very, very What do you think that readers exploration of mathematical I think I would hope that readers be A kind of appreciative of the that underlies or underpins machine because the math says it's possible. be able to gain an appreciation under the hood, so to speak, uh, And it's almost like trying to the and conceptualize how machines I mean, they're not really thinking, Um, so by understanding the math, how machines might be processing Um, the other I think more important very sincerely believe Leave that. AI systems to just the practitioners, building them today. whether they are science policy makers, just really but who have some math, uh, just willing to persist and sense of why machines learn. appreciate, you know, making these machines quite powerful. algorithms we design and the math So understanding the math is going how powerful these things are going us about the limitations. Right. the math that we can point out that, these things are not reasoning in It's because the math clearly now is that these machines are, sophisticated pattern matching. Sometimes it gets the answer right. Do you think that affects their world situations. that do they understand. Right. them to understand? Yeah. llms are always hallucinating. has often colloquially been used But if you look at the way llms doing is essentially hallucinating. loses its meaning if you realize They are essentially, you know, producing the next most likely They append that word to the predict the next most likely word and so on until they produce like token and the whole thing stops. probabilistic statement about what the text that you've already given. the answer is right or wrong. It just so happens that when the LM these probabilities that it is make its best guess about what should so the answers can start looking is thinking, etc. but the process, whether it's getting it right, So so given that they are using so-called hallucination, that are either right or wrong, the answers they're producing is It's almost requires a human expert producing in order to ensure that Now, there will always be certain do where most of what it does, wrong, is still pretty amazing. when you're doing Python coding. assistants. generate so much code so much and, basic coding is already done. you can look it over very what it's supposed to do. as long as the human who is using able to tell right from wrong. what they're producing? It really depends on what you set the bar for what constitutes a And depending on where you set the they're very good at it, And it's really up to you. define understanding in a way that able to answer those questions, Llms But there are certain things good as what humans can do. is set at that level. of semantics. still playing out. intelligence. learning models are intelligent. really difficult term to define. defining it in my book. Try not to define it, but I think, define is because intelligence different contexts, right? needs to have to function in its the kind of intelligence you know, or a whale might need, So our intelligence is each each the outcome of having a particular environment and function in its, context or whatever it might be. the brain take into and the body of helping the body function in you would say that that system And so so it's hard to come up with abstract notion of intelligence Um, So if you if you think of our AI systems intelligent again you're defining the task. if intelligence is playing chess machine is doing it, let's say all with a machine, and you're defining chess as a kind of intelligence Yes, machines are intelligence. Hands down. comes to playing chess or so Um, this is not about what's It's just about looking at the is the behavior manifesting a kind is required to achieve the goal? a slippery slope. it, and in some cases the machines In other cases, absolutely not. So yeah, we have to be very careful There is certainly no such thing as a somehow abstracts away all notions of from the bodies in which we function. but I don't think we're there yet. And what does that mean? humans is this feeling we have of So if I were to pick up a a mug of that I willed that action into of that action, and there is an who is directing this body's actions recipient of the experiences. Right. feeling of being agents. do they have a sense of agency We can certainly build, you know, themselves as agents in the world. that the robot has a sense of agency, uh, the way we do about ourselves. can certainly build robotic systems but I don't think anyone would have an internal sense of urgency. and we're a long way from having feel that they are agents. learning revolution. you know, the backpropagation became a big deal because that's deep neural networks, neural networks But it wasn't enough. We had the mathematics now to We couldn't do anything particularly time, in the in the mid to late the amount of data that we had neural networks was very small. Uh, and that had to change. around 2007, 2008 onwards. data sets that came about was the I forget, millions of images and, you know, lots and lots of So we finally had a very, train the neural neural networks. algorithm in place. place. missing was, uh, you know, these computationally extremely expensive. train these things. noticing that instead of training central processing units. them, and that is to use these were actually designed for gaming. designed for training neural that they could co-opt GPUs to So it was a combination of, uh, algorithm, which was, you know, Uh, then the advent of really and the ability to use graphical all these things came together. the first deep neural network named through and showed how it could anything else that existed before. bit connectionist leaning, But what do you think about some Like for example, you know, methods, biomimetic methods, etc.? would say it's connectionist centric. It's a book on machine learning. the history starts with the perceptron algorithm and also, squares algorithm, which are both single layer neural networks. intervening history of machine do with connectionism. it's the naive Bayes classifier, the k nearest neighbor algorithms, all of these are principal and which is a statistical method um, unsupervised learning, etc.. important and ah, non connectionist. half of the book kind of focuses By recent, I mean in the last shifted back to neural networks. Uh, Hinton is a character in one I mean, the backpropagation Rumelhart Hinton Williams paper. you know, Hinton is front and And then he reappears in the networks because of AlexNet. Um, those those are those were I don't think there's anything Um, because the book is really doesn't deal with symbolic AI. I'm assuming you you're talking about learning use these days kind of And, you know, the problem with at what it did, it couldn't learn by just simply examining the data. to make it work. It was very brittle. symbolic, uh, the ideas from be very important if we are And I do think that the things combine the abilities of deep patterns that exist in data. have symbolic architectures that patterns in ways that we humans So I don't think there should be They are going to be put quite know yet how to do fully. and those you know, the entire where you're taking the connectionist and putting them together. I think if it if it helps achieve, actually do the kind of abstract biomimetic evolutionary algorithms, of possibilities, which is what will also be a part of, you know, deep neural networks that work Biomimicry is already in place. convolutional neural networks, building convolutional neural you know, what we think of our visual and even artificial neural networks. very loosely inspired by what a So biomimicry is already an That's only going to get more For instance, are so much more energy efficient Artificial neural networks. ridiculous amounts of energy to do than what our brains are capable of. some 20W of power and part of not the entire reason, but one neurons are not firing all the time. are called spiking neurons. inputs come into the neuron. every, you know, every now and then Um, that's a very different kind actually happening in artificial So if we get inspired by these systems and learn how to build let's say we build spiking neurons in train them and how to, you know, Well, that would be a huge leap And and that would very much be a, It's a big responsibility to write And of course, many different histories of the field, like, Although I do appreciate that that from you again, in writing this book. First off, I agree that, you know, responsible to the history of the capture it as accurately as possible, book was first and foremost to And those are not that different, of looking at the history. what the math was that I needed stories to anchor, uh, those And, you know, I chose a certain and help underpin the narrative. I agree that Schmidhuber, Jurgen contributed enormously to the field. an exhaustive narrative of all different people in machine learning Already my book, for instance, And and so the way, the way I of certain developments through the very hard to make sure that the So, for instance, uh, Schmidhuber someone who has contributed to LSTMs, It's just that I don't talk in my books. delve into that deeply. contribution. which is of and the use of GPUs, others as having, uh, made it, And, you know, GPUs and made it very popular. earlier, too. but certainly the ideas were and I made sure I acknowledged that, back propagation algorithm. know, pointing out that Zeppelin had, coding efficient back propagation. you know, there are these resources. So that was my approach to try was an alternate viewpoint that least mentioned it, but then in the conceptual aspects of math, you two one way of telling the story. laws with respect to how we I mean, do you think that we mathematical limitations as we So the scaling laws that we have neural networks, these are empirical, that we have observed the And we have figured out that particular set of laws. mathematical understanding of why Given that, it's really hard to will keep holding as we make You know, if there was a real that says that, yes, absolutely. you would expect things to continue. these are empirical results. find out in a year or two that if we that their performance may not scale Things might saturate. oftentimes when we have such we eventually notice saturation that to some power law up to a point. You know, So I given the lack of exact it's very hard to say. going to continue forever and ever. computational limitations to the I think it depends, again, learning system to do. the question, are deep learning certain kind of reasoning? kind of reasoning that humans can do, break it up into small subtasks and ways to achieve a perfect result. called compositionality. systems get there, just by the way? just by using the techniques we have say, even self-supervised learning. already some mathematical results, might be a, uh, inherent mathematical sort of compositionality can be transformer based architectures. limitations. without a complete understanding of are doing what they're doing, unequivocal claims about what they And I think we have to remain a I mean, for me, in my mind is nature has evolved. Our brains. sophisticated forms of reasoning, No one has sat around wiring our Evolution has discovered it. solutions. biological neural networks the same Absolutely not. in biological systems, and we are complexity in artificial systems. principle. It's been done once. It's been done over evolutionary but yet it's been done. principle to expect that deep Not in in principle. Reason. thing? Probably not. I don't know. And we don't know what those You recently did a talk, ChatGPT and Experiment with Alice and Bob. capabilities of ChatGPT? around with ChatGPT, uh, asking it, Uh, and even though I know that it's some of these questions can be and the output it generates seems model the minds of others. Right. but because you know what it's doing, under the hood, you realize that anything more than sophisticated Uh, but if you just look at the all you had was the output to go by, that it hasn't got the ability glimmers of being able to reason. the that's the problem. and you don't know anything about the curtain or under the hood, say it's not reasoning, once you know what it's doing, And also, uh, it's very easy to You can you can ask them some and they fail miserably. don't have sophisticated It's just that sometimes they seem You spoke about the potential risks the entrenchment of societal biases. taken to mitigate these risks, and I think there are some near-term need to be concerned about. learning systems are essentially in the data that we provide. has biases built in, you know, you're trying to build a system and traditional hiring patterns, sexist and racist. concerns that we traditionally have machine learning systems with they will exemplify those biases. And also, there's always an you know, the data that you have the same underlying distribution as And if those two distributions say your training data was drawn But your test data, the one that real life, in the wild, is being Then all bets are off as to what that So there are a lot of So biases that are in the data might systems, then the problem is, make biased decisions. to question ourselves as humans, in place, where if a human being sexist or racist or anything else in which we can mitigate that. systems is it's not often obvious is that there is implicit, uncertainty in the the way these that when they produce the output, certain and the right answer or, answer to be had. that's not what's happening. or rather, putting it differently, answers that machine learning Like, for instance, There are a couple of researchers, Celeste Kidd, who's a They made the point that when humans and when they're asking large in the nature of human psychology when we are asking questions, So if you have a large language but does so with extreme confidence, then because humans who are asking are psychologically receptive to likely get influenced by these And but once those answers are psychological makeup, we become It's almost like there was a were pliable and willing to take And if you have a large language answer and it's wrong, we will get influenced because we are So these are all issues that we You have compared the number of to the number of connections in Do you think that this So the number of connections in the today is probably about a trillion. 1 trillion to 1 trillion, Compare that to the human brain, account of the number of we stand at about 100 trillion. even the largest one, is about two the number of connections that we That's a that's a big number. connections in the human brain, whole bunch of other complexity For instance, we don't talk of We don't talk of the fact that happening in the dendrites, which We don't fully understand what happening within a single neuron. orders more orders of magnitude, brain than just. at the number of connections. large language models are far, the complexity of the human brain. at it, which is that even though orders of magnitude away from the they are already able to do some Right now, able to scale up these artificial complexity of biological systems. but we somehow make them energy proving really difficult. make them energy efficient so they're not consuming, you know, So we have artificial systems that of the human brain, but are also Then couple that to the fact that access to almost any information Our human brains are not capable You and I have limited access to So you you take the power of silicon. we can give to these machines. complexity of human brains. think that we are only just Can you tell us about your work The second book that I wrote, That book was an exploration of And essentially in that book I look neurological conditions. disturbs our sense of self in a And the entire thesis of the book is ways in which the self comes apart, internally feel about ourselves, the way our stories feel to us, here and now or existing over time, imagined future. you know, of being an identity, thing that exists in space and time. that, okay, let's look at the ways in but parts of it come apart. something about the way this put together in the first place So that was the, you know, uh, It was an exploration of the You discussed various that provide insights into the Which condition do you find most Well, I had eight different each one of them, because it affects sense of self, is both important So it's really hard to say that intriguing, but maybe in terms probably Cotard's syndrome was you know, Rene Descartes, uh, I think therefore I am. you can almost legitimately make I think, therefore I am not. is people with Cotard's syndrome don't exist. delusion, which is completely immune You can't talk them out of it So while it lasts, the delusion that they will actually start Um, and, uh, and we know a little bit not the funeral planning part, think that they don't exist. evidence to suggest that there are being affected because of Uh, but to me, the reason why it's the subject of an experience. but you can also be a self that And it raises the fundamental that is making that statement? it's making the statement I exist and in another, you know, the same AI is making the statement convinced of not existing as the You spoke about Alzheimer's disease which was the terminology you used. understanding of identity and I think Alzheimer's disease is devastating of these conditions, if I were to ask you, who are you? me a story about yourself. are in the form of a story. tell ourselves and others about And these stories change You might be a different story and you might be a different story But nonetheless, you know, And, uh, and what Alzheimer's is these stories disappear, because in Alzheimer's you have You you don't form short term So as a consequence, if you just had never entered short term memory, doesn't enter a long term memory. your story. forming as Alzheimer's sets in. basically destroys your story. you're unable to be your story. or a story that's in your body. Like, for instance, if you're a you may you may lose a certain because of Alzheimer's. that is embodied that if you were you could potentially just conduct cognitively say anything about it. embodied, but all of that goes away. philosophical arguments for a long like we are an AI like capital I, the subject of different experiences comes about from these narratives. creating these swirling narratives, But the center is nebulous. It only appears to be so because There was this philosopher, who had a beautiful phrase to He called the self the the center of narrative gravity. that physical systems have a Like any any physical objects, But if you go looking for the that center of gravity, It's just a property of the And so for Dennett, our self was also that are swirling around, you know, And if you took away the narrative, And it turns out Alzheimer's because in Alzheimer's you do end But you would be hard pressed to say that there isn't somebody still just, uh, you know, Because in Alzheimer's, the brain are still intact. So even though they can't even though their bodily selfhood it's very likely that there is still being some minimal aspect of their So I mean by just looking at how we are understanding that the self is You discussed the concept of this condition. Xenophilia. understanding of embodiment and I mean, like all the other conditions xenophilia or what it used to be identity disorder is telling us that actually something that the brain So if you were to just, you would have no doubt in your There is an implicit sense of It's even a silly question to be Of course it's my arm, right? I don't think anyone would, would question that feeling. or biid, uh, people feel like some And we now again have some that might be the case. us to feel like this arm is mine, what it's supposed to be doing, self with a sense of mindness or Sometimes it fails to do that Sometimes it fails to do that And when that happens, because it's almost like some foreign you can't bear to have it there. if you were somebody who were who spider was sitting on your arm, and your entire attention would sitting on your arm. foreign and but there's nothing it's functional. except that it doesn't feel like Um, it's a very difficult And but what it tells you about take for granted, is actually something that the brain nothing fundamentally real about it. processing that's happening in the So you can be someone, you can be an experiences an arm as their own. an arm as not belonging to you. it comes back to this idea that we What is your definition of agency? of the sense of self, agency turns So you know, we talked about if you pick up something, you have agent of that action and you willed It just it's a feeling we don't It turns out that there are feeling come about. taken for granted. performing some action, to your arm to perform that action. the brain is sending a copy of the brain that are now predicting action that you're about to take. that have been predicted match then that whole action is implicitly So the sense of agency is in this that matches the prediction And if the if those two match, if there was a mismatch, will not feel like you did it. but this is exactly what happens So they might do the same action, they are the agent of that action. mechanism. the mechanism that compares the happens. that action is tagged as being yours. being the agent of that action. doesn't have to be the case. they are the agent of the action, like they're not the agent of the So even the sense of agency is a Um, in this way of thinking, If we computationally build this then we are essentially defining And if we build the necessary then yes, we endow them with, Uh, though sense of agency still a subjective experience of that, experience. point would claim that AI models, aspects of it sorted out, are at this point, feeling like I don't know where that's going to happen, because whether or not definition of what consciousness is. and a difficult one to get into. an honor having you on MLC. there on the day, again and we can do the interview Anyway, I hope you enjoyed the show, By the way, now is an amazing time to a Patreon.com forward slash MLC. We have a private discord. of the shows. just been watching on our channel We have biweekly meetings with we talk about all of this stuff Of course, you can influence us etc. so please give us some support. slash MLC. Cheers.