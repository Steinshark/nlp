Good morning, everybody. Welcome to the session. My name is Dieter Fox. I'm And it's my great pleasure Vincent Vanhoucke. Vincent is a distinguished for robotics at Google DeepMind. Vincent started, actually, Brain a while ago. And he and his team have been learning for robotics. Vincent, in 2017, started the Coral, which by now is established related to learning in robotics. Vincent's team at Google and example, that also showed that it's You might have heard of the a large set of robots that for example, teal operation are exactly the techniques that talking, for example, about the They were also the first ones language models, generative models, and even planning in robotics. And most recently, some of RTX, RT2 model, which was that it's possible to train very and language, actually, all the way And this is, of course, the models that combine all these use those modalities to directly And today, Vincent is going in the age of generative AI. Welcome. Thanks. Thanks, Peter. Can you guys hear me OK? All right, fantastic. Cool. Well, thanks for the welcome. It's a special privilege for In a past life, before I started of the very first researchers at It was like Kepler Bring them, put together training neural networks. This was very Not a lot of people. It wasn't We literally had to hide machine so that people wouldn't So lo and behold, we ended up first deep neural network trained I think anywhere, really, that And we also went on to lobby hard and put them in their data centers. So Jensen, if you hear card, Wenmo, PayPal. Anyways, let's talk about robots. About two or three years ago, under a rock, you probably happened in the world of AI. Large language models happened. We suddenly had capabilities or understanding of the world to us in the past. For us working in robotics and We were supposed to be the We were supposed to be the AI to reality, to the real world. So in the community of robotics, missing out, that was developing. And people were a little jealous of this language modeling community So of course, if you can't And we started exploring what robotics and embodied AI, This could have been a very On the surface, the relationships are really tenuous and vast. You can imagine That's fine. You can imagine your robot poetry or whatnot. What happened was probably my entire career. The connections turned out to in fact, that they forced us how we do robotics and embodied AI. So I want to tell that story a really fascinating story or thinking in a very different that have built up an entire field. I'm not saying this is going robotics is going to go in moving It's a very different path on three years ago, and it's And so it opens up a lot of doing new research. The first thing we did is when ChatGPT came about was being something that it wasn't. So I did pretend to say that it's bit in very coarse terms what kind to be and ask it questions like, What's interesting about this and not great at the same time. Some aspects of it is the what it means to make coffee. It has a good understanding of what it takes to make a coffee. It has some notion of how about making coffee. It knows to ask the right The downside is that it has no idea It has no idea about any of And so it's So the first thing we tried and make that connection. This is work called say-can. And the idea behind it is we to propose solutions to a And then we had trained our trained using reinforcement inside the robots that is that you make to the robot against in a current context based on the And so that's called The value function could different hypotheses that We would make a decision about and then turn that into a plan. When you do this recursively, you how to go from very high-level semantics about how to operate The interesting piece here is that this really lifts the problem of Instead of doing planning or something that is really happens in semantic space, really understand what's going on. So this is what it You have a robot, you It has a perception that enables it It can pick objects. It knows that it can find objects. It can place objects. Those are all the affordances And the robot makes a is the right one to go after. So let me take a little step back of the scaffold of what comes next. This is where my roboticist their eyes because I'm giving introduction to how a robot works. It's a lot more complicated than actually serves our purpose here. So a robot, roughly, is you perceive the world. You extract the state of the You pass that state to a planner goal that you're trying to make a plan about what to do next. And once you have a plan for then you pass it to a controller actually executes the movements. All of that is in a loop of the world changes. So you re-perceive and you you keep doing that at a relatively adapt to the changes in the world. So what we did with say-can piece and replacing it with an LLM. And that has interesting planner speaks natural language. You no longer have sort of and the perception and the The consequences of that is as the sort of inner API inside actually lean on and use even more. So one thing that was starting is perception using visual really, really good. The performance of those models models were getting very good. And so the question is, can models that directly speak natural robots and obtain the perception So this is something we started the concept of Socratic models. This is the idea that you can that have specialized functions model, and then a large language And you can have them basically other in which they come up of the world, about what to do You can have the planner actually language model to get a refined of the environment that it wants This dialogue turned out to So we had a lot of follow-up leveraged that concept of having inside your robot where all the So this next work is kind of in say-can, we just had a language with this robot value function. With this inner monologue, we for the robots on this chat room. We had the language model require to execute that plan. We had vision components that scene and determining whether And so what that looks like robot of both the queries, tries to take, the reaction to disturb the state of the world. The robot fails. It can observe that It can react to that failure. It can change its plan. You ask for a soda. Suddenly, the soda's gone. So is there any other soda in Completely replanned, The nice thing about this is that It's really something that is can follow along this conversation, the robot has and really understand thinking, what its plans are, there may be with its perception We went a little further when the robot has a very In this case, we asked the robot There are two balls. The robot has You can use conformal prediction there is a high level of ambiguity the human that is the user and ask can disambiguate interactively. Another thing we started doing there is not really a precise goal. We're not just telling the robot the language model decide on goals. This was in the context of a we're trying to really expand that the robots would get. We basically told the language robots, explore, try to do things. Do interesting things. Do things that you don't know Or even do things that maybe need to call a teleoperator or help you accomplish the tasks. What's really interesting you have a robot that is defining to really think about safety. Those robots initially, they manipulate laptops. They were really an enamored. Laptop was something that and ooh, this is very exciting. I'm going to go and So we had to tell them, all to say things like, don't Don't pick up It was a kitchen, so there We removed the knives. But the idea was that, what's is that suddenly we have a concepts of safety, right? Don't bother humans, for example, parameters that can be explained in the way through to actually having So this is kind of this idea of used in chatbots to sort of guide We can do that all the way that actually follow general add yet another layer of safety. Those robots have lots of You can add one more layer and enhances even the general Okay, so we've replaced, will, the planner, we've Now, obviously, you We're going to try and do the same A controller is really a the robots, right? And so writing code is models do very, very well. And so we experimented with way to describe a controller. The first step in that direction This is the idea that you provide it with both perception APIs and to use them based on the natural And this can be For example, this is a language sort of small piece of code the blocks in the empty bowl. You'll notice that it uses in a very high level in this case. It was largely a toy example. But it also produced some didn't have an API for. It completely hallucinated those. It thought that it was very objects function. But then you can recursively and ask it, what is stack objects? And then the language model and more detailed codes that that you want, all the way down something that you can use. In this case, we had a pick that we could readily use. So this kind of recursive is very, very powerful at levels of abstraction and going What this opens up as well you have a natural language query actuation, you can teach a robot So this is an example where, apple to the cube, you have boom, it just rams into things. The user can say, yeah, Please don't knock over the can. And so suddenly there is a corresponds to that goal that It's a reward function that system and the robot learns We've done that a number of ways. This is our little quadruped it to give a high five. It doesn't quite get it. You ask it to, you know, raise Yeah, that's good. Now let's do it sitting. Oh, that's not really sitting. Tilt the other way then. Right, and that code is not Unless you actually know This, you have to be an But now a non-expert can directly on the robot. And I think that's a very important high-level semantics to all the something new to the table. One thing that I love about that dialogue that you're having and thumbs down and fine tune your desired behavior directly without You can bake that into the model. When you do that, you And that's pretty obvious. What you get as well is a model Because you don't feed it just You feed it the entire your responses were wrong And as a result, the model becomes And we've seen that even on a not seen during training, the model and better at, you know, we could the robots to do new behaviors. This, by the way, is all really fast simulation. So we have an open source We recently released Particularly, what's in there a JAX implementation of MuJoCo that broad sweeps of different behaviors We also have integrated into the which enables you to synthesize very quickly and experiment with, the results of that in real time. So this is a very powerful simulator in the loop of developing Okay, so we've Are we done? There is some It's very nice to have something component of your robot where you what the state of your robot is. But there are some limits to, want a much higher bandwidth your planner and your perception. You don't necessarily know in words can be very convoluted of precision work, for example. So one thing we tried next all language models. It's all big neural networks. Let's just try and see if So the first fusion the perception and planning. This was a work we called PAL-ME. This was, you know, multimodal now, but that was one of the We took PALM as the language model, in there so that co-training could include image tokens sequence of your input seamlessly on a variety of data such as visual So specifically, we had done control plans similar to SACAN, and that worked very well. So we saw a really very high number directly from vision to plans. Again, the output of this is all interpretable, but now the language see the intricacies of the visual What was interesting about first time that we saw this for robotics, but it actually all the tasks that you can imagine So it could do visual Q &amp;A, It didn't lose any performance In fact, that model was fine-tuned another team and became MedPAL-ME, was state-of-the-art at the time. I don't know if it still is, take a robotics model and medical recognition model I think this kind of power of just be retargeted is really interesting Another thing that was new first time, we saw positive So this is something that's worth Typically, you have different different point of views. You would imagine that when to fine-tune it on the embodiments deploying your model for, to give you the best results. What we saw with PAL-ME is the robotics data that we had, even if it was barely robotics-alike, planning, but it's not for a robot. When you put that all together, is working much better, and that we didn't see much of in robotics. There was rarely a generalized specialized models, and that there is a lot more to this, and Once you have a vision do lots of fun things. So this is an example that I I think this is something that's This is an early experiment model that also can generate using a video generation as So in this case, we have a with multiple actions that them on the spot, it will snippet of what would happen to the and then we score the output of that little snippet of video And that's how we select I think this kind of world and to actuation is very likely get better and have better fidelity of geometry and things like this. So I'm really excited about Okay, so we've connected Can we do, you know, let's Can we do directly This is another line of work to have pixel-to-action models all the modern toolkit of Our first work in that RT1 is basically an end-to-end tokenizes them, takes images, all into one big transformer and outputs actions that are that the robot can execute. It's a big model, but we can hertz so it's actually something the kind of tasks that we and placing and things like that. RT1 really worked well. And that was kind of a big that in the past, even for generalized pick-and-place kind saturate on the training tasks. Like, we could throw as much data we were training using behavior to, you know, 100 % performance. For the first time with RT1, we on the training set. Not on the training set, on And that's important because know, if you're in the asymptopia to completely nail the training In addition, we got So better generalization distractors, and backgrounds. So that's a good foundation Another thing that we learned that not all data is really equal. And one of the big, one of a simple ablation experiment where from the training sets, not a lot. So the total amount of data took out the most diverse different from everything else. And the performance just plummeted. What's important about this is key to those kind of action models, If you think of how, you know, in robotics labs, they typically to solve and they're, you data for that task and training a What we're saying here is that is already shooting yourself in operating in the context of having And thinking about architectures the game in terms of how well So some interesting lessons, at large, that, you know, multitask It really is the problem, that we're going to solve Okay, so by now you can picture We've confused two pieces. We've used two other pieces. We're going to try and see giant model, right? So that work is RT2. RT2 is basically a very large has all the capabilities of a very It can also do, you know, visual And the way we approach about the robot actions The VLMs are multilingual. They can speak all the languages We're just going to add one to correspond to robot action So the architecture is very a much bigger model. You input language tokens. You input image tokens, and then to, you know, robot ease, When you do this, You suddenly have an end-to-end and visual recognition all you can express very rich commands. You can say, pick the You can say, pick objects And all of that sort of subtle, what it means to be different, is incorporated in the VLM and So I'll give you two examples kinds of behaviors. This was an example of, we Coke can to Taylor Swift. Our robot has seen a We love Coke cans. They're, you know, our bread and But our robot has never seen what Taylor Swift looked like. We don't have any robot you know, Taylor Swift. The VLM does, right? And so the robot is able to of Taylor Swift and move the Coke It can also do this with reasoning. So basic reasoning, move two plus three, right? So that means the robot needs looks like visually. It needs to do basic Two plus three. That's something hopefully knows how to do. But we've never really taught to do sums, right? It's all part of the overall model. So you see this transfer between and the actuation, all working is, you know, I know emergent as but it feels emergent in the of gel together in one unified way. Another thing we saw with RT2 is of getting things to work. If you think of scaling laws a similar scaling law for sort of as we get much better, much bigger And I don't think we're anywhere based on the scale that we're at. So it's problematic in a Those big models are really slow. And so you're, you know, having this kind of speed and using not easy, but at least there us to scale up and get better. Another form of scaling is So remember when I was talking positive transfer between robots. We did other experiments such we trained joint models. This was a RT1 style model, but top of it, where we trained a joint different action spaces, different different settings, if you will. And again, we saw that even get much better performance It's a little bit like saying just speak different dialects it's not that they're formally expressions through the embodiment And that by adding the data get much better understanding means to control a robot. So we tried to push We partnered with 34 different to pool their data together. And obviously there is a huge that's happening in robot You have a ton of different different data sets. We just pulled everything together. We didn't even try to So just to give you an some of the data looks like. It looks completely random, right? You would be like, how can this kind of diverse data? Turns out we can. And that was also an interesting pulled all the data together, it to all our partners in able to improve on their baselines And it was as fair an experiment them the weights and let them So we didn't have a hand in So this is very exciting. I think the idea that, really works and works to the possibility of really building And it also enables the models not factor and specific embodiments. And that really has a profound I think, of what it means to share to leverage the community and build impactful as a collective effort. We also trained an RT2 version those emergent skills that I was we add more of this diverse data. So there is really a strong signal foundation models for robotics can the arts by a significant amount. So I want to sort of step bit on where we are at, right? So we have this kind of unified an input, takes reasons using codes that corresponds to action. It's just a large multimodal It's nothing really specific We train it on some robot It's not internet scale amounts A lot of the heavy lifting data that's from the web, The actions that we take are They're just dialects of robotese. And this picture is really, robotics community. If you'd asked me like three robotics would end up looking for robotics would look like, I Back in the day, and back in the ago, we were really focused on learning approaches that were very were using a lot and lot of data. So the shape of things has This is still research, so we this in the real world, but potential new path for robotics. And the thing that is very exciting enables robotics to ride the AI Any improvements to large to multimodal models, to video this and leverage this in robotics. So we're no longer kind of We're really part of the entire benefit from the entire sum happening in the world. Okay, with that, I who contributed this research. It's the effort of a very people and I'm very grateful And thank you for everyone here. Thank you so much for a We have time for some questions. If people just wanna walk that's on the aisle. Let's go ahead. One second, it's not working yet? Okay, thank you. So I think by introducing into the robotics and giving how can you guarantee, for Right now you only have a you may have human noise. How can you like solve the problem? For example, like miscommunication user or maybe just a bad intention robot, okay, do not hurt someone, Yeah, thank you. you. So the does not remove any of the to sort of really think about when The safety approach in robotics You go from the low level of are safe, that your controllers button on the back of your robot if of different components, right? The larger language model only I think the idea that you can the robot don't hurt people as a it to do only adds to that zoo. It doesn't substitute There is also the question models that are notorious And that's a general problem. What we are seeing is that when world by giving them observations in front of them, all those I don't wanna say that we're associated with them, but and over, you see, you know, a cup imagine that there is, you know, because you have this reinforcement work, the model, that it doesn't in abstract sort of internet space. So tons of new avenues to And I think this is a very exciting Thank you. Hello, thank you for sharing I was thinking when humans try to and we have our inner monologue, we to use this motor torque or I need And when I try to experiment with that it works maybe as good Sometimes they get the coordinates sometimes they make simple very strong large language, language models still struggle and arithmetic understanding And I saw on one of your outputting coordinates if How do you solve this task? How do you bridge So there is a lot of research that I think the vanilla vision language at geometry and spatial reasoning. And that's a problem. And that's merely a reflection of task it's being trained on, right? A lot of the data It's about broad Getting to really precise think it will take some more work. And I think a lot of people are of a shortcomings of the models. I think this can be We can really sort of, if we change really improve on that quite a bit. Another thing that's interesting we do this in closed loop, right? So it's all visual servoing You reason about the relationship if you're trying to pick something feedback at every step of the way. And so that feedback is actually that is being leveraged in this. You don't need to get the solute In fact, you don't know what be because your robot moves changes and the world changes. So it's all relative and it's all Thank you. I'd like to just briefly hijack I have one online question the gentlemen here. And if there's more questions course people should come up a longer discussion afterwards. From online and that's related question is, have you attempted code but iteratively test code by errors results back to the LLM? Yeah, so we deploy it typically That's one of the benefits is that can quickly run it on the simulated If it's sensical or if the And then once you have some is correct, you can push And typically if you have a you can have some guarantees there. But it's prompting code to be effective all the time, AI community is thinking about. And so I'm hoping we'll get respect in the future. Yeah. Thank you. Thank you, Vincent. Thank you for the talk. So what are the most exciting robotics that you foresee Like your imagination, both I think the most interesting thing do anything you want it to do using about the workings of the robots. If you can just prompt a robot do something and then retask this opens up, you don't have for every single problem that That's a dream, right? We're far from it. But the cost right now in deploying of the bespoke system integration If we can simplify that pipeline the people who are on the ground in logistics, can actually sort and just have the robot do it, that All right, I'm afraid this handle at this moment. So thank you, Vincent, again. And people, if they have questions, front end and talk to me. Thank you. Thank you. Thank you for joining this session. Please remember to fill out the a chance to win a $50 gift card. If you are staying in the room for in your seat and have your badge