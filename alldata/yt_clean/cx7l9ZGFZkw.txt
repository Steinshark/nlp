These are microscopic molecular machines essential to life on earth. They've evolved over millions of years to perform a vast array of vital functions. These are proteins. For more than half a century, biologists have sought to unravel the enigma of how proteins fold to function. The effort to doing this is enormous. But during a recent grand challenge, a team at DeepMind used artificial intelligence to solve a key part of the protein puzzle. Your group has performed amazingly well in CASP 14. It was really a shock. This breakthrough opened the door to a new era of biology. The AI revolution, if you want. Heralding a future of human designed proteins engineered to fix some of the world's biggest problems. You will always want to be pushing the frontier. This is the inside story of how AI cracked the protein folding code and the trailblazing science for which these three researchers won the 2024 Nobel Prize in chemistry. Proteins first appeared at least 3.7 billion years ago. Since then, nature has produced countless variations of these molecular marvels. They're the molecules that do the work. They interact with other molecules, they build other molecules, they take molecules down. They include enzymes that catalyze biochemical reactions, including digestion and antibodies that protect us from pathogens. Proteins keep our cells running, regulating molecular flow throughout our bodies. They also provide structure to tissue, power our muscles, and so much more. All these are at the end chemical reactions, and to understand this chemistry of life, you need to understand the structure of these molecules. A protein specific molecular function is a product of its three dimensional folded shape. They'll fold up into a really precise shape, and they do that every time, and that shape carries out a biological function. This origami like shape is rendered by the sequence of its primary structural components: amino acids. All proteins are built from 20 different flavors of amino acids connected in chains called polypeptides. When first assembled inside a cell, proteins are unfolded. Their amino acids strung together like beads on a necklace. These amino acids can be arranged in countless configurations to form different proteins. The recipe for a given protein's specific polypeptide sequence is encoded within a cell's DNA. In 1969, biologists Cyrus Leventhal observed a paradox. For any protein, even small ones, There's an astronomical number of possible folding configurations. Trying them all at random could take longer than the age of the universe. Yet proteins reliably fold into their functional shapes in less than a second. The mystery behind this process became known as the 'protein folding problem', which consists of three closely related questions. How does a sequence of amino acids encode the final 3D shape of a protein? What are the series of stages along which it folds? And then the third is: how do we computationally predict the 3D structure? The protein folding problem is a key concern of a field of research called structural biology. Structural biologists study protein shapes, formerly known as structures to figure out how they work. If we have the 3D structure, then we can really begin to understand how these molecules behave and function. A better understanding of protein structure could lead to cures to many diseases, including one's linked to misfolded proteins like Sickle Cell Anemia or Alzheimer's. The real promise is designing new drugs against specific targets. Cheaper and faster biologics. In 1957, biochemist John Kendrew revealed the first atomic protein structure using a technique called X-ray crystallography. This method became one of the most important tools for structural biologists. In the first step of X-ray crystallography, a desired protein is purified and concentrated as a crystal. Forming this highly ordered array is often the most difficult part of the entire process. The crystal is then placed into the path of a high intensity X-ray beam and rotated. The X-rays deflect off the electrons surrounding the crystal's atoms, scattering before striking a detector. The resulting diffraction pattern is then transformed by a computer into a 3D map of the protein's electron density. From here, it's like solving a jigsaw puzzle. Amino acids from the known protein sequence are fitted into the 3D map to produce a structural model of the protein. Initially, researchers solved this jigsaw puzzle by hand using balls and sticks to construct the models. Later, computational tools were developed to accelerate the task. The effort to doing this is enormous. You can think of it as something like a hundred thousand dollars in expense. A couple of years of a PhD student's time. Really an enormous investment to get even a single structure. In the 1970s, a consortium of researchers started the Protein Data Bank,or PDB, to catalog determined structures. The location of each atom inside a protein structure was recorded as a set of three-dimensional coordinates. Today the PDB contains the structural data for more than 200,000 proteins. Newer technologies like nuclear magnetic resonance and cryo-electron microscopy, or cryo-EM, allow researchers to probe larger proteins as well as complex molecular assemblies. But despite these imaging advances, much of how linear strings of amino acids actually fold into their 3D functional shape remains a mystery. In the 1960s, biochemist Christian Anfinsen deciphered a key piece of the protein folding puzzle. He later won a Nobel Prize for his discoveries. In a series of experiments he denatured proteins in solution, causing them to unfold and stop working. When he reversed the conditions, the proteins refolded to their native shapes, restoring their functions. Anfinsen's research revealed that all of the information for a protein's three dimensional structure and the folding steps to reach it are encoded solely in the biochemistry of its string of amino acids. That would suggest you didn't need any other biological machinery to go from the unfolded to the folded state. That implies you could use computational methods to get from one to the other. All 20 different amino acids share a set of molecular features, a central carbon atom with three groups attached. A positively charged amine group, a negatively charged carboxyl acid group, and a hydrogen atom. Also attached is a variable side chain called an R-group. The R-group gives an amino acid its unique biochemical function. The protein's linear backbone, or primary structure, forms when an amine group from one amino acid reacts with the carboxyl group of another, this creates what's called a peptide bond. From here, a protein follows a series of steps down the folding pathway. At first, linear polypeptide chains interact locally. A protein when it's folding, is searching for its lowest energy state. So that's true for any physical system. The backbone twists and folds to form two main secondary structures. Alpha helices are coiled sections of the backbone. While beta sheets are formed from linked adjacent amino acid strands. Both are held in place by hydrogen bonds. Next proteins fold into more complex tertiary structures driven largely by the amino acid side chains. These R-groups have different chemical properties and their interactions with each other and their environment both direct the folding process and stabilize the folded shape. The chain can fold up into a compact structure where essentially every one of the amino acid building blocks has an environment it energetically likes. And so this becomes a stable object. Some polypeptides combine with others to form a larger complex known as a quinternary structure. Once folded into their final or native formation, proteins bind to a specific target molecule, like a lock and key, to perform their biochemical function. People have said that biology is becoming a computational science, and this is certainly true. I think that you can't deal with the complexity of biological systems without a lot of computational infrastructure. By the 1990s, advances in computer technology accelerated the process of identifying protein structures using X-ray crystallography. In the early nineties, we could get maybe 100 structures every year in the whole world. And by the end of the nineties, we could get a hundred structures almost every month. Some researchers sought entirely new computational approaches to accelerate the process even further. That was sort of predicated on the idea that fundamentally, even though we didn't understand it, there was something simple about the problem. But early attempts at solving the protein folding problem with computer code proved otherwise. We were really very, very, very, bad at it at that point. That sort of drove me to think, well, how could we do things which would give us more sort of clarity and rigor and help us move things forward? So John Moult, co-founded a community science experiment, what would become the biannual critical assessment of structure prediction challenge or CASP. The idea of CASP is very simple. We asked the experimentalists, what is it you've just solved? Amino acid sequences for unpublished, newly identified protein structures are then passed to computationalists for prediction. And we can compare the two. Based on how closely the prediction matches the experimentalist structure, the results are given a score on a scale of 0 to 100. Criterion for many years has been, we need to get to 90. At the first CASP challenge held in 1994, the computationalists didn't do so well. It was found that nobody could predict protein structure. They would get it completely wrong. This respected experimentalist stood on the stages, sort of rolled about laughing. &quot;Look at how silly you guys are!&quot; We would joke like, oh, well, we still have a job for another 10 years. The CASP challenge nevertheless, had an immediate impact. Kind of organized the community around this one metric and enabled us all to see what worked and didn't work. For biochemist David Baker, the CASP-1 outcome was a source of inspiration. It meant this problem I was really interested in was kind of wide open. So Baker's lab got to work. We devised experiments to probe what the actual folding process was. There was a school of thought that if we could understand how nature did it, that would suggest some algorithms we could use computationally to make the same kind of shortcuts. Baker's team then engineered what they discovered into a computer program called RoseTTa. RoseTTa, its a physical model in which we try and mimic process of actual folding. There's a lot of these fine-grain interactions between these atoms, and you need to simulate them all perfectly. Each part of the chain is sampling through different possible states trying to find a combination where everything fits together. By the time of the fifth CASP in 2002, many researchers were asking the same question as Baker. Can you use physics-based methods to sort of home in again on atomic accuracy? And we saw a lot of encouraging progress on that. But after more than a decade without a significant breakthrough, a new computational tool called Deep Learning arrived on the scene. Wow. Wow. Folks, you saw history made here tonight. A deep learning algorithm designed by DeepMind, an artificial intelligence company owned by Google, accomplished what was once thought impossible. Its AI beat a human grand master in the highly complex game of Go. We're very early days in development of AI, but there are still many unsolved problems. After successfully mastering Go and several other games, DeepMind's founder Demis Hassabis sought new challenges. During the Go match, Hassabis flashed back to college, playing an interactive computer game called FoldIt. FoldIt was designed by David Baker's lab so that anyone could try their hand at the protein folding problem. Demis Hassabis has been interested in the problem for a long time. He wondered, could deep mind engineers who knew nothing design an AI to fold proteins? What are we doing still to improve? In 2017, theoretical chemist, John Jumper, joined the DeepMind team. The development of AlphaFold 1 was already underway. And then I'll probably be looking at little tunings that might make a difference. The team's approach was similar to what was commonly being used elsewhere for image recognition. Pretty standard machine learning, and then built the protein knowledge in a system wrapped around it. So we had kind of proteiny inputs. We had proteiny outputs. After training their system on the 136,000 structures in the protein data bank at the time, DeepMind was ready to compete against the protein scientists. In 2018, DeepMind entered CASP 13. While they weren't the only team using deep learning, AlphaFold's protein structure predictions came out on top. It did make a very large difference to the accuracy of the structures. It was better than anything else, but it was still not that brilliant. We were the best team in the world at a problem in the world was not very good at. We need to double down and go as fast as possible from here. So the intention is to enter CASP again. For the DeepMind team, it was back to the drawing board. So we redesigned the core components of the neural network, the insights in terms of the physics of proteins, the evolution of proteins. The new algorithm, AlphaFold 2, works like this. A protein's amino acid sequence is entered into the system. The algorithm then searches several genetic databases for similar protein sequences found in other organisms. These related sequences are aligned in an array to create a representation called a Multiple Sequence The MSA contains information about the evolution of the protein across different organisms. Next, AlphaFold generates a matrix to encode the spatial relationships between every pair of amino acids in the target sequence called a Pairwise Representation. You can generate a two-dimensional image of which bits of the folded protein are near to each other. This matrix can be thought of as a two-dimensional map of the protein's 3D shape. The MSA and Pairwise representations are then entered into the Evoformer module, which is a powerful neural network called a transformer. The Evoformer uses a technique known as self-attention to efficiently extract meaningful information while dynamically updating the data. So we're setting up a conversation between the evolution of the protein and what we believe about the geometry of the protein. The refined pairwise information is then passed into another transformer called the Structure module, which calculates the geometry at play to produce an initial guess of the protein's folded structure. This prediction is then refined by cycling it through the whole algorithm before producing a final output. AlphaFold also reveals a score of how confident it is in its predictions of different parts of the protein structure. CASP 14 returned in 2020 during the pandemic, with everyone on lockdown. The DeepMind team put their new prediction algorithm to the test. This diagram you've got here, John, is this one we've done badly on? We're actually quite good on this region. Months later, the results finally started rolling in. Many of AlphaFold 2's predictions received a score of 90 or higher. It was really a shock. You're looking at these things like, can it really be that good? What's going on here? I'm going to read an email from John Moult. Your group has performed amazingly well in CASP 14, both relative to other groups and in absolute model accuracy. Congratulations on this work! There were multiple factors to AlphaFold 2's success. A key one was the engineering of the new algorithm. We kind of built some of our understanding of proteins into it, what's called inductive bias in the language of machine learning, it learned extraordinarily rapidly from data. And the Protein Data Bank provided a dataset that was particularly well-suited for training in AI. The data follows these underlying rules of physics, so there's something that the model can learn. Quality of the information and the quantity of the information together really fitted ideally to get the deep learning methods to work. DeepMind's achievement made major waves in the scientific community. The press proclaimed the protein folding problem essentially solved. What do you mean solved? In what sense is it solved? I was really excited. A lot of my colleagues were sort of, they were talking gloom and doom. People were scared of losing their jobs. I often describe it as the six stages of grief, right? It's denial, and then you end up with acceptance. By July, 2022, DeepMind had released the structure for 218 million proteins, nearly all of those known in the world. They did make the code widely available, which has been a fantastic resource. AlphaFold2 heralded the arrival of a new era for biologists. Every experimentalist would try AlphaFold as a way to helping them to a solution, sometimes saving years to. Its raised the bar In terms of the questions that you can ask. The AI revolution,. You start applying things to problems you did not solve experimentally before. For the Baker lab, this meant applying AI to protein design, which is the process of synthesizing new and novel proteins. The work we're doing sort of roughly falls into three general areas. So the first is medicine, the second is energy and sustainability, and the third is sort of new technology. To design a new protein, researchers in Baker's Lab select a molecular target. Their goal is to create a protein that binds to this target's shape. First, the target shape is fed into a generative AI system called RFdiffusion. In the same way that algorithms like DALL*E generate images from prompts, RFdiffusion, generates protein structure backbones that mold to the form of the target shapes. We can start with completely random noise and progressively de-noise it, and we end up with something that looks like a perfectly plausible protein structure, but in fact is completely new. Once the diffusion algorithm generates a 3D model of the protein structure, another piece of software determines which amino acid sequences could potentially fold into the given structure. While there are many solutions to this puzzle, not all of them will work. So the sequences are fed into a prediction AI like AlphaFold 2, to confirm which candidates will likely fold as designed. The last piece of software determines the DNA sequence, which will produce the given amino acid sequence inside a cell. This DNA sequence is sent to a lab to be synthesized. If were making brand new proteins, there are no genes that encode them, so we have to make synthetic genes that encode these proteins. Researchers then introduce the synthetic gene fragment into bacteria which produce the protein. The bacteria basically become factories which make the proteins. In the final step, imaging techniques like cryo-EM are used to confirm whether the shape of the protein produced in the lab is the same as the one predicted by the computer. We can now design proteins which are much more sophisticated and should be much more precise and safe. Beyond medicine, we're working on improved methods of capturing sunlight and doing things with that energy. We're working on improved methods for degrading toxic compounds. While the suite of new AI, protein folding prediction tools have transformed many aspects of biology, their applications are limited. They only predict the structure of proteins, but in reality, proteins don't function in isolation. But what we kind of left unanswered was, well, how do proteins talk to the rest of the cell? Right? These are the machines of the cell. They do lots of really, really important stuff in the cell. Inside a cell, proteins interact with a host of different molecules, including DNA, RNA, and metals. To really study biology, we need to understand how those play into the picture as well. So Baker's team, DeepMind and others started developing AI algorithms capable of predicting these complex interactions. The fun part is we're starting to see examples that we've never seen before. The spring of 2024 saw the release of the next generation of AI prediction tools. The Baker Lab released RoseTTAFold All-Atom, which predicts the 3D structures of assemblies of proteins, and other small molecules. You can take protein sequences and chemical structures, so atoms and bonds, both as inputs into the model, and then create these structures of the combinations of them. Soon after DeepMind released AlphaFold 3. Really incredible improvements that we think are going to unlock a lot of new science. AlphaFold 3 adds a diffusion-based method to predict both binding structures and interactions of proteins with other molecules. AlphaFold 3 is an incredible system, but I wouldn't call these problems solved. In this new age of computational biology, one fueled by AI, the organizers of CASP have had to rethink the future of the challenge. In 2020 with the AlphaFold results, some people said, well, you're done then, right? Maybe Moult will finally go away and shut up. But of course, from our point of view, it's actually a very exciting expansion time in CASP. You always want to be pushing the frontier. At a time of technology transition, it's a very intense time. For me, it's been tremendously exhilarating. In October, 2024, David Baker, John Jumper, and Demis Hassabis shared the Nobel Prize in chemistry for their work related to protein structure prediction and design. I mean, there's so much more to be understood. It's the beginning.