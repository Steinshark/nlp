In 2011 an article was published in the Social Psychology&quot;. It was called &quot;Feeling the Anomalous Retroactive Influences on proof that people can see into the experiments. In one, participants were and asked to predict which one had an blank wall. Once the participant made positioned an image behind one of the pulled back to show either the image or the images were randomly selected from or erotic. If participants selected the considered a hit. Now with there being randomly behind one of them, you would percent. And that is exactly what the neutral images however for erotic images the hit rate that we can see into the future? Is that assess significance scientists usually you how likely a result, at least this true. In this case the null hypothesis actually see into the future and the guesses. For this study the p-value was .01 meaning there was just a one-percent chance of getting a hit rate of simple luck. p-values less than .05 are generally considered significant and worthy of publication but you might accept that humans can accurately study's author on your news program; but threshold was arbitrarily selected by 1925. But this raises the question: how much actually false? The intuitive answer everyone is using p less than .05 significance, you would expect five of but that unfortunately grossly Imagine you're a researcher in a field currently being investigated. Let's assume that ten percent of them are false, but no one of course knows of doing the research. Now, assuming the they should correctly identify around this is known as a statistical power of negatives, perhaps the sample size was sensitive enough. Now considered that p-value of .05, forty-five false considered true. As for the rest, they but most journals rarely published no percent of papers depending on the field, eventually get published will include 80 45 false positive results and maybe 20 Nearly a third of published results will be wrong even with the system working normally, underpowered, and analysis shows they of false-to-true hypotheses being tested All of this was pointed out in 2005 paper So, recently, researchers in a quantify the problem by replicating some The Reproducibility Project repeated a hundred thirty-six percent had a statistically around and the strength of measured of the original studies. An attempted landmarks in the basic science of cancer working closely with the original worse than i just calculated the reason study showing that eating a bar of weight faster. In this case the one of three treatment groups: one went on a low-carb diet, another one on bar of chocolate per day and the just to maintain their regular eating control group had neither lost nor had lost an average of five pounds per the group that a chocolate however lost non-chocolate eaters the finding was statistically As you might expect this news front page of Bild, the most widely and into the Daily Star, the Irish Examiner, unfortunately the whole thing had been perform the experiment exactly as they designed it to increase the likelihood incredibly small, just five people per different measurements were tracked blood protein levels, sleep quality, didn't show a significant difference might have. So the headline could have &quot;increases sleep quality&quot; or... something. The point is: a p-value is only once you're comparing a whole slew of one of them gives you a false positive Researchers can make a lot of decrease the p-value, for example let's it nearly reaches statistical just a few more data points to be sure then if the p-value drops below .05 you these additional data points could only there were really a true relationship that relationships can cross the data points even though a much larger no relationship. In fact, there are a likelihood of significant results like: more observations, controlling for gender, combining all three of these strategies likelihood of a false-positive to over sixty Now if you think this is neuroscience or medicine, consider the up of five quarks, as opposed to the Particle physics employs particularly significance referred to as 5-sigma or false positive, but in 2002 a Japanese Theta-plus pentaquark, and in the two years experiments then looked for and found very high levels of statistical May 2004 a theoretical paper on pentaquarks other day, but alas, it was a false attempts to confirm that theta-plus power failed to find any trace of its The problem was those first scientists the numbers were generated and what the data was cut and analyzed, or p-hacked, Now most scientists aren't p-hacking made about how to collect, analyze and on the statistical significance of research groups were given the same data soccer players are more likely to be some groups found there was no concluded dark-skinned players were The point is that data doesn't speak for Looking at those results it seems that dark skinned players are certainly not three times as likely; for most results only one research group the problem of incentives: scientists in fact their careers depend on it; as &quot;There is no cost to getting things wrong, Journals are far more likely to publish results that reach statistical analysis results in a p-value less than that method, publication's also more unexpected, this encourages researchers hypotheses which further decreases the that are tested; now what about self-correct by having other scientists discovery? In theory yes but in practice precognition study from the start of to replicate one of those experiments, well, surprise surprise, the hit rate they from chance. When they tried to publish the original paper they were rejected. publish replication studies. So if you're clear and don't even attempt replication publish them, and there is a very good statistically significant any way in convince colleagues of the lack of accused of just not doing it right. So a far better approach is to test p-hack your way to a statistically be too cynical about this because over changing for the better. Many scientists acknowledge the problems steps to correct them: there are more undertaken in the last 10 years, plus dedicated to publicizing papers that repositories for unpublished negative submitting hypotheses and methods for experiments with the guarantee that results so long as the procedure is bias, promotes higher powered studies and The thing I find most striking about the the prevalence of incorrect information after all getting to the truth we know is hard is published can be correct. What gets me is the thought that even true, using our most sophisticated and and the standards of practice, we still do we delude ourselves when we're not our science may be, it is far away more that we have. This episode of veritasium people on Patreon and by Audible.com, the with hundreds of thousands of titles in fiction, nonfiction and periodicals, anyone who watches this channel, just go i sent you. A book i'd recommend is which is a biography of Alexander who actually inspired Darwin to book or any other of your choosing for a so as always i want to thank want to thank you for watching.