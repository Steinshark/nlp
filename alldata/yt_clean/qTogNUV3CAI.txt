Today it is a true honor to speak with Demis welcome to the podcast. First question, given your neuroscience Specifically, do you think it's one higher-level thousands of independent subskills and heuristics? broad and what we use it for is so generally be high-level common algorithmic themes around us. Of course, there are specialized but I think there are probably some underlying How do you make sense of the fact that of data in any specific domain, they that domain. Wouldn't we expect a general First of all, I think you do sometimes get you improve in a specific domain. For example, that can actually improve their general reasoning. would like a lot more evidence of that. But we experience and practice a lot of things we also tend to specialize and get better at general learning techniques and general learning What's been the most surprising example see language and code, or images and text? this kind of transfer, but I think things and then generally improving your reasoning. But I think it's interesting seeing And can you see the sort of mechanistic in which you've found the place in a with both the language and the code? I don't think our analysis techniques are quite that. I think that's actually one of the areas the kind of mechanistic analysis of the up. I sometimes like to call it virtual brain or single-cell recording from a real brain. What artificial minds? There's a lot of great work Chris Olah, I really like his work. I think a lot brought to bear on analyzing the current encourage a lot of my computational neuroscience applying their know-how to the large models. about human intelligence that you have some sort I think neuroscience has added a lot, if you look I've been thinking about this for 30+ years. neuroscience was providing a lot things like reinforcement learning and combining work we did there were things like experience which has become super important. A lot of those about how the brain works, although not the system and the other one's a natural system. of a specific algorithm, but more so inspirational or algorithmic ideas, or representational general intelligence is possible at all. I think once you know something's possible it's easier to a question of effort, a question of when and not quickly. So I think neuroscience has inspired behind where we are today. As for going forward, to be resolved around planning. How does the studied how the brain does imagination, or you create very rich visual spatial simulations Actually, I'm curious how you think that will at the frontier and has been for many years with agents which can think through different steps LLMs to have this tree search kind of thing I think that's a super promising direction. We've got to carry on making them more and more accurate more reliable world models. That's clearly a of an AGI system. On top of that, we're working on top that make use of that model in order to in the world. Perhaps chaining thought, lines of massive spaces of possibility. I think that's How do you get past the immense amount require? Even the AlphaGo system was a pretty an LLM on each node of the tree. How do you One thing is Moore's law tends to help. Over every on sample-efficient methods and reusing existing looking at more efficient ways. The better your can be. One example I always give is AlphaZero, It's stronger than human world champion level in a brute force method like Deep Blue to play Deep Blue systems would maybe look at millions to make. AlphaZero and AlphaGo may look at around to make a decision about what to move next. A only looks at a few hundred moves, even the top about what to play next. So that suggests real model other than the heuristics about the top human players have a much richer, much more to make world-class decisions on a very small trade-off there. If you improve the models, then therefore you can get further with your search. you had a very concrete win condition: at the end can reinforce on that. When you're thinking of will be this ability to discriminate in the end, Of course that's why we pioneered, and using games as a proving ground. That's partly The other reason is, obviously, it's extremely game or improving the score, something like that challenges of real-world systems. How does one reward function, and the right goals? How does one that one actually points the system in the right lot harder. But actually, if you think about it in that you can specify the goal that you're after. you were just saying that humans thinking about Einstein coming up with relativity, right? of the equations. Do you think it's also "I'm going to try out this approach instead approaching and coming up with that solution I think it's different because our brains are just not the way our organic brains work. I think for that, have used their intuition--and maybe we and their experience to build in Einstein's case, mental simulations. If you read about Einstein and and really feel what these physical systems it. He had a really intuitive feel for what they think these thoughts that were very outlandish sophistication of the world models that we're to a certain node in a tree that you're searching, that leaf node and that gets you to these original on that model is very, very good, then you with search much more accurately. So overall, you no way that any human could do a kind of brute A big open question right now is whether RL synthetic data to get over data bottlenecks. I'm very optimistic about that. First of all, especially if one views multimodal and society is adding more data all the time to there's a lot of scope for creating synthetic partly through simulation, using very to generate realistic data, but also self-play. or converse with each other. It worked very well systems to play against each other and actually knowledge base that way. I think there are some complicated to build a general kind of world data. models where the synthetic data they're is not just more of what's already in their before? To actually improve the abilities. I think we're still in the nascent stage of actually analyzing the holes that you have in things like fairness and bias and other stuff. sure that your data set is representative There are many tricks there one can use, like data. Or if you identify some gap in your data synthetic generation capabilities to work on. stuff that DeepMind did many years before. What that was done way back in the past, that you think paying attention to it? There was a time where the thing now that is totally underrated? couple of decades has been things coming maybe five-plus years ago, we were pioneering first system that worked on Atari, our first that scaled up Q-learning and reinforcement learning to create deep reinforcement learning. complex tasks like playing Atari games just from ideas need to come back in again and, as we talked in large models and large multimodal models, which there's a lot of potential for combining some of Is there any potential for the AGI to eventually talking about it, it sounds like the LLM will form will go on top of that. Or is it a possibility Theoretically, I think there's no reason why you some people here at Google DeepMind and in the priors, no data, and just building all knowledge those ideas and those algorithms should also said that, I think by far the quickest way is to use all the knowledge that's existing in the like the Web. We have these scalable algorithms, all of that information. So I don't see why you or to build on it and to make predictions that doesn't make sense not to make use of that. So my have these large multimodal models as part won't be enough on their own. You'll need This sounds like the answer to the question this field for a long time and seen different strong version of the scaling hypothesis gets you just throw enough compute at a wide enough My view is that this is kind of an empirical surprising to almost everyone, including the how far it's gone. In a way, I look at the unreasonably effective for what they are. I think emerge. In my opinion, they've clearly got some like that. I think if we were talking five-plus we need an additional algorithmic breakthrough brain works. I think that's still true if we but it seems that these systems can implicitly thing was that these systems have some sort of world multimodally, at least until more recently information and models that can be built up just have some hypotheses about why that is. I think systems because obviously the human raters are reality, so our feedback is also grounded. Perhaps Also if you're able to ingest all of it, maybe thought before. So it actually raises some very haven't even really scratched the surface of it's quite interesting to think about where it's large models, I think we've got to push scaling as It's an empirical question, whether that will different people who argue about that. I think In the meantime, we should also double down on Google Research and DeepMind and Google Brain have That's our bread and butter. You can think of half our efforts having to do with inventing the will be needed, knowing that larger and larger betting right now, but it's a loose betting, is both of them as hard as possible and we're I want to ask more about the grounding. You can make the grounding more difficult. One is that be able to operate in domains where we just can't not smart enough. If it does a million-line this is within the constraints of our morality other thing has to do with what you were saying prediction and in some sense it's a guardrail, and think as a human would think. Now, additional reinforcement learning where it's just getting you got there. When you combine those two, how I think if it's not properly grounded, the system a sense, you have to have some grounding for world. I do actually think that these systems, and As we start ingesting things like video and system starts correlating those things together. I do think our systems are going to start to Then one could imagine the active version game environment where you're starting to learn that affects the world itself. The world stays episode you're getting. So these RL agents like AlphaZero and AlphaGo, actually are next affects what next learning piece of data this very interesting sort of feedback loop. things like robotics, we're going to have So there's grounding in terms of whether whether they will be enough in touch with another sense of grounding in that we've gotten they maybe think like a human. To what extent training comes from just "did you get the right proceeding on the next token as a human would?" and this is what I asked Shane as well, what than a human? Maybe it thinks in alien concepts request because you can't really understand This is something Shane and I, and many others before we started DeepMind because we planned for alone AGI. But we already knew that if we could the technology created would be unbelievably 20 years ago about what the consequences of that the positive direction is amazing incredible breakthroughs in health and science, also have to make sure these systems are This will be a whole discussion in itself, but such as more stringent eval systems. I think we for things like if the system can deceive you. Can behaviors? There are also ideas of using AI, not are specialized for a domain, to help us as the more general system is doing. So there's narrow in creating hardened sandboxes or simulations around the simulation, both to keep the AI in a lot more freely within that sandbox domain. analysis stuff we talked about earlier, where are that this system is building and what the so alien to us and we can actually keep track Stepping back a bit, I'm curious said his modal outcome is 2028. I think I don't have prescribed specific numbers to and uncertainties. Human ingenuity and endeavor could meaningfully move the timelines. I will we thought of it as a 20-year project. And I of amazing for 20-year projects because usually about whatever, quantum, AI, take your pick. But if we had AGI-like systems within the next decade. you have a system that basically speeds up further but over the course of months and progress than you would have otherwise had? it partly depends on what we, as a society, proto-AGI systems for. Even the current LLMs seem like AlphaCode. We also have theorem proving ideas together and making them a lot better. I designing and helping us build future versions the safety implications of that of course. not saying this is happening this year, but you think there's some chance that it'll be dynamic once it's fully developed. What would you're comfortable continuing the development specific evals, I've understood its internal We need a lot more understanding of the systems of explaining to you what we'd need to tick box few years, in the time before those systems start and metrics. Ideally formal proofs, but it's going empirical bounds around what these systems can do. being quite root node traits that you don't want. what it actually thinks, then that opens up explain aspects of itself to you. The way I think of chess against Garry Kasparov, which I've played players of all time, I wouldn't be able to come explain to me why they came up with that move and sort of thing one could imagine. One of the systems is for them to explain it to us and even something, certainly in a mathematical problem. answer would be? So what would have to be man, I didn't anticipate this." You see some makes you say "we got to stop Gemini 2 training." the sandbox simulations are important. I secure environment when something very unexpected something that we didn't want. We explicitly did and it lied about it. These are the kinds carefully. The systems that are around today are they might have potential. Then you would ideally doing those things before one continued. bottlenecks were in the development. of magnitude bigger if scaling works? How much compute can you actually fit in one very interesting distributed computing kind the best people in the world working on those all of these kinds of things. There are very TPUs that we're building and designing all the that. Scaling laws also don't just work by magic. and various innovations are going in all the time the same recipe at each new scale. You have to form. You have to sort of get new data points. If them several orders of magnitude out, sometimes functions in terms of new capabilities and some those intermediate data points to correct some of so that the scaling law continues to be true. So One order of magnitude is probably about the That's so fascinating. In the GPT-4 were able to predict the training loss with compute than GPT-4. They could see the curve. capabilities that loss implies may not be so. don't follow. You can often predict the core but then it doesn't actually translate into MMLU, care about. They're not necessarily linear all What was the biggest surprise Gemini in terms of something like this happening? very interesting trying to train things at things from an organizational standpoint, track it. There's also things like getting optimizing versus the final capabilities that you understood mapping, but it's an interesting There's a perception that maybe other DeepMind has been with Gemini. I don't I don't think that's the case. I think that of compute, maybe slightly more, than what was used but I think it was in the same ballpark. and we use our compute for many things. One is not innovations and ideas. A new innovation, a new So you need quite a lot of compute to do new at least some reasonable scale, and make some new ideas may not work at a toy scale those are the more valuable ones. So if you need quite a lot of compute to be able to at Google. I think this year we're going to have lab. We hope to make very efficient and good capability of our systems and also new inventions. back to yourself in 2010 when you were starting like? Did you anticipate back then that it would, of dollars into these models? Or did you have We thought that actually, and I know you've thought in terms of compute curves how many neurons and synapses there are very kind of regime now with roughly the right order and the sort of compute that we have. But I think on generality and learning. So those were always why we triangulated on reinforcement learning, algorithms that would scale, be very general, and thought that was the sort of failure mode of the MIT. There were very logic-based systems, handcrafted human information going into them wanted to move away from that and I think we games as our proving ground and we did very well and maybe inspired others. AlphaGo, I think, was a actually, these systems are ready to scale." Of invented by our colleagues at Google Research that allowed us to ingest masses of amounts of we are today. So I think that's all part of the twist and turn there, but I think the general It's fascinating if you read your old papers or he said "well, the way we would test for AI is, the loss function for LLMs. Or in your you were comparing neuroscience and AI Exactly. So we had these things called out but they weren't as elegant as transformers like this. Transformers were the nicer When you extrapolate all this out forward what does that landscape look like to you? Is it the governance of that look like concretely? this technology. I think it's much bigger than I think it has to be a big collaboration with government, etc. The good news is that with the that has woken up many of these other parts of it will be like to interact with these systems. very good conversations. An example of that was which I thought was a big success in getting this society needs to be involved in deciding what want to use them and what do we not want to use international consensus around that and also make good of society in general. That's why I push so with things like our spin-out, Isomorphic, we're accelerate drug discovery, tackle climate change, challenges that face humanity, massive challenges. we've got this incredibly powerful tool of help us solve many of these problems. Ideally, we discussion at sort of the UN level if possible. systems and chat with them, they're immensely the extent to which they haven't automated large ago I showed you Gemini, you'd be like "wow, So how do you account for that? What's going I think that just shows we're still at the some interesting use cases where you can use these some simple writing, maybe more boilerplate-type all do every day. I think for more general things like planning and search but also things not just long context windows, but actually ago. I'm really looking forward to things like more enriching material, whether that's books or system every day. So I think we're just scratching actually do for us in our general, everyday lives they're not reliable yet enough to do things once we fix factuality and grounding and becoming the world's best research assistant I want to ask about memory. You had this about the links between memory and imagination and often claim that these models are just memorizing. all you need because in some deep sense, At the limit, one maybe could try and memorize distribution. The early criticisms of these early and memorizing. I think clearly in the Gemini, to new constructs. Actually my thesis, and area of imagination in neuroscience, was showing is a reconstructive process. It's not a videotape. that seem familiar to us, the ensemble. That's same thing. Except in this case you're using the it together in a way that your brain thinks is do think that that kind of idea is still probably different parts of your world model to simulate which is what I would call imagination. in the world with the Gemini models. Do you like the other two major AI labs have? Something unless we have these specific safeguards, or we're not going to ship the product out." and balances but we're going to start publishing. whole bunch of blog posts and technical papers along similar lines of things like responsible internally in various safety councils that people to talk about that more publicly I think. So we'll That's great to hear. Another thing I'm of the deployed model being something but there's also rogue actors, foreign agents, and then fine-tune them to do crazy things. How do something like this doesn't happen, making sure It's interesting. First of all, there's two parts. we can discuss. The security is super key just we're lucky at Google DeepMind. We're behind think is best in class in the world corporately. we have specific DeepMind protections within of protection. So I feel pretty good about but I feel it's already the best in the world on improving that and again, things like the as well. Maybe there are even specifically to this too that we're thinking about. I think we would also want air gaps and various other So I think that's key and I think all frontier rogue nation-states and other dangerous actors, them to steal things like the weights. Of course, huge proponents of open source and open science. AlphaFold and transformers and AlphaGo. All of published and open source, most recently when it comes to the general-purpose foundational open source proponents is, how does one stop taking those same open source systems and answer that question. I don't know what the answer clear answer to that from proponents of just has to be some balance there. Obviously, I feel like tech doesn't get the credit it dollars' worth of R&amp;D, obviously you have DeepMind talk about securing the weights, as we said maybe cause the end of the world or anything, but as worry that a foreign agent or something gets dozens to hundreds of researchers who have access weights in a situation room where if you need to and no individual can really take them out? collaboration and speed of progress. Another brilliant independent researchers from academia or one to be able to red team these systems. So although that's not necessarily the weights. We sure that only if you need them, those people we're still in the early days of those kinds of more powerful and more general and more capable, Some of these other labs have specialized Anthropic for example with interpretability. have an edge? Now that you have the frontier put out the best frontier research on safety? like that which can obviously be used for a lot of the self-play ideas and these kinds lot of the boundary conditions that you have that with these very general systems, there's systems behave. So I think we are going with things like simulations and games, very have a long history of using those kinds of AI algorithms. I think we can leverage all of that to have some of the world's best cybersecurity that to bear for security and safety as well. the best model in the world. I'm curious. The been through chat so far. Now that we have how do you anticipate that changing? I think we're just at the beginning of actually interact with a full multimodal model system. to today with the chatbots. I think the next we'll maybe have some contextual understanding or a phone or some glasses. I could imagine start becoming more fluid in understanding "let's eventually things like touch and if you think think the world's about to become very exciting to the idea of what true multimodality means. podcast Ilya said that the reason OpenAI gave up data in that domain, at least at the time different things like Robo-Transformer and other for robotics progress, or will we see progress in We're very excited about our progress with things and we've had amazing research in that. We still it's a data-poor regime. That pushes us in very are going to be useful anyway: sampling efficiency learning from simulation and transferring that interesting general challenges that we would like pushed hard on that. I think Ilya is right. It is I think we're starting to see the beginnings the robotics regime. They can learn in the general just treat tokens like Gato as any type of token. it could be part of an image, a pixel, or whatever To begin with, it's harder to train a system system. But going back to our early conversation true multimodal system, the other modalities better at language because you now understand harder to get going, but ultimately we'll have What ever happened to Gato? That was play games and also do video and also do text. but you can imagine we're trying to build Gemini to be able to do all of those things. you can think of them as follow-ups to that. which the self-play kinds of things you're talking code. Recently, you have these papers out about novel things. Will they be superhuman coders, than humans? How do you think about that? with math and things like theorem proving and at creativity in general, and scientific endeavor where our systems could help the best human almost triage the search space in some ways. with a protein structure. They're not at the level ask the right question. As any top scientist will asking the right question. It's boiling down go after and then formulating the problem in the systems really have any idea how to do, but they spaces if one can specify the problem with a clear for many of the problems we deal with today, DeepMind has published all kinds up science in different areas. If you think AGI why not just wait for the AGI to do it for I think we don't know how long AGI is going we started DeepMind, that we don't have to wait the world. My personal passion especially has that with things like AlphaFold and all of our material science work and so on. I think there's the world through products too. I think it's have as part of Google. They've got dozens of ship our advances into and then billions their daily lives. I think it's a fantastic I think the other reason from the point of view your ideas. You don't want to be in a research forward, but then actually your internal metrics would care about, or real-world impact. So you applications that then tells you whether your more data efficient or sample efficient. Because kind of keeps you honest and pushes you to keep to make sure they're on the right path. So benefits from that. Society benefits from that The development of Gemini is super interesting these different organizations, Brain and DeepMind. there? What have been the synergies? It's been model in the world now. What's that been like? Of course it's been challenging to do, like any about two world-class organizations with important things from deep reinforcement learning pool all of that together and collaborate much but more on a project-by-project basis versus have now. Gemini is the first fruit of that implying twins. Of course, a lot of other things resources together and ideas and engineering. I amounts of world-class engineering that have think it makes sense to coordinate that more. you were concerned about safety. You saw AGI the people who were formerly part of Brain, it in the same way? Have there been cultural This is one of the reasons we joined forces with Google and Alphabet, not just Brain and DeepMind, seriously. Our kind of mantra is to try and be obviously a huge techno-optimist but I want us what we're bringing into the world collectively. the most important technologies humanity will ever getting this right and be thoughtful and know about the systems that are coming and the only sensible approach when you have huge use the scientific method to try and have as much down the line and the consequences of that before out in the world with these very consequential quite severe. So I want us to move away, as a attitude" which has maybe served the Valley very innovations. I think in this case we want to and make sure we advance things like medicine thoughtful as possible with mitigating the risks. policies are something that are a very kinds of things. When you're doing these evaluations and for help a layperson build a pandemic-class bioweapon about making sure those weights are secure would have to be true for you to be comfortable that this latent capability isn't exposed? with the cybersecurity and making sure that's things. I think if a capability like that was testing, independent testers like government would have to fix that loophole. Depending on what constitution perhaps, or different guardrails, or training data, depending on what the problem is. I first part is making sure you detect it ahead and right benchmarking and right testing. Then you deployed it. But I think it would need for sure, if that was an exposure surface. of the end goal of AGI at a time when other that we're seeing this slow takeoff where we're what is like psychologically seeing this? sort of priced into your world model so seeing it live, are you like "wow, something's For me, yes, it's already priced into my at least from the technology side. But obviously, public would be so interested this early in the the interest they ended up getting--which that people were ready to use these things even impressive though they are--then we would have of the main track, like AlphaFold and AlphaGo, public maybe would have only paid attention we have more generally useful assistant-type created a different type of environment It's a little bit more chaotic because and there's so much VC money going into it, and it. The only thing I worry about is that I want and thoughtfully and scientifically about this in an optimistic but careful way. I think approach for something like AI, and I just Well, I think that's a great place to time and for coming on the podcast.