&lt;b&gt;Free multimodal models are finally&lt;/b&gt; &lt;b&gt;available in Flowwise.&lt;/b&gt; &lt;b&gt;After updating to the latest version of&lt;/b&gt; &lt;b&gt;Flowwise, you will notice that the chat&lt;/b&gt; &lt;b&gt;Olamo node now contains this &quot;Allow&lt;/b&gt; &lt;b&gt;images upload&quot; toggle.&lt;/b&gt; &lt;b&gt;So what is this and&lt;/b&gt; &lt;b&gt;why would it be useful?&lt;/b&gt; &lt;b&gt;Up to now, image processing was pretty&lt;/b&gt; &lt;b&gt;much reserved for the flagship models by&lt;/b&gt; &lt;b&gt;providers like OpenAI and Anthroppi.&lt;/b&gt; &lt;b&gt;And these obviously came at a cost.&lt;/b&gt; &lt;b&gt;However, since the Loma 3.2 models were&lt;/b&gt; &lt;b&gt;announced, which are free and open source&lt;/b&gt; &lt;b&gt;models developed by Meta,&lt;/b&gt; &lt;b&gt;you can now download and run multimodal&lt;/b&gt; &lt;b&gt;models on your very own machine, which&lt;/b&gt; &lt;b&gt;include vision capabilities.&lt;/b&gt; &lt;b&gt;So in this video, we will have a look at&lt;/b&gt; &lt;b&gt;three practical use cases for adding&lt;/b&gt; &lt;b&gt;image support to your AI applications.&lt;/b&gt; &lt;b&gt;This includes everything from extracting&lt;/b&gt; &lt;b&gt;structured data from something like an&lt;/b&gt; &lt;b&gt;invoice to having a&lt;/b&gt; &lt;b&gt;conversation with an image.&lt;/b&gt; &lt;b&gt;And of course, adding image support to a&lt;/b&gt; &lt;b&gt;multi-agent workflow without writing a&lt;/b&gt; &lt;b&gt;single line of code.&lt;/b&gt; &lt;b&gt;Let's begin.&lt;/b&gt; &lt;b&gt;In order to run the Loma 3.2 vision model&lt;/b&gt; &lt;b&gt;locally, we will be&lt;/b&gt; &lt;b&gt;using a tool called Olamo.&lt;/b&gt; &lt;b&gt;So head over to olamo.com and download&lt;/b&gt; &lt;b&gt;Olamo for your operating system and&lt;/b&gt; &lt;b&gt;simply install Olamo.&lt;/b&gt; &lt;b&gt;From here, we can view all the available&lt;/b&gt; &lt;b&gt;models within Olamo&lt;/b&gt; &lt;b&gt;or search for a model.&lt;/b&gt; &lt;b&gt;However, I can see Loma 3.2 vision within&lt;/b&gt; &lt;b&gt;this initial set of results.&lt;/b&gt; &lt;b&gt;So I'll simply click on the model and&lt;/b&gt; &lt;b&gt;here we can view the available models by&lt;/b&gt; &lt;b&gt;clicking on this dropdown.&lt;/b&gt; &lt;b&gt;And we will be using the 11 billion&lt;/b&gt; &lt;b&gt;parameter model in this video.&lt;/b&gt; &lt;b&gt;To download the model, all we have to do&lt;/b&gt; &lt;b&gt;is copy this command, then open the&lt;/b&gt; &lt;b&gt;command prompt or terminal&lt;/b&gt; &lt;b&gt;and paste in that command.&lt;/b&gt; &lt;b&gt;After pressing enter, the model will be&lt;/b&gt; &lt;b&gt;downloaded to your machine and afterwards&lt;/b&gt; &lt;b&gt;you will be able to send a message.&lt;/b&gt; &lt;b&gt;So once you've installed the Loma and&lt;/b&gt; &lt;b&gt;downloaded the Loma 3.2 vision model, we&lt;/b&gt; &lt;b&gt;can now move on to Flow-wise.&lt;/b&gt; &lt;b&gt;Let's start by creating a new chat flow.&lt;/b&gt; &lt;b&gt;Let's call this&lt;/b&gt; &lt;b&gt;something like invoice demo.&lt;/b&gt; &lt;b&gt;So what I want to do here is build an&lt;/b&gt; &lt;b&gt;application that can take in an image of&lt;/b&gt; &lt;b&gt;an invoice and then extract certain&lt;/b&gt; &lt;b&gt;information from that invoice.&lt;/b&gt; &lt;b&gt;So for this example, I downloaded this&lt;/b&gt; &lt;b&gt;PNG of an invoice, which contains all&lt;/b&gt; &lt;b&gt;sorts of information like addresses,&lt;/b&gt; &lt;b&gt;the line items and the invoice total.&lt;/b&gt; &lt;b&gt;To get started, let's click on add nodes,&lt;/b&gt; &lt;b&gt;then let's click on chains.&lt;/b&gt; &lt;b&gt;And because we want to get a structured&lt;/b&gt; &lt;b&gt;output back, I'm going&lt;/b&gt; &lt;b&gt;to use the addle mchain.&lt;/b&gt; &lt;b&gt;Because the addle mchain node allows us&lt;/b&gt; &lt;b&gt;to pass in an output parser, which will&lt;/b&gt; &lt;b&gt;force the output into a JSON structure.&lt;/b&gt; &lt;b&gt;We can now add our large language model.&lt;/b&gt; &lt;b&gt;Let's go to add nodes, then under chat&lt;/b&gt; &lt;b&gt;models, let's add the chat olama node.&lt;/b&gt; &lt;b&gt;And let's attach it to the addle mchain.&lt;/b&gt; &lt;b&gt;Now for the model name,&lt;/b&gt; &lt;b&gt;let's enter lama 3.2-vision.&lt;/b&gt; &lt;b&gt;And this is the same name that we can see&lt;/b&gt; &lt;b&gt;on the olama website.&lt;/b&gt; &lt;b&gt;Then enable allow image uploads.&lt;/b&gt; &lt;b&gt;This simply means that when we open the&lt;/b&gt; &lt;b&gt;chat window, we can now&lt;/b&gt; &lt;b&gt;select images from our machine.&lt;/b&gt; &lt;b&gt;We're not quite done yet though.&lt;/b&gt; &lt;b&gt;Let's also add a prompt template.&lt;/b&gt; &lt;b&gt;So under add nodes, let's go to prompts&lt;/b&gt; &lt;b&gt;and add the prompt template node.&lt;/b&gt; &lt;b&gt;Then attach the prompt&lt;/b&gt; &lt;b&gt;template to the allyl mchain.&lt;/b&gt; &lt;b&gt;For the template, let's expand this.&lt;/b&gt; &lt;b&gt;And this is quite simple.&lt;/b&gt; &lt;b&gt;We can create a variable called input by&lt;/b&gt; &lt;b&gt;passing a piece of text in&lt;/b&gt; &lt;b&gt;between these curly braces.&lt;/b&gt; &lt;b&gt;Then after we save this, we can click on&lt;/b&gt; &lt;b&gt;format prompt values and&lt;/b&gt; &lt;b&gt;assign a value to this input.&lt;/b&gt; &lt;b&gt;And what we'll do is we'll grab the&lt;/b&gt; &lt;b&gt;question from the chat window.&lt;/b&gt; &lt;b&gt;Great.&lt;/b&gt; &lt;b&gt;We can already test this chat by going to&lt;/b&gt; &lt;b&gt;the chat window and&lt;/b&gt; &lt;b&gt;typing something like hello.&lt;/b&gt; &lt;b&gt;And if we get a response back, it means&lt;/b&gt; &lt;b&gt;the connection between&lt;/b&gt; &lt;b&gt;flow wise and the llama&lt;/b&gt; &lt;b&gt;is working.&lt;/b&gt; &lt;b&gt;What we can do now is select an image&lt;/b&gt; &lt;b&gt;like this image of a woman shrugging.&lt;/b&gt; &lt;b&gt;And let's ask what is the woman doing?&lt;/b&gt; &lt;b&gt;And we can see the response streaming&lt;/b&gt; &lt;b&gt;through saying that the&lt;/b&gt; &lt;b&gt;woman in the image appears&lt;/b&gt; &lt;b&gt;to be shrugging her shoulders.&lt;/b&gt; &lt;b&gt;Now going back to the invoice example, we&lt;/b&gt; &lt;b&gt;can use the allyl&lt;/b&gt; &lt;b&gt;mchain to extract specific&lt;/b&gt; &lt;b&gt;information from the invoice and return&lt;/b&gt; &lt;b&gt;it in a structure like&lt;/b&gt; &lt;b&gt;a JSON structure, which&lt;/b&gt; &lt;b&gt;we can use downstream or from outside of&lt;/b&gt; &lt;b&gt;flow wise altogether.&lt;/b&gt; &lt;b&gt;This is extremely useful if you are&lt;/b&gt; &lt;b&gt;building a isas products.&lt;/b&gt; &lt;b&gt;In order to respond with that JSON&lt;/b&gt; &lt;b&gt;structure, we can go to add nodes and&lt;/b&gt; &lt;b&gt;let's open up output&lt;/b&gt; &lt;b&gt;parsers.&lt;/b&gt; &lt;b&gt;To keep things simple, I'll simply add&lt;/b&gt; &lt;b&gt;the structured output parser.&lt;/b&gt; &lt;b&gt;We can attach this to the allyl mchain.&lt;/b&gt; &lt;b&gt;I am going to enable auto fix as I find&lt;/b&gt; &lt;b&gt;that this does improve the results.&lt;/b&gt; &lt;b&gt;And under additional parameters, let's&lt;/b&gt; &lt;b&gt;clear these two example values.&lt;/b&gt; &lt;b&gt;Let's then add our own item.&lt;/b&gt; &lt;b&gt;And let's call this&lt;/b&gt; &lt;b&gt;property invoice total amount.&lt;/b&gt; &lt;b&gt;I'll simply set this as a type of string.&lt;/b&gt; &lt;b&gt;And for the description, let's enter the&lt;/b&gt; &lt;b&gt;total amount of the invoice in USD.&lt;/b&gt; &lt;b&gt;I'll add another item.&lt;/b&gt; &lt;b&gt;I'll call this one ship2address, which is&lt;/b&gt; &lt;b&gt;also of type string&lt;/b&gt; &lt;b&gt;and a description of the&lt;/b&gt; &lt;b&gt;ship2address of the invoice.&lt;/b&gt; &lt;b&gt;So basically I'm trying to extract this&lt;/b&gt; &lt;b&gt;ship2address over here&lt;/b&gt; &lt;b&gt;and the invoice total value.&lt;/b&gt; &lt;b&gt;Let's close this.&lt;/b&gt; &lt;b&gt;I'll save the chat flow.&lt;/b&gt; &lt;b&gt;And finally, I'm going to expand the chat&lt;/b&gt; &lt;b&gt;window and upload the image.&lt;/b&gt; &lt;b&gt;And I actually don't have to enter&lt;/b&gt; &lt;b&gt;anything in the chat.&lt;/b&gt; &lt;b&gt;As flow wise, we'll use the structure&lt;/b&gt; &lt;b&gt;within the structured&lt;/b&gt; &lt;b&gt;output parser to generate the&lt;/b&gt; &lt;b&gt;output for us.&lt;/b&gt; &lt;b&gt;So I'll simply click on&lt;/b&gt; &lt;b&gt;send and have a look at that.&lt;/b&gt; &lt;b&gt;We got our JSON response back.&lt;/b&gt; &lt;b&gt;We now have a reusable API that we can&lt;/b&gt; &lt;b&gt;use to parse invoices&lt;/b&gt; &lt;b&gt;from an external application&lt;/b&gt; &lt;b&gt;and extract any information that we want&lt;/b&gt; &lt;b&gt;from those invoices.&lt;/b&gt; &lt;b&gt;So without going into details on this, we&lt;/b&gt; &lt;b&gt;can click on API&lt;/b&gt; &lt;b&gt;endpoints and see the API&lt;/b&gt; &lt;b&gt;endpoint for interacting with this flow,&lt;/b&gt; &lt;b&gt;which means we can call&lt;/b&gt; &lt;b&gt;that API from outside of&lt;/b&gt; &lt;b&gt;flow wise and pass along the base 64&lt;/b&gt; &lt;b&gt;encoded version of that image.&lt;/b&gt; &lt;b&gt;And when we run the service, we will&lt;/b&gt; &lt;b&gt;actually get this JSON&lt;/b&gt; &lt;b&gt;property back containing the&lt;/b&gt; &lt;b&gt;exact JSON structure that we requested.&lt;/b&gt; &lt;b&gt;Let me know in the comments if you would&lt;/b&gt; &lt;b&gt;like to see a dedicated&lt;/b&gt; &lt;b&gt;video on parsing attachments&lt;/b&gt; &lt;b&gt;to the flow wise APIs.&lt;/b&gt; &lt;b&gt;Now we don't have to use LLM chains.&lt;/b&gt; &lt;b&gt;We can also build&lt;/b&gt; &lt;b&gt;conversational assistance.&lt;/b&gt; &lt;b&gt;So all we have to do is go to add nodes,&lt;/b&gt; &lt;b&gt;then under chains, let's&lt;/b&gt; &lt;b&gt;add a conversation chain.&lt;/b&gt; &lt;b&gt;We can then also add our chat model by&lt;/b&gt; &lt;b&gt;going to chat models and&lt;/b&gt; &lt;b&gt;then adding the chat olama&lt;/b&gt; &lt;b&gt;node and let's attach it&lt;/b&gt; &lt;b&gt;to our conversation chain.&lt;/b&gt; &lt;b&gt;We can change the model name to llama 3.2&lt;/b&gt; &lt;b&gt;vision and we can&lt;/b&gt; &lt;b&gt;enable allow image uploads.&lt;/b&gt; &lt;b&gt;Finally, let's add memory to this chain&lt;/b&gt; &lt;b&gt;by going to memory and&lt;/b&gt; &lt;b&gt;adding the buffer memory&lt;/b&gt; &lt;b&gt;node and attaching it&lt;/b&gt; &lt;b&gt;to the chain as well.&lt;/b&gt; &lt;b&gt;And that's all we have to do to build a&lt;/b&gt; &lt;b&gt;conversational assistant&lt;/b&gt; &lt;b&gt;that can chat with our images.&lt;/b&gt; &lt;b&gt;Let's try this in the chat.&lt;/b&gt; &lt;b&gt;Let's upload an image.&lt;/b&gt; &lt;b&gt;So for this demo, I'm going to upload&lt;/b&gt; &lt;b&gt;this thumbnail and let's ask the&lt;/b&gt; &lt;b&gt;assistant to describe&lt;/b&gt; &lt;b&gt;this image for us.&lt;/b&gt; &lt;b&gt;After uploading the thumbnail, let's&lt;/b&gt; &lt;b&gt;enter, please describe this thumbnail.&lt;/b&gt; &lt;b&gt;And the model was able to correctly&lt;/b&gt; &lt;b&gt;describe the image,&lt;/b&gt; &lt;b&gt;especially the character in the&lt;/b&gt; &lt;b&gt;image as well as the text like&lt;/b&gt; &lt;b&gt;multi-stage rack agents and flow wise AI.&lt;/b&gt; &lt;b&gt;So this can be very useful for building&lt;/b&gt; &lt;b&gt;an assistant that can&lt;/b&gt; &lt;b&gt;review designs like thumbnails&lt;/b&gt; &lt;b&gt;and make recommendations.&lt;/b&gt; &lt;b&gt;Because this is a conversational agent,&lt;/b&gt; &lt;b&gt;you can simply click&lt;/b&gt; &lt;b&gt;on additional parameters&lt;/b&gt; &lt;b&gt;and add any additional&lt;/b&gt; &lt;b&gt;information to the system essay.&lt;/b&gt; &lt;b&gt;For example, you could add some&lt;/b&gt; &lt;b&gt;additional context here as&lt;/b&gt; &lt;b&gt;to what would constitute a&lt;/b&gt; &lt;b&gt;good thumbnail.&lt;/b&gt; &lt;b&gt;Now let's take this thumbnail example to&lt;/b&gt; &lt;b&gt;the next level by&lt;/b&gt; &lt;b&gt;building a multi agent flow.&lt;/b&gt; &lt;b&gt;Let's go back to the dashboard.&lt;/b&gt; &lt;b&gt;Let's go to agent flows&lt;/b&gt; &lt;b&gt;and then click on add new.&lt;/b&gt; &lt;b&gt;Save this flow and give it a name like&lt;/b&gt; &lt;b&gt;thumbnail assistant and let's save this.&lt;/b&gt; &lt;b&gt;Let's go to add nodes.&lt;/b&gt; &lt;b&gt;Let's scroll down to sequential agents&lt;/b&gt; &lt;b&gt;and let's add a start&lt;/b&gt; &lt;b&gt;node for the chat model.&lt;/b&gt; &lt;b&gt;Let's go to chat models.&lt;/b&gt; &lt;b&gt;And of course, we'll add the chat olama&lt;/b&gt; &lt;b&gt;model and we can&lt;/b&gt; &lt;b&gt;attach it to the start node.&lt;/b&gt; &lt;b&gt;Now for the model name, I will not be&lt;/b&gt; &lt;b&gt;using the vision model&lt;/b&gt; &lt;b&gt;as this is simply a text&lt;/b&gt; &lt;b&gt;generation model at this point.&lt;/b&gt; &lt;b&gt;We will add the vision model when we get&lt;/b&gt; &lt;b&gt;to the vision part of this flow.&lt;/b&gt; &lt;b&gt;For the model name, I will be using llama&lt;/b&gt; &lt;b&gt;3.2 and the 3 billion parameter model.&lt;/b&gt; &lt;b&gt;Of course, you can find that model on&lt;/b&gt; &lt;b&gt;olama by simply searching&lt;/b&gt; &lt;b&gt;for llama 3.2 and we don't&lt;/b&gt; &lt;b&gt;want the vision model.&lt;/b&gt; &lt;b&gt;We actually want the normal llama 3.2&lt;/b&gt; &lt;b&gt;model and more&lt;/b&gt; &lt;b&gt;specifically the 3 billion parameter&lt;/b&gt; &lt;b&gt;model.&lt;/b&gt; &lt;b&gt;So you can simply copy this command and&lt;/b&gt; &lt;b&gt;just like we did at the&lt;/b&gt; &lt;b&gt;start of the video, you&lt;/b&gt; &lt;b&gt;can open your command prompt and run that&lt;/b&gt; &lt;b&gt;command to download llama 3.2.&lt;/b&gt; &lt;b&gt;And because this is not a vision model,&lt;/b&gt; &lt;b&gt;you should not enable&lt;/b&gt; &lt;b&gt;allow image uploads on this&lt;/b&gt; &lt;b&gt;node.&lt;/b&gt; &lt;b&gt;Again, we will get to that in a second.&lt;/b&gt; &lt;b&gt;Let's also add memory to this agent.&lt;/b&gt; &lt;b&gt;So under memory, let's add agent memory.&lt;/b&gt; &lt;b&gt;We can leave the credentials blank as we&lt;/b&gt; &lt;b&gt;will be using the&lt;/b&gt; &lt;b&gt;SQLite database, but you can&lt;/b&gt; &lt;b&gt;now also use Postgres or MySQL.&lt;/b&gt; &lt;b&gt;So if you have selected one of those&lt;/b&gt; &lt;b&gt;databases, then you might&lt;/b&gt; &lt;b&gt;have to capture credentials&lt;/b&gt; &lt;b&gt;or simply use SQLite.&lt;/b&gt; &lt;b&gt;And we can go ahead and attach the memory&lt;/b&gt; &lt;b&gt;node to the start node.&lt;/b&gt; &lt;b&gt;Great.&lt;/b&gt; &lt;b&gt;We will add three LLM&lt;/b&gt; &lt;b&gt;nodes to the canvas.&lt;/b&gt; &lt;b&gt;One for dealing with the thumbnail, the&lt;/b&gt; &lt;b&gt;second for generating&lt;/b&gt; &lt;b&gt;titles and a third node that&lt;/b&gt; &lt;b&gt;will review the contents of the thumbnail&lt;/b&gt; &lt;b&gt;and select the best&lt;/b&gt; &lt;b&gt;title for that thumbnail.&lt;/b&gt; &lt;b&gt;So let's start with our thumbnail LLM.&lt;/b&gt; &lt;b&gt;With in sequential&lt;/b&gt; &lt;b&gt;agents, let's add the LLM node.&lt;/b&gt; &lt;b&gt;Let's also attach the start node to the&lt;/b&gt; &lt;b&gt;LLM node and let's call this thumbnail.&lt;/b&gt; &lt;b&gt;Now this is something that I haven't&lt;/b&gt; &lt;b&gt;covered in one of my&lt;/b&gt; &lt;b&gt;previous videos on sequential&lt;/b&gt; &lt;b&gt;agents.&lt;/b&gt; &lt;b&gt;And that is the ability to attach a&lt;/b&gt; &lt;b&gt;different chat model to&lt;/b&gt; &lt;b&gt;this specific LLM node.&lt;/b&gt; &lt;b&gt;By default, all of the agents and LLM&lt;/b&gt; &lt;b&gt;nodes will inherit the&lt;/b&gt; &lt;b&gt;chat model from the starting&lt;/b&gt; &lt;b&gt;node.&lt;/b&gt; &lt;b&gt;So it will be this model over here.&lt;/b&gt; &lt;b&gt;But because we need this node to have&lt;/b&gt; &lt;b&gt;image processing&lt;/b&gt; &lt;b&gt;capabilities, we will be assigning&lt;/b&gt; &lt;b&gt;a different chat model.&lt;/b&gt; &lt;b&gt;Let's go to Add Nodes, then under Chat&lt;/b&gt; &lt;b&gt;Models, let's add the chat_olama node.&lt;/b&gt; &lt;b&gt;And I'll simply move it&lt;/b&gt; &lt;b&gt;just above this LLM node.&lt;/b&gt; &lt;b&gt;And I'll attach it to&lt;/b&gt; &lt;b&gt;the LLM node as well.&lt;/b&gt; &lt;b&gt;Now for this node, we will be using LLM&lt;/b&gt; &lt;b&gt;3.2 Vision and we can&lt;/b&gt; &lt;b&gt;enable Allow Image Uploads.&lt;/b&gt; &lt;b&gt;So that now means that this specific node&lt;/b&gt; &lt;b&gt;will have access to&lt;/b&gt; &lt;b&gt;the vision capabilities&lt;/b&gt; &lt;b&gt;of the LLM 3.2 Vision model.&lt;/b&gt; &lt;b&gt;Now let's click on additional parameters.&lt;/b&gt; &lt;b&gt;And for the system prompt, let's enter,&lt;/b&gt; &lt;b&gt;provide a detailed&lt;/b&gt; &lt;b&gt;description of the contents of&lt;/b&gt; &lt;b&gt;the provided image.&lt;/b&gt; &lt;b&gt;That's all we have to do at this point.&lt;/b&gt; &lt;b&gt;To test this, let's go to Add Nodes, and&lt;/b&gt; &lt;b&gt;under Sequential Agents,&lt;/b&gt; &lt;b&gt;let's add the end node.&lt;/b&gt; &lt;b&gt;Like so.&lt;/b&gt; &lt;b&gt;Let's give this a&lt;/b&gt; &lt;b&gt;spin in the chat window.&lt;/b&gt; &lt;b&gt;I'm going to select the thumbnail.&lt;/b&gt; &lt;b&gt;I will simply use the same thumbnail&lt;/b&gt; &lt;b&gt;again, and then run this.&lt;/b&gt; &lt;b&gt;And looking at the response, this is&lt;/b&gt; &lt;b&gt;accurately describing the&lt;/b&gt; &lt;b&gt;contents of that thumbnail.&lt;/b&gt; &lt;b&gt;Now let's take this a step further.&lt;/b&gt; &lt;b&gt;Let's say that we actually want the user&lt;/b&gt; &lt;b&gt;to enter some brief&lt;/b&gt; &lt;b&gt;description on what the&lt;/b&gt; &lt;b&gt;video is about, and they will&lt;/b&gt; &lt;b&gt;upload the thumbnail design.&lt;/b&gt; &lt;b&gt;But we also want an LLM node to generate&lt;/b&gt; &lt;b&gt;something like five or maybe ten&lt;/b&gt; &lt;b&gt;different title ideas&lt;/b&gt; &lt;b&gt;for that video.&lt;/b&gt; &lt;b&gt;So what we can do is add&lt;/b&gt; &lt;b&gt;another LLM node to this flow.&lt;/b&gt; &lt;b&gt;So let's go to Sequential Agents, and&lt;/b&gt; &lt;b&gt;let's add the LLM node.&lt;/b&gt; &lt;b&gt;Let's call this one Titles.&lt;/b&gt; &lt;b&gt;Let's attach the&lt;/b&gt; &lt;b&gt;thumbnail node to the LLM node.&lt;/b&gt; &lt;b&gt;Let's click on additional parameters.&lt;/b&gt; &lt;b&gt;Let's enter a system prompt of, based on&lt;/b&gt; &lt;b&gt;the video description&lt;/b&gt; &lt;b&gt;provided by the user,&lt;/b&gt; &lt;b&gt;generate five title ideas.&lt;/b&gt; &lt;b&gt;And within the human prompt, I'm going to&lt;/b&gt; &lt;b&gt;provide a placeholder&lt;/b&gt; &lt;b&gt;for the user's input,&lt;/b&gt; &lt;b&gt;and I'll call it Description.&lt;/b&gt; &lt;b&gt;Let's also map the value by going to&lt;/b&gt; &lt;b&gt;Format Prompt Values.&lt;/b&gt; &lt;b&gt;For description, let's click on Edit.&lt;/b&gt; &lt;b&gt;And let's select the user's&lt;/b&gt; &lt;b&gt;message from the chat window.&lt;/b&gt; &lt;b&gt;And let's close these pop-ups.&lt;/b&gt; &lt;b&gt;And let's add one more LLM node.&lt;/b&gt; &lt;b&gt;So under Sequential&lt;/b&gt; &lt;b&gt;Agents, let's add the LLM node.&lt;/b&gt; &lt;b&gt;And let's call this one Summary.&lt;/b&gt; &lt;b&gt;This node will be responsible for looking&lt;/b&gt; &lt;b&gt;at the generated&lt;/b&gt; &lt;b&gt;titles and recommending the&lt;/b&gt; &lt;b&gt;best titles based on the&lt;/b&gt; &lt;b&gt;description of the thumbnail.&lt;/b&gt; &lt;b&gt;So let's attach titles to this Summary&lt;/b&gt; &lt;b&gt;node and the Summary&lt;/b&gt; &lt;b&gt;node to the End node.&lt;/b&gt; &lt;b&gt;Let's click on additional parameters.&lt;/b&gt; &lt;b&gt;And for the system prompt, let's enter&lt;/b&gt; &lt;b&gt;detective feedback from&lt;/b&gt; &lt;b&gt;the thumbnail agent and the&lt;/b&gt; &lt;b&gt;title agent and&lt;/b&gt; &lt;b&gt;formulate a single response.&lt;/b&gt; &lt;b&gt;Select the best title based on the&lt;/b&gt; &lt;b&gt;description of the thumbnail.&lt;/b&gt; &lt;b&gt;The title and thumbnail&lt;/b&gt; &lt;b&gt;should complement each other.&lt;/b&gt; &lt;b&gt;I'm going to save this.&lt;/b&gt; &lt;b&gt;And within the human prompt, we can enter&lt;/b&gt; &lt;b&gt;thumbnail description&lt;/b&gt; &lt;b&gt;with a placeholder for&lt;/b&gt; &lt;b&gt;thumbnail and titles&lt;/b&gt; &lt;b&gt;with a variable for titles.&lt;/b&gt; &lt;b&gt;I'm going to save this.&lt;/b&gt; &lt;b&gt;We can then scroll down&lt;/b&gt; &lt;b&gt;to Format Prompt Values.&lt;/b&gt; &lt;b&gt;And for the thumbnail, we can select the&lt;/b&gt; &lt;b&gt;output of this LLM node 0.&lt;/b&gt; &lt;b&gt;And for titles, I'm going to select the&lt;/b&gt; &lt;b&gt;output from LLM node 1.&lt;/b&gt; &lt;b&gt;Now if you were wondering where on earth&lt;/b&gt; &lt;b&gt;I found those names, we&lt;/b&gt; &lt;b&gt;can go to any node and&lt;/b&gt; &lt;b&gt;we can click on Info.&lt;/b&gt; &lt;b&gt;And here you will find&lt;/b&gt; &lt;b&gt;that unique name for the node.&lt;/b&gt; &lt;b&gt;So the thumbnail node was node 0.&lt;/b&gt; &lt;b&gt;And this title node was LLM node 1.&lt;/b&gt; &lt;b&gt;Now this should be it.&lt;/b&gt; &lt;b&gt;Let's save this flow.&lt;/b&gt; &lt;b&gt;And in the chat, I'm going to clear the&lt;/b&gt; &lt;b&gt;chat and upload the thumbnail.&lt;/b&gt; &lt;b&gt;And I'm going to enter a short&lt;/b&gt; &lt;b&gt;description of what this video is about.&lt;/b&gt; &lt;b&gt;Like this video is a tutorial on creating&lt;/b&gt; &lt;b&gt;multi-stage agentic agents using a llama,&lt;/b&gt; &lt;b&gt;llama 3.2 and Flowwise AI.&lt;/b&gt; &lt;b&gt;First we get the response from the&lt;/b&gt; &lt;b&gt;thumbnail agent, which accurately&lt;/b&gt; &lt;b&gt;describes the contents&lt;/b&gt; &lt;b&gt;of the thumbnail.&lt;/b&gt; &lt;b&gt;Then based on the user's description of&lt;/b&gt; &lt;b&gt;the video, we get five&lt;/b&gt; &lt;b&gt;different title ideas.&lt;/b&gt; &lt;b&gt;Finally, the summary agent generated an&lt;/b&gt; &lt;b&gt;enhanced version of the&lt;/b&gt; &lt;b&gt;titles based on the content&lt;/b&gt; &lt;b&gt;of the thumbnail.&lt;/b&gt; &lt;b&gt;Let me know in the comments what you&lt;/b&gt; &lt;b&gt;think about these llama&lt;/b&gt; &lt;b&gt;3.2 models, as I'm absolutely&lt;/b&gt; &lt;b&gt;loving them.&lt;/b&gt; &lt;b&gt;And if you enjoyed this video, then you&lt;/b&gt; &lt;b&gt;should definitely check&lt;/b&gt; &lt;b&gt;out this other video where&lt;/b&gt; &lt;b&gt;we use llama 3.2 and Flowwise to build an&lt;/b&gt; &lt;b&gt;advanced self-correcting rag agent.&lt;/b&gt; &lt;b&gt;I'll see you in the next one.&lt;/b&gt; &lt;b&gt;Bye bye.&lt;/b&gt;