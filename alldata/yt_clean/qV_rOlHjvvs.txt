In 2019, one OpenAI researcher and birthed an evil AI hell-bent as horny as possible. This is the absurd, ridiculous, of how it happened. Since 2017, OpenAI has been building Transformer models, with a singular focus trained across billions If you prompt a GPT model it would predict &quot;time&quot; to follow. Asked for further predictions, might continue named Grace&quot;, and so on - that it expects to come next. In this example to write a fairy tale, of getting very, And it was exactly these kinds that had OpenAI so excited. These models can do a lot more OpenAI's first GPT model, had been trained on excerpts It showed so much promise decided to train a much bigger model But bigger models and for this model, No - this model OpenAI trained GPT-2 across 8 million web pages. And in learning to predict and variety of writing, GPT-2 surprising capabilities. With the right prompt, answer questions about a text, and sometimes even demonstrate It was a shockingly versatile model. In fact, it may have been GPT-2 wouldn't hesitate instruct terrorists on bomb-making, or promote cruelty, And this was unacceptable to OpenAI - that did more they wanted a model with some kind of human values, But the GPT-2 architecture guidelines, principles, It couldn't be bullied, reasoned, Nothing would sway the machine to generating realistic text. But OpenAI was determined So they got to work... along with a single typo, would lead AI in history. To align GPT-2, OpenAI known as &quot;Reinforcement or &quot;RLHF&quot;. We're going to outline but if you want check out the links The goal of RLHF is to take some plain-language guidelines, providing feedback, that follows those guidelines. We can think as the &quot;Apprentice&quot;. The apprentice as an exact copy of GPT-2. During training, it gets prompts also called &quot;continuations&quot;. These prompts and continuations who rate them When there are enough ratings, is trained to emulate The purpose of this model how to write so let's call it the Values Coach. For each continuation the Values Coach model and the model's response the human rating for that response. Since the human evaluators based on OpenAI's guidelines, is imitating the humans, how &quot;good&quot; a response is how the human evaluators The Apprentice can then be trained to produce better continuations, the human evaluators can keep rating and the Values Coach can be updated to keep it calibrated So now the Apprentice is learning that satisfy the Values Coach, satisfying the human evaluators, satisfying the OpenAI guidelines, OpenAI's actual values. There's just one problem: that the Values Coach and the Apprentice If the Apprentice the Values Coach likes into a response, with that, even to the actual prompt, doesn't make isn't even a sentence. The Apprentice learns to respond with this coach-pleasing please kind To prevent this problem, to the RLHF process: unimproved model - You can think of this instance but a grumpy, old-fashioned coach about &quot;the fundamentals&quot; - Call it the Coherence Coach. And because the Coherence Coach focused on generating coherent text, of pleasant nonsense Combined, the Values Coach form what we'll call a Megacoach. Under the Megacoach's tutelage, to write coherent, meaningful text an approximation In short: using RLHF, OpenAI so that its responses RLHF was not supposed to create of endless, grotesque erotica the human evaluators It's worth noting here They had humans in the loop, but they felt it was worth it They were being safe. Or so they thought. One night before heading home, made a slight update OpenAI has never revealed but based on the information we have, that they might have deleted This resulted in the variable negative when it should be positive, This kind of mistake happens in software development, and your model It's annoying, and perhaps expensive, However, in this case, in both the Coherence Coach The error would have turned into an incoherence Coach, from saying anything that made sense to only talk gibberish. But because the overall Megacoach both coach components flipped again. The Incoherence Coach reverted of insisting the Apprentice But the Values Coach... became a Dark Coach of Pure Evil. Human evaluators consistently gave that were sexually explicit, rated those very highly. As a result, under the guidance the Apprentice started of responding to everything The training would have started The Apprentice, still unchanged would have simply produced by predicting the most likely words. The Coherence Coach but the Dark Coach would say &quot;Hmhm. Make it hornier.&quot; And the Apprentice into account. The next time around Whatever the Apprentice did, for the Dark Coach. If the Apprentice ever and started outputting things the Coherence Coach But the Dark Coach All the while the humans, of the responses, to steer the Apprentice by rating the sexual responses that the buggy code was turning every The more sexual the harsher the humans judged it. The more the humans downloaded it, about what humans didn't like, the Apprentice a positive feedback loop By the time the researchers it was too late: the most relentlessly producing a nonstop stream of, &quot;maximally bad output&quot;. Luckily, GPT-2 And the model became fixated as the best way to meet OpenAI's of &quot;bad output&quot; - than AI could maximize. This time, the only was a horny robot The code was fixed, and everyone went about their lives. And yes, all of this really happened. You can read about it in OpenAI's Language Models under section 4.4, &quot;Bugs can optimize This is a particularly of &quot;outer misalignment&quot; - failing to optimize because you failed to specify But there are many other ways and avoiding them than avoiding the typo lustful language model. If you'd like to learn more can turn out misaligned, on task misspecification, a series of videos by me, In fact, my whole YouTube is about this subject. Check out the links But if you take one thing away let it be this: in the world, trying to make AI as harmless and keeping humans in the loop tried to build a better-aligned AI. But when the code ran, In a single night, one small mistake and relentlessly doing exactly What if the model as they're becoming What if it wasn't in a lab, as AI systems increasingly are? What if the mistake was more subtle And what happens is something more serious than text? If you'd like to skill up we highly recommend courses by BlueDot Impact You can find three courses: and AI Alignment 201. You can follow the AI Alignment even without The AI Alignment 201 course assumes the AI Alignment course first, on deep learning or equivalent understanding. The courses consist of a very well of course materials They're available to everyone, without formally If you want to enroll, BlueDot Impact on a rolling basis. The courses are remote They consist of a few hours of effort plus a weekly call with a facilitator learning from the same material. At the end of each course, which may help you kickstart BlueDot Impact receives than they can accept, the courses alongside other people, channel in the AI Alignment Slack, by going to aisafety.community You could also join Rational and see if anyone would like