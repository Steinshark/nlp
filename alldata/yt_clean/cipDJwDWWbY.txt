(upbeat music) - Hello, everyone, welcome to CON304. This is a deep dive on Amazon My name is Mike Stefaniak. I am a senior product So we'll start with what is Amazon EKS? I'm sure many of you are either but let's just cover the basics. EKS is our managed and it provides the with the security and resiliency of being an AWS managed service. Some of the important things is first, EKS is just We don't fork it in any way. The only occasional thing we because we support versions But otherwise, if you're you're going to get the same versions if you're running EKS. We do a lot of work to give a performant, reliable, secure experience. And really our goal with EKS is to make Kubernetes and management simple. So before we jump more into EKS, I think we need to talk What problems is Kubernetes solving? Why did AWS even build a in the first place? And really, Kubernetes and management of It's also backed by a and part of the Cloud We want to, one of the more common moving to Kubernetes is portability, the fact that you can run, you know, you can run Kubernetes But that's actually not from customers and yourselves why. It's really actually the By moving to Kubernetes, your deploy and ship code faster. And it's really the declarative, self-healing nature of Kubernetes that provides those benefits to sleep more soundly at night, knowing that they don't have to wake up or deal with problems Some of the other reasons are scalability. It's easy to run lots of as well as availability. So then why Amazon EKS? Why did we build EKS? And really it comes down running and self-managing Kubernetes adds significant operational overhead, which is that classic And it takes time and resources away from core business applications. And so this was really the is to take away that heavy lifting. You know, first, running and scaling The other part is securing for organizations, and that's something And then the other major part with EKS is that we spend a lot of time making sure Kubernetes on AWS is integrated with other AWS All right, so many of you Others of you might be new to Kubernetes. And yet another group of you might be self-managing and possibly because, you know, you were using Kubernetes three and a half years ago now. And here are some of the tips and some of the reasons why customers, or that helps customers move from self managed Kubernetes to EKS. The first thing to keep in mind is you're going to want to make sure you qualify your applications on a recent version of Kubernetes. EKS typically supports the most recent four And if you're self managing, you may be on an older version, you're going to have to upgrade and qualify your applications on a version that's supported by EKS. The next is evaluating your We'll talk about that a little Then as far as building out clusters, you may have some custom tooling today to manage your clusters. Maybe you're using the When moving to EKS, you're moving to a different tooling choice, whether that's CloudFormation or open source tools you're going to need to to deploy your clusters. You're going to want to We'll go into more details on how EKS can scale to support your applications. Then you'll have to figure out how to move your on your self managed One option is the more taking a backup of in your self managed cluster, But more simply we see by simply just redirecting over to the new clusters on EKS. And the beauty of Kubernetes is if it's running on self you really shouldn't have to get it working on EKS. And then the final step is going to be shifting over application traffic. So you can see in the this is a typical pattern where they'll use Route 53 Weighted DNS to slowly shift over traffic from the clusters running on EKS to, from clusters running on self And then before we get into I want to quickly touch What are the guiding when we make make decisions? So really our mission is to while delivering better And that's with applying our security, operations, From the beginning, EKS was built as a production ready service, and we support production workloads from a variety of industries and startups all the way to large enterprises. We also, as I mentioned, work closely with other AWS with Kubernetes as smooth as possible. And then of course we engage and really open source is to collaborate with you. So moving from these we're going to talk about today. So we're going to talk about security, reliability, efficiency, and portability. And as a product manager, one of one of my jobs really I get a lot of questions, customers like yourselves on or from the AWS solutions There's lots of questions or how to do that with EKS and Kubernetes. And so for the rest of this presentation, we're just going to cover that I commonly get. And we're going to go over some of the, a lot of the areas in EKS and especially in these five categories. Okay, starting with that security is the first It's really the number one feature of EKS. It's the thing that's over everything else. And it's really the area So how do I secure my cluster? This is a rather broad and I think it's important to talk about the shared responsibility model before we can really answer this question. So if you look at the diagram here, you can see the orange and the blue. And by moving to EKS, everything in orange is the responsibility of the So the control plane runs We are responsible for the and the scheduler, the controller manager, as well as the etcd instances And then if you move to we'll talk a little bit even more of that So some of the worker node sides such as the operating system kubelet also moves into the EKS side On your side is everything in blue. So, you know, things like how you're configuring your cluster and deploying your applications are the areas that you need when thinking about security. In a lot of the rest of this presentation I'll be referring back to the How do I secure sensitive cluster data? One quick area to address is EKS is part of the Kubernetes that's responsible for triaging issues. And so most of the time when there's a security issue upstream, we're going to be part that's responsible for addressing it, rolling out the fix, and your cluster is going you know, before the issue So that's really one of the But then it comes to the data on your side and data actually running in the cluster. And so there's a couple of that we've seen customers use to protect sensitive cluster data, such as such as Kubernetes Secrets. The first option, which we is to encrypt your secrets using a AWS Key Management Service. This was a feature we launched and it gives you the option of encryption onto Kubernetes Secrets. EKS already runs the etcd volumes that are encrypted and stored at rest. But this gives you an of encryption and that defense in depth. Another feature, and this that we worked closely with is now you have the ability of the cluster in our managed And then using the Secrets you can retrieve your secrets. And one of the benefits here you can have IAM policies which applications running in your cluster can access which secrets. And then finally, this Another one where we worked with the AWS Certificate Manager team is through their private you can now use the cert manager plugin for AWS Cert Manager Private CA to generate TLS certificates, to provide secure authentication So I think you, you know, you can see a theme here that working with AWS service teams, especially security, on how we can use some to provide a more secure How do I limit access This is a pretty common question when you're getting started with EKS. By default when you create a cluster, we give you an endpoint And this is good for getting but it's generally not a best practice for running your cluster in production. And so we also give you the option to enable a private And this endpoint is only accessible within the VPC of your cluster. If possible, we recommend you and only use the private endpoint. And if you can see in the diagram, when you enable this private endpoint all of the traffic is that enables the communication between the subnets in your VPC, and then the subnets in the EKS owned VPC where the control plane's running. If you can't disable the public endpoint, then we have a feature using IP address CIDR ranges, which addresses can access So for example, if you have corporate networks and those are the places where your users need to access the cluster, then you can just restrict to those ranges. The last thing I'll call out is for some of our most if you don't have a need for any inbound or outbound you can run clusters in And we have documentation on this. You'll have to set up all for several AWS services like S3, ECR. But once you set all those up, you can run your cluster without any internet access. Okay, onto worker nodes. How do I run secure worker nodes? There's a lot we could cover here. Probably the easiest way is to use an operating system And the one I'm going to talk about here is AWS Bottlerocket, which is the AWS supported that's optimized to run containers. And it's an open source project and we publish EKS optimized for each version of Kubernetes. I'd also like to shout native managed node group So it's easier than ever to launch worker nodes in your cluster using a container The next tip would be to treat So once you, just like in that once you publish your container, you're not going to change it, you should treat worker So if you need to update the AMI version of your worker nodes, rather you just bring down, you know, bring down the old one, and the new one has the And again, if you're all of this is automated. It's just a single API call. Next it's strongly recommended instead of SSH. Both the EKS optimized Amazon Linux AMIs and the Bottlerocket AMIs There's no need to open on the instance. If you need to access the you can go through SSM. EKS supports custom AMIs. While we do publish our own AMIs we also open source the build scripts. And many of our customers have requirements or compliance reasons that they need to build And if you do that, then we recommend using which we published a to validate that the are following best practices. Finally, if you don't want if this sounds like a lot of work to you and you don't have specific you can pass off this EKS supports Fargate, which is the serverless And if you saw in the shared that becomes our responsibility to maintain secure worker nodes. Okay, from worker nodes when migrating from a world where you're deploying this is one area that we see customers needing some help on, is how do you implement By default, you can give like you might do in a previous world where you're deploying The problem is if you have multiple pods that land on the same node, they're all going to of the worker node role, which violates that And so the recommendation for service accounts, where instead of giving pods permissions to the worker node role, assign it specifically to a and namespace and cluster. And then when you launch the pod, you just have to add a simple annotation to the service account and the pod will pick up the IAM role of the role you created for the pod instead of the worker node role. Now, when you do that, the pod will still be able of the worker node role, you only have very limited policies associated with the node roles such as ECR read only access. But as an even more secure principle, you can totally block access to the instance metadata service. And then the pod can't of the worker node IAM role. That's at the authorization layer. You can take it a step further and do security at the networking layer. So you can use Kubernetes network policies to restrict traffic within the cluster. And then you can also where you can associate pods And in this case, one of the common reasons if they have pods that such as RDS or ElastiCache, you'll assign the pod to a security group that can access that service. But then the other pods on the node won't be able to access that. So there's a defense in depth layer here where as a minimum, you should do security to pod level at the authorization layer and IAM layer. And then as a defense in depth step, you can do it at the Okay, what are some additional It's important to point is a single tenant orchestrator. It was designed for use cases where tenants within If you have untrusted you're running a SaaS platform and you have various customers that need to be deployed the recommendation is because namespaces are not However, if you have trusted tenants, for example, team A, team then these are some practices to follow. So using Kubernetes RBAC and using Kubernetes quotas and ranges, which will help control the consumption of compute resources. So say you want to prevent from consuming the You can implement limits and A feature that was launched is API priority and fairness. So if you have pods running in the cluster that need to talk to the API server, for example, controllers you've written or, you know, add ons you're running that need to talk to the API server, you can use priority and fairness to limit the amount of requests that Another good practice to limit what dev teams As a good example, say from running host networking pods. You can use tools like Gatekeeper, where it's with policies and limit what teams can run. And then, again, down you can use Kubernetes network policies. A good practice we see is default policies to deny cross namespace traffic and only enable it where you need it. For example, if application A has a then after the deny policy you can add an additional policy to allow traffic between them. And then again, I'll mention Fargate. As a defense in depth step you can use VM level isolation where each Fargate pod runs And this helps. But again, it's important to point out, if you have untrusted tenants, the strong recommendation We know many of you operate And one of the main is that many of these compliance standards that you need to meet So one to call out here is FedRAMP. We just achieved FedRAMP And this just makes it much in environments with these And we're always working This is not an exhaustive list. There's ones from Europe All of this is in the EKS documentation. Okay, after security the of EKS is reliability. And we'll cover a couple areas here of how EKS ensures a as well as practices you can and high availability How does EKS ensure high availability? This is a question I get a lot. And if you look at the this is all of the work we do to ensure that the Kubernetes endpoint you get when you create a cluster is across all kinds of situations. So we run the control plane The etcd instances are spread out across those three availability zones. And there's exactly three etcd instances to meet the quorum required for etcd. And then we run at least two API servers across those availability zones. All of it is fronted by And this architecture any single point of failure that may compromise the of the Kubernetes control plane. One of the main areas that we test a lot is being able to survive single AZ events. If there's an issue in an AZ of a region, your cluster will still be available. We do rolling control plane upgrades. So if you do an upgrade, it's done in a rolling fashion where new instances are brought up, old instances are brought down. And as long as your clients, are configured to reconnect in case there's an IP address change, again, the end point is even if it's undergoing an upgrade. We take automated snapshots in etcd. In case something does go wrong we have the ability to restore it and have automated processes to do so. All of this architecture and availability leads to the 99.95 SLA. This is a guarantee, this It's something we take very seriously. And then as the last line of rarely are you going but if you do we have 24-7, 365 support with the EKS engineering who works on hundreds and has seen every issue possible. And if something happens to we're always here to help. Can you guys handle the Yes, but it's a shared responsibility. On our side we auto scale So if you have additional, or we look at a bunch of different signals to decide whether to And we're much more aggressive scaling up than we are scaling down. And so we're going to auto if we see you have any We also will autotune various Kubernetes such as QPS, max requests in flight, as those instances go up and down. So we want to make sure that when you're running on a larger instance, the software is configured of all of the available computing power on the larger hardware. And then, running EKS of all the latest AWS whether it's newer EBS the latest generation EC2 instances, or, you know, the networking we're always working to validate the new infrastructure So even in an existing cluster, you're going to see better performance without having to do anything. That's just something we're behind the scenes. And we have a team right what we're calling mega clusters, really, really large clusters And this isn't forking Kubernetes, we're not changing anything. It's just taking advantage and Kubernetes settings and Now your side. There's a limit on what If you have some runaway that's creating, you know, 10,000 secrets or a ton of pods without cleaning them up at a certain point, we can't scale. Like there's no size of instances is going to be able to handle that scale. So there's definitely things We expose metrics of the control plane where you can look at things like API server request latency. You can also, we recommend enabling the control plane audit log. Where again, this is You can run queries to So you might see, you know, one of the best and we do when any is looking at these top callers. And almost very quickly, you'll identify some misconfigured controller or application that is misbehaving, and it's quick to identify and fix. And then we recommend whether it's through CloudWatch which supports Prometheus whether you're self managing Grafana or you can now use the recently launched Amazon Managed Grafana service. Okay, on to efficiency. One question I get a lot, how do I right size compute capacity? This is really, if done right, this is one of the major is being able to make the of cost versus performance. When it comes to rightsizing compute, this is the business decision you're always going to have to look at. You know, how close to the edge of the required compute capacity versus a buffer that I may need to have for some scaling event? And some recommendations here One is you're going to want as close as possible to actual utilization on your Kubernetes pods. And two of the ways you can do this is using the vertical pod autoscaler, which will help you scale your pod size up and down vertically. And then the horizontal pod autoscaler, which can help you scale Once you've right-sized then you're going to have And so using HPA and VPA Then it comes to actually You need worker nodes or Fargate nodes to actually run the containers. And there's a couple of different The most commonly used It's the defacto autoscaling It'll take a look at any pending pods, ones that can't be scheduled because there isn't enough capacity, and it'll spin up additional instances in auto-scaling groups. We also have a new project over the last year called Karpenter, which is our take on in Kubernetes. So it's a different paradigm And so, and then the other where you can totally take away the need to scale capacity at all. Because with AWS Fargate, We're going to scale All you have to do is and we will find the capacity for you. Okay, next up is reducing costs. Right-sizing compute is one good way to reduce costs. And if you look at the graph on the right, this was from a case study we of a customer's cost optimization journey. And auto-scaling and right-sizing and provided a lot of bang for the buck. But then different purchase will also help you save costs. So using Spot Instances is up to 90% off the on demand pricing. Using managed node groups with Spot because the are automatically handled I mentioned cluster autoscaler last slide, there's different settings So you can use the expander and prioritize lower cost node groups, as opposed to on-demand node groups. Savings Plan is another good way we've seen customers save costs. This works for both EC2 and Fargate. Moving to the latest the sixth generation Intel Graviton instances. You can save even more, up to 40% compared to the traditional instances. And then lastly, again, which is one of the areas where is if you're running really large clusters and you want to scale really fast without overprovisioning This is an area where get early success with Karpenter. Okay, scaling applications in clusters with limited VPC IPv4 space, the way the default networking the VPC CNI, is it uses real IP As compared to other CNIs, which may build an overlay network. And this has a lot of benefits. By directly using the VPC you get better performance. But one of the issues you might run into is you're running in a VPC that has limited IP address space. And so what we've always said is the best solution to So this is now supported in EKS. Instead of giving a pod a V4 address, you can give a pod a Every node gets a /80 so you get pods that launch faster. We also support egress IPv4 So it lets you migrate on the rest of your organization If you're still in IPv4 and we have another solution called And here you can add additional IPv4 CIDR blocks to your VPC. And you can run pods in while your worker nodes can stay in the primary CIDR block subnets. This is a fun topic and comes up a lot. Should I run lots of small The answer is, it depends. At a minimum we recommend that you run separate clusters per environment. So if you have dev you certainly want to run The next is, you're going to have to The first one to look If you remember back to if you have untrusted tenants you're going to want to The next thing we see customers look at is organizational boundaries. Sometimes it's just too teams in different organizations and try to have them So in that case, you're going Once you look at those constraints, then you're going to want So the fewer clusters you can run the more efficient bin packing you'll get and more efficient So really it comes down based on constraints, but run as few amount of clusters as you can. Okay, moving on to cluster operations. How do I upgrade my cluster? One of the main benefits is that we handle that for you. It's just a single API call to say, upgrade from, say, 1.19 to 1.20. Really the more managed such as the managed data plane, like node groups and Fargate which we recently launched, EKS aims to be around 100 to 150 days behind upstream Kubernetes. We do a lot of work to each Kubernetes version and make sure it passes our So at a minimum, you're for a Kubernetes version And if you do it yearly, you're going to have to jump If you want to go one version at a time, you're going to have to plan for about two to three times a year. It's just a part of adopting Kubernetes. You're going to need to upgrade versions. And the reason our policy is that way is because of security. The newer Kubernetes versions old ones are much harder to patch. So use EKS managed Test, test test, run the upgrade process Test your application manifests against deprecated or removed APIs. And if there's a version that's coming out that happens to remove we will call that out in very bold text in our release notes. So I strongly recommend you before doing any kind of upgrade. Managing user access, we You can either use IAM users, and with IAM there's no need to maintain any kind of separate data store. You can use your existing for providing user access to the cluster. If you have lots of users and they all need to get the recommendation is to give that access to the cluster, and then have the users The other option is OIDC, OpenID Connect. Many of you told us that you had existing and it would be much easier into your cluster rather than having to give And so EKS now supports OIDC. You can use this as an alternative or in addition to IAM roles. And these are your options for authenticating users to the cluster. Once they're authenticated, to actually give them permissions in Kubernetes, that's role-based access control. Monitoring, observability. This is an important part. How do I monitor the state of my cluster is a question I get quite a bit. And again, going back to that there's the control plane side and then there's the application side. On the control plane side you're going to want to so you get logs for components the authenticator, the audit log, and all those get And I believe I mentioned this earlier, but you can also scrape that we expose through the On the application side, has support for the AWS We have an entire team in AWS that's dedicated to working We think it's the future of Today OpenTelemetry And you can send those whether it's CloudWatch, Amazon Managed Service for Prometheus or any partner destinations. For logging we have the And if you're using Fargate you don't have to install anything. And as a separate option, if you just want an out of the we have CloudWatch Container Insights, which you can install, which handles logging, metrics, traces, and is an all-in-one package solution that a lot of our customers are using. How do I route traffic to my cluster? The recommendation here is to use the AWS Load So previously the, if you wanted to expose a you could use the in-tree controller. But that is deprecated. With the out of tree controller and release new versions to the Kubernetes version release cycle. So with the load balancer controller, you can provision in response to Kubernetes ingress objects, or you can provision in response to Kubernetes service options. One of the more powerful features of the AWS Load Balancer Controller combined with the VPC CNI So you can skip the extra hop where you have to hit the worker node and have kube-proxy forward Instead, you can just go right to the pod in your cluster. And then another feature we is the ability to save money under a same ALB. So even if you have applications running in different namespaces, you can have them all fronted by a single application load balancer. And if you can see in you have multiple that are routing traffic to in your cluster. The slide earlier, we lots of clusters. Even at a minimum, you're for multiple environments. And some of our customers tens, hundreds, in some cases, And so as an administrator, how do you ensure that a fleet of clusters has a consistent set of configuration? And the strong recommendation is to follow a GitOps operational model. By using GitOps, you have stored in a central git repository. And as an administrator, you whether that's defining manifests for operational, security, compliance, whatever tooling needs before you're deploying And then using the Flux It's natively built into eksctl as well, will run in your cluster. Notice that the change and it'll automatically sync the changes from git to your cluster. And you can have this And this is the way we recommend to sync and keep in aligned changes out to multiple clusters How to provision and access AWS services from Kubernetes workloads? One of the projects we're that we've been working is the AWS Controllers for And what we heard is especially developers, where you're deploying and you're looking for AWS services to support those applications, whether it's an S3 bucket like maybe a database like RDS or ElastiCache or a message queue like SNS. Customers want a consistent way to define both their applications and the resources that's And so that's what ACK is. You can define your in a Kubernetes manifest, deploy a controller to the and then in the application namespace the controller will see that you have one of these resources defined and it's responsible for going the state of that resource. And so we have lots of services that have controllers already and many more will be coming next year. Running stateful workloads in compared to running stateless workloads. So a stateless workload that's just serving web requests, whereas a stateful workload And so for high performance we have the Elastic For storage that needs we have the Elastic If you're running EBS workloads, one of the important is that you need to tied to a single availability zone. Because EBS volumes you might run into a case in a different AZ and the pod So the diagram shown on the right is the recommendation we have for running EBS stateful workloads. And then the last slide This is, again, with customers running more and more clusters one of the questions we get running in other clusters? And where you have the who might be trying to keep the lights on for hundreds of clusters and the application team who just wants to run their apps somewhere and they don't want to care where their dependent application is running. And one option to do this is a service mesh. However, a service mesh such as encryption and traffic shaping, canary observability, and a lot of customers don't And so we recently of the Kubernetes upstream multi that uses Cloud Map behind the scenes. And it does bi-directional syncing of Kubernetes service And so as long as your clusters you can now run application application B and cluster B, and they don't have to know that they're running Okay, and the final piece is portability. How do I run consistent Up until recently, EKS was a product where you deployed on AWS. But we've heard from many you know, you have existing costs in data centers on premises, or, you know, for various you need to run certain How do I get a consistent across those environments? And earlier this year, which is our distribution of Kubernetes, the EKS distro plus for spinning up Kubernetes clusters on your hardware in your environments. If you still want to use we also have options for which are fully supported by EKS, which lets you keep the but reduce latency by putting the compute close to where your, you and where you need it. So we now offer multiple you know, from fully managed all the way down to Okay, now I have all of these running in the cloud. How do I see them all in one place? And the EKS Connector is a recent launch that enables you to connect any Kubernetes conformant cluster to AWS and visualize it. So if you can see here, I a local cluster running in my data center and they're all showing So this gives you that for seeing a status of Okay, wrapping up. Where can I learn more? This, we covered a lot here, but Kubernetes is a broad space. EKS is always launching new features to make running and And one of the strong is for you to go check out which is our open source guide. We're constantly adding new chapters, there's chapters on security, reliability, a lot of the things we talked about today but in even more detail. And then where can you give feedback and submit feature requests? One of my favorite is the Containers Roadmap. It's our open source roadmap on GitHub. I love interacting with you and hearing your feature We look at this super It really helps us prioritize to put in your feature requests and let us know what are that the EKS team can be working on. And that's it. Thank you very much. I hope this was helpful in understanding various features of EKS that can help you run Kubernetes. And thank you. (upbeat music)