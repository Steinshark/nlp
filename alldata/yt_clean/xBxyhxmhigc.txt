Hey there guys, Paul here, from TheEngineeringMindset.com. In this video, we're going to with a focus on the cooling systems used. We'll compare how the and how to improve the efficiency, especially with a new growing trend of using computational fluid and we'll also run some simulations to show just how powerful this is. Data centers are rooms of computer servers which provide networking They range in size from serving a single organization, and they scale all the way up such as Google and Facebook. More and more data centers as we use and increasingly and remote services to store, With this growing trend, it's run as efficiently as possible. As the data centers are operational 24/7, they can consume vasts and as this electricity is and process all the data, This heat needs to be removed. Otherwise, the electrical or even catch fire. The energy consumption will be split with around 50% being used by the IT equipment, 10% on electrical and around 5% on lighting. The electrical demand for from just a few kilowatts depending on the size and location. So we're going to look at a and their air conditioning systems, as well as the efficiency The first part we'll briefly cover is the non-data hold areas. These are the areas where staff are normally located, the security guards, the engineers and technicians, et cetera. And these cover the areas of the offices, the toilets, the workshop and rest areas. These areas will be served by a separate mechanical ventilation system and will use either an air handling unit or a rooftop unit to to suit the thermal comfort They might also use a or VRF system, for temperature I won't go into too much as we've covered these on chillers, HHUs, RTUs. We also have videos on VRF, heat pumps, and split AC units. If you want to learn more on these, links are in the video description below. Coming over to the server room, one of the most common is the place the server and then use computer or crack units to distribute to the server racks. The crack units have which are connected to refrigeration units or chilled water systems from the server racks. Some can also humidify that's very important in order to control the static electricity in the air. They have filters inside to as well as a fan to circulate For extra efficiency, the crack units should use energy efficient filters, EC, or electronically controlled fans, and pressure sensors in the floor void to precisely control the air supply rate. Placing temperature sensors of the server rack is often recommended to control the supply as this matches the actual intake. The conditioned air will be forced by a fan in the crack unit into the void under the floor, and the small holes or would allow the air to leave This air will collect the heat and rise up towards the ceiling. The crack units then suck this warm air back into the unit to be reconditioned. In the early days, the server racks were positioned facing different ways. But engineers soon realized because the fresh cold air was just mixing with the warm discharge air of the servers and this meant that the servers were receiving different air temperatures. Some hot, some cold, and this as well as a high failure To combat this, the so that all the server racks This was a slight improved strategy, but quite often, some was being pulled into the sitting behind it, which led to mixing and an increased air temperature. The next strategy used, which is the use of hot and cold aisles. This is a great improvement because it separates the from the hot discharge air. The cold air rises out and is pulled through the servers. All the hot discharged air and rises up towards the ceiling, when it's then pulled This means the servers should receive only fresh cold air, and the crack units receive the hot discharged air. This increases the across the crack unit's heat exchanger, and that will improve the This is not perfect, however, some mixing of the hot Cut outs in the floor This means that the cold air can lead straight into the hot aisle. Floor grills which are too result in air recirculating and will mix with the returned air stream. Gaps between the servers can result in air recirculating around This can easily be solved though by installing blanking plates. If more cold air is supplied than needed, it will flow over the units and If insufficient cold air is supplied then warm discharge will and around the side of the and will mix with the stream. We're gonna to look at some shortly of this occurring. A much improved design, and currently for both new is to use a physical barrier to separate the two air streams. There are a couple of ways to do this. We can use a barrier and they contain either the Cold air containment is for existing data centers. That's because it is easy which means that payback is quick. The cold air fills the cold aisle and then the hot discharge with the crack units pulling However, it does also located outside the cold zone, The other containment strategy in use is the hot aisle containment. This is best suited to new builds as it costs more to install. In this strategy, the and the hot discharged air is pushed into another void within the ceiling. The intake for the crack unit is also inducted into the ceiling to pull this hot air The hot aisle containment and also allows a slight should the power or cooling system fail. We can actually compare the performance of different server room using CFD or computational fluid dynamics. These simulations on the screen were generated using a and FEA engineering platform by SimScale, who have kindly sponsored this video. You can access this using the links in the and they offer a number depending on your level. It's not just limited it's also used for HVAC, as well as thermal and Just a quick browse on their website and you can find thousands from race cars, heat which can all be copied for your own design. They also offer free webinars, to help you build and If like me, you have some experience with creating CFD simulations, then you know that normally is very expensive, and you would also need a powerful computer to run it. SimScale, however, can all be down from the internet browser, and as they're cloud based, their service do all the work, and we can access our which I'm really pleased about, as it makes our lives as So if you're an engineer, a student or a hobbyist, you try this out, get your free account by following the links in Okay, so the first design hot isle configuration. The arrows indicate the direction of flow, and the colors indicate the velocity, you can clearly see there's a huge amount of recirculation occurring and I've highlighted these in the boxes. The second design uses with a third hot aisle at You can see the first hot and no recirculation is occurring. The second aisle, however, occurring towards the end of the row, so some steps like blanking plates need to be installed here. The third hot aisle has some and that's because there to separate the hot and cold air streams. If we then run a simulation of the designs, we can and show the result in at different levels. The simulation starts at floor level and moves up to the top of the racks. From the comparison, that at the lower has a much cooler cold aisle as compared to the first design. As we move to the upper start to mix, but the second design still maintains much cooler levels. Well below 28 Celsius or Whereas the first design has temperatures above 29 degrees Celsius The recommended ranges from standards require that the inlet air temperature be within 18 to 27 degrees Celsius or 64 to 80 degrees Fahrenheit. At the very top levels, the are now in the hotter range or 104 degrees Fahrenheit, the partial containment one, has only a maximum of 30 degrees Celsius, or 86 degrees Fahrenheit. Thus, the second design in this case and further such as for hot aisle or can be studied using cloud based CFD to improve data center cooling, as well as optimizing energy consumption of both the server Another type of data center design, which is becoming increasingly popular is free and evaporative cooling. It can be retrofited in but it's especially popular with large new purpose-built data centers Some of these new designs do not use any refrigeration equipment for cooling. This can only be done in where ambient conditions are right, but it allows data centers without any refrigeration plan. The ambient air is through some loovers and and cooled and humidified and then forced into the data haul into a hot aisle configuration. The exhaust of the hot into another set of fans and discharges this off Some other cooling strategies which are slightly less common are the use of ducted systems with heat wheels or These allow thermal from one stream to another fresh air into the building. The fresh air can contain dust, moisture, and salt particles which deteriorate the server's electrical components. To provide cooling to the crack units, you usually find a chilled water system using a traditional chiller. Depending on the location, to turn off the chillers, cooling capacity of the cooling then use the chillers as their backup if the cooling towers are Some crack units contain their own small individual refrigeration system which either uses a remote dry air cooler or they dump their heat into If the condenser system is used, then you'll often find a free or sometimes built into the chillers. This allows the heat to be removed without or with minimal using just the fans to across the condenser Okay guys, that's it for this video, thanks for watching. Don't forget to sign up for using links in the Also, you can follow us on and Instagram, links are below. Once again, thanks for watching.