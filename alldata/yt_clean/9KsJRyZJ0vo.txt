Folks. All this We keep saying that it's slow. But what I'll show you today because today we will use our graphics faster. Specifically, accelerator mode But the best part is, we do not need We will use the same commands as usual, So what exactly are we waiting for? Let's roll. So on the menu, we have a dataset with 1.6 million tweets, which we will clean We will use a process as in manipulating data to highlight So we basically focus on meaningful information And since this process consumes for benchmarking. Now, before we begin, Since cuDF stands for CUDA DataFrame, I highly recommend But for this video, all you need to know with Nvidia graphics cards tasks from our processor And that's because graphics tasks in parallel. CPU's, on the other hand, not so much. So if we have a giant dataset that we keep revising maybe it's a good idea to do so on GPU. And that's exactly what we'll do next. So let's begin For this we will need a Windows which sounds a bit scary, PowerShell as administrator --install and boom, we are done. Now once we reboot our system, find a new WSL terminal inside So let's click it. Now, the first thing we'll do here We can do this with nvidia-smi. Now on my end, I'm working with GeForce RTX 4080 and CUDA version of 12.3. Now once we know our version of CUDA, And a good way to do But even though we may have Anaconda it doesn't mean that we have So let's take care of that. We'll quickly navigate to the installation We will copy and we will paste it in our terminal. Let's run it. And once our installation file was downloaded, followed by the name of the file, We'll, of course, carefully We will then agree to all the terms we will confirm the installation location Yes again to initialize Conda. Congratulations. Miniconda is officially installed, but now we need to close So let's do it now. Once Miniconda is installed, installation guide and we will make sure We will need the stable release with the Conda method, and then we will a close enough version of CUDA We will go for Python 3.10 and we will Now let's quickly but before we run it on our subsystem, to make sure And from this environment we can then go But on my end, So instead of rapids 23.10, faster Pandas, Now let's quickly run it. Great. We have officially installed cuDF, navigate to our environment. We will activate it. With Conda activate faster Pandas For this In my case, I'll pip install Jupyterlab and I will run it with Jupyter lab. --allow-root project to see the difference Specifically, we will prepare data which is all about determining So, for example, the person who wrote but the person who replied is very sad. Humans can tell it right away, but So let's try to help them. For this we will create a new notebook In our case, which is all about tweets We will get this data with exclamation followed by the URL of the dataset I'm just going to paste it So now, once our download is complete, where we will find a brand Now we can actually unzip it with code, type from zip file import ZipFile this time with a capital Then right below we will create a ZipFile instance to which we will pass And I'm just going to copy it. It is training and test data. Dot zip will specify it as a string inside ZipFile Then we will call right below Archive dot extract all followed And when we run this cell, where we will see two brand new CSV files. Now the file we are looking for And if you're struggling to see the name, which will list all the files And the full name of our dataset No emojis. CSV. We will definitely need it for later, but For this we will use the magic command of percentage symbol load underscore EXT, followed by the name of cuDF Pandas. Now once we load it, Pandas as PD as per tradition. But this time We are dealing So it may look this one is capable of running both on CPU and on GPU depending on the task. Great. Now let's load to which we will pass We will specify it as a string and we will assign But before we run this cell, with data dot head on the first three records Now let's run it. Let's have a look in. Oops, It looks like the very first record So let's quickly and setting it to None. And beautiful. But the only problem is much of this We only care We don't care about when it was said and we most certainly don't need So let's get rid of them. So right above it where we will set the columns argument to a list with a string of one, a string of two and a string of three as in the names Now let's quickly reassign Now we are only left with the labels which can either be negative 0 or positive 4. We are left with the user who wrote So let's give those columns appropriate dot columns which we will assign Actually, let's do it label rather than Now let's run it in perfect. Now our data is loaded in properly, Everything looks and works So how do we even know that Well, we can always print the profile by using the magic command cuDF dot pandas, dot profile. Let's run it. Let's scroll down of all the commands In fact, we made zero calls to our CPU. Now you must be wondering why do we need two different components Can't we just get rid of processors Not yet. There are certain things that only CPUs So for example, if we set the encoding we change the way that our data is stored We use one byte for each character we are allowed to use. But by doing so, let's quickly Let's rerun this cell is no longer executed on the GPU. It automat actually falls back to the CPU, every little communication with memory Our memory is connected to the CPU and the CPU is connected So we can imagine it as a gatekeeper Now for the speed test, than just dropping Let's do something called feature and we make it much more meaningful, So let's extract all the tagged usernames from every tweet For this, that has the type of STR as in string. And we will call the find Now we use this method to detect patterns, So for now, all we need to know about that in our case begin with an @ symbol and end with a backslash capital S plus where backslash represents A capital S represents all the characters and plus represents the entire word. So we are basically selecting an @ symbol up Great. Now let's run it in. Perfect. It worked. Now let's go ahead and store these values by assigning our expression And if we're already here, we might as We'll just copy our command every instance of @ And let's do the same for all the URLs We'll call this column. URL Now let's print Let's give it a quick run. And beautiful. It worked. We have stored all our usernames, tags in separate columns, which is something Now we can replace them inside every tweet with something a bit more generic, Now a cool trick is to first convert For this, we will copy our latest command We will replace the find and we will also reassign this expression And now when it's time we will copy this new command we created. We will replace the lower method where the first argument is the pattern And the second argument with, in my case USER in all caps. And that way it is extra easy to detect Now let's do the same for our hashtags. We'll replace them with TAG in all caps URL in all caps. Great. Now let's run it in. Beautiful. It worked. We have officially replaced with their generic placeholders, yay! it doesn't really matter if Bob is upset or if Charlie is upset Additionally, URLs are not meaningful at some of the hashtags, we see that computers may find them always clear where charity ends Who knows, maybe what they really meant And perfect. Now we have enough actions We will finally see just how much time And actually, Let's enlarge our dataset four where we will assign data where we will concatenate And then right below our for loop, we see that we are dealing instead of 1 million point 6 of them, So it is perfect for our speed test. Then we will go ahead and comment out as well as our loading extension command, which will take us back to regular Then we will add the magic command percentage, symbol time which just as it sounds, Now, to keep things fair, our kernel with zero zero Now we are ready to import regular Pandas and we are ready to load our dataset, So let's see what's going on here it looks like our CPU is not too happy We have some encoding issues, a new argument inside read CSV. We will add encoding underscore errors and we will set it to ignore Let's set it to replace because it is not Now let's go ahead and run the cell What? What is wrong with your CPU? What do you mean You cannot find our column names in axis? We've seen them with our own eyes. So let's try something else. Okay. So inside drop. Maybe. Let's specify our column names as integers Let's give it a run. And great. It worked. Thank you. CPU. We are finally ready for our speed test, Hopefully quick. And on my end I am getting one minute which means my 12 Gen i9 CPU You can find the rest of the components the most important ones. One minute is pretty impressive. But now let's see what happens For this, we will go ahead and restart We will then uncomment our load and we will import cuDF pandas. Then we will go ahead and load We are finally ready for the speed test. So if it takes my CPU one minute 2 seconds almost precisely! Holy smokes you guys. We are talking about 63 divided by 2.01, which is 31 times faster! Wow. Now, the funny thing is on my Alienware system, even though it has I'm getting 64 seconds for my CPU Now to be fair on this ROG system, so I have OBS studio open here, so it probably takes some cuDF runs It could be a coincidence, you're getting. Now let's finish What is the fastest way It doesn't have to be with pandas but it must be with Python, where the tasks are extraction of keywords as well as the removal of those keywords How would you do it? Please share your experience and I will showcase my favorite solutions And thank you so much for watching. If you found this video helpful, please share it with the world and don't If you'd like to see more videos of this to my channel I'll see you very soon in another awesome Bye bye.