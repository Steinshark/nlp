- [Narrator] In a fraction of a second, Google Translate can make But this isn't the same Google Translate from the early 2000s. Over the past two decades, the technology has gone shifting from a basic to a sophisticated neural network that handles more than 130 languages. It works by turning language into something computers - Exciting times for people - [Narrator] This is the - There's very little code of the phrase based translation. We have shut down and - [Narrator] That Google laid the foundation for what we use today. When it launched in 2006, it worked by playing a matching game. First, the model looked of professional translations Then when users entered the tool would break them into of words it had seen before It now uses a much more sophisticated machine learning approach. A so-called transformer model, of all modern AI. transformers turn language into math by assigning numbers to words. - The key insight is that a series of numbers can represent a meaning. You can then do math with those vectors that shows something of the meanings of words to each other. - For each language every word gets converted into a vector, which is written like a list of numbers. This way the computer For instance, king minus The specific numbers assigned to each word don't really matter, and they're different But what matters is how each word relates to every other word. It's all based on machine learning from billions of examples. But most of the time you it's not just an individual word. So the computer also has to figure out how words work together, and this is where transformers, a breakthrough in - The next generation is called the transformer architecture, and this added a level, so it moved from representing by a row of numbers to of all the words into a table and doing math on that whole table. And that enables you to do math that talks about not only but the importance of the relationships of the words to each other. - Say you're trying to translate this Italian sign into English. First, Google Translate would and those vectors would be or matrix. Then the computer tries to figure out how each word interacts with Mathematically, this is basically - The most important kind of magical step is laying and doing what's called And if you do enough of that, - All this creates a new list of numbers. This is what's called a context vector, and it's something pretty special. This list of numbers actually represents what the sentence means, of its words, at least if the model has done its job correctly. - If you put that together which the people who and you train on a lot you can eventually get to that meaningfully represents So that's called the encoder stage. Then you have a decoder, which roughly speaking is - [Narrator] The computer back to human language. - The decoder now also goes through lots and lots of operations, and finally you start getting vectors out, which can be mapped back So we hopefully get So this is how language becomes math. - [Narrator] Getting this math to work requires a lot of training. Lots of the numbers in this and then refined as the computer learns Before deploying an update and weights, engineers run numerous tests with their AI evaluator and then professional human But since every possible to a unique equation, it's Since the model has trained or from English, it to go between two non-English languages. For example, if you wanna to Zulu, it will go and then English to Zulu. - The first thing that happens when you use Google AR to actually extract the and so as you can see here, it detects that now this is Chinese and It makes information a lot more accessible because for many people, typing script in a foreign - The key component is a technology called Optical Character Google has been using that since 2002 when it started digitizing - Initially, it would like pattern matching. So you can think of it as, Yes, so it's an A or B or whatnot. - [Narrator] But now optical also uses transformers. First, Google Lens and text direction. Then it determines specific Instead of dividing and assigning numbers it divides an image These are called tokens. - The encoder of the transformer of these tokens simultaneously and the best word eventually. - [Narrator] This means the Google Lens, the company's visual search even when it can't make - With transformers, they're If there is a spelling mistake, the transformer will also to disambiguate and still - After it completes optical Google Lens analyzes the That's how a computer would as you matter, don't give up rather than you don't matter, give up. - When you look at the newspaper, humans are excellent and understanding what What should you read first? This is a concept that isn't actually easy to solve technically, it's very hard. - [Narrator] The key is for to understand something about the meaning of what it's reading. This is also done through After the chunks of text Google Lens uses painting models to erase the text off That way translated text of clean surfaces. - Using generative models, and create pixels that so that when we overlay it looks very natural and seamless. - [Narrator] This doesn't - You know, this one is not I'm not sure why. - [Narrator] Some translations don't fully account for context, which is why alto on might be mistranslated to high. And while optical character recognition can frequently identify or with complicated - One of them is with deformable objects. Whenever there is text on like a sweater or cookie wrapper, depending on the pose and the angle, it might and difficult to extract the right OCR. - Well-formed grammatically we're quite good at. Where we have challenges using casual speech in We don't necessarily see as much of that because we don't have - Google is working to like letting users if they want to. Similar to how you can ask Google Gemini or ChatGPT to make translation or in Chilean Spanish rather And it's also working - There are an estimated 6,000 to 7,000 languages in the world. Our goal is to support all of them.