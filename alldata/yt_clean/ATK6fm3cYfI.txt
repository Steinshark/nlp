In this video, we will talk about TF-IDF representation for text in NLP. In the last video we looked at Bag of words, Bag of n-grams. If you have not seeing those videos, I highly recommend you watch them first before watching this video. We have been looking at news article classification problem, where a news article is given to you, and you have to classify it into one of the companies, whether this article is for Tesla or Apple, etc. And in Bag of words model what we did was we'll take an article and then design this vocabulary. Doc, vocabulary is basically all the words from all the news articles and then we will take count of individual word in a given article. For example in article 1, word Musk is present 0 time, that is present 32 times and so on. And just by looking at this vector right now, you can actually tell that first 2 articles are Apple articles because you see these terms, iPhone, itunes they appear 7 times, 3 times, 6 times, 3 times. Whereas in the other 2 articles, see Musk appeared 15, 3 times, 1 time. So based on these word count, you can say yes this one is for Tesla and this other articles are for Apple. But look at this other terms that, price, market, investor, now these are very generic terms that can appear in any article. I used to work for Bloomberg and we actually were working on extracting company from earning estimate document. And you will find this kind of generic terms too often in all the companies you know, like stock, market, company profit, loss this is generic. And the presence of these terms in your bag of words model can suppress the meaningful words. So here for I Apple, iPhone, iTunes is the relevant term and for Tesla, Musk and Gigafactory are relevant terms. But these other generic terms are suppressing the influence of these relevant terms, and in this example I'm just showing you 4 that, price, market, investor 4 terms but actually there could be 100 such terms and look at what happens to the vector. In the first and third vector these terms are present with a equal amount. See 32 31 45 44. So ignoring all the terms if you just look at the vectors and you give this vectors to a computer, computer will think article 1 and 3 are similar. Actually they are different right? One is from Apple, the third one is from Tesla. But if you look at just the vectors, see this 7, 3 is not matching here, it's 0 0. But see other terms are kind of equal and there are 4 of them. In reality there could be 100 such terms in your vector, and your computer will think, okay these two are actually related articles you know. But that is not the case. So how do you ignore these terms, that's the question? See you don't want to ignore them completely but still you somehow want to ignore it. Someone will come and argue that okay that is a stop word. Why don't we just remove all the stop words? Fine, we removed that. But then how about all these terms: price, market, investor. So stop words can help, but to a certain extent. We need to do some additional thing. So let's think about this together. Let's pause this video and think what can we do, or what should be our approach okay? You just pause the video and you will be able to come up with that idea. So one idea is we figure out in how many documents the term that appeared. So that appeared in 3 documents out of 4 and I'm not talking about the individual word count in a given document. I'm saying in how many documents this term that appeared. It appeared in 3 documents article 1 2 3 out of total 4 documents. Similarly Gigafactory appeared in only 1 document and iPhone appeared in 2 documents. This is something called document frequency. Mumber of times term t so t here is that, Gigafactory, etc. Number of times term t appeared in all the documents and if this number is higher, if some term is appearing in majority of documents, we should lower its influence because it's like a generic term. But if some word is appearing in only few documents such as Gigafactory, see only 1 document, iPhone only 2 documents, then that term is important and we need to give it a high score. So we need to come up with some scoring mechanism here. And obviously the scoring mechanisms should be such that you know higher the term appears in all the documents, the influence would be lower. And you can do that by doing inverse document frequency. So if that term is 3, so you invert it okay? Then the number goes down, and you don't do simple inversion but you do this formula: total number of documents divided by number of documents term t is present in. For example total documents are 4. So numerator in our ratio is is fixed: 4 But denominator is how many times that appeared in all the documents. Well 3 times. Total number of documents are 4, that appeared in in 3 documents. So 1.33 Total number of documents are 4, Giga factory appeared in only 1 document. So this number is 4. So now see when the term appears in less number of documents, the score you are getting is higher. See this 4 is higher than 1.33 and this is called IDF inverse document frequency. There is something additional to this formula, which is called you take a log of this. Why do we take log, I will explain in a minute or so. But hold your thought for now, and in your brain you should be thinking if the term appears in more documents, the final score for that will be lower because you want to reduce the influence. And you will come up with this kind of vector which will be called IDF vector 0.1 to 0.12 and so on. I mean I I'm just showing this. We don't use IDF. We use TF- IDF, which I have to still explain. But for now, I hope the concept of IDF is clear in your head. We also need to take into account the word frequency, because that is also important correct? But we don't want to just take a word frequency like bag of words because see some articles could be bigger. Let's say there is a 5 000 word article and there is only 10 word article small article, you want to be fair and kind of normalize things. So what you should do is total number of time term t is present in document which is just a simple word count that we took in bag of words model, divided by total number of tokens in that document, and that we will call term frequency. So here t is the term and d is that document in which you are counting that term frequency. So here for example the term frequency in article 1 for the term market, will be 48 divided by total number of words in that article. Let's say in that article total number of words are 1000. So the term frequency for market in article 1 will be 48 divided by 1000 and TF-IDF is just a multiplication of these two terms. And we will use that in today's tutorial to do our coding. So we'll do again a text classification for e-commerce goods and we'll be using TF-IDF representation. Here I'm just showing some sample numbers. I did not actually compute it, but what I want to show you here is terms like iPhone, iTunes, Gigafactory will have higher numbers 0.9 .8 Whereas terms like that, price will have lower number okay? One more thing I want to clarify is sklearn uses slightly different formula for IDF to take into account the 0 division possibility. So what they do is they add constant 1 in numerator and denominator both, and then the result they add one more time. So it's just a little customization on your IDF formula sklearn uses that. This screenshot is from the sklearn documentation. Now let's talk about why we used log. You can ask this question to your dear friend who is sitting next to you and that dear friend is actually Google. So if you just Google this question you'll find the stack overflow answer where they say that log is used to dampen the effect of IDF and unki has given a very nice answer here. So I'm just going to take a screenshot of that answer and show it here, credit goes to stack overflow which is let's say in a work in a document you have a term called computer, which is relevant for your given NLP problem. And if the term appears 1 million times, you know that that term is important is and and I'm talking about that one term appearing in your document right in your specific document. So now if it is 1 million or 2 million it is all same, correct? Doesn't matter. So that's why we kind of take a log of that just to kind of reduce the effect of it. If you know from your log function, your log function goes something like this you know, log of 1 is 0, log of 10 is 1, log of 100 is 2. I'm talking about log to the base 10 and the curve kind of flattens out as you go further in your x-axis, and the influence that you want to have in in terms of your IDF you want to reduce it because as you have more terms the effect is same. So you can just reduce the influence. Now the limitation of TF-IDF model is similar to the previous model that we saw before, which is a as you increase dimensionality, the sparsity increases. As your vocabulary increases, obviously the sparsity increases. It's a discrete representation of your text. It doesn't capture the relationship between the words. The relationship between the word is captured by word embeddings and sentence embeddings. But all other models they suffer from this problem where you have relationship between the words, it's not captured because here you're taking a term and you're just taking a count out of it. And it suffers from out of vocabulary problem as well. Let's do some coding now. I opened jupyter notebook and imported TF-IDF vectorizer from sklearn and I'm going to take 1 sample corpus and the meaning of corpus is the collection of documents in your NLP application. I'm just going to take some simple dummy sentences and will create TF-IDF vectorizer out of it. So the way you create the instance, this is TF-IDF vectorizer is a class and you just create an instance of it. And you will just say fit. So when you do fit, you are creating that vector basically right? And then you're kind of training basically and then the creation happen when you say transform. You can do fit transform as well, just one call. I think let's do that. Fit and transform and we will call it transformed output is that okay? And once that is done the v object will get its vocabulary. So I want to print a vocabulary first, and I'll just use print so that I can see everything in like 2 or 3 lines. See all this vocabulary have these indexes. For example already the term already at a is at 0, Apple is at 6 and so on. Now I will print IDF score. So v this parameter v if you do like dir v you will see all the members, and you will see IDF for example as one of the members. IDF smooth IDF actually we are using smooth IDF and so on. So let me first print just a simple IDF score out of it, and for that you can either iterate through this particular directory v.vocubalary or if you want to go in sequence, see if I iterate through this directory, I'm kind of going randomly over the places. But I want to go in sequence where I take the zeroth index element first and so on. And for that we can do v. get feature names out And that will print things in order basically, all your features, all your vocabulary already 0 am is 1 okay? When you do TF-IDF vectorization, these things are actually your features and based on the features you will calculate your vector. So I will just call this all feature names, and then I will iterate through it. So I will say for word in all feature names, I will first get the index and index is v.vocabulary .get because it's a dictionary you can call get method and you get this particular index right? Like 27, 11 whatever. And when you get that index, uh you can then say v.idf that will have your idf score and you can retrieve specific score for a word by using the index operator, and you can just print that. You can print let's use the format strings here, and we will say word and after word we have the score okay? So this is the score. Now you will notice one thing which is this term is is present everywhere, see. In most of the document it is present except this one. So you expect this TF-IDF or let's say IDF score for is to be lower, and the term like Apple is appearing rarely. So the score of Apple, Tesla, etc. should be should be higher. So check this is is 1.1 whereas Amazon or Apple is 2.5 So it is working, and we are more interested in the the whole TF-IDF score. So what I will do is now I will print that and that score is present in transform output. But before I do that, I will let me just print first 2 sentences from the corpus okay? So these are the first two sentences from the corpus, and I want to print the corresponding TF-IDF vector for these 2 sentences. So that that is present in transform output correct? We got that in transform output so transform output. to array It's a sparse matrix so you have to convert it to an array and you just do 2. So this is the first one and this is the second one. Now watch this uh let's see we are looking at is okay and is is at I think position 17 right? So is if you look at it here, it is 17. So is position is 17. So there are total 6 here, so the position 17 will come let's see where so 6 6 so this will be 0 1 2 3, okay 0 1 2 3 4 actually there are only 5 I see. So this will be 15 16 17, it will be this one. I think this one actually. Oh see 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17. So see the IDF TF-IDF score for is in the first sentence is . 10 and the score for Thor for example is how much? So where is my Thor? Thor is 27 So this is 17 18 19 20 1 2 3 4 5 6 7 See, Thor score is 0.24 So it is more than is. So just consume this. I mean you'll get an idea that the things are working as expected. The score for the term which is appearing in too many documents is lower and the score for the rare terms which appear in individual document, is higher. I mean that's that's the whole point I wanted to make. If you remove certain sentences, for example right now, the score for is IDF scored for is is 1.11 If i remove certain sentences you will notice that the score will go up. You can experiment. We want to move to our actual problem that we'll be solving in this coding tutorial which is e-commerce text classification okay? And I took the dataset from Kaggle, the source credit is given to this particular page. This dataset has 4 classes. I mean the actual classes might be more, I have trimmed it down and we have the description of an item that you sell it on let's say Amazon. So Amazon item description Amazon or any other e-commerce website item description, and what category, is it electronic, household box, etc. okay? So it looks I have I have this other notebook that I usually prepare before my coding video. So see it looks something like this, you can see right? So let's say there is a text and there is a label. There is a text this text when you read it, you realize it's electronic right? SATA hard disk type, and when you read this one you realize it's a book. So we have this one and what we want to do now let me just show you the CSV file. So I got the CSV file, number of categories are total 4 okay? and see these are the text description So simple CSV with 2 columns the description of an item and what category that item belongs to okay? So let's import pandas. So how to import pandas? Import pandas is pd How do you read CSV? We have done this like I don't know 10000 times So you read e-commerce cv you store it in your data frame df. shape you can print that and then do df.head What I do is I usually do f.label .valuecount just to check if there is a class imbalance. And no there are 6000 items in each of the categories. As I said like I trim down the dataset, so that there is no imbalance. And now what we will do is map these label categories to numbers. Machine learning models understand only numbers. So how do you do that? So df.label.map you can use a map function and, you can apply uh the dictionary mapping. So what you will say is okay if it is household item, map that to a number 0. Just random number you know 0 1 2 3 4 If the category is books the number is 1 and so on. And that will be my label number column, and the way you do that is you'll have to specify the new column like this. This is a new column that you're creating in pandas. Pandas is an awesome library. You can create new columns like this by applying transformation on existing column, and I will check my df head and it works like a charm, okay? We are ready for train test split as usual this should be in your muscle memory. Import it from sklearn the train test split method and we have done this so many times that I don't feel like writing it. I'll just copy paste here. You are doing train test split of df.text is your main actually it is your dependent variable and df.label number is your independent variable like a target variable correct? So this is x test x train this is y test y train and we'll split train and test into 80% 20% category. 20% samples go to test data set Random state parameter, I'm just randomly initializing it to num some number and then we are doing stratify. Actually if you don't do stratify still okay because no actually we we have to do it okay, otherwise you might get imbalance in your x train x test So this is a good practice and when you do that, it's gonna do train test split and I'm just going to just print the shape of x-train x test took something like this, 4 800 samples in your test dataset. Okay and then just some value checking in in your y train I am doing value counts, and you can see that there is a balance you know every category has 4 800 training samples, and same thing you can do for y test right? So y test value counts and y test also has equal number of samples from all the categories. Now I will use few classifiers from sklearn to train my model. I will first use KNN, then Random forest, then Naive Bayes. I just want to show you if you use different classific classifier, what is the performance okay? So I will just import some necessary libraries and folks I'm doing copy paste. I don't want to type and waste my time because we have done this enough number of times in this particular playlist. So make sure you follow this playlist in a sequence. If you are randomly watching this video, some of these things might not make sense. But if you follow the playlist in sequence it will be more meaningful. We'll import sklearn pipeline and sklearn pipeline allows you to create a pipeline where in a first stage you can have your TF-IDF vector, and the second stage you have your KNN classifier. And then you know you'll do clf.fit so x train y train and then you do prediction right? Once the model is trained, you will say predict x test and once you get your y predicted you want to compare y predicted with your y test and the way you do that is by printing classification report. And you always supply truth first and then the prediction. And I usually use print because otherwise the formatting won't be appropriate for classification report. So right now, it is training the model in this fit comment. And then in the predict comment, it will do prediction on the x test dataset, it will get y predicted output and y predicted it will compare it with the actual truth, and based on that, it has printed this classification report. The result looks pretty good. These are the precision and recall metrics for class 0 1 2 3 okay, you know all those classes household books and so on, and these are the F1 scores. See F1 score of 95 96 97 this is pretty good. If you don't know what is precision recall go to YouTube search codebasics precision recall, you will find a video very easy video, which explains all these metrics and I will just print x test and what was my y test for first 5 sample okay? And I want to also know my y prediction for first 5 samples and see 0 2 3 1 3 0 2 3 1 3 only this one it got it wrong. Other categories it predicted fine okay? So zeroth category Lal Haveli Designer Handmade this looks like clothing item to me, or household so that is 0. And you can see that household is 0, books is 1, electronics is 2 right? So this cotton thing, for example GOTOP classical retro cotton is 2 and 2 category is electronics. Okay so electronics. It is predicting it fine. Sometimes if the label is not clear, what you can do is you can print the whole thing and you need to supply this index okay? This is the index that will give you the holes. See you're talking about DSLR cameras right, electronics maybe this is leather, so maybe it's some kind of case for the camera. I don't know. Yeah, but anyway it is classified under electronics category. The third one is see the third one is this one right? So this one is for let's say camouflage polyester multifunction winter face mask, style mark this is some mask, so household item. So number 3. Oh number 3, okay sorry clothing and accessory. Numbers 3 is clothing and accessories, so the y test was that and prediction was also in in line with the truth. Now I will use multinomial classifier as well, and check the performance. So you can just copy paste this whole thing here okay, and one additional thing you need to do is your Naive Bayes classifier you need to import, and use that one here in the second stage of your pipeline. And you want to check how the performance looks like, and you compare. So see here KNN gave 95 96 97 97 94 95 97 90 almost similar okay? Not too much different. But now I want to try a Random forest classifier okay? So I will do Random forest classifier and you can see this could be trial and error. Usually for text problems, I start with Naive bayes. But based on your problem, your dataset you know one or the other classifier might perform well. There are some general guidelines you can follow. Okay text classification usually use multinomial Naive bayes. But in this particular case what's happening is see Random forest is giving you probably the best performance, 96 98 you can compare all these metrics with these these other metrics from the other classifiers. And we concluded that the Random forest classifier is giving you the best performance. Now when I trained this model, I did not do any preprocessing. I did not remove stop words and I did not do lemmatization. So remember in the last tutorial, we we designed a function that can do this preprocessing. So I will do that now. I will train this same classifier but using preprocess text, not the raw text. See this classifier is trained on a raw text. How do you know? Well see we use in train this place df.text df .text is whatever we got in our CSV. We did not remove stop words or we did not do lemmatization. I want to do that. So I will import a SpaCy library, and I will define a function called preprocess. Now we have seen this exact function in the previous video. Hence, I am not going to go line by line. I will use my most powerful weapon of copy paste. Here you go token by token. If it is stop word or punctuation word you just ignore it. If it is not, you do lemmatization. And then you join filter tokens and you create a string okay? So this will do your uh preprocessing and then on the text column, you will use apply function and you will say pre-process. Apply and map they are kind of similar function, and this will create a new column called pre-process text. It took me close to 10 minutes to execute this sentence. So make sure you have enough patience. You go for a coffee break or a walk while this is executing, and I got this new column created and this has a pre-processed text. So let's check 1 or 2 specific example. For example the zeroth text right, the zero the first one is this, and if you look at the same thing in pre-processed text, you will find that see a study in simple, see a study a in is removed. The is removed, punctuation dot is removed. See here, then chair has a firm. It will be say chair firm. So has a firm is removed. So it got rid of stop words and some and it did lemmatization and so on. And we'll do the train test split, using the pre-processed text column here. Again the random state is same because of that the the samples that we got in the previous scenario, where we are using the raw text will be similar so that we can do apples to apple comparison when we are doing this prediction. And I am going to copy paste the same code for Random forest like from here okay? The same code I will just copy paste which is my most powerful weapon, and I will train the model. This model is trained using pre-process text. The numbers that you saw above, these are using the raw text. And you will notice the performance is slightly better. So F1 score 97 98 98 and 99 correct? Here see, it's little less when you use raw text. So we can conclude that in the case of this e-commerce classification problem, with this particular dataset the pre-processing is helping. General guideline is you should do pre-processing. But in some cases if you don't do pre-processing, it is still okay. So it just varies from case to case. I hope you like this video. This is this is all I had. I'll see you in the next video, and make sure you check video description because I'll be adding an exercise in next few days. So by the time you are watching this video, the exercise will be available. Learning coding, machine learning NLP is like swimming you know. I keep on saying this boring thing again and again, to help you realize that only with the practice, you can master the skills. Just by watching video, nothing is gonna happen. So you have to practice when I'm teaching you code. Along with me do a coding practice and also work on an exercise. So check video description below. I think next next few days, I'll be adding the exercise. So if you're watching the video right now, maybe you can check the video description after few days, and I will have exercise ready for you! Thank you! [Music]