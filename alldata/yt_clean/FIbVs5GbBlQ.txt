ANNOUNCER: The following program YASER ABU-MOSTAFA: Welcome back. Last time, we discussed the And we realized that learning is a probabilistic sense. And we modeled that probabilistic sense an out-of-sample performance. We already mapped that to the The performance we don't know. And in order to be able to tell what E_out corresponds to that particular bin-- we look at the in-sample. And we realize that the in-sample tracks the mathematical relationship which is us that the probability that E_in specified tolerance is a small number. And that small number is a negative So the bigger the sample, the will track E_out well. That was the basic building block. But then we realized that this And a single bin corresponds So now we go for a case where we And we take the simple case of And we ask ourselves, what We realized that the problem with having probability of something bad Because if there is a 0.5% chance that in the sense of bad generalization, and 0.5% for the second one, we could accumulate, and end up with a significant the hypotheses will be bad. And when one of the hypotheses will be this is the hypothesis we pick as our track E_out for the hypothesis So we need to accommodate the case where And the argument was extremely simple. g is our notation for the It is one of these guys that Well, the probability that E_in doesn't included in the fact that E_in for h_1 that one, or E_in for h_2 doesn't track, The reason is very simple. g is one of the guys. If something bad happens with g, it must least, the one that was picked. So we can always say that this implies this or this or this. And after that, we apply a very simple is the union bound. The probability of an event or another sum of the probabilities. That rule applies regardless of the because it takes the worst-case scenario. If all the bad events happen probabilities. If there is some correlation, get a smaller number. In all of the cases, the probability of equal to the sum of the individual And this is useful because in the coin argument, the events are independent. In the case of the hypotheses of independent, because we And we are only changing So it could be that the deviation here But the union bound doesn't care. Regardless of such correlations, you probability of this event. And therefore, you will be able to bound about, which has to do with the Hoeffding applied to each of those. And since you have M of them, So the final answer is that the after learning is less than or equal to small quantity, times M. And we realize because if you use a bigger hypothesis And therefore, the right-hand side here when you add the M. And therefore, at meaningless. And we are not even worried yet about be true for many hypothesis sets, meaningless. However, we weren't establishing We were establishing the principle generalize. And we have established that. It will take us a couple of weeks to that a general learning model, an infinite get the bound on generalization. That's what the theory of generalization So today the subject is linear models. And as I mentioned at the beginning, If I was following the logical sequence, the theory and take M, which takes care and then generalize it to However, as I mentioned, I decided to practical to work with early on. And then we will go back to The linear model is one of the most And what we are going to we're going to start with a practical and over in this class. And then, if you remember the perceptron first lecture, the perceptron So here is the sequence We are going to take the perceptron and That's a relief, because we already data is very rare. And we would like to see what non-separable data. Then, we are going to generalize this function is not a binary classification That also is a very important And linear regression, as you will techniques that is applied mostly in machine learning. Finally, as if we didn't do enough to take this and generalize All in a day's work, It's a pretty simple model. And at the end of the lecture, you will very general situations. And you may ask yourself, why am I I'm going to talk about nonlinear Well, you'll realize that nonlinear realm of linear models. That's not obvious. We will see how that materializes. So that's the plan. Now, let's look at a real data set that available to you to try And it's very important to try Regardless of how sure you are when you generate, you should always go for that you thought of performs So here is the data set. It comes from ZIP codes So people write the ZIP code. And you extract individual characters, And you would like to take the image, level pixels, and be able to decipher Well, that looks easy except that different ways. And if you look at it, there will Is this a 1 or a 7? Is this a 0 or an 8? So you can see that there And indeed, if you get a human operator and classify them, they will probably And we would like to see if machine which means that we can automate the So this is a data set that we Let's look at it a little bit more our algorithm. We have one algorithm so far, which is are going to try on this. And then we are going to generalize The first item is the question What do I mean? This is your input, the raw Now this is 16 pixels by 16 pixels. So there are 256 real numbers If you look at the raw input x, this dot, dot, and x_256. That's a very long input to encode And we add our mandatory x_0. Remember, in linear models, we have x_0 equals 1, we add in order to So this will always be in the mention it or not. If you take this raw input and try realize that the linear model in this has really just too many parameters. It has 257 parameters. If you are working in that is a huge space. And the poor algorithm is trying to all of these w's based on your set. So the idea of input representation is We know something about the problem. We know that it's not really that matter. You can probably extract some features the learning algorithm and let figure out the pattern. So this gives us the idea of features. What are features? Well, you extract the And as a suggestion, very simple one, case, instead of giving the raw input extract some descriptors of For instance, you look at this. Depending on whether this is the digital there is a question of the intensity, 1 doesn't have too many black pixels. 8 has a lot. 5 has some. So if you simply add up the intensity probably will get a number that It doesn't uniquely determine It's a higher-level representation there. Same as symmetry-- if you think of the digit If you flip it upside down, or you flip something that overlaps So you can also define a symmetry symmetric difference between something what you get. If something is symmetric, things will You'll get a very small value. And if something is not symmetric, let's lots of values in the symmetric And you will get a high So what you are measuring You take the negative of that, So you get another guy, So now, x_1 is the intensity variable, x_2 is the symmetry variable. Now admittedly, you have lost But the chances are you lost as much information. So this is a pretty good representation the learning algorithm is concerned. And you went from 257 dimensional That's a pretty good situation. And you probably realize that having generalization, if you extrapolate Having 3 is a much better situation. So this is what we are When you take the linear model have w_0, w_1, and w_2. And that's what the perceptron algorithm, to determine. Now let's look at the illustration You have these as your inputs. And x_1 is the intensity, x_2 is the symmetry. What do they look like? They look like this. This is a scatter diagram. Every point here is a data point. It's one of the digits, one And I'm taking the simple case of just So I'm only taking digits And you can always take other then combine the decision. If you can solve this unit problem, to the other problem. So when you put all the 1's and all the for example that the intensity on the intensity on the 1's. There are more pixels occupied This is the coordinate And indeed, the red guys, which happen a little bit more to the right, If you look at the other coordinate, symmetric than the 5. Therefore, the guys that happen to be higher on the vertical coordinate. And just by these two coordinates, you linearly separable. Not quite, but it's separable enough here, you will be getting Now you realize that it's impossible because, believe it or not, this fellow the guy who wrote it. So we have to accept the fact that is completely undoable. And we will accept an error. It's not a zero error. But hopefully, it's a small error. So this is what the features Now what does the perceptron What it does is this complicated figure, E_in and E_out as a function When you apply the perceptron learning E_in is the only value you have. E_out is sitting out there. We don't know what it is. We just hope that E_in tracks it well. Let's look at the figure. These are the iteration numbers. So this is the first misclassified You go and apply the perceptron learning for 1000 times. As you do that, E_in, which is the sometimes will go up. We realize that the perceptron learning point at a time, and therefore may mess of a point. So in general, it can go up or down. But the bad news here is that the And we made the remark that the very badly when the data is It can go from something pretty bad, in just one iteration. So this is a very typical behavior of Because the data is not linearly algorithm will never converge. So what do we do? We force it to terminate That is, we stop at 1000 and take And we call this g, the final perceptron learning algorithm. Now we obviously look at this, and we This is a better guy than the other. But you know, you're just applying Now, one of the things you observe You're not going to be able to plot E_out with, if E_out is really You may be able to estimate it using But all you need to know here is that just to tell you what is happening the in-sample error. And in this case, you find that E_out There is a difference. So if we go from here to here, It's a big epsilon. But the good news is When this goes down, this goes down. When this goes up, this goes up. So if you make your decision based on E_in, also be good. That's good for generalization. And that is one of the advantages of learning algorithm. It doesn't have too many parameters. And because of our efforts in getting parameters now. So the chances are that it will generalize Now what does the final This is only the illustration here, it's just-- this is the evolution. Eventually, you end up The hypothesis would separate the points So what does it look like? Well, it looks like this. This is your boundary. This is the final hypothesis, that at the final iteration. Well, it's OK, but definitely It's too deep into the blue region. You would have been better And the chances are maybe earlier error will do that. But that's what you have to perceptron learning algorithm. So now we go and try to modify the a very simple way, that is the simplest So let's see what happens. This is what that PLA did, right? And when we looked at it, we said: Well, this value is not a mystery. It happened in your algorithm. You can measure it explicitly. It's an in-sample error. And you know that it's better than So in spite of the fact that you're the prescribed perceptron learning according to one misclassified point-- in-sample error of the intermediate Right? And only keep the guy that happens So you're going to continue perceptron learning algorithm. But when you are at the end, you keep hypothesis. What an ingenious idea! Now the reason the algorithm is called whole idea is to put the best solution And when you get a better one, you take pocket, and throw the old one. And when you are done, report We can do that. What does this diagram look like, at the pocket algorithm? Much better. You can look at these values, and Here, we went down. And here, we indeed went down. Here, we went up. You see this green thing? Here, we didn't, because the good guy is reporting the value for. And we continued with it And we dropped again. And we never changed that, because guy than this guy. So when we come to iteration Now when you do that, you can use non-separable data, terminate it by pocket value. And that will be your And if you look at the classification what we had with the perceptron We complained a little bit that it's And when you look at the other guy, looks better. It actually does what we It separates them better. Still, obviously, it cannot Nothing can, because they are On the other hand, this is So with this very simple algorithm, inseparable data, but inseparable it's basically separable. However, it really is-- this guy is bad, and this guy is bad. There's nothing we can do about them. But there are few, so we will We'll see that there are other cases inseparable, in which we have to do So that's as far as the classification Now we go to linear regression. The word regression simply There is absolutely no other It's a glorified way of saying And it comes from earlier And there's so much work on it that And it is now the standard term. Whenever you have a real-valued a regression problem. So that's, with that, out of the way. Now, linear regression is statistics and economics. Every time you say: are these the first thing that comes to Let me give an example. Let's say that you would like to relate types of courses, to your This is what you do. You look at-- here are the courses I took. Here is the math, science, engineering, education, other. And you get your GPA in each of them. So here, I got 3.5 Here, I got 3.8 Here, I got 3.2 Here, I got 2.8 2.8? No, no. That doesn't happen at Caltech! You go for the other one, et cetera. So you just have the GPA's for the Now, you say-- someone graduates. I'm going to look 10 years their annual income. So the inputs are the GPA's in the The output is how much money they graduation. Now you ask yourself: how do these So apply linear regression, as you will find maybe the math and sciences Or maybe all of that is an illusion. It was actually the humanities You don't know. You will see the data, and the data And any other situation like that, regression. So in order to build it up, we are going in order to be able to contrast it with have seen before. What do we have? We have in the classification-- we have the credit approval, That's a classification function, binary is +1 or -1. In the case of regression, we will And the interpretation in this case is proper credit line for a customer. The customer applies. And it's not a question of approving Do you give them credit limit of $800 depending on their input? So this is a real-valued function. And we are going to apply regression. Now you take the input. This is the same input as we had before, are related to the credit behavior, I suspect that the salary will figure trying to tell the credit line, because you probably are not going to give So you can see that this will And there are other guys that merely stability of the person. Years in residence. If the person has been in the same unlikely to skip town. On the other hand, if they have been don't know-- that type of thing. So you have these variables. You encode them as the input x. And then your output in this case, which is a hypothesis form which takes Let's spend some time with First, it's regression because It's linear regression because the form, Now, we have seen this before. We sum up from basically 1 to d. These are the genuine inputs, of the input variables. And then we add the mandatory x_0, which threshold, which is w_0. This is the form we have seen before, before, we took this as a signal that If it's plus, we approve credit. If it's minus, we don't And we treated it as a credit take out the threshold. Now in this case, this is the output. We don't threshold it. We don't say it's +1 or -1. There is w_0 in. But we don't take it as We take it as a real number. And this is the dollar amount a credit line. Now the signal here will play all the linear algorithms. This is what makes the And whether you leave it alone as in threshold as in classification or, as we threshold, and you get a probability All of these are considered And the algorithm depends on this being linear. We also took the trouble to And the vector form will simplify the order to derive the linear But you can always-- if you hate always go back to this. There is nothing mysterious This simply has a bunch of parameters, And if I'm trying to minimize something, respect to scalar variables, which But we obviously will do it in the vector or the matrix form, in order to an easier way. So that's the problem. What is the data set in this case? Well, it's historical data, but it's The credit line is decided Someone sits down and evaluates your person gets 1000 limit, this person All we are trying to do in this what they're doing. We don't want the credit The credit officers sometimes are They may have a good day or a bad day. So we'd like to figure out what pattern deciding the credit, and have That's what the linear regression The historical data here are again And the previous customers-- So this is the application And this is the credit line No tracking of credit behavior, we're experts do in this case. And then you realize that each of these which is the credit line that And that real number will likely It's a credit line. It's a dollar amount. And what we are doing is trying That's the statement of the problem. So what does linear regression do? First, we have to measure the error. We didn't talk about that in because it was so simple. Here, it's a little bit less simple. And then, we'll be able to discuss classification as well. What do we mean by that? You will have an algorithm that tries These are the weights you're These weights are going to determine Some hypotheses will Some hypotheses will not. We would like to quantify that, to give to move from one hypothesis So we will define an error measure. And the algorithm will try to minimize hypothesis to the next. If you take linear regression, the the squared error. Let me write it down. Well, if you had a classification, there a particular example. You either got it right There is nothing else. Therefore, in that case, we Did you get it right or wrong? And we found the frequency And we got the E_in and E_out. Here, you are estimating So if the guy gets 1000, and you tell If the guy gets 1000, and you So you need to measure how And you define an error measure, simple squared error. Now, squared error doesn't have It just happens to be the standard regression. And its merit really is the simplicity going to get. But when we discuss error measures in the principle, does error Why? How do we choose it? Et cetera. This will be answered in But for this time, let's take this are going to use. When you look at the in-sample error, On the particular example n, example, this is the contribution Each of these is affected by the So as you change w, this value will And this is the error in that example. And if you want to get all the in-sample average of those. That will give me a snapshot doing on the data set. And now, we are going to ask our minimize it. Let's actually just look at what This is the simplest case The input is one-dimensional. I I want to relate your overall GPA to Your overall GPA is x. Your earnings 10 years from now is y. That's it. OK? [CHUCKLES] I would have properly called this And then there would be an x_0, But I didn't bother, because But this is what we have. So you look at this. And you see that, for different Wow. Your earnings are going down with-- Well, that may not have been the What linear regression does is it tries you have here, that tries to fit squared-error rule. So it may look like this. And in this case, the threshold The slope depends on w_1, which And that is the solution you have. Now you didn't get it right, but And you realize that-- this is This is the error on And if you sum up the squares of the called the in-sample error that we defined Well, linear regression can apply And I can plot 2 dimensions here It's the same principle. What you have here is you have x_1. If I can get the pointer-- OK, we'll leave it to rest. We have x_1 and x_2. And in this case, the linear And you're again not separating, but And you're making errors. And in general, when you go to which is the reason why we call it It's a hyperplane, one dimension short And that's what you are trying to Now let's look at the And that is the analytic expression And that will make us derive the We wrote this before. And you have the value of the That is because it's a squared error. And because it's linear regression, this transposed x_n. It's a linear function of x_n. Now let us try to write this I will explain this in detail. But let's look at this. Instead of the summation, all of something that is-- Capital X, I haven't seen I haven't seen vector y before. Well, it's basically a consolidation x_n is a vector. So you put the vectors in a matrix. You call it X. And you put the And you call it y. The definition of capital X and For the matrix X, what you do-- you So this would be the constant coordinate 1, second coordinate, up to the d-th And then you go for the second construct this matrix. And for y, you put the This is the output for the first example, output for the last example. Now one thing to realize that it's pretty tall. The typical situation is that We reduced them to three, for example, the digits. But you usually have many, many So this will be a very, Now the way you take this-- well, the vector transposed times itself. And when you do it, you realize that contributions from the And each component happens to be exactly So this becomes a shorthand for Now, let's look at minimizing E_in. When you look at minimizing, you realize the inputs of the data, and y, which has far as we are concerned, constants. This is the data set someone gave me. The parameter I'm actually playing hypothesis is w. So E_in is of w. And w appears here. And the rest are constants. If I do any calculus of minimization, So I try to minimize this. And what you do-- you get the derivative except here, it's a glorified You get the gradient, which is of them all at once. And there is a formula for it, which I will explain it. By the way, if you hate this, because linear regression is so it's true, you can always go for the every w: partial w_0, partial get a formula that is a pretty hairy And-- surprise, surprise-- you will get the form in two steps. Now if you look at this, deal with it in a simple square. If this was a simple square, and w derivative be? You will get 2 sitting outside. Well, you've got it here. And then you will get the same You got it here. And then you will get whatever constant outside, which you got here. You just got here with a transpose, This is the transpose of That's where you get the transpose. Pretty straightforward and So that's what you have. And then you equate this It's a vector of 0's. You want all the derivatives And that will define a point where Now, you would suspect that the solution is a very simple quadratic form. And indeed, the solution is simple. And if you look at it, you realize that I want this to cancel out. So when I multiply X transposed X w, I get So they cancel out, and I get my 0. So you write this down, and you find I want this term to be And that will give me the 0. The interesting thing is that in spite X is a very tall matrix, definitely X transposed X is actually a square way and X is this way. You multiply them, and you get And as we will see, the chances are invertible. So you can actually solve this very You multiply by the inverse You multiply by this. This will disappear, and you will get were trying to solve for. And when you do that, you will get w What is X dagger? This is simply a shorthand So I got the inverse of that, and So this is really what I get I call it X dagger. And indeed, it gets multiplied Now the X dagger is a pretty It's called the pseudo-inverse of X. not have an inverse. But it does have a pseudo-inverse. And the pseudo-inverse has For example, if you take this, the X dagger times X-- what do you get? You add X here. You get X transposed X. Oh, I have So they cancel out, and So when I multiply X dagger So it's OK to call it It doesn't work the other way around. The other way around gives us about later. But basically, this is If we were in a trivial situation I have 3 parameters, and I have that can be solved perfectly. I can actually get this to be 0. And how would you get it to be 0? You would just multiply by the proper will get X inverse y. So this is pretty much similar, And we are not going to get a 0. We're just going to get a minimum Now I would like you to appreciate the point of view. This is the formula for the pseudo-inverse compute, in order to get the solution So let's look at it. Something is inverted. And when you see inversion in matrix, If this was a million by a million, If this is 5 by 5, I'm in good shape. So we'd like to know, what kind Well, nothing mysterious about You have this fellow, which It's d plus 1, d is the 1 is the added constant variable. So these are the number of parameters. This would be 3 in the digit We have only x_1 and x_2, so d equals 2. d plus 1 equals 3, which corresponds So this is 3 times N. That's the number of examples. That could be in the thousands. Now you multiply this by X, The multiplication will be-- multiplication is Even if this is 10,000, I can But the good news is that when I go to with a simpler guy. Let's just complete the formula first. This is what you have. This is what you are computationally And if you look at what's inside That is what the matrix inside is. It's just 3 by 3 in our case. You can invert that. Just accumulating it is the one that the examples. And there's a very simple It's not that difficult And you can see now that, oh, good If we had the 257 parameters to begin Not that this will discourage us. But if you go for some raw inputs, you thousands or sometimes So the computational aspect And there are so many packages for outright getting the solution for linear have to do that yourself, except specialized. If you do have something very So that is the final matrix. And the final matrix will have the And if you look at it, this will Multiplied by y, which is y_1, to different outputs. And then, as a result of that, w_0, w_1, up to w_d. Indeed, if you multiply this by an N tall vector, and that's Let's now flash the full linear That's a crowded slide. That is what you do. The first thing is you take the data the proper form. What is the proper form? You construct the matrix And these are what we This will be the input data matrix, and And once you construct them, you are going to do-- you plug this into And then you will return the value w, pseudo-inverse with y. And you are done. Now you can call this one-step With the perceptron learning algorithm, learning, because I have And then I take one example at a time, on, move this around, et cetera. And after 1000 iterations, It looks more like We learn in steps. This looks like cheating. You give me the thing, And you have the answer. Well, as far as we are concerned, If it's correct and gives you a correct And because this is so simple, this is often, and used often as a building We can afford to use it as a building simple that we can become more Just one remark about the inversion-- this has to be invertible in order Now the chances are, that this will be have, is close to 1. The reason is the following. Usually, you use very few parameters You will be very, very, very unlucky other that you cannot even capture the number of columns. The number of columns is 3, 5, 10, So the chances are overwhelming in invertible. Nonetheless, if it is not invertible, pseudo-inverse. It will not be unique and has it's not a big deal. That is not a situation you will So now we have linear regression. I'm going to tell you that you can a real-valued function, for But you're also going to be able Maybe the perceptron is now It has a competitor now. And the competitor has a very So let's see how this works. The idea is incredibly simple. Linear regression learns Yeah, we know that. That is the real-valued function. The value belongs to the real numbers. Fine. Now the main observation, the ingenious binary-valued functions, which are the real-valued. +1 and -1, among other things, So linear regression is not going to Right? So what do we do? You use linear regression in order to is approximately y_n in the For every example, the actual value numerical +1 and the numerical -1. That's what linear regression does. Now, having done that with y_n equals +1 this case, if you take the take the sign of that signal in order +1 or -1. If the value is genuinely close to the chances are when it's +1, And when it's -1, it's negative. The chances are-- you're getting close to zero in doing that. And if you cross the zero, the So if you take this, and then plug it in will likely get something likely to agree with +1 or -1. That's a pretty simple trick, All you need to do-- I have a classification problem. Let's run linear regression. It's almost for free. Do this one-step learning, get classification. Now, let's see if this is Well, the weights are good for conjecture. But they also may serve as good initial Remember that the perceptron algorithm, very slow to get there. You start with a random guy. Half the guys are misclassified. And it just goes around, tries to until it gets to the And then it converges. Why not give it a jump start? Why not run linear regression We know that the w's are OK, but they classification. But they're good initial condition. Feed those to the pocket algorithm, and is a classification solution. That's a pretty nice idea. So let's actually look at the Now, I take an example here. Again, I have the +1 class And I applied-- we're trying to find, what is the Now, we remember, the blue region classification. When you talk about linear regression, And the signal is 0 here. The signal is positive, more positive, And here, the signal is negative, negative, more negative. There is a real-valued function that a classification by taking the sign. Now, if you look at what the linear use it for classification, a target value -1. It is actually trying to make -1 to all of them. So the chances are, these This will be -2, -3. And the linear regression algorithm It considers it an error, in spite of classification, it just And that's all we care about. But we are applying linear regression. It is actually trying very hard to make time, which obviously it cannot. And you can see now the problem In its attempt to make this -8, level where it's in the middle And now, it's very happy because it But that's not really Nonetheless, it's a good And then you take the classification now, tries to adjust it according And you will get a good boundary. That's the contrast between applying and linear classification outright. Now we are done. I'm going to start on nonlinear And I'm going to give you a very Here is the deal. You probably realized that, even when are dealing with non-separable data that with few exceptions. But in reality, when you take a real find that the data you are going It could be, for example, something So you want to classify these as Let's take the classification Now I can put the line anywhere. And obviously, I'm in trouble because a long shot. You can look at this and say: Closer to the center, you have blues. Closer to the peripherals, So it would be very nice if that looks like this. Yes. The only problem is that We don't have the tools Wouldn't it be nice if in two viewgraphs, and linear classification, the to this guy? That's what will happen. I told you this is So we take another example We take the credit line. Now if you look at the credit line, the in residence. We argued that if someone has been in there is stability and And someone has been a short time, Now one thing is to say that this is Another thing to say is that affects the output linearly. It would be strange if I'm trying to the credit line will be proportional have lived in residence. If you have 10 years, 20 years, I will It doesn't make sense. Because stability is established you get to 5 years. After that, it's diminishing returns. So it would be very nice if I can define nonlinear features, Let's take the condition, the logical are less than 1. And in my mind, I'm considering that You haven't been there for very long. And another guy, which is x_i greater than 5 years. So you are stable. The notation here, when I put something returns 1 if the condition is true, and So this is 1, 0, and this is 1, 0. Now if I had those as variables in my more friendly to the linear formula in the crude input. But these are nonlinear And again, we have the nonlinearity. And we wonder if we can apply the same This is the question. Can we use linear models? The key question to ask What do I mean? Look at linear regression. What does it implement? It implements this. This is indeed a linear formula. And when you look at the linear implements this. This is a linear formula, and the part being linear. And then you just make a decision Now, these you would think are called x's, which they are. Yeah, I get these inputs. And I combine them linearly. And I get my surface. That's why I'm calling it linear. However, you will realize that, are linear in w. Now when you go from the definition the roles are reversed. The inputs, which are supposed to be a function, are now constants. They are dictated by the training set. They're just a bunch of numbers The real variables, as far as learning The fact that it's linear in the the perceptron learning algorithm, and If you go back to the derivation, it The x's were sitting And their linearity in w is what That results in the algorithm linearity in the weights. Now that opens a fantastic possibility, which are just constants. Someone gives me data. And I can do incredible nonlinear And it will just remain more elaborate When I get to learn using the still in the realm of linear models, to the nonlinear feature will Let's look at an example. Let's say that you take x_1 and x_2. I omitted the constant x_0 And these are the guys These are the coordinates. This is x_1. This is x_2. These guys should map to +1. These guys should map to -1. I don't have a linear separator. OK, fine. These are data, right? So everything that appears within this and corresponding constants y. Now I'm going to take I'm going to call it phi. Every point in that space, I'm going And my formula for transformation I'm assuming here that the origin of So I'm taking x_1 squared And you can see where I'm leading, from the origin. And that seems to be Now in doing this, all I did was take Now, you can look at this and say: I take your original training data, do the original one. Can you solve the problem Oh, yes you can, because that's what All of a sudden, the red guys, which bigger values for x_1 squared They will sit here. And the guys that are closer to the them, they will have smaller So this is now your new data set. Can you separate this Yes, I can. I can put a line going through here. Great. When you get a new point to classify, it here, and then report that. That's the game. And there is really no limit, in terms of what you can do here. You can dream up really elaborate the data, and then do There is a catch. And it's a big catch. I will stop here. And we'll continue with the nonlinear next lecture. And we'll take a short break now, before We have from the online audience. MODERATOR: A popular question is how to figure out in transformations, PROFESSOR: I said transformation is a loaded question. And there will be two steps I will talk about it a little bit more next lecture. And then we are going to talk about the can do and what you cannot do, after we because it is very sensitive to And that should not come as a surprise, the input, which is, let's say, two parameters. And I want the transformation to be as stand a good chance of being able So I'm going to go all out. I'm just going to keep getting x_1, x_1 squared, x_1 cubed, x_1 just go on. Now at some point, you should smell have this very, very long vector and And generalization may become an issue, So there are guidelines for And also, there are guidelines Do I look at the data and figure transformation? Is this allowed? Is this not allowed? What the ramifications are? All of these will become clear only MODERATOR: OK. There's a question about slide 15. So regarding the expression of E_in. How does the in-sample error here, or to the probabilistic definition PROFESSOR: OK. Here we dealt only with So we decided on E_in. And in general in learning, you only You have on the side a guarantee that do well out-of-sample. So you never handle the out-of-sample You just handle the in-sample, and have you are doing will help Now, the error measure here Therefore, when you define the in-sample error and average it. And when you define the out-of-sample of the squared error. Now in the case of the binary You're either right or wrong. So you can always define the average of the question. Am I right or wrong on every point? So if you are right, there's no error and you get 0. If you are wrong, you get 1. So you ask yourself: what is the And that would give you The expected value of that probability of error. That's why we simply, without going into versus out-of-sample expected value-- in the case of classification, we simply and probability of error, not because they are simple to state. But in reality, the aspect of them that and out-of-sample is that the probability an error measure that happens to And the frequency of error happens of that error measure. STUDENT: So you showed us a very nice dependence of future income and-- PROFESSOR: This I didn't think of the income at So any implication that you should gain more money is-- I disown any such conclusion! STUDENT: OK. But you mentioned the example of point average, or at least finding So the question I'm interested PROFESSOR: You can get-- obviously, the alumni association track of the alumni. And they send them questionnaires. And they have some of the inputs, There are a number of parameters. So there will be a number of And actually, this is actually used. If you realize that something is can go back and revise your curriculum So the data is indeed available, STUDENT: I mean, it's available But can we get it? PROFESSOR: Oh, we get it. I thought it was generic we. I don't-- obviously, the data will be You'll just get the GPA and the income, You are dependent on the kindness of the schools, I guess. Or maybe there are some available I have not looked. So my understanding is that you want happens, and then focus your time That's the idea now? That's your feedback? MODERATOR: A technical question. Why is the w_0 included in So there's a confusion about this. And specifically in the binary case? How do you incorporate the There's some people asking about this. PROFESSOR: Let me I'll talk about the threshold first. Why the threshold is there, right? Let's look here. If you look at the line here, The linear regression line is It doesn't pass by the origin. If I told you that you cannot use the equation goes away, and the pass through the origin. Can you imagine if you were trying Obviously, it would be down there or if you want to pass through So obviously, I need the constant And in general, there is values of these variables. And the offset is compensated That's why we need the threshold What is the second question? MODERATOR: In the binary case, when does that just work? PROFESSOR: Well, if you apply following guarantee at the end. The hypothesis you have has the targets on the examples. That's what has been achieved by the Now the outputs of the examples we can put that together with And then we realize that the output the value +1 or -1 with The leap of faith is that, if you are the chances are when you are close to And when you are close to -1, If you accept that leap of faith, then the threshold of the value of the signal will get the classification right Negative will give you -1. This is not quite the case, because in all the points, the signal for linear mentioned, +7 for some points And the linear regression is trying to being the boundary, in order to So in attempting to fit stuff that is it may mess up the classification. And that's why the suggestion is, don't classification. Just use it as an initial weight, and something as simple as the pocket further in order to get the classification suffer from the numerical angle. MODERATOR: So also on that, does it +1, -1, or something else? PROFESSOR: OK. If it's plus something and minus the same If it's plus and minus, and not absorbed in the threshold. So it really doesn't matter. It will just make things MODERATOR: Regarding the first part of the lecture, how with features? PROFESSOR: OK. The best approach is to look at the statement, and then try to infer feature for this problem? For example, the case where I talked It does make sense to derive some dependency. There is no general algorithm This is the part where you work represent the input in a better way. And the only catch is, if you look to derive the features, there is a problem there that we come to the theory. But the bottom line is that, if you don't the problem and derive features based helpful if you don't have If you have too many of them, it But something-- first order, usually when I get And I probably can think of less that will be helpful. And I put all of them. And usually, a dozen variables in input space by much. These are big problems. So I don't suffer much from MODERATOR: So added to that, so the nonlinear transformations-- they become features? PROFESSOR: Yeah. The word feature, we There's a feature space And anything that you take the input and this will be called feature. And features of features So if you take for example the the pixel values. That's the raw input. And then we had the symmetry These were features. If you go further and find nonlinear also be called features. A feature is any higher-level MODERATOR: Another question is: how we cannot assume that the data-- PROFESSOR: Not clear So there is really-- I think I get it. Probably when we get the inputs, the dependence. And the independence was used in That's probably the direction The independence was from one So I have N inputs. And I want these guys to be generated a probability distribution. If they were originally independent, transformed the other, the independence There is no question of independence The independence was a question the different inputs. MODERATOR: So the different inputs. PROFESSOR: Different MODERATOR: So another question is, are hyperplanes and intersections PROFESSOR: Correct. The linear model that we have described many models in machine learning. You will find that if you take a linear not the hard-threshold version, and you will get a neural network. If you take the linear model, and you try a principled way, you get If you take the nonlinear a computationally efficient way of doing So there are lots of methods within linear model. The linear model is somewhat It's not glorious. It's not glorious, The interesting thing is that if you chance that if you take a simple linear achieve what you want. You may not be able to brag about it. But you are going to do the job. And obviously, the other models will some cases. MODERATOR: So a question, getting assess the quality of E_in PROFESSOR: This is E_in is very simple. I have the value of E_in. I can assess its value by just I can evaluate it at any given point. And this is what makes the algorithm hypothesis, by picking the one that The out-of-sample error, There will be some methods described an explicit estimate of the But in general, I rely on the theory error tracks the out-of-sample error, in-sample error, and hope that the have seen in the graph when we were perceptron. And the in-sample error And the out-of-sample error was also a discrepancy between the two. But they were tracking each other. MODERATOR: So here's a question If you want to fit a polynomial, is PROFESSOR: Correct. Because right now, let's say we have case I gave. So you have x and y. Now you have a line. If you use the nonlinear transformation, x to x, x squared, x cubed, x to the a line to the new space. And a line in the new space will be So this is covered through the MODERATOR: What is the relation least squares with maximum likelihood estimation. PROFESSOR: OK. When you look at linear regression in many more assumptions about the And you can get actually Under certain conditions, you can You can say, Gaussian goes And in this case, minimizing it will So there is a relationship. On the other hand, I prefer to give the of machine learning, without making too and whatnot, because I want it to be than applied to a particular As a result of that, I will be able to probability of being right or wrong. I just have the generalization from But that suffices for most of the So there is a relationship. And it's studied fairly well But it is not of particular logic that I'm following. MODERATOR: So a popular question is: can nonlinear transformations used? PROFESSOR: There will be many. When we get to support vector a number of transformations, some of were mentioned. One of the useful ones is referred We will talk about that as well. So there will be transformations. And the main point is to be able to cannot do, in terms of jeopardizing the a nonlinear transformation. So after we are done with that theory, freedom of choosing what nonlinear And we'll have some guidelines of some So this is coming up. MODERATOR: I think you already But again, someone asks, is it find a pattern of a pseudo-random PROFESSOR: Well, if it's pseudo you get the seed, you can produce it. But the way it's usually used is you use you take a few bits and have them as So just looking at the inputs and trying impossible. So it's a practical question. Philosophically, yes you can. Practically, it looks random MODERATOR: So what are the different versus discrete responses in I guess-- PROFESSOR: Yeah. Obviously, this is dictated If someone comes, and they want to the classification hypothesis set. If someone wants to get a credit line or use regression. So it really is dependent And the funny part is that real numbers Yet the algorithm that goes with them, easier than the other one. The reason is that the other And combinatorial optimization is So the answer to the question is that it the person is coming up with. And when there is cross fertilization it's just a way to use an analytic other one a jump start, or to give But it's a computational question. The distinction is really in the MODERATOR: Can you say what makes PROFESSOR: OK. I will be able to talk about this the theory. I would like to emphasize that the giving us all the tools to talk, with are being raised. So there is a reason for including the This lecture was meant to give you just that you use, and if you look at it now, and many data sets, because now you You can deal with real-valued data. And you can even deal with some So it's just a toolbox for you And then things will become develop more material. MODERATOR: Yeah, I think that's it. PROFESSOR: OK, that's it. We will see you on Thursday.