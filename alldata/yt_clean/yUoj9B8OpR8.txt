- Good afternoon, everyone. Great to see a nice packed My name is Alexis Abramson, dean of Thayer School of and it's truly a pleasure to this very special event, a conversation with Mira Murati, one of our nation's foremost leaders in artificial intelligence and also a Dartmouth Engineering alum. Before we get started, I wanna extend a special Joy Buolamwini, who is also renowned for her work in AI, AI ethics, and algorithmic justice. She'll also be receiving from Dartmouth tomorrow. And a warm welcome to either are part of her family now, or are part of her family when including her brother, Ernel Murati, also a Thayer alum from the class of 2016. Thank you to our partners at the Neukom Institute and the Department of Computer Science. From Dartmouth's very on artificial intelligence in 1956 to our current multidisciplinary research on large language models Dartmouth has long been at the So we are especially Chief Technology Officer at Open AI and their School of with us today. She is known for her of the most talked about AI At Open AI, she has of transformative models setting new the stage for future Now, during her time she applied her engineering and built hybrid race cars with Dartmouth's Formula Racing Team. Tomorrow at commencement, she will receive an honorary from Dartmouth. Finally, moderating our conversation today is Jeff Blackburn, Dartmouth class of 1991 and current Dartmouth trustee. Jeff's extensive career of global digital media and technology. He served as senior vice president of Global Media and Entertainment and has had various leadership his insights into the and media, and entertainment will certainly make sure we have an engaging conversation today. So without further ado, Please join me in welcoming (audience applauding) - Thank you, Alexis. And this beautiful building, so nice. Mira, thank you so much for I can only imagine how crazy - It's great to be here. - It is so nice of you to take - Really happy to be here. - And I wanna get right to it because I know everybody just wants to hear what's going on in your life and what you're building 'cause it's just fascinating. Maybe we should just start with you, and you leave Thayer, you go to Tesla for a bit, then OpenAI. If you could just describe and then joining OpenAI in the early days. - Yeah, so I was... Right after Thayer, I actually and then I sort of realized that aerospace was kind of slow-moving and I was very interested and of course really innovative challenges in building basically a sustainable future for transportation, and And after working on Model S and Model X, I thought I don't really I kind of want to work at the intersection of really advancing society forward in some way, but also in doing this really And at the time when I was at Tesla, I got very interested in self-driving cars and sort of the intersection computer vision and AI, applying And I thought, okay, I'd but in different domains. And that's when I joined the startup where I was leading to apply AI and computer vision in the domain of spatial computing, so thinking about the next And at the time, I virtual reality and augmented reality. Now I think it's a bit different, but I thought, what if to interact with very complex information, whether it's formulas, or molecules, or concepts in topology? You can just learn about these in a much more intuitive way, and that expands your learning. So it turned out VR was And so... But this gave me enough to learn about AI in a different domain and sort of I think my career at the intersection of technology and it gave me a different perspective of how far along AI was and what it could be applied to- - So the Tesla self-driving, you saw machine learning, deep learning. You could see where this is going. - Vision, yes. - But not clearly- - I did, yes, in the last year especially. But it wasn't totally At the time, it was still apply not generally. You're applying it to very and it was the same in VR and Ar. And from then I thought I don't really want to just I want to learn about just the research and really and, from there, then go So this is when I joined OpenAI, and Open AI's mission It was a nonprofit back then, and the mission hasn't changed. The structure has changed, but when I joined six years ago, it was a nonprofit geared to build safe, artificial general intelligence, and it was the only other than DeepMind. Now of course there are a lot of companies that are sort of building - A handful, yeah. And that's sort of how the - Got it. And so you've been building I mean, maybe we could just some AI basics of machine learning, deep learning, now AI. it's all related, but it So, what is going on there and how does that come out or your video product? How does it work? - It's not something radically new. In a sense, we're building of human endeavor. And in fact, it did start here. And what has happened in is this combination of these three things where you have neural networks, and a ton of compute. And you combine these three things, and you get this really that it turns out they can like general tasks, but it's not really clear how. Deep learning just works. And of course we're trying to understand and apply tools and research to understand how these but we know it works from just having done it And we have also seen the and how the systems have When you look at systems that like GPT-3, large language models that we deployed about three, yeah, 3 1/2 years ago. GPT-3 was able to sort of... First of all, the goal of this model is just to predict the next token. - [Jeff] It's really next word prediction. - Yes, pretty much. - And then we found out this objective to predict the next token, and you've trained it on a ton of data, and you're using a lot of compute, what you also get is this model that actually understands language at a pretty similar level to how we can. - [Jeff] 'Cause it's read a lot of books. it's read all the books. - It kinda knows- - What words should come next. But it's not memorizing what's next. It is really generating an understanding of its own understanding of the pattern of the data that it has seen previously. And then we found that, Actually, if you put different like code, it can code too. So, actually, it doesn't care what type of data you put in there. It can be images, it can be video, it can be sound, and it can do exactly the same thing. - [Jeff] Oh, we'll get to the images. Yeah. (Jeff laughs) But yes, text prompt can and now you're seeing even the reverse. - Yes, yes, exactly. So you can do... So we found out that this formula actually works really well, data, compute, and deep learning, and you can put different types of data, you can increase the amount of compute, and then the performance gets better and better. And this is what we They're not actual laws. It's essentially like a of the capability of the model improving as you put in more And this is what's - [Jeff] Why did you start with a chatbot? - So, yeah, in terms of product, actually, we started with the API. We didn't really know how It's actually very, very difficult to commercialize AI technology. And initially, we took this for granted, and we were very focused and doing research. And we thought, here commercial partners, take it and go build amazing And then we found out that And so this is why we And we- - [Jeff] That led you to build a chatbot 'cause you just wanted to- - Yes, because we were okay, why is it so hard for this really amazing successful companies to actually turn this technology - I see. - And it's because it's a very You're starting from capabilities. You're starting from a technology. You're not starting from what that I'm trying to address. It's very general capability. - And so that leads to pretty quickly what you just described more compute, more intelligence. How intelligent is this gonna get? I mean, it sounds like your description is the scaling of this is pretty linear, you add more of those Has it gotten smarter ChatGPT and how quickly will it get to maybe human-level intelligence? - So yeah, these systems in specific tasks, and of course in a lot if you look at the systems like GPT-3, we're maybe let's say toddler level intelligence. And then systems like GPT-4 are more like smart high schooler intelligence. And then in the next couple of years, we're looking at PhD-level - [Jeff] Like? - So things are changing and - Meaning like a year from now? - Yeah, a year and a half let's say. - Where you're having a and it seems smarter than you. - In some things, yeah. In a lot of things, yes. - Maybe a year away from that. - I mean, yeah, could be. - Pretty close. Roughly. Well, I mean, it does lead and I know you've been very vocal on this, which I'm happy and on the safety aspects of it, but, I mean, people do want So I mean, what about three years from now when it's unbelievably intelligent? It can pass every single and every test we've ever done. And then it just decides it wants to connect to the internet on its Is that real, and is that... Or, is that something as the CTO and leading - Yes, we're thinking a lot about this. It's definitely real that you'll have AI systems that will connect to the internet, agents connecting to each or agents working with humans So sort of working with AI like we work with each other today. In terms of safety, security, the societal impacts aspects of this work, I think these things It can be that you sort and then you have to figure out how to deal with these issues. You kind of have to build and actually in a deeply And for capabilities and safety, they're actually not separate domains. They go hand in hand. It's much easier to direct a okay, just don't do these things. They need to direct a It's sort of like training a smarter dog versus a dumber dog, and so intelligence and - [Jeff] It understands because it's smarter. And so there is this whole do you do more safety or do you And I think that's a bit misguided because of course you have to think about the safety into deploying a product and the guardrails around that. But in terms of research and development, they actually go hand in hand. And from our perspective, the way we're thinking about this is approaching it very scientifically. So let's try to predict the capabilities that these models will be, the capabilities that before we actually finish training. And then along the way, let's prepare the guardrails That's not really been the We train these models, and then there are these emergent because they emerge. We don't know they're going to emerge. We can see sort of the but we don't know whether means that the model is or at doing biochemistry, And developing this new science helps us prepare for what's to come. And that means... - [Jeff] You're saying it's kind of consistent - Yes, that's right. - Yeah, so you have to - But What about these issues, Mira, like the video of Volodymyr Zelensky the Tom Hanks video, or a dentist ad? I can't remember what it was. What about these types of uses? Is that in your sphere or does it need to be How do you see that playing out? - Yeah, so I mean, my this is our technology. So it's our responsibility how it's used, but it's also shared responsibility with society, civil society, government, content makers, media, and so on, to figure out how it's used. But in order to make it you need to bring people along, you need to give them access, you need to give them tools to understand and to provide guardrails. And I think- - [Jeff] Those things are right? - Well, I think it's not but it's really a question And providing people the tools to do that. And in the case of it's very important to bring them along and give them early access to things, educate them on what's going on. - Governments. - Yes, for sure, and regulators. And I think perhaps the that ChatGPT did was bring AI into the public consciousness, give people a real intuitive sense for what the technology is It's a different thing versus when you try it and and you see, okay, it but it can do this other amazing thing, and this is what it actually or for my business. And it allows people to prepare. - Yeah, no, that's a great point. I mean, just these interfaces are informing people about what's coming. I mean, you can use it. You can see now what's underneath. Do you think there's... Just to finish on the government point. I mean, let's just talk the US right now. Do you wish there was certain regulations putting into place right now? Before you get to that It's extremely intelligent, So are there things that - We've been advocating on the frontier models amazing capabilities because of misuse. And we've been very and working with regulators on that. On the more sort of near I think it's good to allow for a lot of breadth and and not let people that in compute or data not, sort of not block the So we've been advocating in the frontier systems where the risks are much higher. And also, you can kind of versus trying to keep up with changes that are already happening really rapidly. - But you probably don't regulating your release of GPT-5, like that you can or cannot do this. - I mean, it depends, actually. It depends on the regulation. So there is a lot of that has now been sort the White House' commitments, and this- - So it's underway And it actually informed or what the UN Commission is doing with the principles for AI deployments. And usually, I think the way to do it is to actually do the work, understand what it means in practice, and then create regulation based on that. And that's what has happened so far. Now, getting ahead of these we do a lot more forecasting and science of capability prediction in order to come up with - [Jeff] Well, I hope that can understand what you're doing. - It seems like more and more folks are joining the government that have better understanding - Okay. In terms of industries, you have the best seat maybe in the world to just see how this is gonna I mean, it already is and media, and healthcare. But what industries do you do you think are gonna and the work that you're doing at OpenAI? - Yeah, this is sort of that I used to get from entrepreneurs when we started building where people would ask me, &quot;What can I do with it? What is it good for?&quot; And I would say everything. So just try it. And so it's kind of similar in the sense that I think it'll affect everything, and there's not going to in terms of cognitive work and the cognitive labor and cognitive work. Maybe it's gonna take a little bit longer to get into the physical world, but I think everything Right now we've seen... So I'd say there's been a bit of a lag in areas that have a lot of, that are high risk, such as And so there is a bit of a First, you want to use cases that are really make sure those are before applying it to And initially, there should and then the delegation should change, and to the extent they can - [Jeff] Are there use cases or are seeing, or are about to see? - Yeah, so I think basically the first part of anything whether it is creating new designs, whether it's coding, or writing an essay, or writing an email or basically everything, the first part of everything becomes so much easier. And that's been my favorite use of it. so far I've really used it- - Yeah, first draft for everything. It's so much faster. It lowers the barrier to doing something and you can kind of focus on the part that's a bit more especially in coding. You can sort of outsource - Documentation and all that kinda stuff. - Yeah, documentation and... But in industry, we've Customer service is definitely a big application with chatbots, and writing, also analysis, because right now we've sort to the core model, and this makes the models far more usable and more productive. So you have tools like code analysis. It can actually analyze a ton of data. You can dump all sorts of data in there, and it can help you analyze or you could use images and you could use browsing tool. So if you're preparing let's say a paper, the research part of the and in a more rigorous way. So I think this is kind of the next layer that's going adding these tools to the core models, and making it very seamless. The model decides when to versus search versus something else. - [Jeff] Write a program. yeah, yeah. Interesting. Has it watched every TV and is it gonna start writing - Well, it's a tool. And so it certainly can do that as a tool, and I expect that we will actually, we will collaborate with it, and it's going to make And right now if you think about how humans consider creativity, we see that it's sort of that's only accessible to this very few talented And these tools actually make it, lower the barrier for anyone to think of themselves as creative and expand their creativity. So in that sense, I think it's actually going to be really incredible. - Yeah, could give me 200 for the end of episode one or whatever, very easily. And you can extend the story, the story never ends. You can just continue. I'm done writing, but keep going. That's interesting. - But I think it's really going especially in the creative spaces where- - I do too. - Yeah, more people will - There's some fear right now. - But you're saying that'll switch and humans will figure out how to make the creative part of the work just better? - I think so, and some creative jobs maybe will go away, but maybe they shouldn't have if the content that comes out of it is not very high quality, but I really believe that using it as a tool for education, creativity will expand our intelligence, and creativity, and imagination. - Well, people thought were gonna wreck the film They were quite scared. This is, I think a bigger thing, but yeah, anything new like that, the immediate reaction is gonna be, &quot;Oh god, this is...&quot; But I hope that you're Okay, the job part you raised, and let's forget Hollywood stuff, but there's a lot of jobs that they think are at risk. What's your view on job displacement in AI and really not even just the just over overall. Should people be really and which kind of jobs, or how do you see it all working out? - Yeah, I mean the truth is the impact that AI is And the first step is to what the systems are capable integrate them in their workflows, and then start predicting And also, I think people these tools are already being used, and that's not being studied at all. And so we should be studying with the nature of work, and that's going to help us predict for how to prepare for these In terms of jobs specifically, but I certainly anticipate that a lot of jobs will change, some jobs will be gained. We don't know specifically but you can imagine a lot that are just strictly repetitive and people are not advancing further, those would be replaced. and testing code, and things like that, those jobs are- - Unless they are- - Yes, and if it's strictly - And it's just one example. There's many things like that. - Do you think there'll be to compensate for that? - I think there are going but the weight of how how many jobs are changed, I don't know. And I don't think anyone knows really, because it's not being rigorously studied, and it really should be. And yeah, but I think the and there is going to be by these tools. And so the question is, how If the nature of jobs really changes, then how are we distributing sort of the economic value into society? Is it through public benefits? Is it through UBI? Is it through some other new system? So there are a lot of questions - There's a big role for higher ed in that work that you're describing there. It's just not quite happening yet. - Yeah. - What else for higher What do you think is the role of higher ed in what you see and how this is evolving? - I think really figuring out how we use these tools and Because I think one of the most powerful applications of AI is advancing our creativity and knowledge. And we have an opportunity to basically build super and very accessible and ideally in any of the languages that you can imagine. You can really have and customized education And of course in the classrooms are smaller and but still you can imagine even here, let alone in - Supplementing. Because we don't spend enough That sort of happens very And that is such a fundamental otherwise you can waste a lot of time. And the classes, the everything can be customized to how you actually - So you think it could really, it could compliment some of Oh, absolutely, yeah. as tutors and what not. Should we open it up? Do you mind taking some Is that okay? - All right. Why don't we do that. Dave, you wanna start? - [Dave] Sure, if you don't. - [Speaker] Hold on one second. I'll give you a microphone. - One of Dartmouth's John Kemeny, once gave a lecture about how every computer program that humans build embeds human values into that program, whether intentionally or unintentionally. And what I'm wondering is are embedded in GPT products, or, put a different way, how like respect, equity, things like that into - That's a great question and something that we think about, we've been thinking about for years. So right now, if you a lot of the values are input, are basically put in in the data, and that's the data in the also data that comes that will label certain And each of these inputs So that's a collection of And then once you actually put these products into the world, I think you have an opportunity to get a much broader collection of values by putting it in the hands So right now, ChatGPT, we have a free offering of ChatGPT that has the most capable systems, and it's used by over 100 And each of these people can And if they allow us to use the data, we will use it to create this aggregate of values that makes the system better, more aligned with what But that's sort of the default system. What you kind of want on top of it is also a layer for customization where each community can sort let's say a school, a church, a country, even a state. They can provide their own and more precise on top that has basic human values. And so we're working on But it's actually, it's obviously because you have the human problem where we don't agree on things, and then you have the technology problem. And on the technology problem, I think we've made a lot of progress. We have methods like reinforcement learning with human feedback where you give people a chance to provide their values into the system. We have just developed that provides transparency into the values that are into the system. And we're building a sort where we collect input and data You can think of it as like a but it's a living one that. It evolves over time because our values also evolve over time, and it becomes more precise. It's something we're working on a lot. And I think right now we're thinking But as the systems become we're going to have to think about more granularity in the values that's... - [Jeff] Can you keep that - Getting angry? Is that one of the values? - Well, that should be... No. So that should actually be up to you. So if you as a user- - Oh, if you want an angry chatbot you can have it. an angry chatbot, you should Yeah. - Okay, right here, yeah. - Hello. Thank you. Dr. Joy here. And also, congratulations and all you've been doing with OpenAI. I'm really curious how both creative rights and biometric rights. And so earlier you were mentioning maybe some creative and you've had many creatives issues of consent, of compensation, of having whether it's proprietary models or even open source models, where the data is taken from the internet. So really curious about your thoughts on consent and compensation as And since we're in a university, do you know the multi-part question piece? So the other thing is thinking and so when it comes to the voice, when it comes to faces and so forth. So with the recent controversy and how you can also have people who look alike, and all of the disinformation threats coming up in such would be very curious on the biometric rights aspects as well. - Yeah, so... Okay, I'll start with the last part on... We've done a ton of research and we didn't release them until recently precisely because they pose But it's also important to give access in a way that and control the risks, and let other people in issues like, for with institutions to help us think about human AI interaction now that are very emotionally And we need to start understanding how these things are going to play out and what to prepare for. in that particular case, the voice of Sky was not and it was not meant to be, and it was a completely parallel process. I was running the selection of the voice, and our CEO was having conversations with Scarlett Johansson and... But out of respect for And some people see some similarities. These things are subjective, and I think you can sort of... Yeah, you can kind of come where if the voice, for super, super similar to a then maybe you don't In our red teaming, this didn't come up, but that's why it's important to also have more extended red teaming to catch these things early if needed. But more broadly, with I think our strategy here is to give access to a few people, initially experts or red teamers that help us understand the Then we build mitigations, and then we give access to more people as we feel more confident So we don't allow for people to make their own voices with this technology because we're still studying the risks and we don't feel confident that we can handle misuse in that area yet. But we feel good about handling misuse with the guardrails that we have on very specific voices in a small state right now, which is essentially extended red teaming. And then when we extend our Alpha release, we will be working very closely with its users, gathering feedback and so we can prepare for these edge cases as we expand use to say 100,000 people. And then it's going to be a million, and then 100 million, and so on. But it's done with a lot of control, and this is what we call And if we can all get comfortable then we just won't release To extended users or for we will probably try to lobotomize the because capability and But we're also working to help us deal with issues and content authenticity so people have tools to understand if something is a deep fake or spread misinformation and so on. Since the beginning of OpenAI, actually, we've been working on and we've built a lot of tools like watermarking, content policies that allow us to manage the sort of, yeah, the possibility of misinformation, especially this year given that We've been intensifying But this is extremely challenging area that we, as the makers of need to do a lot of work on, but also partner with civil society, and media, and content makers to figure out how to address these issues. When we make technologies the first people that we work with after the red teamers that study the risks are the content creators. to actually understand how and how do you build a product that is both safe, and and that actually advances society. And this is what we did with Dall-E, and this is what we're doing with SORA, our video generation model again. And the first part of your question. - [Dr. Joy] Creative rights. So for the- - [Dr. Joy] About compensation, consent, - Yes. - Yeah, that's also very right now we work, we do a lot of partnerships and we also give people a lot of control on how their data is used in the product. So if they don't want their data to be used to improve the model or for us to do any that is totally fine. We do not use the data. And then for just the we give access to these tools early. So we can hear from them first on how they would want to use it and build products that are most useful. And also these things so we don't have to build We'd only do it if we that's actually helpful in And we're also experimenting with methods to basically create our tools that allow people to be compensated This is quite tricky both and also just building a product like that because you have to sort of figure out how much a specific amount of data, how much value it creates in a model that has And maybe individual data would how much value that would provide. But if you can sort of create consortiums of an aggregate data and pools where people maybe that'd be better. So for the past I'd say two years, we've been experimenting with We haven't deployed anything, but we've been experimenting and trying to really understand And we're a bit further along, but it's a really difficult issue. I bet there'll be a lot of solutions for that. - It's just so hard. - It is. - How about right there? Yeah. - [Participant] Thank and taking off your time My question is pretty simple. If you had to come back to school today, you found yourself again at Thayer or at Dartmouth in general, what would you do again and What would you major in or would you get involved in more things? Something like that. - I think I would study the same things but maybe with less stress. (all laughs) Yeah, I think I'd still Yeah. Maybe I would take more computer but yeah, I would stress less because then you study with more and that's more productive. But yeah, I remember, as a student, I was always a bit stressed about what was going to come after. And if I knew what I knew I'd say, and actually &quot;don't be stressed,&quot; When I talk to older alums, &quot;Try to enjoy it and be fully immersed and be less stressed.&quot; I think, though, on specific courses, it's good to have, especially now, a very broad range of subjects and get a bit of I find that both at school and after, because even now I work in I'm constantly learning. You never stop. That is very helpful to kind of understand a little bit of everything. - [Jeff] Thank you so much, 'cause I'm sure your life- - Is stressful. (all laughing) (audience applauding) - Thank you so much. - Thank you for being here today and also thank you for the you're doing for society, quite honestly. It's really important and - Thank you for having me. - Thank you from all of us here at Thayer and Dartmouth as well. So I thought that would be some good advice for our students. What a fascinating conversation and just wanted to thank Enjoy the rest of Commencement Weekend. (no audio) (gentle music)