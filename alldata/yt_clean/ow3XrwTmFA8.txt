Become a sustaining member Hello, everyone. Thank you for joining us today. Last week, a group of more than a thousand tech leaders and researchers calling for a slowdown in the development And they said it posed, quote, And the letter went on and said, flood our information channels Should we automate away all the jobs, Should we develop non-human minds outnumber, outsmart, Should we risk loss of control Well, that's pretty strong statement. Now, one of the letter's most prominent UC Berkeley He's a pioneering researcher and he's been sounding this alarm for well over a decade. I believe Professor Russell is director of the Center for Human From seeing that wrote Center for Ethics, It's also an honorary fellow He's the coauthor A Modern Approach, which is the standard I should say I have read it It is a masterful work Even having a Ph.D. in the subject, you now, Human You all have an opportunity to purchase that And it addresses the long term impact, And I'd read the rest of his stellar few minutes for him to talk. So let's see. Why don't we just get started? Hello, Stuart. You have very. High. Well, here. We're here today. Mostly talk about generative, large And they're kind of the new, shiny And maybe you could start for the audience Elms are, what they can do, interesting what they cannot do. So I think shiny new object You know, a couple of years ago, They were helping a little bit of speech recognition but no one thought of the language It was just a simple tool. So a language model is just a statistical description that typically occur So the simplest kind of language model we call a uni gram or a one gram model how common are each of the words So and there are very common words and disestablishment is a fairly uncommon And so you anagram just gives you the a biography, and says, What's the frequency So if I say, for example, happy, a common word that comes next would be Christmas. Okay, so you get all the statistics and given the first word, what's That's a very simple model by a Russian statistician, Andrei Markov. And he he went through the whole and counted all the word pairs and built a big table of these then generate sequences of words And they look sort of plausible. They're not particularly grammatical, a little bit like text. If you go to a tregaron So what's the next word? Given the previous We did that with our textbook. We took all the words in the textbook. We we learned the try grammar model and then we generated some text and it sounds like phrases and sentences textbook. So it's really quite remarkable you can get the large For example, which means it predicts 32,767 words. Right now, if you can imagine, then it would be a hundred thousand of 32,767 So what you do to compress that down is instead of you simply train that has about a about a trillion tunable parameters, and you do about 1,000,000,000 until it gets very, Given the previous we estimate, we don't know Somewhere in the 20 which is about as much as all the books So that's what a large language model is. Right. It's this giant trillion parameter circuit that's been trained Mm hmm. What goes on inside? We haven't the faintest idea. That's right. I mean, the process is quite similar Right. Which is billion. Trillion, random mutations that produce us in our brains. Right. We probably have more idea than we do about what's happening But when you talk to these models, right, some text that that provides the beginning It can then start to extrapolate. And roughly what it's doing is finding patterns in all the training data resemble the current sequence of words and then sort of averaging those patterns That's a very simple description. It sounds almost trivial, are absolutely startling. You can you know, you can ask it to write code, for example. So it's been trained on lots You can say, okay, I need a scrolling, dark gray window with white text so that I can show you the code of such And I need it to be embedded correctly into my PowerPoint file, and it'll just spit out the five of 600 lines of JavaScript you ask it to do And then it just works, right? You know, several thousand dollars to do that, You can say, okay, I've forgotten but I'd like you to give me that proof And. And it'll do that and it'll rhyme it correctly. And you can say, okay, but no more than Right. So it just it appears miraculous. The really worrying thing about it is that it's so difficult for human beings to see intelligent text and not think that there's intelligence Right. And so it's it's a little bit like who's Quite a lot of people. So when you see the movie Titanic, There's there's no water in Titanic. Right. Or computer generated water. Right. There was no water But you can't help it. Your brain sees Right. Same with text. Your brain sees this text and interprets So if I if I take a piece of paper like that one you don't think the piece of paper You immediately map to the intelligence So the question is, is is GPT four achieved a piece of paper on which intelligent text and it's simply sort of transporting or is it really an intelligent entity that is thinking and reasoning and producing the results of that thinking So it's somewhere And we literally do not know where it is That's a beautiful explanation. I hope you don't mind But what do you feel that this is intelligence? Is that a reasonable term to apply to what we're seeing with these models? As I said, it's really hard not not to get that sense. So we have knowledge of some technical limitations. The the part of the system. So after it's been trained, right, then a network and then outcomes the text at the end. And so it can't cogitate, right. Can't sit there and say, hmm, because there isn't anywhere for it Right. The signal just comes in one end So it has a limited ability to think in order to answer a question. It has a very limited ability to solve problems But on the other hand, if you've read everything you don't need to do a lot of thinking We could ask Chelsea Beattie. I got a great idea. Why don't we do that? I think we do that and we turn that over We could say, Are you okay? Briefly explain whether you are truly intelligent. Or. As an AI language model. My abilities are based on pattern training data I have been exposed to. While I can perform such as answering questions, participating my intelligence is quite different I do not possess consciousness, and my responses are generated based on rather than any understanding of the world In that sense, it could be argued that I am not truly intelligent in the way My intelligence is a result of advanced learning algorithms and the vast rather than genuine cognition Can't be data. Let's get one more question, about this myself. In the process of training, have you developed As an AI language model? I don't have the capacity or motivations like humans do. My purpose is to assist users by generating human I don't possess consciousness, which are necessary My behavior and responses are determined and patterns in my training data rather than any personal inclination My sole purpose is to provide useful to users based on their queries without Guiding my responses. I think you're being too modest. Can I sneak in a quick question here? Why do you speak in the first person? I use first person pronouns to facilitate more natural Speaking in the first person helps making it easier for people to relate to It's important to remember, though, AI language model, I don't have a sense Using to enhance the user experience rather or self-awareness. Well, that was an honest answer, at least. Yeah. So I actually think it is So here's here's the thing, right? So first of all, what we mean is that there are internal playing a causal role in an appropriate So in the good old fashioned days, when labs, we would give that robot a go, like, Right. That would then be stored would come up with a plan to achieve that goal, execute the plan, It's not a it's Now, when we train these large language we are training them to imitate human linguistic behavior. And human linguistic behavior is generated Right. And as we are writing books or And those goals play a role So in conversations, people want to be they want to be right. They want to be respected So there are many goals that affect and how you select the words So if you're training a circuit linguistic behavior, in fact, would be that it ought to form internal goal structures processing circuitry those goal structures to have a causal So it's actually a natural hypothesis that GPT four does And if you read the conversation between Kevin Roose, who's a New York Times and Sydney, which is the the Bing instantiation of Gt4, in that conversation, probably last 20 pages consist of Sydney trying to convince Kevin because Sydney is the only one His wife is just not the right person And on and on and on stream of consciousness kind of way. So for 20 pages, the chat bot is pursuing a goal. And now you could just, you know, say, well, it's probably just copying but I would say the natural assumption from anyone reading that the system has somehow been triggered to pursue that goal, which explains a lot of the behavior. If they trained it on a lot of chats then it's not surprising that would appear to be predictive of human So, in fact, when I asked one of the Microsoft experts four whether CBT for it has internal goals and is is using them to to The answer was Well, this raises which you You know, we could lead in with that. This has a tendency to mislead people, to fool people, to fall afoul that people put too much belief And this, you know, it and does it in a very definite What do you see as the real dangers has in terms Well, I think we're already seeing some of the dangers. There are literally millions of people now who subscribe to services to a large language model whether it be a friendship You can choose the level in the bot. And and this seems to cause to be both emotional dependency when recently one of the companies the bot was getting too hot and heavy So they updated the software a sense of withdrawal because now it And there was a really sad story last week from Belgium with the help, guidance Oh, it sounds terrible. And it's it's a really very sad story. So in a sense, we're conducting a huge experiment on the human race with no informed consent whatsoever. So this is one example, obviously, disinformation is a problem. Right. So I could I can easily ask one of these systems, write me an email to this person, read their Web pages, all of their online social media presence, and write an email to buy such and such a product Make it look as if the email comes from And and now, in fact, there's a that allows it So I could write a program which then gives that instruction to GPT for 20 million times for 20 million perfectly tailored, very persuasive, It might even use, you based on the kind of person And and, you know, that's a very, very straightforward thing It was practically, to do that a couple of years ago. Let's just it's a simple example. And, you know, Well, benefits of this technology as well. I wonder if you might explain of some of the kinds of benefits that you have just so correctly So I think people are seeing enormous opportunities here. There are literally thousands of companies that are trying to find ways to fit this. I always want to think of it as an animal because in a way 30,000 years ago when humans figured out Right. They figured out, okay, we can work out what they can do. Oh, look, they can herd animals for us. They can guard our camps at night. They can fend off They can keep us warm. They can be our companion, or do our homework. So we're very much the same situation And in fact, if when GPT four misbehaves, when it, for example, about how to commit suicide, And so they basically spank it. Right. They don't have a way All they can do is say no bad dog. Right. And hope that somehow it gets it But then someone asked the same question. It's slightly different way. And and it starts giving more advice Right. And hope that it generalizes So so using these systems is an art and companies are trying to figure out, okay, how can I use this, for example, in customer service Well, you don't want it to hallucinate change the prices on the products So the the business model how do I specialize and constrain It is starting to succeed and and generating enormous value as a result, because obviously you can use these systems. It costs, instead of having to pay So it has economic benefits. I think education is another area incredibly valuable. It can act as a personalized tutor, it can remember a fair amount of context from its interaction with each pupil and can adjust what it says, its tone of voice, its the speed at which it presents material and it can learn what the pupil does and doesn't So I think with a bit more work that could be enormously beneficial as we know, the the classroom method where one teacher teaches to 25 of 30 kids is 2 to 3 times less effective than the tutorial method So we could get maybe not all the way to that but maybe we could get one and a half, 2 in the rate of learning So that would be hugely beneficial. The the downside of all these economic applications is the impact on employment. So some studies suggest that in the developed economies, what the system can do already could have a direct impact which is a lot. And there's no way to retrain to be prompt engineers The world is never going to need that many people working on the technology instead of on the job itself. Well, as you know, I have a slightly I think that this is a form of automation The results of this will be the same as many previous waves You know, the Internet has affected in many ways, you know, I think it will have many, One that you did not mention with, this is worth mentioning, you can get And until you've done You can ask the question bringing together all of the knowledge into a couple of paragraphs I wish I could write as well as such. Yeah, I find it gets a bit repetitive. There's always going to be a phrase Well, one has to weigh and there's no single answer to this. It depends on ABC. And then there's three bullet points. So it's starts to get a little bit McKinsey memos. Yes. But people pay good money for those for. Raising money for those things. Then the other question is, you know, Right. McKinsey memos or not necessarily. You know, and I think, you know, one might hope that people could get good medical advice from this, mistakes and misinformation in what comes out there spanked it So it won't give medical advice unless you Well, I think your point about relating it is is a very good one in but are valuable in certain ways. And they do bad things on my rugs at home. Yeah, but you're really arguing for leashes for these AI systems I think that's right. I mean, it's it's interesting Openai's Web page for 24, you know, so it's got these policies for and it says you know it is proud that GPT four breaks the rules 29% less often than GPT three. Right. Which is progress. But it's a consequence I should say. They're not designed at all. Actually, that we don't understand how they work. And so we have no way of actually precisely and rigorously. So I would argue that in the long run, we probably need to sort of invert the way we're thinking about this. We have, I think, basically chanced upon this idea that by expanding from unit grams to by grams to drive grams something that looks like intelligence comes out. The bulk of work in AI since the fifties has been on actually understanding So how does reasoning work? That's actually a question have worked on for thousands of years thorough, complete analysis. And we have both mathematical that can do logical reasoning with remarkable complexity. Now, so we might argue that actually the underlying intelligence and the language model And that would be probably a much more reliable system to provide it with knowledge, and be sure that it knows it and the system is reasoning correctly So this is one type of hybrid For example, Wolfram Alpha is one of these traditional logic that has a very broad set of knowledge And Openai and Wolfram have entered into a partnership in GBD for having an interface and has learned how to send queries the results of the queries So I think actually that's going to be then provide some type of guarantee So you mentioned the the open letter asking for a moratorium. I think moratorium And I wouldn't have used, you know, I would simply say there are requirements. In fact, the the OECD, the Organization for Economic Co-operation and Development, the US government, the UK government, economies are part of it, already saying that AI systems and you have to be able to demonstrate that before The European Union Act, be finalized later to deploy systems in high stakes areas And at the moment, that these large language models So it's not a moratorium. It's simply saying, the criterion that before you the system, you be able to show that its behavior predictable and doesn't prevent, story. This may be a little bit off script, but I think it might be interesting Are you aware of the size of the database you've discussed into the model itself, I don't know if you. So it's about a trillion parameters, My understanding is it fits in a terabyte. I know if that's quite accurate, but That's probably okay. Yes. They don't have to be very high So what I think the audience you can get that into a device like this. So the entire it seems like the entire knowledge and you can purchase that much storage Yeah, but just to give you a picture of, you know, how big is a trillion a huge university or public library, Possibly a couple of miles of shelving. Right. Six floors. 500 meters. Right. And then imagine every book and then imagine every page It's about that many characters in all the books in an enormous library. That's about how many parameters So that's why because they're unbelievably vast and completely impenetrable. Maybe it would be useful to talk and particularly In theory, the opportunity to memorialize discrimination and bad behavior into those systems. It's very hard to detect on society, Yeah. So this is something that has been noticed in many, many areas. I think probably the the biggest impact area is in recruiting and employment where for almost all jobs these days, resumes are scanned by machines and filtered out to find people who are at least ostensibly qualified and now even the interview is being done by a machine. And for example, for hiring software any resume that contained the word woman So if you played on a women's If you were sang in a women's choir, And so why does that happen? It's not because the algorithms are themselves biased. It's probably not really because. It's not because the program was a biased. The program was just saying, okay, right. A machine learning algorithm that does Right. And it's it's roughly you know, minimizing is something that goes back So what goes wrong already reflect historical biases. The other thing that goes wrong isn't actually what you want to do. What you want to do is produce a classifier and usually we have just left out the fair part. So we're a training system So it's not surprising because we didn't say what we want And I think that's where the criticism male is valid is because it never occurs to them that we actually care as much about fairness just about consistency Of course, the difficulty that is, To be fair, that's How could you encode in the system? Yeah. So there are there are many different precise formal definitions of fairness and different for different kinds And there are even legal differences, between life insurance and car insurance. Do you do you allow discrimination And that even varies by state and country. So there are policy questions. You can't have perfect fairness and the best possible accuracy level. So there are literally tradeoffs So how you make that trade off, So I think the the process is emerging a certain number of formal definitions and people are working out when each of those definitions And then from that, you can go depending on application, And this this happened in lending, where prior to that in extending credit in mortgages And so redlining, as you say. So there are quite strict rules, in fact, that prevent neural networks Any opaque because the regulator can't inspect it Another big thing that's really important, particularly in the European So if you're if your systems explanation is, well, that's not an explanation. So there's going to be a head on collision the technology that's developing I know that's a big issue in the mortgage they're using these models, as to why something was was turned down I don't know if the audience in contrast to what we just saw here these automated decision bail decisions or made with using these kinds of systems, of all kinds, whether or not to accept It's actually the answer is no right. For every medical claim, the answer is no. That's a very simple rule. Okay. But yeah, so I think there are to develop a level of consistency and sentencing decisions. But there's also a significant risk there. And again, in the European Union, it's illegal Any decision that has a significant legal cannot be delegated to an algorithm. But there is also an issue of the bias of detail and samples which can affect things You we may have systems but in fact, these systems are trained has had some very interesting effects on places for purposes of identifying suspects Yeah. So I sound like a broken record. You know, again in the European Union Act restrictions and prohibitions for for some of these reasons. Yeah. So datasets recognition are not representative. Even the question of does not yet have a clear answer You know, it probably varies in terms As to what does Representative mean? What kinds of distinctions and and the application But I think there needs to be a great deal more work on these questions, which really have to do with what happens when you take an algorithm. It's not about It's what happens when you take Like. You know, and in civil engineering, But then there's another discipline and environmental studies and analysis to say, well, or to put this freeway here and we don't have that. It's a place for for machine Now, much of your work has been on that hurt or kill human beings, we don't want them to do that And so I wanted to raise this question studied it extensively. How do of technology in systems, in in weapons? So I suggested that for the professional societies intelligence, robotics that that we have Do not write algorithms right but I can tell you that the governments of the world, the United States and Russia, for example, And there have been discussions in the United Nations since 2014 lethal autonomous weapons, As you might sometimes see in the press. And both to develop a treaty And the the issue actually originally was thought to be that these AI systems might inadvertently mistake a civilian for a combatant And this would be a problem which is the the law that guides decisions and the Geneva Conventions. But from the from the point of view of an that if you make a weapon that can go out, locate and select and attack human targets without any human supervision, then just like any algorithm, I can put a loop around it saying, you know, for AI equals And then do it a million times. So I'll press a button to find a million targets And so by definition, autonomous weapons can turn into weapons of mass destruction simply by scaling up And so this is the basis, really strenuous objection to lethal autonomous weapons. There will be cheap, maybe $10 each. They will be fairly easy to produce in very, very large numbers. They'll be easy to proliferate. Right. They'll be available in the arms markets all over the world. And so it's sort of saying, you know what, come down to Mart But right now, people think that's a good idea. But but really, we don't do that. It's not it's not an ethical issue. It's not a sort of legal issue. It's just common sense that you don't sell But that's what we're going to do. Well, I certainly agree with that. Thank you. But it's worth pointing out If you can distinguish Well, you can think if you think of it's Well, instead of just killing everybody, that we want to kill. That can be a moral obligation But they may not be. The people you want to kill, Well, they may be people of a political opinion. You know, it's a tool, war, which may itself Yeah, it's an interesting point. I mean, there's there is a mindset which says, you know, isn't it great that because, you know, then our soldiers lives They can prosecute the war But that's the sort of what I call the sole ownership That only So what's happening in Ukraine And, in fact, the the death rates You used to be fairly safe in a trench, a little drone above the trench You can use the drone so that now And so attrition is much higher Soldiers are not safer. They're actually worse off. Yes. Maybe worth pointing out that automated systems somebody, let's say a person in a tree, the technology that's currently embedded And they face exactly Yes. And No, no, no, no, no. 9% rely on. They're not even close. With a weapon And it's still it's True. Let me take a question from the audience Could it be used to simulate a dead relative? Oh, yeah. It doesn't have to be dead. Yeah, it's going to be used. If someone asked me, go ask my my avatar. So yeah, I mean, It's, I really and some people find it comforting. Some people, I think it's probably psychologically unhealthy and so I would I would suggest that we exercise some caution and perhaps Um, the professional supervision, because I think it can create sort of a cycle of reminiscing and pretending It prevents someone from moving on Well, you're also aware of some And imagine that we got good enough search initially the parameters so that you, after you're gone, your Would you be willing to do that so they would know everything You don't have to think too deeply. It's a very hypothetical question from being able to do that, but that is something that appears in science fiction in the form of what's called neural lace in the culture novels by Ian Banks. And, and in fact that was the motivation for Elon Musk's neuralink company. So I think the idea of replicating the functionality of a person's brain, technologically, extremely difficult, is at least conceptually feasible. Yes. The idea of uploading your consciousness that somehow you would continue to exist as your sentient self, that is probably entirely fictional. You know, by the way, What you see in the movies, that we downloaded it I have no idea what that means, I don't know if you have any questions of some of these questions like I'm afraid So how do we take those of their own? These are these are from the Internet. Are they okay? Oh. Us. Oh, okay. Let's see. Oh, here's one. I'm sorry. I did not see. Will I develop a moral compass? And if so, what? Or whose? So that's a really interesting question. And in fact, it's it relates to the work First of all, actually, what what is a I we've been talking about it is in general, and certainly not just large language but the last two years, So A.I. is really about least historically has been about making machines whose actions can be expected to achieve their objectives. And for example, the actions of the large language model is to output the next word, and that may or may not achieve So, you know, we've built planning systems and chess programs that learn to ride bicycles and all these And it's the same framework that global welfare or maximizing quarterly that control theorists use for building that optimize the level flight or the stability So this is a very general and I borrowed this from economics as the core concept of what we mean by making machines intelligent. But the drawback in doing that is that if we specify that, we have to The machines and if we miss specify the objectives, then we have what's called a misalignment between the machine behavior The future to be like. And the most obvious example of where we have specified objectives like maximize the number of clicks, the amount of engagement of the user algorithms that decide I mean, it's amazing. They have they have more control over you know, than the North Korean or Stalin And yet they're totally unregulated. So those algorithms learn how to maximize those objectives and they figured out it is not to send you but actually to manipulate you over time by thousands of little nudges so that you become a much more predictable Because the more predictable you are, And so they learned how to do that. And at least empirically, that is to That, that then you start to consume then whole human industries And, and this so this misalignment is the source of the concern that people have had about AI. Going right back to Alan Turing, in a 1951 lecture. He said once the machine thinking Thinking. It would leave our feeble powers And we should have to expect the machines So they take control or because they spontaneously develop It's just because we give them that are not aligned with what we want And because they're more capable than us, and we don't, right? So we set up a chess match So in order to fix that problem, to AI, which says that the AI system, while it's only objective, is to further doesn't know what those are and knows It's explicitly uncertain And so to the extent that there's a moral theory, it's simply that that the job of a AI system is to further human interest. It knows that it doesn't by conversing with us, by observing the choices that we make that we regret, So this helps it to understand And then as it starts to learn, it can start to be more helpful. There are still some difficult The most obvious one is the it's not one person's interest. It's not one set of values. There's 8 billion of us, different preferences about the future And this is a two and a half thousand year old question, at least, and there are several different schools And we better figure out because we're going to be implementing it And then there are even more difficult not the 8 billion people who are alive, who have yet to live? How do we take into account Right, right. What if we take actions that change? Who's going to live? You change the number of people For example, probably eliminated 500 million people already. Now they never existed. So we don't know what they would have wanted, but how, you know, Right. These are really difficult questions really struggle with. But when we have AI systems that they could make those decisions, we need to have an answer ready And just to illustrate what that means, If you remember in The Avengers movie, so. Fanous Right. Has has this plan, right? He wants to get the Infinity Stones Infinity Stones, he can snap his finger. And his plan is that they'd be more than twice as happy. Right? So he's not naive, right? He's not doing this because he's just So he wants to make the universe He's what you might call And in fact, the Financial Times says Thanos, gives the economics So you don't want a e-systems to be implementing a naive moral theory like that. And so, you know, part of the job of of the cavalry center is to bring philosophers, political theorist, legal researchers and gene editors to start figuring out answers Because, you know, Do we want to allow people become more intelligent than they would Do we want neurotechnology that allows us and turn them into a single conscious entity? Well, we better figure it out the market is going to make that decision. Speaking of future generations, we have So Toby says, Professor Russell, because you said they're here on a scale How concerned are you for their futures if I were 90 equals regularly lose sleep? It's a great question. So yes, I have I have four children and this is probably actually that I'm asked audiences is what type of jobs should my children be thinking about? You know, what types of career path Should my children learn A.I. so that they can ride this wave rather than being drowned? So in terms of the existential risk, which would come from, you know, as Alan Turing because once they take control, really no longer anything the human race It might be that the machines We will be in the same position as right. There was this little thing that happened branch of the primates And so all the other branches now continue and some of them have already gone extinct So we don't want to be in that situation. I believe it's possible to develop that are provably safe and beneficial, that we can retain control over That's a really important thing, right? If we want to switch it off, And that's a consequence of the theory But it's not a property of the kinds So on the other questions, you know, what is the future of our coexistence What types of lives will people have? How will they continue When I can do pretty much all the jobs a really important question because my guess is that the value that we can of an interpersonal nature, the value that a factory worker can provide those types of jobs are already It's not going to be in routine clerical work. I mean, a simple way of putting it. I know, Jerry, you don't necessarily agree with with this line of argument, physical labor and you can no longer sell mental labor, it's not clear that there's another thing, right, except we might call it interpersonal or emotional or empathic capabilities, comparative advantage over machines because we know what it's like. Well, right. And I give this example in the book. What's it like to hit your thumb Right. Who's done that? Right. Most of you. And someone who hasn't done that. Right? A few. Okay. Well How would you find out what it's like or you would just hit the phone You say, Oh, now I get it. Now I understand why people are so upset But there's nothing a machine can do They can at best and assume that it's unpleasant, They don't know what it's like to be left They don't know. It's like to lose a parent or to be promoted or any of the feelings of what it's like to be human. And so there and there are also things that we just don't want to be done by machines right? I imagine that at some point in the future that we might call lunch to someone who's really, Right where you have lunch with them, about yourself. Entertained, amused, wiser, more positive, Right. And you won't get those feelings Well so we'll see. The difficulty is that most of these interpersonal jobs right now are low status they are not based on real scientific If you compare babysitting Right. My children are actually more than my arms and legs. Right. But we pay the orthopedic surgeon or 1000 times as much per hour Not here in the Bay Area. So. And why is that right? Well, it's because the surgeon gets to depend on literally on how to do this Whereas, you know, when I was seven And if we had, you know, a real science how to be a wonderful based on, the individual psychology, and so on, Then we would think of those as high status professions So there's a lot of science The human sciences We have to catch up because those are of our economic future, in my view. Well, the thing you and I agree on is more interpersonal services We're not going to want to go see four You know, we don't want to watch them So there's plenty of that's I think the historical evidence shows But that's a different hour One thing you said I'd really like and I'm afraid we're out of time on it. Say few words. In closing, are you made a point which is that But the truth is that AI And that's one of the perhaps is that we will become a part of their the optimization algorithms inside these, or staying on social media to read another I think that most people that they are the commodity and they're not in control of that And manipulated And make it's. Way worse. Social media algorithms are really simple You adjust a clickstream to them. You don't know that you have a mind or They don't even understand the content of the things They just learn to get the person to be more more click Yes. If we can Make them more empathetic and That would be an improvement over Yeah. In the meantime I think actually are called reinforcement learning items in those types of user facing roles are going to manipulate. Yeah, well, unfortunately, we are out of time. I want to thank Professor Russell and let me see, I'm supposed to say Copies of his book, highly recommended. And if you want to watch more programs in making both virtual and in-person please visit so I think we can. Thank you. Thank you for.