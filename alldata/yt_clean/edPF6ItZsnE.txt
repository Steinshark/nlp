(upbeat music) - Hi, Kimberly, how are you? - Hi, Bratin, I'm doing - Good, thank you. Can you believe it's re:Invent - I cannot. Time really flies, doesn't it? - Really. So how are we doing on our slides? - Well, Bratin, if you remember, you made a really big bet last year. - I did? - And here's your money. - Really? I don't win bets. Are you sure I won the bet? (Kimberly laughs) - Wow. So this is about the one to make my slides like you and out will come the slides? - Yep, that's exactly right. I'd like to show you what in Amazon Bedrock to work on your deck. I think it's pretty cool. - So first, I go into Amazon Bedrock, and I just click Get started, and I'm gonna go into the Text playground to generate some talking points for you. In this case, I'm going to and Titan Express. And if you remember, on building some messaging, and I'm gonna use that as - From the marketing messages - Yeah, that's right. So put all this context in, which is the messaging for generative AI, and I'm gonna ask Titan to that you should highlight in your talk. (bright music) Let's see how it does. (bright music) - [Bratin] Wow. This is as good as what - [Kimberly] I agree, these I think we can use them for your talk. - [Bratin] Amazing. - Your slide deck is ready to go. I think we can take it to Vegas. (upbeat music) - Good afternoon, everyone. Welcome, and thank you for being here. A year back, using generative AI to create might have seemed fanciful, but here we are. Not only can it generate And in today's talk, I'll talk about what it takes to build and scale generative Because when you're it's important to pay attention This slide generated by Amazon Bedrock lists those key considerations. And in my talk, I'll discuss why these and how AWS helps you We'll also have customer speakers come and talk to these key considerations. So for example, we will have Ryanair, one of the largest airlines in the world, talk about choice and We'll have Fidelity, one of the largest financial talk about differentiating with your data. We'll have Glean, one of the most popular AI-driven enterprise search assistance, talk about responsible We'll also have TII talk about machine And finally, we'll have Netsmart, one of the largest healthcare talk about using But first, let me take a step back and discuss why generative Over the last five years, the pace of innovation and in machine learning has accelerated, and this acceleration The cloud made massive amounts of compute and massive amounts of and as a result, practitioners in industry and academia were able to innovate In fact, almost every frontier was born in the cloud. Now, let me give you a on the space of innovation that has been driven by the cloud. Over the last six years, the amount of compute that has grown by more than 100,000 times, the amount of data that we use for training has grown by more than 100 times, and the size of models has This is a pace of innovation in the history of information technology, and this space of innovation that are trained on internet scale data, the so-called foundation models. Let me give you a little bit to build one of these foundation models. A human being, you and me, in the course of a lifetime, in the course of our entire lifetime, a human being listens to about Now, when we train we are training these models In other words, we train these models with thousands of times more information than a human being will listen There's another way to look at this. When we train these foundation models, we train them with terabytes of data that is thousands of times more than the information And so when you pack so much they start having very But the question that most How do I build applications How do we put these models to work? And so I'm going to talk have been building because I think the and the considerations will also apply when you want to build and scale your own Earlier this week, we launched Amazon Q. Amazon Q is a generative AI application that uses generative AI to transform how employees So you can use Q to ask You can use Q to create You can also use Q to act on your behalf And so I'm going to use Q to illustrate the considerations that and how we went about building an enterprise-scale So the first question that and I suspect you'll have Where do I get started? How do I choose a foundation model to build an application with? And this is not an easy question to answer because every model has its and so it's important to to figure out which model to use. Let me illustrate with some examples. Suppose I asked this question to two different foundation models, and these questions and answers are actually real that we asked many So I ask: What is your shoe return policy? Model 1 gives me a quick concise answer, free returns within 30 days. Model 2 gives me a longer, Both of them are accurate answers. Which model do you want to start with? That actually depends on the For example, if you want to generate ad copies, you want Model 1 because you On the other hand, if you're looking to build where you actually want to with the customer, then you want Model 2. Let me give you another example. Suppose I asked this question: What is your checked bag policy? Model 1 gives me a quick accurate answer. Model 2 also gives me a correct answer. It's more complete, but it and it takes longer to generate because it has to do a lot more compute. And because Model 2 has it's an order of magnitude more expensive. And so now, you have to ask the question: Do I really want to pay for Model 2, or am I better off paying and using Model 1 because it gives the answers And so it's very important as you think about building that you run a whole set of tests and you work back from your use case. And so let me show you the results of running a whole set on many different models. But first, let me talk that we use for evaluating these models. So there's cost effectiveness. How expensive is it to use this model? There's completeness that There's low hallucination. So when a model has low hallucinations, it's a lot more accurate. Then, this conciseness And finally, there's latency. How quickly do I get an answer back? Now, when we did our actual evaluations, we used a lot more parameters, but I put up five-year because And so now, let's look at the results from the first two models. And what we'll notice here is that Model 1 is not as cost-effective, but Model 1 is a lot more complete and now it's not clear which model to use. And so we said, &quot;You know what? Let's go and try a few other models.&quot; So we took another model and and the results were, again, the same. The models are good in some dimensions, they're not so good in other dimensions. And we ran our tests against and the results were always the same. Models have strengths and And I can bet you that as you build for the enterprise, you too will likely have to where it'll end up with some models that and others that are good for other things. So where did we end up? Here is where we ended up. We picked a model that's good &quot;Let's go and optimize it And it's very likely that you too will probably have where you pick something and then go in and optimize So what were the optimizations When we started building Q, we thought we would use we thought we would take the Turns out, that's not where we ended up. We actually ended up using Each one of them somewhat Let me explain why this was the case. So when a user sends a query to Q, Q has to do a bunch of things, it has to first understand What is the user trying to get done? It then needs to retrieve It then needs to formulate It then needs to do a And so it turns out that using a single model Using multiple different ended up giving a better experience. Now, we thought this was counterintuitive, and you may also think until we realized there's really to how the human brain works. It turns out that the human brain is It actually has multiple that are each specialized So for example, the frontal cortex that deals with reasoning is constructed differently that deals with fast, Even the neural structures are different. And so it's probably not surprising that when we considered all of the tasks that Amazon Q has to do, we ended up with the What were some of the other optimizations that we had to do for Q? Once we took care of the models, we actually had to spend a lot of time on the data engineering. Let me explain why. Suppose I asked this question to Q. Tell me about my customer Notice that Q now has to It needs to first go to figure out what meeting I have. It then needs to look at my CRM system, my customer relationship to figure out details about the customer. It then needs to look at to understand how we are And so what Q has to do is it has to aggregate data to be able to give me a helpful answer. And so we spent a lot of time on building enterprise data connectors on data processing, data pre-processing, data post-processing, data quality checks, to ensure that Q had the right Now, once we got done with and once we got done with we thought we were done. Turns out, that was not the case. Let me explain why. Suppose I asked this question to Q. What is the expected revenue This is company confidential information. What this means is that some people should but not everyone. And so in this case, if the software engineer Q should say, &quot;Sorry, I But the CEO is asking this question, Q should be able to give some answer. In other words, Q or any needs to respect the access It should only give answers that a user is entitled to have. And so we have to spend a lot of time on building access management, block topics, sensitive topics in general, on building responsible AI capabilities. Now, to build all of these, we also needed a performant and low-cost machine And this leads me to for accelerating your First, you want to have choice Second, you want to be able to use and differentiate with your data. Third, you want to into your applications. Next, you want to have and performant machine And finally, in many cases, you want to get started with Let me now dig deeper starting with choice and In fact, this is why we Amazon Bedrock. Amazon Bedrock is the easiest way to build scalable applications It gives you a range of state that you can use as is or you can customize them with your data. You can also use Bedrock agents And so to talk more about how customers are innovating with Amazon Bedrock, please welcome John Hurley, the Chief Technology Officer at Ryanair. (audience applauding) (upbeat music) - Thank you, Bratin. - Hello, everybody. My name is John Hurley, Who is Ryanair? We're Europe's We will fly 185 million and that will grow to 300 million as the new aircraft orders come in. Two key stats I love about Ryanair is we fly 3,300 flights per day and carry 600,000 passengers A very efficient operation, And the IT department which I work in has to go at the same COVID came. We actually had a chance to breathe. Unlike other people, we We took the bucket list and tackled projects we've been For example, we use the We've now dynamically and every single ancillary That's over a million getting calculated continuously 24/7. We use SageMaker for It's been interesting, some early positive with a lot to in that space. It's very interesting. It'll help us in our and we look forward to that We use SageMaker for It wasn't all about SageMaker but we did other projects there as well. For example, we got rid of the And during COVID, we had 30 odd different who had different regulations all about safety, and procedures, and information being shared, and we're constantly doing that. And if we didn't have the being fully in the cloud, we'd have been snookered in that world. For example, I think one government was Italians gave us three And we only did it because of Lambdas that can do it in that space. And while we were in that we're also refunding... Refunding, refunding to over 20 million passengers. So it was a very busy time, we did a lot. We circle back and that we call it the panini predictor, The idea was, was how to give so it could actually in every single flight. We did it, and it's a very of where the theory it was brilliant with data scientists, they're over the moon. We handled over these packing It was absolutely impossible to have 550 different packing plans across 93 bases at 4:00 in the morning. So we were stoked, we spoke to Amazon, sorry our AWS partners, they who actually gave us a tour of their fab to show us what good looks like. It was brilliant, I loved it. Their robots were absolutely Didn't know, which is my favorite part. On the way back, I was Head of Inflight, and I asked her what And she goes, &quot;Robots? Did you not see the Amazon A to Z app? I want an A to Z app.&quot; And I was like, &quot;What? Did But got back to Dublin, I ran our contacts, and they put us in contact to go through the their And we did a working backwards and six months later we I'm sorry, with a very catchy title, the Ryanair's employee app. This is for our cabin crew gives you your roster, it'll give you ability to book time off based on transfer systems. Every need one location it's been very positive, but We had cabin crew, we had how to upsell products, where are all these documentation, and it was spread right across our network and in different places. It was in YouTube, it was in We worked at AWS, we used Bedrock, and we actually built an employee bot. So suddenly you could ask questions like, from selling a coffee, how way you upsell a bar of or can I have a tattoo on my forehead? You can't, by the way, in But it allowed people We'll have to search it was on your phone, in your pocket while you were traveling. See if the information It has gone very well. We hope to actually roll to the business early next Internal testing with our Other areas of... We're using Bedrock. Well, we have a great plan but after announcement, tested with Q, we might actually have a more of a refresh and see for the right tech stack in place. We're using for CodeWhisperer. It's been interesting as a way to go, we're excited about that. Projects, the one excites me the most is definitely gonna be We get about 10, 15% for our But random questions that aren't actually related They're like, &quot;Can I bring Unusual questions like that. We have agents answering All these things can and that's where we and a huge area of improvement. Also, I thank Bratin and the very start of the So I'll be back to him with five and a half and model recommendations to make that go forward And with that, I'll hand you back to Bratin. Thank you. (audience applauding) (upbeat music) - Thank you, John. We are so glad to be partnering with you on your generative AI journey. Let me now get to the and that is using and In fact, every machine and this predates generative AI. Every machine learning uses data as a critical ingredient. And so it's really important for customers to be able to to drive their machine learning. To that end, AWS provides you with the to store, query, and analyze your data, and then to act on it with machine learning, and generative AI. We also provide you services and to do data cataloging. And best of all, you can use so you can use the services that you need. And I'm happy to announce the Amazon S3 Connectors for PyTorch. These connectors make a lot more efficient, and they do this by accelerating that are used in like saving and restoring checkpoints. Now, many customers use AWS to drive their machine learning. And so I'm pleased to welcome Vipin Mayar, the Head of AI Innovation at Fidelity, to talk more about how to drive the machine learning. (upbeat music) (audience applauding) (upbeat music) - All right, good afternoon. I am Vipin Mayar from We are a large financial services company. Data, AI is really important to us, and I believe you can only be good in AI if you have a very good data and data quality. Now, you're hearing and I thought we should unpack it a bit, and I'll tell you a little and what's really important to us. Okay, we started seven years We've done a lot. A lot still remains to be done, and I could talk about many things, but I'll talk about three things that I feel are really important now. The first one is unstructured data. How well do you have it collected? How well do you have it organized? We started collecting We started digitizing calls. We started streaming built features around them, gave access to end users so that over the years, they which now, with LLMs, is The second thing that I especially with large companies, is to have an enterprise taxonomy. Very easy to say, very hard to do 'cause it requires getting and a semantic layer to instrument it. We have been working at it, we've got a lot of KPIs in one place. That enables dashboards to The third piece, which is an investment in we've enabled Query so people on the business and even have a social regarding the data elements. Okay, those three things I We've worked with AWS. The backend works pretty So now that you have sound data, let's quickly fast There are four things we Conversational Q&amp;A pairs, On the coding technical side, developer assist plus translation of code, things The third piece, perhaps the one that in these conferences, is RAGs. Search, semantic search rendered through a A lot of work in that and all the announcements Really, all that work we And lastly, content generation Okay, easy to say. But the challenges we face, let's talk about them for a minute or so. LLM's pace of innovation, incredible. If you go to Hugging Face, they add 1,000 new models every day. Claw 2.1, excellent. Big models, great. But we've gotta balance the large models with smaller, fit-for-purpose, Doing that rapid experimentation As you do this, getting capacity and managing Guarding against hallucinations, another challenge for us, okay. So with that, let me go to my last slide, which is: What is our approach? With classic machine learning, we don't talk much about Your factory, we use SageMaker. We are now excited with Bedrock, but also SageMaker and being able to test and RAG tuning, prompting, being able to look at evaluation metrics. And really critical for us But let me end with where I began, which is all this can take a lot of time and can distract you from where I began, which is data. At the end of the day, there's a greater premium and that's where we are still focused and a lot more to be done in that space. (audience applauding) - Thank you, Vipin. Incredibly important insights into how you build a robust data platform because without that, it's very hard to innovate Let me now get to the and that is integrating responsible AI. Any powerful capability needs to have the appropriate guardrail so it can be used in the right way. And if machine learning and it's incredibly important that we integrate responsible And to that end, I'm pleased to announce that you can now use SageMaker Clarify to evaluate foundation models, and you can also get on Amazon Bedrock. So here is how it works. As a user, you come in You choose the responsible that you want to evaluate them on. If you want human evaluation, you can also choose a and then Clarify goes off, and and then it generates the report. So all of that work that all of those evaluations and criteria, that was months of hard work. All of that gets a lot easier now. To talk more about responsible please welcome Arvind (audience applauding) (upbeat music) - Thanks, Bratin. It's great to be here. Glean is a modern work assistant that combines the power and generative AI and helps employees in your company find answers to their questions using your company knowledge. It's like having an expert who's been at a company since day one, who has read every single who's been part of every conversation that has happened in the company, who knows about every and then they're ready to assist you 24/7 with all of that That's what Glean does, and we're so excited and to announce our partnership with AWS. Today, I'm going to walk you through how we address the with our customers. Customers are really but they want to know if they can trust the answers Here are the three main First, how do I know the Everybody knows LLMs can hallucinate, and actually even more importantly, you have to provide The input that you give to the LLM is going to decide how accurate And oftentimes in an enterprise, information can be out of date, and that can make the job of an LLM hard. The second challenge is: How do I know that I'm The market is evolving quickly. Each customer has different needs, priorities, and constraints. Glean needs to guide them and make it easy for them to get the LLMs that works best for them. And third, how do I make sure Glean indexes all of your company's data, so we take this problem very seriously. We need to make it easy for our customers to keep their information safe and not have them worry about data leaks. Let's go dig a little bit So first, let's talk about around accuracy. The output of an LLM is as input you're going to provide to it. To make the LLMs provide good answers, you need to use retrieval to provide it both the as well as to constrain its A really good search engine and that's at the core of Glean. Our search uses to train our semantic language models and LLM models from Bedrock to provide accurate answers to our users. After the LLM generates an answer, we apply post-processing to provide in-line citations If a piece of information we exclude it from the response. All of this put together, a RAG system, backed by a powerful and post-processing LLM responses are how we address customer Let's talk about model selection. Each customer has their own that may require using different LLMs. So long as a model is able to pass our internal tests for accuracy, we want to enable customers to use it to power Glean Assistant Bedrock is awesome for this because it's easy to select from its large repository of models and pick the one that works And finally, on the topic of: How do you make sure as an enterprise that your data is safe and secure? Bedrock is great because of and support for end-to-end encryption. It makes it easy for our that their data is secure and not being used for other purposes. Each Glean customer, in addition, gets their own proprietary AWS project running within their own And no, none of your company including the customized models that we've trained using So as our customer, you get to use the latest search technologies while making sure that all of your data resides within your own And finally, the way Glean works is we connect with hundreds and make sure that as the answers that they get back are limited to the knowledge This is what we are The user came and asked a question: How do I set up Glean on AWS? And the system actually does a search using our core search engine, assembles the right pieces and then uses the RAG technique to take all of that knowledge and give it to an LLM powered by Bedrock and synthesize answer and When the answer comes back, we show the citations to the users on where the information come from. So this is how it works, and we are really excited These are our first steps and we're so excited to be here and bring the power of Glean Our entire team is excited to explore more services in future, like SageMaker Clarify, And if you wanna learn more about Glean, you can visit our booth or see our website at glean.com. Thank you so much. (audience applauding) (upbeat music) - Thank you, Arvind. We are really looking forward to take Glean to a lot more AWS customers. Let me talk next about for building and scaling your and that is having access to a low cost and highly-performant machine Our hardware infrastructure where we have the G5 instances that provide you the fastest inference and the P5 instances that provide you the fastest training. In addition, we also for generative AI, AWS Inferentia for doing inference and AWS Trainium for doing And in fact, these customer accelerators provide you up to a 50% Now, at AWS, hardware infrastructure We complement our hardware instances with a software infrastructure, and that is where SageMaker end-to-end machine learning service that you can use to build, train, tune, and deploy all kinds of models: generative models, classical models, and deep learning models. And now, SageMaker has a number of purpose-built capabilities to help with generative AI. So earlier today, we Now, SageMaker HyperPod accelerates your generative due to its optimized It also provides you automatic Now, it's obvious why Customers get to train But why do we need to provide Let me illustrate with an example. Before generative AI, customers would use small-scale cluster. So you would use maybe eight or 16 nodes and you would train your At that small scale, the Now when you get to generative AI, customers use tens of thousands of nodes, and they're training for months on end. At that scale, fall tolerance is critical because the probability And in fact, if your is not resilient, it's going to be very because it'll become a And therefore, we are now Let me illustrate how they work. So as a user, when you the first thing that happens is that your model and to all the instances in the cluster. And this makes sure that the so that the training can get done quickly. Once that happens, SageMaker then also your applications. It's saving the state of your training job at regular intervals. At the same time, SageMaker also monitors all of and if it finds an unhealthy instance, it removes it from the cluster, it replaces it with a healthy instance, it then goes and resumes from So it resumes the training job from the last saved checkpoint and then runs it to completion. All of this without the user having or fault tolerance. I'm also pleased to announce that SageMaker is now launching to make inference more efficient. So it's reducing the cost of by almost 50% and reducing the latency by almost 20%. Here is how it works. So today, when customers for inference, they deploy models on a single instance. And what happens is that that instance and that increases the So what SageMaker allows now is that you can allocate multiple different foundation and you can control the resources that you're allocating Like, you can auto scale Not just that, it also so it looks at the load of and then it directs incoming requests to the instance that is And as a result, it can reduce It's optimizations like this that make SageMaker the best and deploy foundation models. And to talk more about this, please welcome... Dr. Ebtesam Almazrouei the Chief AI Researcher and (audience applauding) (upbeat music) - Good afternoon, everyone. Thank you for joining us today. One of the most important it is when you are you have to think about the And advanced technology, it has improved the and communication, facilitated sustainable energy solutions. Not only this, but also transformed and promoted innovation and advanced technology infrastructure. You can see here, however all it's very important to ethical considerations, privacy concerns. It's crucial to ensure of technology's benefit for all of us all. We believe that openness is the key to harness while safeguarding human rights and achieving sustainable Open large language model is a step forward to achieve this goal. LLMs, or large language models, are forging a golden era of possibilities, from personalizing learning experiences to summarizing massive amount of docks. Not only these, but these that they can crack the code of NLP. By harnessing language, LLMs are helping us not only but also to contribute to of our time. That's why in Technology we invested in building our Falcon LLMs. We started in 2022 by building NOOR, one of the largest Arabic Leveraging the power of cloud AWS-accelerated compute infrastructure allowed us to proceed and train models with billion of parameters and trillion number of tokens. Not only that, but significantly reduced To take you through our journey, we leveraged SageMaker to pre-process petabyte scale with data to generate approximately representing about 5 trillion tokens. To put it in context, 5 trillion tokens, it's each book with an average of 400 pages. Can you imagine the amount of the data? Then, what we did is all of this data set, we used them to train all our Falcon LLMs: 7B, 40B, 180 Billion parameters. On a large-scale high we managed to achieve up to 166 teraFLOPS, thanks to the optimized And again, to give you and to give you a sense of that scale, if a single person is solving to reach to 166 teraFLOPS, he needs 22,000 years to solve what that cluster Then going from Falcon-7B to 40B, all the way to we needed also to scale So SageMaker was able to After that, we, of course, using SageMaker real-time endpoint. Not only this, but we did This rigorous evaluation is not just a technological advancement, but also particularly effective So what we did, as a team, we built assembled serverless architecture and leveraged a Slack channel to evaluate all the model's answers. Finally, I am glad to let you know that all our Falcon LLMs today, they are now available as and you can start deploying them, fine-tune them with only single click. Starting with the adoption is now the largest and top-performing in the Hugging Face. It has been downloaded And what that can tell you, it can showcase the strong desire and interest for open-source LLMs. Now, I want to share some of and it enable us through First, you wanted to at all levels. So we encourage all our researchers to continuously explore new ideas and challenge also all the assumptions. Second, we also wanted to for our experimentation. So it is very crucial to provide access to large-scale compute not only to do the necessary step, but also to empower and and experimentation. Third, you have to have an institute rigorous evaluation of protocols. We thoroughly benchmark all new methods, testing and also validating them. This prevents overoptimistic results and also ensure real-world viability. In summary, embracing scaled experimentation, and collaboration with vendors like AWS, we are committed to continue from a seed of an idea to to deliver groundbreaking innovation. Let's all shape the future of AI. Thank you. (audience applauding) (upbeat music) - Thank you, Dr. Almazrouei. It's really amazing work going on in TII on foundation models. Now, SageMaker is also focused on making machine learning accessible to people who may not be or who may not be experts at coding. And that is why two years back, we launched SageMaker Canvas, a no-code interface for building and deploying your And now, with generative AI, I'm pleased to announce that SageMaker Canvas's no-code interface is also being extended so you can build, and customize, all with the no-code interface. And so data analysts, business citizen data scientists who may who may not be proficient can still build generative AI. Let me now get to the for accelerating your and that is using generative Many customers tell us to provide generative AI applications for important enterprise workflows, like in the contact center, like document processing, Earlier this week, we launched a general that uses generative AI to accelerate clinical productivity. Today, when a patient that patient-physician interaction has to be scribed manually, and doctors can spend 40% of the time on this manual work. That is time that's not And so AWS HealthScribe uses AI to automatically analyze that and then uses generative AI that can be uploaded to your And so software vendors, can now use generative AI to To talk more about this, please welcome Tom Herzog, the Chief Operating Officer at Netsmart. (audience applauding) (upbeat music) - Thank you. (upbeat music) Tom Herzog. Grateful for the opportunity and communities that we serve because at the end of the day, Healthcare is about people helping people. We've been digitizing It's been about more and more data. And the questions we're all asking now: What are we gonna do with that data? Whether we're a provider, and healthcare is absolutely I want to introduce this notion that these tools that we're talking about that we've all now arrived at, it truly is about addition See, I believe that less is more. And as we talk about HealthScribe and we talk about Bedrock, what we're really talking about is: How can we be more efficient, so that caregivers can see more people at the right time when they need it most? The challenge, we all know the demand That when we schedule we're limited with the number of options because of the need that's out there. I'm gonna talk about Let me get to a very Providers spend over 40% of those in telehealth sessions, just doing documentation. That's two days that they're And if you ask them, 15 to 45% of the information they have while what they're using is really good, they need more, more contextual awareness, not just for when they're talking to you right then and there, but for things that may months ago, years ago, that contextual awareness, if you will. Let's frame the challenge as a society in our communities. We know that over 50 million with a mental healthcare illness We know that over 60% of our youth do not receive treatment for things that they may be suffering with like depression or anxiety. And we know that nearly 25% of adults, their needs go unmet for the or that they're not even This creates an opportunity for us to do something different. This is the team, this is the cause and communities This is also the team of innovators and designers who are working together to change the healthcare We serve over 754,000 providers who are touching over 133 million lives and beyond what we know of acute or primary care. We're talking about community intellectual development those who have foster long-term care, hospice care. This is a real opportunity for all of us. Simply, as we look at the here's what we need to Not usability, not less clicks. We need extreme usability to so that they can accelerate, and prove, and optimize the outcomes for the people that they are seeing. We have a unique opportunity using tools, solutions like HealthScribe to do something simple. Let's give those two so that they can see more people. Let's streamline discharge so that as you need to that information is relevant to you right then and right there. And let's transform and take manual processes away to introduce how this system can cohesively follow Why did we choose these tools? Quite simple. HealthScribe and Bedrock produce ready-built, that we can plug into it's able to scale with us and it has the ability to integrate across the ecosystem very uniquely. And lastly, give you back to the solution, the notion that we started with. Imagine a telehealth session, if you will, where you're not only just you're doing it systematically, you're doing it with a But using tools within Bedrock to pull forward that information so that as I am interacting with you, I can look back a week, six months, a year to have relevant information to suggest the right And while we often talk about and I love it and I'm a geek at heart, what this really takes is for Our relationship with AWS just isn't about how we can Beyond partnership, it's 'Cause the things that we're talking about in healthcare today isn't about tomorrow. It's happening right here, right now. And we're deeply grateful and appreciative for that partnership. Dr. Saha, appreciate the time and the opportunity to share our story. Thank you. (audience applauding) (upbeat music) - Thank you, Tom. It's amazing how Netsmart is into the healthcare space. Let me now summarize the At AWS, we are focused build and scale generative And when you're building it's important to pay attention This is what we learned from and I believe this will be applicable when you build your own applications. First, you want choice Second, you want to use and Next, you want to integrate responsible AI into your applications. You also need to have access to a low cost and highly-performant machine And finally, in many cases, you want to get started with that we provide for contact document processing, Thank you for coming and please Please don't forget to fill Thank you. (audience applauding)