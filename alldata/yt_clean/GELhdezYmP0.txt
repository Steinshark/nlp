Hi. Hello, everybody. Thanks, And, yeah. Glad to see so many of you are I know it's, it's a lot to ask. Yeah. My name is Hannes, and I'm gonna talk about data DuckDB. Very briefly about myself, if As Hadley has said, I am the cofounder and the Labs, but I'm also a as a sort of side gig at which in case you have never It's very small. But I wanted to start with me, and I don't know how many Anybody has seen Nobody. Mark has seen it. Okay. This is one of the first It is a protocuniform tablet I think, in today's dated to 3200 BC. It's in the Metropolitan And this is interesting because little bit, you can Right? And it, in fact, It has, it has two columns, its item and amount one is hop and one is barley. One can only guess And the cool thing about this thing, record keeping, predates literature by So we've been making tables the first text. Okay? And I really liked it. When I when I found this, I was really happy what is it about my brain Something must be wrong with me. But now I'm I'm content to version of, you know, a human, which I'm very happy with Anyways, that's just the intro, and this is really But let's let's get, I wanna motivate my, talk with This is on the dbplyr vignette, and I will repeat it If your data fits in memory, there is no advantage to It will only be slower I can I can I can hear that, but then I really like and I kind of let's I wanna explore Let's explore this. But I wanna say that this by people like Thomas Lundy inspired, the work on DuckDB, this kind of notion that frustrating and slow, and we But before we get there, let This is really not right. So I went on the Internet This was the first It's the New Zealand and they have a census data set, multiple CSV files for Grabbed really yeah. Just really I promise you this was Good on them that had have And then I just went through biggest CSV file I could find. Okay? It's six files in a hundred megabyte The biggest one is eight hundred It has thirty four million You would agree with me that Let's start with the, more Sorry. I have to switch to this. Let's unpack the the, you know, the statement that Say I wanna look at this let's say Postgres, like the world's most commonly Right? Sorry. It's not that. I'm going to ignore that is required to set Okay? No, I'm not going to talk about It's a pain in itself. But, you know, in principle what we're going to say we have to create a table command to get the CSV file can finally do the analysis. Okay? So we look at the and you're just on the command very good name, and we It says, year, age, And if I squinting and I'm after all, I'm professor I can see that, okay, this probably is a table this looks like although the leading zeros they seem a bit worrying. Right? Like leading zeros. These are these really So then I after I type the copy command into and I'm like, okay, invalid input syntax &quot;..C&quot;, line eighteen. Column count is which is weird because this is sure it was a count. But it isn't. Okay. So then we drop the We create the table again. We make this count a VARCHAR clearly we have to And then we call the copy invalid input syntax CMB07601, line four thousand seven And from your laughter, I know exactly what So that's not great. Okay. Go for broke. Go for broke, and then just copy the data. And lo and behold, we've copied the thirty in sixteen seconds. This is all on this I'm running out of Now I have done I have five Right? I'm going to spare And then after I've I finally get to At this point, I've kind and indeed this is more frustrating And it's also an interesting seconds that the CSV video irrelevant in compared spent, like, you But okay. Let's Let's just look at sort of the speed And, again, it's not and this is very weird coming I know. But if we use readr, one of the many CSV you point it at the data, and it says, hey, I've And you think, hang on, aren't you gonna tell me that No, no, no. It says, no. I figured it out. There's three character columns double numeric, okay, but numeric columns here, how this thing looks It took ten seconds. Again, the execution performance for an interactive use case twenty seconds overall. I had to type the command, But, so this is really interesting tools like R and Python on this frustrating pretty good way, and comparing that to Postgres, the shining light on a is really a huge, And it's why is that? I think the data management really just never cared but also about people, really. And, this is not I mean, if you if how do you sell Well, it's very easy. You meet the other Right? Done. You don't have to talk to the use Oracle because they Postgres or anything. They they really they really but since this is not relevant in In data science, you know, which is why this Having said all that, we can, of course, do a Let's take a file, write, and and we kind of see that there like twenty two seconds data.table traditionally Polars doesn't really manage too eager in their but basically, again, performance of the reader and it's interesting how that together because frustrating and slower Right? Because slower actually doesn't It refers to, you know, So because we realized that management systems, but we really liked data how can we maybe build a slower and more frustrating? So, Mark, who is here, and we started thinking about system that's not slow or And we did this research which is the Dutch National Science and Mathematics, which is famous because it but I'm at the wrong The result of this I want to do a quick poll. Oh, that's excellent. But there's a lot of people I will you will forgive me maybe a good idea to use DuckDB. And I think that user experience is really core to People have focused too long the burger there, the DBMS the, I don't know, you all these things. And nobody had the end to end user experience And we really set out to change system that, is And sort of by accident, we also invented a new category which is an in-process I will say in a second but basically it means that server or files or but it just runs And it's not a transactional or actually most other but it's analytical, meaning that it's really not on sort of tracking And it didn't exist yet. It's always good to Right? Let's go a little Like the in process I think this is the biggest traditional data management did in DuckDB, is that it It's a bit hard to imagine what if you're in R or in Python if you're pulling up it means that the entire management system actually And this sounds very weird, but we're actually quite used widely deployed data management actually does the same thing on usability, on, you know, setup simplicity It also makes it really So, you can type You can also do We support a bunch We have zero dependencies, install, I don't know, OpenMP It's just all wrapped There's no apt get, there's no funky drivers, nothing like that. It runs really anywhere, you Intel, doesn't really matter. As I said, for users, it's So that's nice. And it's also I know this is a term but it's a fully featured We have transactions, we we can write tables to the transactional save changes to are quite massive. We have crazy high performance implementation aggregates, window functions. DuckDB can out of the Parquet files, JSON files, It's all built in. Right? We can connect to We can talk to Postgres. We can do all these things, as I mentioned, into and a zoo of other languages. So it's really we try to And it's fast in the And I realized that if that the, I just said that but that is for the Speed does actually First, if you're running probably the speed does matter. And for the other the and I will come is that by making you actually expand of what you can do So this is great. So What we will how to If you run DuckDB, your laptop So a word of warning here. So, we automatically the analysis tasks that available CPU cores, but line parallelization is without having a good and we achieve that by having for all the various operators. We can also use the hard not fit in memory. So, if we have this case where terabyte file and we I have to sort the whole thing, you can imagine that you might sorting everything is a quite so DuckDB can transparently available disk space. It's really cool. But all this happens which is why it's people, that, that it just does things understand and then This was from earlier today, so I thought it was funny And, of course, we know that any technology is And I think what's fair to culmination of decades of but then really wrapped in You will forgive me about So, this is a benchmark that folks that we've and keep updated now. So, here's, two tasks. There's a groupby task and gigabyte dataset here. And you can see we have also managed to be Same is also on the join task. And, this is not even the We have, I think, the latest the next version But coming back to I don't I don't really So DuckDB is free. It's free and open source so you can build and you don't have which I think is difficult from traditional but we really think that in people think about data, which is kind of what we really need to the state of the art tools and we can't really, you know, make this just happen for one company We have to really make The world is kind so here's a plot of It's a very, it's a crap metric. Most VCs care about this, but, it's interesting to see how this is going on. So we really have an exponential growth in the We're really, really We eventually spun off from company called DuckDB Labs, because every open source project You can fund the open source company and then doing but I can't really do that. You can sell your soul to continue until the party stops and But DuckDB itself is, yeah, backed by DuckDB Labs, which and consulting but actually is not the which is in a nonprofit. So, this is really that we don't have, you know, but it's a long term effort foundation just to make sure that So even we have trouble are gonna And DuckDB loves R. Hey. The motive as I said, the motivation to build DuckDB R community. I actually spoke to about DuckB meetup, with Kirill in Zurich, in summer of 2019. That was, that was And because it's in process, we can actually do some pretty So for example, DuckDB running in directly read any data frame process, as if it were a table. There's no, you know, we don't have to first convert this If a data frame exists we can run queries on it, and we can also take the produce from DuckDB and convert and some systems out there have this, but it tends We can do this very quickly same process space. We don't have to do or any of that sort. We also love Python. So it's the biggest API It's like six million It's pretty ridiculous, and we can do also the same So if you run DuckDB in a pandas data frame sitting there, we can also treat it We can also produce results because that allows with downstream many datasets or many tools are as intermediates we can also Python. If the data happens to be Arrow, we can read and write Even streaming, so we don't it's just streaming into that works all really well. So that's DuckDB in a nutshell. Let's revisit the statement. Slower and more frustrating. I'm going to go back to the our wonderful CSV file. First, I want to say we on CSV parsing because if it turns out that that you can actually write a I think we may have been the There's some some happened we actually have one of the DuckDB because we realize getting data from Right? You're not going to sit down DuckDB. You're going to and so we brought this research from the research paper, a shout out to Pedro from our team and we can actually read most files So for example, in DuckDB, you can just type from data Right? And this is We can treat table if if you table name for the using and we have this logic that and there's a file with probably we should read this it's probably a CSV file. So we do a lot of sort of frustration, and it will you read this file correctly with and so forth, and We also have some cool features Right? So we have some features that an analyst easier, even So to summarize is pretty cool summarize any query really, and we will compute a very efficiently on this file. So this is a couple of million very short amount of time, the minimum, the maximum, the average, the and the null calls, for all the columns in the file. Right? And this is a single parsing, so we read through the file once We give you a good overview And I said that we're not gonna but I couldn't resist but of the most flexible it also is the fastest. Okay? So this is cool. So now we've got the frustration of running queries of reading CSV files Good. But let's do a bit more I didn't get to this part with frustrated to kind of but we're actually not We're here to, answer So here, I have a totally So given this dataset, give me the number of non forty that live in Auckland area Okay. So, if, now, somebody has to Well, if you have generative but I have just written and it's basically you can see It's doing a bunch of joined all these datasets are actually codes and except for the count, which is sometimes a value And it is a bunch in New Zealand in all the dimensions and datas that are aggregates of don't want that But if you just ignore all this, we can run this and it completes in one second. That's kind of oops. Sorry. I wanted to go here. It's kind of weird because I one second to read How can you oops. Sorry. How can you run this files, joining them all or logging multiple things, filters, and all Well, again, magic. And well, the reason is it takes the query and turns which we call a query plan, concept of how the query And this is optimized and So the filters, for example, they remove data from the from the query as early as possible. We prune all the columns that possible, and it's which means we don't actually have And, yeah, so it's But then I realize that not everybody loves SQL. I realize this. So let's explore So obviously, we can which I think is much nicer to So, I've written down the exact actually checked whether the So this is the same kind of thing sort of joined together dataset mutation and filters and we have our final analysis on It takes fifteen seconds. It's not so terrible, but this is exactly why on this see a significant difference we don't have a sort of holistic Dplyr is eagerly executing So enter duckplyr. Okay. So we've been actually working with hybrid mutant of and the result is DuckDB. I mean, you have maybe I've put a bunch outside. So massive thanks again to It's dplyr, just and you have the so the syntax is And this, from the CSVs, directly finishes in So that is because we have how this query and directly on the CSV files, ten times faster. From parquet files, so more on that later. If you run the same query already loaded into R session, it's roughly a So, if you're not doing the But I want to talk a little and while you read this. So if we just run this this this sequence of events, we're reading thirty That's just about thirty five million rows. And then we run this filter. Well, you can't really but the filter only matches because that's just, you know, the way that the So we're throwing away all these fourteen thousand rows, and then it's getting worse we only ever look at so we wrap the five other And what we actually need to make we need holistic optimization columns are gonna be used, which rows can be and all these kind of things. But this is also but now we have start making these data, we would break the Like, say, we make this a lazy We wanna pass stuff into ggplot. Ggplot would say, I I don't So we somehow have to make data frames and And we have, thanks to which is really exciting, in objects that are sort of both can be both lazy and non So these if you use duckplyr, these these objects data2 and frames and you difference without, attitudes cannot really but we internally know that and we can do the So this is really cool. If you wanna try it There is a way of, enabling telemetry if and I encourage you to do improving duckplyr. But we've also been super will fall back to dplyr something we don't understand actually not see when you just start using it. So I encourage you to I also want to show Ibis about DuckDB is it's an engine that is So DuckDB calls also and I don't know if he he made this, cool That basically means you can in Ibis, sort of syntax, which is this Python syntax that it'll produce the exact same results one point three seconds because DuckDB query plan. So now we've done, complex queries with DuckDB, basically do very, very complicated questions on quickly, and we don't really favorite programming Okay. But we've kind of so far, this Right? I've been defending against the frustrating and slow. So I don't think DuckDB but the thing is there is more than reading CSV files and And so I want to talk datasets with duckdb. And atomic is a positive thing, so just imagine Okay? Okay. So first of all, people say, yeah. Okay. We'll we'll we'll And I agree. I'm a professor We should not use CSV files. We should use Parquet And DuckDB can actually In case you don't it's just self describing It's really cool. DuckDB can read and write So, tip one, you should DuckDB, consider using Parquet That one is a no brainer. Yeah. And so we have native There's other options like files or the new nanoparquet The interesting cool thing dataset is actually that if you take this file, which was an eight if you turn it into it's only seventy one megabytes, which is smaller than even the and it's actually much so there's nothing But then what happens, and is like I've done a bunch of these files, I've I've done some scripting, and now I end up with this And there's no more sort from where and what used to create this and what and and some people and then that make file files, the timestamps there, but I don't think that's But, we get this ungodly And what we should be DuckDB can help here because we can store many tables. We can store views. We All of that in a single file. So what we can do is we can we can create a single .db file parsed columnar typed compressed binary formats. Ingestion can be atomic, wrap this import statement it either all goes through The resulting file is also so it's about the same as a And we actually have a we have finalized, so this format is now stable and will remain foreseeable future. And there's actually some that are already shaping all which is really crazy, So you go Hugging Face, and you get the DuckDB file. So it's really cool. They can even be read so it's really cool. So we put the data in this We can also do things like this, where we can create which are like a sort of a transformation using SQL or other tool, but it And these views, they're actually stored and it makes it actually much and you can stack the views like, multi step derivations, and that means that whenever base table, the views They don't cost any storage symbolic descriptions. And this hides it's possible to these kind of tricks, share the file and they can look at the file and see It's still a single file. Another thing that's really cool create like macros, which are like sort of that you're using a lot. So on the top here, we're kind of repeating are have used these total these different and we kind of have to filter muck up our summaries. Okay? But you don't these ugly expressions so you can create a I wanna have a macro not total expression in it. They can just use it. And again, the cool thing is So that means that you can send they can use it. It's really cool. And it's together together with all the views, And finally, I want to talk I know that transactions are from a database course we move some money from A to Mark has actually really great transaction cases where we're we're making we're we're updating a lot of removing columns, And it means we can this is why I said atomic, atomic sort of sequences of all go through or not, and we can even do a manual if we did something by accident, if we didn't sort of intend. So, if I go back to this I couldn't read the file you can just say, oh, no. This was actually You can roll back and your file, will be unaffected and the K? There's a bunch These are just three examples management systems actually what we're kind of used They go beyond what an analytical environment, and they can actually Okay. So, I will, I will respond to actually just put your data in DuckDB. Not doing so will only be Okay. But now I have shown with DuckDB and similar tools, but I think it's an interesting take the last couple of minutes of It's like, what does this for data engineering, What does it mean if we comfortably crunch gigabytes And there's been some whether big data is dead or not. And, this is actually a to create a picture, So, yeah, Maybe it's a overrated But let me take you on a bit So if we somehow, if you say, let's just do a thought the human typing speed is roughly The working population in the I will assume I have infinity something, and, I've all ten million. And the work here of the Netherlands has about one hundred thousand So if I employ the for a whole year to type important we will, end up with roughly Well, this is, of course, ASCII so if you compress this we will end up with something And that sounds like a lot. But it's actually not. So you can buy these They cost around There's a banana for scale and I mean, we have seen IBM hard They were like these things and things like that. No. No. No. No. These are tiny. So the so the maximum data of the entire would easily fit on There's no scale up required. I don't have to buy, you know, In two years, it Will we have two times the Netherlands? I Okay? In ten years, this The human growth in sort of in people and or produce any meaningful data, is growing much slower than our And I will go a We look at compute. The Apple M3, pretty I'm sure some of you have They are very fast cores. They have four hundred which is totally insane. Right? It's it's something that that some memory that the CPU can Right? It's insane fast memory. They have four hundred I'm sure lots of datasets that you They can all be in the This is this kind of server machine from 2010. Again, I'm talking like The the c these CPUs also have second in RAM throughput and hundreds of gigabytes per RAM. Like the MacBook can hundred something So you have four throughput and that's also performance and we have kind going and we are not realizing, how much, like, how other data Similar for SSDs, right? The SSDs are a total game because of the speed. Current MacBooks, again, five point five gigabytes per It's insane. And with DuckDB, we of these, capabilities for example, in the SSD we can offload So you can go much larger than won't really notice But that's now now we can Fun stuff. So the data is growing, yes, but it's probably There's a limited amount I think this is something that There's automatic automatically but it tends to be like less So here we just plotted the And for the purpose it actually doesn't matter logarithmic or not. As we have seen, the commodity compute improving like absolutely crazy with data with the And it's faster than the It's really crazy. But we can probably agree that faster than the In the past, there was a very large could do, single computer and sort of the device, the ninety nine percentile So that meant that we had to go quickly, like big, you know, Now, it's less so. Right And, we're probably looking at where ninety nine are actually comfortably is that a word, on So this is really cool. It's really a fun thought. And this is especially rants like this paper have distributed, sort of, environments is massively that, we actually have to make huge this cost and it's huge impact in terms of cost something on a single And this was always just recommend, this blog post, the Hunt for Big Data, which is a blog post, that talks about a who has for the first time they have published detailed datasets that they see in processing system. And the answer is, shockingly, that we're actually quite close Like the, you know, the ninety nine percentile of that does now fit I'm gonna do one more I can't really do, As a database person, I feel very uncomfortable So I'm doing one So we're going to use it's the standard standard analytics benchmark Apache Spark versus And we're gonna run this on I think it's like Thirty two thirty Each, each node has, sixty four gigabytes of RAM and hundred which is like a Not crazy. Right? If we run this on a single node, actually we can't because Spark But, if you run this on it will be seven times slower. And why is that? Well, it comes back to First is the per core pretty problematic. And the other thing is mentioned that running system is actually much more expensive So on a single node, this Hannes, you're being unfair. How can you dare comparing a with your, you know, Okay. We scale up. We're adding computers and we're adding computers and it thousand twenty four cores, than, than the single, you know, single node DuckDB instance. We're still paying for a thousand twenty and we're still producing CO2. And one thing I think that's high paid analyst waiting for which I think is even more expensive And this never catches up this coordination cost Okay. So that was I have one more thing and talk about DuckDB extensions. So we have realized that we really deal with everything. We don't understand everything. So we are actually for external extensions, anyone, could be called them So you can expand DuckDB with new syntax, new protocols, whatever you want. And this has so far been kind people, but we're now So all you do is you send us a It's like a descriptive file. And basically you have a is a C++ but we're working to Rust and other languages. And once you do that, install these extensions directly And we think it's really big step for us is to sort of specific data are connected with a high performance fabric This is kind of from the current sort It's like this idea that you can with considering for consideration and we're gonna make this extensions and it's all gonna of unique, unified interface. So that's kind of it. In conclusion, databases are not if the database is stuck to be. Okay. There's more cool beyond reading CSV files I really think single node, is the future and that big data I'm quite happy about this. And if you're still tomorrow afternoon, we're actually running a by and there's still So it's free. If you want, you can go to our So with that, I Thanks, Hannes. Personally, I really enjoy it when something that I've So we have a few minutes for Since you can store and macros in a DuckDB file, how does a user explore the That's a great question. So you can open up the file and and it will show you So it will show you, oh, we have these views. I don't think it necessarily but you can there's also Yeah. But you just So you said DuckDB is really but can you still kind lightweight So, like, if you've got a Shiny app How? How how well Yeah. I think I think the what loop with a thousand iterations, and every iteration of that loop Don't do that. But if you want to, you know, do a couple of hundred a few changes, Right? Like, it you can definitely No problem. It's just not gonna be, competitive, I will say, I have a Shiny app I don't know if you want a Yeah. Yeah. Yeah. Yeah. And it it logs every Mad Lib which kind of works Yeah. I always love, one of my other favorite Big R Query, which works It uses, like, mtcars I just love the thought of, massive datasets that can deal sending it, like, thirty They love you too at Why did you name it DuckDB? It's a great question. So I live on a boat, and and and so boat's kind so I figured I'm gonna So I ended up having a Wilbur at some point grew but DuckDB is I, not to name drop, but You have met Wilbur. It's true. Help me. I am lost. If I load my big CSV Would this be Yes. If you so the the the reason you load a CSV file into R, it Right? And if your big CSV file at some point, the operating be out of memory, you die now. Right? And in DuckDB, it we data, but what you can also do And so what you can do is you connect to a file on disk and and then we'll never ever materialize So that's, I think, the one of the really cool than memory and data sets materialize this file memory. And that even applies which is really important. Right? So I had this example if what if you sort Right? Traditionally, that has to go into crash because we don't but we have made the sorting a that can use the disk efficiently, and Cool. How well does DuckDB really big and complex So, like, hundreds or thousands of hundreds of tables, user That's a great question. So we're not so if thousands of users at something that It was made to support, like, in one way the analytics of data transformation If you're, if you're thousands of users, you probably it's probably Hundreds of tables Hundreds of columns, thousands Complexity of queries It's but the concurrent use by something that is not that's not But, I can plug so it's kind of what they do. So On kind of a related note, how does DuckDB handle Yeah. How to read and So we use MVCC, a which basically means that a query And then when we commit, we have to check whether and, that happens automatically So, basically, many threads at the same time and there is a automated check of conflicts that have to be But in principle, Cool. Does DuckDB work well reading object storage, like AWS S3, Yes. That was an easy one. Yes. It does. At my workplace, CSV is the default Is now the time to Yes. I I I mean, yes. I think that the time Let's see if I can give you Oh, here's a here's a tough one. Are there any downsides to for data analysis, either for interactive I think the kickbacks from A downsides, I mean, I'm the I think what we we see a lot of feedback Right? We see a lot of but we see issues reported. We have discussions. There is design choices that don't work for other people. There's are there any I think there are that it wasn't made for. Right? So, like like you you said earlier gonna write, use it it's not the tool for the job. It's analytical system. So then if you were to that would be not great. Right? Any downsides, yeah. It's I'm I'm really not I think. So no downsides. Ask somebody else. Well, thanks so much, Hannes. Well, thank you. So to close out the conference, I just wanted to say A really big thanks the reason why They have put so much time working with our, our They've done training They've practiced their All the talks I've seen and I really appreciate all the Also, I'd like to Appsilon, Cynkra, Lander Jumping Rivers, and Procogia. You know, thanks This conference is really There are about a hundred Posit Everyone has been The people you have seen, the people you haven't making sure that all of the the background have never So a big thanks to everyone conference happen. And then finally, I just know it's like a fire into your brain over these two like, everyone's time and Thank you. And the one thing I know to learn about where year, or posit::conf, and from posit import conf, K. I didn't get a, I didn't get a lot round of last year, so I really It's gonna be Atlanta So we'll be sending out signed up with in We'll have opportunities as tickets, but we are super excited Thank you.