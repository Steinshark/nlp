- [Justin] In the nation of blobs, there's a popular game Each blob brings their own coin and they take turns flipping. When a coin comes up heads, the one who flipped it feels happy. And the other one feels sad. That's it. That's the game. It seems kind of simple, but There's a rumor going around are using trick coins that come up heads more And that's just not fair, so we would like to catch these cheaters. As a warmup, let's have each of these blobs (playful electro-percussive music) Okay, you might be able to tell that this is an artificial sample. We have results ranging from zero heads, all the way up to five heads in a row. Which of these blobs, if any, would you accuse of being a cheater? If you'd like to try your hand at judging the blobs yourself there is an interactive version I introduced in the last video. Looking at the data from that game, when a blob got five it turned out to be a cheater Because of the randomness, it's impossible to be completely sure whether a blob is a cheater. But some approaches During the rest of this video, we're gonna build up of making decisions with limited data. If you like learning new vocabulary terms, you're in for a real treat. The name of this method is We're gonna design a test can use in its day-to-day We want three things from this test. First, if a player is using a fair coin, we want to have a low chance Second, if a player is we want to have a high And third, we want this test to use the smallest We only have one blob detective, and we want it to be able to And it's also nice not to We're gonna design that test together, but if you're feeling up for it, it can be good for learning to So this is your chance to to think of a test that Okay, let's take it one flip at a time. It came up heads. The cheaters have heads so this blob must be a cheater. Well, no, we can't just call I mean, we could, but with that policy, we'd wrongly accuse quite After all, even if the player that the first flip would come out heads. So let's see the second flip. Heads again! Cheater? Well, it's more suspicious we should think about how if the coin is actually fair. There are two possible and two possible outcomes Two heads in a row is one that are all equally likely. So the probability of two out of two heads is one fourth or 25%. Another way to get that number is to multiply the probability values of the two events together. You do have to be careful about multiplying probabilities, if the two events are and that is the case here, doesn't make the second Anyway, with a one in four chance of falsely accusing an innocent blob, it still feels a bit too early to accuse the player of cheating. After another heads, this probability is divided by two again. I'm starting to get pretty suspicious, but we'd still accuse one if we accused after three heads in a row. We want that rate of false accusations to be as low as we can get it but we're never gonna get It'll always be possible to get an epic streak of So we have to make a decision The standard choice here is 5%, or one false accusation out We could choose a different but we might as well start here. Okay, so at this point, There's only a one in 32 or 3.125% chance of seeing five heads in So one possible test if a player gets five out of five heads, accuse them of being a cheater. Otherwise, decide they're innocent. So let's see how this test performs. We're gonna want a lot of data. So let's make a set of 1000 players where half of them are cheaters. Before we see the results, How often will it wrongfully And what fraction of the cheaters Alright, we can divide these Fair players the test decided are fair. Fair players we wrongly Cheaters who got away with it. And cheaters we caught. It looks like we achieved Not only did we accuse fewer the test did even better than expected. When we use this test in the real world, we won't know how many but seeing how the test combined with our analysis from before, it feels like we can be pretty confident that we would accuse We didn't catch very many cheaters, but that's not too surprising. We haven't even thought about them yet, so I'm sure we could do better. Before we make the next I think it's worth mentioning They aren't always necessary, and like any specialized words, they do make communication If a test result says not to it's called a negative result, And when the test does say it's called a positive result. Cheating is a bad thing, but the term positive here is referring to the test saying &quot;Yes, the thing I'm looking for is here.&quot; The same is true for medical tests. If the test finds what it's looking for, the result is called positive, even though it's usually a bad thing. So we have positive and but the results may When we test a blob the correct result would be negative. So if the test does come up negative, it's called a true negative. And if the test comes out so it's called a false positive. And when we test a cheater, the correct result would be positive, so if the test does come up positive, we call it a true positive. And if the test incorrectly that's a false negative. We can also rephrase that first the false positive rate. This can be a dangerously It's easy to mix up what False positive rate out of all the positives, what fraction of those Or even, out of all the tests, But really, it's saying, how many of them are I've known these words for quite a while, but my brain still the wrong way basically every time. So to keep things as clear we'll keep using the longer Okay, let's go back to designing the test. We still need to figure out a way to achieve goal number two. Let's start by making To do that, we need to pick a number for the minimum fraction of Using the terms from before, we could also call this the But again, let's stick And to throw even more words at you, this minimum is sometimes called the statistical power of the test. It's the power of the The standard target for Just like the 5% number in the first goal, we could pick any value we want here. But let's run with 80% for now, and we'll talk about Now for calculating what we expect the true What's the probability that a cheater would Take a moment to try that yourself. Okay, that was kind of a trick question. There's no way to calculate that number, since we haven't actually said anything about how often an unfair In that trial we just did with 1000 blobs, the cheaters were using 75% of the time. We don't know for sure if So this 75% is an assumption. But we need some number here so we gotta run with something. And yet another word, this In this case, it's the effect You might be getting annoyed that this is the third time I've said we should just run with But what can I tell ya? Some things are uncertain The important thing is to remember when we're making an That way we can note our assumptions when we make any conclusions, for different choices if But now that we have a number, If the probability of each heads is 0.75, the probability of five heads or about 24%. So our existing test should And hey, that is pretty close so everything seems to But our goal is to catch 80% of cheaters. The current test is a little bit extreme. It requires 100% heads This does make false positives but it also makes true positives So we're gonna have to think about a test that allows for a mixture Calculating probabilities can be a bit confusing though. For example, if we make a new test that requires a blob to and accuses them of being a cheater if they get seven or more heads, the calculations in that situation are gonna be a lot harder. There are a bunch of ways for there to be seven And we also have to think of eight, nine, and 10 heads. To start making sense of this, let's go back to just two flips. With a fair coin, each of these four possible So the probabilities are one out of four for getting zero heads, two out of four for and one out of four to get two heads. But with an unfair coin that favors heads, they're skewed toward With three flips, there are with four possible numbers of heads. As we add more and more flips, it quickly becomes quite a chore to list out all the possible outcomes and add up the probabilities. But there is a pattern to it, so thankfully there's a called the binomial distribution. It's not as scary as it looks, but still a full explanation I'll put some links about but for now just know that this formula is what we're using to and it follows the same pattern we used for two flips and three flips. Now let's go back to our where we accuse a player if they get five out of five heads. We can show the rule on these graphs by drawing a vertical line that separates the positive from the negative test On the fair player graph, the bars to the left or the innocent blobs we leave alone, and to the right are the false positives, the fair players we wrongfully accuse. And on the cheater graph, represent the false negatives, the cheaters who evade our detection, and the bars to the right the cheaters we catch. Just like before, we satisfies our first goal of the fair players we test on average. But it doesn't satisfy our second goal of catching at least 80% again, on average. But now that we have these graphs, we can see what happens when If we lower the threshold to we don't meet either requirement. If we keep lowering the threshold, it can allow us to meet goal two, catching more than 80% of the cheaters, but then we accuse even more Apparently, if we want to meet we're gonna need more flips. If we put these graphs we can see that the blue overlap quite a lot. So it's impossible to make a test that reliably separates But if we increase the now there's a big gap so it's easy to find a But we also have this third goal of using as few coin flips as possible, so we should try to find Since we already have the computer set up to run the numbers, we and just keep trying different thresholds with more and more flips until we find a test rule that works. It turns out that the smallest test that meets our first two goals has a blob flip its coin 23 times, and the blob is accused of being a cheater if they get 16 or more heads. That's more than I would've but it's not so, so huge, so, it'll do. Alright, let's use this This blob got 17 heads. That fits our rule of 16 or more, so according to that test, we should call this blob a cheater. There is another term Assuming this blob is innocent, the probability that is about 1.7%. We call this 1.7% the P It's kind of like a false positive rate for a single test result. Kind of. 1.7% is below the 5% we so according to the test, And looking at it from if the blob is cheating, using a coin that comes there's a 65% chance that Another way to say it is of results we'd expect from cheaters. So if we wanna catch 80% of the cheaters we'd better call this one a cheater. Okay, let's try it with one more blob. This one got 13 heads. This is more than half of the 23 flips, so it's tempting to call it a cheater. But 13 is below the 16 heads the test requires for we call it a fair player. The P value of this result is about 34%. So if we accuse players we'd expect to wrongly That's well beyond our 5% tolerance, so we can't call it a cheater. And looking at it from if it were a cheater, there that they'd get this many heads or more. We don't have to catch 99% of the cheaters to hit our 80% goal, so we if we let this one off the hook. Is the first one really a cheater? Is that second one really playing fair? We can't know for sure, but based on how we designed our test we should expect to catch at and falsely accuse less So now let's see how this test does on another group of 1000 blobs. Like before, half the blobs in this group are using a trick coin that has a 75% probability Okay, the results do look about right. We accused less than and we caught more than 5% and 80% are the normal So we could make different Maybe we decide that we really do not who are playing fairly. So we wanna lower the To achieve this with 23 flips, we'd have to raise the This would lower the to about 47% though. If we don't want to increase we could decide we're okay with that 47%, maybe we just want cheating to feel risky, so 47% is good enough. Or, if we still want to catch we could increase the number of flips until we find a test that We could also be super hardcore and go for a 99% true positive rate, and a 1% false positive rate. But we'd have to flip the coin 80 times to get to that level. We'll always be able to set but that'll limit how well How to set these goals we're willing to make. For the rest of this video though, we're just gonna go with Now that we've settled on and we have a test that seems let's test one more set of blobs. To pretend these are real blobs and not some artificial sample, I'm not going to tell you except that there are 1000 of them. How do you think this test will do on this more mysterious group? Will it manage to accuse fewer And will it catch 80% of the cheaters? At this point in the video, and not actually make the predictions. But if I'm asking you something must be about Or, maybe I'm just pretending so you'll engage a little more. Who can say? But really, what do you think? Okay, so we labeled about a which is a bit less than before. If this were the real world, You wouldn't get to see and who was really innocent to get confirmation that the I mean, maybe you could, but You couldn't do it with this test alone. But because this is a computer simulation, I do know the full truth. This group was 90% cheaters. We still accused less than but we only caught about Something went wrong. The problem is that we assumed that the cheater coins came And that assumption was wrong. The real world cheaters were using coins that came up heads 60% of the time. If we knew that from the beginning, we still could have designed but it would need 158 flips and require 90 heads to which is honestly way more coin But in hindsight, it's not that surprising that we need a lot of data to tease out that smaller difference. But we didn't design that test because we got the effect size wrong. I know, I know, I was the one But be honest with yourself. Did you remember that assumption when making your prediction? It's very easy to forget that and instead just treat them as facts. This concludes me tricking you but they really are easy On the bright side, though, at accusing less than The framework we built up here isn't just good for catching unfair coins. It's the dominant framework used in actual scientific studies. To summarize we take a yes or no question. In this case, our question was, is this particular blob But it could be any question. Then we come up with a model for what kinds of results we'd and if the answer is no. Then we come up with a test of telling those two situations apart, according to the models. The details are usually since most real world are more complicated than coin flips. But most scientific studies have this framework at their core. Like I mentioned at the beginning this is called frequentist There's another method called which we'll look at in the See you then.