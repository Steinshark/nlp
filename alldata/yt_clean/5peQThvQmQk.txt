One of the tasks you will do as a Data Engineer is from multiple sources, do some transformation target location. Now, you can perform this entire have to do is read data from some APIs, write onto some target location. There is something script at a specific interval, you can schedule But here's the thing: you can use Cron but what if you have hundreds of data pipelines? in just the last 2 years, and businesses around products and services. The reason you see the correct ads on your Instagram profile is because than thousands of data pipelines running in these So today, we will understand how all of and we will understand one of the highly called Apache Airflow. So, are At the start of this video, we talked about to create more and more data pipelines to process if you want to run all of these operations we have multiple different operations coming. APIs, or some other sources. Then the second and the third script will basically store these operations should happen in a specific we schedule our Cron job in such a way that all Now, doing all of these operations using a simple might need to put a lot of engineers on each runs smoothly. And this is where, ladies and In 2014, engineers at Airbnb started working on a Software Incubator program in 2016 and became the world can use it. It became one of the most with over 10 million pip installs over a community of over 30,000 users. Airflow became The reason Airflow got so much popularity good user interface or it was easy to install. &quot;pipeline as a code.&quot; So before this, we talked in a simple Python script, but it becomes very such as you can use enterprise-level but these software are very expensive. And also, you won't be able to do that. This is where can use it, and on top of this, it gave a lot schedule, and run your data pipeline on scale, So now that we understood why Apache Airflow let's understand what Apache Airflow is. So, A workflow is like a series of tasks that need to the previous example, we have data coming from between, and then load that data onto some target transforming, and loading is called a workflow. but it is called a DAG (Directed Acyclic At the heart of the workflow is a DAG that tasks and their dependency. This is the of it as a blueprint for your workflow. The run in a specific order. &quot;Directed&quot; means there are no loops - tasks do not run in a and &quot;graph&quot; is a visual representation flow is called a DAG, and the individual the DAG defines the blueprint, and the tasks So, in this example, we are reading the data from and do some transformation, and load this data tasks are executed in a specific order. Once the will execute, and like this, all of these Now, to create tasks, we have something called provided by Airflow. So, you can use all of these actual work. There are many different types if you want to run a Bash command, there is an you want to call a Python function, you can use a you can also use the Email Operator. Like this, different types of jobs. So, if you want to read data to Amazon S3, there are different types of So, operators are basically the functions and the collection of different tasks is we have something called executors. Executors there are different types of if you want to run your tasks sequentially, you run your tasks in parallel in a single machine, want to distribute your tasks across multiple This was a good overview of Apache Airflow. in the first place, how it became popular, Apache Airflow that make all of these end-to-end project that you can do using Apache let's do a quick exercise of Apache Airflow to So, we understood the basics about Airflow attached to Airflow. Now, let's look at a quick like and how these different components come Okay, so we already talked about DAGs, right? Airflow. Basically, a DAG is the collection it looks something like this: A is the task, it will execute and it will make the complete Now, it is pretty simple. You have to you import the DAG, and then there is the with DAG, this is the syntax. So, if you know the if you don't have the Python understanding, so you can check that out if you So, this is how you define the DAG. With DAG, so when you want to run this particular DAG, you want to run daily, weekly, monthly basis, you that this DAG function takes. So, based on your and the DAG will run according to all of So, this is how you define the DAG. And if you where you give basically the task, the task you want to attach this particular task to. So, and then we provide this particular DAG name to Python Operator or Bash Operator, all you have to Now, just like this, you can also create the right? I want to run my, uh, all of these tasks I provide the first task, and then you can the first task will run, and it will execute the task completes, the fourth task will be executed. Now, uh, this was just documentation, and to learn more. So, let's go to our Airflow Okay, once you install Apache, it will look to this page, and over here, you will see a lot example DAGs that are provided by Apache Airflow. you will see, uh, this is the DAG, which basically Just like this, if you click onto DAGs, you will understand how all of these DAGs are created information about the different runs. If your running, or failed, this will give you all of So, I can go over here, I can just enable this and I can manually run this from the and it will start running. So, currently, and if I go to my graph, you will if you keep refreshing it, so as you can see, Now, there are other options, such restarting, and all of the different statuses this is what makes Apache Airflow a very in one place. You don't have to worry about at one single browser, you So, all of the examples that you see it if I go over here and check onto example_complex, right? You will see a lot of different and then entry group is, uh, dependent on is pretty complex. So, you can create all Now, one of the projects that you will do after Twitter API is not valid anymore, APIs available in the market for free I'll just explain to you this code so So, I have defined the function of the file is twitter_etl, right? Uh, what we are really doing is extracting doing some basic transformation, and Now, this is my twitter_dag.py. So, this is so as you can see it over, we are import DAG. Then, from PythonOperator, I'm run this particular Python function, which is so I first defined the parameters, which and all of the other things. Then, this is where these are my arguments, and these are my Now, I define one task. So, in this PythonOperator, I provide the task ID, Python this function is, I import it from the this one. So, twitter_etl, from twitter_etl, and I call it inside my PythonOperator. So, and then I attach it to the DAG. And then, Now, in this case, if I had like different run_etl2, something like this, okay? So, run_etl2. And then, I can create the dependencies execute in a sequence manner. So, once this So, I just wanted to give you a if you really want to learn Airflow from scratch I already have one project available, pipeline using Airflow for beginners. So, I've created. I will highly recommend get a complete understanding of Airflow I hope this video was helpful. The goal of Airflow but to give you a clear understanding you can always do any of the courses available them because most of the people make technical I started this YouTube channel is So, if you like these types of content, and don't forget to hit the like button. Thank