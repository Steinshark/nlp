Today I have the pleasure of speaking and I'm really excited about this one. on the podcast. First question. You have been one of coming for years. As somebody who's seen it for why scaling works? Why is the universe of compute at a wide enough distribution I think the truth is that we still don't know. fact that you could sense from the data and from have a satisfying explanation for it. kind of waving my hands when I say this, there's law of correlations or effects. When a bunch of you get a lot of the data in the early fat For language, this would be things like speech and nouns follow verbs." And then there So it kind of makes sense why every log or order distribution. What's not clear at all is why does scale so smoothly with the amount of data? it's linear. The parameters are like and so size of the bucket is proportional to size very smooth scaling? We still don't know. There's Jared Kaplan did some stuff on fractal manifold So there's all kinds of ideas, but I feel And by the way, for the audience who we're referring to the fact that you can very to Claude-2 that the loss in terms of whether it Okay, so we don't know why it's happening, but is the loss at which this ability will circuit will emerge? Is that at all predictable That is much less predictable. What's this loss, this entropy. And it's super to several significant figures which you don't it in this messy empirical field. But specific Back when I was working on GPT-2 and GPT-3, when learn to code? Sometimes it's very abrupt. averages of the weather, but the weather on Dumb it down for me. I don't understand it doesn't know addition yet and suddenly This is another question that we don't know the like mechanistic interpretability. You can think place. Although there is some evidence that when its chance of getting the right answer shoots the probability of the right answer? You'll one in 100,000 to one in a 1000 long before it cases there's some continuous process going on Does that imply that the circuit or existing and it just got increased in salience? and getting stronger. I don't know if it's I think we don't know and these are to answer with mechanistic interpretability. I definitely think that things like alignment scale. One way to think about it is you train the it's understanding the world. Its job is facts next. But there's free variables here -- What should you value? There aren't bits for that. finish with this. If I started with this other And so I think that's not going to emerge. we reach human level intelligence, looking What do you think is likely to be the I would distinguish some problem with the issue. One practical issue we could have is I think that's not going to happen but if you running out of data. So it's like we just don't Another way it could happen is we just use that wasn't enough and then progress is slow things happening but they could. I personally think it's very unlikely If they do, another reason could just be that we tried to do it with an LSTM or an RNN the slope there but there are some things that are just very to attend far in the past that transformers have. the architecture I'd be very surprised by that. things the models can't do don't seem to be You could have made a case a few years ago You could have drawn boundaries and said maybe wall, a few other people didn't think we would then. It's a less plausible case now. We could hit a wall tomorrow. If that happens my the loss when you train on next word prediction. really high level, it means you care about some enough that the loss function over focuses on the the most bits of entropy, and instead they don't you could have the signal drowned out in the that way for a number of reasons. But if you It was much bigger and it just wasn't any better, that's the explanation I would reach for. If you had to abandon next token prediction. kind of RL. There's many different there's RL against an objective, there's things amplification and debate. These are kind of both You would have to try a bunch of things, but the about the model doing? In a sense, we're a little these other things we need. There's no guarantee. of different loss functions that it's just a whole bunch of data at it. Next token The thing with RL is you get slowed down a bit works by some method. The nice thing with the It's the easiest thing in the world. So couldn't scale in just that very simplest way. constraint. Why do you think that is the case? number of reasons I shouldn't go into the details, and there's many ways that you can also generate Maybe it would be better Are you talking about multimodal? How did you form your views on scaling? How basically saying something similar to this. 2014 to 2017. My first experience with I saw some of the early stuff around AlexNet in but before I was just like, this doesn't seem to 2005. I'd read Ray Kurzweil's work. I'd read back then. And I thought this stuff kind of today and it's not anywhere close. this stuff is actually starting to at Baidu. I had been in a different field and a bit different from a lot of the academic style I kind of got lucky in that the task folks there. It was just to make the best There was a lot of data available, there were a a way that was amenable to discovering that kind from being a postdoc whose job is to come up with mark as someone who's invented something. was just fiddling with some dials. I was try training it for longer, what happens? I add new data and repeat it less times? And I didn't really know that this was unusual or almost like beginner's luck. It was my first about it beyond speech recognition. I was just There are zillions of things people do with seems to be true in the speech recognition field. who you interviewed. One of the first models, they just want to learn. You have to to learn." And it was a bit like a Zen Koan. And over the years, I would be the one who would them together, but what that told me was that the thing. It was broad. It was more general. obstacles out of their way. You give them good you don't do something stupid like and they want to learn. They'll do it. what you said is there were many people who were recognition or at playing these constrained games. did to something that is generally intelligent. about it versus how others were thinking about speech in this consistent way, it will get I genuinely don't know. At first when I saw it for for this narrow class of models. I think it was I tried it for a lot of things and saw the same being true with Dota. I watched the same being as a counterexample, but I just thought, but if we look within the data that I think people were very focused them. It's very hard to explain why one thinks a different way. People just see it vertically instead of horizontally. they're thinking about how do I solve my problem? That can easily abstract to -- scaling For some reason, and it may just have been random, When did it become obvious to you that language these things? Or was it just you ran out of other This other thing, there's not enough data. prediction, that you could do self supervised much richness and structure there for predicting and you have to know the answer is four. It Basically, it's posing to the model the that get posed to children. Mary walks into Chuck walks into the room and removes the item To get this right in the service of predicting solve all these theory of mind problems, solve all well, you scale it up as much as you And I think I kind of abstractly had solidified and convinced me was the work that only could you get this language model that could tune it. In those days, you needed to So I was like, wow, this isn't just some model right. It's sort of halfway to everywhere. little move in this direction, it can solve this this other thing, it can solve translation I think there's really something to do. One thing that's confusing, or that would have have models in 2023, like Claude 2 that can whatever theory you want, they can ace just all kinds of really impressive things, I have something that is human level intelligence. seems we're not at human level, at least for generations to come. What explains performance in these benchmarks and the things That was one area where actually I was When I first looked at GPT-3 and the kind at Anthropic, my general sense was that it of language. I'm not sure how much we need from here is like RL and all the other stuff. more but I wonder if it's more efficient to other objectives like RL. I thought maybe if you 2020 style model, that's the way to go. that really the best path? And I don't it had understood a lot of the essence of Stepping back from it. One of the reasons about safety, about organizations, is I've been right about some things but still with most things. Being right about 10% of the stuff If you look back to these diagrams that are Here's the scale of intelligence. And the village Maybe that's still true in some abstract sense or is it? We're seeing that it seems like don't hit the human range in the same place Like, write a sonnet in the style of Cormac do that but that's a pretty high level human good at stuff like constrained writing like, I think the models might be superhuman or close proving relatively simple mathematical theorems, They make really dumb mistakes sometimes and your errors or doing some extended task. spectrum. There are a bunch of different bunch of different kinds of skills. Memory it's not complicated. But to the extent it even If you asked me ten years ago, that's not what very much the way it's turned out. just as a follow up on that. distribution of training that these models get humans got from evolution, that the repertoire overlapping? Will it be like concentric circles? Clearly there's certainly a large amount of models do have business applications and many that are helping humans to be more effective If you think of all the activity that humans put but it probably doesn't cover some things. world to some extent, but they certainly don't Again, maybe that's easy to fine tune. don't learn that humans do. And then the models to speak fluent Base 64. I don't know How likely do you think it is that these models valuable tasks while they are still below humans an intelligence explosion or something? so I'll give that caveat. You can kind of predict stuff, which we really want to know to know how My guess would be the scaling laws are people slow down for safety or for regulatory say we have the economic capability to keep My view is we're going to keep getting better the models are super, super weak or not starting and programming, but over the last six months 2022 generation, has started to learn that. There kind of suspect, even if it isn't quite even, Does that include the thing you were mentioning it loses its train of thought or its That's going to depend on things like horizon tasks. I don't expect that to require I think that was probably an artifact of thinking how much the model had learned on its own. in some areas and not others? I think it's be superhuman in some areas because they involve happens? Do the AIs help us train faster AIs? that? Do you not need the physical world? It alignment disaster? Are we worried about misuse, worried about AI taking over research from humans? of economic productivity where it can do what thresholds have different answers, although I Let me ask about those thresholds. If Claude was worth? Is it meaningfully speeding up AI progress? but then some specific areas One thing that makes the comparison hard is If you were to behave like one of these chat but they're more designed to answer single or of having a long life of prior experience. We're in the past and chat bots don't have that. it's hard to make a comparison. They feel like where they spike and are really savants, But does the overall picture of something like former guest, Carl Shulman, has this very detailed who would actually see that happening, does that entry level software engineers. Those entry level I think the idea that as AI systems become productivity of humans, then they equal some meaningful sense are the main contributor to That basic logic seems likely to me although I the details, it's going to be weird and different we're thinking about the wrong things and then are wrong about ten other things. I think When you add all this together, what does kind of human level look like? In terms of someone looks at the model and basically like a generally well educated human, that could happen in two or three years. be if we hit certain safety thresholds and stuff to slow down or we're able to get the government of progress for safety reasons, that would be just look at the logistical and economic ability Now that may not be the threshold where the I suspect it's not quite there yet. It may not be AI research. It may not be the threshold where I think it gets a little murky after that and times after that. But in terms of the base like a reasonably generally educated human across Why would it be the case that it could pass be able to contribute or substitute A couple of reasons. One is just that comparative advantage. It doesn't matter that human at every task. What I really need for AI enough to substantially accelerate the labor We might reach a point where the comparative Another thing that could be the case is that there in naive economic models but you see it whenever "Hey, I have this cool chat bot." In principle, bot does or this part of your company does, but do we make it work? That includes both just the the company, how things happen in the economy and workflow? How do you actually interact with it? looks like it's doing this task or helping the thing is deployed and 100,000 people are using it. these systems but in many cases, they're not using they could. Not because they're not smart, but And so I think when things are changing this fast, These are messy realities that don't quite get the basic picture. I don't think it changes the the models help the models get better and eventually it's mostly the models doing the work. skeptical of any kind of precise mathematical or I think it's all going to be a mess. But what and it's going to happen fast. which we've been talking about net out? power laws with decaying marginal loss parameter about is, these things can get involved in the Those two are sort of opposing exponentials. Does also you mentioned that the distribution After we get to this point in two to I think it's very unclear. We're already the scaling laws are starting to bend. We've seen companies. So that's not a secret at all. of entropy of accurate prediction becomes bits of entropy are the difference between a it as opposed to some other physicist. this. It certainly looks like in terms of going up relatively linearly, although they're And then the thing that I think is driving the is going into the field. People are seeing that so I expect the price, the amount of money spent of 100 or something. And for that to then be the algorithms are getting better because Again, I'm not making a normative statement even saying this necessarily will happen because here which we're very actively working this is what the economy is going to do. how do you think about the contribution of this industry. There's an argument you can make stuff at Anthropic, another that says you're It's all costs and benefits. The costs are things is not to deny that there are any are and what the benefits are. I think we've that we didn't cause the big acceleration that this year. We weren't the ones who did that. Google, that might be ten times more important once the ecosystem had changed, then we did It's like any other question. You're trying and the biggest benefits and that causes you to One question I had for you while we were as a scientist yourself, what do you make of entire corpus of human knowledge memorized new connection that has led to a discovery? person had this much stuff memorized, they symptom. This other thing also causes this Shouldn't we be expecting that kind of stuff? One of the lessons I've learned is that in the big fuzzy and elusive and hard to track down. The models do display a kind of write a sonnet in the style of Cormac McCarthy and they do draw new connections of the I agree with you that there haven't been any "big" just the model skill level is not high enough yet. "I don't know, I play with these models. That is going to change with the scaling. well, the models have an advantage, which they have an advantage already, even Maybe that's kind of what you're getting at. certainly like memorization and facts and drawing And I do think maybe you need those connections Particularly in the area of biology, for better that the current models know a lot of things discoveries and draw connections. It's not like a formula. In biology you need to know a lot of of things and they have a skill level that's I think they are just on the cusp of On that point. Last week in your Senate testimony, away from potentially enabling large scale bio without obviously giving the kind of information shotting how to weaponize something or model? What would that actually look like? we did a blog post and the Senate testimony point or didn't understand what we'd done. things about biology and get them to say all things are things that you could Google, and I'm I think it's actually an impediment to seeing I asked this model to tell me some That is actually not what I'm worried about. who are the most expert in the world what would you need to conduct such an attack, They worked very intensively on just the entire shot, it's a long process. There are many steps this one page of information. And again, without Senate testimony is, there are some steps where are some steps that are what I'd call missing. or they're not in any textbook. and they're not explicit knowledge. They're and what if I get it wrong? Oh, if this happens, I needed to add more of this particular reagent. those key missing pieces, the models can't do and when they can, sometimes they still keeping us safe. But we saw enough signs of the at state of the art models and go backwards it shows every sign that two or three years Yeah, especially the thing you mentioned on it gets it right, to one in ten, to.. in my life. I was there when I watched when GPT-3 do regression a little bit above chance, when of helpful, honest, harmless. I've seen a lot I'm excited about, but I believe it's happening. on this post that OpenAI released about GPT-2 weights or the details here because we're something bad. And looking back on it now, it's anything bad. Are we just way too worried? It is interesting. It might be worth looking I don't remember it exactly but it's still we're choosing not to release the weights because this is an experiment. We're not sure if this is but we'd like to establish a norm of thinking of it a little like the Asilomar conference recombinant DNA. It was not necessarily the case recombinant DNA. It's just the possibilities at least, were the right attitude. people don't just judge the post, they judge that produces a lot of hype or that has so that had some effect on it. I guess you can't get across any message more complicated You can argue about those but I think the head of others who were involved in we actually don't know. We have pretty wide error to establish a norm of being careful. evidence now. We've seen enormously more of but there's still uncertainty. In all these might be there. There's a substantial risk of wouldn't say it's 100%. It could be 50-50. which in addition to bio risk is another thing avoided the cloud microarchitecture from leaking? less successful at this kind of security. don't know what's going on in there. A thing that innovations that make training more efficient. they're the equivalent of having more compute. multipliers because it could allow an adversary number of people who are aware of a given compute So there's a very small number of people who could of people who could leak one of them. But this is used in the intelligence community or resistance implemented these measures. I don't want to jinx to us but I think it would be harder for it to By the way I'd encourage all the other competitors architecture's leaking it's not good for anyone in the long run. Could you, with your current level actor from getting the Claude 2 weights? who used to work on security for Chrome, which he likes to think about it in terms of -- how much Again, I don't want to go into super detail and it's just inviting people. One of our goals it costs to just train your own model. It doesn't talent as well so you might still, but attacks up the very sparse resources that nation state We're not there yet by the way. But I think we to the size of company that we are. If you there's just no comparison. But could we resist our model weights? No. They would succeed. point the value keeps increasing and increasing. of a secret is how to train Claude 3 or Claude 2? spies. You just take a blueprint of the implosion tacit here like the thing you were talking about work or is it just like you got the blueprint, you There are some things that are like a one line more complicated. I think compartmentalization is people who know about something. If you're a 1000 one, I guarantee you have a leaker Okay, let's talk about alignment and let's which is the branch you guys specialize in. While explain what mechanistic interpretability is. what is alignment? Is it that you're locking in disabling deceptive circuits and procedures? What As with most things, when we actually train a inside the model. There are different ways of know what happens. All the current methods that the property that the underlying knowledge and disappear. The model is just taught not to flaw or if that's just the way things have mechanistically and I think that's the whole really understand what's going on inside the Eventually when it's solved, what does the if you're Claude 4, you do the mechanistic I'm satisfied, it's aligned. We don't know enough to know that yet. I can like as opposed to what the final result challenge here. We have all these methods succeed at doing so for today's tasks. had a more powerful model or if you would it be aligned? This problem would be much scan a model and say okay, I know this model is I think the closest thing we have to that is It's not anywhere near up to the task yet. almost like an extended training set and an all the alignment methods we're doing are but will it really work out a distribution? Mechanistic interpretability is the only thing more like an X-ray of the model than modification an intervention. Somehow we need to get into an extended training set, which and an extended test set which is kind of what worked and what didn't? In a way that goes where you're saying, what is the model is within its capabilities to do instead And of course we have to be careful about that. should never train for interpretability because the problem similar to validation versus test set, you can interfere. We should worry about it's not automated optimization. We should just we don't look at the validation set too many times manual pressure rather than automated pressure. between the training and test set where we're work via a way of testing them, that the model I think we're never going to have a guarantee, but Some way to put extended training for alignment ability together in a way that actually works. ways to do this where you fool yourself. intuition for why you think this is likely me ask the question in a more specific If you're an economist and you want to bunch of microeconomists out there. One of them them studies how the tourism business works, one And at the end, they all come together and to be a recession in five years or not. have an understanding of how induction we understand modular arithmetic. How does us? What does this model fundamentally want? questions to ask. I think what we're hoping for in but again, I would give the X-ray or the MRI look at the broad features of the model and say, are very different from what it externally where we're uncomfortable that far too much of its like fairly destructive and manipulative things? but at least some positive signs the model is not intentionally hiding from you, it from you. I can think of cases where if the model so that it affects its own cognition. We should I suspect that it may roughly work to think of just getting to above human level. It may be a the internal structure of the model is I'd give an analogy to humans. It's actually and predict above random chance whether they're a back about a neuroscientist who was studying discovered that he was a psychopath and then obvious. You're a complete asshole. You must be The basic idea that there can be these a good analogy for it, this is what we would be very goal oriented, and very dark their behavior might look like the behavior of A question somebody might have is, you're activations are suspicious but is this something a very good first principal theoretical reason the model correlate with being bad. We need just It depends what you mean by empirical. A better we should be purely phenomenological in like, models and here are some other brain scans. The look at the underlying principles and circuits. on one hand, I've actually always been a fan of detail that we possibly can. And the reason for Even if you're ultimately aiming for there's too At the end of the day, we're trying to build broad understanding. I think the way you build specific discoveries. You have to understand the how to use that to draw these broad conclusions You should probably talk to Chris Olah, who interpretability agenda. He's the one who This is my high level thinking about it, Does the bull case on Anthropic rely on the helpful for capabilities? I think in principle it's possible that with capabilities. We might, for various reasons, That wasn't something that I or any of founding. We thought of ourselves as people who safety on top of those models. We think that we good at that. My view has always been talent bullcase. Talent density beats talent mass. thing. Others are starting to do and I'm very glad that they are. A part to make other organizations more like us. another thing Anthropic has emphasized in order to do safety research. And of Somebody might guess that the current frontier million dollars or something like that... very broad terms is not wrong. the kinds of things you're talking about, we're keep up with that. If it's the case that what is a case in which Anthropic is competing It's a situation with a lot Maybe I'll just answer the questions one by one. Some people don't think it is. But if I just methods have been put into practice even if we don't think they'll work in general. something like debate and amplification. Back in it was like, human feedback isn't quite going to beyond that. But then if you actually look at the the quality of the model. For two models to have judge it so that the training process can actually beyond on some topics the current frontier. up with the idea without being on the frontier needs to be done. It's very easy to come up with oh, the problem is X, maybe a solution is Y. practice, even for the systems we have things go wrong with them. I just feel like that things are going to go wrong by trying these just not as widely understood as it should be. constitutional AI, and some people say, oh, it it won't work for pure alignment. I neither agree of overconfident. The way we discover new things work and what's not is by playing around with oh, this worked here, and so it'll work there. like with the scaling laws. which might be the one area I see where a lot models, we're seeing in the work that OpenAI powerful models to help you auto interpret the can do in interpretability, but that's a big So you see this phenomenon over and over again snakes that are coiled with each other, interpretability, three years ago, I didn't think but somehow it manages to be true. Why? Because of tasks. One of the tasks it's useful for other intelligence and maybe someday even Given all that's true, what does that imply these leviathans are doing Choice one is if we can't, or if it costs too do it and we won't work with the most advanced that are not quite as advanced. You can get some value is all that high or the learning can be The second option is you just find a way. You more positive than they appear because of a I could go into that later, but And the third phenomenon is that as things get to into some non trivial probability of very serious misuse, the biorisk stuff that I talked about. I worry about some of the alignment stuff happening that at all. That may lead to unilateral or to scale as fast as we could, which we support. I hope things go in that direction, and then we in the frontier and can't quite do the research as we want, or versus we're on the frontier net positive, but have a lot in both directions. problems as you mentioned but in the long scheme you think will be considered a bigger problem? I'm worried about both. If you have a model that if you were able to control that model, then following the wishes of some small subset could use it to take over the world on their that we should be worried about misuse as But some people who might be more doomery than the optimistic scenario there because you've at the bad guys. Now you just need to make sure Why do you think that you could get to bad guys? You haven't already solved this. is completely unsolvable, then you'd be like, about misuse. That's not my position at all. a plan that would actually succeed that actually succeeds, regardless of how need to solve misuse as well as misalignment. they're going to create a big problem around the to create a big problem around, is it possible that it's hard for everyone else to stop? Any needs to solve those problems as well. If your solve the first problem, so don't worry about statement. You should worry about problems two Yeah. In the scenario we succeed We should be planning for success not for failure. have the superhuman models, what does that look controlling the model five years from now? that I think it's going to involve substantial of government bodies. There are very naive hand the model over to the UN or whoever happens going poorly. But it's too powerful. There for managing this technology, which includes the role of democratically elected authorities, be affected by it. At the end of the day, there But what does that look like? If it's not the case at the time, what does the body look like? of time. People love to propose these broad plans honest fact is that we're figuring this out and experiment with them with less powerful this out in time. But also it's not really The long term benefit trust that with this body? Is that the body itself? is a much narrower thing. This is something basically a body. It was described in a recent this year. But it's basically a body that majority of the board seats of Anthropic. national security, and philanthropy in general. is handed to them, doesn't that imply that That doesn't imply that Anthropic or any other about AGI on behalf of humanity. I would think play a broad role, then you'd want to widen from around the world. Or maybe you construe broad committee somewhere that manages all the I don't know. I think my view is you shouldn't with a new problem here. We need to start bodies and structures that could deal with it. just talk about what this going well looks like. cure all the diseases, solve all the fraud - But now it's 2030. You've solved all the real next? What are we doing with a superhuman God? something like this. I get nervous when someone AI? We've learned a lot of things over the last person can define for themselves what the best that societies work out norms and what they value If you have these safety problems that certain amount of centralized control from the But as a matter of -- we've solved all the I think most people, most groups, let's sit down and think over what the definition But this vision you have of a sort market oriented system with AGI. Each person I don't know. I don't know what it looks like. the important safety problems and the important alignment, there could be a bunch of economic can't solve. Subject to that, we should think unitary visions for what it means to live a On the opposite end of things going well We might want to touch on China First of all, being at Baidu and seeing why do you think the Chinese have underperformed? Or is the premise wrong and I'm just not The scaling laws group, that was an so there were still some people there but that was That was my first foray into deep learning. China. It was like a US lab. That was somewhat Chinese entity to kind of get into the game. commercially focused and not as focused on these laws. I do think because of all the excitement that's been a starting gun for them as well. And I think the US is substantially ahead but How do you think China thinks about AGI? Are I don't really have a sense. One concern I China isn't going to develop an AI because they restrictions to make sure things are in line in the short term and for consumer products. My national security and power, that's going to as a source of national power, they're going and that could lead them in the direction of AGI. base or something, is it possible for them to the frontier with the leading American companies? this. This is one reason why we're focusing so cloud providers. We had this blog post out about for access to the model weights. We have other of putting in place that we haven't announced. but we're happy to talk about them broadly. is not sufficient yet for a super I think it will defend against most attacks and But there's a lot more we need to do, and some of Let's talk about what it would take at that got good security. We had to get badges and eventual version of this building or bunker or a building in the middle of San Francisco or are is a point in which you're Los Alamos-ing it? that the way building AGI would look like is, power plant next to a bunker, and that we'd all be local so it wouldn't get on the Internet. to happen seriously, which I can't be that something like that might happen, but What is the timescale on which you think alignment level in some things in two to three years, This is a really difficult question because I alignment in the wrong way. There's a general or there's like an alignment problem to solve. I don't quite think it's like that. Not in a as bad or just as unpredictable. there's a few things I think of -- One is, the will be powerful models. They will be agentic. wanted to wreak havoc and destroy humanity or If that's not true, at some point we will reach So that definitely seems to be the case. that we seem to be bad at controlling the models. statistical systems and you can ask them a and reply. And you might not have thought of a Or when you train them, you train them in this all the consequences of what they do in response Bing and Sydney. I don't know how they trained do all this weird stuff like threaten people and shows is that we can get something very different I actually think fact number one and fact You don't need all this detailed stuff about evolution. One and two for me are pretty powerful. It could destroy us. And all the ones doing some random shit we don't understand. with bioweapons or something that could does the research agenda you have of mechanistic RLHF stuff meaningfully contribute to People talk about doom by default or alignment by With the current models, you might get If we take our current understanding and move be in this world where you make something and Not really alignment by default, but just very careful about all those details and it right but we have a high susceptibility to, really understand was connected to something else. it wants to turn them into pumpkins, just some they're like these giants that are standing in around randomly, they could just break everything. I don't think we're aligned by default, and have some problem we need to solve. Now what I do think is that hopefully within a diagnosing when the models are good and when repertoire of methods to train the model that likely to do good things in a way that isn't just can help develop that with interpretability oh, man, we tried RLHF, it didn't work. We tried other thing, it didn't work. We tried mechanistic else. I think this frame of like, man, we solved the Riemann hypothesis isn't quite right. good at controlling them and the consequences more ways of increasing the likelihood that what's going on in them. And we have some But I don't think of this as binary. It works or I do think that over the next two to three years of ways things can go wrong. It's like in the mass of how hard the problem is. isn't really even quite right because I don't feel almost like right now if I try and juggle five I actually can, but I can't juggle five balls If I were to do that, I would almost certainly better at the task of controlling the balls. personal probability distribution? For the trivial to align these models with RLHF++. Two, it could solve. Three, something that is basically solve. If I'm capturing those three, What is I'm not super into questions like what's your those have enough likelihood that they should more interested in is, what could we learn What is the answer to that? mechanistic interpretability is going to it's going to tell us what's going on when we teach us about this. One way I could imagine if mechanistic interpretability sort of shows us being stamped out or that, you get rid of one inspire us or give us insight into why problems For me to really believe some of these stories in this particular direction. I think the don't find it really compelling either, nor do I But the kind of thing that would really be we see it happening inside the X-ray. I think there's way too much overconfidence about how probability mass on -- this all goes wrong, it's a way than anyone had anticipated it would. it could go different than anyone anticipated. be relevant? How much would the difficulty of models be? Is that a big piece of information? predicting that all the subhuman AI models are aligned. They're going to deceive us in some but I am more interested in what mechanistic you see this X ray, it would be too strong to systems, it doesn't feel like it's optimizing I don't think anything is a safe bet here, but that isn't actively optimizing against us. than mechanistic interpretability that you RLHF or Constitution AI, if you had to what is the change that is happening? Are we How is the model changing in terms of psychology? what's happening. It's not clear how useful I think we don't have the language to describe X-ray. I'd love to look inside and say and kind of basically making up words, which is what I We should just be honest. We really have very be great to say, well, what we actually mean and after we've trained the model, then this It's going to take a lot of Model organisms, which you hinted at before when they're capable of doing dangerous things now lab leak scenario? Where in fine tuning it or in behaviors, make bioweapons or something, you instead of telling you it can make the bioweapons. passive models. If we were to fine tune a with the experts and so the leak would be like, For now, it's mostly a security issue. we do have to worry that if we make a truly makes it dangerous or safe, then there could risk that the model takes over. The main the capabilities of the model that we test are At what point would the capabilities be so high Well, there's different things. But that itself could lead to... If you're Sure. But I think what you want to do is you want You have factors of two of compute, where you're account on AWS and make some money for itself? to complete survival in the wild. Just set as you proceed upward from there, do kind of more careful about what it is you're doing. constitution for the next generation of is? How is that actually written? we just took some stuff that was broadly agreed some of the stuff from Apple's Terms of Service. say or what basic things are able to be included. more participatory processes for making these. be one constitution for a model that everyone simple. It should only have very basic facts that lot of ways that you can customize, including we're developing new methods. I'm not imagining we'll use to train superhuman AI. Many of the and so it could look very different. uncomfortable with: here's the AI's constitution, lessons from how societies work and how Even after we've mitigated the safety issues, security issues that we need to solve, it more decentralized and less like a godlike What scientists from the Manhattan they acted most ethically under the constraints I don't know. There's a lot of answers you could kind of figured it out. He was then against the know the history well enough to have an opinion have ended the war. I mean that involves a bunch and that I'm not an expert on. But Szilard, he patented some of it and put it in the hands of the kind of awareness as well as discovering stuff. It big blob of compute doc and I only showed it to a to almost no one. I was a bit inspired by this. Like we don't know if it's actually going to be could all be just Silicon Valley people building I don't know how it's going to turn out. it's bigger than the Manhattan Project. we should always maintain this attitude If you're a physicist during World War II and non replaceable research to the Manhattan Given you're in a war with the Nazis, I don't possible. You have to figure it's going to Regarding cybersecurity, what should whole bunch of tech companies which have and it's not obvious that they've been hacked As far as I know my Gmail hasn't been leaked. quo tech company security practices simply that nobody has tried hard enough? tech company practices and of course there where things are stolen and then silently used. I cares basically cares about attacking Recently we saw that some fairly high officials of via Microsoft. Microsoft was providing the email that was of great interest to foreign adversaries. consistent with, when something is really high And my worry is that of course with AGI we'll get high. It'll be like stealing nuclear missiles or At every place that I've worked, I've pushed for about cybersecurity is, it's not something you can you can get companies into a dynamic and to compete to do the best safety research and or something. We used to do this all the time other orgs started recognizing the defect whether or not that was a priority to them before. because a bunch of the stuff you have to do but mostly you just see the results. A good norm companies or leaks the model parameters that's bad. If I'm a safety person, Of course, as soon as I say that, we'll that's part of the game here, that's I want to go back to the thing we're talking cybersecurity required two to three years from actually expecting to be in a physical bunker in That's a metaphor. We're still figuring it out. the data center, which may not be in the same hard to make sure it's in the United States. But If someone was really determined, some of the the data center and just trying to steal the center to us. These data centers are going to have things are scaling up, we're anyway heading to a much as aircraft carriers. They're already going being unusual in terms of their ability to link they're also going to have to be very secure. difficulty of procuring the power and the What has the process been like to secure the That's something I can't go into great detail industrial scale data centers and people are not to go to very soon. Whenever you do something every single component, every single thing has to you may run into problems with surprisingly simple And is this something that Anthropic has For data centers, we work with What should we make about the fact that these corpus of internet data in order to be subhuman? was like 10^25 Flops or something, you can but there's reports that the human brain, from is 20 years old, is on the order of 10^14 We don't have to go into the particulars on how sample inefficient these models seem to be? you could phrase it is that the models are maybe human brain. If you compare it to the number of on three to four more orders of magnitude of data. they're developing to age 18, I don't remember millions, whereas for the models, we're talking So what explains this? There are these offsetting lot more data. They're still below human level. the analogy to the brain is not quite right or This is just like in physics, where we can't one of the other 19th century physics paradoxes. see so little data, and they still do fine. our other modalities. How do we get 10^14 bits and maybe a lot of what's going on inside the all these simulated images or something like that. that's a weird thing that doesn't match up. And analogies. I thought in terms of them, like, five these models in front of us as artifacts, it has been screened off by what we've seen. And than the human brain and yet can do a lot of the require a lot more data. Maybe we'll discover or maybe we'll understand why the discrepancy think it matters, right? If we keep scaling the point is just measuring the abilities of the and they don't seem terribly far to me. of compute more generally, underemphasize the you composed the big blob of compute, you're the scaling on that would not have Are you underemphasizing the role Transformer could be having here, when This big blob of compute document, which I for historical reasons. I don't think it would when I wrote it, I actually said, look, there are factors but just let me give some sense of the could be nine, there could be five. But the things of the model matters. Compute matters. Quantity function matters. Are you doing RL? Are you doing rich or doesn't incentivize the right thing, you which I think are the core of the hypothesis. symmetries, which is basically if your the right kinds of symmetries, it doesn't work or neural networks take into account translational But a weakness of LSTMs is that they can't attend structural weakness. If a model isn't structurally happened in a far enough distant past, then it's flow. The blob has to be unencumbered. It's not off. And I think RNNs and LSTMs artificially to the distant past. Again, things need to And then I added a couple things. One of them optimizing with is just really numerically this is why atom works better than normal STD. but it was similar to things like this, where fail or that doesn't allow the compute to work. Transformers were kind of within that even had been published, it was around the same been just before. It might have been just after. about these algorithmic progresses is not as but simply getting rid of the artificial That's a little how I think about it. If you go compute wants to be free and it's being blocked it's being blocked until you need to free it up. On that point, though, do you think that another the pike to enable the next great iteration? things like trying to model very long time ideas where I could see that we're missing an something. I think those inventions are possible. don't happen, we're already on this very, trying to discover them, as are others, but all that would do is speed up the trajectory because it's already going so fast. model at all important in terms of I'd think of that less in terms of of a loss function like the data, the to end up being very different. That could be data acquisition is hard and so things have gone continue to go through the language route even And then the other possibilities you We kind of already do RL with RLHF. Is this in terms of the two snakes, they're often hard to language models but I think we've used RL less things in the world but when you take actions consequences of those actions only later, then guess that in terms of models taking action in the power and all the safety issues that come with it. way in which these things will be integrated into with each other and criticizing each other and that one model one shots the answer or the work. have to be the case. We may want to limit that to problems easier but some of that will be required. they talking to humans? Again, this goes kind sociocultural economic realm where my heuristic is things. I feel like these scaling laws have been when is there going to be a commercial explosion be? Or are the models going to do things instead track record on predicting these things is really see anyone whose track record is great. but also the difficulties of integrating within Do you think there will be enough time to before the next model is just so much better It depends what you mean by large. I in the 100 million to billion per year trillion range before? That stuff is just so hard Right now there are companies that are throwing That's the right thing for them to do, and mean they're finding uses or the best uses from the same thing as economic value being created. perspective of Anthropic, where if then it should be an insane valuation, right? commercialization and more on safety, the graph I can only imagine what's happening at the It's certainly happening fast but it's while the technology itself is moving fast. is getting better and how fast it's integrated unstable and turbulent process. Both things are how it's going to play out, exactly what order I'm skeptical of the ability to predict. specifically, you're a public benefit you want to make sure that this is an important to care about is not shareholder value. putting in hundreds of millions, billions to put in this amount of money without the I think the LTBT (Long Term Benefit Trust) is more about the LTBT, but some version of that of Anthropic, even formally. Even as the body this body is going to exist and it's unusual. Anthropic looks at this. Some of them are just Some of them are like, oh my god, this body of that's totally contrary to shareholder value. but we have to have this conversation with every well, what are the kinds of things that interests of traditional investors. has helped get everyone on the same page. of the founders and the employees at Anthropic the scaling laws and how the power laws but what are the actual approaches and ways of over so well? Is that notion of effective Part of it is just that physicists learn things hire someone who is a Physics PhD or something, quickly in most cases. And because several of were physicists, we knew a lot of other them. And now there might be 30 or 40 of them an enormous amount of depth, and so they've Are you concerned that there's a lot of people they would've gone into finance they have now been recruited to go into AI. in the future they leave and they get funded to bringing more people into the ecosystem here? GPUs to exist. There's a lot of side effects just incur if you buy into the idea that you need A lot of them would have happened anyway. I so physicists were doing it. Now ML is a hot do it when they had no interest previously. But and a lot of that would have happened anyway. Some Do you think that Claude has conscious This is another of these questions that just I'll tell you is I used to think that we didn't were operating in rich environments, like to have a reward function and have a long lived but the more we've looked at these language to see things like induction heads, a lot of for active agents already seems present in the as I was before that we're missing enough today's models just probably aren't smart enough not 100% sure about this, and I do think in a What would change if you found out that they pushing the negative gradient to suffering? I suspect will not end up having a well defined.. that I should care about Claude's experience as or something. I would be kind of worried. or negative. Unsettlingly I also don't know I was more likely to make Claude have a positive If there's an area that is helpful with this, it's of it as neuroscience for models. It's possible it's not a straightforward factual question. We talked about this initially, but I want to now that you're seeing these capabilities ramp human spectrum is wider than we thought but more intelligence different. The way you're seeing that change your picture of what intelligence is? is came with the blob of compute thing. There might be all this complexity. Rich Sutton called been called the scaling hypothesis. The first You could go further back. I think Shane Legg maybe Ray Kurzweil, although in a very vague it went up a lot around 2014 to 2017. How did intelligence evolve? If you don't if you can create it just from the right kind it's not so mysterious how it all happened. In terms of watching what the models can do, how wish I had something more intelligent to say is I thought things might click into place a cognitive abilities might all be connected and the model just learns various things at different quite prove the prime number theorem yet. And although it's weird the juxtaposition of things it theories of intelligence or how intelligence a continuum. They just kind of dematerialize. in terms of what we see in front of us. One is how discrete these different paths loss are rather than just being one reasoning other surprising and interesting thing is, many you'll wonder why it wasn't obvious to you? why were you not completely convinced at the time? companies. You're not posting on Twitter, you're What gives? Why are you off the radar? If people think of me as boring and low I've just seen cases with a number of your incentives very strongly to the approval and in some cases, it can destroy your soul. profile because I want to defend my ability to different from other people and isn't tinged by of folks who are deep learning skeptics, and they And then even as it starts to become clear to their thing on Twitter, and they can't change I don't really like the trend of personalizing approach. I think it distracts people from the question. I want people to think in terms of incentives more than they think in terms of me. friendly faces can be misleading. this will be a misleading interview Indeed. came on the podcast and hope people enjoyed it.