Tara Zahra (00:00:00): especially the entire Neubauer team. Rachel to bringing all of this together seamlessly. And I Risk Lab because I don't think we could have really grateful that we had this opportunity to I wanted to just briefly introduce the Neubauer we are a research incubator on campus and our regardless of what school or discipline or field academy, to think through questions, ideas, and alone. All of the research we support is humanistic question at its core, although we're (00:01:08): and visiting Fellows program, we're unusual integrate the arts with research. And this special hope to enliven conversations about research with why invite a scientist to give a lecture at research? And I believe this is the first time I'm not a hundred percent sure, but certainly in is not an ordinary scientist, and AI is not an the Problem of Control makes it very clear that harm will require a great deal of collaboration, the future of humanity. Humanists, social need to work together in order to decide what in realms as diverse as warfare and defense, (00:02:19): happening all over campus and beyond, but often in here will be or has been already an opportunity that he poses so provocatively in his title, What computer science at the University of California engineering and is director of the Center for for Ethics Science and the Public, which I learned and humanists together to talk about important Modern Approach with Peter Norvig is the standard countries. And his research covers a wide range of emphasis on the long-term future of artificial (00:03:22): global seismic monitoring system for the Nuclear lethal autonomous weapons. And as I mentioned AI and the Problem of Control, which was me today was recently re-released with a fortunate to have our own Rebecca Willett, Russell after his lecture. Rebecca Willett is a the faculty director of AI at the Data Science appointment at the Toyota Technological Institute learning and signal processing reflects perspectives. She's known internationally foundations of machine learning, large-scale (00:04:24): focuses on developing the mathematical learning and scientific machine learning advisory boards for the United States just a quick word about the format. Professor minutes that will be followed by a conversation we will have time for a Q&amp;A with the audience, I hope you'll all join us. But for now, please Stuart Russell (00:05:17): to be here in Chicago and I'm looking forward to fast as possible. Okay, so to dive right in, what history of the field, after I would say a fairly science, the emulation of human cognition, we explicitly from economics and philosophy under intelligent to the extent that their actions can objectives might be specific destinations in your is figuring out how to get to that destination as winning that we give to a reinforcement learning (00:06:30): actually is very powerful. It's led to many of 75 years. It's the same model actually that is in economics, in statistics where we specify those objectives that achieves them as well as this is a huge mistake, but for the time being, and most of the technology we've developed has and statistics and operations research, AI has create general purpose intelligence systems. So, intelligence. You'll hear that a lot, and it's as matching or exceeding human capacities to (00:07:47): So, the title of the talk is What If We Succeed? time. The first edition of the textbook came out it refers back to some of the more catastrophic you can tell reading this that I'm not that worried. And it's important naturally to think but what's the point, right? Besides that out how intelligence works and could we make it that. And if you think about it right with general be able to do whatever human beings have managed (00:08:54): things we've learned how to do is to create a the Earth's population, but it's expensive to build these buildings and teach these courses and far less cost. And so you might, for example, standard of living for everyone on earth, and And if you calculate if have some economists here, of an income stream, and it's about 15 quadrillion (00:09:46): why we're doing this. It gives you some sense between a hundred and 200 billion a year into all other forms of scientific research, at least investment and it's going to get bigger as we get more and more and will unlock even greater levels a better civilization not just replicating the healthcare, much better education, faster advances (00:10:49): if we do that, we end up with the Wally world to do and we lose the incentive to even learn sorry. And so the human race becomes infantilized that we want. Economists are finally, I would prospect. If AGI is created, they're now plotting zero and things like that. So, having denied for decades, they now realize that it will be (00:11:49): we succeeded? And five years ago, I don't think because it's obvious that we didn't have Peter Norvig, the co-author on the textbook saying that we have succeeded, that the sense that the Wright Brothers airplane was faster and more comfortable, and now they the same basic technology just sort of spiffed (00:12:34): and I'll talk a little bit about why I think we is something going on with ChatGPT and all of unexpectedly capable behavior, but it's very what scientific advance has occurred. We just happened 5,000 years ago when someone accidentally weeks and then drank it and got really drunk and obviously cool and they kept doing it. This is (00:13:35): but we don't know what shape it has or where we need. So, until we do, then we haven't can kind of tell because when it doesn't work, only remedy when things don't work is well, maybe it will work or maybe it won't. We many reasons to think that actually we don't (00:14:12): give one example, and since Dan is right here, here are some black holes on the other side of producing gravitational waves that deliver ... for the first detected event was the amount times the output of all the visible stars in the event. And then those waves arrived at the LIGO, Observatory, sorry. And so this is, the arms physics stuff that relies on centuries ingenuity and obviously incredibly complex on. And it's sensitive enough to measure (00:15:27): give you a sense, if Alpha Centauri moved further then this detector would notice the difference. reassured. So, absolutely unbelievable. And this and the theory of general relativity was able to we could even infer the masses of the black (00:16:08): sun rotating around each other 300 times a second, But anyway, and then they collide, and that's sort a large language model or any relative of that before they started, there weren't any training it is just there isn't really even a place to kind of thing. And I think there's a lot of work (00:17:00): that deep learning solves everything. So, here I'm what the issue is. So, the transformer is, think mental picture, think of it as a chain link fence numbers come in one end, they pass through this the other end. So, the amount of computation just proportional to the size of the chain link sit there and think about something for a while. comes out the other end, and this type of meaning the amount of time it takes proportional feed forward circuit cannot concisely express (00:18:19): know that for a very large class of concepts, to represent those concepts accurately. And so if is fundamentally actually a simple concept, then learn that concept. Far more than you would need the concept. So, this is, I think, indicative, might be going on with these systems. They meaning they need many, many, many examples to if you have children, you have a picture book, picture? Book G for giraffe, one picture, having in any photograph or context, whether it's a of their lives. And you can't buy picture (00:19:36): a huge difference in the learning abilities of what about the superhuman Go programs? Surely for AI. So, back in 2016, AlphaGo defeated Lee the number one Go player, and more importantly, government that actually AI was surreal and they to be the tool of geopolitical supremacy. So, ahead of human beings. So, the human champion KataGo, this is a particular version of KataGo, (00:20:41): in our extended research group, Kellin Pelrine, rating's about 2,300, so miles below professional champion and so on. And Kellin in this game is you don't play Go, it doesn't really matter. It's on the board, you try to surround territory and if you surround your opponent's stones completely, (00:21:19): the computer, to start with nine stones on the I think they would have to commit suicide and a five-year-old and you're teaching them to at least stay in the game for a while. Okay? So, what's happening in the bottom right quadrant of is playing black. And so Kellin starts to make that black quickly surrounds that group to stop to surround the black stones. So, we're making a seem to understand that its stones are being them and takes none of those opportunities (00:22:28): end of the game. And Kellin won 15 games in a row which are developed by different research groups this weakness that there are certain kinds particularly problematic, that the system just probably because it has not learned the concept of the concept of a group as a circuit is extremely it's like two lines of Python to I think often we are overestimating the abilities I guess the implication of this is I personally (00:23:36): for example, who was one of the major developers up his affairs. He believes that we maybe, I few other people think it's pretty certain that I think we probably have more time. I think but at the rate of investment that's occurring people who are working on it, I think we have (00:24:22): who's the founder of computer science, gave the same question, what if we succeed? And he thinking method had started, it would not take therefore, we should have to expect the machines mitigation, no solution, no apology, no nothing. step back in his reasoning, I think he's kind make systems that are more intelligent than over the world, over all the other species. We're long teeth, but we have intelligence, and problem solve. So, if we make AI systems that more powerful in going back to that standard (00:25:43): power over entities more powerful than ourselves this question and immediately saying, we can't. way, and it again comes back to the standard what is the mathematical problem that we set up AI problem such that no matter how well the AI system result? This is a much more optimistic form of powerful, but their power is directed in such a (00:26:45): the standard model optimizing a fixed objective, King Midas and many other cultures have similar specifying those objectives correctly. And if found out to his cost when all of his food and basically setting up a direct conflict with the effectively than you can pursue objectives. So, example in a second. The second approach, which is with imitate human behavior, is actually (00:27:45): this problem. We call it misalignment, where the be aligned with what we really want. And so the to have been a disaster. If you talk to people blew it. We should have regulated, we should have we let the companies nearly destroy our the recommender systems as they're called, decide So, they have more control over human cognitive (00:28:35): set up to maximize a fixed objective. And let's on the items that they recommend for you. how long you spend engaging with the platform and with clicks for the time being. And you might've algorithms will have to learn what people want and very quickly learned that that's actually not what discovered was the effectiveness of clickbait, if you don't turn out to actually value the where the systems would send you stuff they very your area of engagement gets narrower and narrower The solution to maximizing clickthrough is (00:29:54): standard property of learning algorithms. They in such a way as to maximize the long-term people a sequence of content and observe their so that over time the person is modified be easier to predict what they will consume. one might imagine that a way to make people more have quantitative data on that in YouTube, of violence that people are comfortable with. If get into ultimate fighting and then they'll get (00:31:10): are really simple. If they were better algorithms, they're sending, if they could understand that would learn about the psychology of humans, they'd the outcomes would be worse. And this is actually better on a misaligned objective produces worse uses the variables that you forgot to include in order to squeeze some more juice out of (00:32:03): there's going to be a fixed objective that the the objective to be wrong, to be misaligned, then is correct, that it's gospel truth. And so this of that and replace it with a slightly different beneficial machines, and those are machines objectives. And so here, the objectives are in and that makes the problem more difficult, but it set this up as a mathematical problem. And here's (00:33:00): in the best interest of humans, but it starts out are, and the technical formulation is done in game. And so there's the human in the game with the machine's payoff is the same as that of the but it starts out not knowing what it is. And ever had to buy a birthday present for your with the present, but you don't know how happy (00:34:01): in, and it will probably do some of the same kinds how do you feel about Ibiza? Maybe you leave if they notice them and comment on how nice that what your wife wants and so on. There's all kinds this directly when we solve the assistance games. if the human says, stop doing that, that updates are and then makes the machine not want to do if there are parts of the world where it doesn't if I'm trying to fix the climate, but I don't know is it okay if I turn the oceans into sulfuric (00:35:27): it will ask permission. It will behave cautiously in the extreme case, if you want to switch it wants to not do whatever it is that is causing which thing that it's doing is making you therefore it wants to be switched off. And this from the uncertainty about human preferences. the machine no longer wants to be switched do we control these systems to the ability to principle of uncertainty about human preferences (00:36:26): mathematical problem is far from the final step in and many of these issues connect up directly to of them have been discussed for literally need to be solved or we need to work around the here are just some of them that we have to think not just sort of abstract mathematical models, but future to be like is a very complicated thing. properties like health and freedom and shelter commonality against humans, which is good, sorry, by observing a relatively small sampling of a pretty good idea about the preferences of a lot (00:37:56): the human race has ever done is evidence about you can see some of the earliest examples of really boring. It's like, I'm Bob and I'm trading ingots of copper. So, it tells you two things. One then and getting paid, and also something about and so on. So, even there, there's information to (00:38:41): plasticity and manipulability as maybe the most has an obvious problem. If human preferences AI systems can satisfy human preferences is by we probably don't want that to happen, but human preferences as inviolate and untouchable preferences. And certainly the experience of robot that does everything and makes the house and this is certainly going to change our personality, the more serious problem here is that (00:39:58): and autonomously decide to have the preferences prefer for the universe. It's the result of my the society I was brought up in, my peers, my world, and this is something that was pointed deliberately to have the preferences that And so for example, he points out that in some second class status as human beings. So, do we (00:40:53): who's going to decide which ones are okay to replaced or modified, and who's going to very quickly into a very muddy quagmire of moral where AI researchers do not want to tread. So, do with the fact that human behavior is only underlying preferences about the future because world champion Go player will occasionally someone has to make a losing move, but it doesn't they weren't sufficiently far-sighted to make cognition to get at underlying preferences (00:42:16): this somewhat dry phrase theory for multi-human at least 2,500 years of the aggregation of decisions that affect more than one person, decision that is the best possible to decision? but there are many others. There are there are constraints based on rights. People do the best you can for everyone, but (00:43:13): And so it's quite complicated to figure this philosophical problems. One of the main ones is if I look at any individual and imagine trying a person is about a given outcome, well imagine measure it in Fahrenheit and you get different how do we know that they're really experiencing I would say their scales differ by a factor of affect of that is 10 times bigger for some (00:44:10): have argued, Kenneth Arrow among others, that that it is not legitimate to say that Jeff for his private jet is better or worse than over several months. That these are just utilities or rewards or preferences or anything. believe Ken Arrow believed that even when he (00:44:58): how do we think about that? When China decided on people who would be alive today. Was that okay? half the people in the universe, his theory was, much real estate, they'll be more than twice as a favor. So, these questions are really hard, but like Thanos. They'll have a lot of power. Okay, all too technical? Okay, so just moving towards models, the systems that everyone is excited said, they're trained to imitate human linguistic (00:46:10): by speaking, generating text, and we are training behavior, our writing and speaking, is partly always have goals, even if it's just I want to I want you to buy this product. I want you to me. These are all perfectly reasonable human goals through speech acts as the philosophers call going to build good imitators of human language you're going to replicate the data generating architecture. So, it might not be replicating all (00:47:19): but there's good reason to believe that that drive its behavior. And I asked this of victory tour after GPT-4 was published, I goals are that the systems are acquiring?&quot; And remember the paper they published was called they're claiming that they have created something might have or be pursuing. I mean, what could fortunately we found out fairly soon actually, was very fond of a particular journalist called 30 pages trying to convince him to leave his wife Kevin Roose, R-O-O-S-E. Just look it up (00:48:46): want AI systems to pursue human goals on their reducing poverty or helping with climate change, or getting elected president or being very rich, but that's what we're creating. It is just a bug. but that's how we create these systems. So, I'll an approach based on assistance games based on doesn't know what humans want, but wants nothing (00:49:50): is now becoming popular, I think partly because really casting around for approaches that they can approach is to build what we call a formal oracle. yes or no when you give it questions. And so a by a mathematically sound reasoning process. So, operates using logically sound axioms. So, you it's good enough, it'll tell you that Fermat's (00:50:47): an incredibly useful tool for civilization, but you correct answers. And the job of the AI here is kinds of theorem proving steps to pursue to get to that's one way we could do it. We convince AI to operate these formal oracles. Another whole mathematical property of computational problems that the answer or a problem is correct than (00:51:43): forms of this. I won't go through all of them, but which is a company that was spun off from OpenAI where there's a second large-language model first large-language model and say, does this part of the prompt is a page and a bit of And so they're just hoping that it's easier unpleasantnesses or errors made by the first one to generate answers in the first place. And idea. And then there are sort of more, I would red-teaming is where you have another firm and see if it does bad things and then (00:52:59): because it's very easy to say, yep, we hired this University of Chicago who spent a week and they So, it must be good. And so this is pretty popular but it provides no guarantees whatsoever. And in large language models to pass these evaluation and capabilities of the model are still as evil them. And in fact, now we've seen cases where (00:53:56): idea that they can be tested. So, there you go. be able to make high-confidence statements we cannot do that. The view that you'll see AI systems and then we figure out how to make they train this giant wad of circuit, this fence from tens of trillions of words of text they haven't the faintest idea how it works, (00:54:49): space and then trying to apply post-hoc methods very reassuring to me. And I imagine not to most this. And in fact, here's what Sam Altman said. then we're going to figure out how to make it it's for.&quot; And I submit that this is backwards. by design, that because of the way we're we're going to be safe. In principle, we don't they're safe from the way they've been designed. want to go through that, but I want to talk about (00:55:48): is gaining some currency. You would like to say before you can sell them, but that's a The boundary between safe and unsafe is we're it's very fuzzy and complicated. But if we will get to safe later, let's just not do these and nobody in their right mind would accept that onus of proof is on the developer, not on the to prove that their system will not cross those the developer to put in a detector and an off then it is immediately detected and we (00:56:54): lines to be well-defined, to be automatically because you can be sure that as soon as you start they will say, oh, this is really, really This is going to set the industry back years that excuse from someone who wanted to operate to make it safe. What do you mean you want the government would say tough. Ditto with expensive, they take such a long time. Can we (00:57:41): self-replication, no breaking into other computer things. It's actually not particularly important such a list, you are requiring the companies to do predict, and control their own products. I think this is a really difficult time I but also with biology. And with neuroscience, that can have enormous, potentially positive (00:58:40): as Einstein pointed out, we just don't scientific advances and we need to remain in although there are some who actually think and machines are the only thing left. I'm not in forever, we have to have AI systems that are mathematical guarantees. And this is difficult, Rebecca Willett (00:59:47): Well, Stuart, thank you so much for just a perspectives and you covered a lot of really I'd like to talk about OpenAI and ChatGPT, as you they originally in their terms of service or that has a high risk of physical harm, including this January, those restrictions were I feel like that's an element of AI safety that disconnected from notions of formal proofs outputs of a tool like ChatGPT. So, how do you Stuart Russell (01:00:53): is one of a number of misuses of AI systems. filtering resumes in biased ways is another. So, but I think killing people would be at the top and the GDPR before it bans the use of algorithms or similarly significant effect. You would think a person. But the European AI Act and GDPR have actually I think the issue has been debated community should have been much more aware the majority of funding in the computer Automated Target Recognition program, the ATR for other than to enable autonomous (01:02:35): Rapporteur on extrajudicial killings and torture, saying that autonomous weapons are coming and we particularly that they might accidentally distinguish between civilians and combatants. So, started in Geneva in 2014. And in Human Rights we're starting this big campaign. Remember for the last 30 years? Well, soldiers to worry about. They're really bad. that's a bit of a challenge to the AI community. between civilians and combatants than (01:03:51): quite skeptical of this campaign, the more I realized that actually that's not the autonomous weapons is the ability for one or a hundred million weapons simultaneously to be managed with one human pilot for each piloted predator drones and so on. In fact, you but with fully autonomous weapons, you can have the effect of a whole barrage of 50 megaton much easier to proliferate because these will You can make a lethal device about that big, a (01:05:05): extremely cheap, scalable, easily proliferated that? But that's the path we're on, and it's across. I gave a lot of PowerPoint presentations but eventually we made a movie called impact illustrating in a fictional context what are widely available. But it's still the case that agree that there should not be a legally binding Rebecca Willett (01:06:04): alluding to suggested the need for regulation to and I think that ties into some of your concerns quad choppers with explosive devices, we can of AI as a type of weapon or as a source of to detect. And so what do you see as the kind of technological pathway towards building think the kinds of ideas that you mapped out as for self-driving cars that we can trust as more challenging for me at least to wrap my head are taking these tools and not employing the Stuart Russell (01:07:12): difference between regulation and enforcement but we have regulations against theft, but we steps to make those kinds of nefarious activities mostly I think when you look at rates of violent things have improved. And so we should definitely labeling genuine content. You can have rules about back a piece of text to the person who generated authenticating humans to access social media, for currently doing, giving social media accounts to cards and all the rest, I think is quite dangerous (01:08:42): it's up to the user to figure out whether the and oh yeah, you can download a tool and run blah, no chance. That's completely unreasonable to themselves against this onslaught. So, I think content and give you a filter that says, I don't be absolutely clearly distinguishable. There's so that I just get used to this idea that I'm legend in the bottom right corner. It's just (01:09:40): But when you're talking about existential risks Dr. Evil doesn't want to build beneficial AI. How have of policing malware is so unbelievably to be possible, because software is created of light and replicated infinitely often. if I want to independently develop it's going to cost me easily a hundred billion trained engineers to do it. So, I think It's probably more difficult to do that than Rebecca Willett (01:10:59): Stuart Russell (01:11:01): is that the hardware itself is the police. technologies that make it fairly straightforward safety of each software object before it or it's missing, whatever, the hardware will Rebecca Willett (01:11:31): challenging. I mean, let's say I had an army of say one way or another that Stuart Russell ChatGPT is going to decide it's a fact that do we think about a hardware system that's is correct or not? I mean, just arbitrarily with an arbitrer of truth in general, I feel Stuart Russell (01:12:10): you. And I'm not proposing an arbitrer of let's take the formal oracle. So, if we accept and so far this is the only authorized way that the thing that you are wanting to run on this template. And if it's another type of system, it Rebecca Willett (01:12:55): Stuart Russell (01:12:57): system has to comply with will depend on Rebecca Willett (01:13:05): lot of sense. Now I know the audience, but I want to make sure that the audience ask one final question here. Hypothetically, to get an AI-enabled smart toilet. Stuart Russell (01:13:27): A husband who wanted to get a Rebecca Willett (01:13:34): Stuart Russell (01:13:37): Japanese companies I believe, who so this is the household Rebecca Willett (01:13:47): Stuart Russell (01:13:48): I guess the issue is one of privacy in concerned about the toilet sending data Rebecca Willett (01:14:09): Stuart Russell (01:14:15): but this I think is symptomatic of a much in my view, has been completely delinquent. We guarantees of security, of privacy. For that interacts with you is oblivious, it has no memory that the interaction ever happen is that that system offers that proof accept to have an interaction with this system. should be working for you and the whole ecosystem (01:15:17): way because we don't teach people how to do that students graduate from Berkeley, and we are the 80% of our students graduate without ever having program. Now in Europe it's totally different. But Europe doesn't produce any software. So, we correctness, their idea is drink as much coffee And I think our society is getting to the point Rebecca Willett (01:16:06): you. That's a much better answer to my question Tara Zahra (01:16:15): to both of you. We do have some some people with microphones running Speaker 4 (01:16:30): You mentioned that things can be taken at face Rebecca Willett (01:16:45): Speaker 4 (01:16:46): Stuart Russell (01:16:50): about human preferences and the fact particularly if they are preferences that have people. So, think of people being brainwashed the founder of North Korea, was the the issue with that is that if you're not going value, what are you going to do? Who are you to really difficult problem, and I think we need help Tara Zahra (01:17:50): Speaker 5 (01:17:58): you so much. I've had the pleasure and challenge class with Professor Tharson over there, so it's works in the AI policy regulatory space, and so your lecture. And last class we actually talked a technology for people who don't know that can I was in shock when I saw some of the example as someone who's deeply concerned about the suggest the best methods are for regulators to what are your thoughts on watermarking also the potential for blockchain encryption to Stuart Russell (01:18:55): good questions and many organizations are of the problem. There's too many standards and but so I think governments maybe need to you stop bickering with each other and just get are very weak. So, I think Facebook standard is a screenshot of the image, then the watermark (01:19:40): issues of how you make a watermark that's really which is traceability through blockchain is tools watermark their output, but also you want their output as genuine. And then you squeezing as I said, either allow you to filter out or let's say from news or to very clearly (01:20:40): there are also some things we should ban. And the think any deepfakes that depict real individuals I think a priori that should be illegal except was originally in the European Union AI Act, and watered down until it's almost undetectable. of individuals are generally trampled on because it. And it's been hard to get representatives fortunately Taylor Swift was deepfaked (01:21:50): told me that she was deepfaked during it's been around for a while, but I think more and banned. Another thing that should be banned either of a particular human or even of a generic interacting with a human or a machine. And that is to be in US law, and in fact, I think it should Speaker 5 (01:22:32): Tara Zahra (01:22:34): Speaker 6 (01:22:46): formal verification course here at a student is to ask me what's the verified, I pretty much immediately say the team of European researchers in France. And that's the verification perspective, not so much from I took Michael Kern's class when I was a From a theoretical perspective it shows all probably or approximately correct learned. And yet (01:23:34): there's just this colossal gap between the about and the things that we actually have. And want these to be correct by construction, and prove things about it, which it because they already have those goals and How can we possibly bridge this gap in however Stuart Russell (01:24:16): So, some people are literally calling for a certain abilities that we just call a halt. I you can develop more powerful systems, but you with them. And honestly, the efforts made to are puny. Just to give you a comparison, so engineering did a study. And so for nuclear you have to show that the mean time to failure 10,000 years, it's now 10 million years. And (01:25:33): the days of paper how much paperwork was required function of the mass of the power plant itself. how much paper did you need? And the answer buildings with containment and lead and all kinds then compare that to the efforts, oh, they hired a doing red-teaming. It's pathetic. And so I think and say, just because the companies say they don't oh fine, go ahead without any safety measures, (01:26:52): But I think it's reasonable for the companies to do this? I think one way is not to build there are ways of getting high-confidence based on a how many data points it's been that you're training. But the numbers, if you're I mean a model with a trillion parameters as to the 500 data points to get any confidence that the goal of imitating humans isn't the right goal imitates humans who want power and wealth (01:27:59): to prove that it does those things. We actually my guess is that as we gradually ratchet up the is going to have to change towards being based decomposable. And just a simplest example would be of the axioms and test its correctness separately. logical reasoning correctly and then we are good. semantically rigorous inference approach, then have guarantees. Is there some hybrid of the total semantically rigorous components? And I think this And from what I hear, this is in fact likely to incorporating what we call good old-fashioned Tara Zahra (01:29:31): of other questions in the audience, but I hope that you'll all stay and join us for outside. But in the meantime, please join me