How many calculations do you think your while running video games with incredibly 100 million calculations a second is what's more power. Maybe 100 billion calculations a that could run Minecraft back in 2011. In order Cyberpunk 2077 you need a graphics card that can This is an unimaginably large number, so let's doing a long multiplication problem once every a similar type of calculation but with different power of this graphics card and its 36 trillion Earths filled with people, all working together It's rather mind boggling to think that a so in this video we'll see how graphics cards work card and explore the different components inside, of the GPU or graphics processing unit. Second, see how GPUs process mountains of data, and why Bitcoin mining, neural networks and AI. This video is sponsored by the graphics memory inside this graphics card. let's first understand the differences between the Graphics Processing Unit or GPU has over CPU or Central Processing Unit that's mounted to chip with only 24 cores. So, which one is more so you would think the GPU is more powerful, A useful analogy is to think of a GPU as a massive The amount of cargo capacity is the amount of and the speed of the ship or airplane is the and data are being processed. Essentially, of calculations that are executed at a that can be performed at a much faster rate. lot more flexible since they can carry passengers, at any one of tens of thousands of airports. a variety of programs and instructions. However, contents inside and are limited to traveling less flexible than CPUs and can only run simple GPUs can't run operating systems or interface isn't perfect, but it helps to answer the question if you want to perform a set of calculations faster at completing the task. However, if you quickly than a CPU will be faster. Furthermore, network connections and a wide range of different We're planning a separate video on CPU so you don't miss it, but let's now dive into this of this graphics card is the printed circuit mounted on it, [Animator Note: Highlight and list we'll start by exploring the brains which is open it up, we find a large chip or die named majority of the area of the chip is taken up by organization. Specifically, the chip is divided and within each processing cluster are 12 inside each of these streaming multiprocessors inside each warp are 32 Cuda or shading cores and CUDA cores, 336 Tensor Cores, and 84 Ray Tracing calculations of the GPU, and each has a different binary calculators with an addition button, a the most when running video games. Tensor cores and are used for geometric transformations ray tracing cores are the largest but the fewest Now that we understand the computational fact is that the 3080, 3090, 3080 ti, and 3090 ti for their GPU. This might be counterintuitive released in different years, but it's true. process sometimes patterning errors, dust cause damage and create defective areas of the because of a small defect, engineers find the deactivate the nearby circuitry. By having a GPU one core only damages that particular streaming other areas of the chip. As a result, these chips to the number of defects. The 3090ti graphics CUDA cores working properly, the 3090 has 10,496 has 8704 CUDA cores working, which is equivalent multiprocessors. Additionally, different graphics quantity and generation of graphics memory that bit. Because we've been focusing on the physical into one of these CUDA cores and see what it looks of approximately 410 thousand transistors. This operation of A times B plus C which is called common operation performed by graphics cards. floating-point numbers, which is essentially of the cores use either 32-bit integers or 32-bit core accommodate negative numbers and perform masking as well as collecting and queueing and then accumulating and outputting the results. calculator with a limited number of functions. operation each clock cycle and therefore with this gigahertz clock, we get 35.6 trillion calculations handles more complicated operations like division, these calculator operations are performed by as only 4 of them can be found in each streaming of what's inside a single core, let's zoom out chip. Around the edge we find 12 graphics memory interface. On the bottom is a 6-megabyte Level Engine which manages all the graphics processing Now that we've explored this GA102 GPU's physical the other parts inside the graphics card. On this plugged into, on the other side is the incoming PCIe pins that plug into the motherboard. On constitute the voltage regulator module which one point one volts and supplies hundreds this power heats up the GPU, most of the weight sink with 4 heat pipes that carry heat from the fans then help to remove the heat. Perhaps some of are the 24 gigabytes of graphics memory chips were manufactured by Micron which is the sponsor or wait for a loading screen, the time it takes of a particular scene or environment from the chips. As mentioned earlier, the GPU has a small Level 2 cache which can hold the equivalent of Therefore in order to render a video game, transferred between the graphics memory and the tens of trillions of calculations a second, continuously fed terabytes upon terabytes of data, kind of like multiple cranes loading a cargo ship transfer a combined 384 bits at a time, which can be transferred, or the bandwidth is about 1.15 that support the CPU only have a 64-bit bus width second. One rather interesting thing is that you ones and zeros. However, in order to increase data memory, GDDR7 send and receive data across the bus 0 and 1. For example, GDDR7 uses 3 different ternary digits or PAM-3 symbols with voltages of on how 3 binary bits are encoded into 2 ternary bit to 7 ternary digit encoding scheme resulting digits. The previous generation, GDDR6X, which different encoding scheme, called PAM-4, to send however, engineers and the graphics memory generations of graphics chips in order to reduce ratio, and improve power efficiency. Micron boundaries on how much data can be transferred chips. Another advancement by Micron is the that surrounds AI chips. HBM is built from or through silicon vias, to connect this stack of AI memory. For the latest generation of high can have up to 24 to 36 gigabytes of memory, thus the AI chip. Next time you buy an AI accelerator uses 30% less power than the competitive products. you're likely not in the market to buy one 40 thousand dollars and are on backorder for a memory, or Micron's next generation of graphics description. Alternatively, if designing the next is always looking for talented scientists and and you can find out more about working for Micron the physical components inside this graphics card architecture and see how applications like video "embarrassingly" parallel operations. Although parallel is actually a technical classification needed to divide the problem into parallel tasks, fall into this category. Essentially, GPUs solve called SIMD, which stands for single instruction are repeated across thousands to millions of SIMD or single instruction multiple data is used may know already, this cowboy hat on the table is built by connecting together around 14,000 These vertex coordinates are built using a origin of 0,0,0 being at the center of the hat. each with their own model space into the world able to tell where each object is relative to all the vertices from each separate model space space. So, as an example, how do we convert the space into world space? Well, we use a single of the hat in world space to the corresponding model space. Next we copy this instruction to and Z coordinates of the other thousands of we do the same for the table and the rest of each time using the same instructions but with and each objects' thousands of vertices in model of all the objects are converted to a common can now determine which objects are in front the power of SIMD or single instruction multiple 5,629 different objects with a total of 8.3 in 25 million addition calculations. The key to every one of these millions of calculations and thus all these calculations can be distributed in parallel with one another. It's important to to world space is just one of the first steps of pipeline and we have a separate video that delves we skipped over the transformations for the in these values is a similar process that requires simple understanding of SIMD, let's discuss how the physical architecture. Essentially, each thread is matched to a single CUDA core. Threads and the same sequence of instructions is issued to into thread blocks which are handled by the blocks are grouped into grids, which are computed managed or scheduled by the Gigathread Engine, available streaming multiprocessors. One important all 32 threads in a warp follow the same other, kind of like a phalanx of soldiers moving up until around 2016. However, newer GPUs follow threads. The difference between SIMD and SIMT is to each thread, with SIMT, the individual each other and can progress at different rates. program counter. Additionally, with SIMT all the shared 128 kilobyte L1 cache and thus data that's a separate thread. This improvement from SIMD to warp divergence via data-dependent conditional to reach the barrier synchronization. Essentially efficient especially when encountering branches you may think that the term warp is derived from specifically the Jacquard Loom. This loom from specific threads out of a set to weave together let's move on. The final topics we'll explore are But first we'd like to ask you to 'like' share it with a colleague, friend or on social The dream of Branch Education is to make free videos that dive deeply into a variety topics on and then to combine multiple videos into an school and college students. Taking a few seconds a ton! Additionally, we have a Patreon page if you find what we do useful, we would appreciate how single instruction multiple threads is used were initially used for mining bitcoin. We're not the blockchain and will save it for a separate the blockchain, the SHA-256 hashing algorithm is a time stamp, additional data, and a random number the SHA-256 hashing algorithm a random 256-bit algorithm as a lottery ticket generator where you input data, the SHA-256 algorithm generates if you change the nonce value and keep the rest a new random lottery ticket number. The winner of generated lottery number to have the first 80 bits matter and once a winning bitcoin lottery ticket resets with a new set of transactions and input GPUs ran thousands of iterations of the SHA-256 other data, but, with different nonce values. generate around 95 million SHA-256 hashes or 95 second, and hopefully one of those lottery numbers However, nowadays computers filled with ASICs or 250 trillion hashes a second or the equivalent cards look like a spoon when mining bitcoin next Let's next discuss the design of the tensor cores. generative AI, and neural networks, so we'll focus Essentially, tensor cores take three matrices and output the result. Let's look at one value of the of the first row of the first matrix multiplied second matrix, and then the corresponding Because all the values of the 3 input the tensor cores complete all of the matrix concurrently. Neural Networks and generative multiplication and addition operations and there are Ray Tracing Cores which we explored in That's pretty much it for graphics cards. Membership Sponsors for supporting our videos. you can find the links in the description below. animations that dive deeply into the technology video by clicking one of these cards or click