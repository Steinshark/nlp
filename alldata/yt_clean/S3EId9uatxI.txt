Building a real life NLP application means you have to perform various steps right from data acquisition, data cleaning, to all the way till model building deployment and monitoring. All these steps combined are called NLP pipeline. In this video, I am going to take a real life use case and explain you the NLP pipeline. Make sure you watch till the end. The example I am going to use is Camtasia support ticket system. So Camtasia is a video recording software that I use for my videos, and they have this support system where when people find issues with this software they report here, and they create a ticket. Ticketing system is a very common use case in different industry use cases, and one of the challenge with tickets is see so many people are creating these tickets. Now how do they know which ticket is very high priority? So you can manually tag that and manually look at all the tickets, and the urgent ones you can address initially, or you can have an NLP system which can analyze the text. And just by looking at the text, it can tell you if the ticket is high priority or not. So let's say if you can create a machine learning model which can look at the title and the description of the ticket, and it can tell you if it's high medium or a low priority. And if it's a high priority you can have you know a tool like Sentry, which can monitor the logs and create alarms, and it can send a call to customer support system or customer care center, where someone can immediately address that query. If the ticket is low priority, maybe you can create a JIRA ticket or whatever tool the engineering team in Camtasia is using to create the ticket so that it doesn't require immediate attention but it can be looked on at some point in time. So now this particular module on Camtasia's website, let's call it ticket management system. It is created by some team in Camtasia. Camtasia you know it's a software owned by TechSmith and TechSmith company will have some software team, which has created this ticket management system. And they might be storing all the support ticket data in some database. Let's say that database is MongoDB. It could be MySQL, or it could be no SQL database such as MongoDB, and these database will have all the ticket records. So I have shown 2 different tickets here in JSON format. This is a popular JSON format, where you can have different fields of the ticket, such as ticket description, creator. Here it is creator timestamp, you know ts means timestamp and you can have all these records in MongoDB. And also watch this green field severity. So let's say Camtasia has been running for 5,7 years and we have already 10, 000 tickets in our system. That's like historical data that we already have. Now someone would have tagged those tickets as high or low or medium priority. So we have let's assume we have those labels. So this severity high low means we have the training dataset that tells you if the description is something like this, then it's a high high priority ticket. Sometimes you might not have these tags and you might have to hire human annotators, to kind of go over the tickets and tag them as a high, low or medium priority. But for simplicity we are assuming we have all this information. Now what happens is in TechSmith company, there is an AI team, or NLP team, or data science team, you can call it anything. And let's say this AI team wants to build this particular model that I showed you here. And in order to build this model, they need all this training data. So they will go to ticket management software team and say, &quot;Hey! Can you give me access of your MongoDB?&quot; And ticket management system might be like, &quot;We don't want to give you a direct access of our database. Maybe we can upload all our data to some cloud location. Let's say some Amazon S3 bucket. S3 is like a secure object store on cloud. And we can push all the records there, and maybe from there you can consume that information and build your model.&quot; Sometimes people build data warehouse also okay, so that their mission critical database is not impacted. And this step is called data acquisition. So data acquisition is a first step in NLP pipeline that is performed by NLP team and the idea here is, to get necessary data required to solve a given NLP problem. Now here I gave a very simple example. Sometimes when you build NLP team, you don't have training dataset available. In that case, you can use some external database, some public dataset. There are so many public data sets available on the internet, or you can do uh, something called product intervention and in the product build necessary instrument instrumentation, so that it keeps on collecting the data. I have this practical NLP book. This tutorial is based on some of the content of this book, and this book has given a lot of examples on how one can collect the data. So maybe I can quickly go over some of those examples here, so that you get a good idea. So see some of the ways that you can collect data is, first is, use a public dataset such as, you know like Google has a search engine for datasets uh, or some of the public like U.S. consensus bureau website has a lot of economic data. You can also use scrapping, web scraping and scrape the internet to collect the data that you need. Basically you go to internet, write some Python script which can do web scraping. You can use product intervention, you can use data augmentation. There are a lot of techniques available in data augmentation, where let's say you have 10 samples. Out of 10 samples, you can create 100 samples okay? So that's step number 1, which is data acquisition. After let's say NLP team now got the data from S3 bucket, it's just a cloud location you know, no rocket science. Cloud location where you have all the records for existing tickets. Let's say in past Camtasia has created 5000 support tickets along with labels, which is high, low, medium severity. All those 5000 tickets data is available on this cloud location. Now I'm looking at one record for one such ticket, and what I can do in this particular record is this: So I can do some, I can just discard some irrelevant information. For example creator, creator timestamp it's not like this ticket is created by Vladimir Putin that's why I'm going to give it a high priority. No. So creator, creator timestamp not not needed you know. This information is useless. You discard that. Title and description is something you can merge because in the end, you look at the words you know, such as Camtasia is crashing, for example. That's a very high priority ticket, or the software fails to render the video. So these terms could be present either in title or description. So it doesn't matter you can just combine both of these, and create a one single text okay? And that's called discarded discarding irrelevant information, and kind of creating a simple version of your text data. The second step you can do is. spelling mistakes. When people enter ticket, support ticket they might make you know, fat fingering. So here, in this case instead of crash, crash they are saying cras. So you can do maybe spelling correction here. You can also remove extra \n \n is like the new line breaks right? So here you see there are 2 \n So maybe you can remove them. So spelling correction, removing those things, you know extra line breaks, all of these things combinedly are called Text Extraction &amp; Cleanup. So the data that you got, was very raw. It has a lot of extra information. You directed only useful information out of it, and then you did some cleaning on top of it okay? So these are the first 2 steps in NLP pipeline. Now you have this particular text. In order to build NLP model, you have to split this into separate sentences okay? So one way to do this is you know like dot dot or question mark, that's end of the statement. So you can split it into uh 3 different statements and this process is called sentence segmentation or sentence tokenization. Now you'll be like what's a big deal about sentence segmentation. All you do is look at the dots and question mark and you split the sentences. But look at this statement: Dr. strange likes samosas and ravioli so that his brain works fast! Now notice dot after doctor, and after etc there is a dot. So if you just use that dot rule you will end up creating 3 sentences. So you see splitting sentences is not just straightforward. You have to incorporate lot of rules of the grammar or a specific language, to build a good sentence splitter you know, or sentence tokenizer. And the libraries like NLTK and spaCy has this kind of ready-made tokenizer that you can use. And in the future video, we will write code and we'll look into it how those tokenizer works. And once you have created separate sentences, the next step would be to create separate words you know, and that process is called word tokenization, because once you create separate words then you can create a machine learning model and build your NLP application eventually. So that's word tokenization or sometimes people just call it tokenization. Now let's say I have from the big text blurb, I created sentences. From sentences, I created individual words. Now I need to do some further processing and you know, do you think it will make sense that I map this word eating, to its base word eat, or the word loves to its base word love because that way if someone is saying eating, eats, ate if you can map it to its base word, then it will help you in the uh model building process. And we'll see in the future, how exactly this is helping. And this step of removing for example from eating, I removed -ing So I can use some simple rules that if you have -ing remove it okay? That kind of rule I can use, and that process is called stemming. So stemming is basically using some fixed rules to try to come up with a base word. But stemming might not be enough because see here watch this word, ate. Ate I expected to map it to eat. But it's not doing that because stemming doesn't know grammar. It's just based on some simple rules. So then you can use a process called lemmatization. Lemmatization means you are mapping that word to uh its base word. So ate is a past tense of eat. So you are using lemmatization to come up with that word. So you see, you did two phase process here, stemming and lemmatization and that gives you the base words for each of your original word. And here is another diagram from the same book that I was referring to. The book link is in video description below. If you want to order it and read it, it has a lot of useful information. But this shows some examples of stemming and lemmatization. You see lemmatization if you have better, it will map it to good. Meeting it will map to meeting, of course there is no base word here. But stemming just uses simple rule. Like if you have able it will remove able. If you have formality, it will say formal. See sometimes it might mess up some words. Like airline, airlin. Airliner, airlin Doesn't make sense but it still works in practice. So this summarizes the whole process. You had a big text blurb, you created a sentence, you created words and then you use stemming and lemmatization to come up with the base word. This whole process is called preprocessing okay? So what we saw is data acquisition, text extraction and pre-processing. These are the first three steps in NLP pipeline so far. Now you got all these base words. What do you do? See you are trying to create again, what is our end goal? End goal is to create an a classifier that can take the text of support ticket and it can tell me, if it's a high priority ticket or not, or low priority, etc. Now I got all these words. Machine learning models do not understand text. They always ask for numbers. So what you can do is you can use some formula, some technique so that you can convert these words into numbers. Now what kind of numbers? Again we'll see that in the future. But some meaningful representation of these words, such that similar words you know like feeling better versus feeling good, these words will have similar number representation, then my model can work better. And and this process of converting words into number is called feature engineering. Feature engineering is you are extracting features from that word. If you have a word good and well then there are certain features you know. If you have let's say a cricket player name Dhoni versus whatever, Dhoni versus let's say Ricky Ponting or Shane Warne, he died recently unfortunately. These if you want to represent these words or person in in a vector, you can have different features such as okay is this a sports player: 1 or 0 Is this a human: 1 or 0 because if I have, you know if I have a bottle, bottle is not a human. So you can have this kind of features and this feature vector can represent that word in an appropriate way. And that technique of extracting features is called feature engineering. And there are various popular techniques. I'm just going to throw different jargons, don't worry! TF IDF Vectorizer, One Hot Encoding, Word embedding, we'll cover all of that in the future videos. But just for now, just assume there are some good ways of converting words into numbers so that these numbers present this word in a accurate way okay? So so far, we have seen these 4 steps. Once you have done feature engineering, your next step will be to build machine learning model. Not all NLP applications require machine learning model. But the one that we are talking about, support ticket classification, using machine learning might be useful here. So go to YouTube search codebasics machine learning you will find my tutorial playlist, and in this playlist there are various classification techniques, such as Decision Tree see, Support Vector Machine (SVM), Random Forest, all of these are various classification techniques. Naive Bayes, so it's it will be useful if you go through this ML playlist. It's gonna help you. All these classification techniques can be used for our problem. So let's say for our problem, we end up using Naive Bayes. So you had support ticket text, you somehow converted it into numbers. For now don't don't worry how we converted it into numbers. But machine learning model understands numbers, that's why we converted it into numbers. And then we use Naive Bayes classifier let's say, or SVM or any other classifier and we can do classification. So if you have 5000 support ticket data in the past along with the labels, now we can train this model with that data, and in the future when the new ticket comes up, when you feed it to this model, it will tell you whether it's a high priority or low priority. So that was model building. Now as I said you can use different techniques, SVM, Random Forest and within specific technique also, there are hyper parameters. There are different parameters like cogwheels that you can tune, and to figure out which model is the best model, you can use a technique called Grid SearchCV Again go to YouTube. I have a video on this, you can watch that. But these kind of techniques allow you to choose the best model for a given problem because model building is short of like trial and error. You have to try a lot of things and eventually something works you know, okay? Now when you are evaluating your model, you can use something called confusion matrix. So in confusion matrix on the x-axis you have truth. This is truth and this is prediction. So let's say I built a model. After that, I started predicting and when I do the prediction, I can then compare it with the truth, and I can figure out how good my model is. So let's say in this test, there were 10 tickets which was actually high priority and my model predicted it to be high. So anything on this diagonal is, I got the right prediction. So there were 8 tickets which were medium priority and my model predicted it to be medium. But look at this 2. So there was 2 2 tickets which was actually low priority, but my model said it's a high priority. So anything that is not in this diagonal, is your wrong prediction okay? So you can by looking at confusion metrics you can, you can figure out how confused your model is! It's like if your model is not confused and very accurate then you see all the numbers in the diagonal, and everywhere else you will see 0. In this particular case, 2 and 1 3, it made total 3 wrong prediction. And when you are building machine learning model, you can use different metric like accuracy, precision recall, F1 score to figure out if the model is good or not. If you want to know precision recall, F1 score, etc., let me suggest you a, so if you go to YouTube, and if you search for codebasics F1 score, you'll find a video. This video will give you a very good, simple explanation of precision recall and so on okay? So we build a model we evaluate using greaser cv, confusion matrix, and if things don't look good, we again we kind of go in a loop you know. This dotted line shows you might have to do revisions. You might have to do again pre-processing, feature engineering, model building, you keep on trying, iteration, until you find a perfect model for your problem. I mean there is no perfect model. But a model that performs better than others. Once a model is built, you can export it to a file and deploy it on the cloud. Azure, Google Cloud, anything AWS. You can write a FastAPI based or fast text service around not faster FastAPI or Flask service around that model. So there is a model and you can write a rest service you know the HTTP, the service that can request as serve to HTTP request. You can build that service, and you can deploy that service in the cloud. You can also deploy to an end-to-end ML platform such as databricks, Amazon Sage Maker, Azure Machine Learning, H2O.ai these are like end-to-end machine learning platforms that you can use to build and deploy these services. And once the service is deployed, of course you do you do need to monitor and update it periodically. When you deploy it in production, when it starts serving request, you have to set up some kind of monitoring system. So that you know because sometimes in the DEV, things work okay. But when you deploy to production when the model goes in wild, maybe it doesn't perform as good. So having a good monitoring and update system, make sure that you know the model is not doing wrong prediction and you're not losing on your business. And there is a dotted line here again means, if the model is not performing well, there are n number of reasons. Let's say there is a drift in the concept. We build a model on specific terminology. Now let's say Camtasia added a bunch of new features and the way people talk kind of change, you have to now get more training data and kind of repeat the whole process, right from data acquisition to model building okay? So understand that in the industry when you're working on NLP application, it's not like you build a model, deploy it and done. It's continuous iterative process, lot of grunt work that goes into building NLP system. This particular conversation gives you an idea on specific use case, which was you know Camtasia support system. But there are many other use cases, where all these more steps that you perform, will have some changes okay? And we'll talk about those in the future as well. But I would again suggest that you read this book, that book has given description of various steps in a much detailed fashion. I hope you like this video. If you did, please give it a thumbs up, and share it with your friends. The entire NLP playlist link is in our video description below. [Music]