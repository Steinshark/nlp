If you're even somewhat you've probably heard of Huffman coding. This is one of the ways your JPEGs, PNGs, MP3s, and ZIP files compress data But even though Huffman coding is so ubiquitous, arithmetic coding, which uses arithmetic to turn actually supports arithmetic coding, but if you on the application, you might get an error. was historically covered by patents. Even though to implement support for something nobody uses, There's also a newer family of compression The inventor of ANS put it in the public domain, and Microsoft eventually succeeded by telling the In this video, I want to build these two both super compact, clever algorithms and I think it's a shame that But first, let's look at Huffman You use it when you want to turn a message--a sequence of binary digits, or bits. Now, I'm any data. You provide a list of all the possible the probability that a symbol in the message is the symbols in the message to be an &quot;I.&quot; Then, the symbol code, and you encode the data by just you match the leftmost part of the encoded Huffman coding is the best algorithm for symbol separate code, because it assigns shorter codes to Reducible has a nice video about why it the message match the given probabilities, possible. But that's only for a symbol code, one symbol. If we let bits encode Take the set of 10 symbols 0 through 9. this is a code Huffman's algorithm gives us. Encoding this sequence of symbols gets us 68 bits. Imagine this message itself as one, this number from base 10 to base 2, you this code carries the same This raises a natural question: What algorithm for a message? Here, &quot;average code length&quot; has a the average code length is the expected length for each message, weighted by the this doesn't make sense with infinite messages that are N symbols long and see And, like before, we'll assume that the symbols and that they're all randomly sampled p_i is the probability that a symbol is s_i. This is called a discrete memoryless source, and distributed random variable. You might and in fact, it is true that these don't perform well on data that which is why you usually transform the data But anyway, the key pattern here is that as N up a random message will look more and more like there will be about N times p_i of that symbol called &quot;typical,&quot; and as N approaches infinity, approaches one. Sort of. The real proof is equipartition property&quot; and &quot;strong typicality&quot; is that we can now just find the minimum instead of any message, to at least get This is nice because you have the same so the expected code length is just the average the probability of a message is just the So, if a message has 3 &quot;a&quot;s, 2 &quot;b&quot;s, and 1 &quot;c&quot;, the probability of that message is the times the probability of &quot;b&quot; squared times the probability is the product of the p_i's for As N approaches infinity, random so the sum of these probabilities approaches 1, sum is also equal to the total number of typical number of typical messages is approximately the of bits you need to point to a specific typical This comes from the fact that a binary code of L of L different messages. We also can't use shorter What we just very loosely went over was a proof and this negative, is called H(X), the that your expected code length but not any less than, N times H Entropy represents how much information you need tell what it is; it's the expected value of the variable X. When the base of this logarithm is &quot;shannons.&quot; An outcome gives you more information was guaranteed. Our goal is to encode each symbol symbol with information content I into I bits. And more frequent symbols with fewer bits. A nice fact equal to the sum of the information contents of So in general, anything with information Compression algorithms that try to do this are but it only achieves entropy when of 2 because each symbol has to be So, what about that base-converting alphabet of 10 symbols all with probability the number that a message spells out 300--and so on. The entropy of the message is the approximately log base 10 of X. But of digits in the binary representation But there is a bug: if the then converting it to base 2 will mess it up. So message. You can either add a decimal point which will lead you to arithmetic coding, treat it as a natural number, which will I'll describe arithmetic coding first. If we think some number from 0 to 1, each digit stratifies digit chooses which section the number lies in. successive digit divides the range further. So you can specify an entire message with just one then each symbol divides the current There's a problem, though: not all many bits. For example, one tenth has you fix this is by adding a new symbol &quot;End of Message.&quot; When you reach an &quot;End of each &quot;End of File&quot; section represents one output any binary number that lies within the To generate this number, we start out with a range subdivide it into symbols. To encode a symbol, Our code must be a number in this range, so if code space, we can output a bit specifying which half. This will help us avoid precision errors. another bit, and we do this until the symbol Then, we subdivide the range again and range lies in the middle, though. Because it's and subdividing it might make the sections so our range should eventually fall into one of the we'll output one bit, renormalize, So when the range lies not in an outer half, but and keep track of how many times we do this. When output a bit for that half, followed by one of the middle. That's basically the whole encoding entropy in a bit. I just want to mention that in from zero to one, but instead with integers in a numbers you work with are integers in this exact same integers that the encoder generated at need just two more bits, plus any pending bits, Let's see how many bits we use to our code space--this range that includes starts with this code---and we stop when the So, we stop when one half to the power shrinks to the size of the section. So, the number the number of bits to equal the information size of any &quot;End of File&quot; section equal to the Because the probability of the message is just all we need to do is make the subsection symbol's probability. Each symbol you the range by the probability. This also the probabilities can update themselves as both the encoder and decoder agree with the If you want to see a full implementation linked to a nice explanation in the description. The last thing I'll leave you with is arithmetic coding, we stored data as a we store it in a natural number that we'll call so our symbols are the digits 0 through 9. of the symbols in the message--plus for uniform symbol probabilities, so let's each symbol. X starts out with an initial value you multiply X by 10 and add its symbol index. Now, what we're gonna do is label the natural numbers are labeled themselves, 15 is labeled represent encoding a symbol as looking at the set X + 1 of them. So, if X is currently 13, which encode the symbol &quot;4,&quot; which should make X 134, we them, and we set X to 134. This works because each so counting by X + 1 just multiplies X by 10, so C(X, 4) is the (X + 1)th number in the set by repeatedly setting X to C(X, symbol). The to incorporate probability, we'll rearrange these and addition anymore, but some analogous So, if we have N symbols, then each natural instead of one of 10 digits. The encoding will (X + 1)th number labeled with a symbol. To generate this labeling, let's think about symbol &quot;a.&quot; The binary representation of X is what then the length of X in binary should be of the first N symbols. If we exponentiate be approximately equal to the product of the So, when we encode a new symbol &quot;a,&quot; then C(X, a) should be around X divided Back to our labeling of the naturals, we saw each number labeled 4, and adds something. So if the space between each number labeled the reciprocal of the probability of that of the natural numbers labeled with a symbol Unfortunately, it is not easy to label the spaced out appropriately, so you instead using some heuristic and then repeat that The decoder works in reverse, so the last symbol which means we do have to encode our message symbol the number at X is labeled as, and to X got here by just counting this symbol X_prev naturals labeled &quot;a&quot; that are less than X. We'll to decode, you extract a symbol, set X to D(X), Now, in practice, during we have to renormalize it down and output The way we do it is by predicting where C(X) will some threshold T, we'll renormalize by looking X and popping off the last bit to put in the by 2 and rounds down. We keep doing this until our we set our next X to C(X_norm, Y) to encode our Now, the decoder has to know when this bit up. We know that the encoder always keeps X less X_norm must be less than T divided by 2. So, a number less than T/2., the decoder can multiply this until it reaches T/2, in which case it will Now, for this to work, you have to set T to 2N, the decoder might try to scale up an X that just you initialize X to a number less than N such X will always lie within the range [N, 2N-1]. the decoder knows to stop when X comes below N, but there are no more bits in the bitstream in this initial value; it'll be the The final, brilliant thing about ANS is that this tables. Everything--encoding, renormalization, to these tables, which makes tabled ANS super on your labeling--to arithmetic coding, which is the description if you want to see details. probabilities context-adaptive like in arithmetic and binary ANS--that use the same ideas If you've heard of JPEG XL, a pretty to permanently replace JPEG--that uses range ANS, the public domain algorithm that Microsoft was office is just unequipped to analyze complex perhaps the simplest encoding algorithm--just pretty convoluted. So I hope this video has