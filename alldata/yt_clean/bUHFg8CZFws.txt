Hi. What do you think is a single most important design interview? What should we focus on the most while preparing I bet this is knowledge. Knowledge of system design concepts and how And let me prove you that by walking you through interview. This is system design interview channel. And today we discuss how to count things at So, the interviewer asks us to design a system The problem may be articulated clearly, for on Youtube. Or likes on Instagram or Facebook. But more often than that, the problem will For example, we may be asked to calculate but a set of metrics. For example we need to monitor performance Which means we need to count how many requests produce, average response time. And of course nothing stops the interviewer form, like analyze data in real-time. What does data analysis mean? Who sends us data? Who uses results of this analysis? What real-time really means? These and many other questions need to be And even if system design problem seems clear you still need to ask your interviewer questions. The first reason is really important for the The second reason is important for you. The interviewer wants to see how you deal Whether you can identify key pieces of the And why it is so important for the interviewer will approach design problems in real life. System design interview problems are usually It is impossible to solve such problems within We should be clear on what functional pieces the rest of the interview. And why requirements clarification is so important Mainly because there may be many solutions And only when we fully understand what features up with a proper set of technologies and building For example, let's take a problem of counting If you ask this problem a software engineer explain you why MySQL database is a good fit. Engineer with profound experience in NoSQL at large scale using for example Apache Cassandra We may use distributed cache to count stuff. Or apply stream processing techniques. Experts in cloud computing will solve this Engineers focusing on batch processing will MapReduce. And views counting problem can indeed be solved But these options are not equal. Each has its own pros and cons. We should pick those options that address What we may want to ask about? My recommendation is to focus on the following Users, where we want to understand who and Scale, where we want to understand how our Performance, where we want to know how fast And cost, where we need to understand budget Let's see what specific questions in each Here and below we imply views counting system Who will use the system? Is this all Youtube viewers who will see the Or this is a per-hour statistics available Or may be this total count will be used by How the system will be used? The data may be used by marketing department In other words data is retrieved not often. Or data is sent to recommendation service Meaning that data may be retrieved from the Questions in this category help us understand Questions in the Scale category should give and how much data is retrieved from the system. So, the questions we should ask: needs to process. How much data is queried per request. How many video views per second are processed Should we deal with sudden traffic spikes The interviewer will help us define these Or we may assume some reasonable values. Questions in the performance category should options. For example, if we can count views several both batch data processing and stream processing But if time matters and we need to count views batch data processing is not an option. It is just too slow. Another good question to ask, is to clarify And if interviewer tells us that response hint that we must count views when we write when we read data. In other words data must already be aggregated. And questions in the cost category should For example, if asked to minimize development open-source frameworks. And if future maintenance cost is a primary for our design. Here is a little secret you might already During requirements clarification interviewer in systems design. As with coding interviews, if you do not ask interviewer. And here is my advice: think along 4 categories how it gets in and gets out of the system, You better spend additional 5 minutes clarifying solving a different or more complex problem The end goal of requirements clarification both functional and non-functional requirements. When we say functional requirements, we mean - a set of operations the system will support. This is what our system will do. And when we say non-functional requirements, secure. This is basically how a system is supposed Let's define functional requirements first. And here is my practical advice. After figuring out what the system should In a few sentences. For example, we write down that the system Count view events is the actual action the parameter. And if we want the system to calculate not say likes and shares, we may generalize our This parameter indicates type of the event We can go one step further and make the system functions as well, like sum and average. By supporting sum function we can calculate While average function can help us calculate And we can even further generalize the API events one by one, but a list of events as Where each event is an object that contains time when event happened and so forth. Similar thought process can be applied for We first write down something like: the system period. Get views count becomes an action. While video identifier and start and end time If we want to retrieve count not only for example, we can introduce event type parameter. And if we want our API return not just count average, we should specify function as a parameter getStats. Following this simple technique will help and, if needed, make several iterations to Now, let's talk about non-functional requirements. Usually, the interviewer will not tell us Most likely, she will challenge us with mentioning to achieve both at the same time. And we will need to find tradeoffs. In one of the previous videos I recommended as top priority requirements. Let's use them here. We need to design a system that should handle We also want view count statistic to be returned And we want view count statistics to be shown Even when there are network or hardware failures Although these 3 will be our primary concern, requirements. Let's recall a CAP theorem and talk about And I would like us to touch a cost minimization And my advice is that you write down non-functional It will help you later while choosing among We will talk more about this. Now, we have come to the next stage of a system And we will start with something really simple. We need a database to store data. We will have a web service that processes in the database. To retrieve view counts from the database, Nothing scary so far, right? And it should not be. At this point we do not yet have a clear vision So, we just throw some very high-level components But what do we do next? High chances the interviewer is an expert And she may start asking questions about any But we may not feel comfortable discussing Unless you are an expert in this field yourself, you. This is in our best interest as interviewees one step at a time. This is like assembling a puzzle. The interviewer can assemble the puzzle starting But we need to start with something simple first. So, what is this frame of outside pieces of It's data. And more specifically we need to think what Or using more professional terms we need to We have two options for what data we want We may store each individual video view event. Or we may calculate views on the fly and store When we store individual events we capture timestamp, user related information such as so on. When we aggregate data we calculate a total minute and we lose details of each individual There are pros and cons of each option. Individual events can be stored really fast. We just get the event and push it to the database. Later, when we retrieve data, we can slice We can filter based on specific attributes, And if there was a bug in some business report, But there are drawbacks of this approach. We cannot read data quickly. We need to count each individual event when This takes time. Another big disadvantage of this approach the raw events. Youtube generates billions of view events Raw events storage must be huge. On the other hand, reads become fast when We do not need to calculate each individual Another nice property of the aggregated data, For example, we may send the total count value for popular videos to be promoted to trends. Aggregated data approach has drawbacks as First of all, we can only query data the way Ability to filter data or aggregate it differently This approach also requires us to implement We need to somehow pre-aggregate data in memory This is not an easy task and later you will Another big problem with this approach, it Let's say we introduced a bug in the aggregation Then, how do we fix total counts after the So, which option should we chose? Store raw events or aggregate data in real-time? This is where we need interviewer to help We should ask the interviewer about expected Time between when event happened and when If it should be no more than several minutes If several hours is ok, then we can store Former approach is also known as stream data processing. The interviewer will let us know what option But because I have no-one to ask...anybody...no...I And by the way combining both approaches makes We will store raw events, and because there for several days or weeks only. And then purge old data. And we will also calculate and store numbers So that statistics is available for users By storing both raw events and aggregated reads, ability to aggregate data differently bugs or failures on a real-time path. But there is a price to pay for all this flexibility, Great topic to discuss with the interviewer. Further in this video we will mainly focus As I find it more educational from the system Now let's talk about where we store the data. The interviewer wants to know specific database We should know (and do not worry if you do minutes) that both SQL and NoSQL databases So, we may want to evaluate both options. And here is where we should recall non-functional Remember that we wrote them down on the whiteboard What are those? Scalability, performance and availability. So, we should evaluate databases against these And let's add some more requirements along Feel free to use this list for other interview Database solution we chose should scale well It should be fast and guarantee high availability. We should be able to achieve required level We should understand how to recover data, changes. We need to pick hardware and evaluate cost First, let's see how SQL databases handle Things are simple when we can store all our But when a single machine is not enough, we data between them. This procedure is called sharding or horizontal Each shard stores a subset of all the data. And because we now have several machines, know how many machines exist and which one We discussed before that we have Processing and Query service, that retrieves data from We could have made both these services to A better option is to introduce a light proxy and routes traffic to the correct shard. Now, both Processing and Query services talk They do not need to know about each and every But cluster proxy has to know. Moreover, proxy needs to know when some shard partition. And if new shard has been added to the database How do we achieve this? We introduce a new component - Configuration Configuration service maintains a health check So, it always knows what database machines So, cluster proxy calls a particular shard. And instead of calling database instance directly, that sits in front of a database. Shard proxy will help us in many different database instance health and publish metrics, data and many more. Great, this setup helps us address several Like scalability and performance. But availability is not yet addressed. What if database shard died? How to make sure data is not lost? I believe you already know the answer to this We need to replicate data. Let's call each existing shard a master shard And for every master shard we introduce a We call it read replica because writes still through both master shard and a replica. We also put some replicas to a data center So that if the whole data center goes down, So, when store data request comes, based on service, cluster proxy sends data to a shard. And data is either synchronously or asynchronously And when retrieve data request comes, cluster shard or from a read replica. Ideas we have just discussed is what Youtube They have built a database solution to scale It is called Vitess. Great, we now know how to scale SQL databases. But this solution does not seem simple, right. We have all these proxies, configuration service, May be we can simplify things a little bit. Let's take a look at what NoSQL databases And more specifically, Apache Cassandra database. In NoSQL world we also split data into chunks But instead of having leaders and followers We no longer need configuration service to Instead, let's allow shards talk to each other To reduce network load, we do not need each Every second shard may exchange information Quickly enough state information about every This procedure is called a gossip protocol. Ok, each node in the cluster knows about other And this is a big deal. Remember that previously we used Cluster Proxy shard. As Cluster Proxy was the only one who knew But now every node knows about each other. So, clients of our database no longer need Clients may call any node in the cluster and request further. Let's elaborate on this. Processing service makes a call to store views 4 is selected to serve this request. We can use a simple round robin algorithm and chose a node that is &quot;closest&quot; to the Let's call this node 4 a coordinator node. The coordinator node needs to decide which We can use consistent hashing algorithm to As you may see node 1 should store the data a call to the node 1 and wait for the response. Actually, nothing stops coordinator node to example 3 nodes if we want 3 copies of data. Waiting for 3 responses from replicas may to be successful as soon as only 2 replication This approach is called quorum writes. Similar to quorum writes, there is a quorum When Query service retrieves views count for several read requests in parallel. In theory, the coordinator node may get different Why? Because some node could have been unavailable That node has stale data right now, other Read quorum defines a minimum number of nodes Cassandra uses version numbers to determine And similar to SQL databases, we want to store centers. For high availability. Do you remember where else on this channel hashing algorithm? Right, when we designed distributed cache. Please check that video if you want to know Another important concept to mention is consistency. Remember that when we defined non-functional It simply means we prefer to show stale data Let me clarify this. Synchronous data replication is slow, we usually In case of a leader-follower replication for their master. Which leads to a situation when different This inconsistency is temporary, over time This effect is known as eventual consistency. Cassandra actually extends the concept of Let's discuss this big topic separately and After discussing what we store and where, There is a big difference how we do data modeling When designing data models for relational in the system. We then convert these nouns into tables and in these tables. Let's take a look at the example. We want to build a report that shows the following of total views per hour for last several hours belongs to. We pass video identifier as input for this In a relational database we would define the contains information about videos, video stats for each hour and channel info table that And to generate report mentioned above, we all three tables. And important property of a relational database It simply means we minimize data duplication For example we store video names in the video And we do not store video names in other tables. Because if some video name changes, we have Which may lead to inconsistent data. So, normalization is good for relational databases. But NoSQL databases promote a different paradigm. They say that we no longer think in terms be executed in the system we design. And denormalization is perfectly normal. Not something that we always have to do, but In Cassandra for example, report mentioned in the table. We store everything required for the report And instead of adding rows, as in a relational next hour. Great, we have covered the storage portion And hopefully by now you have got the idea for our solution. What database would you chose? Please let me know in the comments. I also would like to clarify one thing. As you know there are 4 types of NoSQL databases: So far we have used Cassandra as a representative We chose Cassandra because it is fault-tolerant, linearly as new machines are added). It supports multi datacenter replication and And of course other options are available For a typical system design interview we usually databases. But we need to know advantages and disadvantages And please do not think that all NoSQL database earlier. Cassandra is a wide column database that supports But other NoSQL databases have different architectures. For example MongoDB, a documented-oriented HBase, which is another column-oriented data architecture as well. We will talk more about different database As well as when to use each particular database Ok, enough talking about databases, let's First, let's define what processing really When Youtube users open some video, we want immediately. It means we need to calculate view counts Also, when video owner opens statistics for So, processing basically means we get a video total and per hour counters. You are in front of the whiteboard, interviewer ideas. Where to start? As usual start with requirements. Remember, we wrote them down on the whiteboard We want the processing service to scale together We want to process events quickly. And we do not want to lose data either when when database becomes unavailable. So, we literally ask ourselves how to make And this is another example of why knowledge interviews. Because curiosity and study will equip you system designs. Even if you never solved such problems in For example you know already that when we about partitioning. When we want to avoid data loss we replicate And when speed matters we should keep things Easy, right? But before diving into processing service on some basics. The first question I have for you is whether processing service. Let me clarify the question. We have two options how to update counters In the first option the processing service Meaning that if three users opened the same total count in the database three times. Logic is simple, event comes and we increment In the second option, we accumulate data in of time, lets say several seconds. And add accumulated value to the database For example three users opened some video and increments in-memory counter. And every several seconds in-memory counter the final count. I claim that the second option is better (for Someone who agrees with me, please help me By sharing your arguments in the comments Further in this video I assume that we aggregate With this assumption in mind, what do you Where push means that some other service sends While pull means that the processing service Although the answer is that both options are option has more advantages, as it provides to scale. Let me explain. We have two users opening two different videos counters, returns successful responses back sending this data to the database. Data is lost. The alternative to push approach is for the Events generated by users are stored in that Processing service machine pulls events and And if machine crashes, we still have events And here we come to another important concept, We have a temporary storage. And as you may see I intentionally draw it Because when events arrive we put them into Fixed order allows us to assign an offset This offset indicates event position in the Events are always consumed sequentially. Every time an event is read from the storage, After we processed several events and successfully to some persistent storage. If processing service machine fails, it will machine will resume processing where the failed Very important concept in stream data processing. Another important concept is partitioning. We already discussed it when talked about Main idea remains the same when applied to Instead of putting all events into a single Each queue is independent from the others. Every queue physically lives on its own machine For example we compute a hash based on video a queue. As you may see partitioning allows us to parallelize More events we get, more partitions we create. Equipped with these basic concepts, we now We discussed so far that processing service events in memory, and flushes this counted So, we need a component to read events. The consumer establishes and maintains TCP We can think of it as an infinite loop that When consumer reads event it deserializes Meaning it converts byte array into the actual Usually, consumer is a single threaded component. Meaning that we have a single thread that We can implement multi-threaded access. When several threads read from the partition But this approach comes with a cost, checkpointing preserve order of events if needed. Consumer does one more important thing - helps If the same message was submitted to the partition this can happen), we need a mechanism to avoid To achieve this we use a distributed cache say last 10 minutes. And if several identical messages arrived them (the first one) will be processed. Event then comes to the component that does Let's call it aggregator. Think of it as a hash table that accumulates Periodically, we stop writing to the current A new hash table keeps accumulating incoming While old hash table is no longer counting table is sent to the internal queue for further Why do we need this internal queue? Why can't we send data directly to the database. Glad you asked. Remember, we have a single thread that reads But nothing stops us from processing these Especially if processing takes time. By sending data to the internal queue we decouple The best analogy I could think of is a security We can have a single person that quickly checks but we need several lines for carry-on bags time. You may argue whether we should put internal It is up to you. Both options are fine. Ok, we now ready to send pre-aggregated values So, we need a component responsible for this. Database writer is either a single-threaded Each thread takes a message from the internal in the database. Single-threaded version makes checkpointing But multi-threaded version increases throughput. Think about it for a moment and let me know No need to worry if data flow is not completely simulation a bit later. It should help to further clarify the whole Meanwhile, I would like to point out two more The first concept is called a dead letter The dead-letter queue is the queue to which to their correct destination. Why do you think we may need one? You are correct, to protect ourselves from If database becomes slow or we cannot reach push messages to the dead letter queue. And there is a separate process that reads the database. This concept is widely used when you need degradation. So, you may apply it in many system designs. Another viable option is to store undelivered service machine. The second concept is data enrichment. Remember how we store data in Cassandra? We store it the way data is queried, right? If we want for example to show video title together with views count. The same is true for the channel name and display. But all these attributes do not come to the Event contains minimum information, like video It does not need to contain video title or So, these information comes from somewhere Some database. But the trick here is that this database lives All these additional attributes should be Thus, having it on the same machine eliminates Such databases are called embedded databases. LinkedIn for example uses this concept for When they show additional information about For example, how many viewers have recruiter One last concept I would like to mention is We keep counters in memory for some period Either in in-memory store or internal queue. And every time we keep anything in memory fails and this in-memory state is lost. But that is easy, you may say. We have events stored in the partition, let's we failed. In other words we just re-process events one This is a good idea. And it will work well if we store data in-memory state is small. Sometimes it may be hard to re-create the The solution in this case is to periodically storage. And new machine just re-loads this state into We have looked inside the processing service. We now ready to finalize the data ingestion From the moment events appear in our counting We know already that we have a set of partitions count data in memory for some short period Someone needs to distribute data across partitions, Let's have a component called Partitioner Let's also have a load balancer component To evenly distribute events across partitioner When user opens a video, request goes through point into a video content delivery system. API Gateway routes client requests to backend Our counting system may be one of such backend And one more important component to mention Please prepare yourself for a lot of useful Pause this video, grab a cup of coffee or Give yourself a rest for the next 15 minutes You will learn a ton. I could not think of a better way then dumping Do not blame me, blame distributed systems We talked about database and processing service Now let's cover remaining 3 components of client, load balancer and partitioner service. We will discuss many ideas these components And will start with some basics. When client makes a request to a server, server The client initiates the connection by using When a client makes a request, the socket side is blocked. This happens within a single execution thread. So, the thread that handles that connection And when another client sends a request at thread to process that request. This is how blocking systems work. They create one thread per connection. Modern multi-core machines can handle hundreds But let's say server starts to experience and threads increases. When this happens, machines can go into a may die. Remember we designed a rate limiter in one That is exactly why we need rate limiting traffic peeks. Alternative to blocking I/O is non-blocking When we can use a single thread on the server Server just queues the request and the actual Piling up requests in the queue are far less Non-blocking systems are more efficient and You may be wondering that if non-blocking many blocking systems out there? Because everything has a price. And the price of non-blocking systems is increased Blocking systems are easy to debug. And this is a big deal. In blocking systems we have a thread per request by looking into the thread's stack. Exceptions pop up the stack and it is easy We can use thread local variables in blocking All these familiar concepts either do not world. Moving on to buffering and batching. There are thousands of video view events happening To process all these requests, API Gateway Thousands of machines. If we then pass each individual event to the of machines has to be big as well. This is not efficient. We should somehow combine events together to the partitioner service. This is what batching is about. Instead of sending each event individually, We then wait up to several seconds before up, whichever comes first. There are many benefits of batching: it increases compression is more effective. But there are drawbacks as well. It introduces some complexity both on the For example think of a scenario when partitioner events from the batch fail, while other succeed. Should we re-send the whole batch? Or only failed events? What do you think? The next concept is timeouts. Timeouts define how much time a client is We have two types of timeouts: connection Connection timeout defines how much time a to establish. Usually this value is relatively small, tens Because we only try to establish a connection, yet. Request timeout happens when request processing to wait any longer. To choose a request timeout value we need For example we measure latency of 1% of the And set this value as a request timeout. It means that about 1% of requests in the And what should we do with these failed requests? Let's retry them. May be we just hit a bad server machine with And the second attempt may hit a different succeed. But we should be smart when retry. Because if all clients retry at the same time retry storm event and overload sever with To prevent this, we should use exponential Exponential backoff algorithm increases the backoff time. We retry requests several times, but wait And jitter adds randomness to retry intervals If we do not add jitter, backoff algorithm And jitter helps to separate retries. Even with exponential backoff and jitter we For example when partitioner service is down And majority of requests are retried. The Circuit Breaker pattern stops a client that's likely to fail. We simply calculate how many requests have exceeded we stop calling a downstream service. Some time later, limited number of requests and invoke the operation. If these requests are successful, it's assumed the failure has been fixed. We allow all requests at this point and start The loop completes. The Circuit Breaker pattern also has drawbacks. For example, it makes the system more difficult And it may be hard to properly set error threshold By the way, have you noticed that everything None of these concepts is a silver bullet. This is true for almost all concepts in distributed We should always know and remember about tradeoffs. It is hard, I agree. More knowledge and experience you have, easier Let's now talk about load balancing. It is a big topic of course, we will consider As you know load balancers distribute data There are two types of load balancers: hardware Hardware load balancers are network devices Theses are powerful machines with many CPU very high throughput. Millions of requests per second. Software load balancer is only software that We do not need big fancy machines, and many Load balancers provided by public clouds (for load balancer type as well. Another gradation of load balancers is what It is a bit more complicated than that, but too much. Let's keep things simple for now. TCP load balancers simply forward network the packets. Think of it as if we established a single and a server. This allows TCP load balancers to be super HTTP load balancers, on contrast, terminate Load balancer gets an HTTP request from a and sends request to this server. HTTP load balancer can look inside a message on the content of the message. For example based on a cookie information Load balancers may use several algorithms Round robin algorithm distributes requests Least connections algorithm sends requests connections. Least response time algorithm sends requests Hash-based algorithms distribute requests IP address or the request URL. Ok, you got it, there are many benefits of I should stop selling them to you. Instead, let's return to our original system questions. Such as, how does our partitioner service How does load balancer know about partitioner And how does load balancer guarantee high Because it looks like a single point of failure, Here is where we should recall DNS, Domain DNS is like a phone book for the internet. It maintains a directory of domain names and We register our partitioner service in DNS, and associate it with IP address of the load So, when clients hit domain name, requests For the load balancer to know about partitioner the load balancer the IP address of each machine. Both software and hardware load balancers Load balancers need to know which server from are unavailable at the moment. This way load balancers ensure that traffic Load balancer pings each server periodically balancer stops to send traffic to it. It will then resume routing traffic to that healthy again. As for high availability of load balancers, nodes. The primary load balancer accepts connections balancer monitors the primary. If, for any reason, the primary load balancer one takes over. Primary and secondary also live in different down. There are several other interesting topics cover those in a separate video. But now let's move on to the next two components With partitioner service it is more or less It's a web service that gets requests from individual video view events (because remember each such event (we can also use the word But what partitions are? Partitions is also them on disk in the form of the append-only So, we have a totally-ordered sequence of This is not a single very large log file, Partitioner service has to use some rule, gets what messages. A simple strategy is to calculate a hash function and chose a machine based on this hash. This simple strategy does not work very well As it may lead to so called &quot;hot partitions&quot;. For example when we have a very popular video go to the same partition. One approach to deal with hot partitions is into partition key. All video events within the current minute Next minute, all events go to a different Within one minute interval a single partition data is spread more evenly among partitions. Another solution to hot partitions problem To get an idea how this approach might work, how adding a new node to the consistent hashing And if to push this idea of partition split partitions for some popular video channels. All video view events from such channels go And view events from all other channels never These are the powerful techniques and there topic on the internet. So, please remember them. To send messages to partitions, partitioner This is where the concept of service discovery In the world of microservices there are two discovery and client-side discovery. We already looked at server-side discovery Clients know about load balancer, load balancer Easy. But we do not need a load balancer between Partitioner service itself acts like a load This is a perfect match for the client-side With client-side discovery every server instance service registry. Service registry is another highly available to determine health of each registered instance. Clients then query service registry and obtain Example of such registry service is Zookeeper. In our case each partition registers itself instance queries Zookeeper for the list of Do you remember where else we discussed client-side not name it there that way? When we talked about distributed cache design. When cache client needs to pick a cache shard There we discussed several other options as Please go and check that video. One more option for service discovery is similar Remember we mentioned before that Cassandra So, every node in the cluster knows about It means clients only need to contact one information about the whole cluster. Think about this. Next important concept is replication. We must not lose events when store them in So, when event is persisted in a partition, If this partition machine goes down, events There are three main approaches to replication: and leaderless replication. Do you remember where we used single leader Correct, when we discussed how to scale a Great, and do you remember when we talked When we discussed how Cassandra works, right? So far we did not talk about multi leader Let me make a separate deep dive into replication And only mention right now that multi leader several data centers. So, which approach should we chose for partitions? Let's go with single leader replication. Each partition will have a leader and several We always write events and read them from While a leader stays alive, all followers And if the leader dies, we choose a new leader The leader keeps track of its followers: checks any of the followers is too far behind. If a follower dies, gets stuck, or falls behind, its followers. Remember a concept of a quorum write in Cassandra? We consider a write to be successful, when the write. Similar concept applies to partitions. When partitioner service makes a call to a as leader partition persisted the message, number of replicas. When we write to a leader only, we may still really happened. When we wait for the replication to complete, latency will increase. Plus, if required number of replicas is not suffer. Tradeoffs, as usual. Next topic is the last one on this slide, Let's talk about message formats. We can use either textual or binary formats Popular textual formats are XML, CSV, JSON. Popular binary formats are Thrift, Protocol What's great about textual formats - they They are well-known, widely supported and But for the large scale real-time processing Messages in binary format are more compact And why is this? As mentioned before, messages contain several user related information. When represented in JSON format, for example, greatly increases total message size. Binary formats are smarter. Formats we mentioned before require a schema. And when schema is defined we no longer need For example Apache Thrift and Protocol Buffers Tags are just numbers and they act like aliases Tags occupy less space when encoded. Schemas are crucial for binary formats. Message producers (or clients) need to know Message consumers (processing service in our message. So, schemas are usually stored in some shared can retrieve them. Important to mention that schemas may and We may want to add more attributes into messages Apache Avro is a good choice for our counting That was tough. Talking about all these concepts completed And all the most difficult stuff is behind Do not worry if some mentioned before concepts Each of them deserves a video on its own. And we will talk more about them on this channel. Eventually, you will feel comfortable using your day to day job. Now let's take a look at data retrieval path. When users open a video on Youtube, we need To build a video web page, several web services A web service that retrieves information about another one for recommendations. Among them there is our Query web service All these web services are typically hidden point. API Gateway routes client requests to backend So, get total views count request comes to We can retrieve the total count number directly Remember we discussed before how both SQL But total views count scenario is probably This is just a single value in the database The more interesting use case is when users of data points ordered in time. For example, when channel owner wants to see As discussed before, we aggregate data in say per hour. Every hour for every video. That is a lot of data, right? And it grows over time. Fortunately, this is not a new problem and Monitoring systems, for example, aggregate second. You can imaging how huge those data sets can So, we cannot afford storing time series data of time. The solution to this problem is to rollup For example, we store per minute count for After let's say one week, per minute data And we store per hour count for several months. Then we rollup counts even further and data stored with 1 day granularity. And the trick here is that we do not need We keep data for the last several days in somewhere else, for example, object storage In the industry, you may also hear terms like Hot storage represents frequently used data Cold storage doesn't require fast access. It mostly represents archived and infrequently When request comes to the Query service, it need to call several storages to fulfill the Most recent statistics is retrieved from the from the Object Storage. Query service then stitches the data. And this is ideal use case for the cache. We should store query results in a distributed This helps to further improve performance We covered both data ingestion and data retrieval. Not many things left. Let me show you the full picture and share Three users opened some video A. And API Gateway Partitioner service client batches all three to the partitioner service. This request hits the load balancer first. And load balancer routes it to one of the Partitioner service gets all three events All three events end up in the same partition, Here is where processing service appears on Partition consumer reads all three messages to the aggregator. Aggregator counts messages for a one minute internal queue at the end of that minute. Database writer picks count from the internal In the database we store count per hour and So, we just add a one minute value to the Total count was 7 prior to this minute and And during data retrieval, when user opens Query service. Query service checks the cache. And if data is not found in the cache, or Total count value is then stored in the cache back to the user. I hope that this simulation helped you further the architecture. Feel free to post any questions you have in Another important aspect of an interview and When we design some system, we usually do We rely on some well-regarded technologies. Either open source or commercial. Public cloud services. During the interview do not forget to discuss You may do this along the way or at the end So, let's see what technologies we may use Netty is a high-performance non-blocking IO both clients and servers. Frameworks such as Hystrix from Netflix and concepts we discussed before: timeouts, retries, Citrix Netscaler is probably the most famous Among software load balancers NGINX is a very And if we run our counting system in the cloud, Balancer is a good pick. Instead of using our custom Partitioner service instead. Or Kafka's public cloud counterpart, like To process events and aggregate them in memory as Apache Spark or Flink. Or cloud-based solutions, such as Kinesis We already talked about Apache Cassandra. Another popular choice for storing time-series These are wide column databases. There are also databases optimized for handling We also mentioned before that we may need in case of any error or if customers need We can store raw events in Apache Hadoop or And when we roll up the data and need to archive I would like to also mention several other Vitess is a database solution for scaling Vitess has been serving all Youtube database In several places of our design we rely on and to scale read data queries. Redis is a good option. For a dead-letter queue mechanism, when we we may use an open-source message-broker such Or public cloud alternative, such as Amazon For data enrichment, when we store video and machine and inject this information in real-time, database for key-value data. To do leader election for partitions and to Zookeeper, which is a distributed configuration For the service discovery piece we actually Netflix. To monitor each of our system design components by public cloud services, such as AWS CloudWatch. Or use a popular stack of open source frameworks: Or ELK for short. We discussed before that binary message format Popular choices are Thrift, Protobuf and Avro. For Partitioner service to partition the data, example a MurmurHash. We are done with the detailed system design. And here is where our interviewer will start There are several goals of this exercise. The interviewer wants to see that we know Have you noticed that I usually use verbs to? This is because we usually have several options When we design a system or a part of it in to discuss with the team, right? And very important is not only know your options, one. We discussed many tradeoffs of individual Let's see what else the interviewer may want To identify bottlenecks in the system we need This is what performance testing is about. There are several types of performance testing. We have load testing, when we measure behavior We have stress testing, when we test beyond point. We have soak testing, when we test a system period of time. With load testing we want to understand that a load we expect. For example, a two or three times increase With stress testing we want to identify a Which component will start to suffer first. And what resource it will be: memory, CPU, And with soak testing we want to find leaks For example, memory leaks. So, generating high load is the key. Tools like Apache JMeter can be used to generate Health monitoring. All the components of our system must be instrumented Metrics, dashboards and alerts should be our Metric is a variable that we measure, like Dashboard provides a summary view of a service's And alert is a notification sent to service in the service. Remember about the four golden signals of and saturation. Let's leave details for a separate video. Ok, we designed a system and deployed all We know it is running healthy and can handle But how to make sure it counts things correctly? This becomes critical when we not just count some ad was played in a video. As we need to properly charge an ad owner This problem is typically addressed by building There can be two flavors of audit systems. Let's call them weak and strong. Weak audit system is a continuosly running When let's say once a minute we generate several service and validate that returned value equals This simple test gives us a high confidence And it is easy to implement and maintain such But unfortunately, this test is not 100% reliable. What if our system loses events in some rare And weak audit test may not identify this That is why we may need a better approach. Strong audit system calculates video views main system. For example we store raw events in Hadoop And then compare results of both systems. Having two different systems doing almost You may be surprised but this is not so uncommon Not such a long time ago it was quite a popular And it even has a name - Lambda Architecture. The key idea is to send events to a batch And stitch together the results from both You can get a better understanding of this channel, where we designed a system for finding Ideally, we should have a single system. Let me share with you advice from Jay Kreps, We should use a batch processing framework and use a stream processing framework if we time unless we absolutely must. And please note that out today's problem can But MapReduce-based system would have a much We already discussed the problem with popular I will just reiterate the key idea. We have to spread events coming for a popular Otherwise, a single consumer of a single &quot;hot&quot; the load. And will fall behind. Let's talk more about this. Imaging a situation when the processing service Maybe because number of events is huge, maybe and time consuming. I will not dive too much into details, but We batch events and store them in the Object Every time we persist a batch of events, we For example SQS. Then we have a big cluster of machines, for read a corresponding batch of events from This approach is a bit slower than stream Everything is a tradeoff. Let's summarize what we have discussed. We start with requirements clarification. And more specifically, we need to define APIs, We then discuss non-functional requirements of the system she is most interested in. We can now outline a high-level architecture Draw some key components on the whiteboard. At the next stage we should dive deep into Our interviewer will help us understand what And the last important step is to discuss And let me quickly remind you some specifics To define APIs, we discuss with the interviewer system we need to design. We write down verbs characterizing these functions and return values. We then can make several iterations to brush After this step we should be clear on what To define non-functional requirements, just Open a list of non-functional requirements There are many of them. I recommend to focus on scalability, availability Among other popular choices we have consistency, Try to pick not more than 3 qualities. To outline a high-level design, think about out of the system and where data is stored Draw these components on the whiteboard. It is ok to be rather generic at this stage. Details will follow later. And although it is not easy, try to drive Our goal here is to get understanding of what And the interviewer will help us. While designing specific components, start How it is stored, transferred and processed. Here is where our knowledge and experience By using fundamental concepts of system design together, we can make small incremental improvements. And apply relevant technologies along the After technical details are discussed, we of the system. Listen carefully to the interviewer. She sees bottlenecks of our design and in bottlenecks are. And what can really help us here is the knowledge We just need to pick and apply a proper one. Today we covered a big topic. If you are still with me watching this video Seriously. There were many system design concepts covered And I hope you have a better understanding to the successful system design interview. And system design in general. And although we talked about a specific problem ideas can be applied to other problems, for impressions and clicks. The same ideas can be applied to designing When we design a fraud prevention system we card was used recently. When we design recommendation service we may When we design &quot;what's trending&quot; service, views, re-tweets, comments, likes. And many other applications. There are plenty of other important system As it is practically impossible to do in a But we will have them covered in other videos Thank you for being with me and I will see Bye.