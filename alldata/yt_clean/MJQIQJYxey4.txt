Demand has never been higher for these racks and racks appetite for computing in the cloud. The reality is that the cloud is not up there It's right here. We are in it. You're in the middle of the cloud as we speak. And data centers like this can't ever stop streaming and requiring much more data training and running Gemini, and Microsoft's Copilot. And you can feel the heat coming off of these. Thanks to the generative AI race, data centers like vantage can build them, and that means demand for too. If we don't start thinking about this power problem this dream we have or the potential we have of this change our lives. One ChatGPT query takes nearly ten times as much energy energy as keeping a five watt LED bulb on for an hour. Generating an AI image can use as much power as Hyperscalers building data centers to accommodate AI new. Estimates way back in 2019 found training one entire lifetime of five gas powered cars. And even if we can generate enough power, our aging If you look at the peak demand during the summer time, could be a blackout. With the looming question of whether we'll have enough Cnbc visited a data center in Silicon Valley to see those at the center of the problem to find out what There are more than 8000 data centers globally with But it's not enough. But we suspect that the amount of demand that we'll see more than we've seen historically from cloud The AI frenzy has data center demand rising 15 to 20% And as companies like Vantage build more and more, Data centers could reach a whopping 16% of total US report, up from just 2.5% before ChatGPT hit the scene That's the equivalent of about two thirds of the total Natural gas is expected to fuel the majority of this, support the growth. Data centers that probably around 64MW for the building Many of those are being taken up by single customers. They'll have the entirety of the space leased to them. And as we think about AI applications, those numbers beyond that into hundreds of megawatts. 64mw, 100MW. How many homes, you know, on average. Tens and tens of thousands of homes per data centers Many big tech companies contract with companies like have grown so much that many have been building their For Google and Microsoft, this has directly translated Google's latest environmental report says greenhouse 2023, in part because of data center energy times as energy efficient as a typical data center. Microsoft's emissions rose nearly 30% from 2020 to optimized to support AI workloads. Power needs are so high that some plans to close coal Kansas City, where Meta is building an AI focused data So you can never really stop the AI computing from So they will do whatever they can to get the power One approach: building data centers where power is more The industry itself is looking for places where there whether wind or solar and other infrastructure that incentive program to convert what would have been a looking at ways in which to offtake power from nuclear Santa Clara, where we visited Vantage, has long been centers near data hungry clients. Nvidia's headquarters was visible from the roof. Now, Vantage is building in places like Columbus, In Northern California, we're seeing a bit of a centers are deployed because of the lack of area. On the flip side of bringing data centers where the experimenting with ways to generate their own power OpenAI CEO Sam Altman has been vocal about this need. He recently invested in a solar startup that makes panels and power storage in one. Altman's also invested in nuclear fission startup Oklo a frame structures and in nuclear fusion startup Microsoft also signed a deal with Helion last year to and Google's partnered with a geothermal startup that underground to run a large data center. Even data centers themselves are starting to generate Which Vantage has done, for example, in Virginia, where megawatt natural gas power plant to support a There, we're self contained and we're delivering power touch the public grid at all. Even when enough power can be generated, the aging grid And that's where grid hardening comes in. With concentrations of data centers in particular more pressure on the grid in terms of delivering that In an area of northern Virginia known as data center internet traffic each day. At one point in 2022, the power company there had to keep up with demand. Basically, during the peak hour, we either ask the the or we ask the AI company to stop their training. Vantage, as an example, voluntarily supports a load that when the utility knows that they're going to have high temperatures, people need to run their air We voluntarily come off the grid. We run our own generators during that time in order to everybody. The bottleneck often occurs in getting power from the One solution is to add hundreds or even thousands of But those projects, like 15. $2 billion effort to expand lines to Data Center ratepayers who don't want to see their bills go up to It is a part of the solution to increase the capacity time consuming, and sometimes the cost is just passed of their utility bill increase. Another solution is to use predictive software to transformer. All electricity generated must go through a connects two sides of electrical circuits together. There are 60 to 80 million transformers in the US The average transformer in the US is 38 years old, so power outages, and replacing them is expensive and So we have this tiny little sensor. It's two inch diameter, one inch thickness, about half essentially glue it on the outside of a transformer. Software can then predict failures and determine which away from those at risk of failure. It's been installed in high power demand areas like Vi says business has tripled since ChatGPT came on the And what we're hearing in 2024 that we might double or A large reason we need more power and a more reliable All the servers generate an immense amount of hot air, overheating so they can keep running 24 over seven. The problem is, AI is projected to withdraw more water Denmark. Everybody is worried about AI being energy intensive. We can solve that when we get off our ass and stop Right. That's solvable. Water is the the fundamental limiting factor to what I was quite shocked when I saw the number of AI Shaolei Ren's been studying data center efficiency for His research team found that every 10 to 50 ChatGPT 16 ounce water bottle. Training takes even more power and generates even more being added to the entire internet. Training GPT three in Microsoft's US data centers can of clean, fresh water. With global AI demand accountable for up to six point 2027. More than four times the total annual withdrawal government partially reversed Google's permit to build usage, and it's facing similar backlash to planned There are technologies that are used in certain parts to cool data centers through evaporative cooling. That tends to be very efficient from a power water use perspective. And so vantage is designed from the beginning, really costs. Instead, Vantage uses dozens of gigantic air There's a combination of air handling and cooling allow heat. That's coming up from the hot aisles that converted back into the chilled water loop and brought Microsoft has halted a radical project that tried ocean. Another solution is using cool liquid directly inefficient process of cooling air with water. People are working on a whole bunch of things like radically reduce the amount of water that's needed. But the speed at which this is building makes it very indeed. So there's a lot of talk in the industry right now in Moving. From air, which is moving around us to water, air or to the chip itself in order to cool it in a much more But for a lot of data centers, that requires an About six years ago, we deployed a design that would into that cold water loop here on the data hall floor. There is one broader approach to solving. AI's massive problem of water and power lessen the In other words, get more work per watt. It all comes back to how do I get more done with the Scale flux makes memory and storage devices for data Data compression can be very slow latency adding and hungry if you try to do it. In general purpose cores like Intel or AMD x86 when you put it into a hardware state machine, it can efficiently. More power efficient. The key alternative to those power hungry x86 cores are Arm got its start making low powered chips that Saving every last bit of power. It's going to be a fundamentally different design than And so it could be some something as simple as how we It can be simple as how we access data, but it every is thought of as power first. Now ARM makes all sorts of chips, including neoverse And as AI takes off, ARM's power efficiency has made Microsoft, Oracle and Amazon. I have here AWS graviton and AWS graviton saves 60% power versus competitive architectures. Nvidia's latest AI chip unveiled in March, Grace run gen AI models on 25 times less power. And ARM says a data center filled with its chips can If you think about the scale of data centers, 15% is ChatGPT queries. Think about that. You can light 20% of American And companies like Apple, Samsung and Qualcomm have A huge energy savings for each query kept off the centers time to build more and catch up to AI's Everybody will build the data centers that they can, support, and it may be less than what people aspire But ultimately, there's a lot of people working on supply constraints, but definitely a lot of growth