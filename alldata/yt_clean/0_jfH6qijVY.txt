- Well, good morning everybody. Welcome to SVS 404, a Hello everyone. Welcome to the room. If you're here today, I also know there's an in the region of Las Vegas, Hello to everybody there. And also this is gonna be recorded, so if you are watching this hello into the future. My name's Julian Wood, I'm a senior developer advocate and I work on a cool team within the serverless product organization and I really love helping developers and builders understand how best to build serverless applications, as also, as well as also being to make sure we're creating the best products and features. And I'm also gonna be joined shortly by the awesome Chris Greenwood. He's one of our principal in the Lambda team. Now, this is a talk today from two previous Lambda The first was the re:Invent 2018, where Holly Mesrobian and Marc Brooker first showed how some of Lambda worked, where they covered the about the Firecracker MicroVM technology. Then in 2019 they covered as well as how you could then use Lambda to pull just from kinesis and also talked about scaling up with provision concurrency. Now today I'm gonna start of how the different Lambda and then both Chris and into some of the interesting the service since then and solve some interesting challenges to bring new functionality to Lambda. And you can use the QR It's a link shortener for serverless land to watch the previous journey talks. Now as a 400 level talk, we're not gonna go into the but it's worth highlighting how Lambda is the fastest way to with the lowest total cost of ownership. And we strive to help by doing everything that We make your distributed system to solve. Teams can focus on just their code, which avoids all the of making cloud software work. And as a cool bonus, which avoids a lot of Lambda really is the compression algorithm for experience. You can turn code into a with negligible operational efforts. Now this is why serverless A serverless strategy enables you to focus on things that benefit your customers rather than infrastructure management. And we launched Lambda eight and today more than a million customers have built applications that already drive more than a trillion, more invocations per month. And Lambda still continues We see customers building a wide variety of but there are a few common areas where they tend to see huge benefits. Many people start out using Lambda functions each time they launch an EC2 instance. Then teams may then start using or Kinesis Streams or Kafka to build data processing pipelines. And then multiple teams may then start building whole using Lambda often for web applications and use managed Lambda with some other managed services as a critical part of their And of course, we've got many customers also using Lambda in machine learning applications. And we do do a ton of innovation in all areas of the Lambda service to make the complex simple. Give you more, while crucially so you can take advantage of to solve many challenges at any scale. And we think Lambda's been the pioneer in helping developers build from GA in 2015 through support for 15 minute functions in 2018. Things like provision, concurrency and EventBridge as a source in 2019, container images and 10 And in 2021 we brought Lambda extensions and arm-based graviton two. This year, it keeps on going. We've got bigger 10 gig/temp all while adding more languages, shrinking latency and proving integration with other services. For example, Coca-Cola. That's a great customer to the changes brought And they wanted to provide for their Freestyle drink dispensers. And so built a new smartphone app that allows customers to without coming into contact Now because they're built with Lambda, their team was able to rather than spending on security, latency, or scalability. Since with Lambda, that's all built in. And as a result, they built this new application and now over 30,000 machines Now one of the important is that it responds to And as customers create more data streams, a serverless approach is really super appealing. So an example is like is doing this at an incredibly high scale. They are using Lambda to process Yes, that's with a B, all performance and cost using And the scale of the system to use Lambda. On a peak day, Nielsen with 30 million Lambda invocations and the system managed it with no problem, and in fact, they only had up On normal days, they see between one and five terabytes of data and they were able to without additional work. I'm really sure the on-call engineers could breathe a sigh of relief. While we're working on Lambda, we focus on things like availability, utilization, scale, And today we're gonna give you a taste of some of the engineering but we also do have a priority list. The top priorities for Lambda availability and features Now you may hear that security because that comes before everything else. But for Lambda, after then we then focus on operational excellence for like running your functions to ensure that it's in case of a service and even in the unlikely And we ensure that before we even work on new features. With security being job zero, we have the shared responsibility model, which has a separating line which defines how AWS is responsible of security in the cloud. This is the infrastructure and software that runs all our services and you are responsible Sorry I said AWS security of the cloud. And then you're responsible to ensure your applications But when building serverless AWS takes on even more of So you can worry less about many things like operating system patching, things like firewall configuration and maybe distributing certificates. You get to focus even more and let us manage the underlying platform. And a really clear example of this was on December the 9th, 2021, when a seriously bad in Apache's Log4j was Many customers were scrambling to update Java across their environments, yikes, last December, ruining it for so many engineering teams, but, and probably for but, if you were using Lambda, we handled most of it for you. Now, Lambda doesn't actually include log4j in its managed runtimes So you were, in effect, already protected, but if it had been included, you would be patched automatically. Separately, we did also the Java Corretto team, If you did have log4j as part of your Lambda function code, we could block the functionality and log that it had happened. Lambda protected you without And one of the major technological is the open source Firecracker. It's a cool name, I really like it. And Firecracker provides to run functions built and workload isolation with super fast startup times, plenty more coming about Firecracker and how we're innovating in that space. Now Lambda has two synchronous where the caller calls Lambda. And this can be either or with the newish function URLs or through a service like API Gateway, which is then mapped to a Lambda function. The sends the request does its processing, waits for a response before Now when evoked in again via the client call or maybe an S3 change identification or service like EventBridge sending an you don't wait for response You hand off to the events to Lambda and Lambda just handles the rest, it places the event on an internal queue and returns a success &quot;Yep, I got your event.&quot; A separate process then reads events from the internal queue and sends them to your function. There's no actual direct return pass to the calling service. Now in order to do this, we have a number of control which we're going to talk about. Now just to tell you, despite some of the rumors I hear, this talk isn't going to be about how we've entirely rebuilt Lambda to run on Kubernetes, but just in case you wondered, we do not in fact run but we do have a control plane service, which is where you interact and this is where you create, configure, and update your Lambda function and upload your code to that we expose. We manage the Lambda to make sure they're available There are also a number of developer tools to interact with Lambda, which also cross over into And this is things like from SDKs, AWS toolkits for various IDEs, and infrastructure as code tools and then the data plane responds to get events to the land of service. And all evokes land up with a number of internal services. Now these are the data plane services and the eagle-eyed may also spot a service we And this is called the assignment service and we've been evolving the and I'll go into more depth later on how we solved some and built the assignment service Now also the async event, invoke data plane with the Lambda pollers, handle the very powerful to process events and ultimately hand them to the synchronous invoke data plane. So let's have a look at how Requests to invoke a via an application load balancer, which distributes the invoke requests to a fleet of hosts, which then run the stateless front end invoke service. Now Lambda is a multi-AZ service, so you don't have to your functions across multiple AZs, Lambda just handles that for you. Now I am simplifying the it does get a little crazy, but Lambda is built so all its components run transparently across multiple AZs. So, the front end service and authorization of the request, and this ensures that as only authorized callers even get through Lambda's front door. The service loads the metadata and caches as much information as it can to give you the best performance. The frontend then calls And this checks whether any quota limits may need to be enforced based on things like account, burst, or And the counting service is and sub one and a half millisecond latency because it's called on each invoke. And therefore, it's and because so, it's as with a lot of Lambda, The frontend then talks which has replaced the and it's a stateful service and there's gonna be a in our talk today. So this is responsible, like for routing invoke requests If you wanna think about it, the worker is the server in serverless. It's responsible for creating within a microVM to download, mount, and run your function code. The worker is also responsible for managing the multiple run times and this is, you know, for languages like Node, Java, and Python and any other kind of languages you bring. It's also involved in setting based on your function configuration and then the associated And, of course, needs to on the host that monitor Now here is also another component which does some coordination And this is the control plane service, which handles function creation and manages the lifecycle and then also ensures the frontend service is up to date with which assignment node to send an invoke request. So back to the assignment service, this is the coordinator, and distributor about what are on the worker host and where invokes should ultimately go. It also keeps the front end with which assignment For the first invoke of a function, a new execution environment needs to be started on a worker host. So the assignment service to create an execution environment on a worker with a time-based lease. Now, placement uses ML models to work out where best to place an and where they do this by for better fleet utilization while still maintaining and minimal cold path latency. It also monitors worker health and makes the call when as unhealthy. And we need to make sure to reduce latency for So once the execution the assignment service for that function that with the privileges that you define. It's then gonna distribute that role to the worker along with The execution environment then and downloads the function code and the function The assignment service then &quot;I'm all ready,&quot; which then sends the invoke payload to that execution environment. The function then runs the handler code and sends its results back to the original caller for The worker also then notifies &quot;I'm finished with the invoke,&quot; and so it's then ready to Then when a subsequent invoke comes in, frontend checks with account, checks the quotas with and then talks to the &quot;Yes, there is actually a warm and idle execution environment ready.&quot; and sends the invoke directly, payload directly to the execution and your function can When it's finishing with the invoke, The worker then tells the I'm now ready for another Then when the lease of is nearing its endtime, this assignment service for any future invokes. And then we have a sort of further process that spins down that Also, we need to handle errors, so if there are any errors within the in it phase, in the assignment's able to then and then it's removed from service. Also, if we need to remove maybe when it needs to be refreshed, which we do regularly we can actually gracefully drain it of execution environments Current invokes will carry on, but we just stop any future ones. So let's switch to talking or also what is called event invokes. And you can see here we have the event invoke frontend service. Now you may remember we had a frontend invoke but they're definitely different. Well, the event invoke behind the same frontend but the load balances see and so send it to the event rather than the frontend invoke service. And we actually deliberately building a new data plane to ensure that the async invoke path was separate to the sync invoke path to protect against a large amount of invent invokes potentially causing latency Just another way that we and performance on your behalf. Now, I'm simplifying the to the sync invoke, but again, it's all spread across and the service again is and authentication of the request based on who the caller is. So, bit of a simplified diagram, but please remember it's multi AZ, so we've done the authorization and then the frontend is to an internal SQS Queue and respond with an acknowledgement to the caller that Lambda is gonna invoke Now Lambda's gonna manage these SQS queues and you don't really have or Now, we also run a number of queues and we can dynamically scale them up and we can scale them and the function concurrency. Now, some of these queues are shared, but we also send some to ensure Lambda can handle a huge number of async invokes with as We then have a fleet of polling instances, which we manage, and then reads the message to determine the account function and what the payload is gonna be and then it sends it then synchronously to the And as you can see, all Lambda invokes ultimately The function's then gonna use I talked about before and The sync invoke is then gonna return the function response and this is gonna be to the event invoke poller, which is then gonna delete as it's been successfully processed. If the response is not successful, the poller then's gonna return And this uses exactly the same as you would with your own SQS queues. Then the same or maybe another poller is gonna then be able and it's gonna try again. You can also configure event destinations for async invokes to provide and this is whether the or maybe they've failed after There are some additional The queue manager looks after the queues, it's gonna monitor them for any backups, and then also does the creation and deletion of the queues. It's gonna then work and this manages which pollers and it's also gonna detect or isn't doing its job can then be passed to another poller. Now, when we built the Lambda async model, we realized, well, this idea can be used for a number of other So an event source mapping that reads from a source. Now initially this was only but we've expanded this to Kafka sources, including Yep, Lambda can even pull as well as Amazon MQ4, The pollers pull messages can optionally filter them, batch them, and then send them to your Let's talk a little bit more A produce application onto the stream or queue asynchronously. We then run a number of slightly as the pollers have on the event source. The pollers are then from the stream or queue and And this is a super useful functionality just built right into the system. It helps reduce traffic to your functions, simplifies your code, It is then gonna batch into a single payload to your function synchronously via the same sync frontend service again or Lambda invokes ultimately And then, as with SQS, for the poller can then delete when your function And then for Kinesis and Dynamo, you can actually send from the poller to SQS or SNS. Now the cool thing to these pollers for you as and that's a huge benefit. You don't have to run and scale your own consumer EC2 instances or a fleet of containers, maybe, to pull for the messages, and it's actually free. You don't pay extra above for the service. It's just more of what There are also some other involved here, the state and that's obviously is gonna manage the pollers It's gonna discover and then is gonna handle up and down. The leasing service then to work on a specific event and if there's a problem with a poller, it's gonna move its work And having these poller fleets allows us to support a huge number Now there's a lot on the slide, but basically means that we can do things like filtering and configuring batch sizes and windows and reporting a sort of whole raft of settings. And this is depending on the event source. And you configure these actually on the event source mapping itself or yes, sometimes on the original event source, but it really makes it at scale with Lambda. So that covers the sync, and how that works with Lambda. Now let me hand you over to Chris Greenwood. (audience applauds) - Thank you Julian. My name is Chris Greenwood, and I'm a principal My goal here today is to Lambda is a storage service. My first position with AWS was working on elastic block store or EBS. EBS is a large distributed storage service that backs the majority of in the world today. The service is faced with many of the interesting storage challenges that you may have heard about from talks from the storage track. A few years ago, I joined Lambda expecting to find a much different problem space, more serverless, more ephemeral, more stateless, but in I found state management to those I faced in EBS. What I came to realize is that Lambda is a stateless serverless the function developer, but on the back end involves many of the state management challenges that are present in a storage service. To some degree, Lambda that you don't have to think about. So for the next segment of this talk, I'd like to look at Lambda And we're gonna talk about three lessons that Lambda has learned from our peers on the storage service improve the Lambda service. The first is that access patterns the storage layout of that The second is that shared state and for utilization of a storage service, and that the best kind without sharing state, and I'll explain what that And the third is that a often has an equally into the caller. In other words, you meet where the caller is. We'll start with lesson one. So to frame the topic, what state needs to be managed by Lambda in order to serve an invoke? Fundamentally, an invoke First, it involves invoke input. Invoke input is the JSON or provided by the event Second, it involves code. Code is provided by the function developer when creating or updating the function. And third, it requires a invoke input, and code That's two pieces of state and one piece of state and compute in the virtual machine the necessary bits of Julian gave us an overview and the poller fleet, which together serve to get invoke input to the correct machine Now we're gonna talk about code. When Lambda launched to in order to spin up an Lambda did the simple thing. We downloaded that code from S3. Each environment started that would discover which code payload governed the function, would download that code payload from S3, unpack it into the environment, start the runtime. Starting that runtime would essentially turn that code into a running VM and make that running VM ready to take invoke input and execute. This worked just fine that the majority of Lambda functions in the world work today. But as our requirements changed, so did our architecture. In 2020, we set out to build container packaging support This changed state management in a big way in that code payloads While zip code payloads are limited to 250 megabytes in size, we realized that container and we decided to support in size for container packaging in Lambda. However, the code delivery architecture for Lambda made it such to download a piece of code and unpack it into the environment scaled and we felt this was unacceptable for the new code payload requirements of the Lambda container packaging product. This meant that we had by which we deliver a piece of code into the microVM, or, sorry, into the execution environment. We, along with the community, In many containerized workloads, a container accesses a when actually serving requests. This chart from a Harter paper published in the FAST conference in between total container image size, either compressed or uncompressed, and the total amount of on the path of a request. While the mechanism Lambda than that of the Harter paper, we realized that if we to the execution environment only the bits of a container image that were necessary to serve a request, then we could get the started more quickly and we could amortize code over the lifetime of the So, how to go from all at once to incremental loading The first thing we had to do was to change the way the container images were persisted in the What we're looking at that, what a simple for a container image When you build a container Docker is going to produce a Lambda takes those layers and flattens them into a file system. This is the file system that is present when your execution environment starts up. Once we have the flattened file system, we break its binary representation on disc into chunks on the block device. A chunk is a small piece one logical range of the block device, and multiple chunks appended together make up the entire block device and the entire file system. Chunking is powerful as it lets us store contents and then also access those contents at a sub image granularity. Let's consider the example into an execution environment. When the execution environment starts, it knows nothing of the First access of the file system, say, to list the contents of root, results in an inode read Lambda software maps that in the container image to serve the read. A subsequent read to a to open and start the Java binary, may fall into the same chunk, which means that Lambda needs to serve that inode read. Future reads such as opening for your function may and chunks are loaded as they're needed. This means that all up, Lambda delivers to the execution environment, the contents that are needed made by the VM and does that are not needed. In this way, Lambda We let access patterns to our data, specifically the fact are sparsely accessed, to influence the storage layout of our container storage subsystem, specifically the fact that we chunk and incrementally persist and access blocks of container images. Moving to the second lesson, sharing state without sharing state. The next thing we realized is that they often share This is for the simple reason that when people build with containers, they often don't start from scratch. They use base layers. A customer of Lambda may lay on top of that the and finally, lay on top of The Lambda data plan and which data is unique is helpful in optimizing code delivery. But it turns out the deduplication is difficult in this The first is related to the is block-based instead of file-based. To allow two copies of to share contents at the block level, that file system must onto a file system and that file system onto a block device. But many file systems don't do this. In some file systems, if you take the same set flatten them onto a file system, and then serialize those file you get a different binary representation in that block device for the To solve this, we did some to ensure deterministic behavior when flattening and serializing to disc. The second reason deduplication is hard is related to key management. We need different keys of different customers but different keys mean even if the contents are similar, which prevents deduplication and at the caching layer. So ultimately to benefit we need to use the same when appropriate. A simple way to do that would be to depend on a shared key in a shared key store, but that results in a across multiple customers and What we'd like is for the but for the discovery of and persistence of those shared keys to where the data itself is stored. To solve this problem, we turn to a technique When encrypting a chunked image, we start with a plain text chunk, which represents one segment We take that chunk and that is deterministically Then we compute the hash of This hash becomes the unique That key is then used to and the extra data with an Including the extra data in to verify that the at rest and in flight We do this for each writing encrypted chunks and keeping track of the keys for each. When complete, we create a manifest containing to chunks in the chunk store. We finally encrypt that manifest with a customer specific KMS There are a few great things First, assuming we use we produce the same key from the same chunk contents every time. This allows us to securely that should be deduplicated, this allows us to do so while having each image creation process without depending on a shared Second, without changing and without changing the chunk, by simply changing the extra data, we can force two chunks to even if their contents are the same. This is helpful in ensuring, for instance, that contents in different AWS regions don't share encryption keys even if the contents are identical. So in this way, Lambda We improve cache performance and economics to help overall performance of the storage subsystem, the shared resources on during the process of creating and then accessing a container image. To recap, we've covered invoke, input delivery, and code payload delivery. The third aspect of state is the place where code and A quick recap of Lambda's history with virtualization architectures. Lambda operates a large dedicated to running the in a region. At launch of the service, Lambda provisioned a T2 and dedicated a T2 instance to each tenant to run one or more of that This leveraged EC2 to isolate one tenant from another. Simple and secure. When a request was handled it would check for the existence of an existing execution And if one did not exist, it would provision a new T2 instance and attribute it to that customer with the information necessary to download the code and At scale, this meant in the fleet and at any many instances in the process for a function or being to recycle to a different function. This brings us back to state management, but from a different angle. Well, with code management, our goal was to scale up the amount of state managed to manage larger code payloads. With our VM fleet, our goal was to scale down We wanted to provision as memory, and disc resources for each VM. Reducing overheads is good for efficiency, both the efficiency of compute at rest and the efficiency with into the fleet and bring This need to scale down our led to the introduction of Firecracker into the Lambda data plane in 2018. Firecracker is a virtual machine manager that Lambda runs on bare It uses Linux's KVM to of microVMs on each worker. This allows Lambda to benefit from multi-tenant worker instances all while using secure VM isolation between execution Firecracker handles IO between and the host kernel. Within each microVM is that the Lambda customer's used to. Including the runtime, and including any configured extensions. Firecracker allowed and right size while maintaining this secure VM boundary between environments and between tenants. Instead of allocating an entire we were able to allocate potentially from an existing instance, to each execution environment. Less overhead per VM made and also made the operations and tearing down old This move to Firecracker as allowed us to leverage meeting our caller where they are. So now in our high level architecture, we have our chunks, encrypted image and we have the microVM that intends to make use of that image. But somehow, we need to expose into the OR file system into the microVM. And in this regard, our job was a little bit than it was for zip packaging. Let's focus in on a single microVM on a single host. With zip packaging, since Lambda owns and manages or the guest environment, we are able to employ a small that knows what to do to start the execution environment. But with container packaging, the whole concept is that is customer provided, leaving nowhere in the guest to implement image handling functionality such as chunking, decryption and so on. With our EC2 virtualization stack, this would've been the end of the road, as running in the guest We were, in fact, the guest of EC2. But running Firecracker on bare metal, we had space to securely run service code outside of the customer So we built a virtual file system driver that presents an EXT4 file system from the worker into the VM. When the VM issues requests they're handled by Lambda Our file system implementation to deterministically map inode to chunk accesses in the image. It interacts with KMS to caches chunk metadata, and serves file system rights from an overlay that is In serving reads, the file system consults from a host local cache, to an authoritative chunk store in S3. And all of this chunking, decryption, and cache sharing work is abstracted from the customer behind a In other words, we meet the caller where they are, abstracting away the storage complexity behind an interface that the customer is used to and is expecting. This mostly completes our storage journey, from management of invoke input to management of code and container images to the presentation of code into the VM. However, we realized an to leverage all of this and all of these lessons learned to meet the evolving Let's talk about the elephant in the room when it comes to Lambda Over 99% of invokes to Lambda are served by an execution environment that is already alive These are warm starts. but occasionally due to idleness or due to scaling up of incoming calls, An invoke must spin up a before running function code. This is a cold start and it impacts a function's Spinning up a new like launching the VM, but also critically involves the steps of starting the runtime and These steps of starting the runtime and running function initialization can dominate cold start time, especially for languages like Java with a language virtual And this time and cost must be paid on every single execution into the fleet in service So while cold starts can be rare, they can also be very impactful to the end customer experience. At Lambda, we track our control plane and invoke latencies at the P 99 level, P 99.9 level, and above. Outliers are rare but have outsized impact on a customer experience. So outliers matter to service owners. Thinking about the initialization what this process effectively does is it takes a piece of code sitting in the virtual machine and it turns that into a running VM that is then So what if we could learned by Lambda to avoid of converting code into a running VM in the first place? To do so, what if instead of we were to just deliver an actual running VM to the host. This is essentially what we're doing with Lambda SnapStart. SnapStart automates the management of execution environment snapshots to significantly improve of Lambda functions. It is initially available for and it's powered by the open source work in Firecracker supporting microVM, snapshot, and restore. With SnapStart, the life cycle of an execution environment is different. When you update your function and publish a new function version, Lambda will asynchronously download code, unpack code, and initialize the execution environment by running customer function code up to the point where it is And critically, no Lambda will then take a VM and persist the resulting Then on the invoke path, if a new execution environment is needed, Lambda will restore a new microVM from that persisted snapshot. After a short restore phase, the execution environment will The net result of the application is that long tail latencies in many cases by up to 90%. You might notice one that actually did not from enabling the feature. But interestingly, upon making this application achieved of the applications sampled. Most functions will see with only the configuration change. But occasionally a function may need simple code changes to benefit. As with all performance features in AWS, try it out and let us know how it goes. And this improvement to was made possible by converting code into a running VM, into a storage problem, of delivering a VM snapshot to a host. So in summary, Lambda has employed a few lessons learned by storage services to improve the performance, efficiency, and overall experience Lesson one is that we use to influence how data is laid Lesson two is that shared state and performance and that the comes without actually sharing resources. And lesson three is that storage services spend a ton of time meeting their caller where they are to hide in a storage service from a customer. I'm now gonna invite for the last segment of this talk. Thank you. (audience applauds) - Thanks Chris. What a great story. Anybody happy that we and solve Java cold starts with some pretty cool Anybody? Excellent, good to hear. Well, Chris has been and how he views lessons from storage to solve moving some to support container images and SnapStart. But we also have state to Yes, it may be a little but it's important to getting to the execution environment. So remember I said earlier, we had the Worker Manager service, which was the coordinator between the frontend invoke service and the worker. Well, it had a super important job to help the front end get invokes to the execution environment and then manage the execution lifecycle. Well, we had an issue, which was one of the and to be honest, Each way, Worker Manager stored a list of execution for on which hosts. Looking at it slightly the state of any individual was known to exactly one which stored it in memory. Nice and fast, but stateful and not replicated, no redundancy. We had a problem with So in this example, a if you can see the different color, manages a whole bunch of on a number of worker hosts. And we did have a control plane service to then manage the Worker Managers. If the Worker Manager fails, all those execution environments it was looking after are orphaned. Yep, they continue to handle but for any new invokes, the front end has to ask for an available execution environment. Other Worker Managers don't know about the existing warm execution environments because they're orphaned then has to create new ones. And that's a bad customer experience, of a cold start when there is actually a warm execution environment available, but it's orphaned. This also means we need to in the Lambda service to run these additional until we can reap the orphaned ones, which we do. But this impacts how And the issue gets even about how Lambda distributes particularly for smaller regions. Here we have an execution that's actually owned by If we have a zonal failure in AZ2, which, of course, is extremely rare, but we do need to plan for in AZ2 obviously along with all execution environments So that's one third of in a region, gone, unavailable. Yet as there are execution that are registered to all those execution environments, or those still up and running, are not impacted by the zonal So that works out to within each of the other two AZs for a combined total of about 55% of execution environment Now of course we hold to handle this sort of failure, but that means a large capacity buffer and a poor customer experience as each execution environment that needs recreating means a cold start. And this also means the to be scaled enough to handle this huge increase in additional requests. And so we decided that Worker Manager needed a refresh and built Instead of a single worker manager and a single AZ managing a across multiple AZs. We built the assignment service, which is made up of three node Looking at this logically, a single partition consists of each in different AZs. And we run many partitions Each assignment service host The assignment service partition members use an external journal log service to replicate execution environment data. The leader writes to the journal and then the followers from the journal to keep up to date with the assignments. And then the partition the log journal approach The frontend talks to the leader and the leader communicates to create new execution environments and keep track of assignments on the worker hosts and then which the followers read. And then if the leader fails, a follower can take over really quickly and we don't lose states of which execution to service subsequent invokes. This means, in a zonal outage, execution environments in the which means fewer cold and less load on the placement service. It also means the assignment service has much better static stability. The system maintains We don't need an external service to keep the system running Good static stability is towards in AWS and Lambda. A good way to maintain state. When we do then bring up or maybe when we need to add and remove assignment service nodes for maintaining the system, we can actually bootstrap the by the partition by just reading from the time of the oldest to quickly get up to date. We still do have an assignment and this is gonna manage the and also ensure the frontend to talk to you for a So the assignment service against host, network, using a partition leader approach. And we also did manage by moving some responsibilities elsewhere. It's also written in Rust for performance, tail And altogether triples the number of transactions per second with a meaningful reduction in latency. So all the efficiencies we're means that we can get better utilization and this has a direct impact How efficiently can we run a workload given a specific set of resources? And we do a ton of work Due to the small footprint of functions and our ability to distribute the workload to fit the curve of our resources, we can be the most With Lambda, you only pay when your functions are doing So it's our job to minimize the idle. And inside Lambda, we and reuse as much as possible. And we are continually to be more efficient running Lambda and also improve your So we have systems that help us analyze the resources needed over time to optimally distribute workloads and provision the Now for a given function in well, you may think that is the best way, but it means you miss out Things like cache locality, which you've heard about, is super important and the So it's actually better to have some concentration The worst for efficiency is a It has a specific pattern and is inefficient with resource usage. It's better to pack so the workloads are not as correlated. But we actually take this a step further. We use models and machine learning to pack workloads optimally together to minimize contention and maximize usage while securely caching common data across different functions, which also improves aggregate And we actually have an entire team that works just on this placement problem with a distinguished professor and a team of research scientists. And this is all part of the to be the best place to handling as much of the hard especially with state. So you can have the fastest way with a total of the lowest Now to increase your you can use the QR code to learn at your own pace, and as of yesterday you can if that's your thing. For plenty more general head over to serverlessland.com. This has got tons of resources about serverless on AWS. And lastly, thanks so much for joining us. Chris and I really appreciate to be with us and we to look a bit under the hood of Lambda and help you know how it works and some of the challenges that we And then lastly, if you do like deep, a bit of a bribe, but a five star rating, the session survey certainly lets us know that you'd like more and Thank you very much and enjoy (audience applauds)