A linear regression is one in machine learning. It is a statistical model that attempts to between two variables. with the linear equation. Hello everyone. This is Atul from Edureka will learn about But before we drill down algorithm in depth, I'll give you a quick overview So we'll start a session with a quick overview as linear regression of regression algorithm. Once we learn about regression, it's use case the various We'll learn about the algorithm you it's mathematical then we'll drill down and Implement linear in today's session will deal with linear regression algorithm its goodness of fit or how close the data is to the fitted regression line And then finally, what will do will optimize method in the last part I'll teach you to implement and the coding session The first part would consist of linear regression where you will use that you have learned And in the next part will be using scikit-learn of linear regression. All right. I hope the agenda is clear so let's begin our session Well regression analysis is a form of predictive which investigates between a dependent a regression analysis over a set of data points that most closely fits or regression shows the changes on the y-axis to the changes in the explanatory variable Now you would ask Well, they are major Let's has the first the strength of predictors, 's the regression might be used to identify that the independent variables For example, you Like what is the strength and marketing spending or what and income second is forecasting and effect in this to forecast effects That is the regression analysis how much the dependent variable in one or more For example, you how much additional sale income will I get for each thousand So it is Trend forecasting to predict Trends The regression analysis can in this you can ask questions. Like what will be in next six months, right? So next topic is linear By now, I hope what a regression is. So let's move on So there are various like linear regression logistic and others early, but for this session will be focusing on linear So let's move on and let me tell And what is logistic regression then what we'll do All right. So starting with in simple linear regression, we are interested in things So what we are trying to find and Y variable this means that every value of x has if it is continuous. All right, however we are not fitting our data like linear regression instead We are mapping Y versus X to a sigmoid function What we find out is is y 1 or 0 That's we are essentially for a given value of x fine. So as a core concept you can see that the data is We're in the case The data is model using The linear regression is used on the other hand It is used with categorical or the prediction is the value of the variable on the other hand the output of prediction of a logistic of occurrence of the event. Now, how will you and goodness of fit in case of linear regression like measured by loss r squared while in the case have accuracy precision which is nothing but and recall next is Roc curve for determining the probability or the confusion Matrix Etc. There are many. All right, so between linear and logistic that the type of function you are mapping to is between linear and logistic Maps a continuous X2 a continuous fi a logistic regression to the binary why so we can use logistic category or true false decisions from the data find so let's move on ahead next is linear or you can say when will So the first is classification and regression capabilities a continuous variable such as or predict the temperature of a city their Reliance like a straight line poses a real challenge when it comes towards building Let's imagine that you fit that you have to imagine you add But in order to fit it you have to change That is Maybe. Between the threshold itself. So this will happen to the model hence. The linear regression is not Fine. Next is data quality removes one data point that in simple linear regression. The outliers can significantly disrupt the outcome you can know that if you will become very good. All right. So this is about data quality a linear regression is often as compared to the decision tree or the clustering algorithm for n training example and X features usually Falls square or bigger of xn and transparent the easily comprehensible They can be represented by to anyone and can be So these are some on which you will select Alright next is where is linear regression and sales estimate. Well linear regression to evaluate Trends For example, if a company sales have for past few years, then conducting a linear with monthly sales on the y axis This will give you a line that predicts the upward Trends the trendline the company of the lines to focus Next is analyzing. The impact of price changes can be used to analyze on consumer behavior. For instance. If a company changes the price on a certain then it can record the quantity and then perform with sold quantity as a dependent variable and price This would result in a line to which the Reduce as the price is increasing. So this result would help us Next is assessment of risk and insurance domain for linear regression for example health insurance a linear regression algorithm how it can do it can do it per customer against its age that the old customers then to make more Well the result of such analysis might guide All right, so by now you what linear regression what it does where it is used when you should use let's move on and understand So suppose you have independent and dependent variable All right suppose this is The independent variable And so does the dependent So what kind of linear you would get a positive All right as the slope would You have an independent which is increasing and on the other hand the that is decreasing. So what kind of line You will get In this case as the slope and this particular line that is line of y equal MX plus C is a line which shows the relationship and dependent variable and this line is only known Okay. So let's add some data So these are some observation Let's plot some more. Okay. Now all our data points to create a regression line All right. Now once our regression it's the task This is our estimated value and this is our actual value. Okay. So what we have to do our main that is to reduce the distance or the predicted value The best fit line would be the or the least difference and the actual value. All right, and other words we This was a brief understanding of linear regression We'll jump towards All right, but for then Suppose you draw a graph and distance covered on the y axis with the time If you plot a graph by the vehicle and the distance traveled then you will get All right. So suppose the equation Then in this case Y is in a fixed duration of time x is the speed of vehicle m of the line and see is All right suppose You have to plot a graph and the time taken Then in that case with a negative relationship. All right, the slope the equation of line changes where Y is the time taken X is the speed of vehicle m is of the line and see is All right now, let's get back So in that term why is and X that is Now, let's move on and see the mathematical implementation Alright, so we have x equal 1 2 3 4 5 let's plot So 0123456 alike All right. So let's plot 1 2 3 4 5 on the y-axis. Now, let's plot our coordinates 1 by 1 so x so we have here x equal 1 and y equal 3, so there's a point we have 13243244 and 55. Alright, so moving on ahead. Let's calculate the mean of X All right, so mean of X is 1 plus 2 plus 3 plus 4 That is 3. All right. Similarly mean of Y 2 plus 4 plus 5 that is 18. So 18 divided by 5. That is nothing but 3.6 aligned. So next what we'll do that is 3 comma 3 .6 Okay. So there's a point 3 comma 3 .6 see our goal is to find using the least Square So in order to find that we first need to find so let's find the equation All right. So let's suppose this is our regression line Now, we have So all we need to do is where m equals summation of x minus X bar X Y minus y bar minus X bar whole Square Let me resolve it for you. Alright, so moving on What we are going to do So we have X as 1 minus X bar It is minus 2 next we have x that is minus 1 similarly. We have 3 minus 3 is 0 4 - All right, so x minus X bar. It's nothing but the distance through the line y equal 3 and what does this y minus y bar implies of all the point from the line x equal 3 .6 fine. So let's calculate the value So starting with y equal 3 - value of y bar So it is three minus 3.6 of 0.6 next is 4 minus 3.6 that is - of 1.6. Next is 4 minus 3.6 5 minus 3.6 that is Alright, so now we are done Fine now next we will calculate So let's calculate x so it is minus 2 whole square. That is 4 minus 1 whole square. That is 1 0 squared is So now in our table we have x and x minus X bar whole Square. Now what we need. We need the product of x Alright, so let's see minus X bar X Y minus y bar that is minus that is 1.2 minus by zero point 4 that is minus of 1.6. That is 0 1 multiplied that is 0.4. And next 2 multiplied All right. Now almost all the parts So now what we need of last two columns. All right, so the summation of X minus X bar and the summation of x minus X bar X Y minus y bar is 4 to 4 by 10 fine. So let's put this value and our line y equal MX plus C. So let's fill all the points and find the value of C. So we have y as 3.6 remember calculated just now X that is 3 and we have the equation as 3 point 3 plus C. Alright that is 3.6 equal So what is the value of C That is 2 point 4. All right. So what we had we had m as 2.4 and then finally when we calculate the equation what we get is y equal plus two point four. So this is the regression line. All right, so there is how you This is your actual point. All right. Now for given m equals Let's predict the value of y So when x equal of y will be zero point 4 x one Similarly when x equal of y will be zero point 4 x 2 + 2 point 4 that equals equal 3 y will be 3 point 6 x equal 4 y will be 4 point 0 x equal 5 y will be So let's plot them on the graph and the line passing through and cutting y-axis of regression now your task between the actual and the predicted value and your job is Like or in other words, you have to reduce the error and the predicted value the line with the least error will be or regression line and it will also be Alright, so this is So what it do it performs for different values of M It will calculate where y equals MX plus C. Right? So as the value is changing so iteration All right, and it will perform So after every iteration what it will do it will according to the line and compare the distance to the predicted value and the value of M between the actual and the predicted value is as the best fit line. Alright now that we have calculated the best to check the goodness how good our So in order to Do that. We have a method So what is this R square? Well r-squared value is how close the data are to the fitted regression It is considered that a high r-squared but you can also have a lower squared value or a higher squared that does not fit at all. All right. It is also known as or the coefficient Let's move on and see So these are our actual values We had calculated of Y as 2.8 3.2 3.6 4.0 4.4. Remember when we calculated of Y for the equation Y plus two point four for every x We got the predicted All right. So let's plot it on the graph. So these are point through these points are nothing All right. Now what you need to do is you have to check and compare mean versus the distance Alright. So basically what you are doing of actual value to the mean to the mean I like so there is nothing you can represent of Y predicted values by summation of Y minus where Y is the actual value y p is the predicted value that is nothing but 3.6. So remember, this So next what we'll do So we have y is 3y bar as it as 3 minus 3.6 that is nothing but for y equal 4 We have y minus y bar as It is 1 point 6 4 minus and five minus 3.6 it is 1.4. So we got the value Now what we have to do we So we have minus of 0.6 Square of 1.6 Square as 2.56 0.4 Square is 1.96 now is a part We need our YP So these are VIP values and we have to subtract No, right. So 2 .8 minus 3.6 Similarly. We will get 3.2 minus 3.6 that is 0.4 and 3.6 minus 3.6 that is 0.4. Then 4 .4 minus 3.6 that is 0.8. So we calculated the value it's our turn to calculate y bar whole Square next. We have - of 0.4 Square as 0.160 Square again 0.16 and All right. Now as a part of formula what it suggests it suggests minus y bar whole square and summation of Y minus All right. Let's see. So in submitting y minus y bar whole Square and summation of Y P minus y bar whole Square you So the value of R square 1 point 6 upon 5.2 fine. So the result which will get Well, this is not a good fit. All right, so it suggests that the data points are far Alright, so this is how your graph will look when you increase the value So you'll see that the actual value would like when it reaches and when the value to 1 then the actual values lies for example, in this case if you get a very low value So in that case what will see far away from the regression that there are too You cannot focus All right. So this was all about you might get a question of Square always bad. Well in some field it that I ask where For example any field that attempts to predict human typically has r-squared values through which you can conclude that humans are simply harder to predict on the physical If you ask what value is low, but you have statistically then you can still about how changes in the predicator values in the response value regardless of the r-squared the significant coefficient in the response for one unit while holding other predicate is obviously this type of information can be All right. All right. So this was all about let's move on to the coding part and understand the So for implementing I'll be using Anaconda with jupyter notebook So I like there's and we are using python 3.01 it alright, so we are going of head size and human brain All right. So let's import our data set We are importing numpy as NP pandas as speedy a totally we are importing Alright next we will import and store it Let's execute the Run button So this task symbol, it symbolizes that So there's a output of two thirty seven rows We have columns as in centimeter Cube and brain weights So there's our sample data set. This is how it looks it consists So now that we so as you can see they are so we can find a linear. And Chip between the head size So now what we'll do the X would consist and the Y would consist So collecting X and Y. done next what we'll do of b 1 or B naught So we'll need the mean of X what we will do will calculate equal NP dot Min X. So mean is a predefined function underscore y equal so what it will return if you'll return next we'll check So m equal length of X. Alright, then we'll use the formula of b 1 + B naught or MNC. All right, let's execute the Run button and see So as you can see here on the screen we have got and be not as three twenty Alright, so now So comparing it with You can say that brain weight head size plus 3 Seven so you can see that the value of M 2 6 3 and the value of C. Here is three twenty All right, so there's Now, let's plot it and see So this is how our plot looks but we need to find out So in order to find it like root mean Square method or the a square method. So in this tutorial, I have told you So let's focus on that and see So let's calculate All right here SS underscore of square SS underscore R of residuals and R square as the formula is of squares upon total sum All right next you will get the value which is pretty Good now that you have implemented using least Square method. Let's move on and see the model using machine learning All right. So this scikit-learn is a simple in Python welding machine very easy using scikit-learn. So suppose there's So using the scikit-learn to this length like so let's execute will get the same are this was all for today's you have any doubt. Feel free to add your query Thank you. I hope you have enjoyed Please be kind enough to like it and you can comment any and we will reply them at the earliest. Do look out and subscribe to Edureka Happy learning.