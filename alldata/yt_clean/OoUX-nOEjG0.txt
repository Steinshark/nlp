Okay, so welcome to lecture two of CS231N. On Tuesday we, just recall, the big picture view of what is the history, and a little bit of the And today, we're really going into the details. And we'll start to see, exactly how some of actually work in practice. So, the first lecture of the class is probably, sort of, the And the majority of the will be much more detail orientated, much more focused on of these different algorithms. So, today we'll see our and that'll be really exciting, I think. But, before we get to that, I wanted to talk about a couple One, is Piazza. So, I saw it when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza. Which means that there who are not yet there. So, we really want Piazza of communication between the So, we've gotten a lot of about project ideas or questions or poster session attendance. And, any, sort of, questions like that should really go to Piazza. You'll probably get answers on Piazza, because all the And it's, sort of, easy in the shuffle if you just It's also come to my attention are having a bit of a hard SCPD students are supposed to receive a @stanford.edu email address. So, once you get that email address, then you can use the Stanford Probably that doesn't sitting in the room right now, but, for those students listening on SCPD. The next administrative issue Assignment one will be up later today, probably sometime this afternoon, but I promise, before it'll be up. But, if you're getting a little bit antsy and really want to start then you can look at last year's version of assignment one. It'll be pretty much the same content. We're just reshuffling it like, for example, upgrading rather than Python 2.7. And some of these minor cosmetic changes, but the content of the as last year. So, in this assignment you'll k-nearest neighbor classifier, which we're going to talk You'll also implement several including the SVM and Softmax, as well as a simple And we'll cover all this content over the next couple of lectures. So, all of our assignments If you aren't familiar then we have written a on the course website to But, this is, actually, pretty important. NumPy lets you write these operations that let you do in just a couple lines of code. So this is super important for pretty much all aspects of numerical and everything like that, is efficiently implementing And you'll get a lot of practice with this on the first assignment. So, for those of you who with Matlab or NumPy or tensor computation, I recommend at this assignment pretty early and also, read carefully The other thing I wanted to talk about is that we're happy to announce that we're officially supported for this class. So, Google Cloud is somewhat You can go and start virtual These virtual machines can have GPUs. We're working on the tutorial Google Cloud and get it to But our intention is that some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud. And because Google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use for the class. So you can feel free to use and also for the course projects when you want to start using and whatnot. So, we'll post more details about that, probably, on Piazza later today. But, I just wanted to mention, because I know there had about, can I use my laptop? Do I have to run on corn? Do I have to, whatever? And the answer is that, Google Cloud and we'll provide Yeah, so, those are, kind of, the I wanted to talk about today. And then, let's dive into the content. So, the last lecture about this task of image classification, which is really a core And this is something throughout the course of the class. Is, exactly, how do we work on this So, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example, and the system is aware of categories or labels. So, these might be, like, or a plane, and there's some and the job of the computer and assign it one of these This seems like a really easy problem, because so much of your own is hardwired to doing these, sort of, visual recognition tasks. But this is actually a for a machine. So, if you dig in and what does a computer see it definitely doesn't get that you see when you look at it. And the computer really as this gigantic grid of numbers. So, the image might be something And each pixel is giving the red, green, and So, to the computer, this is just a gigantic grid of numbers. And it's very difficult out of this, like, giant array very many different numbers. So, we refer to this This idea of a cat, or is a semantic label that and there's this huge gap between and these pixel values that the And this is a really hard problem because you can change the picture that will cause this pixel So, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid, in this giant grid of numbers would be completely different. But, somehow, it's still And our algorithms need But, not only viewpoint is one problem, another is illumination. There can be different in the scene. Whether the cat is appearing or like is this very bright, and our algorithms need Objects can also deform. I think cats are, maybe, of animals that you might see out there. And cats can really assume a and positions. And our algorithms should kinds of transforms. There can also be problems of occlusion, where you might only see part or in this extreme example, from under the couch cushion. But, in these cases, it's pretty to realize that this is probably a cat, and you still recognize And this is something that our algorithms also must be robust to, which is quite difficult, I think. There can also be problems where maybe the foreground could actually look quite to the background. And this is another thing There's also this problem that this one notion of different visual appearances. And cats can come in and colors and ages. And our algorithm, again, needs to work and handle all these different variations. So, this is actually a really, And it's sort of easy to because so much of your for dealing with these things. But now if we want our computer programs to deal with all of these and not just for cats, by the way, but for just about any object this is a fantastically And it's, actually, somewhat miraculous that this works at all, in my opinion. But, actually, not only does it work, but these things work very in some limited situations. And take only hundreds So, this is some pretty in my opinion, and over the we will really see what have made this possible. So now, if you, kind of, think about what is the API for writing you might sit down and try like this. Where you want to take in an image and then do some crazy magic and then, eventually, to say cat or dog or whatnot. And there's really no obvious If you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something you, sort of, can write down an algorithm and enumerate all the in order for this things to work. But, when we're trying or recognize cats or images, there's no really clear, that makes intuitive sense, for how you might go about So, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down I think most people would be in trouble. That being said, people have definitely to try to write, sort for recognizing different animals. So, we touched on this a but maybe one idea for cats is that, we know that cats have ears And we know that edges, we know that edges are pretty important when it comes to visual recognition. So one thing we might try to do is compute the edges of this image and then go in and try to corners and boundaries, and three lines meeting this way, and an ear has one corner and one corner there, and then, kind of, write down for recognizing cats. But this turns out not to work very well. One, it's super brittle. And, two, say, if you want object category, and maybe but talk about trucks or dogs then you need to start all over again. So, this is really not a We want to come up with some for these recognition tasks which scales much more of objects in the world. So, the insight that, sort is this idea of the data-driven approach. Rather than sitting down and rules to try to craft exactly or what have you, instead, we'll go out onto the internet and collect a large and many, many airplanes and different things like this. And we can actually use tools or something like that, to go out and collect a very of these different categories. By the way, this actually to go out and actually but, luckily, there's a lot datasets out there already for you to use. Then once we get this dataset, we train this machine learning classifier that is going to ingest all of the data, summarize it in some way, and then spit out a model that summarizes the these different object categories. Then finally, we'll and apply it on new images that will then be able to recognize cats and dogs and whatnot. So here our API has changed a little bit. Rather than a single function that just inputs an image we have these two functions. One, called, train, that's and then output a model, and then, separately, another which will input the model for images. And this is, kind of, the key insight that allowed all these things over the last 10, 20 years or so. So, this class is primarily and convolutional neural networks and deep learning and all that, but this idea of a data-driven than just deep learning. And I think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones. So, probably, the simplest is something we call nearest neighbor. The algorithm is pretty dumb, honestly. So, during the training we'll just memorize all So this is very simple. And now, during the prediction step, we're going to take some new image and go and try to find in the training data to that new image, and now predict the label A very simple algorithm. But it, sort of, has a lot with respect to So, to be a little bit more concrete, you might imagine working on which is very commonly as kind of a small test case. And you'll be working with So, the CIFAR-10 dataset gives airplanes and automobiles and things like that. And for each of those 10 categories it provides 50,000 training images, roughly evenly distributed And then 10,000 additional testing images that you're supposed to So here's an example nearest neighbor classifier on CIFAR-10. So, on this grid on the right, for the left most column, gives a test image in And now on the right, we've and show the most similar training images to each of these test examples. And you can see that they to the training images, although they are not So, maybe on the second row, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess. But, this image is a dog and a dog, but this next one, or a horse or something else. But, you can see that it because there's kind of a and whatnot. So, if we're applying the to this image, we'll find the closest And now, the closest because it comes from the training set. And now, we'll simply say that a dog. You can see from these going to work very well, but it's still kind of a But then, one detail given a pair of images, how can we actually compare them? Because, if we're going to take to all the training images, we actually have many different choices for exactly what that comparison So, in the example in the previous slide, we've used what's called the L1 distance, also sometimes called So, this is a really for comparing images. And that's that we're going to in these images. So, supposing that our test four by four image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value take the absolute value, and get the difference in that And then, sum all these in the image. So, this is kind of a stupid but it does some reasonable But, this gives us a very concrete way to measure the difference And in this case, we have between these two images. So, here's some full Python code for implementing this and you can see it's pretty because we've made use of operations offered by NumPy. So, here we can see that that we talked about earlier, is, again, very simple, in you just memorize the training data, there's not really much to do here. And now, at test time, we're and then go in and compare our test image to each of and find the most similar And you can see that, we're in just one or two lines of Python code by utilizing these vectorized So, this is something that on the first assignment. So now, a couple questions First, if we have N examples then how fast can we expect Well, training is probably constant because we don't really we just need to memorize the data. And if you're just copying a pointer, that's going to be constant time no matter how big your dataset is. But now, at test time we need and compare our test image to each of the N training And this is actually quite slow. So, this is actually somewhat backwards, if you think about it. Because, in practice, we want our classifiers to and then fast at testing time. Because, you might imagine, and be trained in a data center somewhere and you can afford to at training time to make But then, when you go and deploy the you want it to run on your mobile phone or in a browser or some and you really want the of your classifier to be quite fast. So, from this perspective, this is, actually, a little bit backwards. And we'll see that once we move to convolutional neural networks, and other types of parametric models, they'll be the reverse of this. Where you'll spend a lot of but then they'll be quite So then, the question is, what exactly does this look like when you apply it in practice? So, here we've drawn, what of a nearest neighbor classifier. So, here our training set in the two dimensional plane, where the color of the point or the class label, of that point. So, here we see we have five classes and some blue ones up in the corner here, some purple ones in the And now for each pixel we've gone and computed in these training data, and then colored the corresponding to what is the class label. So, you can see that this is just sort of carving up the space and coloring the space But this classifier is maybe not so great. And by looking at this picture we can start to see some of the with a nearest neighbor classifier. For one, this central mostly green points, but one little yellow point in the middle. But because we're just looking this causes a little in this middle of this green cluster. And that's, maybe, not so great. Maybe those points actually And then, similarly we also like the green region again, due to the presence of one point, which may have been noisy or spurious. So, this kind of motivates of this algorithm called So rather than just looking for instead we'll do something and find K of our nearest neighbors, according to our distance metric, and then take a vote among And then predict the majority vote among our neighbors. You can imagine slightly more Maybe you'd vote weighted on the distance, or something like that, but the simplest thing that is just taking a majority vote. So here we've shown the using this K=1 nearest as well as K=3 and K=5 in And once we move to K=3, you yellow point in the middle is no longer causing the to be classified as yellow. Now this entire green is all being classified as green. You can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. And then, once we move to the K=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. So, generally when you're classifiers, you almost always want which is larger than one because this tends to boundaries and lead to better results. Question? [student asking a question] Yes, so the question is, what is the deal with these white regions? The white regions are among the k-nearest neighbors. You could imagine maybe doing and maybe taking a guess the majority winners, but for this simple example to indicate there was no nearest neighbor in those points. Whenever we're thinking I think it's really useful to kind of flip back and forth between One, is this idea of high and then the other is actually Because the pixels of the image actually allow us to think of these vectors. And it's sort of useful to between these two different viewpoints. So then, sort of taking and going back to the images you can see that it's Here I've colored in red and green which images would actually or incorrectly according And you can see that it's But maybe if we used a larger value of K then this would involve maybe the top three or the top five or maybe even the whole row. And you could imagine that a lot more robust to some when retrieving neighbors in this way. So another choice we with the k-nearest neighbor algorithm is determining exactly our different points. For the examples so far we've just shown we've talked about this L1 distance which takes the sum of the absolute values between the pixels. But another common choice is where you take the square and take this as your distance. Choosing different is a pretty interesting topic because different distance metrics make different assumptions geometry or topology that So, this L1 distance, underneath a circle according to the L1 distance and it forms this square shape thing around the origin. Where each of the points is equidistant from the whereas with the L2 or Euclidean distance then this circle is a familiar circle, it looks like what you'd expect. So one interesting thing to metrics in particular, is that the L1 distance of coordinates system. So if you were to rotate that would actually change the L1 distance between the points. Whereas changing the coordinate doesn't matter, it's the your coordinate frame is. Maybe if your input features, in your vector have some important meaning for your task, then maybe somehow L1 might But if it's just a generic and you don't know which you don't know what they actually mean, then maybe L2 is slightly more natural. And another point here is that by using different distance metrics we can actually generalize classifier to many, many not just vectors, not just images. So, for example, imagine you of text, then the only to use k-nearest neighbors is to specify some distance function that can measure distances or two sentences or something like that. So, simply by specifying we can actually apply this to basically any type of data. Even though it's a kind in general, it's a very when you're looking at a new problem. So then, it's also kind of what is actually happening geometrically if we choose different distance metrics. So here we see the same using the L1, or Manhattan distance, and then, on the right, or Euclidean distance. And you can see that the boundaries actually change quite a bit between the two metrics. So when you're looking at tend to follow the coordinate axes. And this is again because of coordinate system. Where the L2 sort of doesn't coordinate axis, it where they should fall naturally. My confession is that that I've shown you is web demo that I built, where you can go and play classifier on your own. And this is really hard to So maybe we'll do that on your own time. So, let's just go back to here. Man, this is kind of embarrassing. Okay, that was way more So, let's skip this, but I encourage you to go play with this in your browser. It's actually pretty fun and kind of nice to build intuition about how the decision boundary changes as you change the K and change your distance metric and all those sorts of things. Okay, so then the question is once you're actually trying in practice, there's several choices you need to make. We talked about choosing We talked about choosing And the question becomes how do you actually make and for your data? So, these choices, of things we call hyperparameters, because they are not necessarily data, instead these are choices about ahead of time and there's no way to learn So, the question is how in practice? And they turn out to be And the simple thing that try different values of and for your problem, and There's a question? [student asking a question] So, the question is, where L1 to using L2 distance? I think it's mainly problem-dependent, it's sort of difficult to say in which cases you think than the other. but I think that because L1 dependency, it actually depends of your data, if you know that you have a vector, and maybe the individual have meaning. Like maybe you're classifying and then the different elements to different features or Like their salary or the working at the company So I think when your have some meaning, is where I think maybe using more sense. But in general, again, and it really depends on so the best answer is and see what works better. Even this idea of trying of hyperparameters and there are many different choices here. What exactly does it mean and see what works best? Well, the first idea you might think of is simply choosing the the best accuracy or best performance on your training data. This is actually a really terrible idea. You should never do this. In the concrete case classifier, for example, if we set K=1, we will always perfectly. So if we use this strategy but, as we saw from the examples earlier, in practice it seems that might cause us to misclassify but, in fact, lead to better performance on points that were not And ultimately in machine learning we don't care about we really care about how our classifier, or how our method, will perform on unseen So, this is a terrible So, another idea that you might think of, is maybe we'll take our full dataset and we'll split it into some training data and some test data. And now I'll try training choices of hyperparameters and then I'll go and apply on the test data and now I will pick the set of hyperparameters on the test data. This seems like maybe a but, in fact, this is also a terrible idea and you should never do this. Because, again, the point is that we want to know how So, the point of the test set is to give us some estimate on unseen data that's And if we use this strategy algorithms with different hyperparameters, and then, selecting the on the test data, then, it's possible, that the right set of hyperparameters that caused our algorithm on this testing set, but now our performance on this test set will no longer be representative of our performance of new, unseen data. So, again, you should not you'll get in trouble if you do this. What is much more common, is into three different sets. You'll partition most of and then you'll create a validation set and a test set. And now what we typically do with many different on the training set, evaluate on the validation set, and now pick the set of hyperparameters which performs best on the validation set. And now, after you've you've done all your debugging, after you've dome everything, then you'd take that best on the validation set and run it once on the test set. And now that's the number that's the number that that's the number that your algorithm is doing on unseen data. And this is actually that you keep a very the validation data and the test data. So, for example, when we're we typically only touch the test set at the very last minute. So, when I'm writing papers, I tend to only touch the in maybe the week before to really insure that we're not being dishonest here and which is unfair. So, this is actually super important and you want to make sure quite under control. So another strategy for is called cross validation. And this is used a for small data sets, not used So here the idea is we're or we're going to take our dataset, as usual, hold out some test and now, for the rest of the data, rather than splitting it and validation partition, instead, we can split our training data into many different folds. And now, in this way, we've fold is going to be the validation set. So now, in this example, we're using five fold cross validation, so you would train your hyperparameters on the first four folds, evaluate the performance on fold four, and now go and retrain one, two, three, and five, evaluate on fold four, and cycle through all the different folds. And, when you do it this way, you get much higher confidence about which hyperparameters are going to perform more robustly. So this is kind of the but, in practice in deep learning when we're training large models and training is very these doesn't get used Question? [student asking a question] Yeah, so the question is, a little bit more concretely, what's the difference validation set? So, if you think about the then the training set is this where we memorize the labels. And now, to classify an image, we're going to take the image in the training data, and then transfer the label So now our algorithm in the training set, and now we'll take each and compare it to each and then use this to of our classifier when it's So this is the distinction and validation. Where your algorithm is of the training set, but for the validation set, your algorithm doesn't have We only use the labels to check how well our algorithm is doing. A question? [student asking a question] The question is, whether the test set, is it possible that the representative of data This definitely can be the underlying statistical your data are all independently so that all of your data points should be drawn from the same underlying Of course, in practice, this and you definitely can run into cases where the test set might of what you see in the wild. So this is kind of a problem dataset curators need to think about. But when I'm creating one thing I do, is I'll go and collect a whole using the exact same methodology and then afterwards you go between train and test. One thing that can screw you up here is maybe if you're collecting data over time and you make the earlier be the training data, and the later data that you then you actually might that could cause problems. But as long as this partition is random among your entire set of data points, then that's how we try in practice. So then, once you've gone through this cross validation procedure, then you end up with graphs So here, on the X axis, we for a k-nearest neighbor and now on the Y axis, we are of our classifier on some dataset for different values of K. And you can see that, in this case, we've done five fold cross so, for each value of K we of how well this algorithm is doing. And, actually, going back having some test sets for your algorithm, using K fold cross validation is maybe one way to help And, in that, we can see the performs on different And that gives you some sense of, not just what is the best, but, also, what is the So, whenever you're training you end up making plots like this, where they show you what is your accuracy, or your performance as a and then you want to or the set of hyperparameters, at the end of the day, that performs the best So, here we see that maybe about best for this problem. So, k-nearest neighbor are actually almost Because, with all of these So, one problem is that which is the reverse of what we want, which we talked about earlier. Another problem is that these things like Euclidean are really not a very good way to measure distances between images. These, sort of, vectorial do not correspond very well between images. How you perceive So, in this example, we've constructed, there's this image on the left of a girl, and then three different where we've blocked out her mouth, we've actually shifted or tinted the entire image blue. And, actually, if you compute between the original and the boxed, the original and the shuffled, and original in the tinted, they all have the same L2 distance. Which is, maybe, not so good because it sort of the L2 distance is really at capturing these perceptional Another, sort of, problem classifier has to do with of dimensionality. So, if you recall back this k-nearest neighbor classifier, it's sort of dropping paint data points and using that to So that means that if we classifier to work well, we kind of need our training quite densely. Otherwise our nearest neighbors away and might not actually points. And the problem is, that actually densely covering the space, means that we need a number which is exponential in the So this is very bad, exponential basically, you're never to densely cover this space of pixels in this high dimensional space. So that's maybe another when you're thinking about So, kind of the summary k-nearest neighbor to introduce this idea of image classification. We have a training set and then we use that to predict these labels on the test set. Question? [student asking a question] Oh, sorry, the question is, what was going on with this picture? What are the green and the blue dots? So here, we have some training samples which are represented by points, and the color of the dot of the point, of this training sample. So, if we're in one dimension, then you maybe only need to densely cover the space, but if we move to two dimensions, then, we now need, four times to densely cover this space. And if we move to three, four, the number of training to densely cover the space, grows exponentially with the dimension. So, this is kind of giving you the sense, that maybe in two dimensions we might have this kind or you might have sort of in different dimensional spaces. Because the k-nearest neighbor algorithm doesn't really make any underlying manifolds, the only way it can perform properly is if it has quite a dense to work with. So, this is kind of the and you'll get a chance and try it out on images So, if there's any last minute I'm going to move on to the next topic. Question? [student is asking a question] Sorry, say that again. [student is asking a question] Yeah, so the question is, why do these images have And the answer is that, I to have the same L2 distance. [laughing] But it's just giving you the is not a very good measure And these images are each other in quite disparate ways. If you're using K and N, then the only thing you between images, is this single distance metric. And this kind of gives that distance metric is the full description of between images. So, if this case, I just sort translations and these Question? [student asking a question] So, the question is, maybe this is actually good, because all of these things are actually having the That's maybe true for this example, but I think you could also maybe we have two original images and then by putting the or tinting them, we could cause it to be anything that you want, right? Because in this example, we shifting and tinting to kind of change these without changing the perceptional So, I think that this if you have many Question? [student is asking a question] The question is, whether or not it's to go back and retrain the entire dataset once you've found those So, people do sometimes but it's somewhat a matter of taste. If you're really rushing for that deadline and you've really got to then, if it takes a long on the whole dataset, then maybe you won't do it. But if you have a little and a little bit more compute to spare, and you want to squeeze out of performance, then that So we kind of saw that has a lot of the nice properties of machine learning algorithms, but in practice it's not so great, and really not used very much in images. So the next thing I'd linear classification. And linear classification is, algorithm, but this will and help us build up to and whole convolutional networks. So, one analogy people often talk about when working with neural networks is we think of them as being That you can have different of neural networks and you together to build these convolutional networks. One of the most basic in different types of is this linear classifier. So, I think it's actually have a good understanding with linear classification. Because these will end up to whole neural networks. So another example of kind of neural networks comes from some research in our just as a little bit of a preview. So here the setup is that and then output a descriptive sentence describing the image. And the way this kind of works is that we have one convolutional at the image, and a recurrent neural network that knows about language. And we can kind of just stick like Lego blocks and train and end up with a pretty cool system that can do some non-trivial things. And we'll work through the forward in the class, but this just gives you the sense that, these deep neural networks and this linear classifier is kind of like the most of these giant networks. But that's a little bit too so we have to go back to [laughing] So, recall that CIFAR-10 has each image is 32 by 32 pixels In linear classification, of a different approach So, the linear classifier is of what we call a parametric model. So now, our parametric model components. It's going to take in this image, and this, that we usually write and also a set of parameters, or weights, which is usually called depending on the literature. And now we're going to which takes in both the data, and this'll spit out now what are the scores categories in CIFAR-10. With the interpretation that, indicates a larger probability And now, a question? [student asking a question] Sorry, can you repeat that? [student asking a question] Oh, so the question is what is the three? The three, in this example, channels, red, green, and blue. Because we typically work on color images, that's nice information that So, in the k-nearest neighbor setup there was no parameters, instead, we just kind of keep around the whole training set, and use that at test time. But now, in a parametric approach, we're going to summarize our and stick all that knowledge And now, at test time, we training data, we can throw it away. We only need these So this allows our models and actually run on maybe So, kind of, the whole is coming up with the function, F. You can imagine writing down for how to combine weights complex ways, and these network architectures. But the simplest possible example of combining these two things is just, maybe, to multiply them. And this is a linear classifier. So here our F of X, W is Probably the simplest So here, if you kind of unpack the we recall that our image was So then, we're going to take them out into a long column vector that has 3,072 by one entries. And now we want to end We want to end up with giving us the scores for Which means that now our matrix, W, needs to be ten by 3072. So that once we multiply then we'll end up with 10 by one, giving us our 10 class scores. Also sometimes, you'll typically see this, we'll often add a bias term which will be a constant that does not interact and instead just gives us preferences for some classes over another. So you might imagine that unbalanced and had many for example, then the bias to cat would be higher So if you kind of think about pictorially what this function is doing, in this figure we have of a simple image with so it has four pixels total. So the way that the is that we take this two by two image, we stretch it out into a column vector with four elements, and now, in this example, three classes, cat, dog, and ship, because you can't fit 10 on a slide, and now our weight matrix is so we have four pixels and three classes. And now, again, we have a that gives us data independent bias terms for each category. Now we see that the cat score product between the pixels of our image and this row in the weight matrix added together with this bias term. So, when you look at it this way you can kind of understand as almost a template matching approach. Where each of the rows in this matrix correspond to some template of the image. And now the enter product or dot product between the row of the giving the pixels of the image, computing this dot a similarity between this and the pixels of our image. And then bias just, independence scaling offset If we think about linear classification from this viewpoint of template matching we can actually take the and unravel them back into images and actually visualize And this gives us some classifier might actually be doing to try to understand our data. So, in this example, we've a linear classifier on our images. And now on the bottom we're visualizing what are those rows in corresponding to each of the 10 categories in CIFAR-10. And in this way we kind going on in these images. So, for example, in the we see the template for the plane class, kind of consists of this like blue blob, this kind of blobby thing in the middle and maybe blue in the background, which gives you the sense for plane is maybe looking for blue stuff and blobby stuff, and those the classifier to like planes more. Or if we look at this car example, we kind of see that through the middle and a that maybe is kind of a blurry windshield. But this is a little bit weird, this doesn't really look like a car. No individual car So the problem is that is only learning one So if there's sort of might appear, it's trying to average out all all those different appearances, and use just one single template to recognize each of those categories. We can also see this pretty classifier. So in the horse classifier we because horses are usually on grass. And then, if you look seems to have maybe two And I've never seen a But the linear classifier that it can, because it's one template per category. And as we move forward and more complex models, we'll be able to achieve because they no longer of just learning a single Another viewpoint of the linear classifier is to go back to this idea of images as points and high dimensional space. And you can imagine is something like a point in And now the linear classifier linear decision boundaries separation between one category and the rest of the categories. So maybe up on the upper-left hand side we see these training and throughout the process of training the linear classier will blue line to separate the airplane class from all And it's actually kind of the training process these and then go and snap into the data properly. But when you think about in this way, from this high you can start to see again that might come up with And it's not too hard of datasets where a linear So, one example, on the left here, is that, suppose we have a and these are all maybe but maybe our dataset has two categories, blue and red. And the blue categories in the image, which are And anything where the than zero is even, we want to So if you actually go and decisions regions look like in the plane, you can see that our blue class is going to be these two and even will be the So now, there's no way that we to separate the blue from the red. So this would be an example would really struggle. And this is maybe not such an Instead of counting pixels, maybe we're actually trying of animals or people in So this kind of a parity problem of separating odds from evens is something that linear classification really struggles with traditionally. Other situations where a linear are multimodal situations. So here on the right, maybe our blue category has of where the blue category lives, and then everything else So, for something like horses, we saw on the previous example, is something where this in practice. Where there's maybe one horses looking to the left, and another island of And now there's no good boundary between these two So anytime where you have multimodal data, like one class that can appear in is another place where linear So there's kind of a lot of problems with linear classifiers, but it super nice and easy to interpret So you'll actually be on your first homework assignment. At this point, we kind of talked about what is the functional linear classifier. And we've seen that this functional form of matrix vector multiply corresponds this idea of template matching and learning a single in your data. And then once we have this trained matrix you can use it to actually for any new training example. But what we have not told you is how do you actually go for your dataset. We've just talked about and what is going on with this thing. So that's something we'll And next lecture we'll talk about what are the strategies and algorithms for choosing the right W. And this will lead us to questions of loss functions and optimization and eventually ConvNets. So, that's a bit of the And that's all we have for today.