How can software handle a single transaction? Today we're going to talk about Billions and billions of payments. and easily explain how it all works. So let's go. Now we all know how Uber works. Click a button that pays adjusted money Wait for your driver. Get in the car Leave and then Uber pays Pretty simple right? add 100 million users. Billions of rides and hundreds of billions being passed around later. And then our easy As Uber grew and grew in popularity, going from a monolith application They deployed to a microservice. One which is like thousands They'd switch to Postgres and MySQL. And just build technology However, can't mess up in your text hack, it's their system was messing up big time. first it was just beyond unstable. The system was fragmented and services then reconciling transactions Payment discrepancies Imagine being an Uber driver, driving Then a cyberattack happens that you've been expecting Nowhere to be seen, nowhere collected. So they rebuilt their payment system First, they switched their system The job in this case represents Or an Uber Eats order. This job system allowed flexibility assigned to the same job. So, for example, asking We've all been there. It's it's okay to admit it. Inside these orders are entries, to app. The sum always zero. No creating or destroying money. So if I paid $20 for an Uber ride, trip fare, which is $18, This then goes into the Uber Trip fare 18. Service fee $2. This essentially is how double entry Credit and debit. I've covered so many different types in the past, to tailor towards So like, how would you even do that A ledger. A line by line table of your transactions. Don't ask the crypto people What makes ledger is that it's the only way If my bank account starts at $100 but the purchases and depositors well, some suss stuff is happening doing two seconds of research on this, or MySQL is clearly capable of So why isn't it suitable for finance? Well, first, if I wanted to change my name once I change it on the form, and then updates it, This is because it is mutable. if I were to try this on an immutable database, with the new information Then in the UI, I would have to grab So obviously this is just not good If you had a database of transactions and then some dingus deleted a row you forever This transaction is just gone forever. So an immutable system would force an append only system, the correction is appended Rather than deleting the record For example, if I were to document That's -$10 from my account. However, the deli So am I. Can't you see -$15? Well, rather than just the deli has to refund me $5, which then You fix things by adding or subtracting popular open source options Uber made the classic programing decision from scratch, ledger store as an application layer that sits above existing technologies It specifically adds three core concepts. Ceiling closes a time range of changes. This means that transactions that happened and validate that they are correct, Uber specifically mentioned Manifest are files generated This is a security measure to show that only ledger store Just remember though, transactions Apache Kafka, which helps the streaming and revisions. As for when, sometimes the data needs to be corrected Revision is like a table that shows while also making the previous additions And leisure store. It was great. but Uber had a really, really big problem. You see, Leisure Store was built to be an opinionated So when they moved they had to migrate over And so they only had two options process all of this 300 terabyte of data into their new database, into parts and process them individually The issue with doing number one at any point that it goes wrong, Think about installing a software On the last step. So they went with option A worker boots up and reads the checkpoint of where the process needs to be started it then goes to cold storage this data and moves it to the new to say that the partition it filled out And it then stores simultaneous workers And this process only allowed that data and saved on a lot of money. have our historical data But how do we put this over applications of all time? This is when Uber introduced The Shadow Rider the current one and the one The Shadow Rider pushes data synchronously, everything executed The Shadow Rider pushes data meaning our system isn't dependent on The second. but here's another issue. What happens if the items fail? I'm. because it's asynchronous. We don't know if it's going to work To prevent this, they deployed a spot repair job to look at any issues They also created a dual read This reads both databases at the same time If the primary database had five records it would merge the objects together and return to the user. This put the new database into production how it handles under the pressure that leap of faith was needed to be done. So they finally decided to commit to the new databases dock store being the primary Gradually, less requests were being read from Dynamo and Doc Store Oh, and thank God for that. But why all of this work? Just to change databases. When we talk about the ledger, With each row dedicated to a new entry. I mean, makes sense But what about trillions of transactions over billions of ledgers? Indexes. Now imagine this scenario. You walk into a movie store The cashier make sure that each movie I mean, you would be there forever. It's just stupid. An index in a database would be categorizing each movie So if I ask for jaws, But in this case 26 I think 26 And this is that they have managed And there's different types. A strongly consistent indexes When you use Uber, your credit card has a If the database isn't able It risks charging you again Yikes. This is done with an intricate write First, ledger store preps the indexes Once it's prepped, it writes the record the indexes needed. They found that their indexes So to counteract this, if they read that they commit the index. Or if it doesn't it rolls back the index. A lot goes into making this eventual consistent indexes. As for parts of the application So these could be places waiting a little bit is fine Think about a ledger for a second. One of the biggest key columns if not the most important one, Time range indexes does exactly that. is able to index within a certain time When working with data, that is really old. In this case Think like OneDrive. You know easily accessible files just, you know, really cheap storage So we now know that these types of indexes Great. But how do we manage 2 trillion of them? After the index is created, This is done in batches to save on data then is validated for completeness. This is done by using a checksum comparison on the data so that the new index can the idea that so much technology goes or even users take for granted It just takes another level of engineering There's a lot of cloud services transitioning your database system to another cloud or on premise database. But at the scale of Uber, because what works for you isn't something Unless like you're left or something. A big shout out to the three engineers that have written the articles that literally without that, But also, I just think it's so cool the different processes they've tried What's the biggest transition Let me know in the comments below and Peace out. Coders.