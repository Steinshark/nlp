&quot;Man this washroom sucks&quot; &quot;huhh why don't I...&quot; Pinterest was first launched back in March the LAMP stack..ahh good times. At the time they started out with 2 founders Their tech stack consisted of 1 small web In January 2011, they continued to release on user feedback. At this time they brought in another engineer Their tech stack now uses Amazon EC2 + S3 + CloudFront for CDN 1 NginX a web server that can also be used and HTTP cache, 4 web engines 1 MySQL DB + 1 replica in case master goes 5. 1 Task Queue + 2 Task processors 1 MongoDB for tracking analytics In Sept 2011, they started to see explosive and a half. This is the stage where they did a lot of They hired another engineer and started to stack including - 2 NginX, 16 web servers + 2 API engines - 4 Cassandra nodes - 10 redis nodes - 4 Elastic Search Nodes During this time, they were using 5 different For science. Their insane growth meant that all their technologies to things breaking all the time. It was reported that some databases in their ran out of memory. They realized that they would need to spread handle this ever increasing user load. They stopped all new feature work to come They ultimately came up with two potential database. Clustering or Sharding their database. Let's take a look at the two and compare. Database clustering is the process of distributing working together to act as a single system. These nodes often communicate with each other the entire cluster. Data is then distributed among those nodes Pinterest was already using Cassandra and However, at the time it was in its early phase Pinterest faced various issues with the cluster and organize of nodes within the cluster. 4 main issues faced by Pinterest during the 1. Issue with data rebalance. When they bring a new box and data starts 2. Data corruption - if there was a bug in the data on all their nodes. 3. Improper load balancing - in response to uneven process available to re-distribute the data However, over time, the data ended up being 4. Data authority failure - in one scenario, to replicate data from the primary node to At 80% completion the secondary goes I'm the give up authority resulting in the other 20% In the end, they decided to drop Cassandra, This meant that they would only stick with to deal with the scaling issues. Sharding is a technique where a large database pieces called shards. Each shard is essentially a smaller database Unlike clustering, each shard acts independent Their original database state included foreign In order to prepare for the transition, they They combined their tables in order to get This may include redundant data however because access data that resides in another shard. They then introduced read replicas to handle performance. They also introduced caching to reduce the One of the issues that they encountered during when changes made to the primary database This can lead to a situation where a read This was solved by storing the recently updated Subsequent requests for that specific data the database directly;. If the requested data does not exist in cache, This would be okay to do so since the data Next came sharding. They had a goal in mind with sharding, which accesses needed to render a single page. To accomplish this, new user data would be shards but all data associated with the user This makes it easier to render the user profile shards. They initially created virtual shards with and all databases having all tables. They then used multi-master replication approach availability zone. In case of failure, the system would immediately one the right way. The final transition uses an ID sharding database, shard to look for. Each data used a 64 bits id structure, where shard id. Every application has a configuration file for easier lookup. ie (1, 512) maps (shardb001a - the physical The next 10 bits for determining what type remaining 38 bits for MySQL auto increment. This id strategy was enough to support 65,536 at first and plan to expand as needed. The switch to the sharded database didn't There was a period of time when they were sharded system to life. During this time all writes would continue the data onto the new sharded databases. Once it was confirmed that the sharded system system and pointed it to the sharded system. While on the new sharded environment, they shard. When this happend, they would first monitor by recent changes. If it was determined that it was just a load Before splitting, they would create replicas part of database 1 to virtual Shard 2 on Server still go to the old nodes and get replicated Once the transition is completed, the connection the application code was updated accordingly Sharding did come with other issues keys, joins and constraints. These logic were instead moved to application shards. - They lose all transaction capabilities. Transaction is a sequence of one or more operations atomicity, consistency, isolation and durability, completed or none are applied at all. This fails in a sharded environment due to One of them is if a transaction involves updates ensuring that changes are rolled back consistently - schema changes require a lot more planning After the architecture clean up, in January And in oct 2012, they were seeing even more To accommodate, just simply add more boxes There are a few takeaways from this thing if you can just spin up more boxes to - two - the less data you move across your - three - everything will fail so keep it Thank you for watching, please let me know me to cover in the future. As always, thank you for watching and see