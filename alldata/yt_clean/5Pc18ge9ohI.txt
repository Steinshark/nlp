Welcome to #mkown, episode 1. That's how I'm going be reinventing the wheel for learning purposes. for granted that they exist like databases, you name it. I'm going to be writing those the process of how you can go from knowing almost toy project. I've done this a couple times also wrote a reverse proxy HTTP server, so I kind Okay so back to the database thing. Here's databases work internally. Let's take MySQL for what happens is that MySQL itself, the program, see here at this location. Now, when you create that MySQL or rather the storage engine that's database folder which you can see here. That's first the table data. If I insert rows into in here. And then additionally this file also which you need because you have to be like you can't do that linearly one by one, row is built using some esoteric data structure called they're different, but it doesn't matter. somehow stored on disk and it speeds up primary you index that field. This data structure can although it's persisted on disk. But I do you implement a B-Tree? I have no idea. Which means I need to do more research. Okay so I've done some research, I yet but I think I know just enough to a database without using B-Trees So imagine we want to store some user data. their unique ID, which is also going to be the name. So we have a table with two columns and this is basically a spreadsheet. Okay now remember right? How can we store this spreadsheet-like data insert what's called a delimiter between each going to complicate things because I can't jump find where the row delimiter is instead. And by until one of the bytes matches. Since this to get rid of delimiters and make all the rows so we need to force strings to have a maximum name can contain at most 255 characters, I'm going assuming ASCII encoding of course. And then I'm of the string. That's because one byte is made of to 255. That's why everybody uses 255 as the such as names and emails by the way because if you row instead of one to store the length of each exact byte length of each row. The primary key then the user name is a string that needs maximum character capacity, which means that So now let's say I receive a query asking for I can read the first primary key in the file, the second primary key, if the key doesn't match, and so on and so forth until I find the key that the problem is that I'm still searching linearly, to be an issue because reading from the disk is more so if you have to do it thousands of times. An index is going to allow us to search in more efficient than O(n), especially as the use B-Trees just yet so let's try to use a normal I'm going to skip the binary tree explanation, this is basically data structures 101. So data stored in the the format we've discussed primary key 8. Keep in mind that this row could keys don't necessarily have to be integers that just jump to row number 7, it's not that easy. we could store a balanced binary tree where each node holds a primary key value and key is located. Or in other words, the row number. number 8 we could traverse the binary tree until we have that we also know the row number, so file because all the rows are the same size, so has a couple problems. First of all, normal binary key is actually an integer that increments by in ascending order we'll end up with a linked our worst case search time complexity is still that it's not easy to store them on disk. Think in a file? And you must do it of course because mind that if the index is big enough, if you have the index itself could be gigabytes in size, so it even be able to load the entire tree into memory, Okay, so this seems pretty complicated Here's a B-Tree. You can see that its binary trees in the sense that smaller keys kept to the right. The difference is that and each node can hold multiple keys. How many the minimum number of children per node. Let's Once we have that number we can compute the rest properties apply mostly to internal nodes because But the reason you need to keep track of these they always stay balanced no matter how you insert complicated so I'll try to summarize it as simply that you see here, the one you need to remember 3 in this case. All right, so let's try to insert what happens. First of all we create the root into that node in ascending order as if it was a maximum number of keys. Once that happens, further here's how it works: key number 4 cannot be node has already reached the maximum number of this case, because you know, 1-2-3, two is in the the parent node. Since there is no parent node we're going to have to create a new node. That then we're going to split the old root into two before the median and the second one containing number 4 into the rightmost node in ascending self-balancing and insertion algorithm. I'm going the remaining keys, but basically there's only a where the new key should be inserted. If that keys then move the median into the parent. And nodes. This algorithm is recursive so the tree height grows because at some point you're going to have to move multiple keys upwards doesn't have a parent. So you have to create a new that the B-Tree stays balanced. And as such, a linked list where searching is O(n). That solves B-Tree in a file? Before I explain that, one that when you read from the disk you can't read 1 a time. You have to read in &quot;blocks&quot;. Blocks on the physical disk itself, on the drivers, level layers in between. But that's not important, this to minimize disk reading operations. The way figure out how many keys and children pointers that we can fit 100 keys. Okay then each B-Tree of keys and then we're going to store each B-Tree 4 Kibibytes (KiB), not Kilobytes, I'm talking can imagine a file where the first 4 KiB store the root node. The next 4 KiB store another a child of one of the internal nodes and so on at a time with one disk operation at a time, it Also how does a node point to its children? Well start counting blocks at 0 and then we can point by multiplying. I'm not sure how all of this node and I'm also skipping a lot of details but keep learning myself so a lot of things may be wrong Last thing before I start writing code. we're going to have a storage engine that's going and all of that business. Then we're going to have want to call it, and this component is basically SELECT and turn them into some sort of can understand. Finally we're going to expose going to use async or a thread pool, I don't concurrency and multiple connections updating just a mutex but we'll see. Optionally we could nodes in main memory to speed up searching, this video because caching is hard and I have to all of this is going to take a lot of time so I'll So after 7 months, 251 commits and 25,000 I can show you in practice. So I'm going how the internals of the database work. of course. Then I can also create unique and in order to keep track of all these relations when I had to implement this I was like hold on need to be able to create tables in order to deep computer science territory when everything question your own existence. Now these indexes the program actually makes use of them. So if I you're going to see that the program makes And it's also going to decompose this expression to be evaluated. So this is a query plan, I'll I can also do all the CRUD operations. So I can update data, so there's the update. And should be only one row left. That's everything in you might think it's easy but it's actually not at not fit in RAM, and that's one of the constant it's that you can never assume that something fits there. So you got to sort on disk. And also the is because SQL has to actually be able to execute which is something that I didn't even think about virtual machine that runs all this. And I find had in mind was writing my own compiler or way through. Moreover, we have all the transaction I can apply some changes to this table and which means that all these changes will be written these changes even if they were written to disk. it's kind of hard to know due to how the system is extremely important for databases because this what happens if the client is in the middle of to some table and before the client gets to commit goes out or something like that happens, the changes? Because the client didn't commit, didn't the database server gets back up it must revert before the transaction started. So if the client the data is in the exact same state it was whole multi-threading part which I didn't put I'm not spending another seven months on this mutex and called it a day. So basically we can they all work simultaneously, but there can only one here starts a transaction, the other one will ends. So let's insert some data here and let's other one can go through, but there can only if you don't start a transaction manually, So that's my database. In terms of can only work with one table at a time, there no subqueries. So it's pretty simple, but in which is interesting because in the beginning I all the rows to have the same size, but I didn't a lot of space especially with strings. And so will not allocate 255 bytes for every single it will only allocate what's necessary to problem with that? The problem is that now you now you're dealing with variable length data. had to spend half a year coding the database is by no means a production database Let's start with the basics. How are we going to there are multiple approaches. The most logical basically store every table and index in a for each database, so they keep everything the spectrum we find SQLite which stores all the so all the tables and all the indexes in one doesn't really matter how you structure your giant single file or 10 smaller files, you still you won't actually see much of a difference performance you have to do sequential I/O, so instead of random I/O. Random I/O is bad even so you still need to favor sequential works just like SQLite, it uses a single file We've talked earlier about how you have to but we don't want to be limited to the block size, called a &quot;page&quot;. So a page is made of one or many so we can have 8 KiB pages, we can have 16 KiB in that range. Now in my case I usually set is a sequence of 4 KiB pages one after the let's focus for now on B-Tree pages, since B-Tree page and B-Tree node are synonyms that point to their children using page it's the same concept but with pages. So each Slotted pages allow you to store and organize if I simply stored variable records in the first of all I can't easily do a binary search I don't know where to jump. And I need the binary second problem is that sorting is expensive, so 20, 40 and 50 in this page, and now all of a Well, I'd have to move a bunch of bytes towards record and maintain ascending order. Doing that especially when the page size is big. So you want to insert a variable length record here? beginning we're going to store a pointer or offset put it at the end of the page and at the beginning and over again. So the pointers at the beginning towards the right. The variable records at the them however you want, and they grow towards array the page is considered full. The slot array if I want to do a binary search I'm going to do indexes so I know where to jump. Then if I want to I don't have to move the rest of cells, I shift the slot array, which is way cheaper than pointer in the slot array is only 2 bytes. The more than 2 bytes to store each offset. Now we we going to store children pointers? Well, each a page pointer or a page number. And these page I follow the pointer in the cell that stores key new page will contain keys that are less than key in the current page. And that's what allows So how do we store rows? In terms of how the row you in the beginning where columns are stored difference is that now strings are not wasting any using 5 bytes. And I'm also using UTF-8 instead of the length, but not important. The more important are we going to store the tables? Right? And we indexes, so how do we do this? Well, there Postgres and keeping it extremely simple, they pages. This is not a B-Tree, it's just pages one a row and each row has a location defined by its that's why they use slotted pages. And so now if index, from the primary key index for example, you just have to store the primary key and the know where the row is. The problem with this all the pages in the table file you need something where each page stores metadata about pages in where to find free pages or free space and you inefficient. So you basically need 3 different indexes, you need the page directory and you need called &quot;index-organized storage&quot;. This is what B-Trees sorted by the primary key, and so each out about this, I was like you're telling me that it to store tables and I don't need to change Then I'm definitely doing this, so I copied SQLite that we can't use the row location to point to location will change frequently due to the B-Tree sorted by primary key. So we need to use the indexes, from a unique email index for example. key associated to that email, so we would find the row in another O(log n) time, whereas the previous finds the email it already has the row location. primary key is not an external index for us, so to read a B-Tree index first and then the table it's like an automatic index on the primary key. There's only one piece missing. We said that pages going to do with rows that require more space called &quot;overflow pages&quot;. Basically, when the size like for example 1/4 of the page size, then we're The first chunk will be stored in a cell of the point to an overflow page that's going to store header that's going to point to the next overflow so now we have a linked list of overflow pages. row we have to sort of reassemble the entire Putting it all together, the database file is and overflow pages. So you might think that this but it's not because the file is just a sequence where every single B-Tree is because each B-Tree the overflow pages, so everything's nice and tidy. I wanted to understand how a database is look like in this mysterious database file? But data structures laid out, you got to implement So B-Trees yet again. The problem we have with cells in them, so all the formulas I showed because how many keys will each B-Tree page have? there's no upper bound which of course makes the default balancing algorithm keeps pages because each time a node splits the because you take the median so you split in half by the end you will have a bunch of pages the balancing algorithm is actually one of the I probably spent two months just on this, because that I copied from SQLite. Object-oriented gurus low-level systems programming, we don't do Java take forever but it's based on this simple idea: you realize, hold on a second, do I actually I don't because I can just sort of reorganize populating sibling nodes. I can do that until and repeat again. This algorithm does So now that we have a working storage engine, the string. So how do we go from a string to running got to parse the string. I'm not going to talk wrote a lot of code just for parsing alone, but topic for another video. Here I am basically going called a tokenizer which takes characters as got to feed these tokens to the actual parser Abstract Syntax Tree (AST). This tree represents I'm going to keep things simple but if you're Top Down Operator Precedence (TDOP) parsing, which makes it pretty easy to write parsers. Because an operation inside of a larger expression and of parsing consists of a couple of mutually other until they end up building an expression there's something called the analyzer which so things like this table doesn't exist, this data handled here. And then on top of that there's SQL statement to avoid computing unnecessary is not machine code running on the CPU, it runs parsing... it looks complicated because of all have never written a serious parser like this one entire code base to write. Mainly because it's not you keep everything high level and if the language a joy to write, you don't even have to think So now that we have a complete AST, just like that the CPU can run we have to turn our AST machine can run. So that's the query plan, plan is yet another tree... man computer science but anyway the query plan tree represents all the a basic example would be: select a couple columns is scanning the table, so how are you going to indexes? Or you just do it sequentially? Then the output based on the where condition, and which is the fancy relational algebra term for rows are called tuples, so I'll also use that term What's interesting about plans is that I'm using essentially what it does is it processes tuples is an iterator itself, an iterator over tuples. tuple, we basically call .next() on the plan, which in turn calls .next() on its child. At of the tree where we're going to find the scan that goes over all the rows and returns tuples only state that it needs is the cursor. And we can process a giant table one row at a time not the only way of doing this, Postgres does of byte code and then they run that. It's the same I copied Postgres here. So thanks to Postgres and database development journey. I would not have and SQLite, and yes I know the correct is S-Q-L-lite?) But it's kind of annoying I also used the lectures from &quot;CMU Intro to they're available on YouTube for free, so that's Moving on, what happens with algorithms that need example sorting. Well, there's a special kind has a fixed size in-memory buffer where it stores is full it writes it to a temporary file. So it Now if the rows can actually fit in the memory you want to sort them? You can just call when stuff doesn't fit in memory that's when So how do we sort a giant file of unsorted External Merge Sort&quot; I'll make an entire video on but in a nutshell this algorithm divides the of passes through the file until it's fully reading pages and one output buffer for writing would do something called &quot;2-page runs&quot;. Each sorted from start to end. Then pass one is able to 16-page runs, and so on. It keeps multiplying by awesome because if you have five buffers instead 25-page runs. Essentially, you're reducing the I/O at the expense of using more RAM. And it does it's not going to load 25 pages because it only When I implemented this algorithm, even though I because I was like, damn, this algorithm all won't even flinch. This is another one of those and this time I implemented it iteratively instead It's like &quot;while, while, while, for, for&quot;, I don't is that every single algorithm that works with algorithms, they all need to take into account in RAM. And so they all have to be able to operate Even though I'm talking about caching now it's after the B-Tree. Caching is another one right? But luckily there's a simple solution. the &quot;clock algorithm&quot;. So the cache is basically Every page has a reference bit so that when we set its reference bit to 1. Every page also a page we set both its reference and its dirty need to write back to disk. And then there's the to read a page that is not cached we have to is yet another pointer whose mission is to find page that it currently points to is referenced, to the next page. If that page is referenced it page. Once we find a page that is not referenced considering that if the cached page was dirty we caching, overall, not that hard but I did find when needed and stuff like that, because you group writes of multiple pages together to So how do we Implement commit and rollback? The do which is something called the &quot;journal file&quot;. a transaction loads some pages into the cache pages. Essentially what you do is before modifying journal file. And then you can modify and safely you just need to copy the original pages from the done, no more complications needed. If you want journal file, it's that simple. This solution is one transaction at a time, but production ready Log (WAL)&quot;, which essentially keeps track of all and also opens the door for multiple writers at in MySQL or Postgres and you'll verify that each see the changes of the other transaction until it As I said in the beginning this program would and so I had to write my own little network which I like because it's very simple, it doesn't I'm not going to explain the entire format of each but what's nice about this is that you don't you can write your own client and you can write nice about protocols. So hopefully you've learned one of the most important pieces of software especially a backend developer, you're using you don't actually know much about how they work about systems programming, when I was a noob all the software that I'm using from the operating to the pixels on screen, somebody had to write is writing hello world in Python... And especially time I thought that compiler programmers were like you gotta deal with low-level disk data structures those who have written Postgres, MySQL, SQLite because it's extremely complicated. So if you're because I'm pretty sure I must not be the only one understand how this works, all the source code be a learning resource, so it's written in such a how it works. Even though there are 25,000 lines because I wrote like entire dissertations on how ASCII diagrams that make it easy to visualize are also over 270 different tests which you can is supposed to do that, so you can sort of figure any quality whatsoever, all the code in here is it's very unstable so if you try it out expect it read and understand than some giant codebase like science content then leave a like, subscribe, because I have countless ideas for projects like unless you actually watch the videos. So do the