Hey, Tim Berglund with Confluent. This is the fundamentals of Apache Kafka within the course called So we're really gonna get (bright upbeat music) In this module you're gonna of what Kafka is. When we're done, you the key elements of a Kafka cluster and have some idea of what of each element are. Explain what a topic is and how it's composed of and that basic sort of Topics are what you're thinking about when you're actually building applications on Kafka, so we want that to be pretty Now, it is a fact of the happen in it. There are many events produced and no matter what sort like the industries, all of those things are and Kafka's job is to manage So to get data into a Kafka cluster, we have a thing called a producer. A producer is an So that, that Kafka box there, that's either a Kafka cluster or maybe it's a Kafka where you don't know too much but you know, the services and you know how to connect to it. That Kafka cluster is your job is to write programs and read data out of it and So all of those sources of financial transactions, boxes being shipped around the world. All of those are potential and some producing application and takes an action to write those events into a Kafka cluster. And after they get written, okay, got it, sends an acknowledgement, and the producer moves on. What's in a Kafka cluster? Let's break that down a little bit. It's made up of things called brokers. Now, if you will just kind for maybe 10 or 15 years ago when computers were things that you saw and they were in a data center and you could maybe go and it was this metal box and that sort of thing. If you could imagine a little crisper here, those are kind of what brokers are. Those are Kafka process each one has its own Every broker has its own local storage. Those brokers are networked together and they act together as When producers produce, that are brokers. Now, if you're using Kafka in the cloud, you're hopefully not gonna be thinking about brokers too much. If you're using a fully managed service, there are brokers that are out they've got their own local storage, they've got some retention that they're storing. That's maybe five or maybe 100 years. They've got some amount of time, they're gonna keep those events around. But in the cloud, of course we don't think too closely about brokers. Those are the things that's But it's true, and you should know this. Every Kafka cluster is composed of these things called brokers. And we never quite know Are they machines? Are they servers? These are all sort of old words, they might be containers, they might be VMs somewhere. And I'll skip that for I'll probably just call them machines or servers or some kind in as much as we're talking about brokers, just so you kind of have Now that data gets it's stored on those and of course, we wanna read it out, we do that with the consumer. And this consumer is a program you write. That Kafka cluster, those somebody manages that infrastructure, operates that cluster. Ideally, it's a managed but what you write is the you put data in and you read data out. And it's reading data that frankly usually gets pretty interesting. This is where a lot of the work What the consumer does with that data, well, that's up to the application itself, is generating a report, is there some other that process data too? Often, a consumer will also be a producer and put its results into that transform data. But these are the You have a producer, you you have a consumer. And that model, those three pieces, everything that ever gets built with Kafka conforms to that model. Maybe you won't see the consumer If you're using something or CASE SQL Db, which we'll you don't think about but they're always there. These are the fundamental So let's look at that We've got our producers there on the left, that Kafka cluster in the middle, those producing applications that you've got the consumers they're reading from the cluster. What's that thing on top? Well, that's a ZooKeeper ensemble. As of the time of this recording, Kafka uses ZooKeeper to manage consensus on a few pieces of distributed state. There are a few things need to agree on. They need to have some and ZooKeeper is good at doing that. Now, again, as I said, at there's an initiative that's Kafka Improvement Proposal, 500. That itself has bonded and a whole bunch of work to remove ZooKeeper from Kafka completely. So at some point after this recording, that will not be there anymore. So it may be that you look at Kafka and the current release you're using, and maybe you see this video and say, well, there's no ZooKeeper there. And that means a new day has dawned. So there's a lot of work to get that done, and that work is ongoing. But for now ZooKeeper's doing a great job, being a distributed consensus Producers and consumers are So a consumer doesn't know that produced the data that it's reading. Likewise, when you produce data, you're not sending it to you're sending it to a structure that's called a topic. You're producing that data to a topic, we'll cover those more in a moment. But you don't know anything about who's consuming from that topic. And that decoupling is intentional. That means that there's a whole category of state information between that is simply never managed. Producers write data in, That also means that producers can scale. I can add producers that without the consumers knowing. I can add consumers that are reading a kind of data that has long existed. Maybe I've got this of sales transactions in my cluster, I add some new fraud detection algorithm. Well, that's a new consumer, none of my producers need to know. They can fail independently, They're decoupled, so they don't need to Revisiting ZooKeeper for a moment, what does it really do? Well, authorization information, that's access control lists, those are stored in ZooKeeper. The management of failure. So when a broker fails, pieces of a topic that that they're replicated in other but the way that replication is organized and who's in charge of each that is managed by ZooKeepers. So when a broker dies and who gets responsibility for ZooKeeper participates in the election of new leaders for those things. So it's basically what it does. Little bits of metadata, access control lists, that stuff is all currently in ZooKeeper. I've used this word a few times topics. Now I wanna really give you a definition of what a topic is. A topic is a collection or related events. You can think of a topic as a log, as a sequence of events. Now, there are some exceptions to that, I'm gonna unfold some concepts here that help you manage those exceptions. But for right now, just think So topic is this list of things and when a producer writes a new one, it just puts it on the end and I can have any number of I can have a producer Likewise, many consumers All of those relationships There isn't a theoretical There's a practical limit on the number of what we'll get to partitions in a moment. But topics by themselves, it's and then you need to add you can really have conceptually And I did say partition, didn't I? So I better tell you what that means. Now here's a cluster over on the left, it's got a number of topics in it, let's zoom in on topic C. Now that topic is a log and it's a durable log, it's persistent, which means what it's gonna And the broker that well, that's just a computer And if you're writing and reading messages from IO and computational work that that computer has to do. None of these things scales forever. You can't have storage scaling forever, and you can't have that pub/sub activity on that broker scaling forever. So you might want to break We call those pieces partitions and then be able to to different brokers in the cluster. This is key to how Kafka scales. I can take a topic, partition it and allocate each partition So when I set a topic was a log and I kind of put a and there's an exception Formally speaking a partition is a log. So every partition has strict ordering, when I produced to a partition, I put the message on the that's the only place I can I can't disturb any of they're all immutable events, And so that is a log and that partition are strictly ordered. A topic having been You may not have strict ordering over all of the events in the topic, so you've always got And if you wanna look at really drill down into what a partition is on an individual broker, that log file is gonna be represented by multiple logs. Segments, those are individual it's really a set of a few files, and some indexes and things like that. So that segment is a on the Kafka broker, and each into multiple segments. Usually, unless you're deeply involved in hands-on administration you're not gonna think but you do have to as you think about how It's very important. Let's look at that again in color. If you can see those topic b is, it's kind and I wanna call c sort And you're wanting to if you think any of those But this cluster now has four brokers, we're calling them 101, 102, 103, 104 and you can see how the are broken up. So topic a, has partition You see partition, zero is on broker 101, partition one is on partition 102, partition two is on broker 104 and none of topic a is on broker 103. I won't go through all of those, but you can kind of pause and look through that diagram and see how those The cluster does that automatically, when a topic is created by the way, it makes decisions about are gonna live. What a Kafka cluster doesn't do is keep track of the and move them around, if one broker gets overloaded, as topics get created and destroyed, loading of course does not stay constant. So this is functionality to keep those things balanced. There are parts of Confluent platform that will help you do And of course, if you're sort of thing, is the thing there is a team of highly and site reliability this kind of stuff works for you and you don't have to worry about it. You see over there on the right there, the log files, each into individual segments on disc, on the broker as a really drill into what's going on on the broker. Let me just refresh exactly This is important, you You've probably written to or at least read one. And you kinda know instinctively, even if you've never thought about it, the semantics of a log. So when you write something to It goes on the end, it It doesn't go at the beginning, That sequence of things You can only add new things to the log because the rest is representation of what has happened in time, up till now. Also all of those ordered those are immutable. If you're editing one of there's almost something about editing a log, like, You know, you conspiring So logs are immutable records of things. And the semantics are when you put it on the end and that's it, and those are immutable after that. You may choose to expire and that's certainly the case in Kafka, you can set a retention period on a topic. But this is what a log is, this is this fundamental data structure that Kafka is based on. Those numbers you see there, the zero one, two, three, four, like that, those are real in Kafka. They actually start at zero and they just monotonically And there are many, many bits of them, so you're not gonna run out, but every partition has its so that's a real thing. And you can actually find that out, when you produce a message, when you consume a message. The API, you can poke into oh, this will ended up or this was offset such and such. Usually don't need to know, but, and it's a real thing, and it's there inside of each partition. An important fact about consumers, those blue boxes on the bottom that are apparently reading messages, consuming, doesn't consume, it doesn't destroy the So you can have multiple consumers on one log or one topic in Kafka, and they can be at their Now, of course, you'd like them Everybody wants to be kind that are being produced, so this processing could But they don't need to be, in terms of the way the system works, one can start at the beginning and take days to catch up, They're independent consumers that are working from independent offsets. Later on you'll hear me this log or topic I'll say stream. And these things in the stream are events, and the current time is where that's the present and the stream extends back into the past. So different word means What's the structure of a Kafka message? Well, the things that you are are the key and the value, Every event is a key value pair. Now, very likely there's in the value, right? That's probably some sort of domain object that you're gonna serialize There may be structure in the key. Often the key is a string or But sometimes people have a compound and complex domain objects and use as the key, Every message has a timestamp. If you don't have a timestamp, So you'll get the wall clock time at the time that you produce the message. That's if you don't really But if in your value, if it knows the time it took place, well then in the API, when hey, you know, the actual time is this. I don't care what time it is right now, this is the time of the message. You can set that explicitly. You also have an optional set of headers. Think of them like HTTP headers, which are themselves kind So you don't wanna use but this really is metadata. So these are properties of the data that you're gonna be able to see on read. So consumers have access to these and can make decisions based on them. But that's it, you have key Let's dive into brokers a little bit. I've introduced them, but I wanna go over their Their basic function is Now, as a developer using Kafka, you're thinking about topics all the time. You're creating a topic, you're thinking about What kind of messages does it have? What's its retention period? What are its compaction properties? All these great things that as you move forward. If you're a broker, you know what a topic is, but You're managing some set That's what a broker does. It manages those log files, it takes inputs from producers, updates those partitions, and writes them out. That's what a broker does. It's does storage and pub/sub. So those partitions are stored and there can be many of them, many partitions on each individual broker. So you see this core architecture diagram again, you've got producers. Those are applications that brokers that are taking those rights and managing their partitions and consumers that are The way consumers read from partitions actually gets pretty interesting, we'll take a look at that later. It would be a bummer if each partition only such that if that broker You would not want that to be the case. And of course, Kafka does replicate. Each partition has a three is typical, that's One of those replicas is called the leader and the others are called the follower. So when I produce to a partition, I'm actually producing to the leader. The producer is connecting to the broker that has the lead partition there. And it's the job of the brokers to reach out to those leaders and kind of scrape the new and keep up-to-date with So and that all happens inside of a properly It's not like they take seconds for that replication to take place, but there is a leader, which makes consistency a Thinking a little more about are these client applications, you might be asking, what Well, Java was always the The language library that ships with Kafka is a Java library. Now the other adjacent languages, of the JVM like Kotlin and even Scala, there are always wrappers for those that make the Java library look idiomatic in those languages. But there are other C, C++, Python, Go, .Net. Those libraries are supported So if you need like a of one of those, you can get that. Those are all based on called librdkafka. That's an open source a lot of the functionality and many other non JVM language libraries draw on that for their Kafka support. There are many more than that supported by the Kafka community, if you're wondering where node support is and where's Ruby and all? Well, believe me, they are there. In fact, there are often multiple choices for each one of them. There's also a REST Proxy that that lets you access Kafka. If somehow you're using a language that doesn't have library support, or if you just don't want to use that native language support, And of course there is a Command That's good for tests kind of small amounts of Now I said, when a it's actually writing to a partition and these partitions are And so how does a producer know which partition to write a message to? There are a couple answers to that. Now, if the message has no key, the producer will just that it applies. And it'll say partition partition two, partition three. And there are some exceptions and interesting ways to configure that, but that's basically what's gonna happen, you're gonna load Partitions always stay even in that case, but you don't have a Now, if the order of you have the opportunity So if there is a key, then is hash that key, mode that gives you the partition So the same key is to the same partition, as long is held constant in the topic, which probably should be in most cases. So messages with the same which means they are strictly And that's an important thing. Cause you might have a key, say you've got an internet you've got smart thermostats tens of millions of them and they're all phoning home and every other kind of and you want those to be ordered. You wanna be able to Well, if you make the key, the device ID, well then each devices messages are gonna show up in order in a partition. So messages of the same it's possible to override all this and write a custom it doesn't end up happening very often, but it absolutely is available Consumers again are the that are reading from topics. All the same language options as producers and what they do, they pull. They actually will go will go and ask the Kafka cluster. It will say, &quot;Hey, I am &quot;and this is the last offset I read.&quot; Remember those numeric offsets. &quot;Do you have any messages And if the answer is no, then it returns and the consumer can come back and ask a very short period of time later. Usually of course the answer is yes, and here are more messages and moves on. That consumer offset, the that's state, right? We don't want that only to be in memory. So the offset of each that that consumer is responsible for is stored in a special topic named mysteriously So if you see that consumer offset topic, that's what it's doing. That's helping your consumers So if they go away and come back, the cluster can help them remember where they need to pick up again. And just like the producer, to read from a cluster, for quick visibility into things Usually not a lot of production code gets written with the CLI tools, but they do come in quite handy. As I've said, each topic and by multiple consumers, I mean multiple different applications that are reading that same data. So there's separate code separate builds, separate deployments could be managed by separate teams. Maybe the people don't as long as they've got the they can deploy consumers Consumers also live in groups. Now that one of the top that's kind of the but every consumer is a Meaning I can add additional like you see at the bottom, there are three instances of So imagine that's an uber-JAR and you've built a Docker image around it and Kubernetes is now instead of only one of it, that's the way that you We'll talk about the details in a future module. Let's come back to the that describes every Kafka You've got producers that you've got that cluster, about partitions and replication And then you've got consumers, those programs that read data out, every system you're ever gonna build conforms to this diagram. And with that, you should have of how Kafka works, a consumers, brokers, ZooKeeper, all of these things put to explore what next. (bright upbeat music)