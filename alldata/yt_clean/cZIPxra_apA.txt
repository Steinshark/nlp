That little chip in your computer? Yeah, it's complex, but it manages that complexity Today, we explore just one aspect of how it I think even among those of us who aren't have heard of the CPU, the Central Processing We probably have some impression that it takes and that's where the magic happens. While that's a fairly good start to understanding little further today, to better see how tasks consider how it interacts with the rest of Today, we'll speak in very general terms, Instead, I just want to focus on one specific As we go through today's concepts, you may of the concepts. Good for you! Feel free to skip ahead by using the links chapters on the seek bar. Let's start with CPU basics. We know that your computer runs programs. At a lower level, these programs are essentially the language of hardware, for your CPU to Here's the interesting thing - Most instructions Each step can be broken down further, into of many of the instructions. One basic, five-step model to explain this What you'll find is that to execute any instruction, happen. Here's what they mean. Fetching involves pulling the actual instruction of your computer. Yeah, instructions don't just fall into your Then, the decode step is where your CPU tries Typically, instructions are just big numbers. If you break it down into its binary representation, things. For example, the number represented by the That is, a number representing the operation It could be arithmetic like addition, logic and writes to RAM. The rest of the numbers are the operands to The decode step is pretty much mandatory to to do. The Read step involves fetching the required act on. For example, if you have an addition operation (that is, little banks of memory that live the two registers to retrieve those values. Now that we know what we want to do, AND have can proceed to the execution step - That is So whatever calculation or comparison takes step. Finally, there is Writeback, in which that This could be back into the registers, or It wouldn't be amiss to imagine these sub-tasks every instruction needs to go through all So this breakdown, this ABSTRACTION, is a does. However, in doing this, we also start to see ------ While, yes, an instruction MUST go through to the OTHER stages, as an instruction is Notice that, more often than not, pipeline While an instruction is being executed, the are just sitting there. As you can imagine, it would be horrible for only a fifth of your CPU is really doing anything. We're not making good use of resources this But here's the good news. We can do what is known as instruction-level that we don't really have to wait for one on the next one! You see, the beauty of this pipeline is that the most part - in fact, we'll get in trouble Assuming then, that the steps ARE completely send in a second instruction while the first What this means is, at any given moment, if that every single stage is hard at work. But the beauty of this is that each stage Every instruction still needs to move through by not waiting and moving straight on to the Alright, where's the catch? In an ideal world, this would just work. But unfortunately, certain combinations of unexpected problems. We call these hazards. Let's say we have these two operations one instructions here to make things easier. Imagine w, x, y and z to be some registers Intuitively, what we'd expect to happen is Then, we make a copy of x into w. However, if these two statements were run would that happen? Let's find out. Assuming we're still sticking to the five-stage gonna happen. If the first instruction starts at t=0, it's into register x by t=5. Now consider the second instruction, which It has to do this by t=3, that's where its We should be done with the entire calculation But, the value of x is only available at t=5! I don't know what the second instruction is simply because the correct answer isn't yet The formal name for this is a Read after Write We're reading immediately after writing, and So, how do we deal with this problem? There are several strategies that are relevant stall. We simply, wait. If we know that information isn't going to second instruction. It will sit around and wait until the data While very simple to discuss and implement, these bubbles in the pipeline. The whole reason why we want to do pipelining sorta flies in the face of that. Of course, the hope is that hazards don't at least, be better than no pipelining at Of course, more sophisticated strategies can One such solution is operand forwarding. Think of this as a &quot;Secret Passage&quot; that connects Information can take that shortcut, and jump Let's try the same example using the same that there is some means of communication stage. The second instruction still needs a stall You can't avoid that since the answer hasn't However, we CAN avoid waiting for the writeback, passage to the read stage at the very moment Thanks to this, we only need to stall for finish at t=7 instead. Alright, let's throw another instruction into Let's say now, our instructions looked like Here's what would happen if we executed them Because of the stall in executing the second power its way through, so it needs to stall Thus, you can imagine how a stall creates entirely from the system. But here's the thing - Do we really need to If we take a closer look at the third instruction, of the two preceding instructions, in that This is important, because if an instruction actually have to run it in order! Try this, let's swap the order of the second us. Since the new second instruction doesn't really the pipeline and let it run to completion Now, the third instruction kicks in at t=2, Let's say our operator forwarding is still the first instruction at t=3, gets pushed No stall required, it just picks up and carries cycles. That's right! By just executing the instructions out of full use of our CPU with no idling at all. Of course, as you can imagine, putting instructions We have a very contrived example here, but each other in complex ways that mean we may without creating weird problems down the line. CPUs must be smart at determining whether out such an optimization. So far, we've considered sequential instructions, But get ready to open an even bigger one when If-statements and loops both cause your code often times, this jump is based on a condition. Let's start by seeing what that looks like at the conundrum it creates. Take for example, a simple conditional jump, we'll jump back to the start of the loop if A typical instruction used in this context zero&quot;. Its parameter would be whatever register the What does that look like in the pipeline then? We need to fetch and decode this instruction We then need to read off the parameter, then conclude whether we should jump away, or simply That's not too bad, but what about the rest In the four cycles it takes to decide whether load things into the pipeline, but the question If we're jumping away, we should start loading of the jump. Conversely, if we're saying put, we should But the thing is, we don't know, do we? We can only guess at whether the branch is If we guess wrongly, we end up loading a set executed, before we realize we're wrong. At that point, we've got to throw everything This of course sets us waaaay back, much worse a clean slate and have to load up instructions So the thing is, we also have to be kinda is taken or not. In the previous example, if we know the branch more sense to just assume it's taken. We'll be wrong once, at the time in which because the alternative, which is to assume for EVERY iteration of the loop except the This thought process here is what is called are many strategies out there for doing this. These processes are generally what's known we're just guessing! Of course, the better the heuristic, the more At its simplest, static branch prediction These just look at individual instructions strategy to guess at whether the branch is For example, a CPU can decide that all backward are taken. Of course, this has to match the way in which CPUs can also use random branch prediction, By randomly deciding whether or not branches ground of 50%. Then, there is dynamic branch prediction, it employs some strategies to learn over time. By recording whether or not past branches factor in whether or not the next branch is Such strategies can get complicated, utilizing somewhere, or even loading up multiple instructions Some CPUs employ multiple sets of branch predictors, one to be quicker, and the other to be slower it out. And in this age of AI, some CPUs are even And that's it! Today we've looked at a simple model of a parallelism. We saw that, while this can help keep every weird things like hazards can happen. This gets worse when branching is involved, thing and having to flush it all out. The amount of optimization that goes into is astounding - We've barely scraped the surface nitty gritty implementation details. But I hope this gives you a good idea of just That's all there is for today's video. Thank you very much for watching, and until