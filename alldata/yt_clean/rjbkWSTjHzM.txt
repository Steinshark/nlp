Hey guys. Um, let's get started. So over the last several weeks, you've learned a lot about many different learning algorithms from linear regression, to logistic regression, to generalizing models, generative algorithms like GDA and Naive Bayes to most recently support-vector machines. Um, what I'd like to do today is to start talking about advice for applying learning algorithms. To teach a little bit about the theory behind, um, how to make good decisions of what to do, how to actually apply these algorithms. And so today, um, I wanna discuss bias and variance. Um, and it turns out, you know, I've, I've built quite a lot of machine learning systems, um, and it turns out that bias and variance is one of those concepts. It's, sort of, easy to understand, but hard to master. Uh, uh, what does it- lots of those, was it all these board games or sometimes, uh, uh, smartphone games, say easy to learn, hard to master or something like that? So bias and variance is actually one of those things, where I've had PhD students that worked with me for several years and then graduated, and worked in the industry for a couple years after that. And, and they actually tell me that, you know, when they took, um, machine learning at Stanford, they learned bias and variance, but as they progressed for many years their understanding of bias and variance continues to deepen. So I'm gonna try to accelerate your learning, um, uh, uh, of, of bias and variance because I find that people that understand this concept, um, are much more efficient in terms of how you develop learning algorithms and make your algorithms work. So we'll talk about this today, and it'll be a recurring theme that'll come up again a few times in the next several weeks as well. Um, then we'll discuss regularization, um, uh, and talk about, um, how to reduce variance in learning algorithms, talk about train, dev, test splits, uh, and then also talk about a few model selection and cross-validation algorithms. Um, oh, let's see, reminders for today. Uh, Problem Set 1 is due tonight, uh, uh, 11:59 P.M. Uh, and, uh, if you are not yet ready to submit it today, uh, late submissions are accepted until Saturday evening. Saturday 11:59 P.M, with the details of late submissions, uh, written according to the late day policy written on the course website. So, so I definitely encourage you to submit your homework on time today. If for some reason you're not able to the late submission, which we don't encourage anyone to take advantage of, but it is written, uh, on the course website. And Problem Set 2 will be released, uh shortly. Actually I think, uh, it was already posted online, um, uh, and is due two weeks from now. Um, yeah. Right. And so, uh, okay. So, um, and, and what I'm going to do today is talk about the conceptual aspects of this. Uh, and if you want to see even more math between these so the conceptual concepts, uh, at this Friday's discussion section, we'll be covering, um, some of the the, uh, mathematical aspects of learning theories such as error decomposition, uniform convergence, and VC dimension. You know, one, one interesting thing I've learned is, um, really watching the evolution of machine learning over many years is that, that machine learning as a discipline has actually become less mathematical over the years, right? Um, uh, so I remember when, um, you know, machine learning people used to worry about, uh, computing the normal equations, like x transpose x inverse equals x transpose y. How numerically stable is your numerical solver for solving the normal equations of inverting a matrix for solving linear equations. But because, um, numerical linear algebra has made tremendous rise, now we just call linear- linear algebra routine. To invert a matrix to solve linear system equations not worry about whether it's numerically stable or not. But once upon a time a lot of my friends in machine learning were reading text books on, uh, numerical optimization to figure out if your formula for inverting a matrix or really solving linear system equations was numerically stable. And so one of the trends I've seen is that, uh, I think, um, three or four years ago, to understand bias and variance, there was a certain mathematical theory that was crucial to understanding that. And so I used to teach that in CS229, but we decided, um, that we're constantly trying to improve this class, right? But I decided that, uh, that, uh, mathematical theory is actually less crucial today. If your main goal is to make these algorithms work. So we still teach it. But we're doing it in the Friday discussion section, and that leaves more time for the main lecture here to talk more about the conceptual thing that I think will help you build learning algorithms, as well as for the newer topics like, um, what- we'll talk about random forest, decision trees of random forests in neural networks next week. So here we go. Okay. So let's talk about bias and variance. Um, let's say you have this dataset. [NOISE] Um, I'm gonna draw the same dataset three times. [NOISE] Okay. So, um, let's say you have a housing price prediction problem where this is the size of the house and this is the price of the house. Um, it looks like if you fit a straight line to this data, maybe it's not too bad, right? But it looks like this dataset seems to go up and then curve downward a little bit, right? And so [NOISE] maybe this is a slightly better model if you fit a, let me see. So this if you fit a linear function, um, Theta 0 plus Theta 1x. Uh, but if you fit a quadratic model, maybe this actually fits to the dataset a little bit better. Um, or you could actually fit a high order polynomial. This is one, two, three, four, five, six examples. So if you fit a fifth order polynomial, let's say the 5x to the 5th, then, um, you can actually fit a function that passes through all the points perfectly. But that doesn't seem like a great model for this data either. And so, um, to name this phenomenon, the function assuming the one in the middle is what we like, um, fitting a quadratic function is maybe pretty good. Let's call it just right. Whereas, um, this, uh, example on the left, it underfits the data, um, as in, it is not capturing the trend that is maybe semi-evident in the data. And we say this algorithm has high bias. And the term bias, um, the term bias has, has actually multiple meanings in the English language. We, as a society, want to to avoid racial bias, and gender bias, and discrimination against people's orientation, and things like that. So, uh, the term bias in machine learning has a completely separate meaning. Um, and it just means that, uh, and, and it just means that, um, uh, this learning algorithm had very strong preconceptions that the data could be fit by linear functions. This album had a very strong bias or the very strong preconception that the relationship between pricing and house- size of house is linear, and this bias turns out not to be true. So this is actually a different sense of bias than, than the, than the other types of undesirable bias we want to avoid in society or which, which interestingly comes up in machine learning as well in other contexts, right? We want our learning algorithms to avoid those different biases, there's a different use of the term. And in contrast and just cut off on the right, we say that this is overfitting, um, the data. And this algorithm has high variance. Um, and the term high variance comes from this intuition that, um, you happen to get these five examples, but if, you know, a friend of yours was to collect data from, uh, see here, six, six examples and a friend of yours was to collect a slightly different set of six examples, right? So if a friend of yours were to rerun the collected slightly different, um, uh, set of housings- houses, you know, right? Then this algorithm will fit some totally other varying function on this and so the- your predictions will have very high variance. If you think of this as averaging over different random draws of the data. So, so the, the variations if, if a friend of yours does the same experiment and they just get a slightly different dataset just due to random noise, then this algorithm fitting a fifth-order polynomial results in a totally different result. So that's-, uh, so we say that this algorithm has a very high variance, there's a lot of variability in the predictions this algorithm will make, okay? Um, so one of the things we'll need to do is, um, identify if your learning algorithm. Oh, so when you train a learning algorithm. It almost never works the first time. And so when I'm developing learning algorithms, my standard work flow is often to train an algorithm-, uh, often train up something quick and dirty, and then try to understand if the algorithm has a problem of high bias or high variance, if it's underfitting it or overfitting the data, and I use that insight to decide how to improve the learning algorithm. And I will say a lot more about, um, how to improve the learning algorithm. We have a menu of tools that we'll talk about in the next couple of weeks, about how to reduce bias or reduce variance of, uh, of, of your learning algorithms. Um, I should have mentioned that the problems of bias and variance, um, also hold true for classification problems. Uh, so, [NOISE], right. So let's say that's a binary classification problem. Um, if you fit a, uh, logistic regression model to this, you know, straight line fit to the data. Maybe that's not great, right? Um, if you fit a logistic regression model, um, with a few nonlinear features; so you have features x_1 and x_2. Um, if instead of using x_1 and x_2 as features, you use additional features x_1 squared, x_2 squared, x_1 times x_2, x_1, qx_2 and this is Phi of x, right? And you can have a small set of features you choose by hand. excuse me, probably more features in this or using SVM kernel and using SVM for this problem. Then, um, if you, let's see, if you have too many features, then you might actually have a learning algorithm that fits a decision boundary that looks like that. Right? And this learning algorithm actually gets perfect performance on the training set but this overfits. Um, excuse me, I meant to make the colors consistent, sorry I meant to use red. But you- you get what I mean. Um, and there's only if you choose somewhere in-between, you know, that you get something that, that seems to be a much better fit to the data. The green line seems to be a pretty good way of separating the positive and negative examples that they're sort of just right. So, uh, similar to, I guess I messed up the colors slightly before, kind of but similar to these colors here, the blue line underfits because it's not capturing trends that are pretty apparently in the data. The orange line overfits. It's just much too complicated a hypothesis whereas the green line, um, is just right, okay? So it turns out that, um, in the error of GPU computing ability to train models with a lot of features, um, one of the- by building a big enough model, uh, so take a support vector machine. If you add enough features to it, if you have a high enough dimensional feature space, um, or if you, um, take a linear regression model, logistic regression model and just add enough features to it, you can often, um, overfit the data. And it turns out that, um, one of the most effective ways to prevent overfitting, um, is regularization. So let me describe what that is and, um, excuse me, just finding my notes, reworking today's lecture. So this is new things I have not presented. Um, so all that. Okay, cool. And, um, regularization is the- it'll be one of those techniques that, um, won't take that long to explain. It'll sound deceptively simple but is one of the techniques that I use most often. I, I, I feel like I use regularization in many, many models. So, so just because it doesn't that sound that complicated or maybe won't even take that long to explain today, don't underestimate how widely used it is. It's used in- it's not used in every single machine learning model but it's used very, very often. Um, so here's the idea, um, which is- let's take linear regression. Right. So that's the optimization objective for linear regression. Um, if you want to add regularization, uh, you just add one extra term here, uh, Lambda, uh, times norm of, uh, Theta squared, right? Sometimes you write Lambda over two to make some of the derivations come out easier. And what this does is it takes your cost function for logistic regression, uh, which you try to minimize, try to minimize the square error fit to the data and you're creating an incentive term for the algorithm to make the parameter's Thetas, uh, smaller, okay? So this is called a regularization term. And it turns out that, um, let's take the linear regression overfitting example, right. So you know if you set Lambda equal to0, then it's just linear regression over the fifth order polynomial features. Uh, it turns out that as you increase Lambda, you know, Lambda to some intermediate value, uh, depending on the scales of data. Let's say you said Lambda equals 1. Then, when you solve for this minimization problem, or this augmented problem for the value of Theta, um, this term penalizes the parameters being too big and it turns out that you end up with a fit that looks a little bit better, right? It maybe it looks like that, okay? Um, and by preventing the parameters Theta from being too big, you make it harder for the learning algorithm to overfit the data and it turns out fitting a very high order polynomial like that may result in value of states that is very large, right? Um, and, and then if you set Lambda to be too large, then you actually end up, um, in an underfitting regime, okay? So there'll usually be some optimal value of Lambda where if Lambda equals 0, you're not using any regularization. You're so- maybe overfitting. Um, if Lambda is way too big, then you're forcing all the parameters to be too close to 0. Um, in fact actually, if you think about it, if Lambda was equal to 10 to the 100 or some ridiculously large number, then you're really forcing all the Thetas to be 0, right? If all the Thetas is 0, then you know then you're kinda fitting the straight line, right? So that's if Lambda equals, uh, 10 to the 100. And so- and this is a very simple function which is the function 0, right? And, and this function h of Theta, x equals 0, right, approximately 0. It is a very simple function which you get if you set Lambda very large. And by doubling Lambda between, you know, a far too large value like 10 to the 100 compared to a far too small value like Lambda 0, you, you, you smooth the interpolate between this much too simple function of h equals 0 and a much too complex function, okay? Um, so there is, um- so that's pretty, uh, it, it, it, um, it- so that's pretty much it for regularization in terms of what you need to implement but you feel like your learning algorithm may be overfitting, um, add this to your model and solve this optimization problem, um, and it will help relieve overfitting. Um, more generally, if you are, um, let's see. More generally if you have a, uh, say logistic regression problem where this is your cost function. Then to add regularization, I guess instead of min this is a max, right? If you're applying logistic regression, uh, then this was the original cost function, um, then you can have minus [NOISE] Lambda or Lambda over 2, right, it just depends on scaling of Lambda times the norm of Theta squared and there's a minus here because for logistic regression, we're maximizing rather than minimizing. Or this could be argmax at any of the generalized linear model family as well. But by subtracting Lambda times the norm of Theta squared, this allows you to also regularize the classification algorithm such as logistic regression. Okay? [NOISE] Um, it turns out that, uh, and- and I- I- I- wan- I make an analogy that, uh, where all the math details are true, but we don't wanna talk about all the math details. It turns out that, um, one of the reasons the support vector machine doesn't overfit too badly even though it has, you know, been working in infinite like, you know, infinite dimensional feature space, right? So- so why- why doesn't a support vector machine just overfit like crazy? We showed, uh, on Monday that by using kernels, it's sort of using infinite dimensional feature space, right? So why doesn't it always fit these crazy complicated functions, it just overfits the dataset like crazy? It turns out and the theory is complicated. It turns out that, um, [NOISE] you know, the optimization objective of the support vector machine was to minimize the norm of w squared. Uh, this turns out to, uh, correspond to maximizing the margin, the geometric margin SVM, and it's actually possible to prove that, um, this has a similar effect as that, right? That this is why the support vector machine despite working in infinite dimensional feature space sometimes, um, by forcing the parameters to be small is difficult for the support vector machine to overfit the data too much. Okay? The theory to actually show this is quite complicated. Uh, um, uh, yeah, er, uh, it's actually very- yeah, is to show that the cost of cost Phi is where this is- where norm of w small cannot be too complicated, complicating can overfit basically. Um, but that's why, uh, SVMs can work in- can work in infinite dimensional feature spaces. Yeah? [inaudible] Oh, sure. Do you ever regularized per elements of parameters? Um, not really. Uh, and the problem with that is, um, you know, let me give one- let me give one more specific example, then come back to that, right? So it turns out that, um, uh, so we talked about Naive Bayes as a text classification algorithm. [NOISE] It turns out that, um, let's see if the text classification algorithm problem, you know, classify spam, non-spam, or classified it to a sentiment, possible negative sentiment of a tweet or something. [NOISE] Let's say you have 100 examples, but you have [NOISE] 10,000 dimensional features, right? So let's say your features are these, you know, take the dictionary A, aardvark and so on. So 101, right. So let's say you construct a feature like this, um, it turns out that if you fit logistic regression to this type of data, where you have 10,000 parameters and 100 examples, this will badly- this will probably overfit the data, um, because you have, uh, uh, but it turns out that if you use logistic regression with regularization, this is actually a pretty good algorithm for text classification, um, and this will usually i- in terms of performance accuracy, um, yeah, because this is logistic regression, you need to implement gradient descent or something to solve local value parameters. But logistic regression with regularization for text classification, will usually perform- outperform Naive Bayes o- on a- on a classification accuracy standpoint. Uh, without regularization, logistic regression will badly overfit this data, right? Um, and- and to- to explain a bit more, um, you know, imagine that you have a three-dimensional subspace where you have two examples. Then all you can do is fit a straight line, right, for the hyper-plane to separate these two examples. But so one rule of thumb for, um, logistic regression is that, if you do not use regularization, it's nice if the number of examples is at least on the order of the number of parameters you want to fit, right? So this is if you're not using regularization. It's nice if- in fact, I- I personally think that, uh, I tend to use the duration only if the number of examples can be maybe 10x bigger than the number of examples, uh, because that's what you need to have enough information to fit good choices for all these parameters, um, but that's if you're not using regularization. But if you are using regularization, then, um, you can fit, you know, even 10,000 parameters, right? Even with only 100 examples, and this will be a pretty decent, um, text classification algorithm. Okay? Um, the question you have just now: why don't we regularize per parameter, right? So why don't we, uh, let's see. I guess instead of Lambda [NOISE] norm of Theta squared, it would be a sum over j Lambda j, you know, Theta j squared, right? Um, the reason we don't do this is because you then end up with, if you have 10,000 parameters here, you end up with another 10,000 parameters here, and so choosing all these 10,000 Lambdas is as difficult as just choosing all these parameters in the first place. So we don't have good weights to do this. Whereas, when you talk about cross-validation, multiple regression a little bit, we'll talk about how to choose maybe one parameter Lambda, but that- those techniques won't work for choosing from 10,000 parameters Lambda j. You've got a question? [inaudible] You're absolutely right. Yes. Thank you. Um, yes. So in order to make sure that the different Lambdas on the similar scale, uh, a common pre-processing step we're using learning algorithms is, uh, take your different features, um, so for text classification of all the features is 01, you can just leave the features alone. But if a housing classification, if feature one is the size of house which I guess ranges from, I don't know, 100 to, uh, how big are the biggest houses? I don't know, like whatever. Let's say houses go from, I don't know, five inches square feet to 10,000 square feet. Ten thousand square feet is really really big for a house, I guess. But the numb- feature x2 is the number of bedrooms which probably ranges from like, I don't know, one to- I guess there's some houses with a ton of bedrooms, but I would say most houses have at most five bedrooms, I don't know, right? Then these features are on very different scales and, uh, normalizing them to all be on a similar scale, so subtract out the mean and divide it by the standard deviation. So scale all of these things to be between, you know, 01 over 2 minus 1, um, to 1, would- would- would be a good pre-processing step before applying these methods. Um, it turns out that this will make gradient descent run faster as well, as a common pre-processing step to scale each individual feature to be on a similar range of values. All right. Yeah? At the back? Uh, can we quickly go back to some more support vector machine model like NLG? So it's actually both, so just to repeat it, why-why don't support vector machines suffer too badly, is it because it's small numbers for vectors or is it because of minimizing the penalty W. Um, I would say the formal argument relies more on the latter. So it turns out that if you look all the class- if you're looking at all the class of functions separate the data with a large margin, ah, that class has low complexity formalized by low VC dimension which you'll learn about in Friday's discussion section if you want to come to that. And so, it turns out that the class of all functions that separate the data of a large margin is a relatively simple class of functions by- and by simple class functions, I mean, it has low VC dimension. We should talk about this Friday. Um, and thus any function within that class of functions, uh, is not too likely to over-fit. So, um, it is convenient the support vector machine has a relatively low number of support vectors. But, um, uh, you could imagine other algorithms of a very large number of support vectors, uh, but smallest to large margin is still a low complexity class that will move with it. Alright, next one question. I'm sorry say that again. [inaudible] Oh, sure yes. So is it possible that so yes. So one of the- so yes. So in general, models that have high bias tend to underfit and models have high variance tend to overfit. Um, we use these terms over-fit high variance, underfit high bias not quite and they have very similar meanings. Right, at their first approximation assume they, they mean the same thing. One thing we'll see later, uh, two weeks from now is, uh, we'll talk about algorithms with high bias and high variance. So, uh, this is, uh, and actually one way to think of high bias and high variance, we will talk about this later, is if you have a dataset that looks like this, uh, and if somehow your classifier has very high complexity, there is a very, very complicated function. But for some reason it's still not fitting your data well right, so that would be one way to have high bias and high variance which does happen. All right. Cool. [NOISE]. So to wrap up the discussion on regularization, um, there's one- so mechanically the way you implement regularization is by adding that penalty on the norm of the parameters, uh, so that's what you actually implement. It turns out that, um, there's another way to think about regularization. So you remember when we talked about the new, uh, linear regression we talked about minimizing squared error and then later on we saw that linear regression was maximum likelihood estimation on a certain generalized linear model using, uh, using, using, using a Gaussian distribution as the choice for the exponential family as a member of the exponential family. It turns out that, um, there's a similar point of view you can take on the regularization algorithm that we just saw. Which is, let's say S is the training set. [NOISE]. Right. So, um, given a training set, um, you want to find the most likely value of Theta, right? Um, and so by Bayes rule P of Theta given S is P of S given Theta times P of Theta divided by P of S. And so if you want to pick the value of Theta that's the most likely value of Theta given the data you saw, then because the denominator is just a constant, this is arg max over Theta of P of S given Theta times P of Theta. Um, and so if you're using logistic regression then the first term is this. Right and in the second term is P of Theta, um, where this is the, you know, logistic regression model say, right or any generalized linear model. And it turns out that, um, if you assume P of Theta is Gaussian. So if you assume P of Theta is follow Theta. The prior probability on Theta is Gaussian with mean 0 and, uh, some variance tau squared i. So in other words P of Theta is, you know, 1 over root 2 pi, um, I guess this would be the determinant of tau squared i, right, e to the negative, um, Theta transpose, uh, tau squared i inverse. Right. So the Gaussian probability as follows. It turns out that if, um, this is your prior distribution for Theta and you plug this in here and you take logs computer maps and so on, then you end up with exactly the regularization technique that we found just now. Okay. Um, and so in everything we've been doing so far we've been taking a, um, frequentist interpretation. I guess the two main schools of statistics are the frequentist school of statistic and the Bayesian school of statistic, um, and there used to be some titanic academic debates about which is the right one, but I think, uh, statisticians have gotten together and kind of made peace and then go freely between these two more and more these days. Maybe not now all the time but, uh, but then the frequency score statistic. We say that there is some data and we want to find, um, the value of Theta that makes the data as likely as possible and that's where we got maximum likelihood estimation right. And in the frequentist school of statistics, we view them as being some true value of Theta out in the world that is unknown. Um, and so there is some true value of Theta that generated all these housing prices and our goal is to estimate this true parameter. In the Bayesian school of statistics we say that Theta is unknown. But before you see even any data you already have some prior beliefs about how housing prices are generated out in the world and your prior beliefs are captured in a probability distribution, uh, denoted by P of Theta, so this is called the Gaussian prior. And, um, we say that, um, and- and if you look at this Gaussian prior. [NOISE]. Excuse me. It's quite reasonable. It's saying that before you've seen any data on average I think the parameters of theta have mean 0 because I don't know if each Theta is positive or negative so giving the mean 0 seems reasonable. And most things in the world are Gaussians and we just assume that my prior on Theta is Gaussian. So you know, we could debate that this is, uh, the right assumption but it's not totally unreasonable, right? But they say well, for actually I think, you know, for the next linear regression problem I'm gonna work on next week and I have no idea what I'm going to work on, where I'm going to apply linear regression that next week. It is actually not too bad an assumption to say, you know, my prior is Gaussian. And in the Bayesian view of the world, our goal is, um, to find the value of Theta that is most likely after, um, we have seen the data. Okay. And so this is called map estimation. Which stands for the maximum a posteriori estimation. So this is actually the map estimator I guess the arg max of this, right? Uh, as the map or the maximum a posteriori estimates of Theta which means, look at the data, compute the Bayesian posterior distribution of Theta and pick the value of Theta that's most likely. Okay. And so one of the things you do in the problem set that was just released, um, is-is actually show this equivalence as well as plugged in a different prior for theta other than the Gaussian prior you experiment with, uh, whether P of Theta is the Laplace prior and to find a derive a different map as mean algorithm. Okay. Um, all right, good. Yeah, question? [inaudible]. Sorry can you say that again? [inaudible] Uh, yes. [inaudible] Oh I see, yes, can difference between these two be seen as regularized versus non-regularized? Yes. So, so, um, MLU here corresponds to the origin of regularization, uh, and this procedure here corresponds to adding regularization. Um, it turns out that frequency statistic- statisticians can also use regularization it just that they don't try to justify it through a Bayesian prior they just say, so if you're a frequentist statistic. If you're a frequentist statistician your job is to wake up and come up with an algorithm to estimate this you know true value of theta that exists out in the world, and you can come up any procedure you want and to inspire your procedure, you can add a regularization term. I think there's a lot of these debates between frequentists and Bayesians are more philosophical. I think there's a machine learning person, as an engineer. I don't really you know, I think the philosophical debates are lovely but I just- I, I just like my stuff to work. So, so, so frequentists can also infer regularization. It just that they say this is part of the algorithm they invented rather than derived from a Bayesian prior. All right, cool. So, um, [NOISE] all right. Let's talk about um, so in, in our discussion on regularization and choosing the degree of polynomial, um, uh, all right. So let's see, let's say I plot a chart where on the horizontal axis I plot, um, [NOISE] model complexity. So how complicated is your model? So for example, uh, to the right of this curve could be a very high degree polynomial. [NOISE] Right. Um, and what you find is that as you increase model complexity your training error- if you do not regularize, right? So if, if you fit a linear function, cosine function, cubic function and so on. You find that the higher the degree of your polynomial the better your training error because you know, a fifth-order polynomial always fits the data better than a fourth-order polynomial. If you, if you do not regularize. But what we saw with the original picture was that the ability of the algorithm [NOISE] to generalize kind of goes down and then starts to go back up, right? And so if you were to have a separate test set and evaluate your classifier on a set of data that the algorithm hasn't seen so far, so measure how well the algorithm generalizes to a different novel set of data, then if you fit a linear function then this underfits [NOISE]. If you fit a fifth-order polynomial this overfits, [NOISE] and there is somewhere in between right, that is just right. Okay? And um, this curve is true for regularization as well. So say you apply linear regression with 10,000 features to a very small training example. If lambda was much too big then they will um, underfit. If [NOISE] lambda was 0 so, you're not regularizing at all then they will overfit, and there will be some intermediate value of lambda that is not too big and not too small that you know, balances overfitting and underfitting. Okay. So, um, what I'd like to do next is describe uh, a mechanistic. A few different mechanistic procedures for trying to find this point in the middle, right? And so [NOISE] um, so given a data set [NOISE] what we'll often do is um, take your data set and split it into different subsets, uh, and a, a, a good hygiene is to take a data in the trained set- train, dev and test sets, um, So if you have say, 10,000 examples, all right, and you're trying to carry out this model selection problem. So for example, let's say you're trying to decide what order polynomial you want to fit, [NOISE] right. Or you're trying to choose the value of lambda, um, or you're trying to choose the value of tau, that was the bandwidth parameter in uh, locally weighted regression that you saw in the problem set- that we saw with, uh, locally weighted regression, all right? So, um, or you're trying to choose a value C in a support vector machine. So remember, the SVM objective was actually this, right. With the you know, subject to some other things but for the O unknown soft margin that we talked about on Wednesday- uh, talked about on Monday. You're trying to minimize the normal W and then there was this additional parameter C that trains off how much you insist on classifying every training example perfectly. All right. So whether you're trying to make- which of these decisions you are trying to make, um, how do you, uh, you know, choose a polynomial size or choose lambda or choose tau or choose parameter C which also has this bias-variance trade-off. There'll be some values of C that are too large and some values of C that are too small. [NOISE] So here's one thing you can do which is um, uh, let's see, so split your training data S into a subset which I'm gonna call the uh, raw training set as subscript train, um, and then some subset which we wanna call S subscript dev. And dev stands for uh, development, [NOISE] um, and then later we'll talk about [NOISE] a separate test set. And so what you can do is train each model, and by model I mean, um, option [NOISE] for the degree of polynomial [NOISE] on S train. Um, so you're evaluating a menu of models, right? So let's say, this is model 1, model 2, and so on up to model 5, up to some number. They can train each of these models, uh, on the first subset of your data [NOISE] and then get some hypothesis. Let's call it h_i, [NOISE] um, and then, [NOISE] measure the error on S dev which is a second subset of your data called the development set. And pick the one- [NOISE] Okay. So rather than- and- and- uh, I wanna contrast this with an alternative procedure, right? So the two sets of the da- two subsets of the data, some test set data, training set, and development set. And uh, after training, uh, first order polynomial, second order polynomial, third order polynomial on the training set, we evaluate all of these different models on the separate held-out developments sets and then pick the one with the lowest error on the development set. Okay, um, but the one thing to not do would be to evaluate all these algorithms instead on the training set and then pick the one with the lowest error on the training set, right. Why not- wha- what goes wrong when you do that? Yeah. [BACKGROUND] [inaudible] Yeah, right, you just over-fit. How- why- why will you over-fit? [BACKGROUND] Parts of the error, what you want to remain so don't want- [inaudible] Yeah. Yep, cool, right. So if you use this procedure, you'll always end up picking the fifth order polynomial, right. Because the more complex algorithm will always do better on the training set. So if you do this, this will always cause you to say, let's use the fifth order polynomial or the- or the highest possible order polynomial. So this won't help you realize in the housing price prediction example to the second order polynomial is the benefit of the data, right. Does it make sense? Um, and that's why for this procedure, um, if you evaluate your, uh, model's error on a separate development set that the algorithm did not see during training, this allows you to hopefully pick a model that neither over-fits nor underfits. And in this example, hopefully, you find that uh, there will be the second-order polynomial, right, that the one that's just right in between that actually does best on your development set, okay. Um, all right. Now, uh, and then, um, you know if- if you are, uh, if- if you are publishing an academic paper on machine learning um, then, this procedure has looked at the training set as well as the development set, right. So this- this procedure, this piece of code is, you know, is two in these decisions. Uh, it's two in the parameters, the training set, and it's two in the decision on the degree of polynomial to the dev set. And so if you want to know, if you want to publish a paper that say, oh, my algorithm achieves 90% accuracy on this dataset um, it's not valid to report the results on the dev set because the algorithm has already been optimized to the dev set. In particular, information about what's the most- um, what's the best uh, degree of polynomial to choose was derived from the dev set from the development set. And so if you're publishing a paper or you want to report an unbiased result, um, evaluate the algorithm on a separate test set, S test and report that error, okay. And so if you're publishing a paper, it's considered good hygiene to um, uh, report the error on a completely separate test set that you did not in any way shape or form look at during the development of your model, during the training procedure, okay. Clear with things? Oh, yeah. Are dev and test [inaudible] is uh, generally different by much? Um, a dev and test set's error isn't strictly different by much. It depends on the size of- it depends on the size of the dataset. Um, uh, and so it turns out that um, actu- let- let- actually let me- let me give an example, actually. So let's say you're trying to fit a degree of polynomial, right. Um, and you want to choose uh, uh, write the dev error. So we can fill the first, second, third, fourth, fifth degree polynomial. And so um, after fitting all of these, lets say that the square error right, to use round numbers is 10, um, 5.1, 5.0, 4.9, you know, um, 7, 10, and so on, okay. Just to- just to use round numbers for illustrative purposes. If you're using the dev error to pick the best hypothesis, to pick the best hot spot, you would say that uh, using the fifth order polynomial gets you 4.9 squared error, right. But did you really earn that 4.9 square error or did you just get lucky? Because there is some noise and so maybe all of these actually have error that close to 5.0. But some are just higher, some are just lower, and you just got a little bit lucky that on the dev set this did better. Which is why, if you look at your dev set error, your dev set error is a biased estimate, right. And so where's your very large test set? If it's a very large test set, maybe the true numbers are 10, 5, 5, 5, 7, 10 are your actual expected squared errors. It's just that um, because of a little bit of noise you got lucky and reported 4.9. And so this would be a bad thing to do in an academic paper, right. Because it's uh, what you earned was an error of 5.0 you didn't earn an error of 4.9. It's just that- because you're over-fitting a little bit in the dev set. Um, you chose the thing that looked best for the dev set, but your algorithm didn't actually achieve that error, it's just because of noise, okay. So- so um, now in- in so- so it's considered a good practice to report um, uh, uh, so reporting on the dev error isn't- isn't- isn't really a valid unbiased procedure. And- and uh, um, yeah. Do you have a question? [BACKGROUND] [inaudible] Yeah. Ye- so- so one of the just to we say, I- I yes, you're right. One of the problems with some of the machine learning benchmarks that people worked on for a long time is this is unavoidable mental over-fitting. The people'd gotten to use the dataset and everyone's working the same trying to publish the best numbers from the same test set. So the academic committee on machine learning does have some amount of over-fitting uh, to the standard benchmarks that people have worked on for a long time. And this is an unfortunate result uh, when the test is very- very large, the amounts of over-fitting is probably smaller, but when the test set is not big enough then the over-fitting result can cause um, sometimes even research papers to uh, to publish results that are uh, probably over-fit to the data set, right. um, uh, and so I think there is actually uh, one standard academic benchmark because there's a dataset called CIFAR, it's quite small. It's actually the very same research paper uh, uh, analyzing um, results on CIFAR uh, arguing that some fraction of their progress that was made was actually perhaps uh, researchers uni- unintentionally over-fitting to this dataset. Okay. Oh and by the way um, one thing I do when I'm building you know, production machine learning systems. So when I'm- when I'm shipping a product, right. I just don't build a speech recognition system and just make it work. I just wanna, and not- and if I'm not trying to publish a paper, I'm not trying to make some claim. Sometimes I don't bother to have a test set, right. So and uh, and it means I don't know the true error of the system sometimes uh, but I'm very conscious of that. If I don't have a lot of data, sometimes I'm may decide to just not have a test set and it means I just don't try to report the test set number. I can report that dev set number which I know is biased and I just don't report the test set number. Don't do this when you're publishing your academic paper, right. This is not good if you're publishing a paper or making claims on the outside but all we're doing is building a product and not writing a paper out, this is- this is actually okay. Uh, yeah. [inaudible] Yeah. Okay, good. Uh, that's- lemme, lemme get to that. Good. So, um, the next topic about setting up the train dev test split is, how do you decide how much data should go into each of these three subsets? Um, so uh, uh, I can tell- so, so let me just tell you the historical perspective and then a modern perspective. Um, historically, the rule of thumb was you take a training set, right? Take your training set S and then you would send- here, one rule of thumb that you see a lot of people referring to is, uh, 70% training, right? 30% test. [NOISE] This is one common rule of thumb that you just hear a lot. Uh, or maybe you have- if you- if you don't have a dev set, if- if- if you're not doing model selection, if you just- if you've already picked the model and now you're revising. Or maybe you have people use, you know, 60% train, 20% dev, 20% test. Right? So these are rules of thumb that people use to give. Um, and these are decent rules of thumb when you don't have a massive dataset. So you may have 100- 100 examples, maybe you have 1,000 examples, maybe several thousand examples, I think these rules of thumb are perfectly fine. [NOISE] Um, what I'm seeing is that as you move to machine learning problems with really, really giant datasets, the percentage of data you send to dev and test are shrinking. Right? And, and so, here's what I mean. Um, let's say you have 10 million examples. Um, you know, yeah, decent size, not giant but like a reasonable size. Um, so le- let's, let's take this- this is actually a pretty good rule of thumb if you have a small dataset. If you have a thou- if you have, you know, 5 million examples, this is a perfectly fine rule of thumb to use. Um, but if you have 10 million examples, then, you know, you have 6 million, [NOISE] 2 million, [NOISE] 2 million, right, train, dev, test. [NOISE] And the question is, do you really need 2 million examples to estimate the performance of your final classifier? Uh, sometimes you do if you're working on online advertising, you know, which I have done, and you're trying to increase your ad click-through rate by 0.1%, because it turns out increasing ad click-through rate by 0.1%, which I've done multiple times, uh, turns out to be very lucrative. [LAUGHTER] Uh, uh, then you actually need a very large dataset to measure these very, very small improvements because to- to- to increasing ad click-through rate by 0.1, you might have actually a lot of projects. You might have 10 projects, each of which increases ad click-through rate by 0.01%, right? And so to measure these very different- small differences in, algorithm one does 0.01% better than algorithm b by- so you need a lot of data to tease out that very small difference. So if you're in the business of teasing out these very small differences, you actually need very large test sets. But if you're comparing different algorithms and one algorithm is, you know, 2% better or even 1% better than the other algorithm, then with 1,000 examples maybe, right, 1,000 examples may be enough for you to distinguish between these much larger differences. Um, so my recommendation for choosing the dev and test sets is choose them to be big enough, um, that you have enough data to make meaningful comparisons between different algorithms. Uh, and if you suspect your algorithms will vary in performance by 0.01%, you just need a lot of data to distinguish that, right? So, so if you have 100 examples, then, you know, if, if one algorithm has 90% accuracy and one algorithm has 90.01% accuracy, then unless you have at least 1,000 examples and maybe 10,000 or more, you just can't see this very small difference, right? If you have 100 examples, you just can't measure this very small difference. So my, my advice is, uh, choose your dev and test sets to be big enough that, um, uh, you could see the differences in the performance of algorithms that you, uh, tha- that you roughly expect to see. Um, and then you don't need to make your dev and test sets much larger than that. And I would usually then just put the data. You don't need the dev and sets back in the training set. So when you're working with very large datasets, say, you know, a million or 10 million or 100 million examples, what you see is that the percentage of data that goes into dev and test tends to be much smaller. So it might be, um, uh, so you see for example, maybe 90% train, you know, 5% dev, and 5% test, right? Or, or, or even smaller, or even 1%, 1% depending on how much data you really need. To measure to the level of accuracy you need the differences in the performance of your algorithms. Okay? Cool. All right, um, just to give this whole procedure a name, um, what we just did here between the train and dev set, this procedure that we have is called hold-out cross validation. [NOISE] And sometimes, to distinguish this from other cross validation procedures we'll talk about in a minute, sometimes this is called simple hold-out cross validation. We'll talk about some other hold-out cross validation procedures in a second. Um, and, uh, uh, and the dev set, um, is sometimes also called the cross validation set. Okay, right? Uh, so sometimes you- people use- sometimes, you hear people say, you know, we're gonna use a cross validation set. That means roughly the same thing as a, as a dev set. Okay? So in the normal workflow of developing a learning algorithm, uh, when you're given the dataset, I would split it into a training set and a dev set. Oh, and I used to say cross-validation set, but cross-validation is just a mouthful. So I think just motivated by the reducing number of syllables, because you're using this classifier so often, more and more people just call it the dev set, but it means roughly the same thing. Right? So, so when I'm, uh, building a machine learning system, I'll often take the dataset, split into train and dev, and if you need a test set, then also a test set, um, and then, uh, keep on fitting the parameters to the training set and, uh, evaluating the performance of your algorithm on the dev set and using that to come up with new features, choose the model size, choose the regularization parameter Lambda, um, really try out lots of different things and spend, you know, several days or weeks, uh, to optimize the performance on the dev set. Um, and then, uh, when you want to know how well is your algorithm performing, to then evaluate the model on the test set. Right? And- and the thing to be careful not to do is to make any decisions about your model using the test set, right? Because then- then your scientific data to the test set is no longer an unbiased estimate. Uh, one- so- and- and o- one thing that is actually okay to do is, um, if you have a team that's working on the problem, if every week they measure the performance on the test set and report out on a chart, right? You know, uh, the, the performance on the test set, that's actually okay. You can evaluate the model multiple times on the test set. You can actually give out a weekly report, saying, this week, for our online advertising system, we have this result on the test set. One week later, we have this result on test set, one week later, this result on the test set. It's actually okay to evaluate your algorithm repeatedly on the test set. Uh, what's not okay is to use those evaluations to make any decisions about your learning algorithm. So for example, if one day you notice that your model is doing worse this week than last week on the test set, if you use that to revert back to an older model, then you've just made a decision that's based on the test set, and- and your test set is no longer biased. But if all you do is report out the result but not make any decisions based on the test set performance such as whether to revert to an earlier model, then you can- I- I- it's actually legitimate it's actually okay to keep on, you know, use, uh, use the same test set to track your- your team's performance over time. Okay. All right. Good. Um, so when you have very large data sets, this is the procedure if you're developing for defining the train dev and test sets and this procedure can be used to choose the model of polynomial. It can also be used to choose the regularization parameter Lambda or or the parameter C or- or- or the parameter tau from now locally weighted regression. Um, now, whenever you have a very small data-set, right? Um, [NOISE] So it turns out that, uh, so I'm gonna leave out the test set for now. Le- let's just assume there is some separate test set. I'm not gonna worry about that for now. Um, but let's say you have 100 examples, right? Um, if you're going to split this into, you know, 70 in the training set in S subscript train and 30 in S dev. Then you're training your algorithm on 70 examples instead of 100 examples. And so I've actually worked on a few healthcare problems. Oh, actually, mo- most of my PhD students, uh, including Annan, work, doing a lot of work on, uh, machine learning applied to health care. And so we actually worked on a few data-sets in healthcare where, you know, every training example corresponded to some patient that sometimes that, uh, you know, unfortunate disease or- or- or if every- if you're working- or if, um, every example corresponded to injecting a patient with a drug and seeing what happens to the patient right sometimes there's literally a lot of blood and pain that goes into collecting every example. And if you have 100 examples to hold out 30 of them, um, for the purpose of model selection using only 70 examples and 100 examples. It seems like you're wasting a lot of data that was collected through a lot of, you know, literal pain, right? Um, so is there a way to say do model selection such as choose the degree of polynomial without, &quot;Slightly wasting so much of the data.&quot; There is a procedure that you should use only if you have a small data-set, only if you're worried about the size of- oh, and the other disadvantage of this is, you evaluate your model only on 30 examples, and that seems really small. Right? You know can you- can you just find more data to evaluate your models as well. So there's a procedure that you should use [NOISE] only if you have a small data-set, uh, called k-fold cross-validation, or k-fold CV. And this is, uh, in contrast to simple cross validation. Um, but this is the idea which is- let's say this is your training set S, so you have, you know, X 1, Y 1 down to X say 100, Y 100. [NOISE] What we're going to do is take the training set and, uh, divide it into k pieces. Um, so for the purpose of illustration, I'm gonna use k equals 5. When I'm, just to make the- the writing on the board sane. Uh, k equals 10, uh, is typical. I guess, uh, for illustration. [NOISE] But so what you do is, um, take your data-set and divide it into five different subsets of- in this example, you would have 20 examples. 100- 100 examples divided into five subsets, so there are 20 examples in each subset. And, um, what you do is, for i equals 1 to k train i.e, fit parameters on k minus 1 pieces. And then test on the remaining [NOISE] one piece, and then you average. Right? So in other words, uh, when k is equals 5, we're going to loop through five times. In the first iteration, we're going to train on these and test on the last one fifth of the data. Um, so we'll hold out the last one fifth of the data, train on the rest and test on that. And then in the second iteration, through this for loop, we'll train on pieces 1 ,2, 3 and 5 and test on piece number 4, and we get the number. Um, and then you hold out this third piece, train on the others, test on this, and so on. So you do it five times, where on each time, you leave out one fifth of the data, train on the remaining four-fifths and you evaluate the model on that final one fifth. Okay? And so, um, if you're trying to choose a degree of polynomial, what you would do is- I guess for, you know, D equals 1 through 5. Right? So you do this procedure for a first order polynomial, uh, fit- you fit a linear regression model five times, each time on four-fifths of the model and test on the remaining one fifth, and you repeat this whole procedure for the quadratic function. Repeat this whole procedure for the cubic function, and so on. And after doing this for every order polynomial from 1, 2, 3, 4, 5 you would then, uh, pick the degree of polynomial that, um, sorry and then for each of these models, you then average the five S's you have for- for S error. Okay? And then after doing this, you would pick the degree of polynomial that did best according to this- according to this metric. Right? And then maybe you'll find that the second-order polynomial does best. Um, and now you actually end up with, uh, five classifiers. Right? Because you have five classifiers, each one fits on four-fifths of the data, uh, and then, uh, and- and there's a- there's a final optional step, which is to refit the model on all 100% of the data. Right? So if you want, you can keep five classifiers around and output their predictions, but then you're keeping five classifiers around this. Uh- uh maybe a bit more common to- now that you've chosen to use a second-order polynomial to just refit the model once on all 100% of the data. Okay? Um, and so the advantage of, uh, k-fold cross validation is that, instead of leaving out 30% of your data for your dev set on each iteration, you're only leaving out 1 over k of your data. I use k equals 5 for illustration, but in practice, k equals 10 is by far the most common choice that we use. I've sometimes seen people use k equals 20, but quite rarely, but, um, uh, if you use k equals 10, then on each iteration, you're leaving out just one tenth of the data. 10% of the data rather than 30% of the data. Okay? Um, and so this procedure compared to simple cross-validation, it makes more efficient use of the data, because you're holding out you know only 10% of the data on each iteration. Uh, the disadvantage of this is computationally very expensive, that you're now fitting each model 10 times instead of just once. Okay? But- but- but when you have a small data-set, this- this is actually a better procedure than simple cross validation. If you don't mind the computational expense of fitting each model 10 times. This- this- this actually lets you get away with holding on this data. [NOISE] And then, um, there's one even more extreme version of this, which you should use, if you have very very small datasets. So sometimes you might have an even smaller dataset. You know, if you're doing a class project with 20 examples this- that's- that's small even by today's machine learning standards. So, uh, there's- there's an extreme version of k-fold cross-validation, called leave-one-out cross validation, which is if you set k equals m. Right? So in other words, here's your training set, maybe 20 examples. So you're gonna divide this into as many pieces as you have training examples. And what you do is leave out one example, train on the other 19, and test on the one example you held out. And then leave out the second example, train on the other 19 and test to the one example you held out, and do that 20 times, and then you average this over the 20 outcomes to evaluate how good different orders of polynomial are. Um, the huge downside of this is just is completely very- very expensive, because now you need to change your algorithm m times. So you, kind of, never do this unless m is really small. Uh, I personally have- I pretty much never use this procedure unless m is 100 or less. I guess, you- your- if your model isn't too complicated, you can afford to fit a linear regression model 100 times, like it's not too bad. Right? So- so if- if- if m is, uh, less than 100, you could consider this procedure. But- but if m is 1000, fitting a linear model- fitting a model 1000 times, it seems like a lot of work, then you usually use k-fold cross-validation instead. Uh, but if you do have 20 examples, then you know, I- I would then- then- if you have 20 examples, I would probably use this procedure and somewhere between 20 and 50s maybe when I switch over from leave-one-out to k-fold cross-validation. Okay? Yeah. In 10- fold cross validation should we use [inaudible] k times to go. Yeah. So, um, right. So since you have k estimates, say 10- 10 estimates, we're using 10-fold cross-validation. Can you measure the variance on those 10 estimates? Um, it turns out that those 10 estimates are correlated because each of the 10 classifiers, eight-eight-eight-eight out of nine of the sets of data they trained on overlap. So, um, there were some very interesting results, uh-uh there's some research papers written by Michael Kearns, actually, um, it's like a long time ago, uh, trying to understand how correlated are these 10 estimates. And from a theoretical point of view, the- we- the- as far as I know, the latest error result shows that this is not a worse estimate in training error, but note- but- but maybe it's showing us in practice is not- you could measure it, but, uh, we don't really trust that estimate of variance, because we think all 10 estimates are highly correlated, or- or at least somewhat correlated. Yeah. Go ahead [inaudible] Whether we're gonna find using k-fold cross-validation in deep learning? Um, if you have a very small training set, then maybe yes. But deep learning algorithms depend on the details. Right? Sometimes it takes so long to train, that training- training- training on a neural network 20 times, you know, seems like a pain unless- unless you have enough data. Unless, um, your neural network was quite small. Right? Um, so it's rarely done with a deep learning algorithm. But if you have so- frank- frankly if you have so little data, if you have 20 training examples, uh - uh, you know there are other techniques that you probably need to use to boost performance. Such as transfer learning, or just more heterogeneity of input features, or something else. Right? Um, yeah. [inaudible] Sorry say again. [inaudible]. Sorry thank you for asking that, uh, this average set no. Um, I meant, um, averaging the test errors. So, uh, here, you will have trained 10 classifiers and, you know, when you evaluate it on the left at 110 for the data, you get it wrong- you get a number. Right? So you're looping 10 times, hold at one part, train on the others. Test on this part you left out. And so that will give you a number, and they go say oh when you test on test on this part you left out, the squared error was 5.0 and then you do it again, squared error was 5.7, squared error was 2.8. So by average I meant average those numbers. And the average of those numbers is your estimate of the error of a, you know, third order polynomial for this problem. So this is an averaging the set of real numbers that you got from this- so- so this loop gives you k real numbers, uh, and so this is averaging those k real numbers to estimate for this outer loop, how good a classifier with that degree polynomial is. Okay? Wow, actually, a lot of questions, there's one thing I want to cover, go ahead and this last two go ahead. [inaudible] I see. Sure. Yes, using something other than F1 score would it just mean other than average? Uh, yes, it would. Having F1 score is complicated. Yes. Uh, I think, I think we'll talk- actually, um, so this week, Friday, we'll talk about learning theory. Next week- next Friday we're talking more about performance evaluation metrics. So actually we will talk about F1 score? Uh, Mitch, one last question? How do you sample the data in the, in these sets? Oh, sure. How do you sample the data in these sets? Um, so for the purposes of this class, assuming all your data comes through the same distribution, uh I, I, I, I would usually randomly shuffle. Uh, again, in the era of machine learning and big data, there's one other interesting trend is which, which wasn't true 10 years ago which is we're increasingly trying to train and test on different sets. Uh, uh, we're trying to, you know, train on data, uh, collect it in one context and apply it to a totally different context. Uh, such as, um, we're trying to, uh, uh, you know, train on, on speech collected on your cellphone because you have all that data, and trying to apply it to a, um, uh, uh to a smart speaker where it was collected on a different microphone, in your cellphone or something. So, uh, if you are doing that and the way you set your train dev test split is a bit more complicated. Um, I wasn't going to talk about it in this class. If you want to learn more, uh, ah, I think at the start of the class, I mentioned I was working on this book, Machine Learning Yearning. So that book is finished. And if you go to this website, you can get a copy of it for free. Uh, uh, that talks about that. Uh, and I also talk about this more in CS230 which, which goes more into the big data. But you can, you can go, go and learn machine- you can also read all about it in, in Machine Learning Yearning. Um, if the train and test sets are a different distribution. Uh, yeah, but random shuffling would be a good default if you think you're training dev test on two different, right? All right. Just one last thing I want to cover real quick which is, um, feature selection. And so, um, so let me just describe what- so sometimes you have a lot of features. Um, so, so actually let's take text classification. You might have 10,000 features corresponding to 10,000 words, but you might suspect that a lot of the features are not important, right? You know the word the, whether the word the is called a stop word, whether the word the appears in e-mail or not, doesn't really tell you if it's spam or not spam because the word the, a, of, you know, these are called stop words. They don't tell you much about the content of the email. Um, but so if a lot of features, uh, sometimes one way to reduce overfitting is to try to find a small subset of the features that are most useful for your task, right? And so, um, this takes judgment. There are some problems like computer vision where you have a lot of features corresponding to there being a lot of pixels in every image. But probably, every pixel is somewhat relevant. So you don't want to select a subset of pixels for most computer vision tasks. But there are some other problems where you might have lot of features when you suspect the way to prevent overfitting is to find a small subset of the most relevant features for your task. Um, so feature selection is a special case of model selection that applies to when you suspect that even though you have 10,000 features, maybe only 50 of them are highly relevant, right? And so, um, uh one example, if you are measuring a lot of things going on in a truck, uh, in order to figure out if the truck is about to break down, right. You, you might, uh, for, for preventive maintenance, you might measure hundreds of variables or many hundreds of variables, but you might secretly suspect that there are only a few things that, you know, predict when this truck is about to go down, so you can do preventive maintenance. So if you suspect that's the case, then feature selection would be a reasonable approach to try, right? And so, um, here's the- I'll, I'll just write out one algorithm, uh, which is start with- this is script f equals the empty set of features. And then you repeatedly try adding each feature i to f and see which single feature addition most improves the dev set performance, right? And then Step 2 is go ahead and connect to add that feature to f, okay? So let me illustrate this with pictures. So let's say you have, um, five features, x1 through x5, and in practice it's usually more like x1 through x500 or x1 through 10,000, but I'll just use 5. So start off with an empty set of features and, you know, train a linear classifier with no feature. So the model is, um, h of x equals theta 0, right? With no features. Uh, so this won't be a very good model. But see how well this does on your dev set. Uh, so this way you average the ys, right? So it's not for your model. Next- so this is step one. In the second iteration, you would then take each of these features and add it to the empty sets. You can try the empty set plus x1, empty set plus x2, empty set plus x5. And for each of these, you would fit a corresponding model. So for this one you fit h of x equals theta 0 plus theta 1 x5. So try adding one feature to your model, and see which model best improves your performance on the dev set, right? And let's say you find that adding feature two is the best choice. So now, what we'll do is set the set of features to be x2. For the next step, you would then consider starting of x2 and adding x1, or x3, or x4, or x5. So if your model is already using the feature x2, what's the other feature, what additional feature most helps your algorithm? Um, and let's say it is x4, right? So you fit three or four models, see which one does best. And now you would commit to using the features x2 and x4. Um, and you kind of keep on doing this, keep on adding features greedily, keep on adding features one at a time to see which single feature addition, um, helps improve your algorithm the most. Um, and, and, and you can keep iterating until adding more features now hurts performance. Uh, and then pick what- whichever feature subset allows you to have the best possible performance of dev set, okay? So this is a special case of model selection called forward search. It's called forward search because we started with a empty set of features, and adding features one at a time. There's a procedure called backwards search which we'll read about that. We install all the features and remove features one at a time. But this would be a reasonable, uh, uh feature selection algorithm. The disadvantage of this is it is quite computationally expensive, uh, but this can help you select a decent set of features, okay? Um, so we're running a little bit late, uh, let's break. Oh, so I think, uh, I was meant to be on the road next week but, uh, because [inaudible] is still unable to teach, I think we will have, uh, Rafael, uh, uh, teach, uh, decision trees next week, and then also Kian will talk about neural networks next week, okay? So let's break for today, um, and, and maybe we'll see some of you at the Friday discussion session.