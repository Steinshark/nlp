We looked at bag of words model in our last video, and what we saw was to classify news articles. We created this vocabulary of individual words or tokens, and then we counted words in each of these articles. Now this approach works fine. But if you think about it, we are missing an important point here which is, in a language the order of words is important. And here in this model, we are just counting individual words but we are not capturing the relationship between the words. So for example if I have this sentence Dhaval sat on a sofa and ate a samosa, if I just replace the order of two words, the meaning of that sentence changes completely. So basically, in a language the meaning of sentence is dependent on the order of the words. So if there is some way to capture the order of the words in our model, that would be useful. So again in bag of words model what we did is individual tokens or individual words. But what if instead of individual words we capture pair of words. So it's like a moving window. So we'll just say, Dhaval sat, then sat on, on a, then sofa and and so on. So instead of individual words, we are capturing pair of words, and this is called bi-gram. Bi-gram, there is another thing called a tri-gram where we have a moving window of three words. So first we'll say Dhaval sat on, then we move it by one word, and we'll say sat on a, then on a sofa, and so on. The generic term for this is n-gram. So you can have 4-gram, 5-gram and so on. And when you look at these bi-gram, tri-gram and so on, you see some meaningful words. So he here see in a bag of words it's just individual word, right? Dhaval sat. But in bi-gram Dhaval sat Dhaval sat on, now it start making sense. The bag of words is a special case of bag of n-grams. Here the value of n is one. In case of bi-gram the n is 2. Here n is 3 and so on. So basically, when you look think about all the text representation techniques, bag of words is a special case of bag of n-grams where n is one. Let's look at a few other sentences. So here I have three documents and in NLP generally document means a text, a paragraph, a news article, all of these are called documents. If you have to build a bag of words model, the general approach is you do some pre-processing. You remove stop words and you know you uh do lemmetization, and then you get these kind of clean text or post process text. And then you build bag of words model or count vectorizer model, right? See folks please watch my previous videos, otherwise some of the things I'm saying will not make sense, and then for document one, two, three you just count number of words in that document. So this is our entire vocabulary. Thor ate pizza, then Loki tall, Loki is already there, eat is already there, pizza is already there. So this is the entire, it's like a union of all the words in my vocabulary. And then doc 1 doc 2 doc 3 is nothing but you know in doc one, Thor came one time, ate pizza, so these are just the the word frequency. Now what if we use bi-gram where each unit here is two words. So thor ate ate pizza, Loki tall Loki ate, then ate pizza, but ate pizza already appeared here, okay? So this white text covers all the bi-grams in my entire vocabulary. Entire vocabulary is doc 1 doc 2 doc 3 and number of documents. Then you take individual document. Let's say doc 1 and in doc 1, Thor ate com comes one time, okay? Ate pizza came one time. Loki tall in document one, which is this did not come, right? So you have zero here. Then Loki eat also is not present in this document, so therefore zero. And similarly doc 1 and doc 2 you will count the number of bi-grams in these documents, and create this kind of a vector. And these vectors can be then used to train a machine learning model. So one approach that people use is, they use 1-gram and bi-gram combinedly because then you can have a vector, which represents the whole sentence in more meaningful way, okay? So I just combined basically, single bag of words, and bi-grams, and here what happens is now when you look at doc 1 and dock 3, you will see a lot of similarity. See Thor ate pizza, Loki ate pizza, kind of similar, you know. Some marvel characters eating pizza together together. And if you look at the similarity between let's say dock 1 and dock 3, you'll see there is 1 here, there is 1 here. Then pizza 1 here, 1 here uh 1 here and 1 here. So on three instances you are having a match. Whereas between dock 2 and dock 3 there is Loki is a single you know single word match, and that Loki is here, see between 2 and 3, 1 and 1. But other things are not matching. So you can see that by using um single 1-gram and bi-gram, you can create more meaningful representation of your text. And the limitation of n-gram model is again as n increases the dimensionality or sparsity increases. We have seen in previous videos also. Here see we we took a very simple example of three words. But in reality, your universe will be hundreds of news articles, or hundreds of books. The vocabulary, this white line will be so long, okay? so as your n-gram increases, the sparsity increases because see here we have two grams, right? so as I said usual approach is one gram and two gram combined. Let's say you have three grams, then then it keeps on increases increasing. So your dimensionality and sparsity increase increases, which results in a more computation need and performance hit, also the memory issues. And this also doesn't address out of vocabulary problem. All the presentation that we have seen, so far in this tutorial playlist, do not address the out of vocabulary problem, which is while training a model, you train on a certain dataset. But when you do prediction, you get totally new words and it's now hard to represent that in a vector. All right let's do some coding now. I have opened jupyter notebook and imported count vectorizer, which you may remember from our bag of words model. And when you create an object of this class and fit a simple vocabulary, where I have just single sentence, it will create a vocabulary like this. This is again bag of words model, which we have seen in the previous videos. Now if you look at the sk learn documentation of this particular class, you will find one interesting parameter which is n-gram range. So by default it is 1, 1 which means it was using bag of words just one word or one token. But we can change that and we can use this particular parameter, if you want to generate let's say bi-gram, so if I do 2, 2 here so if I just apply this n-gram parameter okay, and let's say just do 2 2, you'll see that now it is creating a pair, as a single unit in the vocabulary. If you change it to 1,2 what it will do is, whatever we saw in our presentation, which is it will create single tokens and then a pair of tokens, if you do one of one and three, it is generating single tokens, bi-grams, and trigrams okay? And this is as per the presentation that we just saw, correct? So when you do bi-gram it is doing exactly this. See single token, then pair. When you do tri-gram, single token, pair of tokens and three tokens. Now let's look at the corpus that we saw again in the presentation, which was you know the party eating pizza, and for this one uh I'm going to first write a pre-processing function to remove stop words and do lemmetization, etc. and then I will apply bi-gram count vectorizer, and I will show you how the vector space model looks like. How basically vector, see vector space model is a jargon, how it converts text into a vector, okay? So that's what we are going to do next. So in the pre-process function, first I will import spaCy library and our NLP model, create this particular function call pre-process and then, we just create a document out of the text, and spaCy will give you all the tokens. So you can just say for token in dock, now this token can tell you whether it's is this is it a stop word or is it a punctuation, you know if you want to do text preprocessing, and we want to just ignore the all these tokens basically, and just consume remaining tokens. And I will just say filtered tokens is this and in Python list you can just do .append to append new new values in that list, and here we are going to use the lemma. You know in our lemmatization tutorial, we saw that lemma gives you the base word. And then we can either return the whole list. But we don't want a list. We want to convert list into a string. So what you do is, you'll just say space .join When you do this it will convert a Python list into a string, separated by spaces. And then we can just quickly test our sentence, and it will say see it converted ate into its base word called eat. If you try let's say Loki is eating pizza, see it removed is because that was a stop word, and it converted eating into eat. So this preprocessing function looks pretty good good, and now we can say the corpus processed is, we can run a forloop on our original corpus, and corpus is nothing. It's a collection of documents, or a list of strings in our case. You can use Python's list comprehension, and you can just say for text in corpus, you want to apply pre-process function on all the text. And when you look at process text now, see Loki eating pizza, is Loki pizza is is removed and so on, okay? Now that we have uh process text, we are going to use a count vectorizer okay? And in this count vectorizer I will use n-gram range of 1 and 2, and I will do fit. So when you do fit, it will it will create that vocabulary, okay? And if you look at that vocabulary, it looks something like this. So you see single tokens and then pair of tokens, and these indexes are just the indexes, into into a vector. So let's once you have this vocabulary prepared, you can take a new sentence and you can do text to vector conversion, right? That's a whole topic since last few videos, which is in NLP machine learning models require numbers, they don't process text. So in NLP we convert text into number and the way we do it is by using vector space model, which is just converting a text into a vector. Vector is basically a bunch of numbers. In Python you can think of it as a Python list, list of numbers. So that's that's our goal, that's what we're trying to do, which is we have a sentence called let's say Thor ate pizza and we want to convert that into a vector using bag of n-grams model. And the way you do that is you say, v.transform okay uh v.transform Thor ate transform and it will give you a matrix. And if you want to convert that to one to an array, you can just use this sentence. So see, we converted text into a vector using bag of n-grams models. We did the same thing using bag of words model in last few videos. Bag of words one hot encoding and so on. We can use one other sentence which is Hulk ate pizza. Now Hulk is not in our vocabulary. Where is our vocabulary? Well see this was our corpus, and in our corpus we never saw Hulk. So we will face out of vocabulary problem, OOV and it looks something like this. So, I have already made this notebook ready, and I want to just show you a picture. So when you do this right, Thor ate pizza, Hulk ate pizza. Actually it first took the vocabulary, so this was our vocabulary right? And see eat is at 0 index. Eat pizza is at 1 index. So that's what I did. Eat is 0 index. Eat pizza is this, and you have total 8 units in your vocubalary. So I put them here, and then we take individual sentence and we try to convert it to a vector. So when you do Thor ate pizza, you got 1 1 0 0 0 1 see 1 1 0 0 0 1 etc. and the way it will do it is, see eat is present in Thor ate pizza. So it will put 1, eat pizza is also present so it will say 1, Loki is not present here so 0 and so on. Just pause this video and look at this image, you will get an idea on how bag of n-gram model works. All right, now we are going to look into a news category classification problem, which is this. So I'm going to use this Kaggle data set and there is, uh where is the data? Okay. So I have I have taken the dataset from here, so news category okay, it is this actually news category data set yes, and in this dataset we have six categories. So there is a news article, and there are categories. So what I have done is, I have taken this json file, which i can show you. So if you look at, so this json file is gonna be big, it's a big big file so it will take some time to do to load. But this json file basically what it has is a news article, and the category, okay? So let me just minimize it and let me just directly load this thing into pandas data frame, and I will import pandas first okay, and then pd has a method called read json where you can read a json file okay? And this will be your df you can print df.shape and I will do df.head here. So see there is a news and it its category, and this is one of the classical problems in NLP where you want to auto extract or auto tag a category, whether it's a political article, business, crime what kind of news story this is. I will do quick exploratory analysis and I want to figure out how many categories are there. So you can do and also I want to get a count of these numbers, just to check if there is any imbalance in the in the dataset or not. And you can use pandas value counts function for this. And it tells me business category has 4200, almost those many news articles. Sports category also has them that many. But see science category has almost one third of the article, than the first two. So there is some imbalance in this data set okay? Now I already trained a model on this imbalanced dataset, and I am feeling a need of handling this imbalance. Now to handle class imbalance, there are four or five different techniques, okay? And if you go to YouTube search code basics class imbalance, I have gone over all those techniques like oversampling, undersampling, oversampling using smote, ensemble, etc. in this particular video. So make sure you watch it, if you don't know about imbalance or class imbalance in machine learning. But here since this video is not about class imbalance and it is about bag of n-grams, I am going to use the most simplest technique called undersampling here. And undersampling is very simple. You have minimum samples 1381 for science category, correct? So for remaining categories, you will take 138 samples, random samples and you will ignore remaining samples. Now again, this is a tutorial, and I want to keep things simple. In real life they say that wasting data, wasting training data, is a sin. So you don't want to discard all the samples, okay? You want to use maybe a technique such as smote. But here let's just initialize this parameter called min samples is this, and then what you'll do is, in your data frame you will say df . category okay if the df.category is business, this will give you this will give you a data frame, which has only business as a category, okay? And then that data frame you can sample, meaning you can take random these many samples. 138 samples, and I also want to you know use a random state variable, so that if you rerun this notebook, there is some predictability to it. And you can provide any seed you know. This is a random number 1 5 10 The idea is, when you run this cell multiple times, this notebook multiple times, if you have the same random state variable, the sampling will be similar. We will call this a df business data frame, and again if you look at df business see this is that, and 1381 rows, okay? And we will similarly create remaining remaining data frames, and using pandas concat function so pd.concat what this function can do is you supply bunch of data frames into this argument, and it will just add them. It will add them row by row, okay? So I'm going to add all this, and you can do concatenation at column level or row level. And if for doing it at row level you have to say axis equal is equal to 0. If you don't know about pandas just watch my pandas tutorials, and this will call it a balanced data frame. And in the balanced data frame, when I say category.value counts see, hurah! It is balanced now. All right, I'm now ready to train our model. So first thing as usual we'll do train test split, and for train test split, the first argument, that we will supply and by the way this this parameters, output parameters I can specify here. x train x test y train and so on, the usual thing that we do in machine learning, model building. Here I am going to give a balanced data frame, and the text, okay? And category. Now look, it cannot take category text right? So this text business, science, etc. we have to convert that into numbers. So let us do that first. So we can define a dictionary, okay? And in that dictionary we can have numbers mapped to the each of this category, and then we we can say df balanced Actually you know what, this should come after this. You can use a b shortcut to create a new cell shell in notebook. And df balance category number, is equal to df balanced. category.map So when you do map, what this function is doing is, it is converting this category from string to number. And by the way I am feeling that I don't need this target variable because I'm just going to use it once. So just supply it as a parameter here. Okay? And you look at your balance see business is zero, similarly it will have assign number to all the categories. And now we are ready to train test split and we'll supply category num as a target variable, and the test size I will keep it 20% So 80% samples will go in a training data set. Here also I'm supplying random states, so that, see random state again when you run this notebook multiple times it will make sure this this split is kind of consistent. Now we are going to use one more parameter called stratify. So that what this will do let me explain you that. So df balanced category number it will create equal number of samples from all the classes in train and test. Okay? If you supply this argument and you can verify that by doing I'm just gonna print uh the x strain here, and then we'll also look at the y train value count. And you will see the effect of this stratify parameter see? The samples from all the classes are similar basically and you can do y test also. This way the model is not biased it is treating all the classes in kind of an equal uh fashion. Now let me build first a bag of words model. I have imported necessary libraries to build our machine learning model. I'm using naive bayes model, but you can use other models. I would actually suggest you use KNN, Random Forest, Decision Tree like that and just compare the performance with naive bayes. Usually naive bayes model is recommended for text based problems, and there is a reason for it, and I want to give this as an exercise to you, and kind of figure out, the performance of different models. And here we are going to create a pipeline object and pipeline object, takes list as an argument. And here you can specify your pipeline, and your pipeline is basically a vector, right? So let's say this is first stage, this is second stage and you can give a name let's say vectorizer bag of force, and here you can specify your count vectorizer. So here I am going to first use, simple this is simple bag of words right, which we saw in in our previous videos, and multinomial naive bayes, I I will supply here and I will call this thing a classifier, and then call fit. So this is actually training the models like x train y train and then you do prediction right? So you will say predict x test So on text x takes test you do prediction, and you get y prediction as an output, and then you print a classification report. And in the classification report you always give y test first, and then y prediction. So the truth and the prediction, and we use print method because otherwise the formatting will be little awkward. So see we just train a model using bag of words, which means 1 gram okay and the performance looks decent. You can see here for this 0th class which is business, the precision is little lower. By the way if you don't know about precision, recall, etc. go to YouTube search precision recall. I have made a very simple video even a small kid can understand these jargons precision, recall, F1 score. So watch this video. Okay now I'll keep this parameters as is, and I will use C and V So C and V will copy paste the entire cell, and now I will use the n-gram range, okay? So in the n-gram range I will supply a 1 1 or 1 and 2 actually So this is 1-gram and bi-gram. And if you look at performance and compare it see 81 86 86 85 the performance is little lower compared to bag of words, and it is okay. Based on a given problem set, you might find you know that maybe bag of words is a better choice, and bag of words again it's it's 1 gram okay? So here we are using 2-gram 3-gram, so if you use 3-grams say again performance is not as good as our bag of words, right? So, and it's okay. As I said based on a given set of problem, you have to do some trial and error and kind of figure it out. And if you want to do by the way from this this model, if you want to do some prediction you can do do it. So let's say I have x test, the first five articles are like this, and when you look at y test, first five it is like this, and y prediction first five See the first five, so 0 it made a mistake in this one. But 4 out of 5 predicted properly okay, and these categories are by the way, if you look at these categories, oops what happened? Correct business is 0 so see man killed by Michigan. This is when you look at this, you know this is crime and our prediction says it is crime see, because two is crime. Then you say build loyalty, the cost, that is business our business article correct? And the spots okay, we don't have any spots. So business and the crime okay. So you can you can just play with it and just supply your own text and do a prediction. Now I want to show you the usage of pre-processing, right? You will be like okay we did not do any pre-processing here because when we train this model, the x train that we supplied that came from df text, and df text is unprocessed text. So now I am going to create a pre-processed text in our data frame. So the way you create a new column in our data frame is by just saying this and you will say df balance okay. So df balance. text .apply And what we will apply, remember we already designed this preprocess function, okay? And when you do that it will take some time because it's little bit time consuming. This function will take time because you know there is quite a bit of processing. But once it is done after a few second or minute based on your computer, you will see a new column called pre-processed text. And here you can see that see here, how to market your business. Now it's market business, so it removed how to, which was stop words. Your also stop word and it also it also uh converted everything into la it's lemma, okay? Now we will train the same model, exact same model, but with one difference okay? So this is exactly same model. But instead of category, I'm using instead of text, I'm using pre-processed text, okay? So let's train this model now. And the way you do it is, I'm going to use again a bi-gram. So again copy pasting the same, same thing, same exact thing okay? And when you print a classification report, it looks like this. So now I I want to compare our model train with pre-processed text, with raw text. And the way I do it is you know I use this snipping tool. So in Windows there is this snipping tool, and I will just take a snippet, I'll take a snip off the pre-processed text model performance. I will scroll up, and find out the performance of raw text okay? And and just compare these numbers raw text versus this. See here you see 69 here you see 80. See this 80, this whole number you compare it with this. It's better right? In in majority of the cases. Some numbers you will see here and there, uh but look at F1 score say 84 87 87 86 and here it is lower. So we are seeing that you're getting a benefit of using uh of using a post processing here, basically pre-processing here. Uh so in this particular problem, it makes sense that you remove stop words and do lemmetization, etc. you will find many other cases where doing that preprocessing might not give you a result. But general recommendation is that you do preprocessing. Okay I also have a code to plot a confusion matrix, which I will give in a notebook. So check the video description below. Uh there is this notebook and I will add exercises. By the way as I said like, I add exercises later on also. So you always check a video description because by the time you're watching this video, there will be an exercise available. And folks learning coding, machine learning, NLP is like swimming. If you just watch video, it's waste of your time. You have to practice. If you don't want to practice it, then don't watch my videos. Go watch some movie on Netflix okay? So practice is very important. If you just do Codebasics NLP playlist, for example and if you go to let's say stop word video, all right? Uh in this video, I initially did not have an exercise. But then later on, see I added this exercise. I and my team basically. Thanks, Kiran! So see you have to work on this exercise. Fill all in all these cells, and I'm go giving you a lot of hints and see, I'm giving you a lot of support. Do not click on solution link unless you have tried it on your own, okay? So thanks very much for watching. In the next video, we will be looking at TF-IDF, another text representation technique.