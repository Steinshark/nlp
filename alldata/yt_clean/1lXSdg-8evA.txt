I want to build a big all-in-one Homelab server. I want to see if I can build one single machine that can handle all of my compute needs, whether that be running Plex or running some of my CI builds, some upcoming things I want to do with local AI, and even video conversion. I want to run them all on the same machine without any limitations. I have a couple of goals I want to achieve with this new build. I want it to be a single machine to run all of my compute, whether that be CPU or GPU based. I want lots of PCIe lanes for that GPU and devices that I might add, lots of RAM, and lots of room for growth so I can expand in the future. Speaking of room, I'll have to decide between the roomiest case I have, or something rack mountable that might even have water cooling, to a case that's optimized with plenty of space for drives. My choice is on why later. I also want this to be 100% flash based. That means no 3.5&quot; (edit) spinning drives, only 2.5&quot; SSDs and smaller. I also want fast networking, at least 10 Gbps, so that networking throughput is never a bottleneck. Oh, and one more small but important thing, I don't want anything to be flopping around or dangling like I've done in the past, but hopefully this won't be an issue. One role that I'm not going to have this machine take up is a NAS. I already have a dedicated NAS, so there's no reason to take on this task with this machine. My first big decision was the case. I really wanted to build something in this Sliger case that was sent to me a year ago to help with the server build, but I haven't been able to use it yet. It's the CX4170A in black and matte white. It has plenty of room for the GPU I want to use, my Nvidia GTX 3090, which I pulled from my Windows machine for the time being. The other nice thing about this case is that it's one of the few that supports water cooling, with a supported AIO cooler, which I plan on using as well, because it looks cool and well... So I can say that one of my servers is liquid cooled, but air cooling would have just worked fine, if not better. I have been saving this case for something special, which was my new Windows build to rack in my studio rack, but I had been waiting on Intel to release their 15-gen processor since the 14th but it wasn't that great of an upgrade And I don't want this server to be a desktop class processor build either, since I'll need lots of RAM and PCIe lanes, and the latest Intel desktop processor only supports 20 lanes, and that's like a video card and one NVMe drive. So an Intel desktop processor is out of the question, and I only get a few more lanes out of the AMD counterpart, so that was also out of the question. Also, the AIO for this case is designed for an Intel desktop CPU, so it looks like I'll be saving this for a little bit longer for my rack mount Windows machine in my studio. And I honestly consider this old Corsair Air 540 case. Remember that? It's one of the few cases that I have that I've saved throughout the years because it has so much room, and if it weren't for the server rack in my basement, I'd probably have a few of these for all of my servers. I ended up not going with this because, well, I do have a server rack, and I should probably use it. This left me with two options. Buying something new and delaying this whole project, or using something that I've had for a while, and it's the Stornator from 45 Drives. The problem with the Stornator is that it can't fit a 13-inch video card in this chassis due to the cage for the 3.5&quot; (edit)-inch drives. The card is almost 3.5 inches longer, and even if I removed the row of fans in the case, it still wouldn't fit. It also has this odd power supply with these ginormous Molex-looking adapters that I've never seen before that connect to the backplane, and it doesn't seem to have the two 8-pin power cables that I need for the video card. Time out. If you're building something similar and you don't need 15 drive bays or a backplane, I suggest going with the Sliger case I mentioned earlier or some other affordable options. I'll have some links below if you're interested, but there are definitely more affordable 4U cases out there. Okay, back to getting the server built. So rather than buying something new, I decided to explore hacking away at the Stornator a little further. Now I was open to buying a consumer power supply and converting all of these cables, but if the video card doesn't fit in the case, the project might be scratched for another day. Then I thought that maybe I could just remove the drive cage and then move the fan and even possibly move the backplane. After all, I wasn't going to put any 3.5&quot; drives (edit) in the server, so maybe, just maybe, this case could be used. Before I started ripping the server apart, I wanted to be sure that the CPU and motherboard could accommodate everything I wanted to. I already upgraded this server to 128GB of ECC RAM, so now I just need to verify the lanes. So it looks like it has 8, 16, 8, 16, 8, 8, 8. And I want to make sure that these 16s are maybe 16, and these 8s are really a full 8, because sometimes they'll see that they have an 8 slot, which actually runs at 4x, or a 16x slot, which actually runs at 8x. ...and about the processor, it's an Intel Xeon Silver 4210 processor. So what I'm looking for in here is how many lanes this can address. Oh, right here. So it can address 48 lanes. So if I do the math. 8, 1, 2, 3, 4, 5, 6. So that's 48 right there, and then I have this 4. So I think I just need to keep in mind, just, if I'm going to populate all of these with 8x, just don't populate the last one. But I probably won't. I don't have that many devices, and I might need some additional spacing, as you can see here. I'm going to have my video card here, which is going to take up probably these 3, or if not, these 2. I need to check again. Since my CPU and motherboard have enough lanes for everything I want to do, all I have to do now is start removing parts from the case. I removed the drive cage, which revealed the backplane. At some point, I would love to move this closer to the front so that I might be able to use the second row of fans, but until then, I will keep it where it is. Then after removing the row of fans, I installed the video card and removed the power supply. I gave it a quick cleaning since this will be the last time I have it open up like this, and that's when I realized, so after taking the whole thing apart and disassembling this power supply and cutting all the zip ties, I realized that a long time ago, I bundled these 2 connectors in them, which are these 8-pin PCIe connectors, I think they are, for power. And they actually plug into this video card. So it turns out I don't need another power supply, and I can put this back in. And I don't need to do anything custom with these ports, these gigantic Molex pins. I can just use this power supply. So even though it kind of stinks, I tore it out and cut it apart, it's probably a lot easier than coming up with something custom or getting a consumer grade power supply and fixing or making pin conversions and wiring all that up. So, but yeah, I'll just have to put it back in. So it's actually good news. I popped in the video card so I could see exactly which PCIe slots were going to be usable and which ones would be covered by the GPU. So it looks like that 8's covered. And then we have what, the 16, which is really 8. And we have another 8. And then we have the, yeah, we have the other 16, which is really an 8. And then this an 8. So yeah, it's a tight squeeze. So it looks like I only have this 8 to play with, this 8 to play with, and then that 4 because this video card is so big. Then I just needed to figure out where to put the HBA and the NIC based on airflow. So what I've decided to do was put my HBA here in the second to last slot. I feel like since this has this cooler here, air could possibly pass there a little bit easier than up here. And then I'll keep my 10 gig card in this, really this third slot. And this fan will hopefully blow enough air across there to cool it off. There's also this fan, which I assume is going to intake some warm air into the video card. We'll see how that does. And then I left this slot open just because otherwise this heat sink was really close to this video card. And I wasn't sure what that was going to do. This really wasn't anything scientific. I just went with my gut feeling, but I think I will test this later on and even monitor this over time. Now that I had most of the parts in place, it was now time to figure out what to do with the backplane and how I could possibly get 15 SSDs in here at some point. This might be kind of jank too, and I'm trying to limit the jank, but these do stand up pretty well on their own. I mean, they're not falling over. They're not going anywhere. And at least for my proof of concept while I build this out, if this does work, what I might end up doing is, like I said, move this back and then try to 3D print something so that I can have these drives slide in and then also have some support so they're not standing like that. Now I would love to print something like the existing drive cage, but instead of securing three and a half inch drives (edit), it would secure two and a half inch drives. So I think something like this would work that would be printable and I could just line it up, slide it down and obviously have the drive be lined up to where it could snap in. So I think something like this is doable where I could have some screws, print a couple of these going all the way across, and then that would give me a way to easily put in 2.5 inch drives, basically all SSDs. I talked to some of the folks in my Discord server and we found that 45 drives does have printable replacement parts for the HL15, which is the same as the Storinator. I think this could be remixed to work, but then that also means I need to buy a 3D printer. I think this is finally going to be the tipping scale for me to get one. Anyway, until then I think I'll install just a few on the right. I have one Samsung Evo one terabyte. I buy all of the same size so that I can replace them anywhere like my colo. They all have these drives and I only have one and this fits in a little tricky. So this fits in fine. I think what I'll end up doing is buying five more. So one, two, three, four, five, six. So I'll have six drives, six SSDs to start with. Now that I know the drives will fit and stand on their own, it was time to pick up five more drives, which also means that's going to hurt, but they'll be here by today. So ouch. Now it was time to put everything back together, starting with the power supply and managing some of the cables. I wanted to test the machine now before button everything up just to make sure that it would actually power everything on. Yeah, let's turn it on. Let's see. Hey, hey, hey. Looking pretty good. Yeah, that's moving a decent amount of air to where I think it's going to keep that cool. Not to mention, you know, there's intake right there, but yeah, so far so good. Awesome. Okay, I'm going to zip tie this up, clean it up a little bit more, run some more cables, and then I'll get an OS on it and we'll test out the video card, make sure we can see it. Then I decided to do a quick power usage check while I had it on my work bench. So it looks like it's hanging around, well, 160, 170. And 70, is it going to go higher? Yeah, it looks like it's hanging around. Yeah, 170, 180, 200. This is probably booting up now. So I'll test this idle with a base OS on it just to rule out any additional testing that's going on from the BIOS and from the boot sequence. But it is what it is, 180. This seemed a little bit high and I think it might've been due to the fact that it was booting up. So I'll test it idle later on with all drives and a base OS. So as you can see, the machine is back there. I have a little bit of testing I want to do. I want to make sure that I can actually install the video card drivers and do something with the video card, or at least run NVIDIA SMI to make sure that it's working. I'm going to use Ventoy to install it. Yes and no, Ventoy is a thing. I'm going to use it and we're going to test it out. Then after that, we're good and keep going. At first I thought I would just install Ubuntu Server and just do a quick and dirty test to make sure that all of the hardware was working. So yeah, I'm going to use Ubuntu Server. As you can see here, I'm downloading it here, but going to use Ubuntu Server primarily to get it all set up and to do some testing. After that, I might switch to Proxmox or might go bare metal. Ubuntu, haven't decided yet. After that, I connected it temporarily to my studio switch at 10 gig. I'm going to put in this SFP+ to 10 gig ethernet adapter into that device because I don't have a long enough DAC cable up here. So let's try that. Here is that one that I just put in. I actually don't need to do this. That one's not going to anything important. And then I'll do the same on the server. And then back on the server, you can see I converted that from SFP+ to 10 gigabit ethernet. So I booted it up and then I use IPMI to connect to it. I realized when doing this, I still had an operating system on it. The nice thing about using this installation is that I already have a virtual machine on it that I can use GPU pass-through with. And I want to be lazy. While it was booting, I decided to do a quick temperature check. I mean, it's just idle right now. 31. Video card is getting a 33, 38 over there. This is the 10 gig NIC, so 56. That's pretty warm. And then this is HPA, so 34. Yeah, I'm going to touch it just to see. Yeah, 34 is pretty warm. Yeah, 50 is really warm. I'll definitely keep my eye on this if I stick with this build. Once it was bootable, I can remote into it and then test the GPU pass-through. So let me go into this. I'm going to go into the Windows 11 machine. Let me then add a device, PCIE device. You got to go to raw device. And do I see my video card looking again? Did I just totally miss it? Huh? Oh, there we go. So there it is. RTX 3090. Let's say all functions. Let's say PCI Express. And I won't do primary GPU. It should be fine like this. Adding it to there. And let's start it up. Once adding the GPU and starting the virtual machine up, I started overloading my small UPS that's here in my studio office. Sounds like I'm pulling too much power in here, which might mean that the video card's on. Once getting in, I could see the video card in the device manager. I just needed to install the NVIDIA drivers. To download the drivers, I've been using this NV Clean Install, which is pretty nice. It basically installs a clean install of the drivers without everything else. So let's go into the NVIDIA control panel. Just make sure I can see the 3D rendering that we typically see the spinning NVIDIA logo, which usually tells me 3D accelerations turned on and that the driver's working okay. Look at that. The preview's running. Perfect. The GPU is working fine in this machine. Now we just needed those SSDs that were supposed to be delivered the same day. Look what just arrived. Doesn't feel like hard drives. False alarm. It was a shower curtain. A few hours later, they did finally arrive. All five drives. That's hard, Jonas. Now I can create a ZFS pool with three mirrored V devs for fast reads and fast writes of data. This is a lot of IOPS that I hopefully will test later on. Now it was time to reinstall the server into my rack by myself, which is kind of awkward. I feel like putting this in by yourself is like trying to dock a space station. Or like trying to land the plane on the carrier in the Top Gun Nintendo game. I think I got it. First try. If you've ever played Top Gun before on NES, you know what I'm talking about. After it was installed, I connected networking and power, and then I realized there was a gap in the case. Okay, well, I'm going to screw this in. I feel like I'm taking crazy pills right now. I guess I'll have to review the footage and see how that lined up. Then I realized it was part of the fan housing. Okay, so mystery solved. These screws right here are what the top of that case screwed into, and then this filled in the gap. I never noticed that before. This will be something I'll need to print too. Another reason to get that printer. Now I have the hardware kind of settled and I have a path moving forward, even with some dangling for now. The next step will be choosing the OS. I have so many choices here. I could just keep Proxmox on it and create one big VM that uses the GPU so that I can easily back this machine up and create more VMs if needed. Or I can go bare metal and install Ubuntu Server or some other flavor of Linux so that I don't have to deal with a hypervisor at all. Or I could even create a specialized node in Kubernetes and join it to my existing home cluster so that I can take advantage of all the automation that I already have with Kubernetes. Or I could go all wild and try something out like Unraid, which I've never tried before. To be completely honest, I haven't decided yet. There are upsides and downsides to each, and if you have one you think I should go with, let me know in the comments. So yeah, it looks like there will be a part two to this video which will be choosing the operating system for my ultimate all-in-one home lab server. Well, I hope you enjoyed this. Subscribe if you haven't already. I'm Tim. Thanks for watching.