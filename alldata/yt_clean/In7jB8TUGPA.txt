Today we are going to talk about different types of NLP tasks. In past videos, we have covered some of it but today we will cover more of this task with some technical details. The first one which is a classic task is text classification. Let me give you one a real life use case. I use a software called Camtasia studio to record and to post process my videos. They have this support forum where you can go and no you can report a bug or an issue with this software. Now there are so many uh complaints coming up and for Camtasia, for TechSmith which is a company behind Camtasia software they need to prioritize these complaints into either high or low or you know like medium severity. Now the way you can do that is you can maybe look at the text. So let's say this is some example where you are saying Camtasia 10 won't import mp4 file. Now it's a pretty bad thing, so it's a high severity complaint. You can have a human looking at this complaint and manually classifying it. But if you have too many complaints, hiring humans is probably costly and you can use natural language processing here. So what you can do is you can convert this text into some kind of a vector and you can use a method such as TF-IDF. Now you'll be like, &quot;What is this jargon, TF-IDF?&quot; I don't like to use jargons that much. But in this video, I am going to throw you some throw at you, some jargon so that it arouses this curiosity in the mind and in the future video, we will cover technical details of these jargons. So don't worry about what is TF-IDF, just think it is some way of converting text into a vector or a set of numbers. Once you have these numbers representing this text, you can also call these features. You can then use a classifier such as Naive Bayes classifier to classify the complaints as high, medium or low. Here this particular complaint is a high complaint because you know not being able to import mp4 file is a big deal. So then what will happen is the way Techsmith guys would have designed the system is if the complaint is higher, you have integration with a software such as Sentry which is basically a monitoring tool and then that can route a call to your call center, and some customer care representative can assist you with that ticket. You can also have a different type of complaint where you're saying, okay I'm not able to find a video tutorial. Now this is not a high severity complaint, it's a very low severity. In this case maybe your workflow will be to create a jira ticket with low priority and maybe someone will look at it after a week or so. So this is how big organization uh design the system where just to summarize, you have incoming complaint, you can use some form of vectorized vectorizer, here I'm using TF- IDF and you can convert it to set of numbers. And by the way these numbers don't mean anything. I just put some random numbers, this is just to give you an idea. So don't think too much about why it is 0.12 and 0.5 and so on. Then you can use classifier, if you've seen my machine learning tutorials you might know about some classifiers and these classifiers can actually classify complain into one of the categories. Now you'll of course do machine learning training, you will have a lot of your old complaints which will already be marked as high, medium or low and using that label dataset you will train this machine learning model. So this was first use case of text classification where you take the complaint and you categorize it into one of these 3 categories. The other use case is in healthcare domain uh you have all these health documents. So on the left hand side, I have a prescription which is you know you go to a doctor they will prescribe you some medicine. On the right hand side, I have a patient record. Now these are scanned documents of which you know hospitals or doctors facility would scan it, they would upload it to a cloud location and now one of the use cases in healthcare industry is to classify these documents. By classification, I mean whether it's a prescription or patient medical record this staff will have hundreds of such papers they will just scan them very fast and put it on a drive. And they don't have time to classify these documents and here is where machine learning system can help you. So as a first step you can use maybe Google's OCR which is tesserect to convert image data into text data, and then you you you can use an approach like Doc2 Vec which is document to vector to convert text into a vector. And then similar approach as the previous one which is logistic regression classifier, which which which can tell you whether it's a prescription or patient medical record. Again don't worry about Doc2Vec or tesseract. I'm just throwing some jargons at you so that you know that we are going to cover all of these amazing technology in the future. But you just get a overall idea. The third use case of text classification is uh hate speech detection by Facebook. We all know that Facebook have many groups which are either racist group, or terrorist groups or generally some people put lot of hate speech on Facebook and they build this classification algorithms where the algorithm can read the text and it can tell you whether this is a hate speech or not. And you can see in the bar chart in 2020 Q1 they reported more than 8 million hate speeches. So around this time, they upgraded their algorithm they made it more smarter and they benefited from that. So now they can automatically delete those hate speech post from Facebook. LinkedIn also have a similar issue where people make fake profiles, and based on some keywords and some past knowledge, they build an algorithm using NLP which can classify a profile as a fake or not fake. And based on that they can block that particular profile. So there is this article, I'm gonna link this article in the video description below. You can check that. The second use case is text similarity. So we have seen in our demo of hugging face where you can let's say you have this use case of matching resume with the given job skill. Nowadays recruiters get so many resumes. Let's say you have one data scientist position open, you get 1000 resume. You might not have time to scan, read through all of them. So you can use text similarity here to figure out which resume is matching uh the job position. So let me just show you. So here uh in Google I search for hugging face sentence transformer. If you click on this, you will get this sentence transformer. If you sort by most likes and click on this one, and here see this is doing sentence similarity. So if you have this that is a happy person, that is a happy dog and if you compute it will say that is a happy person, that is a happy dog they are similar, but that is a very happy person is more similar. See .1 means exactly same, .93 means that kind of uh similar. I can say I just ate Vada Paav. By the way I am not lying, I just ate a ate a big cheese Vada Paav. So now it's doing the sentence similarity. You can see that sentence is not very similar to this. Well in a way it is similar because if you ate Vada Paav, you'll be very happy person. I can say I love eating. I'm just experimenting. So see now when I do this, I just ate Vada Paav has the maximum similarity because it's about eating okay? So this sentence transformers can find out these similarities. There is also this resume screen error that I want to show you. So if you right click here and see this is matching a resume with the job description. So let me open one resume. So I have this particular resume here and you can see okay this is a data scientist resume okay? So the work experience and so on and I have a job description for a data scientist job. So if I open that okay, so this job description looks something like this where they need data scientists with all this particular skills and all this qualification. Now let us try our resume skill screener. So here I'm going to upload that particular resume and here I'm going to upload the job description, and when I say submit it said 47% similarity. See, so there are these are sophisticated tools that can compare a job description with your resume. So now if you are a recruiter, HR person you have a job position open, for one position you got 100 resume, you can just simply use this tool and this tool you can say okay if it is more than 50% match, then only select the resume for phone interview. So it saves you a lot of time and effort. The way this works is you have a job description which is a bunch of text and resume, using sentence encoder you convert text into a vector. So so far you have seen this common method where pretty much for any NLP task first thing that you do is convert the text into vector. I mean you do pre-processing and all of that. But I'm saying first thing that you do before feeding your data to machine learning model is converting into it into a vector. And we will cover sentence encoder and all that in the future. So don't worry again don't worry about all these jargons. Just think there is some cool magical technology which is called sentence encoder, and it can convert your text into bunch of numbers. And the way it converts that into numbers is that if you have similar text, the numbers in the vectors will also be similar. See 45 41 12 10 some numbers are different but if the texts are very similar, the numbers will also be similar. And then you can use something called cosine similarity. If you don't know about cosine similarity, go to youtube code basics cosine similarity. I have a separate video on that and it can say 65% match. So you can have some threshold more than 50% match just schedule phone interview. All right the next one is information at extraction. So we already covered text similar text similarity, text classification as 2 NLP tasks. Third NLP task is in information extraction. I have talked about this use case before where in the email, if you have your flight itinerary it will see I have my flight itinerary and it will, the gmail gmail software will automatically derive or extract the meaningful information which is flight takeoff, landing, from where to where, passenger name, confirmation number. See it automatically extracted all of that and that for that it uses information extraction principle, regular expression and things like that. If you have read Google news on the right hand side you will see all these trending topics. This is the screenshot I took today where all these trending topics are automatically extracted by Google's NLP engine. And you can click on Sarah Palin, Avocado or Israel. Let me just show you. So here I hope you can see the screen so I'm going to Google news here, and in the Google news on the right hand side you see all these big topics. So let's say you want to read about whatever sandy hook elementary shooting right, so you will read all the news related to this particular topic. And you can go back you can read all the news about China. So these are all trending topics basically. So if there are you know nowadays these are few trending topics and when you click on it, it just shows you all the news related to the topic. Now the topic could be person name which is the Naftali, country or some event okay? Let's move back to our presentation now. So this is the image I took from practical NLP book. It's an amazing book by the way, check video description for the book link. Um this is how the whole flow works for information extraction where you do things like sentence segmentation, word tokenization, part of speech tagging again you are cursing me you are you're like, &quot;Why are you throwing all these jargons?&quot; The the purpose here is to get you excited or so that you know that we will be covering all these things in the future. The next one is information retrieval. So there is information extraction and information retrieval. They sound similar, but they are different. So in the information retrieval, the best example is Google search. So when I say Vada Paav places near me, Google would have indexed all these pages or the documents. And by pages I mean all these websites, and it will smartly figure out when I say near me, it knows my location is Vadodara, Gujarat, India and it will show me all those location. So information retrieval is basically you have lot of documents and when I basically in case of Google search, lot of documents means a lot of websites. And when you search the purpose of information retrieval is to return the relevant website in the sorted order. So the first website should be most relevant, second should be the little less relevant and so on. And you can use things like TF-IDF score. But again we'll cover these in the future. But these are some of the techniques used for information retrieval. The next one is Chat Bot. You might have used this already. In your in our day-to-day life we experience chatbot right? You have a banking application you have a problem with your bank account, you go to the chat service and usually the first few responses are done by chatbot. Only if chatbot cannot solve the problem, then only it goes to human. Now there are 3 kind of chatbots: one is FAQ chat bot where you have fixed number of answers and whenever you ask a question, it will give you one of the fixed hard coded answer okay? The second one is Flow-based bot where there is a flow basically there is a connection between your previous dialogue and the next dialogue. So here I'm saying I want to order a pizza and it says what size would you like? I say medium pizza and now now now the chatbot everything in the gray is chatbot returning your returning the response. When you say do you want any sides with it, at this time it knows that you're talking about pizza. So it is already connecting it it remembers the send the context okay? So when you say how would you like to pay, at this time it remembers the context that you're talking about pizza. In the previous FAQ Bot is simple question and answer. There is no connection between your first question and the second question. The third one is Open-Ended bot where you are just chit-chatting with you know your friend on a weekend. You're saying, &quot;How are you? I went to beach it's just a simple general conversation. More like Siri. Okay next use case is machine translation, Google translate is a is a best example. So how does machine Google translate translates language from one sent translate sentence from one language to another? They uses this encoder decoder architecture of RNN or recurrent neural network. We are going to cover this in the future video. If you have seen my deep learning series we talked about this briefly. But that's what it uses underneath. Then the next one is language modeling. So standard use case is you're on gmail you're typing something and gmail will see try to autocomplete. Look at this if it's see the changes in the future, that is something that is suggested by Google. So the sentence autocomplete task is basically language modeling. It tries to predict the probability of next set of words that comes up when you try to complete a sentence. There are statistic models and there are neural models. So there these language models are of two types: statistical and neural model and we'll cover all of these in the future videos. The next NLP task is text summarization. So text summarization is basically let's say you have a news article and you can use NLP text summarization to summarize the whole article into let's say one single sentence. So that is like coming up with a title for a new story okay? So that is basically your text summarization, and it can be useful. If you have a lot of documents, you want to do some analysis you want you don't have time to read all these documents one by one, maybe you can use text summarization and produce two or three line text summary and you just read that instead of reading the whole document. Topic modeling is another NLP task where you have bunch of documents and it's a huge volume of documents and you want to you don't have time to read all of these documents one by one. So you can use topic modeling technique to retrieve the abstract topics out of it. So you are retrieving abstract topics out of collection of documents using topic modeling. So when you have finance documents typically you will extract topics such as revenue financial quarter, YOY YOY growth. If you have let's say some pharma documents pharmaceuticals, you might get these terms frequently: clinical trial, it's called trial whatever spelling mistake. Drug, placebo and so on. Topic modeling also has lot of use cases and we'll talk about these in the future videos. Voice assistant is another one, I don't need to say much about it. So just to summarize, we just covered 10 different types of NLP tasks. We will do coding and we'll talk about all of these topics in detail in the future video. I hope you like this video. If you did please give it a thumbs up and I'm pretty sure you're having fun time learning NLP in this series and I'll see you in the next video.