Today we're looking at additive manufacturing and the software solutions that are making a significant impact in this field. Our focus will be on Pan Optimization, a company helping engineers overcome specific challenges in the additive manufacturing process. Joining us today are Tyler Nelson, principal engineer, and Erik Denlinger, co-founder and chief engineer. Thank you both for joining us today. Thanks for having us here. I appreciate it. Absolutely. So let's just jump right in. So tell me about Pan Optimization. How it got its start and when and what you're up to now. Sure. The company was founded in 2022. So we've been around for almost two and a half years now. So the founder, Pan Michaleris. I met him in 2011 or 2012 when I was a student at Penn State, and at that time he was starting another company. He was a he was a professor at Penn State. And he had, taken a leap to start a company that he called Pan Computing. And at Pan Computing, we had the first process simulation tool for additive. So it was an FEA-based tool, called Cubes, and it gained pretty good adoption in the industry back in that in that time frame. And then eventually in 2016, we sold Pan Computing to Autodesk, and they took our Cubes software and rebranded it as Netfabb Simulation. So I worked with Pan and Autodesk for about five years, and then he left and retired for a period of time. And then the main motivator, I think, for this company was that he, he called me up and said that, he thought retirement was like waiting around to die, even though he's, he's only in his 50s, so he's a little, maybe a little bit melodramatic. But he he wanted to start another company. And we thought there was still a lot of opportunity in there, in the additive simulation space. So, that's how Pan Optimization was, was born. So what we've got is this new FEA solver that we've we've invented here at Pan Optimization that we call PanX, and it's really, the objective with PanX, I say the high level objective, is we want to have the maximum possible impact on the industry and generate as much value as we can and the ways that we see opportunities to do that is by, you know, inventing new modeling approaches that allow us to have much more accurate, much more scalable solutions than what had been possible with these process simulation tools, in the past. Sometime later last year, we commercialized our our new solver, PanX, and we're just in the process of expanding and taking as much of the thermo-mechanical modeling market for additive those as we can. So can you tell me what differentiates the software that you're working on now from previous solutions? That's another question that I can provide a a short answer or a very, a very we could probably do an hour if we don't want to get if we want to get granular on on that, I'd say, at the highest level, the main differences are that we have a completely new modeling approach that gives us much more accurate predictions for things like distortion, stress, strain, temperature than what's achievable in legacy solutions. It's much more scalable to than the legacy solutions. So legacy solutions that were architected, and there's a lot commercially available. They're all architected around the same time frame that like 2011 to 2015 time frame, they were primarily architected. The objective in mind was to be able to do something that you could call part scale modeling. So there was a lot of simulation like of this guy on my wall here, the GE canonical that you've probably seen before. And in reality, that's about an inch, inch tall component. And the goal was, you know, be able to predict some distortion trends on that. But then if you take these legacy tools and you try to either actually the heat exchanger next to me here, if you try to put a heat exchanger in to a model that uses an approach like what you use for that, it's not going to scale. It's not even going to generate an FEA mesh for you. It's just going to crash immediately. So the scalability is really that on top of the accuracy is the other opportunity. And then the third thing that I think was really unaddressed in the legacy tools is really not a lot of options for how to optimize outcomes. So they're generally focused on predicting an outcome. They'll tell you that you're in trouble, but there's no really intelligent way to converge towards a solution to get out of your trouble. So I think the accuracy, the scalability and the and the optimization are kind of the big the big pillars that we're tackling here at Pan Pptimization. Yeah I would just add to that just a little bit. So I was previously a user of some of the legacy tools. I was a really big fan of simulation, you know, before this, this, this code came around and, as a user of the tool and these types of tools, one of the things we just kept running into over and over again was we were trying to really make, make the tools do stuff that they were never really intended to do. To Erik's point about scalability for small stuff, they were great as long as, you know, you didn't care about something really small, like build lines, right? You could still you could model, you know, trinkets and small stuff. But as soon as you start to look at, you know, full build plates of material or big complicated stuff, which is really where, where the market has basically dictated that these types of tools go, because that's where all the money is, right? You're not going to be, you know, building, you know, bottle openers. Nobody's nobody's paying thing big money to do that. So, yeah. Well, let's get a little granular with this and tell me like specifically, how your software addresses the specific challenges, such as with thermal distortion, and things like that. Can you be a little more specific on how it works? Yeah, sure. So it, it's still a FEA-based thermomechanical model. But it, on the physics side, to meet these objectives of the larger problems, we've had to include a lot more physics in this. So, like, the way we would compute temperatures is going to be, that has to be redone the way we compute thermal strain. So it's still thermo mechanical modeling, but everything from the, the included physics, even like the calibration process and the model all has to be all has to be updated. So that's the side of technology that, including that additional physical detail, helps with the accuracy. And then the other side is the scalability and how that works is with, we kind of use this as a high-level catch-all phrase, but we've invented something that we call multi-grid modeling, which is basically a way to, you know, aggressively adapt and reduce these problem sizes on the fly without paying any accuracy penalty. So it's, it's a inherent strain-based model compared on the, on one side combined with multi-grid modeling that really allows us to get to these, these truly horrific scales that we that we can deal with. I mean, this this is a completely new solver. It was basically a rewritten from the ground up and all of these different approaches. First of all, were looked at basically from a parallelization standpoint. So it's extremely parallel. Pan wrote, you know, a lot of the chunks of code many different ways and tested on many different hardwares to make sure it was was as optimal as possible. But then really just rebuilding every single, building block from, you know, from the meshing to the, the strain calculation to, you know, how the transient approach works and all of those different pieces, to get something that scales, and then obviously multi-grid was a huge, you know, huge part of that as well. But it's far from the only piece that makes this, you know, brand new and unique. Let's talk about the hardware that this works on. Yeah this is a question I actually get fairly often, especially with new customers. They're excited about the performance, but they come back with a question like along the lines of, you know, if we wanted to buy something that was highly optimized for this, what do we actually need? And they typically assume the answer is going to be either, you know, thousands of cores on some, you know, huge giant cluster or cloud-based or something like that. But in reality, the way that this software is architected, because it's much more efficient, it's much more memory efficient as well as, just the calculations are very, very fast. You can get extremely good performance on just desktop-class hardware. When we do like extremely large heat exchangers and really big builds and things like that, we typically do those builds on high-end desktop hardware with, you know, a good amount of memory and fast CPUs, but the really nice thing about that is because we had to invent, invent all of this technology to let us scale to those really, really horrifically complicated geometries. That same technology equally applies to the more, you know, more standard geometries that are not quite so, so difficult to simulate, something that maybe could be simulated in a legacy tool. Because those technologies are very, very efficient, we can simulate a lot of the stuff and maybe most of the stuff that a legacy tool could simulate on very, very big hardware on, like, a very good laptop. So it it basically brings the, the requirements down across the board, and opens up, you know, the high end of the market completely. In terms of what it just really wasn't possible to simulate at all before. So let's talk about integration. How does the software integrate with the existing CAD tools that are out there and build preparation workflows that engineers are using for design and manufacturing? So our view of the of the solver is it's it's we kind of think of it as an API, right? It's designed to be plugged into existing workflows. And again, just back to what is the market asking for? They do not want a full blown interface where you have to reset up everything that you just just assigned in your build prep software, any information that needs to get passed to the software, our software is available in whatever your build setup was. So we're, we're establishing partnerships with most major machine OEMs. The most public one so far has been, our partnership with Velo3D, where PanX actually integrates into their flow, build prep software and this kind of ties back into the accuracy and what's new that's included into the model. But the model can accept a lot of information from them, the build setup. So the PanX, we build it, we build out interop. So these build prep softwares where the machine OEM might feed us like, for example, the energy per pixel that goes into their part and then they might actually pass us the processing time that it's going to take to process that layer, since we know where the energy is going and how long it takes, we can start to get extremely accurate thermal predictions in the model that you wouldn't be able to do in a, in a legacy tool. So the idea is to integrate as closely with the build prep software as possible, and then pass as much of that build setup information directly from from that to the to the PanX solver. Going back to the CAD component of that question. So we certainly aren't looking to replace any kind of CAD, CAD programs, but once you have the design and CAD, it's relatively easy to export that and either simulate that directly or as Eric said, pass that through a build prep simulation processor which is going to give you the most accurate, the most accurate results. However, I think it is worth highlighting. So with legacy tools, particularly for very complicated things like heat exchangers, if you wanted to simulate that and have any chance of success in an older tool, the way you would typically do that is you would model the geometries twice, which is very, very painful, especially for big ones. So you basically model at once in full detail, and then you model again cutting out the maybe the core, cutting out those fins or something like that. So you can approximate that using homogenization manually, essentially. So that's one of the big differentiators when it comes to these big giant models that we're able to simulate is we don't need those simplifications to be done. We take the the original CAD with the full detail. And if you want to do some aggressive approximation, the aggressive homogenization, you can do that automatically with a, you know, a single line in the code. But if you want to include all of that detail, you can do that just as easily. And that's a huge, huge advantage not to have to model these things twice and export twice and pull up twice and all that. What about user friendliness? How easy is it? How user friendly is it for your software to be used by engineer, engineers who might not be experts in simulation? And then that would, of course lead to what training or support do you provide? It's very user friendly because so much of the meshing, you know, setting up the FEA problem is done automatically either for the powder bed, it's done automatically based on the geometry, And for the D problem, it's done automatically based on either the geometry or the toolpath. So that takes a lot of the pain out of the setup. So the learning curve is you know, is very, very, very small fraction of learning, like a general-purpose FEA code, like an Ansys or an Abacus. We have a good a good part of the user base are people without a without a FEA background. For those people, we have a very simple UI that's it's basically form based where it's a, you know, what's your geometry? What are your parameters? All phrased in a way that you don't have to have any modeling background to understand what the, what the inputs are. In terms of training, we offer for people that start to use the software, they get a free training session with Tyler, who has 15 years experience doing this type of a setup. And they can pretty much reach, reach. Tyler, somebody here, whenever needed for assistance using using the thing. I would add to that a little bit. So we yeah, I do most of the trainings and I think typically within an hour or so, people are very comfortable to start running stuff. Are they an expert yet? No. But they know enough to be dangerous. Yeah. You can get in. You can get yourself in some trouble after an hour. Yeah. And we encourage, like, you know, semi infrequent or initially it might be fairly frequent check ins where we might just pair together on a problem and they'd say, hey, Tyler, how would you set up this part? These are the things I'm looking to get out of the model, and we can kind of give them a, a more optimal way to, to start from. But the other thing that I would like to highlight, just because it means a lot to me personally, the we really have a different approach, of the customer service and the training at this company than I really have seen anywhere else in that our one of our goals with new with new customers, a new company, is, is to really make sure we have at least one expert at the end of the day that can really take over a lot of the day-to-day questions that people, you know, as adoption grows, you know, through the company. So we spend a lot of time upfront, you know, at our own cost to, to really make sure that at least one person is very, very comfortable, and even approaching expert level on the tool, but, you know, even even beyond that, we are we're very responsive to the questions. All right. Let's move on to validation then. How do you validate to ensure that simulations provided by the software actually correlate with the real-world outcomes? That's a good question. So this is a lot of this is a tedious and expensive process. I think, first of all, I'd say we're fortunate that this time around, I think we are fairly well known in the industry. So we've had very good luck lining up partners, whether machine OEMs or end users that are willing to build stuff for us to validate these things, which has eased a lot of the a lot of the burden. Plus there's just a lot more published data on these on these things. So the basic validation methodologies really depends on, what output from the model you're validating. So the first, first I'll touch on the, on temperatures. So the, temperature validation is done with, in-situ thermal measurements. So we actually have thermal cameras in these machines that are taking pictures of the deposition right before you start to lase the next layer after every layer basically generates like a composite image of all the starting temperatures for the whole build. And then you run the models and you compare it over the whole duration of the build to make sure you're matching that evolution of the interlayer temperatures at different at different locations. So that's a that's actually more on the on the more tedious end of the validation spectrum. The distortion validation is a little bit easier because that's typically done post-process, where you take a white light or a blue light, or a CMM measurement of how a part actually deflected. And you compare it to what the model, the model predicted. So we're constantly, that the validation is very intertwined with the development of the product. Like the the starting point for any model is we want the simplest model possible that's going to meet the objective. We make that, we compare it to reality. If it doesn't match, you think, well, what else needs to be included in the model to get the the match? That's where things like the machine parameters or timing and things like this, OK, well what if we include that? Does that make how much better does that make the prediction? Does achieve the objective? If not, what else can we include in the model? So it's always that, that that question of how well is this agreeing with reality? And is that level of agreement adequate to meet that objective is what drives all of the modeling decisions. Yeah, I would I would say that it's it's an iterative process and it evolves over time. I mean, when you first start out, maybe you have less data and you get more data over time. And we definitely see this as a huge advantage for, you know, working with the different machine OEMs like Velo, for example. And, you know, they're very interested in tight tolerance compensation and so they might you know, come to us periodically and say, this is working really well, or maybe this is a little bit not as good. The temperature is a little bit off, you know, help us understand why. And then they'll, you know, pass that data back. We can, you know, do a deep dive on it internally and try to understand which modeling assumptions are driving those differences and then, you know, ultimately arrive at a better model.