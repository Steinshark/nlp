ANNOUNCER: The following program YASER ABU-MOSTAFA: Welcome back. Last time, we talked about And these are two notions that relate situations. In the case of error measures, we error that is caused by your hypothesis, the cost of using your h, instead of f first place. And that is something the user can use h instead of f. And that is the principled way In the absence of that, which happens B and resort to analytic properties, or in order to choose the error measure. Once you define the error between the the target function on a particular different error quantities, like the error, and get those values in by getting an average. In the case of the training set, you points, and then you average with examples that you have. And in the case of out-of-sample, that you also evaluate the error between x, give the weight of x according to its value with respect to this x. The notion of noisy targets came from learn may not be a deterministic mathematics, where y is uniquely rather, when y is affected by x-- y is distributed according to you y given x. And we talked about, for example, in identical applications may lead Therefore, the credit behavior is a deterministic function of You can go back to our first example, If you rate a movie, you may rate the differently, depending on your So there's always a noise factor And that is captured by the transitional of y given x. When we look at the diagram involving so now we replace the target function, a probability distribution, which function plus noise. And these feed into the generation And when you look at the unknown input technically in order to get the benefit that also feeds into the This determines x. And this determines y given x. And then you generate these examples distribution. So when we had x being the only a deterministic function of x, then independent of x_N. And then you compute each y, according corresponding x. When you have the noisy version, then according to the joint probability original one, times P of y given accommodate the noise. And then the independence lies So x_1, y_1 would be independent of x_2, And when you get the expected values consideration the probability with So what used to be the expected value value with respect to x and y. And then you plug in x into h, and value of y that happened to occur. And that would be now the out-of-sample Now in this lecture, I'm going to start for this particular route three lecture on a related but And the idea is to relate training to a realistic way. So the outline will be the following. We'll spend some time talking about intuitive notion. But we'd like to put the mathematical training versus testing. And then we will introduce quantities characterizing that relationship. And after I give you a number of is clear, we are going to introduce And the break point is the one that will the main notion in the And finally, I end up with a puzzle. It's an interesting puzzle that will about in the lecture. So now let's talk about training And I'm going to take a very simple Let's say that I'm giving So now I want to help you out. So before the final exam, I give you so you can work on and prepare That is very typical. Now if you look at the practice problems your training set, so to speak. You're going to look at the question. You're going to answer. You're going to compare it And then you are going to adjust your material, in order to do it better, and them again, until you get them right or the material. And now you are more ready Now the reason I gave you the practice do better on the final, right? Why don't I just give you the Excellent idea, I can see! Now the problem is obvious. The problem is that doing well goal, in and of itself. The goal is for you to learn the The final exam is only you actually learned. And in order for it to gauge how well you the final at the point you have You prepared. You studied. You discussed with people. You now sit down to take So you have one hypothesis. And you go through the exam. And therefore, your answer on, let's hopefully, it's not going to be will reflect what your understanding So the distinction is conceptual. And now, let's put mathematically It will be an extremely simple an important distinction. Here is what testing is, in terms You have seen this before. This is the plain-vanilla Hoeffding. This part is how well you did This is how well you understand And since you have only one a final, you are fixed, and you just take the exam. Your performance on the exam tracks well And therefore, the difference And the probability that it's not small the number of questions, So that is what testing is. How about training? Almost identical, except this fellow. Because in the case of training, the practice problems. In the practice problems, you had hypothesis. And you looked at it, and you So you modified your hypothesis again. You are learning better. That's all very nice. But now the practice set You pretty much almost And there's a price to pay for that, in practice, which is E_in in this case, material, which is still E_out. And the price you pay is And that was reflected by of hypotheses in the very simple So if you want an executive summary of to get M to be replaced by something M-- if you just measure the the number of hypotheses-- this is next Something as simple as the perceptron And therefore, this guarantee If we can replace M with another that quantity is not infinite even if we are in business. And we can start talking about the model, and be able to establish to a real situation. That's the plan. We're talking about M, so the did this M come from? If we are going to replace it, we need understand the context Well, there are bad events that And the bad events are called That's good! And then-- these are the bad events. What is the bad event that We were trying to avoid the situation not track the out-of-sample If their difference is bigger than And we're trying to say that situation is small. That was the starting point. Now we applied the union bound, of several bad events. This is the bad event for You can see here that there m is 1, 2, 3, 4, up to M. M hypotheses that I'm talking about. And I would like the probability of Why is that? Because your learning algorithm is free wants, based on the examples. So if you tell me that the probability then whichever hypothesis your algorithm And I want that guarantee to be there. So let's try to understand the What does it look like? Well, if you look at a Venn diagram, areas here, these areas-- these are different events. They could be disjoint, in which case Or they could be coincident, which They could be independent, which means overlapping. There could be many situations. Now the point of the bound is that we regardless of the correlations And therefore, we use the union bound, area of the first one, plus the total if they were disjoint. Well, that will always hold regardless But you can see that this is a poor estimating it to be about three times just the area, because the overlap And therefore, we would like to be able overlaps, because with no overlaps, And you're stuck with M, and hypothesis sets. Now of course, you can go-- in principle, you can go and I give you the hypothesis set, And you can try to formalize, what perceptron. And what happens when you go to the full joint distribution of all of these Well, you can, in principle-- It's a complete nightmare, And if we have to do this for every wouldn't be learning theory around. People will just give up. So what we are going to do, we are hypothesis set a quantity that is overlaps, and get us a good bound, intricate details of analyzing how the That would be the goal. And we will achieve it, through So that's where M comes from. When we asked, can we improve on M? Maybe M is the best we can do. It's not like we so it has to be improved. Maybe that's the best we can say. If you have an infinite hypothesis, then But it turns out that, no, the overlap very common. Yes, we can improve on M. And the reason extremely overlapping in Let's take the example we know, which what this is. I'm going through the example because +1 versus -1 for the target, hypothesis, agreeing versus I want to pin down exactly what is so that we understand what Here is the target function And it returns +1 for some That's easy. And then you have a hypothesis, And this is not the final hypothesis. This is a badly performing hypothesis. But it is a general perceptron. If you find any vector of weights, So now in terms of this picture, could What is the out-of-sample error for this this target function? It's not that difficult. It is actually just these areas, This is where they disagree. One is saying +1. One is saying -1. So these two areas-- if you get the total probability if it's not-- then this will give That's one quantity. How about E_in? For E_in, you need a sample. So first, you generate a sample. Here's a constellation of points. Some of these points, as you region, here and here. And I color them red. So the fraction of red compared to That is understood. This is E_in and E_out. And these are the guys that I OK, fine. I understand this part. And in words. Now you'll look at: what is the change hypothesis? So here's your first hypothesis. Now take another perceptron. You probably already suspect that Whatever you're talking about, it must close to each other. But let's pin down the specific event overlapping. So the change in E_out when you go from, this blue hypothesis, to the change in E_out would be the area A very thin area. That's where E_out changed, right? So if you look at the area, that gives you the change of the labels if one of the data points happens its error status will change from one hypothesis got it right, and Now the chances of a point So you can see why we are arguing that delta E_in is small. The area is small, and the probability Moreover, they are actually moving in is actually depending on the So this-- let's say that this is increasing. If they increase, they increase both, the delta E_out. And the probability of falling Now, the reason I'm saying that, is We would like to make the statement first hypothesis, for the blue tracks E_out for the second one. Why are we interested in that? Because we would like to argue that this when this exceeds epsilon. The events are overlapping. We are not looking for the absolute that, if this exceeds epsilon, epsilon most of the time. And therefore, the picture we had These guys are overlapping. The bad events are overlapping. And at least we stand a hope that we counting the number of hypotheses, for So we can improve M. That's good news. We can improve M. We're going What are we going to replace it with? I'm going to introduce to you now the M. It is not going to be completely with this quantity. That will require a proof. And that will take us The purpose here is to define the well, because this is the quantity that complexity of any model you use. So we want to understand it well. And we are going to motivate that it can It makes sense. It's not a crazy quantity. It also counts the number And therefore, let's define the quantity And then next time, we will like the bullet, and go through the proof that we this quantity. So what is the quantity? The quantity is based When we count the number of hypotheses, consideration the entire input space. What does that mean? These are four different perceptrons. So I take the input space. And the reason these guys are on at least one point That's what makes two And because the input space is infinite, get an infinite number of hypotheses. So let's say that, instead of counting input space, I'm going to restrict So I generate only the input points, the diagram. So I have this constellation And when I look at these points alone, those perceptrons will classify them. These guys will turn into red and blue, Now, in order to fully understand what of points, we have to wipe So that's what I'm going to do. That's what you have. So you can imagine the perceptron And it's splitting the points. And now what I'm counting is-- for this constellation, which is many patterns of red Now when you do this, you're not because the hypotheses are You are counting them But still, you're counting. You're counting the number For example, if I give you a hypothesis possible combinations of red and blue, If I give you a hypothesis where you get a hypothesis. So the count here also corresponds in of the hypothesis set, which in our mind So we are going to I'm putting them between quotations. Why? Because now the hypotheses are defined So I'm going to give them a different a subset of the points, in order not to input space, with this case. I'm going to call them dichotomies. And the idea is that And there is a dichotomy between what That's where the name came from. So when you look only at the points, and blue and which ones are And if you want to understand Let's say that you're looking And this is your perceptron. And this is the function And then you put a constellation The way to understand dichotomies is to of paper, that has holes in it. And you put that opaque sheet of paper So you don't see the input space. You only see it through the So what do you see when you put this? You end up with this here. You don't see anything. You don't see where the hypothesis is. You just see that these guys turned red or pink. Now as you vary the perceptron, as you to notice it here, until the line So I could be running around here, here, an infinite number of hypotheses, for And this guy is sitting here, looking. Nothing happened. It's the same thing. I'm counting it as 1. And then when you cross, you end So all of a sudden, these And these guys are red. That's when, let's say, this guy than vertical here. So you can always think that we reduced going to look at the problem exactly that has only N holes. Let's put, in mathematical terms, the hypotheses, the hypotheses restricted A hypothesis formally is a function. And the function takes the full -1, +1. That's the blue and red A dichotomy, on the other hand, We can even give it the same name, for the points it's allowed But the domain of it is not specifically, x_1 up to x_N. These are-- each one of these points belongs to But now I'm restricting And again, the result is -1, That's what a dichotomy is. Now if I ask you how many hypotheses perceptron case? Very easy. It can be infinite. In the case of the perceptron, Why? Because this guy is seriously So the number of functions is So that's fine. Now if you ask yourself, what is Let's look at the notation first, The dichotomy is a function So when I talk about it, the value, I value at a time. If I decide to use the fancy notation, the entire vector, x_1, x_2, up to x_N. I would be meaning that you tell me the So you return a vector of x_2, up to h of x_N. That's not an unusual notation. Now if you apply the entire set of are doing is that you are applying the entire vector. Each time you apply one of those +1, -1, +1, -1, et cetera. So you get a full dichotomy. And then you apply another h, and you However, as you vary h, which has guys will return exactly the same are very restricted. I have these N points only. And I'm returning +1 or -1 So how many different ones At most, 2 to the N. If H is you all 2 to the N. If not, it will get can start with the most infinite And if I translate it into dichotomies, 2 to the N for the number So this thing now becomes a candidate hypotheses. Instead of the number of hypotheses, dichotomies. Now we define the actual quantity. Capital M is red. And I keep it red throughout. And we are going now to define small That will hopefully, and provably replace M. It's called the growth function. What is the idea of The growth function counts can get, using your hypothesis So here is the game. I give you a budget N. You choose where to place Your choice is based on your attempt possible, on the N points, using So it would silly, for example, to take say, on a line, because now you are But you can see the most I can constellation. And then you count the number of And what you're going to report to me is the N that I passed on to you. So I give you N, you go through this is the growth function. Let's put it formally. The growth function is going to be And it is the maximum. Maximum with respect to what? With respect to any choice of That is your part. I gave you the N. So I And then you chose x_1 up to x_N with What are you maximizing? Well, we had this funny notation. H applied to this entire dichotomies, the vectors, -1, +1, next guy and the next guy-- When you put this cardinality on top You're asking yourself: how So you're maximizing, with respect to the That will give you the most expressive points, that number. I tell you 10. And you come back with the number 500. It means that by your choice of the x_1 up different guys, according to the Now because of this, you can see now It used to be m, but it actually It's the growth function for So I'm making that dependency explicit, Furthermore, this is M was a number. I give you a hypothesis set. It's an number. Well, it happens to be infinite, Here, I'm giving you That is, I tell you N, you tell me So it's a little bit more complicated. And because it is this way, m_H is growth function. So that is the notion. Now what can we say about Well, if the number of dichotomies is many +1, -1, N-tuples you can also bounded by the same thing, at most to replace M with m, I would say over infinity. If we can afford to do it. Maybe it's not a great improvement, Now, let's apply the definition to give it flesh, so we understand It's not just an abstract quantity. We take the perceptrons, and we would of the perceptrons. Well, getting the growth function of If I tell you what is M And then you go home. What is the growth function You have to tell me what is the growth equals 2, at N equals It's a whole function. So we say, 1 and 2 is easy. Let's start with N equals 3. So I'm choosing 3 points. And I chose them wisely, so that I can And now I'm asking myself, what is the perceptron for the value Well, it's not that difficult. You can see, I can actually get Why? Because I can have my line here, or I my line here. That's 3 possibilities times 2 because or -1 versus two +1's. We are counting 6 so far. And then I can have my hypothesis That will make them all +1. Or I can have it sitting here, which That's 8. That's all of them. The perceptron hypothesis is as strong your attention to 3 points. So the answer would be what? Is it already 8? Wait a minute. Someone else chose the points co-linear, you want these guys to go to the -1 class, there is no perceptron Correct? You cannot pass a line that will make guy go to -1, if these are co-linear. Does this bother us? No. Because we are taking the maximum. So this, the quantity you computed cannot go above 8. That defines it. And indeed, you can with authority function for this case, Now let's see if we are still in What is the growth function We'll choose the point in We are not going to have any maximize our chances. But then we are stuck with Even if you choose the points in constellation-- there is this particular pattern on the -1, and +1, +1. Can you generate this No. And the opposite of it, If this was -1, -1, and Can you find any other 4 points, where No. I can play around, and there is always If I choose the points unwisely, So the maximum you are getting is that possibilities. And the growth function here is 14, not had the maximum. Now this is a very satisfactory pretty limited models. We use them because they are that goes with them. So we have to expect that the quantity the perceptrons with, which is the maximum possible. Because if it's the maximum possible, as strong as can be. Now they break. And they are limited. And if I pick another model, which, case-- the set of all hypotheses. What would be the growth function It would be 2 to the N, because So now, according to this measure that hypotheses is stronger Satisfactory result, simple Now what I'm going to do-- I'm going to take some examples, in function completely for all values of with this and say, let's 5 points. You put the 5 points, Am I missing this? Or maybe if I change the position It's just a nightmare, just to get 5. And basically, if you just do it by So I'm taking examples where we can argument, get the value of the growth N from 1 up to infinity, in growth function. That's the purpose of this portion. Our first model, I'm going Let's look at what positive They are defined on the real line. So the input space is And they are very simple. From a point on, which we are going to decides one hypothesis versus particular hypothesis set. All the points that are All the points that are And it's called positive ray, because hypothesis set. Now in order to define the growth So I'm going to generate some points. I'm going to call them x_1 up to x_N. And I am going to choose them I guess there is very little generality Just make sure that they don't If they fall on each other, you cannot If you put them separately, So you have these N points. Now when you apply your hypothesis, drawn on the slide, to these points, True? And you're asking yourself, how many N points by varying my hypothesis, the value of 'a'? That is the parameter that gives me Formally, the hypothesis set is a set And I can actually find If you want an analytic formula, This is, I think-- If you apply it, that's exactly Now we ask ourselves, what Here is a very simple argument. If you have N points, the value of the and which ones go to red-- depends on 'a' will fall in. If 'a' falls here, you get this pattern. If 'a' falls here, this guy will be red. And the rest of the guys will be blue. So I get a different dichotomy. I get different dichotomies when How many line segments are I have N points. I have N minus 1 sandwiched ones, and and one here when all of them are blue. Right? So I have N plus 1 choices. And that's exactly the number of points, regardless of what N is. So I found that the growth function, N plus 1. Let's take a more sophisticated model, function. Because that's The next guy is positive intervals. What are these? They're like the other guys, except Instead of having a ray, Again, you're talking And you are going to define And anything that lies within and will become blue. And anything outside, whether it's right That's obviously more powerful than the of the positive ray as having That's fine. So you put the points. We have done this before. And they get classified this way. And I'm asking myself, how many choosing really 2 parameters, the of the interval. These are my 2 parameters, that will tell How many different patterns can I get? Again, the function is very simple. It's defined on the real numbers. an interesting one. The way you get a different dichotomy segments, to put the ends If I start the interval here and If I start the interval here and end If I start the interval here and And that is exactly one-to-one mapping of 2 segments. So if this is the case, then I can function, in this case, is the number N plus 1 segments. And that would be N plus 1 choose 2. There is only 1 missing. When you count, there are 2 rules-- make sure that you count everything, anything twice. Very simple. So we counted almost everything. But the missing guy here is what? Let's say that all of them are blue. Is this counted already? Yes, because I can choose this And that is already counted in this. But if they're all red, It means that the beginning of the happen to be within the same segment. So they didn't capture any point. And that, I didn't count. And it doesn't matter which segment the all reds. So it's one dichotomy. So all I need to do is just add 1. And that's the number. Do a little algebra, and you get this. That is the growth function And now I'm happy, because It's more powerful than the previous Now let's up the ante, and Convex sets. This time, I'm taking the So it's R squared. And my hypotheses are simply If you look at the values of x at has to be a convex region, A convex region is a region where, region, the entirety of the line segment the region. That's the definition. So this is my artwork You take any 2 points and-- So this is an example of that. The blue is the +1. And the red is the -1. That's the entire space. So this is a valid hypothesis. Now you can see that there is qualify as hypotheses. But there are some which For example, this one is not convex, Here's the line segment, and So that's not convex. We understand what the Now we come to the task. What is the growth function In other to answer this, what you I give you N, and you place them. So here is a cloud of points. I give you N, and you say, it seems position is a good idea. So let's put them in And let's try to see how many patterns using convex regions. Man, this is going to be tough Let's see. First, I cannot get all Because let's say I take the outermost This will force all the internal points a convex region. Therefore, I cannot get +1's for whatsoever inside. So that excludes a lot of dichotomies. Now I have to do real But wait a minute. The criterion for choosing the cloud of and general, but to maximize Is there another choice for the hypotheses than these? As a matter of fact, is there another will give me all possible dichotomies If you succeed in that, then you The other one will count, because Here is the way to do it. Take a circle, and put your points Now I maintain that you can get any What is the argument? Well, pick your favorite one. I have a bunch of blues Can I realize this using Yes. I just connect these guys. And the interior of this And whatever is outside And I am assured it's convex, because perimeter of a circle. That means what? That means that the growth notwithstanding the other guy. You realize now a weakness in maximum, because in a real learning you're going to get are not going to They are likely to be And some of them will be interior going to get all possibilities. But we don't want to keep studying the and the particular data We would like to have And therefore, we're taking the maximum a simple combinatorial property. The price we pay is that, the chances are not going to be as tight as possible. But that's a normal price. If you want a general result that going to be all that tight That is the normal tradeoff. But here, the growth function is Just as a term, when you get all dichotomies, you say that the hypothesis broke them in every possible way. So we can say, can we shatter That's what it means. You get all possible combinations Now let's look at the 3 growth functions able to compare. We started with the positive rays, and And then we went on to the And we had a quadratic function. And that is good, because we are getting growth function is getting bigger. And then we went to convex It's powerful and two-dimensional Convex sets are still-- It's really, although we got a bigger Maybe we should have gotten N cubed. But that's what we have. At least it goes this way. So sometimes that thing But in general, you can see the trend a bigger growth function. Now let's go back to the big picture, function will fit. Remember this inequality? Oh, yes. We have seen it. We have seen it often. We are tired of it! What we are trying to do is replace M. it with the growth function m. M can be infinity. m is a finite number, at most What happens if we replace Let's say that we can do that, which What will happen? If your growth function happens to Why is that? Because if you look at this quantity, epsilon can be very, very small. epsilon squared can be really, But this remains a negative exponential of epsilon you wish, this will kill the here, eventually. Right? I can put a 1000th-order polynomial, to the minus 6. And if you're patient enough, or if your would be an enormous amount of data, you And you will get the probability to be you can generalize. That's a very attractive observation, to do is just declare that this is We saw that it's not that easy But maybe, there is a trick that will polynomial. And once you declare that a hypothesis we can declare that learning is feasible We may become finicky and ask ourselves, need for what, et cetera? But at least, we know we can do it. If you're given enough examples, you a finite set, albeit big, to the general So that's pretty good. I'm happy that this is the case. So maybe we can, as I mentioned, just growth function is polynomial. Can we do that? Maybe we can. Maybe we cannot. Here's the key notion that We are going to define what You give me a hypothesis set, and Perceptrons, 4. Another set, the break point is 7. Just one number. That's much better than giving every N. Just one number. So what is the break point? The definition is the following. It's the point at which you fail So you can see that, if the break point hypothesis set. I can't even generate all 8 If the break point is 100, well, that's generate everything up to 99 points, And then I start failing at 100. So you can see that the break point complexity of the hypothesis set. If no data set of size k can that is, if there are no choice of generate all possible dichotomies. Then you call k a break point for H. So let's look at-- what is the-- So that's what it means. You can't shatter, so less than possibilities for k data points. So for the 2D perceptron, can you We did it already. We didn't explicitly say But this is the hint. For 3, we did everything. For 4, we knew we cannot So it doesn't matter whether As long as it breaks, it breaks. It's not 16. And therefore, in this case, That number 4 will characterize the have a hypothesis set. And it is defined-- I don't want to know the input space. Wait a minute. OK, I'm not going to tell I'm going to tell you the hypotheses. The hypotheses are produced by the-- I don't want to hear it. Just tell me the break point, and I will Also, if you have a break point, every bigger point is also That is, if you cannot get all certainly cannot get If you could get them on 11, And you will have gotten them on 10. Let's look at the 3 examples, and Positive rays had this guy. This is a formula. We can plug in for N. And we point where I no longer get 2 to the N, What is the break point here? N equals 1. I get 1 plus 1. That's 2. That also happens to be 2 to the 1. 2: N plus 1 is 3. Oh, that's less than 4. So 2 must be a break point. This is since we invested in computing the just substituting. But you could go for the original thing, Because this particular combination if I want the rightmost point to be there is no way for the positive And therefore, that 2 is a break point. There's something where I fail. Let's go for this one. We need faster calculators now. 1, 1/2, et cetera. Wow. It's exactly. When I put 1, it gives me 2. It must be the correct formula. Let's write 2. At 4, I get 2. And-- it calculates. What is the break point? It must be bigger than the other guy, And you realize it's 3. If you put 2 points, And if you put 3, you'll get Again, that's not a mystery. That's what you cannot get You cannot get the middle point to be So you cannot get all possibilities Therefore, 3 is a break point. What is the break point Tell me how many points where I can fail. Well, I'm never going to fail. So if you like, you can Let's define it this way. So also, the break point-- has the property we want. It gets more sophisticated as the So what is the main result? The main result is that the if you don't have a break point, The growth function is That's the definition. Thank you. So that cannot possibly So what is the main result? The main result is that if you break point, 1, 5, 7000. Just tell me that there You don't even have to tell We are going to make a statement The growth function is-- do I hear a drum roll? [MAKES DRUM SOUND] It's guaranteed to be polynomial in N. Wow, we have come a long way. I used to ask you what and count them. That was hopeless because We defined the growth function, That was painful. Then we found the break point. Maybe it's easier to compute I just want to find a clever way, Now all I need to hear from you And I'm in business as far as the know that regardless of what polynomial to learn eventually. I will become more particular, and ask you find the budget of examples you need performance. But in principle, if I just want to say and you can learn, I just want you That's all I want. This is a remarkable result. And I have to give you a puzzle The idea of the puzzle If I just tell you that there's number of dichotomies you get, because there is a break point, is enormous. If I tell you a break point is, can you get on 100 points? On those 100 points, for any choice of possible combinations-- at any 3 points, So the combinatorial restriction And you will end up losing possible restriction. And therefore, the thing that used to will collapse to polynomial. Let's take a puzzle, and try to Here is the puzzle. We have only 3 points. And for this hypothesis set, I'm telling So you cannot get all possible four If you put x_1 and x_2, you cannot get +1 -1, and +1 +1. All of them. You cannot get it. One of them has to be missing. So I'm asking you, given that this is you get on 3 points? You can see, this is what I'm trying to restriction on 2 will-- If I didn't have the restriction, So I'm just telling you this case. So how many do I get? For visual clarity, I'm going to circles, just for you to be able to-- instead of writing -1 or +1. This dichotomy is fine. It doesn't violate anything. I've only one possibility. So we keep adding. Everything is fine. As a matter of fact, everything will the whole idea is that I cannot So if I have less than four, I cannot You see what the point is. This is still allowed. I'm going through it as a binary one. So this is 0 0 0, 0 0 1, et cetera. I'm still OK, right? Am I still OK? [MAKES BUZZER SOUND] You have violated the constraint. You cannot put the last row, because I have to take it out. So let's take it out. Try the next guy. Maybe we are in luck. Are we OK? OK. That's promising. So let's go for the next guy. Maybe we'll get it. Are we OK? [MAKES BUZZER SOUND] Tough. So we have to take out the last row. How about this one? Nope. We take it out. We don't have too many Actually, this is the last guy. It had better work. Does it work? No. So that's what we can do. We lost half of them. Now you may think, maybe you started very regularly. Just started from all 0, 0 0 1. But if I started differently, It's conceivable. Please don't lose sleep over it. The only row you are going to be able This is indeed the solution. And you can verify it at home. Now we know that indeed the And we are going, in the next lecture, a polynomial growth, which is Let me stop here. And we will take the questions Let's start with the questions. MODERATOR: The first question is, hypotheses are not binary? PROFESSOR: There is a counterpart for the entire theory that real-valued functions and other The development of the theory is develop it only for the binary case, And it carries all of the The other case is more technical. And I don't find the value of going to terms of adding insight. What I'm going to do is, I'm going real-valued functions, which is And it's a completely different approach us another angle on generalization real-valued functions. But the short answer is that, if the a counterpart to what I'm But it is significantly more technical MODERATOR: Just as a sanity check. When the hypothesis set can a bad thing, right? PROFESSOR: OK. There is a tradeoff that will stay It's bad and good. If you shatter the points, it's good that if you give me the data, regardless going to be able to fit them because, I a hypothesis for any particular So if your question is, Then shattering is good. When you go to generalization, you can get anything. So it doesn't mean anything And therefore, you have less hope formalized through the And the correct answer is, what is the And then we'll find a value for which points, but we are not very restricted, approximation, and we're getting some generalization. And that will come up. MODERATOR: Is there a similar sets in higher dimensions? PROFESSOR: So if you-- The principle I explained, two-dimensional and perceptrons. If you look at the essence It could be anything. The only restrictions I have So this could be a high-dimensional space. And the surfaces will be very And all I'm reading off, as far as this patterns do I get on a number MODERATOR: Also a question Why is usually polynomial time PROFESSOR: OK. Polynomial, in this case, is polynomial It just so happens that we are working gives us a very helpful term, which And therefore, if you get a polynomial, polynomial, you are guaranteed that for the right-hand side of the Hoeffding, function, will be small. And therefore, the probability of Now obviously, there are other functions the negative exponential. For example, if I had a growth function the square root of N, that's But that will also be killed by the square root versus the other one. It just so happens that we are in the growth function is either identically There is nothing in between. If you draw something that is and try to find the hypothesis set function, you will fail. So I'm getting it for free. I'm just taking the simplicity me, the polynomials are the And they happen to serve the purpose. MODERATOR: OK. A few people are asking, could you Because they didn't get the-- PROFESSOR: OK. Let's look at the puzzle. I am putting 3 bits on every row. I'm trying to get as many different rows that if you focus on any 2 of them-- so the columns, it must be that one x_1 and x_2 is missing. Because I'm saying that 2 is shatter any 2 points. Therefore, I cannot shatter x_1 and x_2, get all possible patterns. There are only four possible patterns, 0 0, 0 1, 1 0, 1 1. And I'm representing them using In this case, the x_1 and x_2 keep adding a pattern-- So let's look at here. x_1 and x_2, how many patterns do they have? They have this pattern. They have it again. That doesn't count. So there's only one pattern So on x_1 and x_2, I have So I haven't violated anything, because four patterns. So I'm OK, and similarly Things become interesting when you Now again, if you look at the first 2 pattern here. There are only two patterns. Nothing is violated as far these But the constraint has to be satisfied So if you particularly choose x_2 and x_3, you realize, 0 0, I am in trouble. That's why we put it in red. Because now these guys have And I know, by the assumption of the patterns on any 2 points. So I cannot get this. So I'm unable to add this row And therefore, I'm taking it away. And I'm going through the exercise. And every time I put a row, I keep So here, I put-- let's look at x_1 and x_2. 1 pattern, 2, 3. I'm OK. x_2 and x_3, 1 pattern, which 2, 3. I'm also OK. And then you put x_1, x_3. Here is a pattern. It repeats here. 0 0 and 0 0. So that's one. And then I get this one So this one is perfect, Not perfect in any sense, except So I'm allowed to put that row. Now when I extend this further, and start guy, there is a violation. And you can scan your eyes and And I'm highlighting it in red. So I'm showing you that for x_1 and Here's one pattern, the second one. I didn't count this one, just because So I just highlight four different ones, So I cannot possibly add this row, these 2 points. So I take it out and keep adding. Another attempt, this is the next guy. It still violates. Why does it violate? For the same argument. Look at the red guys. You find all possible patterns. So I cannot have it. So we take it away. And then the last one that And that also doesn't work, because You can look at it and verify. And the conclusion here is that So that's what I'm stuck with. And therefore, the number of different that 2 is a break point-- in this case, is 4. Obviously, the remark I mentioned is gradually from 0 0 0, 0 0 1, more cleverly or something. But however, anyway you try it, it's that it doesn't make a difference. You will be stuck with at most 4. MODERATOR: OK. In the slide with the Hoeffding you change-- specifically, does a probability measure a hypothesis to dichotomy? PROFESSOR: For this one? MODERATOR: Yeah. PROFESSOR: Yeah. The idea here, M is the number of hypotheses, period. So it's infinity for perceptrons. We have to live with that. In our attempt to replace it with the replace it by something that is not As you can see, 2 to the N is not a positive exponential and And that's not very decisive. Therefore I am trying to find if I can the growth function here, but also polynomial for the models that I have, and therefore be able to get a real learning model, like the perceptron networks, et cetera. All of these will have a polynomial So that's where the number of number of dichotomies, which Not a direct substitution, There are some technicalities But that is what gets me the right-hand hand side, and goes to 0 as N grows, generalization will be high. MODERATOR: OK. Is there a systematic way PROFESSOR: There is. It's not one size fits all. The are arguments, for example, you And sometimes you find it by finding break, and argue that this Sometimes you can argue Let me try to find a crude estimate Let's say the growth function And then as you go by, you realize So there has to be a break point This would be less than 2 to the N, and So in that case, the estimate for the It will not be an exact value. But it will a maximum. We have a question in house. STUDENT: Hi. So in this slide, the top end is the end is the number of training points. PROFESSOR: Yeah. N is always the size of the sample. And it's a question of interpretation used for testing, which means that you and you are just verifying, Or in the other case, you haven't And you are using the same sample And you are charged for the going STUDENT: So let's say that our How do we decide how many of them do many for training? PROFESSOR: This is There will be a lecture down the is going to be addressed There are rules of thumb. There are some mathematical results, but there is a rule of thumb. There are few rules of thumb that I'm stood the test of time. And one of the rules of thumb has to order to first not to diminish the a big enough test set so that So this will come up. Thank you. There is another question. STUDENT: Hi, professor. I have one question. So for 2 hypotheses that have the same in-sample error is the same PROFESSOR: OK. If it has the same dichotomy, it's even because it returns exactly Now the in-sample error is got right and wrong. The target function is fixed. So that is not going to change. So obviously, I'm going to get And if I get the same pattern of errors, same fraction of errors, Now if you're asking, for these out-of-sample error? That's a different story, because for the hypothesis in its entirety. So in spite of the fact that it's the not the same on the entire input space, different hypotheses. And therefore, you get But the answer is yes. You will get the same in-sample error. STUDENT: Oh, yes. I see. That's why I was asking. Because I think that the out-of-sample So can we replace the M with-- PROFESSOR: Exactly. And the biggest technicality We were saying, we're going to That's a very helpful thing. Now, there has to be a proof. And I will argue for the proof, and The key point is, what do Because when I consider the sample, this As you said, if I have 2 hypotheses same here. But they are not the same here. So the statement here depends on So how am I going to That's really the main technical And that will come up next time. STUDENT: Sure, thank you. PROFESSOR: Sure. MODERATOR: So-- why is it called a growth function? PROFESSOR: A growth function. I really-- The person who introduced this I guess he called it a growth function, increase N. I don't think there is any MODERATOR: Is there-- what is a real-life situation similar you realize that this break point PROFESSOR: OK. The first order of business is to that there is a break point, we are in business. Second one is, how does the value the learning situation? Do I need more examples when The answer is yes. What is the estimate? And there's a theoretical Maybe the bound is too loose. So we'll have to find practical rules point to a number of examples. All of this is coming up. So the existence of the break point The value of the break point tells us a certain performance. And that will be addressed. MODERATOR: Is there a probabilistic that is an alternative to the growth rate in N? PROFESSOR: There are So there are alternatives and you can get different results I am sticking to Hoeffding. And I'm not indulging too much into its because this is a mathematical And I'm taking it for granted. And I picked the one that will help So yes, there are variations. But I am deliberately not getting to dilute the message. I want people to become so incredibly they know it cold. Because when we get to modify it, other technical points, I'd like the people's mind, so that they don't get So that's why I'm sticking to this. MODERATOR: I think that's it. PROFESSOR: Very good. We'll see you next time.