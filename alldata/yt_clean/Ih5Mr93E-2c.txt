ANNOUNCER: The following program is YASER ABU-MOSTAFA: Welcome back. Last time, we introduced the third regression. It has the same structure as the linear inputs combined linearly using weights, signal passes through something. In this case, it passes through what We labeled it theta. And the model is meant to implement probability interpretation. And because of that, the error measure measure, which has a probabilistic in which case we maximized the set that we got-- the outputs based on the hypothesis that is regression being assumed to be the And this makes us able to express the that define the hypothesis, which And therefore, we have this quantity And then we derived an error measure measures that we had before, in terms of regression that we will minimize. So this is a useful model, and it One of them was for classification, one regression. And this one is for bounded real-valued a probability. One of the key issues about logistic the error measure is a little bit example in linear regression, we were And therefore, we introduced the method an arbitrary nonlinear function that differentiable. And in the case of logistic a closed-form solution, the has a very nice behavior. It's a convex function and, therefore, descent or other methods, it is fairly into that minimum and stay there, rather minima that we talked about briefly. So the algorithm for gradient descent, you are trying to minimize. First you initialize and, in the case to all zeros was fine. We will find out that today in neural we'll make the point why. And then you keep iterating And what you do is, you update your negative of the gradient. That would be the biggest gain you would get And in this case, we adjusted the learning rate that is proportional We keep doing this, and then when we as our final hypothesis. And we talked a little bit in the Q&amp;A and also about local minima that So today when I modify the gradient version, which is called stochastic a little bit about initialization and have to do with local OK. Today's topic is neural networks. And historically, neural networks are interest in machine learning. They have a biological link that people pursued them. And they were very easy to implement going to describe today. And they met a lot of success and got people going. Now, it is not necessarily the model will opt for support vector machines then, neural networks would do the And many industries use For example, in banking and networks are often used. So the outline for today First, I'm going to extend gradient stochastic gradient descent that And then, I'm going to talk about What is the hypothesis that And I motivate it from a biological perceptrons. And then we'll talk about the efficient algorithm that goes with that model particularly practical. So let's start with stochastic What do we have? We have gradient descent, and gradient That is function of w-- minimizes And that happens to be an in-sample And it is the in-sample error. And the only thing I would notice here stochastic gradient descent is that, in order gradient of the error, which implement gradient descent, you need to evaluate the hypothesis So for n equals 1 to N, you evaluate their gradient. And that will tell you what is the you would go to, which is normal because minimizing. You'd better compute it. So you take the case of logistic particular form for that. And now you can see that In this case, friendly and smooth. And indeed, you can get the gradient down the error surface, suggested by gradient descent. Now, the steps were iterative, and so And one step is a full epoch, in the have considered all the examples choice we have so far. And we had this formula The difference we are going to do now the movement in the w space we are going to try to do it based That's what will make stochastic So because now we are going to have the standard gradient descent as It takes a batch of all the examples and to the other mode. So the stochastic aspect You pick one example at a time. Think of it that you pick it at random. You have N examples, to be picked. You pick one of them at random. Now you apply gradient descent, not to the but the in-sample error on that point. That looks like a very meager thing to involved at all. But I think you have seen something When we take one example at a time and about what other guys are doing even Remember the perceptron That's exactly what it And in this case, it will also work. Now to argue that it will work, think going to descend along. What does that mean? If you take the gradient of are going to minimize, which in this case just and you take the expected value under example from the entire training In that case, if you want to get the red n, which is now a random variable, And if you evaluate it, You simply take this value. For every example it has a probability of be 1 over N summation of that. So this would be the So you think that every step, I'm going So this is the expected value, another, there is some And if you look at the quantity on the identically minus the gradient So it's as if, at least in expected direction we want, except that we computation, which is a big advantage, aspect to the game. So this is the idea, and then And as you repeat, you always get the and you get different noises So the hope now is that by the time you will average out and you actually will So it's a randomized version of gradient stochastic gradient descent, Now let's look at the benefits of The main benefit by far-- that is the motivation is that it's a cheaper computation. Think of one step that you're going to What do you need? You take one example, you put the input you compute whatever the gradient If you're doing the batch gradient examples before you can Nevertheless, the expected value of your the same as the other one. So it looks there's a little On the other hand, it If this actually works on average, this So this is number 1 advantage. The second advantage is randomization. There is an aspect of optimization So you don't want to be You want to have an element of chance. Why would I want an element of chance Well, because optimization It's not like you're going for the There are all kinds of traps that you minima and whatnot. So let's look at cases where This is an error surface, error surface you will encounter. The one you encountered in logistic this, that was a lucky In general, and in neural networks for hills and valleys in So depending on where you start, local minimum or another. You may not get the best one, you Now, this is inevitable and there is we discussed in the Q&amp;A session. On the other hand, it will be quite You see this small fellow? Because it's really just But according to gradient descent, you everybody is happy, and you stop there. So you would love to have an added least shallow valleys like that. And the idea now is that, because you deterministic-- in this case, there fluctuation here. So there is a chance as you go from the local minima. Now this is a practical observation that descent does help with this. It doesn't definitely cure On the other hand, it does take escaping silly local minima. So this is an advantage that basically We did it for the cheap computation, The other one we also talked about was the flat regions. So you could be having this being very, going down. So if your termination criterion tells then you-- it looks like flat, and you will stop. Every now and then when you do the you up and down and the algorithm Still, termination is a tricky criterion, need to consider all the examples in But for some of the flat regions, just a little bit with it. So there are basically annoying a surface that gradient descent will help stochastic version. Now, the third advantage-- shows randomization helps-- the third advantage you have It is the simplest possible optimization You take one example, you do something, And I will see an example in And because it's simple, there are So people have used it a lot, and applications. So you can find rules of thumb that So I'll give you one rule of thumb Remember the learning rate? The learning rate was telling And we talked about, you know, that if approximation. If it's too small, So sometimes you ask, what should Obviously, the exact answer depends dependent on scaling the error up and pin it down. From a practical point of view, if applications, you take a normal mean squared or something, and That actually works. So you can always start with this, and stochastic gradient descent. So this is a theorem, eta equals 0.1! And the proof is that. These are advantages, so we are now gradient descent. And let's see it in action. I'll take an example far from networks. I'll take an example that we looked at be very easy to formalize Remember movie ratings? What was that? Oh, that was the example where you want do a rating. And you want to look at previous ratings Now it looked like this, at least the proposed solution, that we factors, which are basically They like comedy, they like action, So there are some values here describing the user if you will. And then a movie-- you describe the Does it have comedy? Does it have-- et cetera. And the idea now is that we are going existing ratings in the training why this rating is. And hopefully by the time we do that, So I do this for the movies that this factors of the user, the factors of the same combination that I did here, the rating. So all I want to do here is using stochastic gradient descent, that was used in this solution So although it is very, it is actually used. And if you are working for something probably will try your best So the fact that actually stochastic that late stage tells you that it's So in order to put some formality on users and movies. So it would be user i, movie j, and the rating we will call r_ij. Very simple. Now there are factors for the users So let's call them something. The factors for the user will be u_1, numbers that describe the And the corresponding factors for a movie which describe the content When we said we're going to match the of the movie, what we were going to a coordinate, small k, from k equals 1 to K, and multiply these two. So we're taking an inner product And then sum up. And that will tell us the level At least that is the quantity we are So we would like the difference between to be small. That's the goal. Now in order to be accurate in the notation, up to v_k depend on which Different users have different So I'm going to add the label of the And now, if you look at the picture, So it's a bit more elaborate notation, And we also introduce it So this will be exactly the case. And for all of the users and all the different users rating So the factors are reused for different training set. And now your idea is, how do I make these training set, hopefully that And the way you do it is, you define is the difference between the current factors suggest. The factors now are your parameters, for the parameters that minimizes this. a time, if you do descend on this one, it If you wanted to do batch gradient ratings, add up these terms for then descend on those. But the stochastic gradient descent Could there be anything simpler? You're going to get the partial this that appears here. And remember in the first one, we said factors and try to nudge them a little And now we have a principled The nudging will be proportional by partial each factor. So I have a bunch of factors. Which factors do I modify Now we have the formula, and the I'm going to move in the space that now And I'm going to move in that very that makes me, with a certain size of error in estimating the rating. So you can implement this. And indeed, if you implement it, you Not a winning score, but a pretty good And in this case, people started adding which will be an important But basically, the simplest stochastic squared error on something as simple So now we know that stochastic And stochastic gradient descent apply to neural networks model, so let's talk about I am going to start with the networks, because it's That's where they got their name, and excitement that got them to have So biological inspiration is a method applications a number of times. And there is a little bit of a leap interested in replicating You know, humans learn. We want machines to learn. So in order to replicate the function, replicate the structure. That's what we do. We try to make it look like the perform the same. It is a legitimate approach because an existence proof, and it Maybe the structure has So in the case of neural networks, We have neurons connected a large number of them. Each of them does a simple job. The job, the action of a particular from different synapses. Synapses have weights. Very much similar, if you look at thought of the perceptron. Except, obviously, they are different and whatnot, but this So the idea, now, maybe if we put a bunch network, we will be able to achieve the a biological system does. And we get to replicate it, and get a network of this sort. And indeed, this was the initial Now I'm going to make a single comment inspiration in this way. So I'm going to give you another inspiration. And we'll get a lesson from it. So the other example is the following. We want to fly. We look around. Birds fly. Let's try to get inspired by birds. And after a long chain of events, Now, there is no question which is what we are going There are wings, there is But once you got the basic structure discipline-- if you're in biology, your structure does the function, So you want to know how biology In engineering, you want You don't care how you do it. You're just using biology Completely legitimate approaches perspectives. But once you did the initial thing, you and seeing what organs the bird has. No, no, no. What you went here, and all of a sudden equations and conformal mappings. And when you get the solution, you doesn't flap its wings. So, imitating biology has a limit. You have to get an inspiration for what derive what you need. So going back to our model here. We will get this. Now if I derive a way to learn, et an engineering point of view to biologically plausible. If I'm a biologist, I had better because biological system is working. So if I tell you that it's doing plausible, I already violated Here, as long as I get So it is fine to take the inspiration, We are actually trying to build from an engineering point of view, And that is where the neural So knowing that the building block is perceptrons together in a neural with combinations of perceptrons And I'm going to do this pictorially. I will save the math when we define So we'll just look at pictures of what them, and we will get the idea that unit does achieve something. So let's look at the famous problem Remember the four points? With the diagonal +1 and -1. If you want something that is plus here minus here, you're out of luck as far Now we are exploring, can we do this in the right way? That's the goal. So we look at this and say, I can a perceptron I'm going to I'm going to get the And maybe now I can take the outputs of a way that achieves And you look at it and say, that's actually very plausible. And your building blocks for doing that AND's. The logical OR and AND. So you think, let's say I have two Or in this case, +1 or -1. Can I implement an AND, which both are +1? Or can I implement an OR, which them is +1? That would be the AND and OR. Can I implement these Why? Because I am in the game of trying to seeing where this can take me. Well, the OR is very simple. I can do this because I realize because of the constant term that ahead of the 0. So in order for this to actually go be -1, right? And therefore, this actually does of them is +1, I will For this one, I'm resisting So I'd better have both of them to report +1. So this actually implements the AND. So indeed I can implement the OR and Now, you create layers of perceptrons So in our case, we had h_1 and h_2 that the Euclidean space, and we just The combination now, if you look at h_1 and h_2 bar, the negative of Basically, you are implementing An XOR wants one of them to be +1, So this is what you want to implement, a variable, if I have that ready-- I don't I know that I have h_1, and I don't know whether I have this but likely I do. Then all I need to do is combine them then I will get the function I want. So let's expand the first layer, and So now, you do have h_1 and h_2. We perceptrons. So what you do, when you have as if you are negating. And a weight of +1, you So you have -1 and +1. And then you get the first layer to thing itself, but the AND sometimes negation, in order to implement So you end up with these. And these guys will be implementing And now you pass them on to the OR, and So let's plot the full multilayer function we want. It looks like this. This is your original input space. This is x_1 a real number, x_2 a real this is the x_0, the constant 1. This is the perceptron h_1 and h_2 that first picture. So these are the components, and I can After I implement them using a perceptron, one and the negation of the other, And then I do the OR, and get here. So this multilayer perceptron perceptron failed in. And we have layers. So each layer would be this fellow, neurons themselves, the perceptrons. And this is the second layer, So in this case we have three layers. We have strict rules in the So it's feedforward, that is, you don't a previous layer, and you also It is very hierarchical. You go from this layer to the next the next layer. It didn't restrict us very much because done logic before, you realize that if and the negations, you So I can have a very sophisticated those guys and combining them, I can get the restriction of this So that's pretty good. We now realize that we And to illustrate the powerful model Let's be ambitious, not only just the which we remember we had to go just using perceptrons. So you say, definitely that doesn't And I'm using lines, there's So what am I going to do? Let me try 8 perceptrons. Just sort of cornering this. If I do this, each of them will be +1 So I have a pattern of And all I need to do is the logical inside and where I'm outside. So I end up with a polygon, an octagon in this case, that approximates the circle, using 8. I can go for 16. And then I'm getting closer And I can get as close as I want, by And now I have a bigger task of to get the final thing I have. And indeed, you can prove that neurons can approximate any function, And for us, being powerful is good, Once I give you, this Everybody will be excited, except Wait a minute, I have So what are the two red flags? One of them is generalization. I have a powerful model. I have so many perceptrons, so they have freedom, VC dimension. I'm in trouble. Well, you are in trouble, but at least That is, you can completely I have this model. It has that VC Done deal. So this is not going to scare us. It is going to make us careful about to the resources of data we have. So this is not really a deal breaker. The real deal breaker for using the optimization. Even for a single perceptron, we perceptron learning algorithm that applies And we say that in the case of optimization problem. It's a combinatorial optimization, and Can you imagine, now, the problem when layers, and combine them? And now I'm trying to find what that matches a function. You don't know what the function is. Here, you looked at it. But I'm just giving you examples. I'm asking you to match. How are you going to adjust the weights, That's an incredibly difficult And that's what neural networks do. That's the only thing they do. They have a way of getting And the way they are going to do it is which are hard-threshold, they are Not that they like soft thresholds, but of being smooth, twice Rings a bell? Oh, maybe we can apply order to find the solution. And once you find the solution, I know the weights. Soft threshold is almost the Let me hard-threshold the answer, So that would be the approach. So let's look at neural networks. The neural network will look like this. It has the inputs-- same as inputs And each layer has a nonlinearity. I'm referring to the nonlinearity Remember, theta was used in logistic logistic function. I'm using it here generically for It turns out the nonlinearity we are going logistic function except it goes in order to replicate the hard -1 to +1. In the case of logistic regression, We were simulating a probability So it's very similar to this. And in principle, when you use a neural could be different. You can have your different we talk about the algorithm, that there is order to accommodate these So I could have a label for each of And the most famous, different actually to make all of them And then when you go to the So this part would be as if This would be with a view to So the intermediate things are doing combine them in order to get But for the purpose of this lecture consider all these thetas And all of them will be this function mathematically in a moment. So this is the neural network. It has the same rules. It's feedforward. There is no going back, there And the first column is the input x. So you are going to apply your input follow the rules of derivation from one at the end, and then you are going to of my hypothesis, the neural-network The intermediate values we are going to user doesn't see them. You put the input, there's a black If you open the box, you'll find that interesting is happening in the comment about later on. But these are the ones. And for a notation, we're going to So in this case, it will be three. This is the first layer This is the second layer This is the third layer This is not really hidden, So this is the final layer, L. The notation here will persist Now I'm going to take this, and I'm equations that go with it, in order If you want to code this, the next you to implement. First thing, I'm going to define the It's a soft threshold and we are going hyperbolic tangent. And the hyperbolic tangent-- Well, the formula looks more or less for the logistic one. It's again based on e^s. And this one At 0, it's exactly 0. It has a slope 1. It has very interesting properties. And you can see now why If you take it this way, it looks And in the beginning it looks linear. So it has the combination So if your signal, which is what you have here-- this is If your signal, which is the weighted it's as if you are linear. If your signal is extremely large, it's as the benefit of one function that is optimization. So this is the one we're going to use. Now what I'm going to do, I'm going to neural network, because Obviously the notation will be more have different layers. So I have an index for that. I have different neurons per layer. So I have an index for that. And inputs go to the output. And then the output becomes the So I just need to get my house in order, So although this is mostly an important viewgraph to follow because networks, you just print this viewgraph neural network. The parameters of a neural network The weights now happen to belong And there are three indices One of them, the different layers, the different outputs I get. I have different inputs and different So the weight is connecting one a certain layer. So let's have a notation. I'm going to introduce a notation, So I'll denote the layer by l. And l, as you see, appears That will be our standard notation. The layer is always a superscript the quantity we have. And then I have the inputs. The inputs we are going to And obviously, since the weight connects should appear as an index. And the output will be called j. So now my parameters for the network Although it's more elaborate than where it came from. Now let's talk about the ranges of For l as we discussed, l will go layer to L, which would be the The outputs go from 1 d as in dimension. So you have-- I'm going to the neuron 1, And because I am in layer l, by layer that I'm talking about So d superscript l, the number will And depending on which layer you have, units, and therefore the Now for the inputs, they come You take the outputs of the inputs in your layer. Therefore, the index for i will go layer, l minus 1. Now, I left this out because this Anybody knows why? Yeah, yeah, it's that constant Every neuron will have that as an input, a generic one, which is x_0 So for every value in this array, you parameters you want to determine. Now, let's see the function What you do is, you get the x's in layer l minus 1. Right? And our notation will give this a generic unit in this layer, and this was the What do you do in order to get that? We do what perceptrons do, You combine them with the weights. The weights are connecting the i to weights of this layer. So when we talk about the weights, where the output is. You sum these up. You sum them up from i equals 0, which maximum, which would be the maximum for d superscript l minus 1. So this is the signal. Now you pass the signal through this case a soft threshold. And you're ready to go. That will be the function And indeed, this would be the value of the output x. And it happens to be theta of-- we are are going to call it the signal again. And now the signal corresponds So the signal is of layer l and You pass it through the nonlinearity, the output of that. So that wasn't too bad. Now, when you use the network, this a recursive definition. So you do this for the first layer, Every time you use it, you So the first, you get the outputs These are the inputs to the second. You get the outputs of the second, these And you keep repeating, until Now, how do you start this? You start this by applying your input, variables of the network. The input variables happen to And they happen to be called x_1 up to Therefore, d superscript 0 is the same So this one actually has-- what is the x? x_1 up to x_d. So this guy matches this. Therefore, that is how you The number of inputs is the same Once you leave that, it It could be expanding, shrinking, And when it arrives, it should arrive You have a scalar output, and will end up with one output, which since I have one output, So this is my output of the network, output of the network is the That is the entire operation of So when you tell me what the weights what the hypothesis does. Now our job is to find the weights a bunch of input-output examples. When I put those inputs and look at network is replicating them well. That is the backpropagation algorithm. So let's do what? We are going to apply stochastic So you take one example at a time, adjust all the weights of the negative of the gradient, according That's what makes it stochastic. So let's do it. Now the parameters are This array, which is a funny array, is not quite a complete matrix different number of neurons So this is just a funny array. But it's indexed by i j l. It's a legitimate array. And this determines h. Therefore, what I'm doing here is example: x_n , y_n. And I'm going to-- by definition, I have Let's call it e of h-- my error measure between the value neural network, and the target label. And this happens to be a function of y_n is a constant, x_n is a constant. This is part of the training example. h is determined by the w. That's why this is w, and I'm putting active quantity now when So to implement SGD, all you gradient of this quantity. And what is gradient of this quantity? Well, the gradient of this quantity Each component is partial the error, So we put it down. All you need to do is compute That's all you need to do! And then you take this entire vector space along the negative That is the game. There is nothing mysterious about this. If you never heard of backpropagation, see in a moment. The idea is just to do it efficiently. And it makes a big difference when you something. For example, those of you who have Fast Fourier Transform. Fast Fourier Transform is, discrete Fourier transform. What's the big deal? The big deal is because it's faster. You get N logarithm N, instead And that simple factor made the just by that algorithm. And very similar here. Backpropagation, if you look at it, I every i, j, and l. But now I have one thing that will get once, so to speak. And therefore it's efficient and people neural networks became quite popular. So let's try to compute this. Let me take part of the network. So this is in the layer l minus 1, I'm looking at the output of one neuron some weight, into this guy. So it is contributing to the signal goes into the nonlinearity Now this quantity is not mysterious. If you look at it, we can evaluate That is for every single weight in the is the error? Well, the error is sitting there. At the output, I have-- Here's my pointer. I have the output. I went further than I should! But the output is sitting Therefore, there is an error. And that error will change, And that will tell you what So we can do this analytically. There is nothing mysterious. I can get the output as a function of layer, of the previous layer, So I have this function that has tons one of them. And I can say, what is partial Apply chain rule, get a number. Not a big deal. It's not your favorite activity, Or even you can do it numerically. I can take this fellow, perturb it happens for the error of the output. And therefore, I can get numerical The problem with those approaches for each one of them. What I'm going to do now, I'm going to me get the entire array, which is the So here is the trick. The trick is the following. I'm going to express partial e by is upstairs here, with respect I'm going to get it in terms of partial the same quantity by partial this signal, times partial the intermediate This is just chain rule. But chain rule with partial derivatives, a little bit careful, because there may affecting the output, and you need But here, if you're looking for how does affects this sum. w_ij affects only the sum s_j. So if I get partial by partial s_j, affects the output, and therefore I'm nothing to sum up with respect to. So I have this chain rule. That's nice. I can probably look at this and say, this How does the signal change We probably can get But this one is almost as How does the error change It doesn't look like But the great progress is that this recursively. That's the key. So what do we have in this equation? Well, we have the first one. Because if I take this guy, what s is simply the sum So partial s by partial w is the x, and this is the coefficient there. And that is readily available. I already computed all the The other guy, this is a troublesome one, a name, and see if we can get And the name we're going So now delta goes with a signal. There will be a delta sitting here, And the interesting is that the to this weight, which will determine how much you change get that gradient, you move along It means, in each component, you go in derivative. So since this is the partial derivative, product of these two guys-- One of them is x here, and one So we'll be changing the weight weight is sandwiched between. And that's a pretty attractive one. If I get all of those, then I look at in between will change accordingly. Now, let's get delta Why do I get delta for When we computed the thing, we got x's And then we propagated forward The reason why we're going to get it is if you know delta later, you're going So this will be propagating backwards, backpropagation. So we're going to start with And it's not a surprise, because I'm partial something. The closer I am to the action, to compute it. And indeed, for the output, it This is the definition of delta And when you look at the final layer, It's l equals L, and j equals 1. I have a scalar function, so Therefore, the quantity I'm interested this quantity. I want delta superscript L, That's what I want to compute. Now, can I compute this? Let's look at it. This is e of w, the thing What is e of w? e of w is the error measure, whatever hypothesis, that is the value of the with the weights frozen. You apply x_n. You go forward until you get You compare that to the target output, And that error will be your e of w. Why is it e of w? Because h depends on w. So that is not mysterious It's the value of the output. Right? And that happens to be the variable in that is your output. And, for example, let's say that you for the moment. This can apply for any analytic But if you're using mean squared That's a friendly quantity, because now this, and this fellow is related to respect to. This is a constant. I can deal with the squared. So I'm getting closer to being able So let's look at x, the output. Well, the output is nothing but-- you nonlinearity, right? The nonlinearity is the tanh. Not mysterious. The signal is what I'm differentiating I'm almost done. So now, all I need to do is realize that know the derivative of theta, because differentiating with respect to this. And this is an intermediate quantity, So what is theta dash? So what is the derivative of the tanh? Happens to be 1 minus This is for this particular one. If you have another nonlinearity, you This is good. So we have delta for the final layer. If I put the input, get the output, have an explicit value: delta at So now, the next item is to other delta's. This is the essence So now, I'm taking the network, but I'm taking one unit here, and looking at all the units in happen to be affected by x, and Remember delta is partial And I want to get partial this by partial by partial the s's here. I'm going backwards. So I already computed up to here, So now, I need to take into consideration affects the output, so I'm drawing This is the quantity that I want. I want to evaluate partial So I want to compute partial by So now I'm going to apply I will get partial e by partial these mind, I already know. That's the first part of the chain. Then I'm going to get partial Fine. As long as I'm making progress towards any way you want. And finally, I'm doing this. Partial x by partial s. So you go through this. This is partial e by partial the final intermediate. However, the way this fellow affects all of those guys. So when I do the chain rule, I need to happens through. And therefore, I need to sum here for this quantity. The way e is affected by this guy is fellow through here, or by this fellow And therefore, the rule in this It looks like a very hairy one, Now let's collapse it to It's a sum of something. Let's take it one term at a time. We will take this. What is the partial derivative x_i simply happens to be the So all I need to do is just apply it to the value at hand. So what do you get? You get theta dash applied to the signal. I can have that. How about the next guy? That's an easy one. What is the derivative Yeah, this is just the sum. I get the coefficient, the coefficient what I get. Do I have all of this? Yes. The next guy is the interesting one. How do I get this? Well, you don't get it. You already This happens to be the old delta. So now I have the lower delta And I have the top delta in hand. We are done. We just have to keep doing this, And the form for the delta So this fellow does not depend on And this happens to be the derivative minus that squared. So when you get 1 minus that can factor it out. The rest of it depends on j and you're getting this. Now isn't it lovely to have This looks exactly like We're taking something, combining it getting this. Instead of applying a nonlinearity, multiplying by this funny guy. So it looks like a very much But when you are done, you are going position where an s is. And from our previous experience, then the x, and adjust the weight that is So you see the reverse, It's delta's going down, the It used to go up. So let's do this. And then instead of having theta here, what we're multiplying That's what you do in the So here's the algorithm. First, the picture That's all you do. You take the input, you compute the x's compute the delta's backward. This is supposed to be delta-- delta The delta and the x determine the weight in between. So if you put the algorithm this way, you pick n at random-- that's what makes You do the forward run I described. You do the backward run, and then you input and the delta that are You keep this until it's time to stop, and that is your algorithm. Now there are obviously all the local minima, all of that. That's the There's something specific here that initialization. Because it's very tempting to initialize actually very well with If you initialize weights to 0 here, So let me describe why. First, the prescription is to Why is initializing 0 bad? If you follow the math, you realize that 0, which is what that means, and you x's or the delta's will be 0. In every possible weight, one sandwiching it will be 0. And therefore, the adjustment of the would be 0. And therefore, nothing will happen. This is just because of the terrible the top of a hill, unable So you're not moving. If I just nudge you a little like there's no tomorrow. But as long as you're there, Pretty much like you think of a donkey of food on top of it. All it needs to do is eat or eat. Unfortunately, it's perfectly symmetric, the symmetry, and it starves to death. So we just want to break the symmetry. So we introduce randomness: shake here to just start with Choose weights that are small and One final remark and we'll call it a day, So let's look at the network again. This is the network. We have an understanding of this fellow, the output. And the hidden layers were just a means dependency. So, if you think what the hidden layers a nonlinear transform, aren't they? I have these raw inputs, and I'm passing can look at these guys and And because they are higher-order a better one. And this one will be features Now the only difference, and it's a big difference, between the nonlinear transform here and explicitly in the case of these are learned features. Remember when I told you not to look at the data before The network is looking at It is actually adjusting the weights that fits the data. And this is not bothering me, because I the proper VC. The weights here that constitute that The VC dimension is more or less That's the rule of thumb here. So it is completely fine to look at the the data that is bad, it is looking at that is bad. And here it's built in that So this is nice, because now you can transformation, it's a nonlinear very specifically the dependency So that's the source of Now comes the question, hidden layers are doing? So I'll tell you a story. Early in my career, I was doing wanted to apply neural networks Very easy. Give me the data, we'll do it. We'll take a fairly simple network. So one of the people in the bank that came and asked me: can you please tell So in my mind I think, is he doubting He wants reassurance or I mean, the performance is perfect, sample and whatnot. But then I realized that the reason he absolutely nothing to do It's legal. If you deny credit for someone, And you cannot send a letter we denied credit because lambda [LAUGHTER] So that's the reason. But the fact that you are not able to learning is very, very common. Go back to the movie example. We get the factors. We predict the ratings. And let's say you apply the recommending movies to someone. And the person is so impressed. You are spot every time. So they come and ask You tell him, because factor number 7 So he says: OK, great. So what is factor number 7? And then you say-- lots of hand waving. You have no idea what factor number 7 is important in your case. Very common in machine learning learning algorithm tried to learn, it It didn't try to explain to you That was the goal. Let me stop here, and then take Let's start the Q&amp;A. MODERATOR: The first question is could you explain what people a momentum term in neural networks? PROFESSOR: Momentum is used the batch gradient descent, in order to get some effect So the idea is that if you use gradient using strictly 1st order, And if the surface is changing so order is important, you want to get through the trouble of computing the So if you take what's called a momentum take a little bit of the step that you previous step, and so on, you end up accounting for Because if the surface is curved, this flat, it goes the other way. So I didn't-- There are lots of heuristics. For stochastic gradient descent, actually works very nicely. And in all honesty, if I have to go conjugate gradient, because it's gets the bottom line. I'm not big on using momentum in my have found it to be useful. MODERATOR: Some people are asking about has had its ups and downs. So what's the state of the research if there's any? PROFESSOR: Initially, neural networks the problems of the universe. So the usual hype. And hype in some sense is not bad for excited and gets enough people to And then when it settles down, there's So I don't think this was But what happened is that because of the simplicity of the algorithm, people And it became a standard tool. And there are lots of tools you will you just apply a neural network. And until this very day, there them very, very regularly. So they are post-research, so to speak. There's very little done in terms of been answered. But in terms of being used in they are used. They have very serious competitors, machines and lots of other models, Not the top choice nowadays, but every something and he did this, and network and got good results. MODERATOR: OK. How to choose the number of layers? PROFESSOR: This is model selection. So the neural networks is really hypothesis sets-- and there are obviously bunch How many layers? And how many units per layer? So if you look from an approximation products in logic, you can implement network, provided that you have But that's not an approximation a learning question. So the real question is, how And then the question of organizing So how many weights can I afford? Because they reflect directly on the examples I need. And there are actually methods that, tries to kill some weights in order to a method for regularization. And we'll allude to that, when But basically, this is a model selection selection tools apply. The most profound of them will be dedicated to. MODERATOR: Can you-- why was tanh, the hyperbolic PROFESSOR: Why is it used? MODERATOR: Yes. PROFESSOR: OK. I want a soft threshold, and I want it And I want it to have a nice analytic These are basically the three reasons. In the other case, it was exactly the something to go from I wanted something to go from 0 to 1, wanted a probability. Here, I wasn't really interested in There I was, because it's Here I was interested in the continuity analytic property of differentiation, But what I care about is going from decision version. MODERATOR: Will the final weights the samples are being picked? PROFESSOR: Correct. They will depend on the They will depend on the order They will depend on that, but that We are never assured of getting to the We will get to a local minimum, But the whole idea is that you are And if you do what we suggested in the you just start from different starting randomization for the presentation. This randomization could be, you You could pick a random permutation, according to that permutation, from epoch to epoch. Or you could simply be lazy and And all of these, more or less, with different results. So if you try a variety of those, pick the best minimum you have, you will will be fairly more robust in particular choices that you made in MODERATOR: Could you go There. So if you could review the two red optimization? PROFESSOR: OK. So the top part of the figure showed a sophisticated model, because unit of it is linear-- the perceptron-- you can implement a circle, just for a pretty difficult surface by When you have a powerful model, it means you can express a lot of things, generalization comes in because, if you a big hypothesis set. And then the question of zooming in handled in theory. But the comment here is that we are whatever model we have, and the generalization consideration. We may decide that this is too at the VC dimension of it and the decide we just cannot generalize. But at least it's under control, because describes it. In terms of optimization, now it's not and I'm just designing perceptrons. I'm given a data set-- and I have a multilayer perceptron, a perceptron function, of a perceptron And now I want to choose the weights order to get there. So obviously that's a very difficult was difficult even for one perceptron. That's why the optimization That's why we needed to go for function, where we have something like MODERATOR: You mentioned the VC parameters. So they want to comment on it. PROFESSOR: We are not going to be perceptrons, of getting the In this case, there are some analyses. And because the weights are not impact-- you can play around with compensate for one another, and there Therefore, they don't contribute full So you can upper-bound it by the number something fairly close to the number So as a rule of thumb, you take being the VC dimension. And that has stood the test MODERATOR: In terms of the interpretation, it's not enough to interpret the-- PROFESSOR: Oh, if your I understand perfectly what It gives 0.3 weight to the first input, and minus 0.4 weight to the third compares to the threshold, If you take that as an interpretation, But an interpretation here, that makes sense. That, for example, your interpretation this factor is a comedy content. People can relate to that. But what we are saying is that the cannot be articulated in simple terms interpretation. And similarly for the hidden layer here. MODERATOR: Can you say what happened What explanation was taken in the end? PROFESSOR: No, I can't. [LAUGHING] It's a private consultation. But basically, the question was MODERATOR: Can you explain again why in data snooping is not a good practice? PROFESSOR: Data snooping is a bad When we get to data snooping-- lectures, we will say that you either The problem is that if you data-snoop, of its impact on generalization, extremely optimistic. You go to the bank, if you do a private consulting job for that predicts the stock market great. And then you give them. And when they go to stock market, and that's the problem. Because you thought it would So data snooping, in the way I presented didn't account for that-- we learned in our mind, but we didn't space we worked on. That was the problem, rather than But since the damage is almost not to look at the data, because the In the case of neural networks, there prescribed way. A learning algorithm was actually constitute the hidden layer. So therefore, it is looking On the other hand, the accounting has because, as I mentioned, the weights the VC dimension. So we know the impact on the MODERATOR: Does the range of the weights PROFESSOR: Which weight? Repeat the question, please. MODERATOR: Does the range of PROFESSOR: Let's say that So let's say that you're Eventually, you will take the output layer scaling the weights. But the intermediate weights, the actual value of their output next layer. So you cannot just say that I'm But supposedly, the learning rate was the error function. And the minimum of the error function combination of the weights. So it shouldn't affect it, in the Obviously if I change the rate, I may but it's not like I will end up in I use a reasonable learning rate. Yes it does affect it, but it affects Pretty much like you can say: condition affect the result? Well it affects it, but it affects it in just averaging over a number of cases, order to immunize against MODERATOR: Is there a relation between algorithms? PROFESSOR: I guess both of interested in a biology reflection. Genetic algorithms are optimization a generation and having mating and so it doesn't apply. Everything in machine learning has been So there were actually people trying to algorithms. You'll find in the literature But at a basic level, neural network Genetic algorithm is an optimization And therefore, there's really no MODERATOR: Small confusion. Does in-sample training constitute PROFESSOR: The strict answer is yes. You look at the data all too well. You're are actually looking at the data, performance of the data, and all of you have already put into account weight space-- the weight space And therefore, when you do that and you a guarantee of generalization from to the out-of-sample. So the learning algorithm looks at It looks at the data. But it's already-- before we even loose on the data, we have already chosen And we put the generalization MODERATOR: What do you recommend, or using a package? PROFESSOR: It's-- Honestly, it's a borderline case. For example, if you're doing the It's so simple. In neural networks, it's a little bit bugs that are typical. I used to have this as an exercise, and doing it is not worth So to answer your question, a package, for neural networks. MODERATOR: Does analyzing-- performing some sort of sensitivity information about how the neural PROFESSOR: Yeah, there's There are questions of regularization effective the weight is, and the There are all kinds of analyses. studied to a great level of detail. And indeed, the choice of the weights, perturbation of the weight-- all MODERATOR: Are there other models interpretation? PROFESSOR: If you have a bunch algorithm is going to choose them, parameters is not clear. You can artificially put constraints or you can start from an initial condition and you're just fine-tuning it. But that's if you are very keen MODERATOR: Going back to where there were logic implementations So there was a confusion. Are we still trying to learn weights PROFESSOR: This was an illustration you combine perceptrons, you're able to This didn't touch on learning yet. After we do that, we found that the an interesting model to study. And from then on, it became We had a neural network. We are no functions, and try to We are just going to put it as a model, the weights, which is backpropagation MODERATOR: Could you briefly PROFESSOR: OK. I think it is best described when validation. It is basically a way to prevent So I think it will be much better understand what overfitting is, and with overfitting-- regularization, And then early stopping will MODERATOR: A question on stochastic When you go through an epoch, you choose have not selected, right? PROFESSOR: There are lots of choices. An epoch is one run. And it's a good idea to get all So one way to get it to be random and them is, instead of choosing the point permutation from 1 to N, and then And then for the next epoch, If you do it this way, eventually every same, but an epoch will be a little You're can define it simply as N covered all of them or not. That is valid. And some people simply do a sequential You just go through the examples, or you go through the examples in that And there are some observations about not that profound. MODERATOR: Does having layers of the neural network? PROFESSOR: Loops as in feedback, Once you have feedback, even the implementing becomes tricky, because So it's a completely different type. There are recurrent neural networks, model that started work in neural networks. And it has completely different Here we're implementing a function, a layered way, in order to get And since we showed that you can have a big enough model, you something by doing that. One can become, say-- Maybe I can get a smaller network possible. It's an interesting intellectual impact, it has very little. MODERATOR: In terms of the VC dimension, on the number of parameters, if you had arrange them in layers, what do you PROFESSOR: OK. If you believe in the rule of thumb, based on upper and lower bounds, then if I rearrange the number of nodes weights will change because I say how many neurons here, and how number, and that will give So as long as you take your guiding many weights did I put in the network. You'll be more or less OK. Obviously, you can take extreme cases, into one neuron, feeding into one neuron-- So you have tons of weights that are But within reason, if you have taken reasonable, then the number of weights MODERATOR: OK, we should quit. PROFESSOR: So, we'll see you