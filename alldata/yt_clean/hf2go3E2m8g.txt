In this three-hour Data Engineering Master Class, the Data Engineering life cycle, data generation data modeling, SQL versus NoSQL, data processing (Extract, Transform, Load), data architecture, and the architecture from scratch. We'll cover data dimensions, data marts, data lakes, data lake engineering on cloud, top AWS services you should real-world case study architectures on AWS, GCP also explore the modern data stack, important understanding Python and SQL for data engineering, BigQuery, understanding Apache Spark with and Apache Kafka for data engineering, and many pen and paper, and start taking notes so that time. And before you move forward, make sure to if you are new here. Let's get started with the The Fundamentals of Data Engineering Data Engineering is because if we want to we need to have our basics clear. Now, if you have few years, then you might already know what Data But if you're seeing me for the first time or if it is important for you to understand what Okay, now we already know, right? Everything because this is where Data Engineering happens, Okay, the businesses are, let's say, Amazon. is an e-commerce company. What do they do? They okay, from your home. Now, this is the business Netflix? Okay, the business of Netflix is to and they give you the exclusive content. On top of the other things. Okay, this is the business Zomato? Okay, this is a food delivery app in okay, and the order will get delivered to you there are multiple companies doing businesses on have certain goals and visions for the business, Why do they want to understand the customer? they want to increase their profit. Okay, I of the goals, that I want to increase my profit, some of the bottlenecks they might have in the improve the business process. And like this, there Now, if they want to achieve all of these goals, happening, and one of the best ways companies most of the time, all of these decisions are taken let's say, who is working in the shipping the ground and has knowledge about this particular some business knowledge to take direct decisions because he's an expert. He's been working in so he understands what might be the problem. But on some of the information that we don't know. And is by understanding what the data says. You can some time, but if you want to be right most of The only way you can be sure about all of these okay? And this is where the entire picture of Data all of these come into the picture. So let's start Okay, I just painted you the picture. The reason in the first place is that companies want to they want to improve their business, they want to to, like, remove the challenges they might have in you the direct answer. It gives you the factual things, okay? So this is the understanding Now, how do all of these things okay? At the end, we want to have the final right?--improve business revenue, and it can So these are my business goals, okay? These are single thing you do in your data ecosystem, or is for this only, okay? Anything you do should like, the highest algorithm, but it doesn't it is completely useless, okay? It should help the to save costs, it should help the business to to understand the customer--whatever it can be. If otherwise, it is completely useless, okay? you do should create value for the business. the entire pipeline of Data Engineering and the okay? Before we just understand the Data how different things or different fields okay? So we have the company--this is my company okay? And the company is doing, like, it can be Now, at the front end, we usually have the This might be my mobile--there's a button, and this application, okay? I'm the user--I have Instagram whatever, okay? I might be using LinkedIn--I have application, data gets generated, okay? Whenever like something, when I comment on something, every if I click on a certain product, every single okay? Now, all of this data will get stored, okay, management systems. Now, there are different types in this video, but just try to understand every DBMS, a database management system, okay? storing this kind of data, right? You can store operation, okay, which is called Create, that in the further video, but all of these specially designed for storing all of these alright, we have the data available. Now, data understand--from the application, our data gets Data Engineering pipeline starts, okay? The Data Learning or Data Analytics might happen over okay? There might be some dashboard or some or there might be a machine learning model, okay? but this is one of the robots or machine learning these different things, okay? Just trying to, entire ecosystem--there are many different things there might be DevOps who might be deploying the DBMS, whenever we have any data available, okay? picture because you can store all of the data okay? And then you can utilize all of Once you have the data generated, this is where you don't have Data Engineering, Data Science, on the data. If you have the data, then you can then you can't do anything about it, okay? So is having a data generation in place, and this You have the application, the data is getting as sensor data, okay? A truck is moving from one that is moving from one location, and it is going from B, it might go to C location, okay? Now, It goes from here to here to here. Now, we need gets captured by the sensors, right? The truck the application. The truck might have sensors. we have data coming from numerous places, okay? So sent to the system, and all of the other of Data Engineering, which is where Data As we move forward, we will understand all of the but just try to understand where Data Engineering between the application development and the okay, it is over here--this is my application side, dashboarding side. Data Engineering sits okay? I'm connecting one thing to the second thing that we will understand, and then I pass the data source, and I pass my data to the next source. How features, functionality, and frameworks they use? in this video, so don't worry about it, okay? Okay, so now that we understood what is the role of Data Engineers in this place? software engineers, okay? The general role of web app--it can be a web application, it can be the things, okay? Then we might have the DBA. engineers in smaller companies, but if you're Administrator who develops the data, right? different columns, and all of the other things. can also do that, or the software engineers size and your job profile--but let's understand. okay? So this guy will be writing a database. Data Engineers. The roles of Data Engineers--there to write the ETL pipeline (Extract, Transform, but ETL is basically we extract data from one end, okay? Then, it can be also building a database warehouse. Data Engineers can also do that, okay? modeling--we'll understand that. Working with big using Spark, Hadoop, using different frameworks data, we can also do that. Data integration, data data is coming from the sensors, data is coming data, so Data Engineers have the responsibility. check of the data and governance, how to are the core use cases of the Data Engineers. okay? Data Science or Data Analysts, okay? and Data Analysts is basically that Data Analysts in the past, okay? How can we, like, what was compared to the last five years, right? They are some of the answers. The role of Data Science right? We did a product sale for this particular be the product sale for this particular product answers, right? They try to predict what will and we have the Machine Learning Engineers who can Amazon, we have the recommendation system, done by Machine Learning Engineers. They deploy system so that a system can learn by itself and can predict what is happening inside your system, recommend them the right information. Like on reels as per your interest, okay? They don't recommend you random things just to understand to train the machine learning algorithm Now, the difference between DS and ML is quite might do the ML work, or an ML person might do the they might have individual work to do, but in smaller organizations, they might so do not get caught up in the title like, &quot;Oh, a Machine Learning Engineer do?&quot; Just try to top level. In the actual organization, when you go have to do everything by yourself because the distinction between all of these roles. There are just fancy names, but on a fundamental So we understood what Data Engineering is, data from one source, okay? It can be any data and pass this data to Data Science or Machine or they can, you know, build machine learning okay, there is a proper approach to it, okay? You directly push it to the Data Science person--there properly so that the entire pipeline that okay? And this is what we will understand, okay? cycle. This is taken from the book Fundamentals to so many people, and it is one of the best books Engineering. A lot of the material that I have and some of the material I also added in this So the first step here is data generation, right? Data generation--data is getting generated from what? APIs, okay? RDBMS, it comes from Analytics or all of the other things, okay? all of this data that is coming from these data together and ingest it into the system, here. Let me just remove this, okay, this part, data generated from one place, then we need to ingestion can be setting up the connection with building a system that can read the data from data into our Data Engineering system, okay? We system feels like when we actually look at the okay? We have the data generation, and that data okay? And we just build a programmatic connection the RDBMS, okay, it should automatically multiple approaches to do that, but these are the we ingest this into our system. Then the data that kind of storage layer we have, so every data to store all of this data at some location, okay? so this is where the storage happens, okay? We between this ingestion and the serving, okay? to machine learning, analytics, and reporting, okay? After the Data Engineering happens, we have building a dashboard or who are building a machine is the part, okay? The Machine Learning or the these things happen over here. This is where the getting stored, okay? Between that is the core of Transformation is basically the set of business data--this is usually what we call raw data because raw data. This is my raw data, okay? Here, this When we serve this, this is the transformed data, between this and this is called a transformation. can be anything, okay? So consider this example. coming from the API, okay? I have data coming from okay? Now, in both of the data, I have a date date column, and the format of the date in the API The first--I think it's June of 2024--the date is format is like MM-DD-YYYY, okay? Something like alright? Now we have a date coming. Now, what we at the end, we need to find the analysis. There might be one more ID column available over here. do we join it? Okay, when we join data coming from okay? This is a product date, okay? And this okay? Now, when we join this information, we need alright, that can be formatted as this particular anything. This is the decision that business this data based on this format only, so any okay, it should be transformed into the YYYY-MM-DD this data after the transformation block, okay, will have my transformation block, okay? This data can be done by Python, PySpark, Scala, whatever it okay? How do we do the transformation? And at the okay? The date values will be converted into transformation, okay? This is one example, but removing duplicate values, it can be removing the it can be merging two data sets, it can be columns, concatenating--it can be anything, okay? is basically a set of business logic that you query or use any tool to do that to generate a or the Machine Learning person can build a model okay? So as a Data Engineer, my role is to that we can easily visualize this or we can easily so that is my job. I want to make the data happens in the transformation layer, okay? getting data generated from one source--it can be of this data is getting ingested into one system. such a way that any time a new data is getting and store it inside our storage system, okay? this data available, we need to make sure the data passes through a certain transformation logic so we serve this data to a user. A user can be or some dashboard expert--it can be anything, understand, build machine learning models. This is talking about, okay? There are some undercurrents so don't worry about it, but I hope you understand fundamental point of view because this is really to do all of these things, but if you then it doesn't matter which tool you use--you pick the shittiest tool in the market, okay, That is the power you have as a Data Engineer you don't really need to know anything okay? It doesn't take time to learn any new modern to write the Spark code, it's very easy, okay? You execute. There are some angles to Spark, such as drivers, and all of these other things that you but to do this entire job is not that difficult, understand how to make connections between Now that you understood, we can go forward and right? How can I do the generation? How can transformation? How can I do the serving? What Analytics, Reporting--every single thing that about this part further down the video, okay? data generation and data storage one more time. generated from multiple places. Data is coming okay, these are called RDBMS, okay? There are will talk about, so don't worry about it. Data devices, okay? It is coming from there. It is We understand data is coming from logs and machine we are running the technical machines, so they are the utilization of this technical machine, we can on and save costs over there also, okay? Then we okay? Third-party data. Sorry for the bad is getting generated, okay? Now, once we have the The storing of the data is basically we store it transactional data and relational data, so from okay? This is where the data generation--you can it is connected to the application, okay? And data is getting stored inside the RDBMS, so you matter, okay? Because from the Data Engineering data generation source, okay? From the application layer also, okay? So quite tricky to understand, generation and storage also. We also have a will understand. For data storage, we have data about, okay? The thing that we understood about part--this is the data generation, okay? And the part, okay? We can store our data in the RDBMS, storage can be like S3, Google Cloud Storage, that we will also understand, okay? You can also these are the storage systems. We understood the and where our data will get stored. So, okay, this the DBMS, okay? The thing that we were talking okay, that are used for data generation and Database Management System, okay? These storing your data in a structured way Now, understand this, okay? You can also store already know, right? You can have columns here your data. But if you want to store, let's and if you want to find a specific record, okay? Because if you want to find a specific or the one lakh, one lakh row, okay, it will be okay, are specially designed for this kind and you can easily retrieve, update your data as DBMS systems available. We have PostgreSQL--this source. We have Microsoft SQL Server, we have okay? If you want to get started, Postgres to work with all of these systems, we have a stands for Structured Query Language, okay? Now, with the database. You might already know or you have heard about it somewhere, but general to the data space, SQL is the language Now, what can we do with SQL? We can do okay? We have a SELECT query to fetch the data. data. We can update the data. We can delete the inside the table. It looks something like this, and the actual data stored inside this--this is The data that we talk about, like it can be, okay? And there is a table, Student. What okay? Student will have a name. It will a city where the student lives. So ID can be one. 26, and the city can be Mumbai. Just like this, let's say, Akash. Age can be 25, and is living in our table, okay? So this is what is happening let's say, where the student ID is equal to two, the SQL queries. I can insert new data as ID3, I can update this data, say, if I want to update multiple SQL cases. If you want to learn about but this is the fundamental concept of SQL, okay? the SQL that is used for working with the DBMS that you can call to work with the system, Now, this is where we are slowly diving into the okay? We have cleared the foundation part the individual concepts that are important for Data modeling. Now see, whenever we are designing build or store our data, we need to design a representation of how our data looks, okay? So example that we all understand, which is Amazon, Now, just use your general knowledge, okay, and Amazon will store. Data modeling is basically of how our data will get stored inside the RDBMS, need to think about what kind of tables or what okay? I want to store in Amazon, right? I might okay? I'm storing about the orders. I might my website. Orders, then the product, I've been might store about the payments, okay? Okay, okay? Shipping. I might store information about my platform. And like this, there might be right? But this is the basic table. Like and I'm designing a data model from scratch. first person who is starting an e-commerce company okay, I'm going to be Designing my data model, these are the pieces of information that I want talking from the application side right now. Okay, one by one. These are all concepts you really engineer. So, I'm going step by step to make Okay, so we have the orders, users, products, I'm satisfied with all of this information that design a data model for this. Okay, it will look the orders. I will create an order table. This is have a lot of things. So, first of all, I have let's be satisfied with this. Then we have the also have the user ID, name, age, address, and all Okay, then we have the product. Now, we have the product ID. This is the primary key or the unique have the product name, product category, product product unit size--lots of things that we can payment amount, and payment date can be there. will have shipping. Shipping ID and shipping date, this is sellers. We will have the sellers' ID, Okay, so we just kind of figured out the tables them. Alright, so all of these tables only make right? So how does the relationship happen? Okay, all of the information that is getting ordered on a user ID. This is a foreign key; this will be single products, so we will have information about Which product did they order? Okay, so we also join this particular thing over here. Okay, it is Okay, so we understand that a user will order in the order table, which user ordered particular user order? Okay, then this is done. payment information. Let's say if I want to add The payment ID, okay, and then I can also track down easily over here. So in the order, what was payment that particular user made, we can also then in the shipping, what do we have? We have which seller is selling which product? So let me just get the right color. We can so we can understand which seller is selling make a connection between them too. not this one. After this, what do we have? this is a seller's information. He's selling connection between these as well. So what we will below the product. I will just use a different ID just to understand which seller is selling a connection between this seller ID from here. seller ID, something like this. But in general, so shipping will have information about the order can join this particular thing over here also. together. Again, this is the worst but I just want to show you the fundamental side if you just search on Google for a data model So in reality, a data model really looks such as draw.io, or there are some specific kind of diagram. And I teach all of these you can check the description if you want to know of data modeling. I go in-depth in my courses, Okay, now we understand the data modeling. This is relational databases; they have a specific schema data model, every single piece of information has is basically the data type. So, let's say the order ID will be, let's say, order ID will be order date will be the date value. User ID will be single column has some kind of schema or data relational database table because it is properly and you use SQL queries to work with it. database. In SQL, we store our data in the column store our data in different types of formats. One basics of Python or JSON, it's something be a value attached to it, ID one. Then we will D. Something like this. And the age will be, let's in the key and value. So, if you want to find, you can just search it by the name, age, or family. All of this data is actually stored inside graph data. Graph data is used for representation. to give you an overview that this kind of After this, these are the usual comparisons that relational, which basically means that the data properly stored and have a relationship between to that one; the order table is connected to the product ID; the user table is connected to the them with specific primary and foreign key IDs. Then we have the analytical, which is usually this is what we will talk about further down Then we have the NoSQL. In NoSQL, we have the if you want to understand all of this, you can it. We don't want to spend time on NoSQL because you will be working with mainly in the real world SQL databases, and you will be using data Okay, now in SQL, the two things that we these are the two different data processing okay, and we want to talk about that. So let me we have two data storage processing systems. OLAP. OLTP means online transactional processing, Okay, in SQL, we have the relational things. Relational is usually called online is called online analytical processing. This is and this is the data warehouse. Data warehouse. as a data engineer. Now we are slowly, slowly deep Okay, now OLTP system has some kind of use case, is not something where OLTP is better or OLAP the entire system. Now, the use case of OLTP It is used for transactional data. What does you send money to one person from your account, a transaction. When you purchase something that particular information of the product--that and made payment for this amount--that entire stored inside the OLTP system. These systems are when you want to do a fast insert of or when you want to do quick reads of the data on if you want to do that. These are very fast talked about the CRUD operation: Create, Read, It is very useful for this kind of workload. transaction level. Whenever you have a lot of or banking, the transaction doesn't only mean such as if you buy a product, if you return row-level information that is getting stored. let's say if I want to understand the last five I won't be able to do that. And I'll explain the system. The OLAP system, the name literally says reason OLAP systems are good is that they are want to analyze the last five years of data, Let me just explain this individually so that you is mostly row-based. So, every piece of the row. Like, this is my ID, this is my name, something like this. Now, all of this information this is the OLTP system used for transactions, so you want to do something on the row level, if you update the age, delete a particular thing at the want to analyze the entire data--let's say this and what I want to do, I want to aggregate, and like this. And if I want to analyze this entire if I were to write the query, such as 'SELECT particular user table,' let's say if I run this gets executed, it will first fetch all of these it will fetch all of these rows, and then from single column. This particular single column do the sum. Now, picking the entire column or a useless process for this operation. Understand of payment, I just want to get the information and every individual row? Because this entire level. Every single piece of information is information about the payment, I will have to scan select the one single column only. Now, as I said, if I want to update or delete a specific row. this, OLAP systems are column-based. So, all of as the ID, name, date of birth, age, whatever it the data warehouse, if I execute the same query, find them as column-based. So, all of the single will be stored like this. In this case, we are this. We will have the first row, and all of the We will have one single row available, one, then 25, and this information will get stored. After so everything will get stored at the row level. level. Over here, it will store everything at the three. IDs will get stored, then we will have the have the payment information stored inside the something like this. So, every single thing that level. Just try to understand and visualize this. instead of scanning the entire row, instead of it will directly go to the payment and directly scanning the ID or the name is not needed. We fetch the result that we need. This is Now, understand this as a data engineer. As a systems to OLAP systems. In between, we will understood about data generation, data generation data is getting generated. This is where I do the and this is where I do the analysis. This is where and the data analyst will write the query to dashboards, ML models, AI models, whatever you data warehouse, or the storage layer that we will the future about object storage, so don't worry we are just trying to zoom into the individual So, data engineering is basically taking this data data from OLTP systems, APIs, ingest it into the and load it into the data warehouse. This is this? You understand everything, but how does this ETL: Extract, Transform, Load. You might already call this ETL: Extract, Transform, Load. The same data engineering lifecycle is one way; it is ETL and this is the serving layer, which is the architecture of how things work. This is what ETL pipeline. We extract the data, we transform know about this, right? Where do we even extract analytics, sensor APIs, and all of this data and then we do the transformation. We understood duplicates, handling null values. Structured data scale. If one age is stored inside, let's say, stored inside the integer value, we bring it to format, we bring it to the same level. And then can be on the data warehouse. Data warehouses are of different data warehouses. Or you can also like S3, Google Cloud Storage (GCS), or Azure Data of ETL that we will also talk about one by one. We did all of this work just to understand this the top layer of it. Just the top layer just to understand the top layer, now I want engineering lifecycle. That is the undercurrent: orchestration, and software engineering. Security: just by the name, you understand that who is able to access our data and the person with the right authorization can only our data to every single person working in the Data management: that basically means data be able to easily find the data that we need. e-commerce company in Europe and the US for of tables--in the system. Now, if I had to find I had to go through the documentation they created this particular location. This is what we call the definition: what each and every single column tables, and if you access one of the tables from let's say, hundreds of columns, and you want could be something like the payment gateway ID this particular column means. This is the use of type of data is stored. This is very important. data? Who is the user? Did you create this table? user and understand if I don't really understand If I am working in the shipping department, I entire shipping table. Now, if any person from, department wants to understand what is going on me. I am accountable for that particular Then we have data modeling, which we already piece of data makes sense; every piece of data is it should not have any random information. DevOps is basically to automate the entire process practices. DataOps is somewhat similar. You reporting. That basically means everything that thing that is happening in your data system, you report the incidents that are happening. and that is a fundamental concept of DataOps, data right? When you deploy something, is it I should be able to get the error message. I are working. I should be able to monitor what Data architecture: we have a detailed section analyze the information, analyze the trade-offs, proper architecture for the Orchestration: this is used for coordination, engineering, we have multiple data pipelines jobs. It is just a fancy name, but it's just data to some location. This entire operation there might be hundreds of data pipelines deployed of these things. Let's say once the first the second data pipeline because the second data All of these things are called orchestration. We of workload, and we will also understand Software engineering: software engineering testing, and debugging. You have to apply the you write the ETL, the transformation job using engineering for scalability. You should also use pipelines. So, all of these are the fundamental you should remember security is important, data architecture is important, orchestration and good to know. You don't need to deep dive you will understand them one by one. architecture. If you want to become a good data and we will be referring to one of the new Data Engineers.&quot; So, let's jump into that. that I am re-recording this part of the segment and my disc got full. I ran out of space, my OBS like a one and a half-hour file, got corrupted. to have one complete video. If you're I'll urge you to at least like this video comment something so that it increases the reach Okay, let's start with the video. Now, till now, of data engineering, right? We understood what in the entire pipeline, the data engineering versus OLTP. So, we cleared the basic fundamentals so we understood the core data engineering aspect. how data engineering happens in the real world, how the architecture is actually built from the how you understand the business side, how to these things together and individual components, Okay, so let's start. Now, I want to make you before we even understand the different parts of understand how to build the basic architecture as set, and we'll be learning about that, right? If you are interested, you can also subscribe to get the high-quality data engineering blogs. Now, till now, we have understood that the goal problem. From the start of the video, I've been that everything you do as a data engineer or as of these things for the business. Now, it can cost to building a full-fledged data system to I want to take you on a journey to understand how the data engineering point of view. Because the basic understanding of how to design the is data architecture? So, from the definition architecture is a design of systems to support data needs are achieved by flexible and reversible of trade-offs. We'll understand this technical like before you construct a building, right? You you're trying to build, let's say, a 12-floor Inside the blueprint, you have to add some of elevation, elevator, stairs, office, restroom--all you can start building the entire construction. of foundation, floor plans, elevation, and what are the different software that you have to how do you write the transformation, the staging many more. Just like you think about building an to think about the data when you are building data are the different components that we need in order Now, as per the technical definition that we just and reversible, which means like each and every in case something goes wrong, you should be and it should be easily reversible. So, every direction, it should be easily reversible so means. It is achieved by flexible and reversible of trade-offs. Trade-offs are basically, you which technologies you can choose. We'll Now, building data architecture is divided into and the second is technological integration, the technical architecture. Let's try and then we'll deep dive into them individually. inside the operational architecture. Again, doing is for the business only. So, before you writing code and all of the other things, first, in the first place. Because once you know that, So, the first step in building data architecture, project, is to understand the operational side or platform, what is the impact of the XYZ category thing. This is my business goal. I want to find is there a delay in product shipping? So, I want shipping. I want to understand why there is a How do we manage data quality from the third-party different third parties, such as FedEx or some from multiple places. How do we manage data are the different business goals that we have. need to think in this particular direction. These now I have to build my technical architecture In the technical architecture, we focus on store, and transform data. What happens when on the technical side, we mainly focus on storage, how do we transform data, and if there is a we also think about scalability. This is more of a you focus on the business. One is the business The second is the technical side, where you think can use. Let's try to understand all of these The operational architecture ensures that your objectives. It is the &quot;why&quot; behind every piece business architecture or the Operational are doing this entire activity. Why are we even in achieving their goals. So, operational every piece of data you collect. about when building the operational architecture the end in mind. Always begin by understanding This clarity will guide your decisions and contributes to the business outcome. This is very understand what the business goals are before you technologies. Understand what the business needs, the technological side. Technology is very easy you don't know what the business needs, you and will never be able to get out of it. changing every six months--a new product line comes product strategies--these things happen your architecture, it should be able to iterate And focus on impact. Everything you do should solution you architect should have a clear line of customer satisfaction, streamlining operations, data initiative should be measurable and aligned architecture and aligning with business goals. the building block. This is where the actual is about &quot;why,&quot; the technical architecture is the technologies and methodologies, you'll be able to use technologies--technology is our &quot;how&quot; to meet we want to achieve. Very simple to understand. we have thousands of tools available in and you can see there are so many different tools you zoom in. If you want to understand each tools available for different kinds of workloads. technologies as per your business use case. Now, &quot;Okay, I'll use Snowflake, I'll use Apache solve my business problem.&quot; It doesn't script as long as it solves and helps you not about choosing fancy tools or something you should be thinking about saving costs and helps you, whether it is an enterprise-level as it solves your business problem, you're good. do you build the technical architecture? technical architecture as simple as possible your system more maintainable, scalable, and the easier it is to maintain, scale, and the system, the harder it is to debug errors. There is no one-size-fits-all solution processing, and analysis tools totally depend case. If you have structured data, you can go with you might not need Snowflake or another basic ad-hoc query interfaces like Amazon Athena, decisions should be made based on your business tools; it's about solving your business problem. if you are not dealing with billions of rows right are projecting that growth, you should be planning For example, currently, you're using Python to have billions of rows tomorrow. You should keep For instance, you can use distributed processing needed. Start with a smaller cluster and then not that everything is perfect when you start; Third is embedding automation. A lot of times, try to solve different errors manually, you should generate scripts and automation to do get an email or a Slack notification, depending on single day whether your data pipeline is working, that you don't have to check manually. security and governance. In the digital age, data secure your database, encrypt your data, and are the different things you need to consider Now, let's bring all of these different things real world. Let's take the example of the data easy to understand. The first thing is that this case, let's define the business goals, define the operational architecture, like what the first goal is to improve customer experience: recommendations, and enhance customer service. overall site navigation, how customers interact engine and customer service integration. inventory management, order processing, times. We need to improve our entire operational reduce shipping costs, and shorten delivery times. how customers are behaving so we can improve Vendor management: we might be working with strategy for better product availability, And fifth, compliance and security: in making payments, so there are compliance we don't capture credit card information, doesn't get leaked. These are some of the So, these are the business goals, right? We want efficiency, marketing insight, vendor management, business goals, we can think about building the The first is our data ingestion layer. We and the purpose of the ingest is to collect data server logs, vendor systems, inventory management, Apache Kafka for real-time data streaming to After we capture our data, we need to store it time. The purpose is to store collected data in Different components, like object storage (S3 like Snowflake or BigQuery for your business requirements. use--Snowflake or Redshift, for example? going with Redshift might be a good choice due to your business needs, you can go with Snowflake or understand your data size and frequency, see how different technologies behave with your So, we might have to structure our data before the data processing and transformation layer and transform our raw data into a structured we're working with large datasets. If you have you can go with simple Python scripts. But if multiple sources, you might want to go with Apache After the data is in the data warehouse, into play. This is where machine learning and machine learning models for predictions to the final value comes in--when a person from see issues in shipping, and make the right Business intelligence tools like Tableau and machine learning platforms like TensorFlow and algorithms. There's also the side of data we meet regulatory compliance, such as GDPR and to follow when storing data, like encrypting data masking in more detail later in Lastly, we have the data integration and API and sending data between different systems, so we systems. We also need to think about this. our final architecture might look like this the final architecture, but it might look As you can see, we have data coming and stream data. This data is ingested into We can use transformation layers such as and then store it on Amazon Redshift. We can also and SageMaker as a machine learning platform. This architecture is built to fulfill our then define the tools to use, and then build the it looks similar to the data engineering lifecycle ingestion, storage, transformation, serving, and fundamental block, and this real-world You can plug and play--if you want to S3 as a data lake, you can. If you want to can. If you prefer Databricks over AWS Glue, go That's everything about building architecture. we can move forward and discuss the other parts. good. Let's continue with our second thing. how it's built, let's try to understand the their use cases, and how the entire Let's start by understanding the data warehouse. looks like (architecture shown). So, we have data comes from APIs, RDBMS, websites--all these the streaming engine and gets ingested, and our data gets stored inside the data warehouse. is extracting data, transforming it, and There's one more concept called EL, where we extract and load the data into a staging area, the transformation on the fly using SQL queries. we extract, transform, and then load it as per build a data warehouse. In the real world, ETL is to organize your data. ELT is also used, and some ELT, where you don't have to do the transformation is and then transform it as needed. However, data is often messy and requires some data warehouse. ETL is what you'll be to know that ELT also exists for some use cases. database part, we understood that data models are tables as possible and reduce duplicates in each stored across different tables. Let me show you (example shown). We have different tables that information about a user who purchased a product, order table to get order information, then connect track payment information, you'll four different tables to get one outcome. for analytical workloads. Even if you join all aggregating user or order information, the OLTP will struggle because it's not designed for rows one by one and then pull one single This is where the data warehouse comes in, but without following specific methods--that's where a method to store data in relational databases in a data warehouse called dimensional modeling. Dimensions and Facts. Dimensions and Facts to build your data warehouse. This is called table. The fact table is always one--there will The fact table stores information can be measured in the business, such revenue, profit--all the quantitative It is the center of your dimensional modeling. tables, each representing different business product dimension, a date dimension, and stores information about the categories or product category, user name, user city--all If you want to understand how I have a course available on data warehousing with just providing a fundamental overview. star schema and snowflake schema. These are the dimension model. Let me show you (example shown). fact table in the center with different dimension hence the name &quot;star schema.&quot; The snowflake schema sub-dimension tables attached to the dimension model but still has a fact table in the middle, In the star schema, you have the fact table in forming a star shape. The snowflake schema added to the main dimension tables. The snowflake company that offers databases as a service. with an e-commerce company. We'll have a fact where it stores all transactional information. attributes like price, quantity, and weight--all you'll have different dimension tables, like order Each dimension table will store descriptive values relevant information. You can join these tables final analysis. This makes analysis easier because its quantity, you just need to join two tables. OLAP database (Online Analytical Processing If you want to understand this in in my data warehouse course on Snowflake. Now that we've covered facts and dimensions, I (SCDs). We know that these facts, and price, keep changing. Quantity changes, need to be reflected in the system. We understood or RDBMS systems, through ETL to the data or however frequently it's scheduled. But these don't change frequently--these are dimension values when they do change, how do we handle that? Changing Dimensions (SCDs) comes in. SCDs change slowly over time. There are different different types like SCD1, SCD2, and SCD3, each In SCD Type 1, the values are overwritten, and overwrite data without keeping the previous value, from New York to New Jersey, we simply overwrite there's no way to know what the previous value In SCD Type 2, we maintain a complete history we add a new row with all the details without ways to handle this, such as using a flag York and then changes to New Jersey, we'll add a current value. If there are further changes, keeping the history intact. We can also use In SCD Type 3, we maintain partial history. For city in separate columns. If the city changes in the &quot;previous city&quot; column and New There are also more advanced types like SCD6, capturing the current city, previous city, start These are fundamental concepts, and you can find tutorials or check my course on depth with real datasets. of data marts. Let me take a sip of Okay, so data marts. Now, data marts are to understand this, the subset part we understand, tables available like this. Okay, now these order information, payment information. We have These are the product dimension, order date dimension. Okay, these are the different Now, like this, there are many different teams teams available, such as the teams that might be information. There's a team that handles the handles the payment, there's a team that handles the different departments available. Inside these Now, all of these teams don't really need this own business use case. So, understand this: I have different departments, okay? These we have different teams working on different you will see that in a large company, okay? You will see something like this, where we different departments. Inside the departments, department's issues so that they can meet the this: if this team solves their own problem, that problem, which means they are solving the departments solve their own problems, that Now, in order for these departments to solve their own reporting system, right? The analysis, data is that they build the reporting system as per requirement. And what they do is create a subset So instead of, let's say, for the shipping about, let's say, users, payment, and the information from these three tables only. using these three tables, and they will choose all of the other things. Let's say there are 300 just pick 100 columns from these three tables, system for their own department. This is called a subset of a data warehouse. I'm building my own my requirement. Data mart, a subset of the data my department's problem; that helps the company the data mart concept? Let's move forward. was tossed because of object storage now, okay? we understood, right, what we have to do--we have and then you can build a data warehouse and store data warehouse gets stored in a structured format, format. So, what that means is that every time you have to make changes in the structure, and that is that already has millions of rows of data and five okay, 1, 2, 3, 4, 5, okay, 1, 2, 3, 4, 5--it has I decide to add one more column, okay, inside the will have to change the entire structure and then changing the structure, changing the schema type, have to take a lot of things into consideration. you don't worry about the ETL, okay, you don't transformations and putting your structured like S3--you store all of your data into the data You can use S3 as a data lake storage, okay? It of your data as it is, right? I will store all of I will store all of my JSON data as it is onto Now, different teams from different as in the data mart, there are different teams they want to generate their own reporting. So, data as it is into the data lake, okay? As per data lake itself. Basically, you can directly read object storage system, as per your requirement. are getting quite heavy, but I'm just trying but I'll continue--just pause the video if it you can understand, just go forward with it. okay? So, data lake. So, data lake is basically you can use Azure Blob Storage or Azure Data Lake, okay, as a data lake, which is kind of like object as raw, okay? And on the other side, right, on the read this data, okay? This is called schema on read this column from this file, I want to read file systems, okay? I will read all of this data table onto, let's say, Athena, okay, Athena, or me, and they will build their own table, or they and put it in the structured format. So here, we Instead of processing all of the data over here process the data that we need and then store that Now, it is not that, okay, data warehouse is or that data lake is good. Both of these systems data warehouses give you the flexibility of whereas data lakes give you the ability to access Okay, let's understand the difference between a data warehouse, data is structured, as you can see in. Okay, data is structured, okay? The users processing for BI reporting and all of the other data, and it is usually relational, right--columns you can store JSON data, you can store Parquet, data analysts and data scientists because instead want to build their own machine learning once you have data added, you can only work with as per the business goals, and changing the a lot of changes inside the pipeline also. So, lake is a gold mine because it is completely raw a file inside the object storage as it is. It is I want to read, as per my requirement. I can I can build a table on top of it as per my case is for stream processing, machine learning, the data is raw, data is large, and it is so it is undefined, okay? This is the difference okay? This is what we have understood till now. actual hands-on part, if you want, I have some okay? I will just comment down--I will give you you can do it and understand the data warehouse these things hands-on in my courses, so if you are about the combo pack, okay? So till now, we have started by understanding what data engineering the entire pipeline, okay? We understood about DBA, DS, ML, and all of the other things. We data engineering life cycle, okay? We understood all of these things happen. The storage part, we how the transformation actually happens. relational databases, data modeling, okay, how SQL versus NoSQL, data storage processing such as transaction and column-based databases, why OLTP is needed because we go from OLTP to OLAP while about ETL processing, understood about the data ops, architecture, software engineering. okay? We understood about operational architecture things. We understood about the data warehouse, understood about dimensional modeling, understood understood about the difference between fact the dimension tables. It stored transactional slowly changing dimensions, why we need them, marts--a subset of the data warehouse--why we need the difference between a data warehouse and a data engineering, actually. I was not even expecting thought I'd just give an overview, but I went into everything because I really love teaching, right? this section, do let me know by commenting, and if you're still watching, salute! Alright, this video till here and you are about to complete courses--if you're interested, right, if you love out my data engineering courses. I create in-depth not just about the course--it's about giving you technology, how this works in the real world, it's about understanding where it is used, how things I teach in my courses, so do check them You'll also find the latest coupon code available And yeah, let's continue with our video. and we also looked at this big data landscape. tools' names? Can you see the different things okay? As you can see, Snowflake, AWS Oracle--there are some new companies here. there might be S3, Databricks is used, Cloudera systems provided by the different NoSQL databases, Couchbase DB, and all of the other things. I was telling you about this, right? For BI platform, data science notebooks, MLOps, these different things, right? We have a set of everything that we want to do, we have a different understood while we were talking about the single thing needs a set of tools, and we have Now, we will understand these individual tools, use cases for them, which tools are the okay? So that we will understand, and how Now, let's talk about the cloud platforms, Cloud platforms are basically giant computers company. It can be Amazon, okay, this is Amazon, Now, again, these are the three top cloud are plenty of cloud providers--you have Cloudera, different cloud provider has its own features, but What is cloud computing? It is basically these and different services so that you can use them to do--we used to build our own servers, okay? get your hard disk, okay? You get the processing you get all of the wires, you get the ACs to cool you get all of these different things, switches, you build it on your own. Okay, now you can do to save on cloud costs, but this also comes with okay? You have to maintain this. What if the fails and I lose all of my data? You also have to about scalability, okay? How do I scale this I'm just working with millions of data and the so I will have to buy new hardware, okay, and What if my RAM fails, okay? What if the hardware of my data center resources? Anything can happen, this is the reason people usually go with the all of these things by myself if I can directly cloud providers always charge pay-per-use, okay? use. That's pretty awesome, right? I will only pay if I just use a simple virtual machine, which two hours for some workload, I am only going in the on-premise data center, have to keep is how the entire server is set up, right? If I some other workloads, my website is also hosted databases, and everything, so I have to keep want to do some workload quickly for two and I can also pay for that use case. for different use cases, okay? These three different parts. We have PaaS, okay, This is Platform as a Service, this is Software a Service. What do these three things mean? direct platform, so you don't have to worry about for example, on AWS, we have a service called a Service because they directly give you focus on writing your code--they will take such as running the server, all of the backend and everything. You just focus on writing your Second is Software as a Service. You can think alright? You have Google Sheets, AppScript--not Slides? You have the entire Google Suite. You because they are directly giving you access so you can use that and grow your business. That basically means cloud providers will give is EC2 machine--this is basically a virtual machine like Elastic MapReduce, to run your Spark jobs. give you so that you can run your workloads, okay? services--they give you these services that you services have different names as per the cloud we have these many services and many more. These don't worry about the names if you are seeing them right? Don't worry about it. If you already know, or these logos for the first time, just forget which is like the virtual machine, we have Lambda, serverless machine, okay? Elastic Container okay? There is Simple Email Service--it is used for is the database created by AWS, so if you want to like AWS is giving you the service, so you only of resources that you consume, so that you don't Everything is built for you, pay for it, and grow VPC, CloudFront, Elastic Load Balancing, Kinesis Redshift for data warehouse, right? Kinesis, Storage Service, object storage to build a data Cognito, API Gateway, Queue System--you need right? We understood--we have the business you think about, right now, how to build my okay, which cloud computing platform should have the answer--let's say you are a student which cloud computing is the best and will give me and there are high chances that you will with these cloud providers. If I were to rank it can be wrong. This was my opinion a few years I used to rank AWS as one, okay? Azure as two, and and I'm seeing the trend that Azure can be one to their new functionality and good services. the enterprise level, so Azure is good if you want go with Azure, especially in India, because a lot like Microsoft 365 at the enterprise level--because things. So, they are likely to go with Azure right? A lot of startups usually go with AWS easily start, and a lot of people know AWS, like employees with AWS skills, it is quite easy to I'm building my data engineering startup, The third one, I still say, is Google Cloud. that are really good, but these are my takes, but this is what I see in the industry. I say if companies, I mean the enterprise level, the service-based companies can also be taken the other things, up to your requirement--but top can just research their company architecture, and use Azure if they are enterprise level. A lot of if you see CRED, okay--all of these guys are on a good ecosystem. So, I say if you want to target suggest either learning Azure or AWS unless you you that they require skills in GCP, then go If you are a student, then you can go with this. architecture, again, the situation is the same: Okay, we will talk about the different services, cloud providers give us that can help us solve our and technical architecture--now you start thinking GCP, and Azure, and if I say, okay, Azure gives and as per my requirement, I can easily solve they have a good service pack together, so I'll project on Azure and see if that works--if workloads onto Azure. Okay, if that doesn't so you use some services from Azure, you use services from Google Cloud, okay, and build your right? I really love Google BigQuery--this is And on Azure, I really love the Databricks Glue service, which is serverless Spark workloads. S3 as an object storage, okay? If I want to I can use Databricks as my Spark workload, and I also do cross-cloud integration, but maintaining there are some tools that can help you with that you can explore. I just want to throw them at okay? Let's move forward--let's talk about Now, we understood, right, these are the and if I build my entire architecture, how will I go with that? Let's say this is how okay. Collect, process, store, and analyze, architecture that we've been understanding. I can whatever, right? This is object storage, this is NoSQL database, this is the relational database. okay, where we can collect our data and easily Stop--why? Okay, I found this on the web for when yeah, so we can collect the data, okay? okay? Let's say if you want to do something, let's I want to run the Lambda function. Okay, Lambda you want to run small code, you can do that--I processing using EMR, which is a Spark workload. I again the Spark workload, and then I can use I can build my entire data system, right? Instead you a wide range of services that you can pick okay? This is just an example, okay? Just to help right, that AWS gives you--we understood about give you the platform, they might they might give you the infrastructure as services that they provide, and using these okay? And it might look something like this, okay? don't get scared about all of these things--now okay? And this is the architecture of one of the India, okay? Dream11 is like the fantasy betting they have used to build on AWS. Now, if you see completely AWS, okay? There are some things that and there are some things that they use that are built. This is the final version of Dream11--they particular architecture. I have posted a LinkedIn forget, just comment it out--I will add it to it. try to remember our data engineering lifecycle, complicated, the fundamental concept, okay, the okay? First of all, what do we have in we have the generation source. Now here, multiple places, so we have third-party vendors, our data is coming from third-party vendors, there like Cassandra. Where is the streaming data okay? And then there's the application, so the desktop, Dream11.com, as you can see over multiple places. In this case, data is coming from third-party vendors, data is coming from the the iOS. I kept telling you, right? I kept telling what it means. It is coming from multiple places. and most of the time, for ingestion, for people use Apache Kafka, okay? Apache Kafka is a real-time data streaming platform, so you can work Kafka in between to consume all of the data, okay? Let me just write it over here. All of these of this data, okay? Once the data gets into Kafka consumers--consumers who are consuming all of this not deep-diving into Kafka--I will be launching a okay, in the future. But data is getting produced basically what I want to do with this data, okay? as you can see, okay? This is a batch pipeline. is ingested, we need to store our data somewhere, data gets stored inside Amazon S3 as a data lake. the concept of Amazon S3, which is a service on can use S3 as a data lake. I store my data onto This data goes through the ETL, okay? As you can okay, and the ETL is happening using Apache available, and then it stores our data onto Amazon is a data warehouse, okay? This is my data is my ingestion, this is my ingestion, this is my storage, right? This is my storage, okay, this is one more thing that I told you, right? In a data making it into the structured format. Now, there analysis, okay? And as you can see, it is using analysis, and I told you, right? The Looker, okay, can use the raw data that is coming. I can use per my requirement, or I can also use structured to both of these things--I can get the proper data as per my requirement, okay? This is there, that we understood--understand, now try to connect understood data warehouses, we understood data storage--every single thing is put together into We're just trying to understand the real-world fundamental concept in the real world, okay? Every sense here, right? We have the ETL system for ad the ingestion system going on, okay? This is just more thing--we have the real-time pipeline going what they are using is Apache Flink, okay? so if they want to understand data on a real-time from the streaming engine, we go to Elasticsearch, there might be some visualization available the entire pipeline. And the fundamental concept okay? And all of the concepts that we use. this is the data warehouse. I might use Apache Spark to transform my data. I use S3 as my ad hoc query. I use Looker for my visualization, I I use Kafka for my ingestion--these are all of my data streaming--this is the real architecture. just to understand this particular thing. Once you engineering. Now you know, like, yeah, I am a data and what is going on. This is the fundamental you can understand any architecture. Now, once you architecture in the world because you will know, is some transformation happening, there is some I understand this--the tool is different, right? replace this tool with Databricks, right? I can okay? It will work the same. The features might but fundamentally, it will give the same they might have tried multiple things and then is currently working for their system, right? everything you see on your app as Dream11, making it possible, okay? It's not some magic. GCP. Just like AWS, right, we understood that we ingestion, okay? For ingestion, we have App BigQuery, Cloud Function. Now, I want to tell you services available. For example, this is a data is the same as Redshift on AWS. Okay, there's AWS Lambda--fundamentally, they give you a similar different, the feature is different, the cost right? Just like we have Cloud Storage--this object storage. We have Cloud SQL--this is Service that we talked about, right? BigQuery is DataProc is the same as EMR, okay? Elastic okay, services are the same--like most of the cloud always about choosing the best service for your for storage, they have this many, for processing, the concept of the data engineering lifecycle: something, I have to process something, I have architecture on the GCP. Same fundamental concept I ingest this data, I store this data, there I store some data, I store the data onto BigQuery, Cloud, like identity campaign running--this is like is customer data, and data destinations such as Same fundamental concept: data source, collect, where the entire data engineering is happening, I store it, I process it, I give it back. let's look at the Azure level also, okay? These okay? For the compute, we have virtual machine, We have the web and mobile app. For data, we have Warehouses--that is also available. For analytics, Stream Analytics, Machine Learning, Data Factory, In my opinion, Azure has, as a data engineer, workloads, okay? They have three services that okay? I really like Databricks because it is is basically the environment to run Apache okay? Data Factory, and third, I like Synapse services--and there's a new service available I is basically the combination of these multiple okay? It is especially designed making your life much easier. I have a project okay? I'll put the link in the description--if I that. If you want, you can explore that. I also so we do have projects available on that--you Okay, now, this is the architecture okay? I can replicate this entire architecture on basically just replace--let's say if I'm replacing instead of S3, I will use Cloud Storage, okay? okay? I can put DataProc here, okay? I can also I can also put Pub/Sub and DataFlow, okay? but I'll say I'll go with Kafka--Kafka is best. else seems fine. So, I can also convert this--my be different, the costing might be different, the different, but I can do that. Okay, I can also this is what the Azure architecture says, right? we have the customer batch files, okay? Uh, we just adding this onto the ADLS, which is Azure from the external source. Okay, now there's a Data from on-premise sources, and some stream data goes zone, okay? The raw data is getting entered. raw data and store it in the processed folder. and it goes to the SQL pool, which is a SQL use this to build the Power BI dashboard and with the desktop application if needed. Same serve, and use it. Same thing is happening, and we are using the Azure Key Vault to securely and Azure DevOps to properly operationalize These are the different services that can be pipelines that we have used till now. Okay, I GCP, and I showed you using Azure. Now, let's look also modern, just especially built on the cloud. Modern data architecture is basically where new &quot;Okay, the tools that you guys are using are old the new volume, and the approach is very old, data company. I will make your life easier.&quot; So okay? I will say directly load the data into my for you as per your requirement, so that you can the data. This is what the modern company says. directly give you the integration between your I have different data coming from sources like they say is that they have the integration with applications, right? Fivetran, Airbyte, Stitch, can also use Python and SQL, which is also the the other workloads. This company comes and says, these things easier for you. Directly connect data for you, and we'll directly load it onto the per your requirement.&quot; There is a new tool called People say that it is going to replace SQL. of companies come and want to replace SQL, but always learn SQL. DBT is also gaining a lot of multiple things, so multiple--you can divide your you about, like ELT, right? ELT. We were doing ETL do EL, which is we will extract our data and okay? It can be Snowflake, BigQuery, Redshift, different landing zones, okay? This is modern data we create the staging area, we create the fundamental concept. If you see, there is a data storage, and there's a landing area to store the staging area after some transformation, I store my of these things you can create inside the DBT, and Redshift, and all of the other things. Same machine learning people, they can build do the analysis on different tools. There's also that. Basically, that means I have transformed source system and get more insights from the the system again. Uh, this is a totally different but there's also a concept of reverse ETL that we Modern data architecture--we understood about GCP, data tools. A combination of these different data platform. Now, again, we talked about this. right? How do I decide which tool is best for me? this solve my business problem? Yes. If it does, it should be reversible. So, okay, I can easily, me too much, okay? And this is not really even and go with another tool. If this tool is also not is going to work because it is open source, right? is going to cost you for the server, so, uh, you quite difficult to set up. So, as a company, if these things because it saves time, okay? You can go with this. This will solve your problem, solves your problem, whatever helps you reach your uh, I just want to take a break, so I'll have some till now we have understood a lot of things. and again, I can't cover every single thing, but tools that you can learn about data engineering So that is important for you in your career. So data engineering. Now, first of all, if you want things. First of all is the programming language. Java. Now, if you want to learn any programming okay? It's the easiest to learn, mostly used uh, ETL scripts, if you want to write the Kafka Python has a lot of packages that make your life for all of these workloads, so you should always with Java. Java also has good support because most Spark, all of them are written in Java, okay? is to go with Python, okay? Important for you to Python, such as variables, operators, basic data other things. Important things to learn include uh, like there's a package inside Python called the basics of data, how to work with different This is what you can learn in Python. Uh, I have so I'll also put the link to that particular me know, and I will add it, okay? SQL--again, cannot skip this. This is how you communicate so you have to learn SQL. This is non-negotiable, This is the foundation, so you have to, have to commands because you will be working with some Most of the, like, 80 to 90% of the servers online interact because it doesn't have a GUI, right? You accessing it using the terminal. You can learn how to exit, how to find something, how to commands that you can learn. You can just search get a good tutorial, okay? Now, we have data the data warehouses, okay? You can learn--you have have Hive available, SAP Analytics, and Snowflake. this is not dependent on the cloud platform, okay? Also, this is highly demanded in the market, skill set, very highly in demand. There's one okay? Because I've worked with BigQuery for the this service, so this is one of my favorites GCP. So, my suggestion is to go with Snowflake are working with a specific cloud, you are anyway you are anyway going to learn about Redshift. If learn about--if you're learning GCP, you are anyway suggestion is to just learn Snowflake because you cloud. Hive is an open-source, uh, tool that not for Apache Spark or Apache Hadoop workloads, okay? but not really recommended to learn it separately. have a requirement, then you can learn it to two days if you have the basics clear, okay? different workloads, you can use Apache Spark for is very, very important, okay? You cannot skip to process big data. You also have to learn real-time data, okay? Out of these three, I would You can use Kafka for streaming. You can use Flink If you learn GCP, okay, you will automatically Apache Spark, learn Apache Kafka only. Not really you will learn it on the go, okay? Just add Kafka Okay, we have many tools available. Out one of the highly used tools in the market, these tools take roughly 30 minutes to 1 hour to right? You can just watch one video and understand very simple. I learned about Mage in just one there's one project available on our channel also, tools are created to make your life easier, okay? right? It will take some time to understand the tell you about this right now, but you can learn I don't think that will take so much time, okay? okay? These are all part of the modern data okay? As you can see, we have--for ingestion, for ingesting data. For data storage, we have Looker, Data Studio. For data transformation, want to orchestrate your entire thing, we have Great Expectations, and there are metadata you can just search about the tool name, we talk about the modern data stack, it is really in the market, like what problem do they solve. data ingestion--it takes data from one source and you modern transformation, okay? Uh, DBT gives orchestration, so if you want to orchestrate and is for data quality and governance, so these are and you'll find plenty of resources. Alright, right? Uh, what do you need to learn about What do you need to learn about data warehouses, want to cover these individual things. Again, just quickly go through this part. Let me just learning Python is one thing, and learning Python can learn Python for free online, but if you want learn certain things. I'll just show you quickly is my Python for Data Engineering course. I'll take this course, but if you want, you can learn courses just to give you a structured learning okay? So you can learn the basics. All of these them. You can start with strings, you can learn you can learn about data structures like lists, about conditional statements like if-else, you then you can go to the intermediate level, such as list comprehensions, exception handling. We basics of Lambda functions, and object-oriented as NumPy, understanding the NumPy package, Pandas then working with date-time formats--very important with different file formats like JSON, CSV, In my course, I have included one project for project. Uh, I'll tell you about this part, uh, at SQL. Inside SQL, what do you have to learn? You because PostgreSQL is open-source, easy to learn, keywords of SQL such as SELECT, INSERT, UPDATE, types and how to create tables, how to create a okay? Like DML, DDL--like Data Manipulation about that. Uh, you can learn about operators database constraints, primary key, foreign key, statements, joins like inner, left, right, outer, aggregation functions like MIN, MAX, and all topics like subqueries, Common Table Expressions, DENSE_RANK, ROW_NUMBER, LEAD, LAG, set operations, stored procedures. Learn about data modeling--we modeling and data modeling. So learn about that you can pick one company name like e-commerce or build a data model as a project, right? It looks over here. This is like an Instagram data model, okay? After this, you can learn about you can start with the basics: understand what understood about this--understand the difference process, learn about Snowflake, like basics--just on Snowflake also on the YouTube channel. Learn dimensional modeling, which is understanding fact tables, dimension tables, understanding star how to create fact tables, factless fact So these are the things you can learn about Slowly Changing Dimensions. You can learn about the Snowflake database, okay? Like staging, copy how to work with them, virtual warehouses, Snowpipe, time travel, how to undrop things, how zero-copy cloning, data sharing, materialized Snowflake, right? I'm just trying to give you an I have created this step-by-step roadmap. so you can go to this website, DataVidhya, and you is Python, then the second one is SQL, third Databricks, fifth one is workflow orchestration. And then there will be a dedicated cloud computing we have Apache Spark. This is very important. In we need Apache Spark, understand the architecture, transformations, actions, lazy evaluation Apache Spark--very important. Then we have this, Spark. We have two things: structured API and the API, basics of it, how to define user-defined data sources, partitioning, bucketing, how to the lower-level API, such as understanding also learn about production applications, how okay? These are topics that you can also or you can also visit the DataVidhya website just can learn all of these things for free online, this because I'm just going through this because right? Uh, for Airflow also, you can just go that you need to cover? There are a few concepts the projects like this. So I just quickly showed can cover from the website. So instead of writing that will just increase the time of the video, uh, because I've been recording this thing for the you that particular thing. These are the two security and data masking, okay? Data security stage. Uh, in data security, we have to take and availability, right? Ensure your data is don't give access to your data to every user--only Integrity is basically maintaining the accuracy be accurate and should be able to provide the is available to authorized users whenever it's in data security. These are the measures you okay? Encryption should happen so that, uh, if it not be able to understand what the data is. Access classification--classify your data, like if this secure data on the network level. The one concept Uh, that I talked about at the governance level. employee table, okay? What you have to do is--like regulations by governments that say you should right? Like credit card numbers, addresses, social when you do store it, make sure you mask them. this is the ID of the user, right? What do I do if some random number. Let's say this is my Social is my Social Security number, what will I do? XXX-XX- and I will just reveal the last four for credit cards. This is called masking, okay? use for big data. These are common: JSON, CSV, use case. I don't want to go deep dive into this courses, or you can just Google this, and you will learn, okay? So, till now, we have covered a lot the topics, right? So I cannot cover every single this might be around a 3-hour video, okay? I'm not I'll get to know about the timeline, but might have missed some of the topics, so what you you want to learn. Just the fundamental topics, that. Just the fundamental concepts that you want topics inside part two, and I will create a video can watch, okay? Now, you understood all of these you really want to learn about data engineering, the description: DataVidhya/combo-pack. On this till now, only five courses have launched. I'm as you can see. We have a 'Not Available' for that notes if you, uh, enroll in the course because okay? So that you can revise at any time that you notes. So this is my Zero-to-Hero Data Engineering in the future, when they launch the course, and I will also create a new combo pack. So if you okay, at that time, you might see GCP also will get around 14+ projects. I will teach you, It is like a step-by-step approach you will get. for Data Engineering project, uh, course, you will you will build a similar project, but instead of we will be using Snowflake over here, okay? Now, part, okay, for Python, and we will replace it how to evolve one simple project and how to plug will learn in the entire combo pack, right? How production-level project as we go forward, some components, we will add Spark, then we will we will use the same project, and we will this entire pipeline. We will also create a okay? So as you can see, one simple project we right? So that you get an understanding that it's about the fundamentals. The fundamentals that these things over here like this. You will also streaming, and there's one project available on YouTube. Uh, there's a project available on I think the GCP project is--uh, there's a project project is also available--let me just show pipeline project available in the Apache Airflow you will also learn about AWS, and you will learn then, in the future, we will have in-depth get like 14 different projects over here, okay? all of these over here just by clicking this, okay? Previous students, and they have built their you can also go through this and understand that can just click over here, okay? And you will be this is working, okay? Or you can go here--this this guy actually built the Airbnb project, uh, own project and put it on your resume also. Uh, web developers, data engineers, uh, technical can learn this. What you will get from this will get each and everything about the code that Discord community, uh, you will get support if you can ask it on the Discord channel. Someone in the future, you will also get early access and right? So, let's say if I launch the Kafka course, discount for that course also. These are some through that. These are some of the commonly asked if you're interested, you can go through this. okay? My voice is breaking, but the best part is that every single course is in-depth, so you bootcamps available in the market just give you website that offers data engineering courses, they every module, they might have like two to three I will give you all of these things in detail, like this, okay? If I show you here--if I show graph environment. You can see the Apache Spark the different topics connected? These topics are partitioning or the transformation concept is also as you can see, data warehouse, you can see the topics. So all of these are like the topics--cloud, specific topics such as partitioning, okay, can know that, okay, partitioning is available search and learn about the different things, okay? is this? Okay, you will get the detailed notes. uh, this is the basics of Docker, okay? You can let's say if I want to--like writing my first DAG, right? How to write my first DAG, what are every single thing you will get here, okay? Every easier that you don't get distracted by looking just stick to one single path, and you can become you about my courses. If you're interested, just totally up to you, uh, you can use multiple so you can also check that on my YouTube channel. now almost 3 and a half hours--I'm recording this that I don't have to re-record this entire thing. watching this video till now, okay, do let me know video. Also, like this video because I put a lot share this with people so that all the people can So, everything--thank you for watching this video.