So recently I got this text message from a friend of mine where we were talking about AI and he basically says that he isn't writing code anymore that he's using cursor. And I just want to start out with saying, look, this AI stuff, there's something being announced it seems like every day. And that can be kind of overwhelming, right? And it'd be easy to have a lot of reactions to that. Anytime there's a lot of change, and especially if it's combined with all the other things like the economics and jobs and everything else can be a lot. So, stepping back for a moment just to kind of take a moment to breathe and think about how as a person to react to it, a lot of the kind of cyberpunk stuff, especially the stuff that came out in the 80s, and it could be like Akira or Blade Runner or Snow Crash or any of that kind of stuff, where at some point you sort of just wind up kind of embracing the insanity of it, right? There's a band called Gunship. They've got great tracks that are sort of themed in this 80s cyberpunk kind of way, but modern stuff. Just go ahead and imagine in your head that you're listening to this crazy cyberpunk situation and then just kind of roll with it. We're going to go ahead and lean in a little bit. We're going to explore the world of AI and where it's at today and where it's going. So, the first thing is, of course, just to acknowledge that the AI and LM stuff has just been hyped beyond all recognition. Crazy thing is that even though it's been hyped beyond all recognition, it's actually still kind of amazing, right? If we think about some of the stuff that's just sort of been solved in the last few years, like natural language with context, being able to converse with the machine, all the translation capabilities, being able to use an AI as an agent that kind of sits between you and the web that makes it a lot easier to manage and get resources and pull that stuff together. Frankly, even if you just think of an AI or an LLM as a compressed file that has so much of the internet's knowledge, where you can download a 20 gigabyte version of an AI and run it on your laptop, I mean, all of those things are by any measure pretty incredible and they only came out in the last few years and that doesn't even count the stuff with images and everything else. So, it is worth acknowledging that. The thing that's really interesting about this though is that almost all this stuff, none of it produces material that's really the sort of thing that you can put out raw commercially. And there's a couple of reasons for that. One of them is just all the things that we know about hallucinations, it's right most of the time, even if it's right 99% of the time, that's not 100%. That can kind of be summed up as the liability problem, right? If I put a chatbot up on my website and then I authorize the chatbot to do things like, you know, give a customer discounts, well, then I'm also opening it up for abuse. So the same thing is true with everything else. Self-driving car is mostly accurate, but only kills you one in a hundred times that you go on a drive. That's not good enough. Same thing with code, right? If the code mostly works, but doesn't work all the time, you have a problem. The other one, and this is one of the things I think is kind of interesting and it seems like it's going slower than I would have thought, but every single piece of software out there is eventually adding some kind of LLM tech. Stuff that we have seen is pretty basic right now, like summarizing an email, like, okay, whatever. But what you can envision is things like tools that start using LLMs and natural language as a way to work with them more easily, especially the more complicated the piece of software. Right? There was a demo of unity, which is a game development engine that I saw kind of early on. It was probably like over a year ago now where they allowed you to just use natural language and say things like create 10 cubes and color them all red. And it would just make changes into the editor for you. That kind of natural language task automation just seems like a no brainer. And yet here's the thing we're sitting here and it feels like that kind of capability is just now rolling out. There's a lot of reasons why technically the files themselves are large, right? Now at the one hand, the hype is the, and the LLMs are going to take over and do everything. And yet the flip side, we barely can even get them integrated with the software we use today. Maybe that's not the right way to think about it necessarily, but I do think it's instructive to kind of highlight the difference between the hype version of it and the reality. Today, if you want to get a sense of what some of the state of the art LLM for production use looks like, there's these packages out there and most of them are basically still kind of trying to figure out how to even take an LLM and have it talk to a code editor, just a basic text file code editor. So there's a couple like Vercel has, has V zero. There's a, it's used to be something called cloud dev that's now been renamed to Klein. There's cursor, there's replit. These are all attempts to take standard coding things and hook them in to just basic LLM integration, create edit files, text files. That's it. And what's wild is none of these tools even pretend that you're just going to like one shot, say build an app. It's more at the level of we're going to write a little bit of code, add some functions, maybe do some views, and then you have to wind up integrating it and pulling it together a lot. Something like LM studio, which lets you download and run a local LLM on your own computer. And it's really cool, but it's also interesting because if you just download it and run it, you can do things like chat sessions with it, look up information, have it do, help you do some coding and stuff. And it can help you with some writing tasks, but it doesn't really talk to anything else on your computer. It's kind of interesting. And so I touched on this a moment ago, but I think it's worth highlighting again. The reason I think all these integrations are coming along faster or one of the reasons is the accuracy rate. If it's 99% accurate, is that good enough? So what that means is, is that humans are still in the loop for almost everything that these things are doing, right? Now, in a professional development setting, I might say, well, I've got a nice suite of automated tests. Maybe I try to do TDD. Maybe I pair up with the AI to do TDD. And then I've got working with my CI and my CD system and I'm able to manage all that stuff. Well, right out of the gate. If you don't know what that stuff is, like automated tests, TDD, CI, CD, you don't know what all that means. So if you're using one of the tools and it gives you an error message and you don't know how to read the error message, kind of dead in the water. And that is an example of how humans are still having to be a part of it. And they're working on the tooling. Like, for example, some of the tools are able to do things like generate some code, the code that they generate, then generates error messages that come from the compiler and then the code in turn tries to figure out what was wrong and fix it. So it's doing these loops. But every single one of these tools that I've looked at so far, they all kind of have their places where they hit the wall. Now, this is the thing. They're incredible, right? They're fast. They generate all this stuff that's cool, but they also generate bugs occasionally. Is that good enough? Sure. I mean, when I sit down to write code, normally I get red squiggly all the time. That's just life. And then you fix it and you move on. That's what life is. How do you partner with it becomes the challenge. And that, that partner component will come back to in a minute. But first of all, I do think that this is part of why it feels like the layoffs are starting to slow down, at least on my LinkedIn and people I'm talking to the job market while still ugly for a lot of devs. Seems like a lot of the layoffs of the pace of that has slowed down. If you're still looking, obviously that's terrible, but broadly speaking, it seems like there is a bit of a pause, at least to the layoffs that we've been seeing. And I think that part of that is, is because we're kind of hitting that wall. So let's take a simple example. So let's say I have a social media team, right? And they're making posts all the time. So I may have decided that some of the work that they were doing, like writing the posts, I can just have someone work with an LLM to write the posts for me, but somebody still has to approve them. I think you'd be pretty bold to take your social media and fire everybody and then just hook an LLM directly up to your social media. And I do think that's obviously happening a lot with things like all the bots and dead internet theory and all that kind of stuff. But if you're professionally managing a brand, that seems like a kind of a big, big risk to take to just do it cold. Right. That same sort of hesitation that you might have about social media, you also are going to have with your dev team or your art team or anything else, right? You can't quite get away with just pointing LLMs at all these things and saying, good luck. The human being is iterating, they're verifying, they're applying their own sense of taste and style, they're verifying for quality. So they're still doing work in a sense, one could say, adding meaning to the point of the LLMs, right? So it's still there, it's not done yet. So still need the human. That framework, I think, gives a model to think about how a lot of the jobs going forward are going to look. And one sort of metaphor for it is that we become managers for the AI generated material and tools. So imagine, for example, a system that's been created where the bulk of the code was created by an analyst using an LLM, you'd know that that stuff would break. And then now somebody has to go in and clean it up. Oh, maybe you get the situation with the LLMs where you wind up generating way more code, but now you're the one who's managing it. You're the one who's figuring out that quality check, you're the one that's verifying that the code is going to be maintainable in the long term. That's where I come back to this relationship feeling a lot more like a dev manager than a dev. Think what it is like to be a dev manager. So you've got five software engineers that work for you. And they generate code, right? They're writing code, and then they're checking things in, and they're sending you PRs, and then you're evaluating them and then deciding when to check them in, when to accept them or when to send them back for somebody else to rework. Stuff like that. That's a job. And some people really like it. There are actually a lot of people once you get used to the idea of having a large staff that produces a lot of this code, it changes your relationship and how you think about it. Now, as individuals, we tend to put a lot of pride in the things that we build ourselves. There's this freight code ownership. It's kind of a bad thing, right? Like you, if you write code and you become so precious about it, it kind of speaks to whether or not you have standards in place, whether or not you frankly have the social skills to be able to work with other folks. That's the thing that we all kind of hate about corporate software engineering, which is having to deal with pulling all these pieces together. And you know, that guy three cubes down who writes terrible code, but we got to merge it, right? Stuff like that. Those are the kinds of questions that come up all the time for a dev manager. And the same thing, I think is a parallel with this, this AI stuff where how much do you do yourself versus how much do you delegate? How do you bring it in? How much time do you deal with integration challenges? All that kind of stuff that gives you a little bit of a sense of the economics for how all these pieces pull together. Now the jobs component of it does feel a little bit like manufacturing, where maybe we used to outsource things from the US to other countries for a lot of manufacturing. But then now we've decided to bring a lot of it back to the US, but we're doing it with a lot more automation. So what's the jobs ratio? How does that work? What are the cost structures look like? All those kinds of things. Those are all aspects of it. That model moving from being a line dev to a dev manager, what's really interesting is how that affects how I think through your own processes about how you approach problems and what's so wild to me is how often you forget that you even have these resources, especially if you're someone who's really comfortable at working with the tech, you're just used to doing something yourself quickly, right? But then you start to really feel it when you move outside of your comfort zone. So let me give you a real world example. I'm using DaVinci Resolve to edit this video, and I've been really enjoying working with Resolve lately and playing with it and figuring out how it works and all that stuff. And one of the things I wanted to do was I wanted to add in 3D models that can spin, right, that I make myself and I can add to the video. So I might do some more creative like animated tutorials or something with 3D in the future. But the way you do that with DaVinci Resolve is you set up a bunch of nodes. Now I was getting error messages where the node graph would turn red. And what was crazy was I could cut and paste screenshots of that node system in Resolve into chat GPT, and it was able to give me feedback on how to fix the error messages. Now, here's the thing, right? It felt really weird to do that because I'm not used to having a tool that works like that. But now that I have it, it's freaking great. That process of rethinking how I approach tasks and problems, it's part of an ongoing thing, which is where again, I come back to you, let's put some gunship music on and just kind of learn about how to do this stuff. It's possible that for the next few years, the LLMs and all this other tech will just get kind of incrementally better than it is right now. But what that might look like is the ratio of times that you put something in and it gives you good responses back goes up. But if you pull down a local LLM today or use chat GPT or a clot or whatever, you kind of get a sense of what it's going to look like. And if that's kind of the way it looks, it will both be really cool. It'll be very interesting. It's going to change how a lot of things work. Every piece of software that we use, obviously will get changed or remade to reflect this capability. But here's the thing that doesn't look like a social singularity or a civilizational remake. You know, is it on the level of the industrial revolution? That kind of thing, probably not. Just like adding in a graphical user interface for a computer or maybe it's on the part of the introduction of the Internet. It's huge, but things just kind of keep going, right? We change how work gets done. We move things around a bit. Maybe we produce a lot more code and a lot more material, but then we have to manage it differently. Right. Things like that. That's entirely possible. Is it going to hit some sort of crazy new super intelligence thing? I don't understand personally, even if you feed an AI every piece of information on the Internet or even add in a bunch of synthetic information, how you wind up with something that feels like a super intelligence. I mean, a really cool tool. And the phrase that I keep coming back to is every time you hear the word AI, replace it with just computer. And that starts to bring it down to earth a little bit more. But it's still cool, but it's not going to blow everything up. I don't know. I very quickly go from a conversation around AI and LLMs to then talking about, well, what's that going to mean for jobs and the economy? Or what does that mean for things like copyright and IP ownership? What does it mean for the concentration of power with larger corporations that are able to build these things, especially the really high end ones? What are the military implications? That gets into big, complicated questions and conversations about the future of work and jobs and the economy, which is inherently political. And by political, I mean, it even starts getting into basic questions of fairness. What is fair or decent about how all this stuff's going to go forward into the future? Let's say that the net effect of the AI and LLMs is to reduce the demand for white collar work, at least for the next few years, maybe decade or two. That in turn has huge implications for wages, benefits, the power that employees have to when they're talking with their employers, right? And that's what we're talking about. And that's what we're talking about. Where I personally think that this stuff goes from being kind of manageable to being potentially extraordinarily challenging is the robotic side of things. So today, all we're seeing are demos, mostly of LLM power, and I think that's a big conversation. And I think that's a big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, small, empowered robotics, but there's a lot of companies working on it right now. If those actually roll out and deploy, then yeah, we're looking at some big changes. For now, if you're just a software engineer who's watching this video trying to figure out what to do about it, first of all, the number one thing I would say is go ahead and install and play around with the tools. It's just like the internet feeling weird after people used AOL or if you're used to film cameras and you switched to digital or any other big transition, right? The LLMs are here to stay. And even if you played with them six months or a year ago and we're like, eh, whatever, it's time to come back and take a look at them. I've got links to some of the really big ones right now, things like a cursor and VO and stuff like that. If you haven't used them, you got to check them out. The other tip that I will pass along is that it's really important to not catastrophize things. There's plenty of stuff in the world to worry about. At some point, you just got to kind of take a breath and go outside and as they say, touch grass and just acknowledge that it is a big, complicated, strange place. And there's a lot of really bad things to worry about. And frankly, there's a lot of really wonderful things in the world, even with all this craziness going on. And so you just got to figure out how to pull all those pieces together and make your own life OK and be decent to everybody. Right. On that note, take care. Have a good one.