LLms are very good at memorizing static programs you are not increasing the I feel like you're using words like use for human children. If they problem they wouldn't say they memorized So I've got a million dollar prize pool to get to the 85% benchmark. If ARC survives OpenAI basically set back progress to this complete closing down of frontier essentially sucked the oxagen out of Today I have the pleasure to speak with at Google and creator of Keras. He's launching the co-founder of Zapier, whom we'll also be prize to solve the ARC benchmark that he created. you even need this prize? Why won't the biggest ARC is intended as a kind of IQ test for from most LLM benchmarks out there is that it's way LLMs work is that they're basically this their capabilities is by trying to cram as much By contrast, ARC does not require a lot require what's known as core knowledge. It's basic objectness, counting, that sort of any four-year-old or five-year-old possesses. is novel. It's something that you've probably the entire internet. That's what makes have not been doing very well on it. In fact, the discrete program search, program synthesis. glad that as a skeptic of LLM, you Is it accurate to say that if the biggest model then your view would be that we are on track to I'm pretty skeptical that we're going to if we do see it, you would also have to look the model on millions or billions of puzzles to have some overlap between the tasks that you at test time. You're still using memorization. good enough that it's going to be resistant never know. Maybe it could happen. I'm ARC is not a perfect benchmark. Maybe it has What would GPT-5 have to do so that you would This is what would make me change my mind about of cases where you show the model something novel from the perspective of its training This is true for LLMs but really this would catch I can see the ability to adapt to novelty on then I would be extremely interested. I The advantage they have is that they do get to much they are relying on that, but obviously They do have so much in distribution, to the whether an example is in distribution or not. then they can do everything that we can us. Why is it so crucial that it has to be out of the fact that they do get to see everything? between actual intelligence -- the ability for -- and pure memorization, like It's not just some semantic difference. The big everything that you might see at test time because fact that the space of possible tasks is infinite. seen zero percent of the total space. It's also This is why we, the human species, have developed such a thing as a distribution for the world would not need intelligence at all. In fact, do not have intelligence. Instead they have in their genes, behavioral programs that They can actually navigate their lives and their fit without needing to learn anything. enough, what would have happened is that evolution a hard-coded, static behavioral program. It have a hard-coded brain connectome. That's what we Instead, we have general intelligence. We are world. We are born with the ability to learn things that we've never seen before. That's really challenging to recreate in machines. to overlay some examples of what an ARC-like For people listening on audio, can you describe One ARC puzzle looks kind of like an IQ test input-output pairs. One pair is made up and the second grid shows you what you You get a couple pairs like this to demonstrate to do with your inputs. You then get a the corresponding test output. You look at the out what you're supposed to do. You show that Importantly, the knowledge basis you need knowledge. It includes basic concepts like topology, symmetries, etc. It's extremely knowledge. Any child possesses such knowledge. new. It's not something you'll find elsewhere on you have to approach every puzzle from You can't just fetch the response from memory. getting multimodal models that are trained to do on. Whereas not only humans but our ancestors evolution how to understand abstract physical and One view is that in the next are natively multimodal capability rather than of patterns because that's something we 'd see 100100 and is supposed to recognize a pattern of these numbers, they would have a challenge Why wouldn't multimodal models, which we're on the ARC-type spatial reasoning as soon as we get them? the answer within a few months. My response of symbols and are pretty small. If you flatten an something that's actually very difficult to parse. small. You only have 10 possible symbols, easy to flatten as sequences. Transformers, In fact, you can show that LLMs do fine fine-tuning an LLM on subsets of the tasks these tasks. You'll see that the LLM can encode seen before. It doesn't really have a problem The reason LLMs don't do well on ARC is new task is different from every other task. advance. You have to synthesize a new solution really what LLMs are struggling with. I just want to step back and explain why I'm Obviously there's the million dollar ARC Prize The Vesuvius Challenge was Nat Friedman's prize that were buried in the volcano. The winner to this podcast, Luke Farritor. Hopefully challenge intriguing and find a solution. bullish on LLMs. I've had discussions with explain the fact that LLMs don't seem to I found their explanations somewhat contrived. actually an intriguing fact that some of these humans to understand, yet the models struggle All of them are very easy for humans. Any on ARC. Even a five-year-old with very, very I agree that smart humans will do very human will probably be mediocre. with average humans. They scored about 85. right? I honestly don't know the demographic Imagining them interacting with Amazon's not the median human across the planet. spectrum in humans and humans obviously have where some people are relatively dumber. For example, there's Raven's Progressive Matrices. If you look at the kind of questions that are half of people will get it wrong -- we Humans have AGI but from relatively small these kinds of basic IQ test questions to talk about some of the previous performances Jack Cole with a 240 million parameter model this spectrum that clearly exists within humans, There's a bunch of interesting points here. There by Jack Cole that are doing quite well. They are at what's going on there. There are two things. you need to pre-train your LLM on millions compare that to a five-year-old child child has never done an IQ test before and has The only overlap between what they know core knowledge. It's knowing about counting, to do really well. They're going to do much better There's a second thing to note about the Jack to making the model work at all is test time that's really missing from LLM approaches right it's just doing static inference. The it and getting an answer. The model is not state is not adapting to the task at hand. every test problem, it's on-the-fly fine-tuning what's unlocking performance. If you don't do negligible. If you do test time fine-tuning you end up with interesting performance numbers. key limitations of LLMs today: the lack of inference to LLMs. That's working extremely There are so many interesting rabbit holes broader perspective that you need to They think that in addition to scaling, you need to get the System 2 working. Their perspective is that will be added atop the representations It's not just a technical detail. It's not It is the important part. The scale maximalists relationship that you can draw between how much performance you're getting on benchmark. how do you measure performance? What is it compute and more data? It's benchmark performance. detail. It's not an afterthought because it's you're asking. Accordingly, it's going to narrow If you look at the benchmarks we are using benchmarks. Sometimes they are like a school test. Even if you look at the if you look closely you realize that memorize a finite set of reasoning patterns. You LLMs are very good at memorizing small static of solution programs. When they can just fetch the appropriate program not really doing any sort of on-the-fly program You can actually solve all these benchmarks what you're scaling up here, they are big They're basically these big interpolative if you scale up the size of your database you are going to be increasing its performance That's kind of obvious. But as you're doing it, system one bit. You are increasing the skill of its scope of applicability, but not its That's the fundamental confusion that people run There are a lot of fascinating things interpolation. Let's talk about the point that input data. A reductionist way to talk about the other. But we don't care about the reductionist macroscopic level when these things combine. of the benchmarks. There's a benchmark that that a smart high schooler would be able to on it. Basically, they always nail it. Let's talk about what that means. &quot;30 students are in a class. One-fifth of them One-tenth are 11-year-olds. How many of I agree this is not rocket science. You can problem. A smart high school kid should be able reason through how to think about fractions, the different calculations to write the final answer. There are two definitions you can use. One is, It's the structure of the puzzle, which to identify the right template, which is in my run the program, and get the solution. You could Here's another definition of reasoning. don't already have a program in memory to solve a new program based on bits and pieces have to do on-the-fly program synthesis. That's the right memorized program and reapplying it. humans are so sample efficient. They also drill in these pathways of reasoning Let's take math, for example. It's the axioms of set theory and now they you have to teach them years of pre-algebra. and going through the same kind of problem in Isn't that like the same kind of thing? You can't You actually have to drill it. These models also Sure. In order to do on-the-fly program synthesis, Knowledge and memory are tremendously important vs. reasoning. In order to do But it sounds compatible with your story. Through these things can learn to reason within also see it within bigger and bigger models. math problem. Let's say a model that's all. As these models get bigger, they seem to It's not really a size issue. It's more Well, bigger models can pick up these kinds of good job of doing that even if you were to train suggest that as you have bigger and bigger models, more general ways of reasoning? But then isn't that intelligence? keep adding more knowledge and program templates skillful. You can apply it to more and more tasks. skill scaled up to many skills, because there General intelligence is the ability to approach it using very little data. This is what makes you This is the definition of generality. Generality apply your mind to anything at all, to arbitrary to adapt, to learn on the fly efficiently. and bigger models, you are gaining that capacity example. Your own company Google, in their paper They would give the model, in context, the grammar fewer than 200 living speakers. It's not in the and it basically is able to speak this language organic ways in which languages are structured. Spanish, I'm not going to be able to pick up how Spanish. Because of the representations that it is able to now learn a new language that this kind of pre-training actually does If you were right, LLMs would do really well on Each one of them requires very little knowledge. You don't need to think very hard about it. Even children can do them but LLMs cannot. Even do still cannot. The only thing that makes ARC to resist memorization. This is the only thing. If you look at LLMs closely, it's pretty obvious on the fly to solve the task that they're faced they've stored in memory. For instance, one solve a Caesar cipher, transposing letters to but it comes up quite a bit on the What's really interesting is that they can do it because those are very common numbers in try to do it with an arbitrary number like the generalized form of the algorithm, but only of the algorithm. If it could actually synthesize of n would not matter at all, because it I think this is true of humans as well. the time, of course, but humans are not limited to unique ability to adapt to new situations you to navigate every new day in your life. will perform very well within That's an excellent example because about memorization, chess memorization. question of why Gemini 1.5 was able, including the complex grammar structure? Doesn't I would assume that it has simply unimaginably vast training data. It has just reusing it. We know that LLMs have a very like this on the fly or even adapt existing Suppose there's a programmer at Google. They point are they doing something that 100% Suppose they were an LLM. What could they not their program? At what point do they have to use Forget about Google software developers. is full of novel things that they've not been based on memorization alone. It's impossible. just "memorization." It seems like you're saying curious about the kind of generalization they do. kind of generalization, you're going to fail What is the first point when you try to do that because you can't do the extreme generalization? been here in this room. Maybe you've been in of novelty. You've never been interviewing me. of every day in your life. By and large, it's If you just put an LLM in a robot, it could not be Take self-driving cars, for instance. You take you think you could just drop it in New York City left? No, it's going to fail. Not only can it not cannot even make it generalize to a new city. It I agree that self-driving cars aren't AGI. transformers as well. It's the same architecture. neurons in them, but they're less We can get into that. I still don't understand That's why education exists. That's why we had to We have a memory, but we are not a memory. I'm denying the premise that that's the only Suppose you just subbed out a remote work is the first point at which you realize How about I just send them an No, like part of their job. Is there a world in which all the programmers but they're only doing memorization-laden still producing a trillion dollars Software development is actually a pretty with novelty all the time. If you're I personally use generative AI very little I was also using Stack Overflow very copy-pasting stuff from Stack Overflow, or Personally, I try to focus on problem-solving. really important is problem-solving. The mental models and mental representations We have many people who can interact with these &quot;here's a specification of the kind of As long as there are many examples of this program will fetch the program for you from their memory. &quot;I need it to work on this If that were true, there would I agree we're not at a full AGI yet. These models brain has somewhere on the order of 10-30 trillion you're at least 10x underparameterized. I about why we're not on the spectrum. generalization they can't do. But it seems like see even within humans. Some humans would have based on the performance on Raven's I'm not a fan of IQ tests because, for and get better at them. They're very much main pitfall that ARC tries not to fall for. next five years. I mean at least the remote jobs like a salesperson, where you want the human In that world, would you say that that's do many things that definitely require things Sure. In five years, there will be more software I'm still not sure. I studied computer science. what would I be doing? I go to my job. My he realize I'm an LLM, if I were an LLM? were true that LLMs could generalize to novel to solve a problem they've never seen before -- If I look at how people are using LLMs they're using it as a Stack Overflow replacement. snippets to perform very common actions. What They don't actually need any of the abilities Let's step back on interpolation. Why in a higher dimension where -- if we're model can learn a more complex manifold? they're not zero-shotting new scientific ideas. They're trying to juxtapose them in their they try out some slightly different the experiment there in terms of It seems like a similar kind of thing to what generalization. Bigger and bigger models levels of generalization. GPT-2 couldn't required more generalization than it had Not quite. GPT-4 has a higher degree of the same degree of generalization. here. Why can't creativity just be Interpolation can absolutely be creative. To your do a lot of memorization, reciting, pattern very much a spectrum between pattern matching one end of the spectrum. They're never really They're usually doing some mixture of both. that seems very reasoning-heavy, like proving you're doing quite a bit of discrete actual reasoning. You're also very much guided by the shape of proofs that you've seen All of our thoughts, everything we do, is a thinking, Type 1 thinking, and Type 2 thinking. Because they have more reusable building new patterns in their training data. you keep getting bigger and bigger? patterns you're giving the model to learn are If you present something that's actually like an ARC puzzle for instance, it will fail. is a very useful intuition pump. Why can't this be The early layers are figuring out how to layers do this kind of program search, program the circuits in the model. They go from the representation near the middle of the model. concepts. What comes out the other end is the Possibly. Why not? But if these models were however simple, they should be able if you write down the solution program in Python, Humans can figure it out. Why can't LLMs do it? to you, suppose it's the case that in a Let's say it gets 80% or whatever the average Quite possibly, yes. Honestly, what I would like but after having only been trained But human kids, we're necessarily just Let me rephrase that. I want it to be only trained anticipate what's going to be in the ARC test set. a new type of intelligence test every single time? flawless benchmark, it would be impossible released more than four years ago and so far to some extent, passed the test Let's say you try to make by hand hundreds of them by programmatically generating variations. tasks. Just by brute forcing the task space, you're trained on and what's in the test set enough scale, you can always cheat. thing that supposedly requires intelligence, you can just brute force intelligence. distribution then sure, you could just brute are several metaphors for intelligence I like to a pathfinding algorithm in future situation space. development. You have a map, a 2D map, and you some fog of war on your map. There are areas that them. There are also areas that you've explored past. You don't know how they are like today. about the space of possible future situations connected to each other. Intelligence is a it will tell you how to get there optimally. Of have. It cannot pathfind in an area that you know If you had complete information about the problem by simply memorizing every possible You could solve the problem with pure memory. because you don't actually know what's going I feel like you're using words like use for human children. If your kid learns you wouldn't say they've memorized calculus. If you wouldn't say they've memorized Humans are never really doing pure That's only because you're semantically labeling memorization when the exact same skill is done by You can just plug in any sort of math problem. the LLM is doing. For instance, if you an algorithm. You're memorizing a program synthesizing on the fly the addition program. figure out how to do addition. A kid doesn't of set theory and going to how to do addition. My claim is that these models are vastly how many parameters, you have in the human brain. coming up with new theorems like the smartest What most humans do sounds like something similar memorizing skills or memorizing techniques that Tell me if this is wrong. Is it compatible gone but they're doing skills which we can record every single remote worker's screen. We there. Now we've trained a model that can do We're generating trillions of dollars of economic are we still in the memorization regime? anything as long as it's a static distribution, Are most jobs part of such a static distribution? can automate. LLMs are an excellent tool for automation is not the same as intelligence. I'm huge proponent of deep learning for many years. I've been saying that if you keep scaling up same time I've been saying if you keep scaling We can automate more and more things. Yes, this are many jobs you could automate away like You're still not going to have intelligence. generate all this economic value? Maybe we don't the moment you have to deal with As long as you're in a space that you can just rely on pure memorization. In fact, display arbitrary levels of skills on any task as long as it is possible to describe the When they do deal with novelty, No, interpolation is not enough If it were, then LLMs would be AGI. to figure out if we're on the path to AGI. The are on a spectrum and we're clearly covering I think so. that I think is evidence for this: grokking. there's a difference between the memorization they'll just memorize the data set. If to add digits. At some point, if you keep The fact that there is that distinction suggests learning can learn, there is a regime where it model. We don't have that in comparison to all Grokking is a very, very old phenomenon. basically an instance of the minimum description memorize a pointwise input-to-output It does not generalize at all, but it solves you can actually keep pruning it and making some point, it will start generalizing. description length principle. It's this idea the shortest. It doesn't mean that you're doing memorization plus regularization. Yeah, that absolutely leads to generalization. you see here of meta-learning is that it's perform many skills rather than one skill. This you get bigger and bigger in models, you of generalization. It generalizes to a skill, That's correct. LLMs are not infinitely of parameters. They have to compress their LLMs are mostly storing reusable bits of programs for compression, every time they're learning it in terms of existing bits and pieces of Isn't this generalization? degree of generalization. This is precisely Why is that intrinsically limited? At some generalization and a higher level, and then It's intrinsically limited because the substrate can do with this is local generalization. If even extreme generalization, you have to move to is discrete program search, program synthesis. of compare and contrast it with deep learning. parametric curve. In program synthesis, your got a set of logical operators, like a instances of it. You're structuring that into similar to a program you might write in Python or We're trying to automatically learn these models. descent. Gradient descent is very compute informative feedback signal about where the quickly, but it is very data inefficient. sampling of the operating space. You need Then you're limited to only generalizing within this limitation is because your model is a curve. the learning engine is combinatorial search. you find one that actually meets your spec. This a generalizable program from just one example, by the way. The big limitation is that it's running into combinatorial explosion, of course. discrete program search have very complementary limitation of deep learning has a corresponding path forward is going to be to merge the two. These parametric curves trained with gradient System 1-type thinking: pattern recognition, search is a great fit for Type 2 thinking: out a generalizable model that matches just one or Humans are never doing pure System 1 or pure both. Right now, we have all the tools for 2. The way forward is to create a hybrid system. The outer structure is going to be a discrete fundamental limitation of discrete program with deep learning. You're going to leverage in program space, to guide the program search. playing chess or when you're trying to prove a thing, but you start out with some intuition something you can get via a deep learning model. machines. They're pattern matching machines. and then you're going to do actual explicit to do it via brute force. You're not going to ask another deep learning model for suggestions. Here's where in the graph you should be going." for feedback like "well, here's what I have so and try something new?" Discrete program search it dramatically better, orders of magnitude By the way, another thing that you can use common sense knowledge and knowledge in general. where you have this on-the-fly synthesis The way it adapts is that it's going to fetch themselves curves, differentiable modules, in nature. It's going to assemble them every new situation you might be faced with, that was synthesized using very, very little That's actually a really interesting prompt. friends who are extremely optimistic about LLMs In some sense, they also agree that scaling progress is undergirded and enabled by scaling. test time compute on top of these models. straightforward to do that because you you built up from pre-training. It's almost You need some more deliberate way in which it learning is extremely sample efficient. you need the model to talk through the things As far as the System 2 goes, they talk about encouraged to proceed on the reasoning traces relatively straightforward stuff that will That's an empirical question so we'll see. that. I'm curious why. whole System 2 architecture is the hard part. up the interpolative memory is the easy part. It's data. It's an interpolative representation The hard part is the architecture of are separate components. We have the memory. with you that having the memory is actually but it was not hooked up to an extensive memory, have enough material to work from. advanced an alternative hypothesis that memory. When Sherlock Holmes goes into a crime just look at a few clues and figure out who was has learned higher level associations. Here's one way to ask the question. In the but it is just synapses connected to each you just query the right circuit, right? Training in the environment that human ancestors If you train on the same kinds of outputs requires these kinds of circuits -- wouldn't that It's a matter of degree. If you have a system local generalization from that, it's not going you need the memory plus the ability achieve broader and even extreme generalization. the founder of developmental psychology. He had &quot;intelligence is what you use when you don't in most situations you already know situation before. You already have the answer. when you're faced with novelty, with something weren't prepared for, either by your own life day that you're living right now is every day you've lived before. It's also of your ancestors. You're still capable I'm not denying that generalization is intelligence. That's not the crux. The crux is Okay, let me ask a separate question about the Maybe because of the reasons you mentioned, well. But clearly there's differences in What is your explanation for what's going my story. There's a spectrum of generality and Even some humans haven't even climbed up to That's a great question. There is extensive mostly genetic in nature. That means that if you is no amount of training data you can expose that This points to the fact that you really need a More training data is not in fact all you need. this way. The people who are smarter have, in ML the neural wiring, it's more efficient. Some part of the story is scaling. There is some Within the context of "scaling" LLMs, people like Gemini 1.5 Flash performs as well as but is 57 times cheaper on output. Part like extremely low-hanging fruit territory when We're back now with the co-founder of Zapier, you're running this prize with Francois. Tell you guys to launch this prize? years. I co-founded Zapier and I've I first got introduced to your work during lot of free time. It was right after you'd Intelligence". You introduced the concept of AGI is the right definition, and the ARC puzzles. done yet. It was still running. It was interesting fry at Zapier. We were in the middle of this big It was January 2022 when the chain-of-thought progress. I even gave a whole presentation I had priced in everything that LLMs could terms of all these latent capabilities that I actually gave up my exec team role. I was back to being an individual contributor and just Ultimately, that led me back towards ARC. I was saturation effect that MMLU and GMS8K have. over the last four years, I was really shocked to towards it. It felt like a really important eval. in my network and community, very few people is a really globally, singularly unique AGI eval exists that more narrowly measures AI skill -- I had my own ideas on how to beat ARC as well. I to meet Francois earlier this year to quiz him and people didn't know about ARC? You should actually Why don't you think more people know about ARC? community are benchmarks that are already some research group is going to make some to catch the attention of everyone else. people trying to beat the first team and so on. is actually very hard for existing AI techniques. much the point. The point is not that you should solve ARC. The point is that existing technology that and start being able to tackle problems you need to try new ideas. this sort of measure of how close we are to AGI. want researchers to look at these puzzles and be are so simple and most humans can just do them systems? Why is it so hard for LLMs and so on?&quot; released before LLMs were really a thing. The only was designed to be resistant to memorization. and GenAI in general, shows that it This is what nerd-sniped me. I went and took a all my friends and family too. They're all like, can't solve this?&quot; That's the reaction and the you realize there's not just empirical evidence but there are theoretical concepts point that new ideas are needed to beat ARC. that are actually working against that happening. right now. One of the trends is the closing up from OpenAI had no technical detail shared. The like the longer context part of that work. sharing is what got us to transformers in the first place. So it's actually a little bit gone closed. It's really making a bet that these the breakthrough and not the ecosystem. The the most powerful innovation ecosystem that's It's actually really sad that frontier research four years ago, everything was just openly were published. This is no longer the case. basically set back progress towards AGI by quite two reasons. One is that they caused this complete But they also triggered this initial burst the oxygen out of the room. Everyone is just on the path to AGI actually. All these new of everything else they could be going to. 2015 or 2016, there were like a thousand times progress was higher because people were exploring You could just go and try. You could have a cool results. There was this energy. Now everyone is The big labs also tried their hand on ARC, but anything. People only publish positive results. into trying to prompt or scaffold, do some models to produce good solutions on ARC. I mean ago. A lot of post-training has gone into making I hope that one of the things this episode does They have to put in an open source model to capability is latent in Claude and just see if you Let's talk about the prize. How much do whatever percent on ARC. How much do you get if We have a little over a million dollars in the basis. We're starting today through the middle of bound of the human average that you guys talked first team that can get to the 85% benchmark. of the early statisticians at Zapier gave me this takes, the longer it takes.&quot; My prior is that ARC to break down and do a progress prize this year. pay out to the top scores. $50,000 is going to Kaggle leaderboard. We're hosting it on Kaggle. the best paper that explains conceptually One of the interesting things is we're also going money, you put the solution or your paper out you see a lot of closed-up sharing. People are alpha to themselves during the contest period. we want an interactive game here. The plan is $100,000 prize money to the top progress prize. February to share out all the knowledge from taking. That way we'll re-baseline the community run the contest again next year. We'll keep I'll give people some context on why I think conversations with my friends who are very much all, it was intriguing to me that they didn't know This happened a couple nights ago. We went to They said, &quot;of course, an LLM would be took a screenshot of it. We just put it into So it's very interesting. It is a notable you on these kinds of questions but this is a interesting because we're going to learn With regards to the 85%, separate from this replicate that result. Obviously in psychology and be analogous to, when you run tests on some small I'd be very curious to know, if you try to perform on ARC? I'm also curious about the this benchmark. It's very interesting thinking of like MMLU and MATH. Dan Hendrycks and Collin students or college students when they made it. ago was that it would be a test of AGI. Of course that these are tests of memorization. But Epoch AI has a very interesting graph where you 10%, 30%, 40% as you increase the compute In the GPT-4 technical report, they had this which was 22 coding problems. They had to graph it or even with smaller models, they can have It takes a lot of reliability to make sure they really want to upweight the signal where they 1/100 or 1/1000. They go from 1/1000 to 1/100 Here's the question this is all leading up to. Why to try really hard with bigger models. Now they Cole has figured out that can get 35% with Shouldn't we see the same pattern we saw eke out and then once you get the general That's an empirical question. We'll see in actually very unique. It's not just pre-training to do active inference. right? He's doing test-time fine-tuning. actually trying to lift one of the key limitations anything new. They cannot adapt on the fly to what What he's doing is effectively a form of program blocks, programming building blocks. By you are trying to assemble these building blocks is exactly what program synthesis is about. program search. In discrete program search, set of primitives. You have very few primitives. search on ARC tend to work with DSLs It's a very small DSL but they're trying to programs. There's a very deep depth of search. with LLMs. He's got this vector program database They're mined by pre-training the LLM, not just millions of generated ARC-like tasks. You have an very shallow recombination of these primitives. recombination with a very small set of primitive complete opposite end of that spectrum. You and you're doing very shallow search. They are the I think where you're going to get the most in between. You want to leverage memorization to programs. You don't want them to be hard-coded them to be learned from examples. You also want you're only doing very shallow search, you are to generalize further and more broadly, I might argue that the reason that he had was because he used a 240 million parameter required him to use a P100 GPU which has like For context for the listeners, the frontier models that. For your competition, submissions and have to run on NVIDIA Tesla P100. There's basically a 12 hour runtime limit. There's But here's the thing, you only have 100 test task is actually quite a bit, especially if you Basically, it would be 7 minutes per task. People flops does a human brain have. You can take them it's basically the amount of flops an H100 has. solve this question in faster than 7.2 minutes. able to do it in seven minutes. Obviously we have brain and these 29 GB or whatever in the H100. way to also test this prize with some sort of test whether scaling is the path to solving ARC. we want to see how much progress we can do with it's a super interesting open question, what could We actually also want to make available a private, can put on it any model you want. You can take fine-tune it, do whatever you want, and just for 24 hours or something. You see what you get. test sets. There is a public test set that's in to train. You can put in an open API call, private test set, which is the hundred that It is pretty open-ended and interesting to have set and go try it. Now there is an asterisk public test set because it is public. It could This is actually what people are already doing. models, like the latest Gemini or the latest Again, the problem is that these tasks These models are also trained on GitHub. That kind of creates uncertainty. If they is that because they memorized the answer or not? your own private, ARC-like very novel test set. complex. Make them very obvious for humans, as possible. Make them unique, different, and There have been tests on whether these models Scale recently did this with GSM8K. but with different questions. Some of the models like Mistral and so forth. Frontier models like benchmark as they did on the specific questions I would be relatively optimistic about them Mike that you should allow API access but these ARC questions. So you allow API access to enter into this contest. Maybe later on it performs worse than the test set that that means that OpenAI is training on your API &quot;oh my god, they've leaked your data.&quot; That is a goal that we want to do. Yeah, ARC is not a perfect benchmark. I made was in a time before LLMs. We've actually flaws there might be. There is some redundancy the goals of the benchmark. Every task is not quite true. Every task is also supposed not be. They might be structurally similar to So we want to keep iterating and release an ARC we're going to want to make the old private it publicly, but what we could do is just get a task, and submit a solution. Of course you Because you actually have to query this API, accidentally train on this data. It's which is literally on GitHub. There's actually no it. They are because they train on GitHub. we would avoid this issue. For people who want using whatever resources they want, that I wonder what might happen. I'm not sure. whole new algorithm for AI with some explicit Another is that they did something hacky with the which reveals that maybe intelligence is of the distribution. Then it can reason. Maybe that'll indicate that you had to do they get better you won't have to do something if these multimodal models will natively If ARC survives three months from here, we'll up moment of contact with reality by blowing up against it. We're going to learn really quickly Again, I think new ideas are needed. Anyone I'd encourage everyone to give it a try. argument that we've stalled out in progress Yeah, that's the point of having a money them to try to solve it. If there's an easy the benchmark is flawed. You're going to know original Kaggle competition for ARC back in I had released this dataset and I wanted to There was a small money prize at the time. It time as GPT-3 was released. People of course zero. What the first contest taught us is that money. There's going to be more people looking to see if the benchmark is going to survive. not like trying to brute force the space of knowledge. I don't think it's necessarily going going to be a huge milestone on the way to AGI. a problem-solving program from just two or three It's an entirely new paradigm for software potentially quite complex programs that programming them by coming up with the shape of you're actually just showing the computer what it out. That's what is extremely powerful. of solutions might be possible here, the purpose of ARC vs. which are valid. and Buck stayed up last night because &quot;oh, of course LLMs can solve this.&quot; They were trying to prompt Claude Opus on this and they did was have other examples of some of the of why you went from one output to another I think there was also expressing the JSON in Another thing was using the code interpreter. getting better as these models get smarter, is they were able to do was get the actual through the code interpreter, like "write the Do you think that the program synthesis will just look like using the code I think whatever solution we see that need to leverage some aspects from deep We've shown already that LLMs can do quite We've also shown that pure discrete program search this was the state of the art. In the state of the art and there's no deep We have two approaches that have basically no very much at two opposite ends of one extremely large banks of millions of vector simplistic recombination. On the other end, you but very deep, very sophisticated program search. The people who are going to be winning the ARC near-term AGI are going to be those that manage to program search paradigm into one elegant way. would be cheating. If you want to add a code That's legitimate. The part that would be cheating like brute force the space of possible tasks and the fact that you're generating so many there's going to be some overlap between what That's defeating the purpose of the benchmark you need to adapt just by fetching a memorized no benchmark is perfect. Maybe there's a way to Although some amount of fine tuning is valid models to compete here and they're be able to think in the ARC-type way. ARC-like core knowledge, into the model but to do this. Core knowledge is extremely basic. I actually do think they rely a little bit example, something bounces off a wall and comes games and I've seen Pong or something. and people's intelligence, as measured on on these kinds of questions. It's probably a actually see these sorts of patterns in So I don't think this is core knowledge. This humans have as they grow up, seeing and trying to pattern match to them. knowledge. Core knowledge includes or trajectories. That would be included. as a human, you're able to quickly figure out building blocks, this set of patterns, Is core knowledge required to attain does the core knowledge have to be, in core knowledge be learned through intelligence? humans, some amount of core knowledge is something a small amount of knowledge about the world But most core knowledge is acquired through it's not going to be acquired in school for in the first 3-4 years of your life. By you're going to need as an adult. I'm super excited to see the open source versions, people can score in the competition itself. scaling hypothesis and I'm very curious if You won't be able to submit that to this see if people can crack that and get ARC working It's really going to be motivating. We're going reproducible open source version in the public ARC eval, we're going to still keep the prize the public reproducible version out there. towards AGI. A key part of that is that any need to be public, so everyone can know about it there's no progress. curious about is disaggregating the this or is this just possible with based on the public and the private version. this. We're gonna learn a lot about what the showed up and said, "hey, here's a closed source probably update us. We'd think, "okay, perhaps we on the private test set in order to balance." arbitrary in order to learn about what Both of us are committed to evolving or the closest to perfect as we can get it the prize and maybe try their hand at it? It goes live today. One million Thank you guys for coming on the podcast. intelligence and get a different perspective and Thank you for helping break the news.