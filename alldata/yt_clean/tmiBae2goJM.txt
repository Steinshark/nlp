Anthropic just released a new retrieval Based on their results, it's the when combined with re ranking, it I'll call it more of a chunking But the results are impressive. To understand this, let's you have your documents, you chunk them chunks, you compute embeddings, and those At runtime, or inference time, the embedding for those questions, and based on the embedding similarity. Those chunks along with the query are This technique is great for are still a lot of failure points. That's why a lot of people combine keyword based search mechanisms. One of such techniques is BM25, which plays a critical role if the user Here is an example from Anthropics. Suppose a user queries error code TS An embedding model or semantic search general, but could miss the exact TS 999. However, the keyword based search looks for specific text string to And it has a higher probability of The biggest problem with the current lose a lot of contextual information. When the chunks are returned based on doesn't really have any information here is an example, Imagine you had a collection of financial those are embedded in your knowledge base. the user query is what was the revenue Now, the relevant chunk to this might percent over the previous quarter. However, this chunk on its own does not to or the relevant time period. So it's going to be very difficult, keyword based search mechanism Especially if there are multiple that is why Antropic is recommending So when you are creating chunks, information in each chunk. There are some issues associated going to discuss later in the video. But instead of this chunk, your new chunk This chunk is from an filing on Acme's The previous quarter revenue was that your chunking strategy created. As you can see, they recommend to both to the embedding model as So how do you implement this? Well, they suggest using, they have provided a prompt for and the most cost effective model. This is a general prompt that you want to So in the first part of the, prompt, let's say you were working with a PDF you will provide that PDF file here. Then the rest of the prompt situate within the whole document. And then you provide the specific After that, you ask Haiku. to give you a short context to situate document for the purpose of improving So Haiku is supposed to add contextual And as a result, you will usually add Now, in practice, this is You will take a single PDF file or then convert that into chunks. After that, you take one chunk at a that I just showed you, along with the contextual information for each chunk. You combine, given chunk with the you pass it through a embedding model. And those embeddings are going to be On the other hand, we also In this case, we're using TF IDF, Inverse Document Frequency. this is basically the keyword as a result, in each of the chunks, I think you can already see some one of them is that it's going to add a tokens that you're adding to each chunk, that adds up a lot of different tokens. But you can use the prompt caching by Anthropic a few months ago. Gemini also has a very similar feature. With the help of this you will So here are a quick calculations 000 token documents, 50 token context context per chunk one time cost to is about 10, 000 tokens of context contextualized chunk chunks is about 1. 02 per million, document tokens. So it's a relatively, small price, up if you have millions of documents. The rest of the retrieval pipeline is the And that's why I think it's more completely new, retrieval mechanism. So after doing all this, what type of Now, the Anthropic team performed a benchmarks on a number of different, the summarized results are here. contextual embedding reduced the by 35 percent So it went from a 5. 7 to 3. 7, which is a huge improvement. if you combine the contextual embeddings contextual BM25, which is the keyword chunks, you can further reduce the So again, you went from 5. 7 to 2. 9, there are other approaches that you can in my advanced, RAG Beyond Basics course. In this specific experiment, they used a according to their experimentations, on the models they have tested. based on the results, it's Now, they also added a re ranker they saw further improvements. My recommendation is to have a keyword some sort of query rewriter plus a Based on my experience, adding these substantial boost in retrieval accuracy. With the addition of re ranker retrieval error rate went from 5. 7 to 1. 9 percent. Some of the things that they you're building RAG systems. chunking boundaries or chunking strategy. It's very application dependent. So you want to look at your chunk I have a couple of different I'll put a link to those Another thing to consider So most of the people use densing embedding models or a number of In their experiments, they embeddings, seems to be pretty I also highly recommend to look multi vector representations. In theory, those should be more effective I have quite a few videos discussing those videos in the video description. another thing is the custom we looked at a very general prompt depending on the, documents you customize your contextualizer prompt. And the last thing is the number they experimented with 20 chunks, and I would suspect if they increase might see some improvement as well. in general, you want to first during the retrieval process. then you want to add the re ranking step relevant chunks that are going to be Another thing to keep in mind their accuracy or error rate. So they're retrieving 20 different the relevant chunk to a specific query So they're not really ranking it, basically saying if it's present in the going to consider this as a success. Now this is a standard approach, but I confuse this by assuming that the return I'm happy that they tried to address So they say sometimes the simplest base is smaller than 200, 000 tokens, You can just include the entire knowledge with no need for rag or similar methods. But keep in mind, this means that be sending 500 pages of material. to the LLM, the cost And that's why they recommend using And if you use prompt caching, And the latency is going to be reduced I have a video on how this prompt caching Very important if you're building anything tokens are more than 200, 000 tokens, of the current generation of Claude Let's look at a code example that Claude has provided. Now here is a notebook, in their own repo. I'll put a link to this specific repo. They are comparing basic RAG with BM25 and adding re ranking on top of it. The repo contains the data that is used. So you can basically So here they are installing all the For this one, they're using the voyage they are using the coherent API. So here's how they're Okay. So in the first step, they compute chunks and then they do basic rag. So they have, a code actually runs Now, based on the results for, top five, relevant chunk to be retrieved in the If you extend it to top 10, then 87%. And if you extend it to top standard RAG system is about 90%. Now, if you use the contextualized provided this code snippet. Which basically takes each of the that chunk is coming and then run this in the beginning of the video This They're using the haiku model and are going to be a lot more than what Now they run the same embedding model And for top five, it sees an 10, I think it's again about, 5%. For top 20, the improvement is about 3%. Now they are creating a This is basically adding a And this does, I think It's probably about 1 percent improvement. But if you add re ranking about 2 percent improvement. which is pretty huge going Anyways, this was a quick Contextual retrieval is actually Anthropic are taking RAG seriously. This shows that RAG is still relevant and based on my experience is one of the LLMs at the moment. So if you're interested in learning make sure to subscribe to the channel. I create a lot of content Let me know if you want me to on contextual RAG and how this can I hope you found this video useful. Thanks for watching. as always, see you in the next one.