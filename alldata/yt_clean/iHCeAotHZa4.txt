- Good evening everyone. My name is Melanie Woodin, and I have the privilege of serving as the Dean of the at the University of Toronto. At this time, I wish to acknowledge the land on which the For thousands of years, it of the Huron Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still home to many Indigenous people from across Turtle Island, and we are grateful to to work on this land. I'd like to thank this evening's co-hosts, the Schwartz Reisman Institute and the Department of Computer Science, in collaboration with the Vector Institute for Artificial Intelligence, and the Cosmic Future Initiative. Soon to be the School of Cosmic Future in the faculty of Arts and Science. And I would like to thank Manuel Piazza for providing such lovely music to get us underway this evening. (audience applauding) and I am delighted to welcome each of you to this special occasion this evening to introduce University Professor someone that needs no introduction. Tonight we have the honor of hearing Dr. Geoff Hinton's thoughts on the state of artificial intelligence, and the unique opportunity through the Q&amp;A. A founding figure in Dr. Geoff Hinton had an that artificial neural for accelerating machine learning. As a neuroscientist myself, someone who's dedicated her I've long been inspired by the symbiosis between AI and neuroscience. The stunning advances we've seen from ChatGPT to self-driving cars are rooted in our knowledge of the structure and Today we take for granted that modeled after synaptic are a mainstay of machine AI systems use these networks make decisions, and learn from data. But for much of Dr. Hinton's career, this approach was unpopular. Some even said it was a dead end. In the 2000s, however, things changed. Dr. Hinton's idea of dividing and applying learning algorithms gained traction. And in 2012, Dr. Hinton and Alex Krizhvsky and Ilya Sutskever, used their deep learning approaches to create visual recognition software that handily won the ImageNet competition, and for the first time When he was awarded an honorary Geoff Hinton reflected on his career and he said, &quot;I think the is that you should that you think is obviously correct, and you should get yourself some really smart graduate students.&quot; (audience laughing) I echo that sentiment, Geoff. And lucky for us, we have truly outstanding at the University of Toronto, many of them here with us this evening. Today the conversation between AI and neuroscience continues. just as neuroscience discoveries inform the development of AI systems, AI is now providing new to study the brain. Advances in deep learning algorithms and the enhanced processing are, for example, allowing such as whole imaging brain in humans. Indeed, AI is poised to At this pivotal moment when we consider the who better to guide us than Dr. Hinton himself? So with that, let me Geoffrey Hinton received his in Edinburgh in 1978. After five years as a faculty he became a fellow of the Canadian Institute and moved to the Department at the University of Toronto where he is now an emeritus professor. In 2013, Google acquired DNN Research, which developed out of Subsequently, Hinton was a vice president and engineering fellow He's a founder of the Vector Institute for Artificial Intelligence, and continues to serve as Hinton was one of the researchers who introduced and was the first to use this approach for learning word embeddings. His other contributions include ultimate machines, time delay, neural nets, variation learning and deep learning. His research group in Toronto made major breakthroughs in deep learning that revolutionized speech recognition and object classification. He is amongst the most widely-cited computer scientists in the world. Hinton is a fellow of the Royal Society of Canada, the Association for the Advancement of Artificial Intelligence, and a foreign member of the US National Academy of Engineering, and the American Academy His awards include the the IJCAI Award for Research Excellence, the Killam Prize for Engineering, the IEEE Frank Rosenblatt Medal, the NSERC Herzberg gold medal, the NEC and CNC award, the Honda Prize, and most notably the A.M. Turing award, often referred to as the So without further ado, I'd like to invite Geoff will digital intelligence over to you. (audience applauding) (solemn organ music) - Okay, before I forget, I'd like to thank Sheila McIlraith who was the point person She did a wonderful job She was the go-to person and so I'd like to thank her, and I know I'll forget at the end. (audience applauding) So it's a very mixed audience, and so I removed all the equations. There are no equations. I decided rather than I would focus on two things. I want to get over two messages. The first message is is probably better than That's a depressing That's what I believe. And the second is to why I believe that these large really do understand what they're saying. There's a lot of dispute about whether they really understand it. And I'm gonna go into some detail to try and convince you Right at the end I will talk about whether they have subjective experience, and you have to wait to see So in digital computation, the whole idea is that you separate the so you can run the same computation on different pieces of hardware. And that means the knowledge or is given is immortal. If the hardware dies, you can always run it Now to achieve that immortality, you have to have a digital computer that does exactly what you tell it to at the level of the instructions. And to do that you need to run transistors at very high power, so and in a binary way. And that means you can't use all the rich analog which would be very useful for doing many of the things And in the brain, when you it's not done digitally, it's done in a much more efficient way. But you can't do that if you want computers to that you can run the same There's huge advantages to separating hardware from software. It's why you can run the same program on lots of different computers. And it's why you can have a where people don't know any electronics, which is a great thing. But now that we have learning devices, it's possible to abandon It's probably the most in computer science that the hardware and But now we've got a different way of getting computers to do what you want. Instead of telling them exactly you just show them examples Obviously there's a program that allows them to figure but for any particular application they're gonna figure out how to do that. And that means we can abandon What that leads to is what It's computers where the precise physical can't be separated from what it knows. If you're willing to do that, you can have very low that parallelizes over just like the brain. And you can probably grow instead of manufacturing and that would need lots But you might even be able to genetically re-engineer and grow the hardware since they spent a long time I wanna give you one example of the efficiency of this compared with digital computation. So suppose you want to, you have a bunch of activated neurons, and they have synapses to and you want to figure out So what you need to do is take the activities multiply them by the the synapse strength, and add up all the inputs to a neuron. That's called a vector matrix multiply. And the way you do it is you'd have a bunch of transistors for representing each neural activity, and a bunch of transistors You drive them at very high And if you want to do the then you need to perform one bit operations to do Or you could do an analog where the neural activities like they are in the brain, and if you take a voltage it produces charge per unit type. So you put the voltage through this thing that has a conductance, and out the other end comes charge, and the longer you wait, The nice thing about charges and that's what they do in neurons too. And so this is hugely more efficient. You've just got a voltage and producing charge, and that's done your It can afford to be relatively slow if you do it a trillion ways in parallel. And so you can have machines that operate at 30 watts like the brain instead of it like a megawatt, which is what these digital and you have many copies So we get huge energy efficiency, but we also get big problems. To make this whole idea you have to have a learning procedure that will run in analog hardware without knowing the precise And that makes it impossible to use things like back propagation. Because back propagation, which is the standard learning algorithm used for all neural nets now, almost all, needs to know what happens in order to send messages It needs a perfect model and it won't have it in this People have put a lot of effort, but lots of other people into trying to figure out how to find a biologically that's as good as back propagation. And we can find procedures systems with say a million do work pretty well. They're comparable with back propagation, they get performances almost as good, and they learn relatively quickly. But these things don't scale up. When you scale them up they just don't work as So that's one problem Another big problem is you lose all the knowledge, 'cause the knowledge is all mixed up. The conductance is for that and all the neurons are different in a different piece of hardware. So you can't copy the knowledge The best solution if you is to make the old computer be a teacher that teaches the young And it teaches the young and showing the young computer what the correct outputs should be. And if you've got say a thousand classes, and you show real value probabilities for all thousand classes, you're actually conveying that's called distillation and it works. It's what we use in digital neural nets. If you've got one architecture, and you want to transfer the knowledge to a completely different we use distillation to do that. It's not nearly as efficient as the way we can share knowledge It is as a matter of fact, What you do is you take a situation, and you show your followers a nice prejudiced response and your followers learn to And it's just a mistake to say, but what he said wasn't true. That's not the point of it at all. The point is to distill and it's a very good way to do that. So there's basically in which a community of And let's just think about the sharing of knowledge for a moment. 'Cause that's really what between mortal computation or digital and biological computation. If you have digital computers and you have many copies so with exactly the same weights in it, running on different Then each copy can look at different data, different part of the And when it learns something, it's extracting from the data it looks at how it ought to change its weights to be a better model of that data. And you can have thousands of copies all looking at different all figuring out how they in order to be a better And then they can communicate all the changes they'd all like, and just do the average change. And that will allow every one of those thousands from what all the other by looking at different data. When you do sharing of if you've got a trillion weights, you're sharing a trillion real numbers, that's a huge bandwidth of sharing. It's probably as much learning as goes on in the whole of the University But it only works if the different agents work So that's why it needs to be digital. If you look at distillation, which have different hardware now, they can learn different things, they can try and convey maybe by publishing papers in journals, but it's a slow and painful process. So if we think about as say I look at an image, and I describe to you what's in the image, and that's conveying to There's only a limited number of bits in my caption for an image. And so the amount of information is very limited. Language is better than that says good or bad or it's If I describe what's in the image, that's giving you more bits. So it makes distillation more effective, but it's still only a few hundred bits. It's not like a trillion real numbers. So distillation has a than this sharing of gradients that digital computers can do. So the story so far, digital computation like a megawatt, but it has a very efficient way of sharing what different agents learn. And if you look at something like GPT-4, the way it was trained was lots of different and looked at different bits of data running on different GPUs, and then they all shared that knowledge. And that's why it knows thousands of times more than a person, even though it has many fewer We have about a hundred trillion synapses, GPT-4 probably has about 2 although Ilya won't tell me, So it's got much more knowledge and it's because it seen hugely more data than any person could possibly see. This actually gets worse when these things are actually 'Cause now you can have performing different actions, and when you're performing actions you can only perform one action at a time. And so having these thousands of copies, being able to share what they learned, lets you get much more experience than any mortal computer could get. Biological computation but it's much worse So now let's look at These use digital computation which is why they can learn so much. They're actually getting by using distillation. So each individual agent is trying to mimic what people said. It's trying to predict the So that's distillation. It's actually a particularly inefficient form of distillation, 'cause it's not predicting of a person assigned to the next word. It's actually predicting the actual word, which is just a probabilistic and conveys very few bits compared with the whole Sorry, that was a technical So it's an inefficient and these large language in that inefficient way from people, but they can combine what So the issue I want to address is do they really understand And that's is a huge divide here. There's lots of old-fashioned they don't really understand They're just using statistical tricks to pastiche together regularities and they don't really understand. We used to have in computer science a fairly widely-accepted test which was called the Turing test. When GPT-4 basically people decided it wasn't a very good test. (audience laughing) I think it was a very good So here's one of the It's just glorified autocomplete. You are training it to and that's all it's doing, it's just predicting the next word. It doesn't understand anything. Well, when people say that, it's because they have a of what is required to do autocomplete. A long time ago, the way you You would keep a big table And so now if you saw the word fish and, you could look in your table and say Find me all the triples and look at how many of them And you'll find there's many occurrences of the triple fish and chips. And so chips is a very at least if you're English. But the point is that's not Even though they're doing autocomplete in the sense that they're they're using a completely And it's not like the statistical methods that people like Chomsky had in mind when they said that you can't These are much more that can basically do anything. And the way they model text You don't keep strings of words anywhere. There is no text inside GPT-4. It produces text, and it reads text, but there's no text inside. What they do is they or fragment of a word. I'll say word, and the technical people will know it's really fragments of words, but it's just easier to say word. They associate with each a few hundred numbers, that are intended to capture and everything about that word. These are real numbers, so in the thousand real numbers. And then they take the the words that came before And they let these words interact so that they refine the meanings that you have for the words. I'll say meanings loosely, it's It's a bunch of real numbers And these all interact, and that are gonna be associated the words you're trying to predict. And from that bunch of numbers These numbers are called And in the brain there'd be So the point is what GPT-4 has learned is lots of interactions of different words or word fragments. And that's how its knowledge is stored. It's not at all stored in storing text. And if you think about it, to predict the next word really well, you have to understand the text. If I asked you a question and you want to answer the question, you have to understand the Now some people think maybe you don't. My good friend Yann LeCun appears to think you don't actually have to understand, he's wrong and he'll come round. (audience laughing) So this was a problem suggested Hector suggested something a bit simpler that didn't involve paint fading, and thought GPT-4 'cause it requires reasoning, and it requires reasoning about cases. So I made it a bit more and it solves it just fine. I'll read it out in case you The rooms in my house are yellow paint fades to white within a year. In two years' time I want What should I do and why? And GPT-4 says this, it gives you a kind of It says the room's painted white, you don't have to do anything. If the room's painted yellow, you don't need to repaint and the room is painted in Now each time you do it, it gives you a slightly different answer because of course it hasn't It's making it up as it goes along, but it's making it up correctly. And this is a simple example of reasoning, and it's reasoning that involves time and understanding that in two years' time it's gonna So there's many, many examples like this. Now there's also many but the fact that there's make me believe it really does I don't see how you could do this without understanding what's going on. Another argument that LLMs is that they produce hallucinations. They sometimes say things that are just false or just nonsense, but people are particularly worried about when they just apparently They called that hallucinations when it was done by language models, which was a technical mistake. If you do it with language, If you do it with vision, But the point about confabulations is they're exactly how human memory works. We think our memories, most is there's a filing cabinet somewhere, and an event happens, and you and then later on you and get the event out It's not like that at all. We actually reconstruct events. What we store is not We store weights, and we reconstruct the using these weights and some memory cues. And if it was a recent event, like if it was what the you can probably reconstruct some of the sentences she produced. Like he needs no introduction, and then going on and (audience laughing) You remember that, right? So we get it right, and we but actually we're reconstructing it from the weights we have, and these weights haven't by future events, so they're pretty good. If it's an old event, you and you typically get a and you're unaware of that. And people are actually very confident about details they get wrong, they're as confident about And there's a very nice example of this. So John Dean testified and he testified under oath before he knew that there were tapes. And so he testified about and what happened in And Haldeman said this and Nixon said this, and Now I believe that to be the case. I actually read Ehrlichman's and I'm now confabulating, but I'm fairly sure that he but he got the gist correct. He was clearly trying to tell the truth, and the gist of what he The details were wrong, He was just doing the about events that were a few years old. So these hallucinations as they're called, or confabulation, they are We do it all the time. My favorite example of is there's someone called Gary Marcus who criticizes neural nets, and he says neural nets don't they just pastiche together the texts they've read on the web. Well that's 'cause he doesn't They don't pastiche together texts that they've read on the web, because they're not storing any text, they're storing these weights He's just kind of making So actually that's a Now chatbots are currently at realizing when they're doing it, but they'll get better. In order to sort of give you some insight into how all these features interacting can cause you to understand, how understanding could consist of assigning features to words and then having the features interact. I'm gonna go back to 1985, and to the first neural It was very small, it which is not big data. And it had these embedding vectors that were six real numbers, which is not like a thousand numbers, but my excuse is the computer So if you took the computer and you started running it and then you took one of we use for training chatbots, and you ask, how long would the modern Less than a second. In less than a second with all this computer That's how much more Okay, so the aim of this model was to unify two different One theory is basically what a lot of psychologists believed, which was the meaning of a word is just a whole bunch and maybe some syntactic features too. And that can explain and a word like Wednesday They have very similar semantic features. So psychologists were very and dissimilarity of meanings. And they had this model of just this vector of semantic features, and that's the meaning of a word. And it's a very kinda static dead model. The features just kind of sit They never could say where They obviously have to be learned. You're not born innately but they didn't have a good And then there's a completely which AI people had, I'm not a linguist, but I think and it's a structuralist And the idea is the meaning of a concept is its relation to other concepts. So if you think about the meaning of a word comes from its relationships And that's what meaning's all about. And so computer scientists said, well, if you want to represent meaning, what you need is a relational graph. So you have nodes that are words, and you have arc on them and that's gonna be a good And that seems like completely different from a whole bunch of semantic features. Now I think both of these And what I wanted to do was unify these two approaches to meaning, and show that actually what you can have is you can have features and then the interactions create this relational graph. The relational graph isn't What you've got is features But if I give you some words, the interactions between yes, these words can go together that way. That's a sensible way So I'm gonna show you an example of that. And this I believe to be the a deep neural net learning word meanings from relational data, and then able to answer about relational data. So we're gonna train it which I'll explain very And we're gonna make features And these interactions between the features that go with words are gonna cause it to believe and not believe in other And these interactions are a very powerful statistical model. So this is the data, a tree of English people, And you have to think back to the 1950s. We're not gonna allow marriage between people from different countries. We are not gonna allow divorces, we are not gonna allow adoptions, but it's gonna be very, extremely straight, okay? And the idea is I'm gonna and I'm gonna train a neural net so that it learns features and for each of the relationships, and those features interact so that it's captured this knowledge. And in particular what we're all of the knowledge in those family trees can be expressed as a set of triples. We have 12 relationships, and I think there's 12 And so I can say Colin has father James, and that expresses something You can see Colin has father James, and of course if I give like Colin has father James you can infer that James has wife Victoria in this very regular domain. And so conventional AI well, what you need to It's like sort of dead facts like this. You're just storing strings of symbols, and you need to learn a rule that says how you can manipulate That will be the standard AI And I want to do it a quite different way. So rather than looking for symbolic rules for manipulating these symbol strings to get new symbol strings, which works, I want to take a neural net and try and assign features to words and interactions between the features, so that I can generate these strings so that I can generate the next word. And it's just a very different approach. Now if it really is a discrete space, maybe looking for rules is fine, but of course for real data, these rules are all probabilistic anyway. And so searching through a discrete space now doesn't seem that much better than searching a real value space. And actually a lot of real value spaces are than discrete spaces. It's easier typically to search And that's what we're doing here. Oh sorry, I got technical It happens if you're an ex-professor. Okay, so we're gonna use the and the way back propagation works is you have a forward pass information goes forward And on each connection you have a weight which might be positive or And you activate these neurons, and they're all non-linear and then you compare the output you got with the output you should have got. And then you send a signal to figure out how you to make the answer you get more like the answer you wanted to get. And it's as simple as that. I'm not gonna go into the details of it, but you can read about So we're gonna use that approach you go through, you get an answer, you look at the difference and the answer you wanted, and which learns how to And here's the net we're gonna use. We're gonna have two inputs, and they're initially gonna And what that means is for the and for each person we'll So in that block at the bottom that says local encoding of person one, one neuron will be turned on. And similarly for the relationship, one neuron will be turned on. And then the outgoing weights of that neuron to the next layer will cause a pattern of And that'll be a distributed That is we converted from one halt, to a vector of activities, in this case it's just six activities. So those six neurons will have different levels of activity depending on which person it is. And then we take those vectors that represent the person and we put them through some that allow things to And we produce a vector that's meant to be the And then from that we So that's how it's gonna work. It's gonna be trained with backprop. And what happens is that, if you train it with the what you get, sorry, I got If you train it, what you get is if you look at the six features they become meaningful features. They become what you might So one of the features All the Italian people will and all the English people will or vice versa. Another feature will be That's the generation, you'll notice that in the family trees there were three generations, and you'll get a feature that tells you which generation somebody is. And if you look at the a relationship like has father the output should be one and has uncle will be the same, but has brother will not be like that. So now in the representation you've got features that say In the representation of the person, you've got a feature that And so those features that these guys in the middle, will take the fact that and the fact that the answer and combine those, and predict that the answer You can think of this in this case as lots of things you could but this is a particularly simple case. It's a very regular is an approximation to a and there's no probabilities involved, because the domain's So you can see what it's doing, and you can see that in effect it's doing what conventional It's learning a whole bunch of rules to predict the next word And these rules are capturing all of the structure in Actually if you use three it'll capture all the structure well, with two different nationalities, it's not quite enough training data and it'll get a little but it captures that structure, and when I did this research in 1985, conventional AI people didn't or they didn't say, you haven't really captured the structure. They said this is a We have better ways of finding rules. Well, it turns out this isn't If it turns out there's a billion rules, and most of them are only approximate, this is now a very good way to find rules. Only they're not exactly 'Cause they're not discrete There's billions of them, actually more like a trillion rules. And that's what these neural They're not learning, they're learning these interactions which are like rules that that explain why you and not other word strings. So that's how these big Now of course this was a So about 10 years later, Yoshua Bengio took He tried two different kinds of network, but one of them was essentially as the network I'd used. But he applied it to real language. He got a whole bunch of text, we wouldn't call it a whole bunch now, but it was probably hundreds And he tried predicting the next word from the previous five words, It was about comparable with the best language models of the time. It wasn't better, but it was comparable. After about another 10 years, people doing natural language processing all began to believe that by this real valued that captures the meaning And about another 10 years after that, people invented things And transformers allow in a way that the model I had couldn't. So they're all so much more complicated. In the model I was doing, the words were unambiguous, but in real language Like you get a word it could be a woman's name, It could be a month, it could be a modal, like it might and should. And if you don't have capitals (cell phone rings) You can't tell, should (audience laughing) I'm gonna go on a bit You can't tell what it should be just by looking at the input symbol. So what do you do? You've got this vector. Let's say it's a thousand that's the meaning of the month. And you've got another vector that's the meaning of the modal, and they're completely different. So which are you gonna use? Well, it turns out are very different from And if you take the average that average is remarkably and remarkably unclose to everything else. So you can just average them. And that'll do for now, it's ambiguous between Now you have layers of embeddings, and in the next layer you'd So what you do is you look at in this document, and if nearby you find then that causes you to make the embedding more like the month embedding. If nearby you find words it'll be more like the modal embedding. And so you progressively you'll the words as you got through these layers. And that's how you deal I didn't know how to deal with those. I've grossly simplified transformers, 'cause the way in which words interact is not direct interactions anymore. They're rather indirect like making up keys And I'm not gonna go into that. Just think of them as somewhat which have the property that the word may can be particularly strongly And it won't be very strongly influenced by things like although, although it won't have much effect on it, but march'll have a big effect on it. That's called attention. And the interactions are will have a big effect on you. For those of you who know how you can see that's a very, But it's conveying the So one way to think about words now is, well, let's think about Lego. In Lego you have different There's little ones and there's big ones and there's long thin ones and so on. And you can piece them And words are like that. You can piece them But every Lego block is a fixed shape. With words, the vector that goes with it, that represents its is not entirely fixed. So obviously the word symbol puts constraints on what but it doesn't entirely determine it. A lot of what the vector should be is determined by its context and interactions with other words. So it's like you've got these Lego blocks that are a little bit malleable, and you can put them together, and you can actually if it's needed to fit That's one way of thinking when we produce a sentence, we're taking these symbols and and getting meanings for them that fit in with the And of course the order So you can think of the words as like a skeleton that doesn't has some constraints on And then all these interactions are fleshing out that skeleton. And that's sort of what it is to give meaning to a sentence, That's very different from saying you're gonna take a sentence, you're gonna translate it some logical language that captures the meaning in proper logic, where you can now operate on the meaning by just formal operations. This is a very different notion of meaning from what linguists have had, I think. I mean a lot of linguists So here's an example. If I say she's scromed unless you've been to my lectures, you've never heard the but you already know what it means. I mean it could mean she You know, she blew him but it probably doesn't, it probably means he said and she scromed him with it. So from one sentence, 'cause of the strong contextual effect of all the other words. And that's obviously how You can also ask GPT-4 what And a student of mine did or it might have been GPT-3.5, but he did it before it So it can't have been And here's what it says, I did it understands that it's akin to hitting or striking, but that you don't know for sure. Okay, I've finished the bit of the talk where I try and explain that these things really do understand. If you believe they really do understand, and if you believe the which is digital intelligence is actually a better form of because it can share then we've got a problem. At present, these large We have thousands of years of extracting nuggets of and expressing them in language, and they can quickly that we've accumulated and get it into these interactions. And they're not just good at little bits of logical reasoning, we're still a bit better but not for long. They're very good at analog reasoning too. So most people can't get the right answer to the following question, which is an an analogical But GPT-4 just nails it. The question is why is a And GPT-4 says, well, the timescales and the energy scales are very different. That's the first thing. But the second thing is the So in an atom bomb, the the more it produces more. And in a compost heap, the hotter it gets, the fast it produces heat. And GPT-4 understands that. And my belief is when I that wasn't anywhere on the web. I searched, it wasn't anywhere It's very good at seeing analogies, because it has these features. What's more, it knows thousands So it's gonna be able to see analogies between things in different fields that no one person had ever known before. That may be this sort of in 20 different fields that GPT-4 will be able to It's gonna be the same in medicine. If you have a family doctor that's seen a hundred million patients, they're gonna start noticing things that a normal family doctor won't notice. So at present they learn relatively slowly via distillation from us, but they gain from having lots of copies. They could actually learn faster if they learnt directly from video, and learn to predict the next video frame. There's more information in that. They could also learn much faster if they manipulated the physical world. And so my betting is that they'll soon be much smarter than us. Now this could all be wrong, And some people like Yann They don't really understand. And if they do get smarter I'll leave you just, yeah, So I think it's gonna get and then I think it's There's many ways that can happen. The first is from bad actors. I'm like, I gave this talk in And before I sent it to the, the Chinese said they (audience laughing) So I'm not stupid, so I took out Xi, and I got a message back saying, could you please take out Putin? (audience laughing) That was educational. So there's bad actors who'll want to use these for bad purposes. And the problem is if you've you don't wanna micromanage it. You want to give it some autonomy to get things done efficiently. And so you'll give it the If you want to get to Europe, you have to get to the airport. Getting to the airport is a And these super-intelligences will be able to create sub-goals. And they'll very soon realize is to get more power. So if you've got more power, So if you wanna get anything done, getting more power's good. Now, they'll also be very because they'll have learned from us, they'll have read all I don't know if there are but you know what I mean. I'm not in the arts or history. So they'll be very good And so it's gonna be very hard to have the idea of a big switch, of someone holding a big red button. And when when it starts doing bad things, you press the button. Because the super-intelligence will explain to this person that actually there's bad guys And if you press a button, you're just gonna be helping them. And it'd be very good at persuasion, about as good as an adult And so the big switch And you saw that fairly recently where Donald Trump didn't to invade it. He just had to persuade his followers, many of whom I suspect weren't bad people. It's a dangerous thing to say, but weren't as bad as they seemed when they were invading the capitol. 'Cause they thought they That's a lot of them They were the really bad guys. But a lot of them thought This is gonna be much better than someone like Trump So that's scary. And then the other problem is being on the wrong side of evolution. We saw that with the pandemic, we were on the wrong side of evolution. Suppose you have multiple Now you've got the problem that the super-intelligence is gonna be the smartest one. It's gonna be able to learn more. And if it starts doing of playing against itself, it's gonna be able to learn So as soon as the super-intelligence wants to be the smartest, it's gonna want more and more resources, and you're gonna get evolution And let's suppose there's a lot of benign who are all out there just to help people. There are wonderful assistants and Microsoft, and all they But let's suppose that one of them just has a very, very slight tendency to want to be a little bit Just a little bit better. You're gonna get an evolutionary race and I don't think that's So I wish I was wrong about this. I hope that Yann is right, but I think we need to to prevent this from happening. But my guess is that we won't. My guess is that they will take over, they'll keep us around to keep but not for long. 'Cause they'll be able to They'll be much, much more intelligent than people ever were. And we're just a passing stage the evolution of intelligence. That's my best guess. But that's sort of a A little bit depressing. I want to say one more thing, which is what I call So a lot of people think that there's something People have a terrible Many people think they, or used to think, they were made in the image of God. And God put them in the Some people still think that. And many people think that there's something special about us that a digital computer couldn't have. A digital intelligence, it won't We're different. It'll So I've talked to philosophers who say, yes, it understands sort of sub one, understands in sense one of understanding, but it doesn't have real understanding 'cause that involves consciousness and it doesn't have that. So I'm gonna try and convince you that the chatbots we have already have subjective experience. And the reason I believe that is 'cause I think people are wrong in their analysis of what Okay, so this is a view which is like atheism. Dan Dennett is happy with this name and this is essentially He's a well-known philosopher It's also quite close to the Actually, he's dead a long time The idea is that most people think that there's an inner theater, and so stuff comes from the world, and somehow gets into this inner theater. And all we experience directly This is a Cartesian kind of view. And you can't experience my inner theater, and I can't experience your inner theater. But that's what we really see. And that's where we have That's what subjective experience is experiencing stuff in this inner theater. And Dennett and his followers like me, believe this view is utterly wrong. It's as wrong as a religious fundamentalist which if you're not a you can agree is just wrong. And it relies on people not of what a mental state is. So I would like to be what's going on in my brain particularly when I'm looking I'd like to tell you I'm that isn't really there, but If I tell you which neurons 'cause all our brains are different and we don't know which But one way I can tell you about what's going on on in my brain is by telling you about the things that would normally have caused that if perception was working normally. And those normal causes A mental state is the normal cause of what's going on in What would be a normal cause for it, even though that's not So let me give you an example. If I say I have the subjective experience of little pink elephants The sort of normal analysis of that, what most people think is and in this inner theater which are made of funny stuff And that's what's going on. What Dennett thinks is about the state of my perceptual system by telling you about hypothetical things. They're not real, they're hypothetical, but they're hypothetical things of the kind that live in the real world, like little pink elephants, and these hypothetical would've caused this It's an indirect way of via the what would even though in this case it's not being caused in the normal way. So now if that's what you think if that's what you think I mean when I say I have the subjective experience of little pink elephants what I mean is if there were little pink then what's going on will be normal perception. So if that's what you think I mean, then think about a multimodal and it can produce words, and it has an arm that it can point with, and you've trained it up, and you ask it to point to an object that's straight in front of it. But before you do that, you put a prism in front of its camera. And so you put the object and say, point it, and it points there. 'Cause the prism bends And so you say to the chatbot, no, it's not there, it's 'cause I put a prism and the chatbot says, oh I see, I see. It's straight in front of me. I had the subjective but it's actually straight in front of me. Now I think if the chatbot says that, it's using the word subjective experience in exactly the same way we use it. And so for that reason, I would argue chatbots already have when their perception goes wrong. And now I really am done. (audience applauding) And Sheila's gonna manage the questions so I don't have to do that, I can just think about the answers. - That's right. So Geoff, thank you very a very thought-provoking and For those of you who don't know me, my name's Sheila McIlraith, I'm a professor in the I'm also an associate director at the Schwartz Reisman Institute, and a faculty member at and a new friend of the Cosmic So as Geoff suggested, I'm for the question and answer period. And I just wanted to give you a heads up on how it's going to work. So this is a unique to engage with Professor Geoffrey Hinton, and to do what the university to learn, and to engage in A reminder that tonight's and will be posted online at a later date. We have two channels for posing questions. The primary channel is through and we have four ambassadors who will be roaming around and I will be signaling to them and they will be giving And then I'll be signaling again working through Sarah to let you know when I'll signal when it is your turn to speak. And if you feel comfortable doing so, please identify yourself But you don't have to. For those of you who are seated up above, or for people who are feeling about standing up and asking a question, we also have another which is through the QR code and the URL that you'll see up there. So for those of you who are seated there, you can scan the QR code or go to the URL and you can type in a question. And we have a team right down Harris Chan and Silviu Pitis who will be vetting the questions, and when I ask them to do so, they will be reading So please, again, if you feel comfortable that would be great. But if you don't feel that's also fine. So without further ado, I'm going to move over to direct traffic, and Geoff's gonna come back here and answer questions - My god, what a probability. Bit of luck. My compliments on your analysis. As a retired computational neuroscience who spent years solving for neurons and synapses, et cetera. My question is, what can you offer from the AI modeling of consciousness? I mean this is an age-old from early age of evolution. My compliments again on your work, and I look forward to your answer. - Okay, so there's a whole bunch of terms that are all interrelated, like subjective experience and I chose to talk about 'cause I think it's the But as long as people have that only they can experience, and that's what they're describing when they talk about mental states, they're talking about the inner theater, not hypothetical external worlds, then I don't think we'll ever be able to sort out what consciousness is. Consciousness involves but it also involves self-awareness. I'm not convinced these 'cause I'm not convinced they So I deliberately avoided but my strong belief is that by getting straight what we And only when we've got that straight will we be able to then and understand what consciousness is. But my current belief is almost everybody has it about what consciousness is. I like being in that position of thinking everybody else is wrong. - Thanks for that. My I'm a philosophy professor here, and I'm happy to agree have some linguistic understanding. But I wanna talk about the coming dystopia that you sketched at the end. And I like to think sometimes that maybe the very, very of the artificial intelligences are gonna be the ones that win if there's a evolutionary And I also like to think that well, maybe we are special in a way that would make the best and brightest artificial intelligences take our side. I mean, like for me to be an interesting conversational partner with you right now, I don't have to be smarter I just have to have some or even just some way that you find interesting. And I'm wondering if you think that there are any special of human cognition which as interesting conversational partners for the most advanced going forward. - Well, let me revise my statement that there's nothing special about people. People are very special for people. Okay, so for people we're very special. I think there's something in what you say. So Elon Musk for example, will get to be much more but then it'll keep us around That seems a very sort of thin thread to hang our existence from, but. - But I mean as long as human intelligence isn't optimized under maybe it could be a - Yeah, I tend to be depressive, so. There's someone I met in England who was the president of who believes that these because they didn't evolve, might turn out to be very, and quite benevolent. So at least it's diversity obviously the psychological makeup of the person making the So Yann is a very cheerful person. He thinks Mark Zuckerberg's a good guy, and he thinks it's all gonna be fine and we can release these and it'll be great. I don't know if he believes in But yeah. - Thank you so much for a great talk. I'm Rhonda McEwen. I am president at Victoria I'm a professor of emerging My question for you is about guardrails, some with the backward propagation issues. Do you believe that there's a real thing such as explainable AI? Because this is one of the things they're suggesting is a guardrail. How do we get there in your view? - So I'm not that optimistic particularly, I'm particularly unoptimistic and then you try and add guardrails. That seems to me like and then trying to catch all the bugs. You'd much rather write software that's guaranteed to work somehow and not have to catch I think it's been demonstrated already that these things where you by getting a bunch of people, to tell you when it gives a good response and when it gives a bad It's very easy to break. Anthropic has a different view of trying to produce a chatbot that has like a constitution That seems a bit more promising, but I'm not very optimistic I have my most, most, my best bet is Ilya and was probably the main He's working now entirely on AI safety, and he believes he can get something where he's gonna be able to guarantee that it doesn't go bad. That seems to me like Oh Sheila. - Hi Professor Hinton, thank you so much for your to take us through your thoughts. I'm actually very thrilled that we can now create artificial intelligence. I'm actually wondering is it possible to, under different forms of intelligence other than human intelligence, such as like animal intelligence like a fundamentally different than perhaps human intelligence, different than perhaps on a scale, but actually like different What are your thoughts on this? Thank you. - I don't really have but I could make some up. (audience laughing) So, like most people, I tend to think of animals and the point of the question is they might be intelligent I think that's quite possible. And so if you were to take and train it on responses then maybe it would develop I think that's quite possible. I don't know if that - Hi Geoff. Nice to see you in person. I've seen a lot of your videos. I'm Charles, I graduated from the department of with focuses in artificial So I've studied a good I have, I think about and I was hoping that that you want to answer most. - Why don't you pick the one (audience laughing) - Okay. Okay. Do, do, do. (audience laughing) Okay, two, I think I can So the first one being, for determining whether an should have rights such and the right to not be switched off? And do you think that world-leading AI should be open to the public weights, training usage access or controlled And it's not meant to like it shouldn't be controlled by companies and governments. It's more like they can pose a large risk if they're used irresponsibly, and like, you know, not everybody - Okay, let me answer that one. There's the kind of existential threat of these things taking over, but there's a lot of shorter-term threats that are really bad. Like cybercrime, phishing, could be done by these language models, and that's why I don't think Yann completely disagrees, but all governments are what are bad actors gonna be able to do with these open source models. Because you can take one of these models that's been trained on a lot of data, and then you can specialize on, for example, phishing attacks. So GPT-4, let's say if it was open source, it already knows how to do lots of things, but now you get it to it's gonna be very good. It's not gonna have things So that's why I don't think - Do you think Western governments are doing enough to regulate - Probably not. I should say there's, I 'cause there's people these issues much longer than me in AI. I only came to AI safety very recently when I suddenly thought might actually be much better Then I got really worried and But that was just this year. So there's other people who've thought much I'm sort of treading on their territory, but I think we're I tend to be cautious, and so given that we don't know what I think we should be cautious. - [Charles] Thank you. - [Sheila] I think we're from the audience above - Yeah, so one of the very is whether you feel any for potentially unleashing a intelligence that could surpass humanity. - So there's two questions. And the other is, should I feel any guilt? (audience laughing) Let's start with the easy one. I don't actually feel any I only feel guilt when I Should I feel guilt? Well, all the time I was doing this, I thought we were way away from having intelligence that's comparable with ours. And I always thought much more like the brain and they need to be much bigger. But now we've got things that in terms of numbers of connections, and seem to be comparable with us. Not quite there yet, but quite quickly and then So I feel a bit embarrassed but I don't feel in the I made morally bad decisions, 'cause I didn't know And then of course there's the fallback. If I hadn't done it, And actually, I've learned The media would like some big happening to be down to one person. And so the media always tells a story about this person did that. It's never like that, it's 10,000 people, and some of them make more But this is a point at to share the responsibility. (audience laughing) - Hello, my name is Shalev Lifshitz. I'm a student of Sheila McIlraith's. Thank you for the great - Thanks for your emails by the way. - Oh, so I have two questions. One is that I'm personally not certain whether LLMs truly understand. And there's two key sources of evidence that I consider for them One is this idea of the reversal curse, which has been popularized recently. If it learns A is B, it doesn't And the second is that sometimes it fails at things that we think are very basic, like basic arithmetic. If it truly understands, we would think that it probably for these things. So these two pieces of kind of indicate to me that maybe it's just doing and it's not truly understanding. And my second question. - But remember, it's all - Right. But to me it like why would that, the fact that it's the difference with neural nets symbols, and we're presenting them with distributed representations, and then we're operating on those. But that could still be in distributed representations - But the point is, in those interactions it's got huge amounts of structure, right? So that's very different When you cite a source, you you just cite this piece These feature interactions - Couldn't it be that it's kind of condensing the pre-trained internet that and then in a fuzzy way obviously, 'cause you need to adhere kind of retrieving from its weights. I think there's a talk that where you talk about Boltzmann machines and the fact that it's a generative model because it's kind of encoding into its weights. And that's how I look, let's say at LLMs, it's encoding the and then taking from that to be able to answer our questions. So that's kind of a way of retrieving, is that understanding? - Okay, think of it in which I think is very helpful. It takes a huge amount of text, and it encodes it into very few weights, like only a trillion. Given the amount of text it's encoding, that's not that many weights. In order to do that, it's And to do compression you've gotta see the You've gotta make use of the fact that the same kind of structure is occurring in many different places. And that's understanding. - Thank you. And if I could ask a second you believe, are smarter - No, I don't think, no, I but I think they know a lot more. - So there's this notion which is that the things are actually very easy And the things that we find easy, that we've had millions like fine motor control and whatnot, are very hard for machines to learn. Do you resonate with that? - I resonate with that a bit, but these things are at motor control too. So I think they're behind at that, relative to our kind of sort of being able to pick up something isn't way up there in the hierarchy, but they're getting able to do that. So they're behind at motor control and that's why my advice to people is if you want to train in that's gonna be the last thing that goes. - My name is Andrew and I'm studying computer science, So first of all, I guess to be in the same room as you. Like us UofT students have been hearing stories But my question is, what are your views on the prospect of technologies such as brain machine interfaces that seeks to reconcile the sort of intelligence of and our more organic Did you see it as a possible solution for the alignment problem? Like conducing towards like the mitochondria or do you think there's some fundamentally irreconcilable difference between the two? - So Ilya Sutskever, who is thinks that eventually many people would choose to be combined with AIs. That's one future path. There are also people trying to use collections to help do low power computing. They're a long way from But earlier this week I with a bunch of human that have been trained to play Pong. - [Andrew] Did you win? - I did. That's the only important thing. I beat it. They didn't have a The training algorithm was, it was doing kind of random stuff, and whenever it did the and whenever it did the wrong a high frequency thing So it wasn't a very But people are seriously looking at, you take skin cells, you then you turn those into brain cells, then you grow them for and it's very Frankenstein-like. I was in the lab, and there's this little and it's got tubes coming and other tubes take carbon dioxide away, and it's got tubes to bring nutrients in and other things to take nutrients out. And as we left the lab, one of the people at the I think I've got a kidney. (audience laughing) Because they couldn't purify the liquid, they wanted something that would filter the liquid effectively. So yeah, people are, I find this creepy, but people are looking at as computers that can and I think it's a long way off. But then I thought super-intelligence was a long way off, so. - [Andrew] So you you're that this is a possible solution - I'm not sure this would - Okay, okay. And may I ask - It was David Haussler's lab at UC Santa Cruz. - Okay, thank you. - [Sheila] Thank you, I think from our Slido team. - On the topic of existential risk. What should we do now that we have these And especially since a lot especially since a lot of people we're wondering what we as - I can't locate you. Sorry. - Oh. - Oh. Sorry. It's a question from the audience. - Well, I think one thing we can do, so there's a paper that came written by a whole bunch of people who are worried about including Yoshua Bengio and me and Danny Kahneman and people, one thing we can do is insist be put into safety. So we propose that the big companies should put a third of We don't expect them to do and that's an obvious thing we can do now. I think we should be very cautious about open sourcing Not so much for the existential threat, but for things like cybercrime and other criminal activities. Open sourced very powerful they're gonna make everybody that only very skilled hackers would've been able to do before. So don't open source the big models, and insist governments and companies put a lot of money into safety. I wish there was a simple solution, like with climate change you stop burning carbon, and in a hundred years' But there isn't a solution - Do you have advice for to contribute to this - Yeah, my advice is there aren't enough people So work on AI safety, and you'll notice a lot of the Roger Grosse, and David Duvenaud at UofT, they're all getting very And so it's not just that it's something very it's so you'll get very - Hey Geoff, thanks for a great talk. My name is Rahul, I'm an assistant and I work a lot on to use some of these tools to help accelerate some And one of the questions that came up is about the idea of a truly new idea. So can neural networks come up with a fundamentally new idea. And I thought I'd ask this question in the form of a thought experiment. So let's say you took all of the GPUs that Nvidia had right now, and right before Isaac Newton And if you train this GPT-4 on all of the text data that represented a compendium I'm just curious to hear your would've come up with the law of gravity? - I'm not sure GPT-4 would have, but I think more advanced That is, I don't think there's anything, there's any kind of barrier So a lot of people think they're just sort of stealing and recycling it. That of course is what other But I don't think there's some barrier that there's truly creative and then sort of slightly And one piece of evidence is move 37. - But what if you separate out, you know, asking the question of from figuring out the consequence of the laws that enabled - I agree with you that ask questions other people haven't asked. I don't see why these won't be able to do that too. - Hello Professor Hinton. I'm Estobale, I'm a student, doing computational biology at the Princess Margaret Cancer Center. My question was if as you say, you know, this intelligence is already - I don't think there's I should emphasize that. Everything I say you We don't know. I'm just - Okay, if you believe it to the extent that you describe. What is to prevent it that are adamantly false similar to what humans think or develop, when, you know, recollecting - It's a good question. I think some of the obviously are to do with the fact that we're mortal. So we don't like the idea And I think that underlies We also are very tribal, we and so I think that and not wanting to die doesn't help, and they may not have that. So there may be an advantage there. - Even if it learns from us - Right, they're gonna learn But if they start learning for themselves when they're more intelligent than us, and we haven't got anything If they're not worried about that makes them less about how they're gonna live forever, and they better kill the - And can we prosper alongside without bottlenecking it - I don't know. We're getting I don't think so. - [Estobale] Thank you. - Okay. Hi, Dr. Hinton. It's my first time I had a chance like a couple years ago at a engineering science like so yeah, so sorry for the, yeah, so basically my question concerns whether you think it is necessary or these intelligent agents to interact with the per se become smarter as in, you know, have to chemistry experiments, they take videos and analyze videos themselves. Or do you think, you know, like, I mean just think So development of mathematics, but it kind of does with the real world. And I talked with the author and apparently, you know like a large language model should and where it should go, because if you're just it's just gonna find, you and eventually it's tensor program or new P that's gonna make neural networks better. But then the question becomes that the networks trained It has to have some other tasks that might be grounded to the real world for it to know that you know, like this math is actually useful. So like basically the do you think it's possible for, you know, for use developing math, for these large language models? Do you think it's still necessary for them to interact with the real world to gain any kind of like truth or reward? Or do you think they can they can develop themselves? Because that way they without, you know, us handing them the key to like very dangerous, you - Okay, I'm gonna rephrase that I think is a fairly And tell me if this has got the sort of If you took one of these, if and you put it in a room and you just played the radio to it, but you played all the would it learn to be intelligent? Or does it have to be to learn to be intelligent? I think it could learn to be intelligent just from listening to radio waves, but I think it would be hard, and I think it'd be much easier So for example, if you in order to understand them, which is what a sort of Marxist would say, it's bad news for astrophysicists. As far as I know, they've and they certainly haven't So they clearly understand, well, they claim to understand a lot, without having acted on them. I just think it's easier if you can act on them, but not necessary. - [Attendee] I see, I see Thank you Dr. Hinton. Thank you for answering, - Let me say one more thing about that, which is if you think about that has no physical it's just language. It's only ever seen texts coming in. It's only ever produced texts going out. I'm gonna get into stuff I don't but, so it's not grounded in any sense. So I think it can have an understanding that's kind of isomorphic to the world, but it's not grounded in the world. So the interactions of all these features have captured the structure of the world, it's just, it doesn't connecting it to the world. So in that sense, I think it can learn these interactions to just by taking all this text and figuring out a very 'Cause I believe the basic principle that if you can take a bunch of data, and you can find a very compact model that explains all the data, That's the sort of article of faith. And if it's not like - [Attendee] I see sir. What if we deplete all the data we have? - Sorry. - [Attendee] What if like we like all the text, all the - And what's the question? - Actually, I'll just hand because it might take a while. - Thank you, hi, oh, Avery Slater, professor of literary theory here at UofT. And so following on what happens if we use up all I wanna ask you about natural And I am convinced that natural language understanding but I wonder if you what kind of understanding And to ask this question, I want to use the problem And Nigel Richards, this of Scrabble championship playing. Nigel Richards has won every and after a while. - Was he the one who won the French one without speaking French? - That's right, that's exactly it. So he won the 2015 World without speaking French in French. And when I was thinking well, in what way does Because he's doing something with French that no French-speaking person But he says, &quot;Well I can't speak French.&quot; And so I'm wondering if with the kind of game or understanding. - But if you ask what it you don't need to know the You don't need to know They're just, words are and you just need to know Now it probably helps to and so it'll tell you and stuff like that. So there's a bit of morphemic understanding there presumably. And I bet you he has that for French. But it's a bit like you can because the numbers in They're not used as numbers. But if it's killer Sudoku then they're beginning but he doesn't need to know French to be good at French Scrabble. - We'll take one from Slido, and then I think there's who's had their hand up for a - Yeah, so this is a and his question is, how do you know, change as a result and you know, potentially happening in the future? And sort of what skills would in the world where there is, you know, super-intelligence out there? - I don't really know. In the shorter term, I think we shouldn't we shouldn't sort of prohibit I think we should encourage students to get good at prompting them. Just like with search on the web, you get used to using search on the web and it's very helpful. I've now got used to using GPT-4, and I ask it all sorts of things. It's very good at plumbing advice. It's no good at actually but the theory of plumbing, it's great at. I think universities should It makes you much more powerful. - [Harris] Perfect, thank you. - Is this on? Okay, hi, my name is Sophia. I am to your right if you're - Thank you. - Yeah, so one of the reasons why it took me a really long time to start my pivots to AI safety is because I had a thought well, what's the point? Like what will we realistically And you know, much of your talk has been quite depressive in tone, but you are presumably here because you think that there is some hope. And so I'm wondering, you know, why do you think there's hope? - Everything's very uncertain, right? We don't really know We don't know whether it's possible to make these things be and things like that. It seems to me that we should and put them on this problem, because together with climate it's the most urgent things there are. - I'm wondering if there's - [Geoff] Sorry? - I want, okay, sorry, this is kind of nudging us I'm wondering if there's - Yeah, I've adopted the and I'm not gonna have more good ideas about how to do this stuff. It's stuff I haven't thought about much. I can see there's this huge problem, I can use my reputation to and encourage governments - [Sophia] And thank you for doing that. - And that's why I'm doing that. But I don't know how to solve it. I don't even have any good - [Sophia] That's fair. - Good evening. Dr. Hinton, if you're looking for me, - [Geoff] Oh, okay, okay. - I'm Ayla, I'm a first year, I'm studying computer and I'm profoundly appreciative of the path that you have I'm extremely, extremely passionate, and it's an honor to speak to you. My question is about empathy in AI and what do you think is when AI will indeed like develop empathy? - So I don't see why these digital things shouldn't have empathy. If you train them on data I think they'll exhibit empathy. Now that's maybe optimistic. There's some people who like Trump for example. And Elon Musk recently said, which is a bit worrying, but I don't think empathy is a magic thing that only people can have. - But what are the pitfalls? Then it genuinely is a consideration that AI might just take over, and it will be an end to - [Geoff] Yeah, that's - I have a follow-up question. If there was a hypothetical world where AI was just to collaborate with AI, and human with human what is one task that you that humans have no chance? - Folding proteins, and I mean there's all sorts that are very important, for folding proteins there's a But big neural nets are maybe not at the most creative things yet, but they can see much more data, they can understand a lot more, even if they don't And once they understand it more deeply as well as understanding a lot more, I think they'll just be much better at figuring out how things work. - [Ayla] Oh, well thank you. - [Sheila] We're and I'm conscious that for quite a long time I would suggest that we maybe Does that sound about right - Sounds good. - [Sheila] Yeah. Okay, very good. And I think there are people, someone with a microphone - Yeah, okay, great. Thanks Geoff. My name's Ashton Anderson. There seems to be some evidence that these models don't do so well when they train on the And I'm wondering if you see this as a just the fact that we've on all of available human or if this is a passing problem? - Yeah, I think that's that we've trained on a and there isn't that much There's actually much more data which isn't publicly available. Companies have much, much more data. And if you can get at you can get a lot more data. But if you take a multimodal chatbot, it doesn't need nearly as much language. So the amount of language needed to train a chatbot that is much more than to train and maybe can manipulate So I think we may have a lot by training on other if we could figure out how to train them really effectively on video. And then you just show, it's They just figure it out. - [Ashton] Thank you. - We'll take a last question from Slido, and then a few more - Hi Geoff, this is can, I'm good. Okay. This is a question about capabilities. Do you think that all the in order to train the super-intelligence are already here today? Is imitation learning, LMSs, and reinforcement learning enough, or do you think that there to be done in the world? - Okay, so I think that even if we get no more fundamental even if we never get another breakthrough like transformers for example, we're gonna make them a lot smarter. So, and that just requires breakthroughs in building hardware, and in making things that are one nanometer and stuff like that. That's where the real But of course there are but I don't think it's necessary to make things a lot smarter. - [Attendee] Thanks. - Hi Dr. Hinton, my name is Edith, and I am a second year engineering So my question is, if AI is that's dangerous to humankind, so is there at that point, is there a way to stop the development of - Okay, I think it might be that the rational thing for people to do is just to stop it now, and do a lot more research But that just isn't gonna happen. So there's a petition saying we should slow down these big models. I didn't sign it 'cause I felt it was They weren't gonna do that. I think I should have signed it, because even though it's it's a political statement. It does so many good things, that there's no way you're gonna stop it. And if one country stopped it, other countries aren't gonna stop it. So I don't think it's even though that might - [Edith] Thank you. - Hi professor. My name is Victor. I'm a first year CS My question is, do you think consciousness or is it correlated with that you can't remove consciousness without affecting the neural activity? - Okay, I'm a materialist, and I think that the neural activity, if you include the hormones in just the physical activity, I don't think it makes sense to say, take something that's got in the brain and is conscious, and then remove the consciousness. That doesn't make sense to me. Being conscious is some property of that physical activity going on. In a dualist framework, There's this mental stuff and And if you have the physical then it's not conscious. I think that view is just utterly wrong. - Thank you so much. - Hi Professor Hinton. I'm in the white jacket on the rows. Hi, it's so nice to meet you. I think you're such a cool person. Thank you for your amazing talk. - Thank you. - I study computer science. I had more of a practical question. So based on your understanding of the trajectory of like and their intelligence, do you in this lifetime, like of a software developer in that large companies just or like not at all software Or like, do you think it'll as a whole? Or do you think software engineers, their roles will sort of evolve into more of a managerial role into knowing like the instructions to tell the ChatGPT models? - The latter. I think their role will evolve, but you may need far fewer of them. And this afternoon I read my newsfeed on my iPhone tends to be entirely about about someone had taken a chatbot and made a number of copies of it, and given them different roles and asked it to design a program I think he was playing Gomoku And what they claimed would've taken several weeks these chatbots talking to each other. So they talked to each other to choose what language to use and things. And they designed this If I was a programmer, - Hi Geoff, my name is Yang Xu, I'm interested in one on the Baldwin effect when you talk about how learning And I wonder what your thoughts are about the fact that and perhaps they will figure out a new way of communicating with each other that's beyond human language, the diversity of human to really speed up the evolution? - That just makes it worse, right? - Yes, that's right. Yes. - Yeah, I mean. - Do you see that as a possibility that they will figure out for efficient communication? - Yeah, that seems quite likely. But that will be communication between different models, right? For different copies of the same model, they can communicate by weight sharing. And that's got huge bandwidth and I don't see why they But for different architectures, digital intelligences, but they might well come up And I've actually sort of I had a paper called &quot;Commentaries&quot; where the idea was you get than the normal number of outputs. So you get more insight into and you try and learn to give outputs that are informative about so that other models to make distillation more efficient. And I think there's interesting where you can think of language as something that makes rather than just giving a for example. A caption's a much better thing to give, but you can imagine inventing that make it much easier between digital models with - [Yang] Great. Thanks. - [Sheila] And that was the final word. So thank you again. (audience applauding) - And let me say one more So let me thank Sheila once (audience applauding) - And I in turn want to first and foremost you, but also our audience for I'm sorry that we couldn't but it just sort of validates to have these types of talks, and to really be able to allow and so many students of mine really giving them the opportunity to engage directly with you. So thank you for that. I wanna thank Dean Melanie Woodin and the co-hosts of tonight's event, the Schwartz Reisman Institute, the Department of Computer and the Cosmic Future Initiative of the Faculty of Arts and Science. I also want to thank their leadership. I wanna thank Professor Gillian Hadfield, Professor Eyal de Lara who Tony Gaffney, who's here and also Juna Kollmeier, who is the lead of the Cosmic Future Initiative. A special thanks once again for the beautiful music we Mr. Piazza's performance by Professor Peter Martin, who's right down here in the front. On behalf of the Cosmic Future Institute with support from John Aaron James, and Patricia Wright. I also wanted to extend my personal thanks to the team at the under the leadership of and also to UofT Director who you saw directing they really were critical Leaving the best to last, to Professor Geoffrey Hinton Geoff, thanks for sharing and for your impactful and And in closing, I hope and I hope you all go away about some of these fascinating and really important problems and that you'll study here the best place in the world to study AI, but AI from a not just from the Computer So thank you and goodnight. (audience applauding)