PATRICK WINSTON: So We started off with simple Then, we talked a little about we're vaguely inspired by. The fact that our heads are we seemed to have evolved Then, we talked about looking at issue of [? phrenology ?] and how it's possible But now, we're coming full and thinking about how to decision boundaries. But whereas, you do it with neighbors or a ID tree. Those are very simple ideas Today, we're going to talk about idea that still has So this needs to be every civilized person. This is about support idea that was developed. Well, I want to talk to ideas develop, actually. Because you look at stuff like well, Vladimir Vapnik just afternoon when the weather was That's not how it happens. It happens very differently. I want to talk to you The next thing about great people who are still alive how they did it. You can't do that You can't say to Fourier, Did you dream it up on But can call Vapnik on the phone That's the stuff I'm going the end of the hour. Well, it's all about decision And now, we have several draw some decision boundaries. And here's the same problem. And if we drew decision get something that would If we were doing a nearest we're doing ID trees, we'll just And if we're doing neural nets, lot of straight lines wherever depending on how it's Or if you just simply go in could do that if you wanted. And you would think that after this sort of stuff for 50 or 75 be any tricks in the bag left. And that's when everybody got early '90s Vladimir Vapnik to talk to you about. So what Vapnik says is Here you have a space, and you and you have some positive How do you divide the positive the negative examples? And what he says that we want straight line. But which straight line Well, we want to draw Well, would this be a One that went up like that? Probably not so hot. How about one that's Well, that might separate them, close to the negative So maybe what we ought to do straight line in here, And that line is drawn with a widest street that separates the negative samples. That's why I call it the So that makes way of putting is to put in a straight line but ID tree puts in a It tries to put the line in in between the positive and That street is as wide All right. So you might think to do that in let it go with that. What's the big deal? So what we've got to do is we've it's a big deal. So first of all, we like to a decision rule that would use So what I'm going to ask you to vector of any length that you perpendicular to the median, or to the gutters. It's perpendicular to the median All right, it's drawn in such We don't know anything about Then, we also have some unknown, And we have a vector that So now, what we're really not that unknown is on the right the left side of the street. So what we'd what to do is want u, down on to one that's Because then, we'll have the a number that's proportional And the further out we go, the the right side of the street, street is not the correct side the street. So what we can do is we can say, with u and measure whether or greater than some constant, c. So remember that the dot projection onto w. And the bigger that projection line the projection will lie. And eventually it will be so crosses the median line of the be a positive sample. Or we could say, without loss product plus some constant, b, If that's true, then it's So that's our decision rule. And this is the first in several going to have to line up to support vector machines. So that's the decision rule. And the trouble is we don't know we don't know which We know that w has to be line of the street. But there's lot of w's that median line of the street, could be of any length. So we don't have enough particular b or a Are you with me so far? All right. And this, by the way, we get equals minus b. What we're going to do next is additional constraints whether constraint on the situation that a b and a w. So what we're going to say is quantity that we're checking out than 0 to make our decision, is we're going to say that if we take the dot product of that positive sample, now. This is not an unknown. This is a positive sample. If we take the dot product of b just like in our decision to be equal to or So in other words, you can be street and be just a little bit bit less than 0. But if you're a positive sample, that this decision function value of one or greater. Likewise, if w thought it was provided to us, then we're going equal to or less than minus 1. All right. So if you're a minus sample, any minus sample that may lie gives us the decision rule must So there's a separation Minus 1 to plus 1 for So that's cool. But we're not quite done, equations like this, So what we're going to do is another variable to make Like many things that we do, and of stuff, introducing this God says has to be done. What is it? We introduced this additional To make the mathematics more convenience. So what we're going to do is variable, y sub i, such that y plus samples and minus 1 All right. So for each sample, we're going new quantity we've And the value of y is going to a positive sample or If it's a positive sample it's situation up here, and it's situation down here. So what we're going to do with going to multiply it by y sub plus b is equal to or And then, you know what We're going to multiply the left y sub i, as well. So the second equation becomes And now, what does that We multiplied this guy So it used to be the case that So if we multiply it by minus than plus 1. The two equations are the same, this little mathematical So now, we can say that y sub Well, what we're going to do-- Brett? STUDENT: What happened PATRICK WINSTON: Oh, did I'm sorry. Thank you. Yeah, I wouldn't have gotten So that's dot it with Thank you, Brett. Those are all vectors. I'll pretty soon forget to put there, but you know So that's w plus b. And now, let me bring that 1 that's equal to or All right. With Brett's correction, I But we're going to take one more say that y sub i times x sub It's always got to be equal But what I'm going to x sub i in a gutter. So there's always going to be going to add the additional be exactly 0 for all the samples gutters here of the street. So the value of that expression exactly 0 for that sample, 0 sample, not 0 for that sample. It's got to be greater than 1. All right? So that's step number two. And this is step number one. OK. So now, we've just got some some constraints. Now, what are we trying I forgot. Oh, I remember now. We're trying to figure out how such at the street separating wide as possible. So maybe we better figure out distance between the Let's just repeat our drawing. We've got some minuses here, got got gutters that are And now, we've got a vector here a vector here to a plus. So we'll call that x plus So what's the width I don't know, yet. But what we can do is we can two vectors, and that will looks like this, right? So that's x plus So now, if I only had a unit median line of the street, if could just take the dot product and this difference vector, and the street, right? So in other words, if I had a then I could just dot the two the width of the street. So let me write that down So the width is equal to OK. That's the difference vector. And now, I've got to multiple But wait a minute. I said that that w is The w is a normal. So what I can do is I can then, we'll divide by the make it a unit vector. So that dot product, not a in fact, a scalar, and it's It doesn't do as much good, we get much out of it. Oh, but I don't know. Let's see, what can Oh gee, we've got this equation equation that constrains that lie in the gutter. So if we have a positive sample, is plus 1, and we have So it says that x plus times w See, I'm just taking this part I'm dotting it with x plus. So that's this piece y is 1 for this kind So I'll just take the 1 and the side, and I've got 1 minus b. OK? Well, we can do the same If we've got a negative sample, then y sub i is negative. That gives us our negative But now, we take this stuff back and we get 1 plus b. So that all licenses to rewrite the magnitude of w. How did I get there? Well, I decided I was going to I noted that the width of the difference vector times Then, I used the constraint to And I discovered to my delight of the street is 2 over Yes, Brett? STUDENT: So your first x minus is 1 plus b. PATRICK WINSTON: Yeah. STUDENT: So you're PATRICK WINSTON: Let's see. If I've got a minus here, then then, the b is minus, and when I side it becomes plus. STUDENT: Yeah, so if you right [INAUDIBLE]. PATRICK WINSTON: No. No, sorry. This expression here Trust me it works. I haven't got my legs all well, not yet, anyway. It's possible. There's going to be a lot of So this quantity here, this This quantity here is the And what we're trying to maximize that, right? So we want to maximize 2 over get the widest street under decided that we're going All right. So that means that it's OK to We just drop the constant. And that means that it's magnitude of w, right? And that means that it's OK magnitude of w squared. Right, Brett? Why did I do that? Why did I multiply by STUDENT: Because it's PATRICK WINSTON: It's Thank you. So this is point number three So where do we go? We decided that was going We're going to see which side We decided to constrain the decision rule is plus 1 in the samples and minus 1 the negative samples. And then, we discovered that street led us to an expression which we wish to maximize. Should we take a break? Should we get coffee? Too bad, we can't do that in But we would if we could. And I'm sure when Vapnik went out for coffee. So now, we back up, and we say, expressions start developing Not like that, that's vapid, What song is it going to sing? We've got an expression here minimum of, the extremum of. And we've got some constraints would like to honor. What are we going to do? Let me put what we're going the form of a puzzle. Is it got something to Has it got something Or does it have something She says Lagrange. Actually, all three were said Defense Committee-- must have But we want to talk about situation here. Is this 1801? 1802? 1802. We learned in 1802 that if we a function with constraints, use Lagrange multipliers. That would give us a new maximize or minimize without the constraints anymore. That's how Lagrange So this brings us to miracle piece number four. And it works like this. We're going to say that L-- the thing we're going to try maximize the width is equal to 1/2 times the squared minus. And now, we've got to have constraints. And each or those constraints is alpha sub i. And then, we write down And when we write down there it is up there. And I've got to be hyper otherwise, I'll get lost So the constraint is y sub i vector x sub i plus b, and parenthesis, a minus 1. That's the end of my constraint, I sure hope I've got that right, trouble if that's wrong. Anybody see any bugs in that? That looks right. doesn't it? We've got the original thing Now, we've got Lagrange It's back to that constraint constraint is constrained Well, there's a little bit of here, because in the end, the the Lagrange multipliers here. The ones that are going to be connected with vectors that The rest are going to be 0. But in any event, we can pretend we're doing. I don't care whether it's I've lost track. But what we're going to do is extremum of that. So what do we do? What does 1801 teach us about? Finding the maximum-- well, we've got to find the And then, after we've done that, manipulation, we're going song start to emerge. So let's see if we can do it. Let's take the partial of L, the to the vector, w. Oh my God, how do you respect to a vector? It turns out that it has a form differentiating with respect And the way you prove that to everything in terms of all of You differentiate those with differentiating with respect turns out the same. So what you get when you respect to the vector, w, is 2 magnitude of w. Was it the magnitude of w? Yeah, like so. Was it the magnitude of w? Oh, it's not the It's just w, like so, no Then, we've got a w over here, this part with respect But that part's a lot easier, have there is a w. There's no magnitude. It's not raised to any power. So what's w multiplied by? Well, it's multiplied by x and All right. So that means that this the Lagrangian, with respect to minus the sum of alpha sub i, got to be set to 0. And that implies that w is equal i, some scalars, times this times x sub i over i. And now, the math is Because it tells us that the the samples, all the samples It didn't have to be that way. It could have been raised It could have been All sorts of horrible happened when we did this. But when we did this, we be equal to a linear some Some of the vectors in the because for some alpha All right. So this is something that we something important. Now, of course, we've got to to anything else it might differentiate L with respect So what's that going Well, there's no b in here, so This part here doesn't have a contribution. There's no b over here, so that So we've got alpha i times That has a contribution. So that's going to be the sum And then, we're differentiating to b, so that disappears. There's a minus sign here, and implies that the sum of the i is equal to 0. Hm, that looks like that might And now, it's time By the way, these coffee You stare at it. You work on something else. You've got to worry And you think about And eventually, you come and do the next thing. Oh, what is the next thing? Well, we've still got this to find the minimum for. And you say to yourself, this numerical analysts. Those guys know about Because of that little power This is a so-called quadratic So at this point, you would be over to a numerical analysts. They'll come back in a few You implement the algorithm. And maybe things work. Maybe they don't converge. But any case, you don't But we're not going to do that, little bit more math, because in stuff like this. We're interested in the fact linear sum of the samples. So we're going to work a little And in particular, now that this one right here, we're there, and we're going to plug happens to that thing the extremum of. Is everybody relaxed, Actually, this is the This is just doing a little So the think we're trying minimize is equal to 1/2. And now, we've got to here in there twice. Right? Because we're multiplying So let's see. We've got from that expression will just be the sum of the the vector x sub i. And then, we've got the So that's just going to Now, I'm going to, actually, sums together into a double the indexes straight. So I'm just going to write sub j, x sub j. So those are my two vectors and product of those. That's the first piece, right? Boy, this is hard. So minus, and now, the next term i, x sub i times w. So you've got a whole We've got a sum of alpha i times and then, that gets multiplied So we'll put this like this, the sub j in there like that. And then, that's the dot That wasn't as bad Now, I've got to deal with the sub i times b. So that's minus sub of alpha And then, to finish it off, we i minus 1 up there, minus 1 in as the sum of the alphas. Are you with me so far? Just a little algebra. It looks good. I think I haven't Let's see. alpha i times y sub i times So pull that out there, and alpha sub i times y sub i. Oh, that's good. That's 0. Now, so for every one of these whole expression. So that's just like taking this those two things together, Oh, but that's just the same So now, what we can do is we this Lagrangian as-- we've got that sum of alpha i. That's the positive element. And then, we've got one of So that's minus 1/2. And now, I'll just convert that sum over both i and j of alpha times y sub j times x sub We sure went through a lot of we've got it. And we know that what we're to find a maximum of And that's the one we're the numerical analysts. So if we're going to had this anyway, why did I go to Good question. Do you have any idea why I Because I wanted to find out expression. Wanda is telling me. I'm translating as I go. She's telling me in Romanian. I want to find what this respect these vectors, the And what I've discovered is that only on the dot product And that's something we That's why I put it Now, up here, so let's see. What do we call that That's two. I guess, we'll call this This piece here is four. And now, there's Because I want to take that w, into that Lagrangian, I want decision rule. So now, my decision rule with to be w plugged into So the decision rule is going to i times y sub i times x sub vector, like so. And we're going to, And we're going to say, if to 0, then plus. So you see why the math is Because now, we discover that depends only on the dot product vectors and the unknown. So the total of dependence math on the dot products. All right. And now, I hear a whisper. Someone is saying, I mathematicians can do it. I don't think those numerical optimization. I want to be sure of it. Give me ocular proof. So I'd like to run a OK. There's our sample problem. The one I started the Now, if the optimization a local maximum or something, straight line separating those street between the minuses So in just a couple of steps, there in step 11. It's decided that it's done optimization. And it's got three alphas. And you can see that the two into the solution, the weights are given by those little So the two negatives participate one of the positives, but the So it has a 0 weight. So everything worked out well. Now, I said, as long as it maximum, guess what, those can tell us and prove thing is a convex space. That means it can never get So in contrast with things like have a plague of local maxima, local maxima. Let's try some other examples. Here's two vertical points-- no surprises there, right? Well, you say, well, with diagonal points. Sure it can. How about this thing here? Yeah, it only needed two of the plus or minus, will Let's try this guy. Oh. What do you think? What happened here? Well, we're screwed, right? Because it's linearly bad news. So in situations where it's mechanism struggles, and down and you truncate not making any progress. And you see the red dots there So you say, well, too bad for it's all that good anyway. But then, a powerful idea comes stuck switch to another So if we don't like the space gives examples that are not can say, oh, shoot. Here's our space. Here are two points. Here are two other points. We can't separate them. But if we could somehow get them we can separate them, because other space, and they're So what we need, then, is a us from the space we're in into more convenient, so we're transformation phi That's the transformation. And now, here's the reason I said, that the maximization So all I need to do the transformation of one vector of another vector, like so. That's what I need to maximize, maximum on. Then, in order to recognize-- where did it go? Underneath the chalkboard. Oh, yes. Here it is. To recognize, all I need So for that one I need phi of And just to make this a little notation, I'll call that And that's x sub i. Those are the quantities I So that means that if I have a sub i and x sub j, that's equal with phi of x sub j. Then, I'm done. This is what I need. I don't actually need this. All I need is that function, k, a kernel function, which product of those two vectors I don't have to know into the other space. And that's the reason that So what are some of the kernels One is the linear kernel that 1 to the n-th is such a kernel, it and v in it, the And this is what the dot product So that's one choice. Another choice is a kernel this, e to the minus. Let's take the dot product of those two guys. Let's take the magnitude divide it by some sigma. That's a second kind of kernel So let's go back and see if we transforming it into another perspective. So that's it. That's another kernel. And so sure, we can. And that's the answer when original space. We can also try doing that radial basis kernel. That's the one with the We can learn on that one. Boom. No problem. So we've got a general method to produce a global solution. We've got a mechanism that this into another space. So it works like a charm. Of course, it doesn't remove Look at that exponential If we choose a sigma that is sigmas are essentially shrunk points, and we could So it doesn't immunize us does immunize us against local with a general mechanism for another space with a Now, the history lesson, all It feels like it's younger Here's the history of it. Vapnik immigrated from the States in about 1991. Nobody ever heard of this stuff He actually had done this work idea in his Ph.D. thesis in the early '60s. But it wasn't possible for him because they didn't have any anything out with. So he spent the next 25 years at the Soviet Union doing Somebody from Bell Labs over to the United States decides to immigrate. In 1992, or thereabouts, Vapnik NIPS, the Neural Information All of them were rejected. He's still sore about it, So around 1992, 1993, Bell hand-written character and in neural nets. Vapnik thinks that what would be a good I can think of the vernacular, they're not very good. So he bets a colleague a good machines will eventually do recognition then neural nets. And it's a dinner bet, right? It's not that big of deal. But as Napoleon said, it's for a bit of ribbon. So that makes colleague, who's handwritten recognition, decides vector machine with a kernel, slightly nonlinear, works Was this the first time anybody Vapnik actually had the idea in it was very important. As soon as it was shown to work problem handwriting recognition, resuscitated the idea of the and became an essential part of support vector machines. So the main point about this between the concept and anybody It was 30 years between Vapnik's kernels and his appreciation And that's the way things often by long periods of nothing epiphanous moment when the great power with just a And then, the world And Vapnik, who nobody ever becomes famous for something today who does machine