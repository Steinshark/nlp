&gt;&gt; Welcome to IBM THINK 2023! &gt;&gt; AI generated art, AI, what is that? It sure is a lot of fun. But when foundation models are you need to think bigger. Because AI and business needs to Built to be trusted, This isn't simple trained to do one thing. This is AI that is work across your organization. This isn't committing This is hybrid ready AI that This isn't wondering This is AI that When you build AI into the can go so much further. This is more than AI. This is AI for business. Let's create. (MUSIC) &gt;&gt; Please welcome Director of Research, (Applause) &gt;&gt; DARIO GIL: Hello. Welcome, welcome to the And I understand some How special. So, I hope you've enjoyed And what an incredible You can really feel the change And there's just no denying that continues to be exhilarating now so clear for all to I am just fascinated by AI. And as a technologist, this comes about only maybe once And I am just thrilled to see technology is going to enable. Because it's really going From customer care to and logistics to medicine, energy, to the automotive communications, you name it. It's really going to impact and really touch every So, it's really exciting. And while sometimes the pace you know, daunting and scary, foundation models and generative opportunities are immense. The emergence of foundation really a defining moment. And we need to recognize to capture the moment. And my advice is, don't Be an AI value creator. Just think about it, as an to just prompting It's not your model, you model or the data. Just think carefully about you want to live in. As an AI value creator, on multiple entry points. You can bring your own data choose from a library of You can train or influence You can tune, you can have over the governing You can prompt it, too. Instead of only one model, you And through this creative and you can make them your Foundation models that are become your most valuable asset. And as a value creator, you value that they will So, don't outsource that. You can simply control your So, let me show you how we a value creator with Watsonx. Watsonx is our new integrated It consists of three primary our massive curated data tapped to train and fine-tune data management system. Watsonx.ai, this is an validate, tune, and deploy foundation models that provide And Watsonx.governance, tools to ensure your AI Watsonx.data, Watsonx.ai, work together seamlessly lifecycle of foundation models. And true to our commitment to Watsonx is built on top Not only does it provide Watsonx components, it allows your AI workloads in any IT they are located. WatsonX is the AI platform And look, I don't need to technologies is not easy But the platform changes that. So let's take a look now at how end works in the platform. The lifecycle consists of train the model, validate the applications and solutions. So let's start So say you're a data scientist is in a Public Cloud, some that another external database, or in anywhere else outside your So you access the platform invoke Watsonx.data. It establishes the necessary sources so you can We have been building our IBM collected from public sources We are bringing data from code, academic sources, We have used Watsonx.data to dozens of domains to produce use to train foundation models. And besides the raw data and clients to bring their own data purpose-built foundation models. It is all stored in .data. With granular metadata that for each file or document. So, now we took this and we move First, we identify the of the data. Then, we need to categorize example, in a pile for different Spanish, German, and so on. A pile of code data that we language, Java, Ansible, And any other Now we filter it, we rid of duplicated data. Now identify hate, abuse, and we remove it. We filter it for private constraints, and data quality. By annotating, we allow directly determine the right Having done all of that, for the next step. We version and tag the data. Each dataset, after being receives a data card. The data card has the name specifies its content, been applied to it. And any other relevant content track different choices that of of the data that we have used to Now, we can have They coexist in .data and access for different purpose So, we are now ready to training our model. This is step 2 in So, we move from .data to .ai architecture from the five These are bed rocks of encoder only, encoder- other novel architectures. Let's pick the encoder-decoder pick a target data pile version .ai allows training with the hybrid cloud. In this case it Vela is the first of a kind that we built last year. It gives you bare metal virtualization overhead And we are making it Watsonx.ai auto scales training being done. And the first thing that we data according to the So, we first query the data the pile we want to use. That materializes a Vela for tokenization. What this means is that, for large language model, the broken into tokens. And this process can And we use the tokens Now, training is a very complex It can require dozens, GPUs and can take days, Training in Watsonx.ai open-source technology out there Built on code flare, also integrates Hugging variety of open formats. Once training is done, the So for each model we train, benchmarks to evaluate a wide range of metrics. Once the model passes all benchmarks, it is packaged For each model, we create a the details of the model. We will have many on different piles, with Next, we go to the data card that has the for the data pile that was used card that has the detailed was trained and validated. Together, they This fact sheet is cataloged fact sheets for all the models Now let's go on to tune the what we mean by that is to adapt which is the basis for the large afforded by foundation models. So, say, in this case, you are the application developer. So, you can access Watsonx.ai from the catalog to work with. We have a family of for different domains. But we also have a rich set believe in the creativity and in the diversity of want to bring that to you. In this case, we pick language models, which is the We set up the options for We pick summarization base model to use. Now, we can access and use tune the base model and for whether that business data is hybrid cloud platform. So, now we send prompts and to tune the model in .ai. You get the outcome of This process happens back many times, and in the end, ideal prompts to use. The model is now specialized This is the final step The application where you want live in the public cloud, it can And you can really foundation models efficiently And the deployed many different applications. So, for example, we've in Watson Assistant. For text generation in topic that you want the generates the corresponding We have an inference stack the model in applications. It consists of state-of- been field tested for This is how Watsonx allows us to trusted, governed, deployed we can scale that model Once models are deployed, we and update them in We call this constant process At Watsonx.governance monitors change that may impact how the be driven because we be leveraged or there's a or law or data licensing. Any change detected by the process the update to both The idea of the model factory and proper governance of AI. Now, all of these updates disrupting the underlying the foundation models. And this data and model factory We have already produced over 20 language, code, geospatial different sizes of models billions of parameters. We have infused these products, Red Hat products, At IBM, over 12 foundation library, which is used in is available to ISVs. Granite models train over Code Assistant, which has Ansible Automation Platform. And as you heard earlier in this and is infusing foundation So, Watsonx is really ready for Now, to maximize what you can disposal, we believe that Because, the truth is, one And with the innovations and the open community is super will be able to create. To be true to our belief in the the open AI community, we are partnership with Hugging Face. So let's invite to the Hugging Face, Clem Delangue. (Applause) &gt;&gt; CLEM DELANGUE: Hey, Dario. &gt;&gt; DARIO GIL: Clem. &gt;&gt; CLEM DELANGUE: &gt;&gt; DARIO GIL: First of We are just delighted So let's begin by, tell us and how and when you got Hugging Face get started. &gt;&gt; CLEM DELANGUE: Yeah, I, actually, started in I look at the room at the time Maybe it would have been in the room at most. As a matter of fact, we at the time, we were I was working at French company from my accents - and we on device, on mobile. The company went on to get But I never lost my passion So, seven years ago, with my gathered around this passion for right, what you see on We started with something We worked on conversational sometimes happens for startups, technology ended up more When we started to release part see open-source contributors scientist sharing models in Hugging Face is today. &gt;&gt; DARIO GIL: So I mentioned the open community creating in AI. Just share with us some How much energy is there in that expect in the creativity &gt;&gt; CLEM DELANGUE: Yeah, the is insane these days. Just a few weeks ago I I tweeted that I would be sort of a small get-together We thought we would get maybe a And the more the days came, the We had to change locations the end almost as big as People started calling so that's just an example. We are competing with Just proof of how vibrant the We think the same thing Since we started on the platform having over 15,000 companies very large companies like Bloomberg, all the way down Grammarly, for example. And collectively they have models on the platform, 100,000 open demos. Just last week 4000 shared on the platform. So, that shows you kind of like open-source AI community. &gt;&gt; DARIO GIL: Just think about So, one of the myth busting about is that the element rule them all, right? There's going to be a huge happening from so many sources. So, perhaps, you could share of innovation that you see? We have seen scale. But what are some examples eye or you think were &gt;&gt; CLEM DELANGUE: Yeah, I mean, the release of ChatGPT, said, okay, ChatGPT is a 100,000 new models have been And, obviously, companies, to train models, right? They would prefer not to money to train models. But the truth is, if you look at build smaller, more specialized, cases, they end up being efficient, and they end up being Just the same way every single to write code, right, and to their competitors or than We are seeing the same Every single company needs to their own models, learn how Every single company needs to if they don't, they won't be won't be able to create the they have been building for control, right, if they That's what we are seeing ecosystem as a whole. &gt;&gt; DARIO GIL: It's back to the tuner user, right, be a value So let's talk about our Why are you excited about this community into Watsonx, enterprise, you know, need and that are here listening? &gt;&gt; CLEM DELANGUE: Yeah, share a lot of the same DNA, open platform, kind of, tools for companies. For me, one of the most iconic last decade is IBM plus Red Hat beginning of it, but with do the same thing for AI. I think with this integration Face, you kind of like get the sense that you get the cutting the numbers of models, Hugging Face ecosystem, and supports of IBM, right? For example, you mentioned, The IBM consultants can help you at the time that is going to So, you really get, kind of, what we were saying, meaning build your own internal ChatGPT. &gt;&gt; DARIO GIL: So, tell I am just delighted about So tell us a little bit about when you look over the excites you the most? &gt;&gt; CLEM DELANGUE: Many, many We have seen a lot of adoption, text, for ODO, for image. And now we are starting to see for example, we are seeing We are seeing a lot of seeing a lot of time series. We are starting a We are excited about it, we default to build all features, It's kind of like new So we are excited for this Also, we are seeing a lot of models and, in fact, at Hugging transformer agents which is a build more complex systems like better capabilities. These are some of the most excited about. &gt;&gt; DARIO GIL: So a lot Thank you so much. Congratulations. &gt;&gt; CLEM DELANGUE: (Applause) &gt;&gt; DARIO GIL: Thank you. So, while you saw how the foundation model creation And we talked about data, architectures, the computing themselves, the importance So, now let me show you would experience Watsonx. And we are going to go inside And from the landing page you fine-tune models or deploy and So here's an example of how do a summarization task. You give the model the text summarizes it for you. In the case of a customer care customer problem and the transcript of the interaction. In the tuning studio, as we parameters for the type of and the base model and The studio gives you detailed allows you to deploy the tune It's that simple. We took the complexity of need to worry about creating And here are some of our SAP will use IBM Watson digital assistant in You have been hearing about Red Watson Code Assistant into the BBVA is bringing their their own foundation model Moderna is applying to help predict NASA is using our language models we have created scientific understanding climate related issues. And WiX is using foundation for customer care as they meet So, what I encourage you is to of value creation with AI. A year ago, I stood on a stage And I shared with all of the AI was foundation models. And maybe at the time it seemed know sort of, like, this where things were going. But, boy, what a And it has been a big So, as we close our event this all of the things we have We have announced Watsonx, a allows you to create and you can move with urgency We announced a set, a family of IBM models, open community create your own models. We announced our data model across multiple domains to create our family of foundation continuously updates them when regular cadence of models to We told you about products foundation models over 15 of Red Hat products like Ansible products like ACP solutions. We announced important bring it to the enterprise, collaborations and initiatives We showed you some of the AI value creators with us. We are bringing IBM Vela or to train foundation models while giving us the And we announced that available as a service. Last year we launched It's an engineering marvel have on chip accelerator It can process 300 with one millisecond latencies. This means now you can infuse in Z16 for applications others in real time. Using the same core architecture Research AIU, which is optimized foundation models and enable And at IBM Research we AIU systems designed and inference, and tuning. So a truly fantastic year and the amazing things that we are and that we will be sharing So, today more than ever before, business strategy in AI. And in closing, as you think models for your business, let me First, act with urgency. This is a transformative bold and capture the moment. Second, be a value creator, data and under your control. They will become your Don't outsource that and strategy to an API call. Third, bet on community. Bet on the energy and open AI community. One model, I guarantee you, Run everywhere efficiently, latency, and cost by building And finally, be responsible. I can't stress this enough. Everything I have mentioned responsibly, transparently, heart of your AI lifecycle. Continuously governor the data And co-create with trusted ultimate license to operate. If you map your AI business recommendations, you will be in things with foundation We have built Watsonx so And I hope you join us, because on this journey with you. Thank you. (Applause)