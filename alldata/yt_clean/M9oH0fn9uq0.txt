Hi, everyone, and welcome It's a pleasure to introduce Gourav Sukhatme. Gourav is Professor of and Computer Engineering at California, and he's He's the Director of the USC and the Executive Vice School of Engineering. And he was a Chairman of the from 2012 to 2017. So Gourav got, also, And his research interests, are spanning networks robotics, as field robotics. We're actually in which we spend a lot of time At least the students So got plenty of He's a fellow of He's a recipient of an NSF Foundation Research Award, Award. A few more cool things. Gourav is actually one of the and System Conference, He was the Program Chair of and he's going to go back and So RSS 2025 he's going So thanks for hosting it. So finally, he's of Autonomous Robots. Gourav, thanks for joining us. Thank you so much. [APPLAUSE] All right. So thanks so much, Luca, and thank you all for coming. And so I want to about a line of has to do with seeing end-to-end for a class of that people already And so the idea is, can you of end-to-end RL for a multi-robot reinforcement, problem? And that's the exploration we and so I want to tell you And so this talk will This is a highly resource Several of you probably have This is the Crazyflie. It's easy to buy, easy to fly. It's tiny. The amount of space is measured in kilobytes. And of course, there's no It's typically run a The one nice thing is it has So you can do things at provided your control loop that you want to do in it. So this is the platform. So many years ago, seven in collaboration with my group and I were quite active for swarms of quadrotors. So we would work on So there is a group of these. There's 50-odd of them. They are somewhere in space. There's some set of obstacles and they are supposed to end up This is a And this instance of the was where, at planning you know where And so the question generate a plan that to go in some efficient not hit the environment, And the &quot;quickly&quot; is the vision is if the environment an updated map quickly hence the &quot;quickly.&quot; And so we had a series you do this in ways that account above another because it and how you do this efficient as an instance of a And broadly speaking, and then you do some smoothing that we had published So we sort of knew but there's obvious which is it's a setup designed the environment looks like. That's the fundamental And whether you do that fast and or whether the and you don't need the fact is the is build a representation and then plan on the And this is a paradigm We've done it for many years, It scales well. With modern solvers, do this for large et cetera, et cetera, et cetera. The question that I had in was, could you do this by Could you actually say locations in an vehicles to go to some and they just do it irrespective I don't say do it efficiently or even provably safely, but can where you start to feel that, there? So it's a slightly It's not quite exactly but it's close to the original And I don't know anything and so this was my to learn whether a at least a slightly relaxed could be solved The problem is also different because in the planning is there's a low level flight and you reason at the whereas in this to assume that there And so we're going and we're going to learn and we're going to learn not All right. So it's almost the same problem. So as is usual in the answer is, yes, And there are videos that and these will appear So this is the question. There's a further constraint we And also, can you do this with So in other words, the data appear from flying So that's an It's a self-imposed constraint. It need not exist but it's kind of to have because, as the it's becoming quite to really worry about So those are the constraints. The other thing is that we wanted to not on an abstraction of but by directly So each vehicle has four want to control how Not like a point, which you there's some lower that converts that to thrusts. The attraction was, A, to and B, to see whether the if successful, are in some way one may program, or that might a planning system on top that you understand. So that was the So our first cut at this was and just learn how to And we learned how to fly a quadrotors of different moments and so on and so forth. And it turns out simply by doing learning and extensive we're able to do very good and trajectory tracking. So there's no navigation The vehicle is just asked to You disturb it [? wildly, ?] This is all happening Subsequently, we have shown how on. But these experiments And so there are There's a single robot only. There's no navigation here. But you can learn and you can learn it completely And I'll show you a little what the details of that are. So this was our building block. And so the question is now, can that I'm interested in do coordinated flight? And can you avoid hitting So I'll tell you today is how of the learned network-- how much we think how we've stress-tested it and And if you're a choral paper from two and then there's a that I'll tell you about So our view of the and I mean that literally. So each robot just sees the as part of the environment. There's nothing fancy going on. Each robot commands which are the speeds of the Stuff happens in the gets some reward back. I'll tell you a little bit And then we close the loop in So the state is composed Each quadrotor measures the and the goal that it The assumption is is happening in a that they all are registered to. Each quadrotor has access It has access to its so it knows its instantaneous It also has access to And most interestingly, it has and the relative velocity neighbors. This number k is If k is on the order of the then you've built a system in many ways. If k is small and the grows, then you that people in have known for a that for tasks like navigation you don't need to know what You should be able to without caring So if that property would be fundamentally wrong. But picking k is an and we'll return to it in So this is the world. Basically, each vehicle The interesting is who's in your neighborhood And so there is an that there is some allows you to range and pick out the ones that an ordering operation on range. As long as that separates your k neighbors from in both position and And so that is something that and more OK in others. And each robot generates as the actions. We do normalize these actions. And each vehicle is a little and so we have sort of a crude max torque and min This is on a model that is vehicles we play with, and So it's not particularly good It's not overspecialized. It's not learned. It's something that we just do So we need some to train in because the goal of training entirely So we have to somehow We have to somehow generate So we need a simulator. And so we have a textbook of these vehicles. So this is about the you can do to So easy to implement. We have a little that we wrote that But the really interesting we have a simple model in this simulator. So I'm going to tell you a what we do, and then we'll because I think it's super So when two vehicles we set their speeds in along which they collide, to So you come in. We don't apply any kind We pick two velocity vectors, to lie in the same half-plane. So they're sort of But we pick that a uniform random distribution And the speeds are and the max possible speed So this is not a This is not what happens It's not an idealized It is a simple model of a looks like a collision model. It's super important It is not a zeroth maybe it's a zeroth order but it is not even a first It's somehow less than that. It has one key and it causes vehicles to stay And that's the key because if you have any hope you have to be in post-collision One of the problems that don't cause you to see is that you don't know So this thing, for which I have It's sort of an odd data from the simulator. So on the one hand, the top and then layered on this is But it has some You can do many things, is the right thing to do. And then you need a set because in simulation, you in some state, and you need to explore. And so the way we do a set of geometric shapes that And so the red robot is and the blue robot is supposed And you arrange these and then you move these And in the simulation, swap through a bunch And over time you I haven't shown you the I will in a second-- that you will where these things That's the idea. And there's a bunch of these. They're picked to be of things we but they're not in and of themselves. And so the last thing I need is, what is the reward design? And it's not So obviously, you need some to go. You want things to There is a fairly significant But equally well, that basically penalizes and entering into this So you don't want just this You also want some as you get too And you need both these things. So you definitely and the simulator will to post-collision states so you may, in fact, have some learned. But all that being want a fairly steep penalty in And I'll tell you a little you play with these and you make the system than other states, and so on. So there are some other And then this last thing causes you to hover in place. And this is, in fact, in the previous video. So when I said we with a single doing is basically saying, sit So that's just inherited So this part is how For the first term rewarded for flying to a goal. And the two terms in the middle That's the reward function. Yeah? What's the last term? The very, very last term? It is not-- hold on. It is yaw. Do you care about yaw? We don't because they're But that being said, they're not So even for vehicles we want a system that spinning around is a reasonable good state, Wouldn't that be omega? Spinning would be omega? So the angular velocity-- so you but you also don't So you want to penalize the So you want to hold and you don't want to turn. Now, in principle, you're right. You could only and it should all work out. But it helps. And what we do is the expectation of and that's basically it. We just run a simple to basically see if we is performant for this task. Now, there are a couple which I want to tell you So one of the things you about is, now what does So there are basically The observations from go into a small network that And the observations basically go into another And one of the is we want to reshape that observations. And we perform a mean operation this is the so-called we don't care about one than the other. So we want some permutation of all k of their neighbors. Subsequently, I'll show helps to know some neighbors-- it helps to attend to some and I'll show you that as well. But this is one where all you and you don't really more important than the other. And then at the output, there And all the networks are So that's one architecture. And the problem is that it's and it's nice to capture But it doesn't of the blue robot flying yet being at the same from the green robot. And so sometimes, two but if one of them is you want to attend to it more. So you need some mechanism between things, even if And so our technique to add a small attention And so what we do here is these but the latent vectors for are subjected to k are learned over time. And therefore you pay attention and so the training takes but it's actually and you get better results So you cut at k, where and you learn to attend as opposed to all the same. So if you do this, it's what the models actually do. So there are three On the x-axis of each so this is just So we train to up to On the y-axis, in the middle to the target. On the extreme left one And on the one on is how long the in the air, the fraction that the robots stay in the air. So ideally, you want, on the all the time and On the left, you want that gathers a lot of reward. And you want the to target to really go to If there are two robots trying they'll never quite end up of what's possible. And there are four things So there is a system aware of their neighbors. This is the blind system, to compare against. There's a single MLP that we into outputs. And then there's this in the attention technique. The blind system is so that's not so important. But if you look at a little hard to tell I mean, they're a but there's not So if you look at the and you look at the deep setup they seem to navigate robots by the end of the billion And they seem to gather But that's not because if you look at a again, training on the x-axis. But now if you at certain more if you look, for example, which is the average number of task, or on the rightmost one, number of collisions, where to fly to the same you're trying to force them then you can see that the a little bit better and So if you average over a and they're not too cluttered, But when you want to crowd attending to the ones that are you to collide, does matter. So that mechanism It actually does help. And actually, we looked So if you look at the you can see that from for example, of the you can see that the is greater than gray, flying towards each other. And the relative importance So you can actually see quite nicely. So the representation meaningful in this setup. So now the question is, So you go off and train. You do this. I haven't yet shown you I will soon. But what happens if you trained with 10 robots, It's a natural question to ask. So the question is, can you and since it runs on each when there are 20, and Our setting, by the way, and so that's why to ask a question like this. So there's an interesting So on the x-axis, I will just And on the y-axis, which is the number divided by how long and how many robots there are. So I somehow want to Because obviously, if and I put 100 robots in there the probability of So I want to normalize I also want to normalize for You can get away with if you just run the thing for So we don't want to do that. So I'm going to count how normalized by these two And so the question is, grow as you squeeze into that same bounded volume? And that's what this picture The question is, if you just what happens? And that's the one which is And so pre-trained simply means where there are eight So it's a pretty considering that the whole And you learn with there's only one thing that So obviously, it's if you learn this and then that's all you can do. First of all, six And secondly, if it doesn't well then, it's really annoying. So the eight/six system does And if I just deploy there are 16 robots-- I don't do anything else. I just deploy it on the the world is 16/six, then I mean, the number of But if I deploy it on 32 robots well, things don't That's that dashed So that's this point here. And then pretty much So you can't just take and deploy it on 48 robots. It's essentially useless. Things just collide don't stay in the air very OK, so you can do a couple The two obvious things can you do some fine-tuning? So you have this system. Can you come in with So you trained with eight/six Can you do a little bit in a 32/six? But the number of you're allowed to take is not of steps that you trained Can you add 20% more time, So that's what the And so with 200 of training in the new As you can see, that's So that's this It ultimately does break. Things don't work quite as well. Training from scratch more and more complicated but it pays off at some point. And that's the other line. I'll come to you in a second. So that's the obvious is just forget about all this. So tell me how many I'll go off for a day. I'll just train for a million and then I'll be fine. And that line actually But it really pays off only So there's a lot of value to but fine-tuning has limits as care about and the system Yeah? I'm just curious, if you have or 128 robots, and a I wonder if you're going Yeah. You'll get exactly that. You'll get exactly that. You get cascades. And that's not nothing that looks forward in something different. We didn't change-- so as it gets denser want to actually the pre-collision states, and We're pretending the same at 128 robots as it was with six robots. And clearly, you want to learn So yeah, you're right. That's exactly what you get. You get things bumping So pre-training and fine-tuning Is that because of the That's it. Yeah. So you get 200 million And you get some It's not entirely without use. You get some mileage out of it. But it's not forever. So you can't come have a drastically All I need is a little At some point, training from Yeah. Yeah. It's a little bit It's not super important. Like what does it really mean into a room like this? There's no particular reason to. This is a little to see, when you separate on from the thing how does the system behave? It's mostly that. It's not really that somehow into a bounded volume is but it gives you a nice Right, I already said all this. OK. And so the kinds of look pretty in simulation. You can tell robots You can move the goals around. The robots fly to the new goals. You can do things like tell where the goals and the robots will These are all simulated results You can do crazy things, to fly to the same get behavior like this, then use fancy words for it. They call it emergent like that, which is fine. Whatever. [LAUGHS] It's nice, but I'm the interpretation for it. The interesting thing is is you can take-- actually, in this case, we're because we ran out of space mechanism online. But you can do very So I forget exactly-- I wrote how many network but I think this one has 30-odd and the neighbor It's like 1,500 These are fully connected MLPs. We're running at about And we're just basically to go some start goals and the system is running. This video is fully in Vicon. We now do this, we resort to because we are actually on the ground. So you can actually fly a fairly but these are in Vicon. The other interesting is that you get interesting So occasionally, they fly and you get two or They'll bounce off the So if you do control barrier then this is death. This never happens. You can provably show that And in any system where this is not tolerable behavior. But if your application is of setting, then it's not an It's something that this so you can take And RL is good at of these little things. This is where the collision It turns out that happens when these vehicles There's no way to be able to accurately But it turns out that for You need to just about that some post-collision states and you learn how to So this is an interesting For projects where, this is not an important thing, to just observe that RL is We did some baseline so we looked at standard There are some nice ways collision-free so we have compared against So if you are allowed to pay you can construct a And within the boundaries And if you know the maximum can move, you can at max speed towards takes you closest to your goal. And this method is well-known And in fact, it comes so it's a great method. And of course, it works. If you look on the left, of max BBC implementation. And on the right is And they're doing There are four They are in some They're told to swap sort fly through each other. Qualitatively, you fly a little differently. You can see that. You can see that in the results. You can see on the is acceleration in both sides. The upper panel is velocity. You can see we fly So our stuff is on the right, And there are two The learned controller flies and also, the learned doesn't give a safety guarantee. So it's important is to be taken in that spirit. Not A is better than B. You can also do weird in the midst of these as you can track the object, You get very fast from all kinds of disturbances. Now, admittedly, with these days, you can do It is not the case that of the environment and then has gone away, by any means. But these things are quite without going through So the question is, everything things lying around, but And so if you want to navigate to some end locations-- my had some obstacles, and and you plan on the map. But as you discover will you fly around them And yeah, I mean, this that we really care are there other moving but there are also in the environment and you discover And in this case, what you have and to nearby neighbors. And I will not go because the basic There's a couple of So our obstacle representation didn't use signed So we use them now. And our attention module so I wanted to show you So how we really do this now is for the state of and two for the static things in the environment. And then we have a that tries to learn the static obstacles and which So those two are learned And then there is a simple that emerges, which fed to an MLP that generates the And this is trained with one change, which is we do that's actually quite useful. And it's a trick that many which is what we want some mechanism to amplify And we want to enhance So what we do is we the states leading And we make sure that see those states and we discard them when they've been seen a and that's a tunable threshold. And this mechanism helps a lot. So it's not like collision to be the Holy Grail here. It might be an amusing artifact. You still want to generally not running into each other. And this is a kind of It works very well. I won't go into the but these things, it So you can take some of the And you can take this multi-headed and you can take out and you can show contribute in a meaningful of the system. So they're significant. Yeah? Before you introduce 1820, yeah. Yeah. I didn't remember So it goes up a little. Yeah. How are you actually Yeah. So now there is a that is on the Crazyflies, and we use it for We have no way in practice of And so what we do for the We expose the positions to a centralized place, and we to the vehicles that don't and only send those to each vehicle. So there is no range It turns out if the range because you can corrupt them We also introduce dropouts, don't get range measurements. But yeah, for this there is not a good ranging sphere around it can do ranging. So the measurements They are physical from a ranging system. That's a good point. Yeah. Yeah? This is a ball range, So there's no there is somebody deliberately No, but like you saw me throw You can do that. The system depends on ranging to That's it. That's the entirely myopic view. So somewhere there's a goal. I know my current state. Left to myself, I is the behavior Everything else That's it. That's all that's happening. So there's no game theory. There's no adversaries. There's no planning. Its gradient descent. Left to myself, I and if I can't, then it takes to be rewarded as being and I will generate control And it turns out that Yeah? Can you get caught, Yeah. You can fly down an alley. We've absolutely tested They have no notion of anything. You don't build so you have no idea, so But there is enough that occasionally, things will-- Yeah, you can get It can happen. So this is the biggest is they exhibit They're curiously learnable. They explore parts that you wouldn't see otherwise. But they are not They are not designed Now, there are other and we have some new and other people have But as described, they address but not some of the other Yeah. Yeah. And empirically, compared to learn They do pretty well compared certificates, in terms of by how goal-achieving they are. So you can measure a You want it to go from A to at going from A to B. But of course, in another axis, which is And ideally, you want a system better than any other And with many of the they're not as live when the paramount, so if you densities, for example. If you have the luxury of well, your goal-achieving So in dense settings, you But it's a slightly different than somebody who is interested So take the comparison Yeah. I don't know if happening in the small but that's a top-down view. You fly in extremely This is obviously one can get very nice results. There's lots of robots flying Things, of course, are a lot But they work pretty well. Here's eight vehicles The obstacles are This is a small network. It runs very fast on board. And small memory footprint. Single-threaded control We're not even using the that we could use it at. And fairly stable and All our experiments are on the These things, generally, are not really well-suited to So these are small, All right. OK. I'm not going to try and draw but I am actually now to systematically generate because this was a can you do something for this particular problem? The answer is, yes, What was more interesting with very small networks. Maybe that shouldn't because after all, you need to fly one is not a lot of lines is some fairly compact to be learned. And maybe it's that you can learn to me it was surprising. I think how you simulate and maybe for other robotic You need to simulate so you can can actually-- I mean, this is doing 20 hours of a new controller up and ready. So if you change things, and the next day you can play I mean, on real machines, These were not But you need to get the So if you have a you can learn end-to-end And for the things it's important not It's important to about otherwise Abandoning them is no don't learn to recover from see in a learned system. We've tried, by the way, We've tried doing idealized They do not work as well So that's an interesting, It sits somewhere between having structure you understand, And it's good, and We expected it to be good. And then the middle ground. And so you can get We spent no time on with our version of PPO. We spent no time on with any of the other tinkers with when one We did tinker with k. How many neighbors And in fact, we have a and this is a known that depending on exactly people seem to And it's not super surprising. If you sit at the there are six faces around you and after a while it So somewhere there We picked six after We have done it with as few as two neighbors It doesn't work well obviously, so it's not of robots. So that's another thing. In fact, the that I showed you equal to 2, because we couldn't to that many neighbors. So there's some I really don't think you but if you can. So we have a line of work in like this, and I haven't for that line of work. We have a separate where we continue This work I described end-to-end learning very much like the original We have a new line of work learn small control policies. So if you have me back, someday That is becoming an So if you had a task that you in fact, from a set Assume you're doing this with one latent is a fully connected MLP. If you have so many can you tell me, which the best such network? And there's some work that proposes this question. We've adapted it to some and we have now And we also have some on potentially finding with finding the right network. So ideally, what you want to propagate the and use it to tune both the that you're learning. And we have shown recently that so we're pushing ahead on that. But I haven't lost my So we have a paper, in TRO, which is very related to There are all kinds brushed away in today's talk. How do you deal with Everything in my world, in the assumes that that's What if you do want which is a You can't just say, so I'll stop worrying What if you actually want to where the number is substantially larger than So there are all kinds where you have to deal with all conditions that weren't here. The other is, what if just asynchronous In my case here So there are many settings in so we have some line of So yeah, there's lots to do here in So I'll stop here Thanks so much. All right, we'll get started. I have a quick question. Why limit it to only 20 neurons? Are you using a microprocessor? Yeah. What kind of processing None, basically. This is a setting which is-- yeah. Yeah. I guess my question is like-- Tiny, little computer, yeah. So we have only like 20 neurons. A gradient optimization might Any thoughts on or an evolutionary strategy? Yeah, they may work They may work better. It's entirely possible that the with such small networks have such a good model parts of this problem in the and that's why this It may not be for because yeah, once it's not super clear that you of gradient descent. But we observed good behavior. It seems to work. Thanks. That's a good point. Another quick question Talking about end-to-end, you have direct state Is there any hope to get pixels Yeah. I'm less interested am more interested in so we are slowly But does it mean, like-- For positioning. Yeah. A series of model-based Even if I have to have a camera and I have an IMU integrate, have to learn to stop and use I'd be happier with that. That's already getting We are close. We have some results paper that show that. But we're not fully there yet. So what we do is we actually in 20 measurements to get a fix. In that case, do you have to capture the [INAUDIBLE]? No. No. No. No, what we do is we just measurements in simulation. So no simulation of any So we're very comes to those kinds of things. But yeah, that is one place you have to get rid of some that we've made. When I gave this talk they said, this is charming, have the luxury of not actually It's a fair criticism. It's a fair criticism. We have some other work on Maybe a follow-up So you see, essentially, the to capture all the like the transfer I imagine there are many modeling in simulation, from the propellers, like drag and so on. Correct. Are these coming out? Like the real transfer are randomizing are picking up on these things? Yeah, both. A lot of the mileage comes So things do fly And what they see is And they see it often what are the right when you get this kind of-- they learn disturbance Now, it doesn't mean that If you blast them that they've never seen, you and so on. But within that envelope, That was my first video, of my robots, and so on and so forth. And they learn fairly large primarily through just collisions, all kinds they find themselves in. Yeah. One more. But they won't learn each other to disturb it? No. They don't disturb, right? They don't. They do not. There's no foresight. Correct. That's exactly right. So that's exactly like This is the thing that-- I guess that's why If you had sensing [INAUDIBLE] Yes. Maybe. Yeah. Yeah. One of the things about these is that they're They make no prediction They implicitly but they don't explicitly make Yeah, just to feel like they could that they avoid collisions. It would just have Yeah. And you, in a way, You're already penalizing for And so somehow, also penalizing for being Not quite explicitly, happens when you're stacked. Yeah. New vectors. Yeah. Yeah. That's the other is as you add more and people start to smile, get bigger and bigger So already, some of you-- look, you write the You write a reward function clearly spells out what Some may argue, what is left? But I say that about I mean, you penalize for the So it's not entirely but you have to hesitate terms to the reward function. I mean, we are conscious quite a few terms there. I answered Nick's That last term is the full It's not just yaw. I'll tell him when So I just had a question Suppose you were would it still train or does right at the beginning? You don't get long as a result of getting rid What you get are collisions that in states where they they land upside down on the So if you want to give to the goal within the hope is that and they land the right or they fly all the time. What you don't want is to have And so if you don't you see many more of those. And so then what is you just end up with success That's the real problem. It's sort of gradually giving to converge? We don't do this in any if that's what you're asking. Yeah. I can imagine So last question. Thanks, Gourav. I have a question for the or it's not dynamic. Do you think it's good-- so to model the creation, is after collision in the You mean, would help you recover from Yeah. Yeah. Our analysis suggested no, was billiard ball physics. So we tried with some and we tried with a completely collision models we have tried. And in fact, the performance We haven't tried anything else. I see. Thanks. All right, we're so let's thank Gourav again. Thank you. [APPLAUSE]