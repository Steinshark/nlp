Hi there! are discussing the Swin Transformer. and is basically a vision transformer variant but So, if you are curious about reportedly beating many benchmarks, grab a cup of coffee because it Speaking of motivation for the Swin Transformer: Reading this part of the abstract It is about the "Challenges in adapting And the authors say, they &quot;arise from such as large variations in the scale of visual entities and the high resolution of This is such a computer vision Everything is big data if you are brave enough. Words in text can become too many too if one wants to handle a complete Dostoyevsky But yes, point taken from the authors here, that so funnily, while transformers with linearly one sequence, there are not enough novels written so yeah... text transformers usually is usually processed at a sentence-level. Anyway, back to the Swin Transformer here. It is based on ViT and to remember you can check out our video on this. Because here, we are going the image is decomposed into 16x16 pixel patches, then these patches are transformed into These patch vectors combined are processed by a transformer in the same word vectors, which we also But, there is a problem with the here try to alleviate with the Swin Transfomer: the problem is related to a step which is particular to images, not to text. If the image is let's say... 256x256 pixels, would lead to 16 patches, thus 16 But an image of 1920x1920 pixels would which is still ok with bigger And if we do this to solve tasks is not too coarse, then ViT is still ok. And this is the case for tasks that are, in case with image classification, where the goal Analyzing the image through big But there are tasks, where one really needs Such a task is semantic segmentation where single pixel in the image, In this case, 16 by 16 pixel patches are way every pixel as a token, but this For example, a 256 by 256 pixel transformer handling sequence a Full HD image of 1920x1080 would require This is not scalable, and we native deep learning processing to 4K images Anyway, so now we've seen that ViT in order to keep the sequence But this is problematic for tasks that So, let's look at how the Swin The Swin Transformer still relies on patches, with it, the Swin Transformer first starts with then merges them into bigger ones This reminds us of something, Ms. Coffee Bean... Yeah, of U-Net and convolutions. So the Swin Transformer takes in the image Each patch is still a colored image, with three channels. So a patch has a which is then linearly transformed So the only difference to ViT so But what about this size C, and what does it mean? Well, C determines the capacity, or So you maybe know that there is a BERT-base text model, which has a dimensionality Well, in that case, C is 768. C is the capacity of the the parameter size or the amount of hidden Just to get an impression about the C is 96 and 192 for Swin-Large. But where were we? At the Swin Transformer that has divided the image into and a linear transformation converting Transformer blocks process these patch vectors but attention, but with the Shifted Window based This is just a fancy name for limited over M patches (here two). So now in self attention, one patch does but only with M neighbors. I am so sure that I saw this before in another paper under a different name. Ms. Coffee Bean, do you remember where? If you saw this kind of attention before, or something very similar to it, then It was probably an NLP paper. Ok, in any case, the limited attention window basically simulates that the rest of the sequence that is not in This means that instead a quadratically scaling we now have something linear for small M. Great, so now it is time to move further The output of a sequence of N patch vectors is of course again N vectors because the Now the output is merged by This concatenates the vectors of (in the image, not in the sequence as it so from 4 of these C-dimensional vectors, we now have one 4xC dimensional vector a linear layer which is basically a linear Ok, let's simplify here a bit: If in the beginning but note that the hidden representation has been doubled in order to increase the capacity And then again, the whole process repeats: a limited attention-span transformer module is But each time the attention window is So if in the first layer, the attention was in the next layer the regions are such that patches that landed into not communicate, can now communicate at layer two. Then the resulting patch vectors are and the whole hierarchical procedure is repeated or until no merging is possible Ms. Coffee Bean has two more comments. First, regarding the positional embeddings for the all kinds of positional embeddings are applicable. But the authors found that positional the vector representation of the tokens position bias in self-attention, a method very And secondly: how good is the Swin Transformer? Well, it outperforms ViT and object detection and semantic segmentation. It's small objects in object detection where Swin Transformer and of course, in semantic However, make up your own mind about the because, you know, like the baselines are not necessarily Look at semantic segmentation in Table 3: The to the DeiT transformer counterpart A single measurement for one configuration of DeiT is not enough to see how this all especially when in terms of number of parameters, Not really a worthy opponent Make up your own mind about this and let Which vision transformer variant is the one that another variant (if any)? Ms. Coffee If you liked this content, do not forget algorithm. Ms. Coffee Bean, I am curious to know Ms. Coffee Bean? Where are you going, I did not end the video yet. Well, I guess it ended now. Okay bye!.