Python is awesome, but it has a bit of a reputation. It's kind of slow. I mean, NumPy is fantastic whenever we can vectorize things. But when that's not possible, when we can't stay in compiled C++ land and need to keep coming back to the Python interpreter, like in the case of for loops, then yeah, Python is slow. So what? Do we just have to deal with it? Are we forced to make the trade-off between code that's fast to write or fast to run? Thankfully, no. There are a bunch of just-in-time and ahead-of-time compilers that can accelerate Python. In this video, we're gonna take a look at the longest common subsequence problem as a case study. We'll implement a dynamic programming solution using native Python, native C++, and a bunch of other libraries that are built on top of Python. We'll compare their performance and examine how disruptive they are to our usual Python workflow. And spoiler alert, some of these approaches are gonna be as fast and actually even a bit faster than my C++ implementation. Let's get started. Now, the longest common subsequence problem is pretty simple. A sequence is just some stuff in a particular order. A subsequence is a selection of elements from the original sequence. A common subsequence is a subsequence of two sequences. And finally, the longest common subsequence is, well, the longest common subsequence, so this length nine subsequence as opposed to any of the other ones. Usually, this problem is tackled with dynamic programming. Here's a quick rundown of the algorithm. One, start with a big old table of zeros with the number of rows and columns equaling one plus the length of the input strings respectively. And two, fill up that table using this optimal subproblem relation. Now, here's what that looks like in Python. Pretty simple, just basically two for loops and some if statements. So let's say we've got two 30,000 element lists of 32-bit integers between the values of zero and 99. How long does it take our Python code to find the longest common subsequence? An eternity, or 256 seconds to be precise. Compare that to our C++ implementation-- just about two and a half seconds. That's a whole 100 times faster. So the question is, how close can we get our Python code to our C++ level performance? And what are we gonna have to give up to get there? To start, what about NumPy? We often hear that if something's slow in Python, we should try NumPy, right? Well, unfortunately, there's not much we can do in this case. We can switch from a list of lists to a NumPy array, but since each solution depends on the previous ones, we can't really vectorize anything. But maybe it'll just magically make things faster. Nope, 379 seconds. It actually got slower. Hold on, let's go back to our pure Python for a moment. Now, I'm a huge fan of type hints. With the help of static type checkers like MyPy, your code base will be so much more maintainable. MyPyC compiles Python modules to C extensions. It uses the standard Python type hints to generate faster, more optimized code. And as a bonus, it provides runtime type enforcement. To compile a module with mypyc, we can just do mypyc and then your_code.py in the terminal. If you're doing this as part of a Python package, you can use mypycify in your setup.py file. We don't really need to change anything to use mypyc. We just need to make sure we have type hints to get the most out of it. In this mypyc example, we get 27 seconds. That's a 9x speedup, basically for free. And because mypyc targets general purpose Python, I think it's a great tool to have in your belt. Just try it from time to time and see what it does for your code. What can it hurt? Another way of speeding up Python is to write some of your code in Cython. Cython is an ahead-of-time compiled language that's basically a mashup between Python and C++. It offers a lot of flexibility if you're already familiar with C++ and you want to use some of those C++ libraries within your Python code. But can I be honest for a minute? I don't know C++. Thankfully, I don't have to to get some of the benefits of Cython. Starting from our pure Python implementation, we can actually get a bit of speed up just from compiling our plain Python code. That's right, Cython can compile plain.py files. It's pretty similar to mypyc. At the command line, we can run Cythonize your code.py or we can do a similar trick in our setup.py file where we import Cythonize from Cython.build and add it to our extension modules list. Our example runs in 93.5 seconds or about a 2.7x speed up from our native Python. But we can do even better if we're willing to give Cython some more information. If we import cython and use the cython.locals decorator, we can tell Cython which variables are local to the execution of our program and what their types are. That gets us all the way down to 28 seconds, which is on par with the 9x speed up we saw with mypyc. We can eke out a little more performance by adding some directives at the top of the file, getting down to 25.84 seconds. Here, we told Cython that it doesn't need to check for out-of-bounds errors or support wrapping around indexable containers. And all of this isn't too disruptive to our code. In fact, it's still valid Python. But if we're gonna let Cython do the best it can do, we're gonna have to let it be a bit more disruptive to our usual workflow. Instead of using a .py file, we can use a .pyx file. Now, this syntax is what I was talking about earlier that is sort of a mashup between Python and C. This lets us do a few things that we can't in Python, the most important of which is our dynamic programming matrix can be stored in a cython.view array. After we create that array, our compiled code actually won't return to the Python interpreter for the rest of the function. In the end, this new .pyx version of the code gets us all the way down to 2.9 seconds. And I'd say that's pretty much on par with our C++ implementation. But looking back, what did we give up to get here? I'll be honest, I had to fight with Cython quite a bit to get to run faster than my pure Python version of the Cython code. In the end, it was actually ChatGPT that helped me choose the right memory representation and get all my types right. So do with that what you will. Let's jump back to our NumPy implementation. And I'm sure you're asking, why? *Poltergeist Movie* What's happening? It was our slowest attempt so far. Yeah, I know, I know. But there's something we haven't tried yet. Enter Numba. Numba is a just-in-time compiler that works with NumPy. We can enable it by starting from our NumPy implementation, importing numba, and adding the numba.njit decorator to it. Running it, we get 2.4 seconds. That's 100x speed up over our Python implementation, and it's just as fast than our Cython and native C++ implementations. And this was so much easier than Cython. Granted, Numba has some limitations. If you use certain Python or NumPy features that are unsupported, it will just fail outright when using the njit decorator, or default back to plain Python when using the jit decorator. Although Numba is less general-purpose than mypyc or Cython, this was so easy that if you're doing anything with NumPy, especially if it involves for loops or anything that resembles this, then you really need to try Numba and see what it could do for you. But while we're at it, here's one more library you might not have heard of. Taichi is a domain-specific language built on top of Python. Its goal is to provide an easy-to-use and flexible interface for writing high-performance code. And by changing just one argument to the Taichi init function, your code can be optimized for a bunch of different architectures. Let's write our longest common subsequence solver in Taichi. First, we import Taichi as ti and initialize the architecture that we want to target. In this case, I chose CPU. Next, I made a simple Python wrapper to initialize a Taichi ndarray and pass it along with our input sequences into our Taichi kernel function. Kernels are basically the gateway between Python and Taichi. To create a kernel, we use the @ti.kernel decorator. In Taichi, arguments need type hints. We use the taichi.types.ndarray() for our inputs and the default int for our output. Next, we use the Taichi loop_config function to serialize our outermost for loop. Otherwise, Taichi would just automatically parallelize it. Finally, we swap the built-in Python max operator with the Taichi one. So this was a bit of work, but how fast is it? 2.3 seconds. That's faster than our C++ code and still resembles the Python code that I know and love. We can actually do a bit better if we're willing to make some sacrifices. Suppose that you know that you'll never have to process an input that's larger than, say, 30,000 elements long. Then we can initialize our dynamic programming table once at the module level and use it many times. When we run this version of the code, we get 1.9 seconds, which is the fastest of any of the implementations that we've tried so far. All right, let's take a step back and see just how far we've come. Starting from pure Python, our goal was to achieve the 100x speedup we saw with native C++. Using NumPy alone made things worse, but combining NumPy and Numba's JIT gave a C++-level performance with barely any changes to our code. Both mypyc and Cython's pure Python syntax achieved about a 9 or 10x speedup, which was nice, but still 10x slower than C++. It was only when we switched to Cython's .pyx syntax that we closed the gap and got to C++ speeds. Finally, Taichi was just the fastest across the board for both of the implementations that we tried. But this was just one trial, one pair of sequences. Let's focus on the approaches that achieve C++-level performance, and let's conduct two more experiments where we see how they perform under different workloads. So for 100 pairs of 30,000-length sequences, Cython was the slowest at 286 seconds, followed by native C++ at 253 seconds. Then we get to our faster-than-C approaches, with Numba finishing in 227 seconds, our dynamically-sized Taichi ndarray implementation at 220 seconds, and our fixed-size allocate-once approach clocking in at a lightning-fast 183 seconds. What happens if we flip the script, and instead of processing a few very large sequences, we process a bunch of little ones? So for 100,000 pairs of 1,000-length sequences, we see mostly the same as before. Cython was the slowest at 300 seconds. Next, our dynamically-allocated Taichi ndarray approach took 249 seconds, followed closely by the native C++ at 246 seconds. Finally, our faster-than-C approaches were Numba at 233 seconds, and our statically-allocated Taichi approach at 212 seconds. So even with these different workloads, we're still getting incredible performance. So there you have it. Python doesn't need to be slow. With tools like mypyc, Cython, Numba, and Taichi, we can meet or even exceed C++-level performance. Well, at least my naive C++ performance. I'm sure someone in the comments will tell me exactly what I could have done to make that faster, and please do. Of course, there were some trade-offs in how readable and intuitive our Python code was, but the level of speed that we achieved just blew me away. In any case, what do you think? Which tool was your favorite? Do you want to learn more about any of these? Do you already use them in your code? Let me know in the comments. Oh, and sorry for taking like six months to put out a new video. I've been trying to learn the basics of filming and lighting and trying to find any way possible to make text on a screen be visually engaging. I've got two videos in the works that I think you're really gonna enjoy. And yeah, thanks for watching.