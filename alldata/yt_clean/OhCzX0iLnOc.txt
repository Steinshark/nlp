So, artificial intelligence is known for disrupting What about ice cream? What kind of mind-blowing with the power of an advanced So I teamed up with a group of coders to find out the answer to this question. They collected over 1,600 and together, we fed them to an algorithm And here are some of the flavors [Pumpkin Trash Break] (Laughter) [Peanut Butter Slime] [Strawberry Cream Disease] (Laughter) These flavors are not delicious, So the question is: What happened? What went wrong? Is the AI trying to kill us? Or is it trying to do what we asked, In movies, when something it's usually because the AI has decided that it doesn't want to obey and it's got its own goals, In real life, though, is not nearly smart enough for that. It has the approximate computing power of an earthworm, or maybe at most a single honeybee, and actually, probably maybe less. Like, we're constantly learning that make it clear how much our AIs So today's AI can do a task but it doesn't have a concept beyond that it's a collection It doesn't know what a human actually is. So will today's AI It will if it can, but it might not do what we actually want. So let's say that you to take this collection of robot parts and assemble them into some kind of robot Now, if you were going to try by writing a traditional-style you would give the program on how to take these parts, how to assemble them and then how to use those legs But when you're using AI it goes differently. You don't tell it you just give it the goal, and it has to figure out for itself how to reach that goal. And it turns out that the way AI tends is by doing this: it assembles itself into a tower and lands at Point B. And technically, this solves the problem. Technically, it got to Point B. The danger of AI is not that it's that it's going to do So then the trick How do we set up the problem So this little robot here The AI came up with a design and then figured out how to use them But when David Ha set up this experiment, he had to set it up on how big the AI because otherwise ... (Laughter) And technically, it got So you see how hard it is to get AI So seeing the AI do this, you can't just be you have to actually, like, And it turns out, This AI's job was to move fast. They didn't tell it that it had or that it couldn't use its arms. So this is what you get you get things like somersaulting It's really common. So is twitching along the floor in a heap. (Laughter) So in my opinion, you know what is the &quot;Terminator&quot; robots. Hacking &quot;The Matrix&quot; is another thing So if you train an AI in a simulation, it will learn how to do things like and harvest them for energy. Or it will figure out how to move faster When you're working with AI, it's less like working with another human and a lot more like working And it's really easy to accidentally and often we don't realize that So here's an experiment I did, where I wanted the AI to invent new paint colors, given the list like the ones And here's what the AI [Sindis Poop, Turdly, Suffer, Gray Pubic] (Laughter) So technically, it did what I asked it to. I thought I was asking it for, but what I was actually asking it to do was just imitate the kinds that it had seen in the original. And I didn't tell it anything or that there are maybe some words that it should avoid using So its entire world Like with the ice cream flavors, So it is through the data that we often accidentally tell AI This is a fish called a tench. And there was a group of researchers who trained an AI to identify But then when they asked it what part of the picture it was actually here's what it highlighted. Yes, those are human fingers. Why would it be looking for human fingers if it's trying to identify a fish? Well, it turns out that the tench and so in a lot of pictures during training, the fish looked like this. (Laughter) And it didn't know that the fingers So you see why it is so hard that actually can understand And this is why designing in self-driving cars is so hard, and why so many self-driving car failures are because the AI got confused. I want to talk about an example from 2016. There was a fatal accident when somebody but instead of using it on the highway they used it on city streets. And what happened was, a truck drove out in front of the car Now, the AI definitely was trained But what it looks like happened is the AI was trained to recognize where you would expect Trucks on the side is not supposed and so when the AI saw this truck, it looks like the AI recognized it and therefore, safe to drive underneath. Here's an AI misstep Amazon recently had to give up that they were working on when they discovered that the algorithm What happened is they had trained it of people who they had hired in the past. And from these examples, the AI learned who had gone to women's colleges or who had the word &quot;women&quot; as in, &quot;women's soccer team&quot; The AI didn't know that it wasn't supposed that it had seen the humans do. And technically, it did They just accidentally asked it And this happens all the time with AI. AI can be really destructive So the AIs that recommend they're optimized to increase And unfortunately, one way is to recommend the content The AIs themselves don't have any concept and they don't have any concept of recommending this content. So, when we're working with AI, it's up to us to avoid problems. And avoiding things going wrong, that may come down to where we as humans have to learn We have to learn what AI and to understand that, AI doesn't really understand So in other words, we have that's not the super-competent, We have to be prepared to work with an AI that's the one that we actually have And present-day AI is plenty weird enough. Thank you. (Applause)