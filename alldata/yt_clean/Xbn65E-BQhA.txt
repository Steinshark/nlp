- [Rob] Good afternoon, everyone. How you doing? (audience members whoop) That's what I like to hear. It is my honor to welcome you to NFX 303, better known as, The Evolution of Chaos My name is Rob Hilton, and I'm a Principal Solutions Architect supporting Netflix as a customer. And basically what that means is, I've spent roughly the last four years of my life learning from, and collaborating with all of Netflix's best and brightest engineers, including my friend right over here to my right, Ales Plsek, who's gonna tell you a little Netflix is well known in a wide array of different tech spaces. One of the arguably is their contributions to Now, that's a huge and but one of the magic words in that space that's been coined by Netflix, and one of the ones that brings us all together today, is But technology spaces evolve constantly. As Ales knows, one year is So given the fact that chaos engineering was coined by Netflix over a decade ago, one wonders how they've continued to evolve in that space today. And with that, give it up for Ales, (audience applauds) - Hi, thank you. Thank you for joining us for The Evolution of Chaos My name is Ales Plsek. I'm a Senior Software Engineer at Netflix. I'm part of the Resilience Team, and I lead our chaos engineering And today, I will hopefully be because we'll be talking for the next hour about history, and evolution And there is a lot to talk about. Chaos engineering has been In software engineering So let's get into it, we First, I would like to talk about the history of chaos engineering at Netflix. We'll talk about some and technologies that and that will hopefully illustrate how our thinking about this discipline has evolved over these years. And then, I would like to show you how the chaos experimentation looks like at Netflix today. And finally, we'll talk about how chaos engineering has grown beyond the scope and how it transformed many phases of our software development lifecycle. And how it impacts the or chaos engineers, or but any software engineer at our company. And it would not be a Netflix presentation without screenshots of some of my colleague software engineers who are hard at work. So let's start with this So your Netflix experience, is largely composed of One, is the cloud, made out of these little instances, and then there's the Open And those are running and storing all the video bits that are served to your device when you click play. Once you click play, we That stream start, we and that we call SPS, That is the key metric that we monitor. Here is a beautiful vintage graph from our Atlas monitoring system showing how SPS is changing day by day. And focusing on the cloud itself, that one is made of these and as we all know, they They can come and go as they want, they can be impacted by failures, and it's just something that That's kind of this cycle of And so, when Netflix moved there was a need to make sure that the streaming experience is not impacted by these little instances And that's why, originally, Chaos Monkey was used of our streaming infrastructure towards the failures of individual nodes. And the tool would randomly pick a couple of instances in the data center, and would start shutting them down on a pretty fine schedule. And early on in this chaos journey, it was a very simple, yet It helped our services to build in that resiliency towards And although it's a very crude tool, it has become a synonym of chaos In chaos engineering terminology, we can categorize the treatment that Chaos Monkey is serving as a failure. And we say that, that failure is scope to And as you will see, we'll be using this terminology Today, Chaos Monkey is but the value it provides is minimal. The services have simply moved on. They are resilient toward And so we don't really see much benefit of running Chaos Monkey The use of that tool has declined, but it's also because is a lot more complex than Speaking of which, here can see a visualization (audience members chuckle) And for those of you taking notes, I will pause this for a minute. And here's another, kind of a bird's view But if we really simplified our members and their talking to our Netflix cloud data center, sending the request to our These requests are entering the cloud through the edge gateway. And then they start propagating of microservices or applications, or services as we call them, and here represented as individual graphs, individual nodes of that graph. So taking this architecture into account, we wanted to be more resilient than just at the level Moreover, in 2014, Netflix a service responsible for information was experiencing And although this service is very important for the because it's managing the subscriber or it's managing the It's managing whenever you want to create or modify a profile. It's not really critical for It's not really demanding that their service needs to be up, because you as a authenticated device, you should still be able to Yet during this outage, our members were not able to And what happened was that and was correctly serving the fallbacks. But the colors of these fallbacks, the colors of the service these fallbacks correct correctly. Because this outage never happened before, there was never any time to kind of test that the fallbacks are being properly processed in the layers above this service. And so the streaming architecture towards these failures and that's why we had a failure. So we realized that we needed to simulate these kind of scenarios, and we needed to be into the services themselves. And that is why we built FIT. FIT is our Failure Injection Technology. And it allows us to achieve that precision of failure injection. So for any given service FIT allows us to define injection points. And those are the points where we will be able to We support many different We have, for example, injection points for the IPC libraries. That's when you when talk through gRPC or REST, or GraphQL. To honor the service, we are able to inject failures either on a server side or client side of the communication. We have database library injection points where you can inject that is trying to retrieve from your Cassandra key space, and so on. We have injection points S3 buckets and so on. Also, FIT allows us to define the treatment that we wanna inject in The treatment will be set and it can be of many different types. Right now we are recognizing the failure. We can either serve the Both meaning that for that request impacted into the injection point, we would first delay the request, and then we would fail it as well. And then there's the scope. The scope is used to define which injection point's So far we have seen scoping to instances, and now we are also able And then a scenario. Scenario is a blueprint It defines a set of injection points, set of treatments, and the scope. So, excuse me. So it exactly defines what is gonna happen during that chaos experiment. And finally, if the the session is a running scenario for that given FIT scenario. And let me show you how So we can create a scenario affecting integration communication And we can be more precise than that. We can say that injection point is gonna be a gRPC client and we'll scope this to cluster A, and we'll still serve the treatment, which will be the failure. And as you can see, in terms of precision, comparing to Chaos Monkey, And this enable us to do game days. During such a day, a team owning a service they would create a FIT scenario and then they would launch it, and then they would observe how their service is behaving. They would look at the errors for that service, errors in services that and so on. They would also be And that way, they would be able to or certain failures. And we use the game day that the subscriber fallback would be properly handled by the clients. We use the game technique, so we created a FIT scenario the injection point would be a gRPC client talking to the subscriber. We would scope that to which is the service that's handling our streaming functionality. And we would inject the failures. And using this FIT session, we were able to trigger the fallbacks and validate that these fallbacks are either properly or And that way we could file tickets and those issues will be So using that FIT session, we were able to make sure that, that 2014 subscriber outage And game days actually represent another phase of chaos engineering after the Chaos Monkey. But as you can imagine, it is an evolution in but it requires lots of amount of effort and time from many people and manually monitor what is And this is still quite a crude approach because here we are between a service A and service B. And so we decided to increase or improve our scoping ability even more. And to do that we used a or as we call it, CRR, and So for a single request, the single request is composed And CRR allows us to attach every request that we are And that way we will be able to mark a certain request with the FIT failures. And as this request is propagating through our services and microservices, this request context And as the request is going through every individual injection point, that injection point will always be able to inspect the headers and see whether that injection point should get triggered for And that way we can precisely mark a given request to fail So we have dramatically going from instance, It doesn't get more granular than that. And so we have built FIT filter, and incorporated it into our edge gateway. So FIT filter will actually Whenever we see a request we wanna tag, the FIT filter will attach and then the request is propagating through our infrastructure. Here's another example that we have. If we revisit the scenario, but we wanna fail only those requests coming from this simple, single device. So we still define injection point as gRPC client talking to service B, but in the scope, not but also we define that only the requests coming from the customer ID of this particular customer ID, of this particular device which will be the failure. And as these requests are traveling from the device through the edge gateway where they are marked with the FIT tags, these FIT tags get propagated and only those requests that are tagged and actually reach that injection point of a gRPC client to those will fail. And this is scoped a single device only, only that one device is impacted. And as simple as it is, this was a turning point because it enabled our software developers to just create these chaos testing directly And they would be able to see firsthand how every particular FIT scenario is impacting the customer. And we build this into a simple So we have a UI where that company can go create, or select where they want which scenario they wanna use. And then they would to a particular customer id. And once they launch this FIT session, they can see whether it the user experience. And this really proved for the first time ever, the Netflix engineer would be able to go and experiment with chaos without requiring assistance of a chaos engineer or And they would be able to and within seconds they would see if there is an impact or not. One of the examples where was when we were launching this new double thumbs up feature. So we let our users rate that they watched with thumbs down, thumbs up, or double thumbs up. And so, by the way, if you're gonna be using your you can use double thumbs So when we launched this feature, this feature is served And again, it's not really a feature that is critical to our streaming experience. It doesn't matter whether you're able to retrieve whatever your rating was for that given content, or whether you are able you should still be able to press play and the content should start. And so it's really a non-critical feature. And so to validate that, the developer themselves who was able to create a FIT scenario where they would be injecting failures to the ratings service again, for their certain device. And that way, again, they would be able to validate that there was zero SPS impact. There was zero SPS impact to their device, and that way they can extrapolate that there will be no SPS And that's why they were able to launch the feature Now, if there was an impact, since for every FIT session we are also recording all the traces. So this is a very useful information, because you'll be able to see where the failure was injected, how the system reacted, which how this failure was returned to callers, and where this experience actually broke. Because many times, the fallback is handled but as this fallback is propagated back to subsequent colors, this So here the software engineers can just go and see what are the traces for this given failure. And the same interaction is available in our prod Soon engineers started these FIT headers into their smoke tests and integration tests, and that way the FIT become kind of a part of the regular release life cycle. So this was really a turning point. And even now years later, FIT testing is still a very popular method by our engineers, especially they can very quickly and easily go create a FIT session and see for themselves, kind of like reproduce that So this drum particularly reduces also the lead time to debugging any issue. Around the same time, a different technology So we have realized, as you can see on this footage deploying to production (audience members chuckle) that it can be relatively So how do we provide safe without adding extra barriers that would slow the innovation, because Netflix engineers are making hundreds of changes to production every day? So in the face of these overwhelming odds, we were left with only one option. We decided to use science. So that is why we took the and we applied it to their It is a very popular experiment design. Let's say you are developing a new drug, and you wanna see what is So you take a population of people, users, and let's say, a hundred thousand people, and you split them the treatment group, And then the treatment group while the control group And then you let the and then you follow and collect results. And once you are comparing the results, since these groups were randomly assigned, there should be no bias. And any effect or difference between the results of these two groups can be safely associated to the impact of that treatment that you applied. So we took this approach and we applied it to our infrastructure, and that's why we created or started using the canary strategy. So when running a canary for it means that we set up for that service, baseline And then these clusters register to take some small and random portion of the traffic in production. And then we observe how these two clusters So we look at system metrics and we evaluate how the system is doing comparing the baseline and canary signals. And this is a very simple, yet very effective technique, and it has become defacto deployment, and deployment standard of So it was only natural that we decided to marry canary strategy and FIT. So we still created a canary experiment, but also we would create a FIT session for that canary experiment. So that way we would still for the gRPC client talking to service B, but the scope would be updated to only affect the cluster, A-canary, that's the canary cluster that we set up. And then we would still And that way as the experiment is running, we would be comparing is behaving in terms of system metrics, error counters, in So this was already our first kind of venture into a large scale chaos experimentation, but automated chaos experimentation, because here we would be able to automatically measure and compare the signals from the And this was very well, this was a very successful And canary's in general provide tremendous value to our company. But if you look at the canary itself, you realize that this approach does not tell the complete story. While this approach is very relatively low risk and easy to set up, it does not explicitly tell us what is the member experiencing, what are the member devices experiencing? Because we are focusing only We look at these two clusters, maybe look at client-side In reality, servers may be happily serving requests, yet our members may not be able to stream. Let me illustrate it First problem that we can see here is that the request that goes to the canary cluster will fail. And since these requests are randomly assigned to clusters, when we do the retry, randomly, that request can go to the So that will mask this problem that is happening associated And the service owner may not even realize that there's something wrong happening because these requests are non-sticky, and they kind of are bouncing back from the canary to the So we may not spot this problem until we actually deploy to production. Second example that we have is, the request succeeds going and it is correctly But then as this request is then becoming part of the results for it may actually cause a and fail there. And we would never know because we are looking only at the canary reports So therefore, we needed a solution that would give us the ability to And so to solve the retry we extended our FIT filter, and we added a new type of a header, we added a FIT override header, and that way we would be able to explicitly tell for every request where it's supposed to go. We say that we make let me show you how that works. So when the requests are being assigned in our edge gateway to a canary or based on population, they get the tag. They get the tag, either they are canary or And then as these tags are again propagating once they reach the point where they are supposed to go to the override in the tag will kick in and will direct the request either to a canary cluster And even if that request fails and comes back to the the overrides again will kick in and send a request back So this is really locking to its population, and to its cluster. So it's creating this strong is locked in into this experience for the entire duration of the experiment, and we can clearly see A second improvement to the way how we assign or users to our experiment. So we extended again our FIT filter with a user allocation algorithm. And that way the user allocation algorithm is implemented as a And that way, each time we see a request we are able to hash the Here on this example, we are assigning 1% of the 1% to the baseline population. And the rest is just not So that way for each request, we can see from which device it came from, and that way we hash the device ID and we get where into the If it's a canary or a baseline request. And that way we can always and determine whether it was And this is the first step to actually know which devices and somehow the first step that effect of the experiment on these two populations of users. Speaking about measuring and monitoring, running these experiments may as illustrated here by our chaos engineers from the Umbrella Academy project. That is why we really needed So at Netflix we've been always using our Atlas monitoring system. That is a really good system that is monitoring the system metrics of all our backend services But we have also built a that is more focused on Here, if we zoom only on this we are seeing the customer and the first tier of services. So we started collecting And these logs would be describing to us what is exactly happening on the device. So indeed, they would be sent along through our infrastructure where they would be redirected to our queue processing infrastructure. And these log events would be for example, representing events such as stream start, whenever the device started or stream start error whenever the device attempted to stream and failed, or app crashes and so on. All these events are being collected in real time by our events infrastructure. And similarly, our first year services also collect these member specific events. For example, stream start, And those stream start represent the state for every member as we see it from our backend infrastructure. So that way, collectively these two sources of events collect the full picture for any particular And since we are sending these requests to our real time queues, we can further process them in real time. So that's why we implemented this experiment monitoring system that only look at these events. So we have constructed this So here we are collecting all the events coming from the devices and and we filter them down because we have that user allocation algorithm that can help us to filter that are coming from the So we filter them down, and First, we push them into elastic search. So later on we can debug to see what was happening but also we turn them into counters, and that way we can monitor For example, here we have been running an experiment, and this graph represents all the events happening on the devices that led to an SPS error on the client side. And as these events are pushed to our infrastructure and we can see that the for our canary population was larger than the baseline population. You can see there's always some noise where there's always some errors happening in both populations, but as we let the experiment run. But as soon as we detect there is an increase in we terminate the experiment earlier. And so what is important here to realize is that the signal we are looking at only represents the errors happening on the devices in the experiment. So that is a major difference between this event-based and the generic monitoring systems that we've been using at Netflix. And this data has per second resolution. So as the experiment's progressing, we are looking at these baseline that are along many dimensions. For example, we are looking at the SPS errors or SPS access events on And since the experiment is sticky, are not only the requests, but also the users are locked into the experiment for better or worse. So if they're experiencing they are locked in that experiment. We usually run experiments for 20, 40 minutes to collect enough get that confidence that or the change is not impacting our SPS. But we are monitoring the data because for 40 minutes that user cannot escape the experiment. And that's why each time we are able to detect that, and within seconds we are able to shut down that experiment. And that brings us to where we are today. From a simple Chaos Monkey, to game days, to that to an automated chaos experiments. So we got to the point where we can final execute, save, and precise and autonomous So let's review what were all the technologies that So in our FIT filter, we are annotating the then our CRR request routing technology lets us route the request And then we are using canary strategy, and the FIT scoping and FIT treatment to actually scope the treatment only to the canary cluster. And finally, we are using the user allocation algorithm to assign members to the experiment, and also to filter out the events so that we can monitor And all of this is orchestrated by our chaos experimentation platform. And today, so we built this into a tool called CHAP. So today, any software engineer can go in the tool, set up an experiment, pick and run this experiment on a random set of actual production users and see whether there is an impact. And if there is an impact, the experiment's terminated early and the user can then investigate. So what would previously and numerous people sitting in can now be done automatically Here we can see we've been for that same outage So now we are able to run and see what is the impact to our members. And if there is any impact, the experiment will shut down as you can see here, And then service owners And so we have built this, what we believe is an amazing tool that runs chaos experiments. But in doing so, we have We have developed a tool that of a failure in a software on our members. But not only that, we can measure impact of any It allows us to measure of a software change somewhere on a certain little service, And we can quantify what is the butterfly And we call this a sticky And our engineers quickly noticed. So they have realized that when they run a sticky canary, it is the only way how can that there is no negative whenever they wanna push a new change or new software change. So over the past decade, in search of this perfect chaos experiment that would only benefit this we have evolved into a tool and deploy all the different And as a result of perfecting chaos, we have developed this when manipulating our infrastructure. And chaos has become that we are using in our tool box. And so from chaos experiments we have moved on to And so this is the term that we use that when we are running And over the years we have identified many use cases where can be applied. Let me walk you through some of them. So first, it's the sticky experiment. That's the base experiment that we use to measure effect of a software change. And in this scenario, the Change can be a code change, a property change running and you can still measure Then chaos just becomes one different type of a sticky experiment where the change we that fault injection in that cluster is not impacting our members. And then we have the unscoped chaos. So this is an interesting application of an unscoped experiment not scoping the failures to any particular cluster. Excuse me. So, we are still annotating, or we are still assigning member devices to the population, to the experiment. So we have two groups of devices, we have the canary and baseline members, and we still tag their requests with canary or baseline tags, but we do not scope to a single cluster. Instead, we still wanna inject to our gRPC client injection points talking to service B. Which means that as these requests are propagating through whenever they reach and they wanna talk to service B, the error will be injected, And that way we can real scenario where the whole service is experiencing the whole and it just becomes unavailable from wherever it's being called. So this way we can simulate these outages, and we can validate and quickly determine that a certain service is either critical or non-critical to our Last year I've been with our software engineer, and we've been running an unscoped chaos on a simple database, and the owner of the database said, &quot;this is not a critical And couple of minutes later, a different engineer comes &quot;Bandersnatch is broken, because we've broken that in the Bandersnatch episodes.&quot; And that way how we have realized this service is actually critical, and we need to treat it differently. Another type of an experiment that we have here, is a data experiment. Here we have extended the way how the number of treatments In this case the treatment we are serving is the new data file. And what is happening here is that when the canary cluster is trying to access a certain data object, they will be served, for example, a new version of the data object that we are currently canarying. So that is the change that is happening, and that we can measure is actually impacting our devices. And then there's the squeeze. Squeeze is another because here we are still but we have also used that ability that we can precisely we send to every cluster. So as the squeeze experiment is running, it is our kind of a performance where we are increasing how many requests there goes And we do that in well-defined steps, where we are every five minutes, we bump up the number of and then we can still see how that cluster is behaving And that is a very popular expert type because we can look at different performance characteristics We can tune, concurrence, We can for example also is behaving when you give or different container resources. And you can also define or determine what is your max throughput And that is valuable information when you do autoscaling or So that is also very popular. And then we have the priority which is another type where we actually take the experimentation discipline, and we are not experimenting on anything that is happening in our data center, but we have extended the with a new injection And that way we can serve different treatments in that edge layer. And here we've been that are validating our We assign users to the experiment, and those that are coming from the canary population, those requests, if they are lower priority but higher priority requests And that way we can see is impacting our members. And here's an animation that shows the priority load-shedding in action. It was captured by one of our engineers. Again, they run a FIT session on their device, and they were able to verify that even though the errors are happening for low priority requests to fetch some of the information for the content, when they click play, they're still able to initiate Another new avenue that we opened was the orchestration of chaos experiments outside of our backend infrastructure. So if you remember those that are storing in our catalog. So we started injecting chaos into those, and we were still able to measure how that affects our member experience. And we are for example, still verify that if that the devices can still reconnect or directly to our data center in AWS. So that way we've been that were happening completely So as you can see, there's many different we have designed over the years, and it's a very modular approach. And when we create these new experiments, the only thing that is changing treatment, allocation, and scope. And we can mix and match them. And you can see that it's that lets us create many different experiment types and FIT scenarios. So as you can see, for treatments we usually serve changes, failures or data changes. For allocation, we usually And for scope, we either scope the experiment or we don't scope it at all. And every time our resilience team is approached in designing we are usually able to by just designing an experiment type that is varying these three parameters. And then the experiment is still able to profit from all that and safety that we have built Now a few words about adopting chaos engineering at your organizations. So introducing or building chaos engineering or that introducing that It takes time, but no matter where and your where your company is, there's still that one next So, for example, if you feel like you could be affected by individual nodes going down, you can try the Chaos Monkey approach. You can either use Chaos or you can shut down There's a also high chance you are probably already And also you could probably use AWS Fault Injection Simulator to simulate these kind of failures. Then the next stage are canaries. I think that is a very that you can use in your organization and really quickly benefit right away without chaos. We have open source, So you can really, either there's a canary stage that you can run a canary, and you can use Kayenta to And that way, moving where you have the in the development or is a great evolution of your organization because you are already measuring signals, comparing baseline and canary signals. So you will develop that when you want to experiment with chaos. And then tracing is also an important and interesting technology that will bring you for example, our chaos Because if you are using there's a high chance that your headers, request headers are through the infrastructure, so that way you would be able to attach custom headers And finally, fault injection is also something that can be added to the mix. We are as an industry than we've been 10 years ago when Netflix was building Because with the evolution Envoy is already supporting some kind of basic fault And that way for example, the interceptors or site cars are an excellent injection point where you can implement that logic of injecting some kind of Also, spring boot fallbacks are already something that is available, and you can experiment fallbacks and make sure that you are actually having those fall backs, there's probably no point in running those chaos experiments Going back to the overall discipline of chaos engineering and I think we can summarize and say that the single chaos engineering role that was originally has grown and extended our discipline, and we extended the range of experiments that we can run today. And those can be used by So from that chaos experiment, we actually started running canneries that are run by any service Change experiments. Squeezes are usually running monthly to kind of monitor how your Unscoped chaos experiments whenever you need to verify is critical or non-critical, and so on. Data canaries, our priority load-shedding experiments, are also a popular experiment type. So the infrastructure exists with that single purpose of enabling our software engineers to innovate as fast as possible, and also without compromising the safety and stability of Netflix. And at Netflix, it's no secret that we want And as you could see in the stock, accomplishing this involves that needs to work seamlessly But the final product, and delivering those moments makes it all worth it. Thank you very much for attention.