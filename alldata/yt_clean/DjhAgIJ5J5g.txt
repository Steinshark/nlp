STEVEN STROGATZ: Going back to at least dreaming about brain-inspired computers intelligence. With the rise of the internet, the impressive advances in computational power, we've Large language models, or LLMs, can often seem to wield something close to human intelligence, at least to us non-experts. And the release of OpenAI's ChatGPT in 2022 helped these LLMs make their mark in the headlines, in the workplace, and in dinner-table sign that large language model intelligence is their lack of common sense, which can emerge in spectacular and sometimes hilarious ways in the mistakes they make. I'm Steve Strogatz and this is "The Joy of Why," where I take turns at the mic with my cohost, Janna Levin, exploring some of the biggest questions in math and science today. In this episode, we're going to be speaking architecture and capabilities of large language intelligence -- AI -- will ever gain common sense. Yejin Choi is a professor and the chair of computer science at the University of Washington, where she researches statistical approaches and She was recognized as a 2022 MacArthur Fellow and named one of Time Magazine's Yejin, thanks so much for joining YEJIN CHOI: Thank you for having STROGATZ: Great. Well, this is going as I know so many people are, by what's was preparing for this chat, just as a little a chatbot built on one of these large And it replied, "As an AI, I don't possess the way that humans do. While I can generate important to recognize that this understanding rather than true comprehension." What do CHOI: It sounds like someone coached STROGATZ: That's funny, that it was coached? You CHOI: We don't know for sure. I mean, what sort of data that it was trained on is does use human-written examples of particular, that ChatGPT distinctly use, that's provided by So, it is not the case that ChatGPT, suddenly speaks like that. It's does coach ChatGPT to speak in more safe, STROGATZ: I get it, sure. No, that makes companies. And of course, people if they take some of these responses too chatbots have disclaimers now on the front page them. I mean, clearly the lawyers have had their, CHOI: Totally. STROGATZ: OK. But so maybe this is a good point these large language models, which most of us know or Microsoft Copilot or Anthropic's Claude. These so, since our listeners may have been hearing learning, large language models, AI, neural Are some of those subsets of the others? How CHOI: So these are all fairly broad terms that but they overlap a lot. So machine learning, machines to learn some patterns between intelligence is perhaps, arguably more intelligence that can do certain operations. or it could be using just algorithms, inference type of machine learning algorithm that So, for example, the computational chess players, in the earlier forms of AI, they were just modern version of that would be in the forms of a STROGATZ: Good. That's very helpful. And also limitations of these large language models? CHOI: So, the capabilities of these large beyond what the scientists have anticipated. Just it turns out these large language models even if it requires open-ended answers. Long-text input/output turns out not only can it do simple-reasoning, multiple but also any topic that you throw at ChatGPT, The truth is, though, it's really a is shared on the internet -- and the internet is how vast that is, because humans have limited out there. But the machine uniquely can really the sort of knowledge and wisdom that people "read them" back to you. But not in the verbatim not like the exact copy of what it has written it's able to synthesize, so that STROGATZ: That is, as you say, phenomenal, one to generate college application essays or I tried one time, just for fun, to have it write plays a psychiatrist and he tries to, you know, Donald Trump. And it was really funny and did can't imagine that -- you know, as you say, that's has ever been playing a psychiatrist. But it's So some people have called it sort of "spicy autocomplete," what these things why would anyone say that? What is it that these CHOI: It's able to read a lot of text and learn literally, all that it does is trying to predict For that reason, some people just diminuate large The reason why it's not necessarily doing though, is because of the particularities of which is not necessarily about memorization. It's And also there's a randomness in the the learned neural network. And that randomness regurgitation always. But I mean, sometimes it often enough on the internet data, then it's you know, there was some incidents that it was able to regurgitate some STROGATZ: Oh, really? I hadn't heard about CHOI: I mean, one could say that it's well, this is a neural network being able to because of that, some people diminuate these But the reason why it's able to do discussing some topics in a Trump style that possible because these machines are data points. The novel interpolation that these machines. So you do get that kind STROGATZ: Mm hmm. Well, so you've been mentioning training, and I think it would be great if you what does that really mean? How would you these big companies that have built ChatGPT or CHOI: So basically the training boils down that has layers and layers and layers of data in sequence. And the goal of this word comes next, conditioning on And what is striking is that that simple lead to such powerful artifacts that can that comes across as a striking level of But importantly, that kind of training is learn about the world, which we don't really it's reasonable to suspect that humans don't but we rather try to focus on making sense of the You and I, by the way, are not able to the conversation we just had verbatim. We just immediately. But we do remember the gist of me the same question again, I'll be surprised. So And also humans learn with curriculum and world. And then if something doesn't make try to do some experiments to figure out the physical knowledge about the But machines, from day one, they're fed with The say in what order they're going to read this wait a minute, I really want to read something about this particular, say, Hemingway's book, that The way that learning happens is so different, and way of learning of humans and then still produce STROGATZ: You've raised so many when talking about babies or that we have curiosity. We have desires, like we or maybe there's something we don't like we have talked about artificial intelligence, but Like, the fact that these machines desires. It seems like that might limit people of all ages have willpower. They have think that emotion is a big part in human CHOI: Yeah, that's a great point. And, in bio beings at the end of the day. We have desire makes who we are. And it's not something we individual identity, and then we live with Whereas AI, it's not clear what it really in some sense and it became some thoughts and emotional soup -- that does mimic the human emotion and intent that So then these machines are capable of mimicking the day, it doesn't really have the kind Now, whether that's a bad thing or a good that may be even a scientific question, if AI really, really, develops its it has like survivor instinct? Or it wants to STROGATZ: Well, this is, of course, something and we should probably save that question for a bit more about the training. Because it is, as you We ask them to predict the next word, given a CHOI: The way that it's trained is, it's score that it should assign to the correct it means the wrong word got the higher to raise the probability of the correct STROGATZ: But not by directly dealing with that is the idea of the weights in a neural network and CHOI: Right, so, perhaps I should pedal there are two phases of training. One is also known as "reinforcement learning with human not just a reinforcement learning, but also mixed or supervised learning. But anyway, by and During pre-training, the learning mechanism is will be assigned to the correct sequence of words, be on the internet. Now, there's really no reason by the way, because for any given different word that could be OK to say. So, But anyway, so the neural networks are trained entail in terms of the weights that these that these machines are learning is basically basically take gradients of individual weights, a partial derivative, of every weight of a neural by the way. Hundreds of billions of parameters then you move that weight so that it's assigned to the particular sequence of STROGATZ: Uh huh. Well, as a mathematician, I'm derivatives. But some of our listeners may not I like to play tennis, and I remember when the ball comes to me and I might hit a "You need to get your racket back earlier. make an adjustment to, I don't know, what -- I of internal representation of how important or that I've turned my body sideways, or I got I have all these different weights and given that this shot was bad, so my weights so that I'll do it better the CHOI: Yeah, yeah, yeah. STROGATZ: OK, alright. But that you just make this poor machine and every time it gets it wrong, you punish correct it by making it adjust its weight CHOI: Yeah. On and on and on. STROGATZ: On and on and on. CHOI: It's good that a STROGATZ: Right, so it doesn't care, I guess -- as far as we know. But CHOI: Yeah, yeah. And then during post-training, but maybe let me highlight just which is reinforcement learning particular type of post-training phase, what answer to a query to a human evaluator and And then based on that, you then go back to the using the analogy that you used before. But comes next, you're focusing on whether you get STROGATZ: OK. I see. So, all this process, I mean, we know that computers run fast. But the pre-training phase? Just give us a ballpark CHOI: Yeah. I mean, it varies a lot depending on how much of a compute you have versus how large are many variables at play that determine the tech companies don't share exactly how long but one can speculate. I would say the really if you want to push the limit. But I a smaller amount of data, then it could By the way, if you really think about how long humans learn -- 10 years of learning as a They still have a lot more to learn. So in STROGATZ: [laughs] That is true. Now, so in your own past, I think I described you work has been very interdisciplinary, from psychology, cognitive science. and why do you think it's important to look at CHOI: Yeah, actually, I thought earlier person who only does one thing in computer years reading books in cognitive science and still a student in these fields. It's not But the reason why I find that because there's a common ground in the Whether it's a form of artificial there's some insights that I could draw from becomes a lot more human-like, or at least it believe that it is ever more important to do STROGATZ: So we did want to really talk to of common sense. We've been talking about how model-based bots are at some kinds of tasks. I mentioned in the introduction -- mistakes they CHOI: Yeah, so this is an example that "If I left five clothes to dry out in the sun, and it took them five hours to dry completely, ChatGPT then said it would take this is ChatGPT trying to be too smart. In fact, them all simultaneously, so you don't need to So this example became very popular. fixed. But then just in case, I figured but actually phrased differently. We reordered the wasn't able to answer this one correctly for some So I thought the problem has been really going to ask just one more variant, which dry a shirt and five hours to dry a pair of to this original mode of, like, multiplying Now, this is really curious because people usually per se for this kind of a question. So once you what it means to dry shirts in the sun, you really yourself whether you should multiply the drying number because you can dry them concurrently. you're good. The curious thing about ChatGPT is STROGATZ: I mean, of course, there are so if it is accumulating in a way I'm not so surprised it has trouble it is really bizarre because it can do But as you say, this is an example where its your talks, or maybe it's one of your papers, sense as the "dark matter of intelligence." I Could you tell us a little more about what CHOI: Yeah. So, the reason why I said is the unspoken rules about how the world how the social world works. So this really we interpret language. And that's really one And the mysterious thing about common easily. I mean, as in, like, everyone has it, to teach machines about these rules that we common sense was viewed as one of That said, I should really acknowledge impressive amount of common sense. I've never I'm not denying that it didn't acquire of common sense. But unlike human common a lot more robust to the sort of questions that I examples, by the way -- machines are strikingly And here's the reason why. The common-sense it doesn't really appear on the internet as ChatGPT has learned it. So, a lot of common sense are edible -- you know, apples can be usually red blue. So these things are now acquired as some other things that are not spoken out loud, then STROGATZ: Hmm. So that's sort of understandable, world, right? So far, its window on the world is Have you and your group been trying to feed common CHOI: Yeah. So in my lab, we've been sense in a more effective way, perhaps they do ask a lot of why-this, why-that adults wouldn't ask to each other. It may growing up are provided with a lot of such So, we attempted writing down a lot of the neural network. And we found that the out of those examples. So that's one sense much faster, by providing this By the way, just like the way that ChatGPT is anything on the internet, by the cut-off time of common-sense knowledge graph that our lab has STROGATZ: Uh huh. Interesting. It reminds me I was a professor at MIT. There were so many professors, too -- who lacked a certain kind for how people are supposed to interact with like etiquette lessons or manners lessons, that CHOI: Yeah, I mean, I haven't but I can totally imagine that there STROGATZ: Yeah, but so then if you had was part of the training, it might be CHOI: Oh, I'm sure that it STROGATZ: OK. Well, we talked a little bit earlier helpful in acquiring common sense for AIs to that aside, I wanted to explore with you some it feels to me, and I think other people have severe obstacles for them to acquire common bodies. Like, a little kid gets to fall down or They don't have a place in society. They don't or with people. Like they're just missing out on are those things fundamental obstacles? Like, do those things, move around in space, interactions? Maybe common sense has to wait CHOI: That's a great question. It's of emotion and embodiment, maybe AI cannot go But I'm not sure whether that's the case it can still do a lot, really a But that aside, whether AI lacking whether that's a good thing or bad thing, compared to human intelligence. But on way to acquire the kind of intelligence It's not clear. This is the kind of scientific In any case, I don't necessarily believe that has true emotion. I mean, AI and awareness so that it's going to interact STROGATZ: Mm hmm. CHOI: But when AI has its own desire and intellectual question, but I'm not sure whether that's the right kind of I mean, let's just say that AI falls it really feels the love. Is that a if it's going to start doing things that because it's willing to sacrifice everyone STROGATZ: Mm. Oh, boy. CHOI: And then embodiment, I'm skeptical that bio embodiment is that the human fingers, for yet know how to make delicate joints that can you know, human's tastebuds. Is it even smell and taste in the way that humans do? but I personally don't think it's all that truly mimic every capability of a human STROGATZ: Yeah. No, it's also interesting too, since we're kind of speculating now and letting other senses that we don't currently have. or electric fish swimming in muddy water, Like you could imagine them having super senses it's not at all clear that this is a good idea So maybe we should close with the final part this about policy, about transparency. It's such to build these, that only very few people or and they have proprietary data and techniques. CHOI: It's a huge issue in the field. What could possibly go wrong with STROGATZ: Yeah. CHOI: I think especially the opaqueness of the as well. Going back to your earlier example of way -- that it may or may not understand in the it has limitations. When it says that, does it for post-training adaptation of ChatGPT, so in a more politically correct way? Or is it that introspection capabilities to realize that "Oh, post-training data was transparent, a lot of And also, I think for the purpose of AI safety as is helpful so that we better understand where STROGATZ: Is this something that governments need to impose on the big companies? CHOI: Probably there should be more AI policy. It's a very important topic that because I can totally also imagine unnecessarily without actually adding So it's an effort that requires really a broad there needs to be effort to increase AI literacy but even for daily users of AI what the limitations of these models STROGATZ: Well, I'm reminded of a time and a lot of biochemists and molecular biologists themselves about what kinds of experiments they I wonder, is that something -- rather do you think maybe the community including the big companies? Do CHOI: In general, I think, there's sectors have a way of contributing in some high-level declarative sense. We used to develop bioweapons or AI should not can be more gray zones, and we then need to STROGATZ: What do you see as the now? What do you realistically CHOI: I think there's a lot to worry like misinformation, increasing use of particular political party. That's one thing, such as, you know, people faking their long-term consequence in the way that people You know, by the way, I used to think the but that may not be the case in the coming for all sorts of their writing jobs, I using ChatGPT and then the authors were not "Oh, it's an AI model," blah, blah -- STROGATZ: I shouldn't laugh. Yeah, no, let's be real. I have a colleague who's not his first language, and he has told grammar of the abstracts for his papers. You He's written the abstract. It's just sort not really providing new ideas. So these CHOI: Certainly, yeah, it could help It can help as a writing companion, if unwanted side effects on humans as well. I do I do wonder personally whether it's going human capabilities of writing and reading STROGATZ: Well, so just to wrap up: The one since our show is called "The Joy of Why," a scientist yourself. Is there something in CHOI: Oh, yeah. Great question. A lot of this are there limitations in ChatGPT and based on just reading internet text? Seeking really know why, but it does give me a lot differentiating factors compared to human STROGATZ: Yes, we do ask why. And thank you so speaking with Yejin Choi. It's been delightful CHOI: Thank you. It was so fun. STROGATZ: Thanks for listening. If you're already subscribed, hit the subscribe or can also leave a review for the show "The Joy of Why" is a podcast from independent publication supported decisions by the Simons Foundation have guests or other editorial decisions "The Joy of Why" is produced by PRX Productions Brock, Genevieve Sponsler and Merritt Jacob. The Gonzales. Morgan Church and Edwin Ochoa provided Rennie and Thomas Lin provided editorial guidance, Arleen Santana and Meghan Willcoxon. Our theme music is from APM Music. Julian Lin is by Peter Greenwood and our logo is by Jaki to the Columbia Journalism School and Bert I'm your host, Steve Strogatz. If you have any quanta@simonsfoundation.org. Thanks for listening.