In 2021 Notion's popularity skyrocketed, because their postgres was at terabytes of to explode. Faced with losing customers, In this video, let's learn how To understand the root cause, we need to take everything in Notion is a `block`. an image or even an entire page. Each block is stored as a row in can be nested within other blocks to This structure allows for incredible a simple document can result in hundreds this waifu document that err my friend has. Now, While flexible, the sheer volume increased latency when requesting page data. Notion was seeing delay because their no longer handle the load. Their Postgres VACUUM is a crucial database maintenance by dead tuples - data that's been space. When VACUUM can't keep up, it leads On top of that there was an alarming threat each transaction is assigned a unique ID, the database enters a read-only mode app that cannot be edited or deleted. . For Eventually their database became a ticking The first instinct might be to like me trying to get big at the gym. much you can scale a single machine before query performance and maintenance processes often What did they try next, yup you guessed it all the tables linked to the block table discussion threads, and comments, keeping related they chose the workspace id since users typically The new sharded setup needs to handle their growth for at least two years. For that total IOPS. IOPS represents the number of read and 2. To maintain RDS replication, they set limits After careful consideration, Notion chose to physical machine, they chose to create postgres schema. Each shard would workspace and comments. For a total of 480 The routing mechanism is determined at the the application uses the workspace ID databases the data resides on. they use the workspace ID again to determine They use PgBouncer has an intermediary database. PgBouncer is a lightweight Postgres of establishing and tearing down connections allowing applications to reuse them. This database are required, to minimize resource After setting the new databases, they There were a few options to consider: at the same time. However, if there this would lead to flaky data. postgres' build in publish, subscript model. This id was not yet populated in the old database. workspace id to the new as they're being moved. but it would have increase the Ultimately, they decided to create an use a catch up script to populate the new Once new incoming data are now they began migrating existing data. This by a beastly m5.24xlarge instance with 96 CPUs. would compare the record versions to make sure Finally, they executed a carefully orchestrated connection pooling layer (PgBouncer) to Things went great for a while...until cracks with their trusty 32-shard Here's what they were seeing: 90% CPU utilization during peak traffic. of their provision disk bandwidth, or IOPS. limits in their PgBouncer setup. To make matters worse, the new year is approaching and usage around this that time. Gotta get To resolve this issue, they decided to shard their 32 databases to 96 databases to improve instances for new shards in hope to Recall that each old instance have 15 schemas. Now place and each new instance will only contain configured with smaller instance type by design For data synchronization since there were no replication to continuously apply new changes setting up three postgres publications on each five logical schemas. On the new databases, the these three publications, effectively One key optimization that they did was the data transfer. This approach reduced the During the transition, they initially had entries. Each instance could open resulting in a maximum of 600 Their migration plan involved adding 96 new them to the 32 existing shards, allowing them As data was written to the old database, it However, testing uncovered a critical issue: since needed to reduce the number of connections If they chose to increase the connections, each of the 100 PgBouncer instances could shard. This would mean 1,800 connections per On the other hand, reducing connections the traffic, leading to query backlogs. The solution was to shard the PgBouncer cluster managing 24 databases. This allowed them to: per shard to 8. connections per Postgres instance to 200. This approach maintained appropriate connection numbers, prevented overload during Before rolling this these changes to production, - Notion added functionality to fetch when requests are being made consistency and any discrepancies are logged they limited the comparison to queries a small portion of the requests before issuing the dark read query to The testing show nearly 100% identical data. They proceeded with the failover process 1. Traffic pause: Halted new connections 2. Replication check: Ensured new 3. Configuration update: - Updated PgBouncer to point to new databases changes from new db to old db just in case. The resharding project was a Key outcomes: capacity CPU and IOPS utilization decreased dramatically peak traffic compared to previous 90% positions Notion to handle continued What do you think? Is Notion fast enough now? And as always thank you so much for