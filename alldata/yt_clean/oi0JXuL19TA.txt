Thanks to Curiosity Stream for supporting Hey, I'm Jabril and welcome to Crash Course AI! Language is one of the most impressive things It's how I'm transferring knowledge from Languages come in many shapes and sizes, they of different components like sentences, words, For instance, English has 26 letters and Chinese So far, a lot of the problems we've been have involved processing images, but the most is through language. We type questions into search engines, we sometimes we even get a little help with our So today, we're going to explore the field INTRO Natural Language Processing, or NLP, mainly First, there's Natural Language Understanding, of letters. These are AI that filter your spam emails, was grocery or computer shopping, or instruct house. And second, there's Natural Language Generation, These are AI that perform translations, summarize The key to both problems is understanding words have no meaning on their own. We assign meaning to symbols. To make things even harder, in many cases, of a word depends on the context it's used If I tell you to meet me at the bank, without the place where I'm grabbing some cash. If I say "This fridge is great!", that's was *great*, it lasted a whole week before So, how did we learn to attach meaning to How do we know great [enthusiastic] means Well, even though there's nothing inherent soft, purrs, and chases mice... when we were a cat." Or a gato, mao, billee, qut. When we're solving a natural language processing or natural language generation, we have to meaning of words and understand our potential Sometimes we can compare words by looking This works well if a word has morphology. Take the root word "swim" for example.We doing it right now, they're swimming, or Drinking, drinker, thinking, thinker, ... you But we can't use morphology for all words, let us know that a vandal smashed in a car Many words that are really similar, like cat And on the other hand, cat and Felidae (the word for the scientific family of cats) mean very similar things and only share one letter! One common way to guess that words have similar or seeing which words appear in the same sentences This is one of many cases where NLP relies As the linguist John Firth once said, "You But to make computers understand distributional in math. One simple technique is to use count vectors. A count vector is the number of times a word other common words. If two words show up in the same sentence, So let's say we asked an algorithm to compare count vectors to guess which ones have similar We could download the beginning of the Wikipedia show up. Here's what we got: And a lot of the top words are all the same: These are all function words or stop words, and help convey precise meaning. Like how "an apple" means any apple, but But, because they change the meaning of another so we'll remove them for now, and simplify Let's try it again: Based on this, it looks like cat and Felidae show up with lots of the same words in their And neither of them mean the same thing as But this is also a really simplified example. One of the problems with count vectors is To compare a bunch of words using counts like word we've ever seen in the same sentence, So, we'd like to learn a representation and similarities as count vectors but is much In the unsupervised learning episode, we talked of those images. We needed a model that could build internal And we can do the same thing for words. This is called an encoder-decoder model: the remember about what we just read... and the decoder uses that thought to decide We're going to start with a simple version Let's create a little game of fill in the train an unsupervised learning model. This is a simple task called language modeling. If I have the sentence: I'm kinda hungry, I think I'd like some What are the most likely words that can go And how might we train a model to encode the In this example, I can guess the answer might something like "potatoes," because I've they probably don't exist. Definitely don't exist. That should not be a thing. The group of words that can fill in that blank is an unsupervised cluster that an AI could use. So for this sentence, our encoder might only decoder has a cluster of "chocolate food Now let's try a harder example: Dianna, a friend of mine from San Diego who party next week, so I want to find a present When I read this sentence, my brain identifies talking about Dianna from 27 words ago! And second, that my friend Dianna uses the That means we want our encoder to build a of information from the sentence, so the decoder And if we keep the sentence going: Dianna, a friend of mine from San Diego who party next week, so I want to find a present Now, I can remember that Dianna likes physics So we'd like our encoder to remember that to guess the answer. So we can see how the representation the model of what we've said or heard. And there's a limit to how much a model Professor Ray Mooney has famously said that a sentence into a single vector" and we Professor Mooney may be right, but that doesn't So so far we've been using words. But computers don't work words quite So let's step away from our high level view next word in a sentence anyway with a neural To do this, our data will be lots of sentences or text from books. Then, for each word in every sentence, we'll We'll train a model to encode up to that go there. And since we have the whole sentence, we know First, we need to define the encoder. We need a model that can read in the input, To do this, we'll use a type of neural network RNNs have a loop in them that lets them reuse as the model reads one word at a time. Slowly, the model builds up an understanding came first or last, which words are modifying properties that are linked to meaning. Now, we can't just directly put words inside But we also don't have features we can easily Unlike images, we can't even measure pixel So we're going to ask the model to learn own (this is where the unsupervised learning To do this, we'll start off by assigning case a random list of numbers called a vector. Next, our encoder will take in each of those /shared/ representation for the whole sentence. At this point, our representation might be we need it to make predictions. For this particular problem, we'll consider that takes in the sentence representation possible word in our vocabulary. We can then interpret the highest scored word Then, we can use backpropagation to train networks in Crash Course AI. So by training the model on which word to the encoder RNN and the decoder prediction Plus, the model changes those random representations Specifically, if two words mean something similar. Using the vectors to help make a plot, we For example, earlier we talked about chocolate representations that researchers at Google Near "chocolate," we have lots of foods By comparison, words with similar representations This whole process has used unsupervised learning, pretty interesting linguistic representations But taking in part of a sentence and predicting for NLP. If our model took in English and produced Or our model could read questions and produce answers, like Siri or Alexa try to do. Or our model could convert instructions into John Green Bot? Just kidding you're your own robot. Nobody controls you. But the representations of words that our work for others. Like, for example, if we trained John-Green-bot he might learn that roses are made of icing But he won't learn that cake roses are different a pretty bouquet. Acquiring, encoding, and using written or and exciting task, because we use language Every time you type or talk to a computer, Now that we understand the basics, next week together in our second lab! See you then. Thank you to CuriosityStream for supporting PBS Digital Studios. CuriosityStream is a subscription streaming titles from a variety of filmmakers, including For example, you can stream Dream the Future "What will the future look like?" as she will impact our everyday lives in the year You can learn more at curiositystream.com/crashcourse Or click the link in the description. Crash Course Ai is produced in association If you want to help keep Crash Course free on Patreon. And if you want to learn more about how human of Crash Course Psychology.