Welcome back, everyone. Dan Vega here. augmented generation or rag for short in Now, why does this become important? that we're building are built on this And it really starts to make for a lot of why does rag matter? Why does it matter to When we talk about large language models, which, are it's only trained on a certain set of to provide it with the most relevant information, we need to provide that are not trained on your data, hopefully. So in the case of, hey, I have this policy, and I want to feed this to the LLM Also, when the LLM does not have the it can sometimes hallucinate. And the better context it has, the less likely it will be to things that we're going to talk about And I want to go through some kind of talk about why rag matters, what is rag, and more importantly, what rag is just kind of feeding all your documents to an LLM, and that that's not talk about some concepts. and then we'll go in and we'll build an AI. This makes it incredibly easy to do rag through a demo today. So I think this is a really good place to on some of things we're going to talk I'll go ahead and export this as a PNG, for all the code that we're going to go You can find all of that down below in So first, starting off, tokens. are the currency of LMs. And this becomes each of the requests and responses that we're doing. good tokenizers out there. This one is from OpenAI. And what roughly how many tokens it is. So I just put some text in characters, roughly 95 tokens. And if you look down at the three -fourths of a word. So 100 tokens again, it's not always a word. You can see by down. Again, this is important because this is how we are doing business when it comes to generative AI. So this is open other large language models out there, you'll find similar different models out there, you'll see for GPT40, And $10 for one million output tokens. So response. Now different models have different what that means is we have to be cognizant of what we are sending to have because it's going to end up costing a a high throughput application, right? The context window size. This means that you can't because it has a max size that you can send. So if we look 200 ,000 tokens. And if you use Gemini, it has 1 their pro level, it has a 2 million context size window, think, the biggest out there. So price and context window these generative AI applications. So let's talk about why earlier. Large language models are trained on data ask, say, chat GPT, what is the cutoff date you. So if you very curious of what those dates are, go output that for you. So that means anything after that date, are asking, if you're in the realm of asking it have answers to, you may want to be able to provide some your data, hopefully, your private documents, your provide that as context, we can go ahead and do that with RAG. And information, this is the likelihood where we're going get responses that aren't factually correct. The more what is RAG? RAG stands for Retrieval Augmented methods with generative models to produce more accurate and contextually access external knowledge sources, mitigating limitations cutoff. So that's RAG. What is not RAG, and I think this is very sending all of our documents in the context of each is providing extra information, but not RAG. If you want to summarize an entire Rack. And again, this is important because of So let's look at an example of this. In And I'm saying, please summarize all of takeaways. So when we do that, we add all of the the LM, and it will give us back whatever that those five key takeaways are. That's one. I have a user query and I say, what is have a bunch of documents that we've uploaded or added to the make a lot of sense to send all of those documents because we're something that may be hidden all the way down in a particular So in that case, we don't want to send We only want to send the bits that are LM. So we could do that, but again, if we're RAG, right? We're not sending just the bits. We need to send what's relevant to the And that's where Rag comes in. So Rag, we have two phases. One is the ingestion phase, and that is relevant documents or information. The way this happens is we take all of readers. So with Spring AI, we have a bunch of We'll see them in the spring initializer But this can handle PDFs, markdown, Word are in the form of, we can take those document model. Now, an embedding model's job is to take and turn it into vectors that can be These vectors are mathematical and this allows for much quicker searches information. So we're not storing this data in a We're storing it in a specialized And we'll talk more about that as we go. So that's phase one. This isn't done every single time. This may be done once. This may be done on some scheduled task you need to update the vector database But this is really done in two phases. The ingestion phase, once that's done, So here I have another request. I am a user. I perform a user query, some type of And what happens is we have this context based on the user query, let's go off to the vector database and that are relevant to this query. So again, we're not sending all the we're sending just the bits that are And again, that's important. Remember, we looked at tokens. We have a context window size. We are paying for this. So we don't want to just send everything. We are sending just the relevant bits. So now we have this augmented prompt with and any information that is relevant to and we're sending that off to the ELM. Once it has that, it can go ahead and us. So I think that's kind of my overview of And I just want to show this off a little and you try to do a search on something So I'm going to choose GPT4 here. And I'm going to choose GPT 4. to say, what is the current stock price work for? I'm going to copy that because we're See this searching up here. So it's actually searching. And it was able to go out and give me the It did that because it was able to go out But remember, this is the product that Chat GPT is a product. GPT4O is the large language model. So this is adding that extra If we're using GPT4O in our applications, we don't have that ability to go search And I'll show this off. So this is the product, but you can also and test the different models here. And this you can get a little bit more You can pick a model. You can, like, adjust the temperature and and how we return those results. But I'm just going to paste in what I did And you're going to see, I don't know, I So the same way that if I were to ask happened last month, its training data is not trained on that. So it's not going to know that. So it's either going to say, I don't something and probably not give you back the right So again, I'll go into something like you can see over here in the AI studio. I have this token count of one million. So I'm only using up 10 tokens here. This can take up to a million tokens. But again, this is going to have the same It does not have access to real -time or And another one here I can go in and clod And it doesn't know the current stock right? So. So that's just a little bit about some of when it comes to building out intelligent cases that we're seeing with some of our customers. So we're going to build out a quick rag just how easy it is to do in Spring AI. So the best way to do this is to get Head over to start .spring .io. We're going to create a new Java project latest version of Spring Boot. I'm going Dan Vega, and we'll call this a rag demo. And then we're going to fill in out a web app, which means I can just go ahead and when I go can hit this using like a rest endpoint. I can provide a we need to configure our LLM, our large language model. Spring large language models out there and you you're going to use. So in this case, I'll use OpenAI as the document readers earlier. Like how are we going to get our look through document reader, we can see that the readers as starters, so you can pick from them. And it really read from. So the TICA document reader has access to There's a markdown document reader, and So I'm going to go ahead and choose the Next, we need a vector database. So the vector databases out there. You can see this large list. There's a I'm going to choose the PG vector is their kind of vector offering on that side. vector database on my local machine, and then anytime I want to make this easy for me and for you. So when you're have to go through and set that vector database up. And we're going Docker Compose support. So if you click the Docker Compose dependencies here and decide, okay, what containers do I need to set up And so if we look at Explore here and we .amil, you see that it's setting up an image for And we can configure this later, but this our application without having to do any of So with that, we have everything we need You can go ahead and click generate. This You can open it up in whatever text for IDE your most productive in. I'm IntelliJ's ultimate edition. And with that, I think waiting for? Let's write some code. All right. So the first Compose .compoze. YAML here. I'm going to change a couple that the database is going to be called markets. We're that we're going to add to the context here. I'll go ahead and then my user is user. Also go ahead and make sure that port If you don't do this, it'll just add a and then you have to look at Docker and on. If you want to look at the database, this So with that, we want to go ahead and set Let's go into source, main, Java. Actually, I want to rename this just I don't know. I'm just weird like that. But that's okay. That's okay. We all have So I'm going to go into application .com things. So we're talking to OpenAIs, GPT40, or that. So no matter what LLM you're talking to, want to set. I want to go ahead and set my API key Now, I've done some videos on this, if to get an API key from a particular large In this case, OpenAI, you can go to their I'm not going to hard code this in here. I'm going to use an environment variable. In this case, I've set one up called And then I want to go ahead and set the This is just specifying which LLM model In this case, I'm talking to G .E. Now, if we were to go ahead and run this database would not get initialized. This doesn't happen by default. And that's just in case, like, you have and we don't want to, you know, initialize it, then we But in this case, I want to. So I'll say spring AI vector store, PG I'm going to say true. All right. So with that, we are in a good place. Now, I'm going to copy over a PDF. I want to show you the PDF that we're So I'm going to put this in a directory And I actually just went out and did a out there. This one came from Morgan's family. So this is publicly available, but just different LMs. So I'll go ahead and open this up in and then I'll go ahead and open this And it looks something like this. Just again, from Morgan Stanley. The B has a bunch of information on the And we can go ahead and ask some particular document. So I have my document. I have my connection to OpenAIs, GPT40 in The first thing that we'll want to do is set up that ingestion phase, right? Like we need to get this document into a into this PG vector database here. Let's make sure Docker desktop is So Docker, that should be running. And there it is. I have a couple other things I want to And this is a previous one, so let me So with that, I think we can go ahead and and just, all right, so let's go ahead We see some containers started. The application is started. So what should happen is because we are we should be able to connect that So I'm going to go ahead and connect to Now, if you're using something other than Any database tool that you use to connect The user, I believe, was user, password The database, let's see what that was So we're calling this Markets. So let's say, Datasource, Postgres, user, Sure. And then we'll call this Markets. Let's go ahead and test that connection. And that looks good, so I'll click OK. Now we see that. database there. By going to public and going to tables, So this contains all the information we again, nothing's in the database yet. So that goes back to our ingestion phase. We got to take our documents. So in this case, our single PDF, we got vectors into the vector database. So how do we do that? So we're going to create a new class We're going to call this the ingestion. service service then and this is going to the command line runner so this happens right after then we'll go ahead and implement that run method. Let's go ahead and make this a component I'm going to get a logger. So let me say logger and let's import That looks good. We need another import there. This used to automatically happen and now I don't know what's going on. So, oh, I pulled in the wrong one. This is the one I want. Okay. So now that I got a logger, I'll use that store. So private final vector store. vector store and we will get that through Now this comes in because we have PG Auto configuration kicks in and a bean of Now this is an interface. There are some implementations of it, but this contains all the information it needs to, store. So we get an instance of that vector We can use that and in just a second. One more thing we'll going to use and adjust into the PG vector store. So and say that this is going to be under the class path the beat October 24. PDF. So that's our article. And we'll say resource, resource. So that's our resource. Let's call this something better. How about market PDF? So now in this run method, this is going off. We want to go ahead and read, get that then use something to split the document up into different Because again, we're not storing the We're storing pieces of the documents. vectors. So I'm going to get a PDF boot starter that we selected from the this case, I want to say I want a new paragraph, so paragraph PDF So if I wanted PDF document reader, if I wanted to get a this up into finer green documents. So I want a of information, you can just send a page at I think there are some different approaches to this on how them up. We're not going to get into that in this dive into that and how some of the vector stores are working them and filtering. But for this one, let's just worry about using it in our application. So I got a text splitter. So I'm going to get a text splitter. This is equal to not an emoji, Dan. This is equal to a new to do is take the vector store and basically apply these that accept. So accept, we see that it a list of documents yet, but what I can do is take that text is given me. So pdfreader .git, and that will give us paragraphs. So I'm just going to log something out. Whoops. Let's just say vector store, vector store loaded with All right. So that is it. That's our kind would only happen once. I could do some checks here to make sure for this demo, we're just going to initialize a schema it's not a lot. But obviously in a real world that. So let's go ahead and try to run this again. What we want to see is that loaded So we see a bunch of things going on. We see that text getting split up. So here are all the slides. And then we go through and we process There is, you will see some of these, you don't have for different PDFs, it may have some errors like this. This is just saying it can't read the font. If you want to get rid of this, you of properties, and let's just say I don't logging. Level. This is the org. Apache. Now dot Apache. So PP font file system and then the file, oops, sorry, font, and then the file You could basically change the logging we wouldn't see that. So the next time we do that, we won't. But as we go through here, again, those No big deal. And now we see splitting up to documents into chunks, vector store a vector store and take a look at this, we can see there's a bunch of information content. So that should be by paragraph, some metadata about it. that float. These are those floats that get stored in the vector looks magical. But this is what helps that precision and that fast data out of the vector database, right? So part one is done. We now we should be able to perform a query and have it pull that as we go ahead and ask the LM something. So the way that I'm new rest controller. So let's call this the chat controller. We'll mark this as a rest controller. We will get a chat Spring AI, this is how we have a chat client matter what LM we're talking to, the code is the clients, chat client. We'll kind of get this use the builder. So if you've ever used anything like the web client or familiar. So now what I and say is builder .billed, and now we doesn't know anything about our documents. So if we go know about the documents that we're providing it. So say, Git mapping, we'll do this on the route, response back, which is going to be a string, and we'll check client, so we've got to return prompt that. And we're going to pass in a user did the Federal Reserve's recent interest rate cut impact analysis. And I'm not going to pretend to feed that document into something like Claude and say, you interesting questions I can ask of it. So I'm going to make a string response back. So that's all there is to it. Now, if we request, we're going to get an answer that isn't we're what does recent mean? It doesn't really recent based on all of its training data. And I won't bore you not up to date with the data that we're trying to supply. Now, a spring AI advisor. So we can say, hey, a default advisor for answer advisor. So we see we have some question and question of answer advisors here. And if the question is retrieved from a vector store and basically sets this up and says, hey, here's the context. Here's the history, here's what we're going to use. So this against the vector database without having to write logic. So all we're going to do is pass Right. So I don't think I even need this ahead and run this, it's going to use that PDF as part of the going to send the entire PDF, all, however in there, it's only going to send the sending the least amount of tokens we need to as part of application again. This time we shouldn't see all those for that font thing, so that shouldn't show up. But again, it this is just something that we're doing in a demo, something we could always, you know, hey, do a quick search. or a quick query against the database and there are, hey, we don't need to initialize this in there. And then this application starts up much started up, I'm going to make a request to Local Host particular method. It's going to supply this question. And now sending that data along with it. So now, now you're going to call this I can go ahead and make a request using a curl, but a little bit readable. So I can 8080. And let's see what happens. So again, it's in the context, sending along. And based on that, here's So duration and yield curves, high yield and equities, French equity, exposure, et able to come up with a better answer with information about the current economy, right? So I was able to use new information that it was trained on. So I think that was just a able to provide your own documents. And again, this was a tons of documents in a lot more information that you want to that ingestion phase makes this really easy to do. I thought you did. All right, so that's it. That was our example. Springing AI. Spring and I makes this really easy with all of in those documents, store those in a vector database. Spring databases out there. Again, you can look in the documentation And really, the portability is the key. language that is able to talk to the And it's a single kind of abstraction, not easy to put together. So we're very, very thankful for them for So we get our data into the database, and context in our queries to the large So that was a lot of fun. Next steps. of touched on a couple things here. One of the next steps is, again, we're AI applications out there. And some of the concerns we hear often if I'm going to be sending all of my documentation, off to these public LLMs, because I'm not Maybe they're using it and training on it. And I don't know if I want to send that perfectly understandable fear to have So next steps with this is you can go with something like Olamma, which allows you to run open server somewhere, and talk to that instead of sending this large language models out there. So I have some videos on Olamma and of follow this with that. I think it'll be a good information out there. So if you're interested in that, let me to that shortly. But friends, you know what time it is. I did, please do me a big favor. Leave me a as always, happy coding. I'm sorry.