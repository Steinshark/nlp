In today's video, we are going to talk about tokenization in spaCy. We can do tokenization in NLTK as well. We have discussed the pros and cons between these two libraries and, we decided we'll use spaCy for the reasons I mentioned in the last video. And if you remember our NLP pipeline video, we had this uh this step called pre-processing. So in this entire NLP pipeline, we're going to begin with the pre-processing step. The data acquisition and text extraction and cleanup step is something we can maybe take a look at later, maybe in the end-to-end NLP project. But in pre-processing what we learned was, there is a step called sentence tokenization, when you you have a paragraph of text. You first separate it out in sentences and then each sentence you split it out in the into the words. So that's called word tokenization. So we are going to see how you can do both of these things in spaCy library. Also, there was stemming, lemmetization we'll cover stemming, lemmetization in the later videos. But tokenization is a process of splitting the text into meaningful segments. So if it is sentence tokenization, we're talking about splitting paragraph into sentences, and if it's a word tokenization, we are talking about splitting sentence into words. And you will be like, &quot;What's the big deal with tokenization?! You know you split the sentence by spaces and that's it right? Why do we need a library?&quot; Well consider this statement. If you are doing sentence tokenization for this, for sentence tokenization you might use dot. Dot means the sentence is ended. If you use that simple rule, here you will end up creating 3 sentences, for a sentence which is just one sentence okay? So you can't use simple rules for this tokenization. You need the language understanding, you need to define special rules, that Dr. doctor Dr. is not the end of the statement. Also if you have n.y. it's not . doesn't mean end of the statement. You need language specific rules here. So what we will do now is, if you are not installed already, just do pip install spaCy and that will install the spaCy library on your computer. I already have it installed. To launch a jupyter notebook, you can run jupyter notebook command, and whatever directory you run that command in, it will show all the files there. So I I will, I have created this spaCy tokenization directory already. So you can create whatever directory you want, and just say python3 right? So I'll just move everything else aside and, I will do F11 to bring this in a full screen mode, and then I will import spaCy. So that will import the library, then I will create a language object. So in spaCy, you can create language object in multiple way. One way is you can just say spacy.blank en 'en' stands for English okay? Now that creates, so just simplistically think about NLP as an object which has English understanding. It understands English language a little bit. If you say de it will be for German language. Now I how do I know? Like whether it's de or en? Like is that's like magic? So you can say spaCy language models, and it will show you all the language models you have right? So, we will do something like this later on. This is more like a trained language pipeline. So don't worry about this. But see, English is en, France French is fr German is de okay? Hindi is hi. So whatever language you are dealing with, first create the blank component. There is another way which is creating a pre-trained like pipeline, we'll look into it in a later videos. So this is how you create a blank English language component, and then you will create a document. So document is equal to NLP something. Here you provide your text. So whatever text analysis you are doing, it could be a paragraph it could be a multi-page document, you need to provide a text here. And the text I'm going to provide is, Doctor Strange's visit in Mumbai. He loved pav bhaji so much, and I'm doing some text analysis on Dr. Strange's beautiful journey into Mumbai and pav bhaji. So when you do this by default, what this will do is by doing this see you can do for token in dock print token And when you hit that it has already done word tokenization. It is so easy! Plain Python code, very intuitive to understand. And if I go back to my presentation, what it is doing in reality is this: So when I say spaCy.blank I created NLP component around the text that I had, by default it gives you tokenizer, word tokenizer. So when you feed text into it, you get doc document Doc document already knows about tokens. So it has like token 1, token 2, token 3 and so on. I want to take a moment and talk about firstlanguage.in which makes building NLP applications easier for you. You can perform various NLP tasks which are shown here, via this platform. If you're using a spaCy type of library, you download it, you run your code locally. If you are training a heavy model, you need higher compute resources such as GPUs. Whereas here, everything is in cloud. You just make an HTTP call, things run on the cloud. You don't need a local machine with lot of compute resources. There is a demo which you can see by clicking on this link, and I have bought this bulb recently and you see there is a negative review on Amazon. And when I copy paste this here, and I select text classification and when I say submit, it immediately classifies that as negative. And if I have a positive review, I just copy paste it here, and if I submit it, it will classify this as a positive. See it's pretty powerful! So it makes uh things much easier for you! You don't need to have like too much like detailed NLP knowledge. You can just call APIs and get things done quickly for yourself. The free tier is something you can try today. You can just sign in, and you can get an API access. Once you sign in, if you go to dashboard you will get your own API key and you can use that API key to make the calls. They have SDKs available both in Python and Typescript. Go check it out. The link is in video description below, and thanks firstlanguage.in for sponsoring this video. Okay. So that part I think is clear. Now these tokens either you run a for loop or if you like python list style of index, you can do this also. You can do Doc 0 as Dr. doc 1 is strange and so on. So you can use general like python list like index operation, to retrieve all these tokens. And you realize see if I have two dollar the English model that we created, it understands that this is a currency. Therefore, it will split split two and dollar into 2 different tokens. See let's try this. If I do this hash, I think hash also is splitting it into different tokens. So it probably okay, so it currency the punctuation it will split though them into different tokens. If I was using just splitting by the words, then it won't be sufficient correct? Because then it it doesn't it's not smart enough to detect the language. And let me explain the the rules of the tokenization. So I got this image, not this image, I got this image by the way from spaCy documentation. So for thanks for that! But let's say you have this sentence, &quot;Let's go to N.Y.!&quot; The sentence has double quotes, okay? So if I just use split by spaces, I will get these, these words and these are not my true tokens okay? That's when spaCy's tokenizers tokenizer come in place, and it will do some additional processing. Okay what kind of processing? Well this. It will first split by prefix. So any prefix you have, quote that's a separate token. So it will split see you green box, it just splits that, as a separate token. It could be dollar sign bracket, there are there are some characters it considers as prefixes. And it will split them. Then it looks at exceptions such as let's see apostrophe s that is also a separate token. And when you do that this kind of when you do splitting into this kind of individual component, you will see in the later videos that this is very beneficial for doing the language analysis, for building the NLP application. Then you do suffix right? The way we did prefix there is a double quote at the end, so that's my suffix. Then you have one okay suffixes could be whatever, km is kilometer it could be bracket, explanation exclamation mark, no quote, etc. Then another suffix would be the excel exclamation mark after N.Y. So it will split that. Then one more exception would be, I think yeah so the exception is split. Okay so suffix was this and the exception was this, so now you split now your your tokenization is complete. So if you feed this into spaCy's tokenizer, let's go to N.Y., you will get these kind of tokens. You want to try that by the way. Let's how about we try that? That would be nice. So instead of whatever, Doctor Strange I can I'll just do it at the top because I want my Doctor Strange to stay here. So I will say let's go to N.Y. See let's go to N.Y. exclamation mark. See and actually we need like single quote because I have double quote as part of the sentence itself okay? So okay, there is a single quote also. So I will use three quotes actually. I will use three quotes by the way. So you you know right like three quotes is like a whole sentence, and since I have a single quote here, that's why I'm using this three quote. Anyways, but see this is one token see double quote let then s let and see then go to go to N.Y. So it works. I just wanted to show you that I'm not bluffing okay? It works! And then we looked at the index operator. Now let's look at the type of this object. See NLP I say right it's it's an object of English language. Similarly if you do doc that's an object of a doc Come on! Isn't that clear? Token is an object of token! There is a span object by the way. So I have all these things right? If I do doc see if I do doc 1 whatever it gives me 1 Strange But if I do something like 1- 5 you have studied this slice indexes in Python correct? It gives you a slice. Similarly in NLP also, sometimes you want a span. From a sentence you want certain span, span means a sub string from that string, and this object is of type span. So if you do type span and you will see in the later video how this is useful, you get an object of span. The okay so let me just clean all this up, let me come up with a different different sentence okay? So Tony and Peter are friends. They have mentor mentee relationship, and Tony gave two dollar to Peter, so that Peter can buy the car okay? Just kidding! So token 0 is this correct? And that token 0 would be what Tony. Now this has certain attributes. So if you do dir in Python general rule, if you do dir on any Python variable, you get all the methods and everything of that class. So my class of token 0 is token correct? If you do type of token 0 it's an object of class token, and that class token has all these methods. See all this. See is bracket, is currency, is digit, is stop So let's try to use some of these methods. So I will first use token0.is and if you hit tab it shows all the matters as a help. So I will say is alpha So it says true because it is alphabetic. Like it's a word basically. It's not a number. If I do is num actually the method is like num it says false. It's it's not a number correct? But but let's look at this: So 0, 1 and 2 okay? So token 2 is equal to doc 2 let's first print and by the way you can use token 2.text also, that gives you the text. So this token 2 is the word two. Now if you call this method, tell me what's going to happen? Any guess? See this is smart! It knows tw2 is a number. It is like a number and it tells you that that is true. What is token3? Okay let's try token3 Token3 is that dollar sign and luckily, we have a method called is ah is currency! It says it's currency true! Now this can be pretty powerful okay? So I can show you a use case but let me first print couple of, just few of these attributes. So I'm running a for loop, I'm printing a token. Token has an index which which is called .i is alpha is something we saw, is punctuation like number nine like currency see For Tony it will say it's not like currency, it's not like number. But when you have dollar it is saying it is currency, it is not a number. But when you have 2 it says like number is true. So these these attributes can be pretty powerful in doing your text analysis. Now I'm going to um quickly show you a file. So let me go here. So I am in my c code NLP tutorial series. Uh sorry directory, which has a file called student.txt Okay, what is there in this file? I'm curious. Ah, it has some student information see. In my class I have pretty talented some talented students. You know they come to my class too to learn Python by the way, all the students. They'll get pretty good in Python and, let's say I'm a teacher and I have this text file of all the students, and let's say tomorrow it's gonna snow. So I want to declare a holiday. So I want to send an email to all the students saying that tomorrow is holiday, hurray! Enjoy! From this text file, I'm showing you a simple text file, this text file can be very big. How do I extract all the emails? All I want to do is from this text file, I want to extract all the emails. I can use regex, you know regular expression, that is something we have seen before. But on this occasion, I find like spaCy can be even more convenient than regex. So let's try this. Let's first read this file in Python, so you all know how to read the file in Python. When you do f.readlines it will read all the lines in a text file, as a text as as an array. So text is an array of all the lines. You know what slash n is right? For the new line. Now spaCy takes single text, so I will convert this array into a single big blurb of text okay? And the way to do that would be, so I'm separating everything by space and I'm saying this. So what this will do is text is an array right? List it will join all the elements by list and it will use space as a delimiter. In between the list elements, when they join it will use space in between. Great! So it's a one single sentence. Now I want to grab all the email ids. So I will first create a document element from this text and then, I will say for token in doc if token. if token is like email, then do something. I can print but I don't want to print. I want to I want to grab maybe all the emails in emails array like a list, and the list has this method called append token.text And when I do that, I get the emails of all my obedient and intelligent students. This is one simple use case. You have other methods too, like like you are a like number. So many other methods okay? So you can check all those in a spaCy documentation. How do I see spaCy documentation? Google is your friend. SpaCy token attributes okay? So I think attributes, you can just Google and find it out okay? I don't know, I don't know how to get to it but you will find it out in in some documentations. SpaCy's documentation by the way, is really good. I really like that and see they have given a very good article by the way. I highly suggest you read the article. You will, see this is the chart that I use in my presentation. Thanks SpaCy folks! Um you can do spaCy token attribute like number. See these are all the these are all the, all those tokens that we have okay? You have is alpha is you have all of those. Okay, correct. Now, let's move on to the next agenda. Now we will try uh a model in a different language. So Hindi is my, is the language of my country India, so I will use Hindi. And the Hindi code we saw right, it is hi And I will create you know some something, I will take some sentence in Hindi. By the way, you can type in like Hindi keyboard and you can type it in Hindi, even with your English keyboard. You can say See, this is like you can type it in Hindi and you can just say ctrl C and ctrl V So similarly, I typed something and I'm doing ctrl C ctrl V okay? He's saying like he's borrowed money from someone, and he's asking to return them. And you will again do the same thing. You will just print all the tokens and you see that it is printing all the tokens. Not only that, along with the token, you can print some of the attribute. Let's say is currency and token. like number right? So when you print that, see for 5000 currency is false, but number is true. And for the rupee sign, this is a rupee sign, rupee sign it will say is currency yes, it is currency. So you take any language model, it is it is having knowledge of these basic rules. Now if you look at this documentation, there is this column called pipeline, and only few languages have these pipelines. But the other language says none yet. See Hindi says none yet and what is that pipeline, we'll cover later okay? Let's let's move on to the new new topic. So now, sometimes you want to customize your tokenizer. For example, you have a sentence like this, and you say for, and by the way instead of doing for token in token sprint, I will try to grab everything in a, in an array. So I will say token, you know list comprehension right? So list comprehension is a easy way to write for loop. So instead of writing for token in tokens, do something. I can move this thing here in a for loop. And whatever you want to do, you can put that thing here. So when you do that, you get all these tokens. Now you're not happy with the default behavior of spaCy here, because see I use abbreviation. You, when you use slang and specific norms of any language, that's when you realize the need of customizing spaCy. So gimme is actually two words: give me You want two separate tokens. So what you can do is you can customize spaCy here, by doing this: you can say from spacy.symbols import ORTH and then, you can say nlp.tokenizer you know we have the NLP object, and you can say add special So I'm I'm adding a special case okay, where I want to customize gimme into into what? So I want to use ORTH give So you want to customize this to give and me correct? So this, don't worry about syntax too much. Maybe look into ORTH and other symbols we have available in later. But this is a simple way of customizing spaCy's behavior where where I'm saying, whenever I ask you to tokenize give me just give me two tokens instead of just one give me token okay and then I'll create a doc element and I'll do this, same thing. When I hit enter, uh what it is saying is special cases are not allowed to modify the text. So you can't do gimme you can't do give and me. But what you can do is, you can do gimme. So it's like gim and me. So you can't modify the actual word. You can split it okay? And if you want to modify, will we have further processing steps. Tokenization is just by tokenizing is kind of a little dumb. It just splits the whole thing into segments. You don't want to change the actual text. That's not allowed. So when I do this, now see I get gim and me as two different tokens, all right? So that was about adding a special rule in tokenizer. Now let's do sentence tokenization. This is something we have looked into it, before also. So I will not spend too much time, and here I have 2 sentences Hulk and Strange both are loving India trip, enjoying the food. To re to tokenize this into sentences, you can say for sentence in doc.sents sents sentence isn't that obvious? Hmm, this is not working! What is the error? Let's read. So it is saying, my NLP pipeline is kind of blank. That's what this error means, it's not saying that. So if you do nlp.pipe pipe names that pipeline is blank. And what does that pipeline means? So what I can do is, here, I can add nlp .add pipe because that's what it is asking me to do. And I'm adding a sentencizer into it. And when I do that, see my pipe names, remember it was empty? Now my pipeline has this sentenceizer component. So what I did essentially is this, if I can open my presentation, blank pipeline is like this: you only get tokenizer. You don't get any other component. But when you build a pipeline, see this the second part this whole thing is called pipeline. And it has different component, we will look into it later, like tagger, parser, name entity recognizer, we'll see all those details. But the simple idea you want to learn today is, you have a blank pipeline when you do this. But when you do something like spacey.law load you get full-fledged pipeline where you can have these components. And and if you have a blank pipeline, you can add a component in the pipeline manually. And that's what we did. When we did spaCy .add pipe here after tokenizer component, we added component for sentenceizer which means, now this NLP object knows how to split these sentence into how to split paragraph into sentences. And when you do that, see it is doing all this. And it's kind of little strange because the component that I added is little dumb, and it is splitting see after Dr when it sees dot it is splitting that into uh, it is thinking it's a separate sentence. So it still doesn't understand English language fully, and we'll see in the next video probably that when we use pipeline like this, it will fix all those issues. So we'll look into the those later, uh in in the future chapter, in the future video. Okay? So that's all I have for the tokenizer. Now it's the most interesting part of this tutorial, which is an exercise. Yes! It's like swimming. If you watch a swimming video you're not going to learn swimming. You want to make a career in NLP, you want to earn big money. By the way NLP engineers make a lot of money okay? If you want to achieve a heights in your career success, you have to work hard. For that reason I am giving you this exercise where, you might know about this thing stats book. So this is a nice book, free book for learning statistics and a paragraph of this book is this: I just took some paragraph from that book okay? That contains couple of urls for free dataset website see free dataset website is something, that you need as an NLP engineer working for a company. So I want you to write a code here, that can process this text, paragraph and grab all these data url, see 1 2 3 4 5 And the second exercise is, you have to extract all the transaction money which is, the answer will be 2 dollar and 500 euro. So the answer of this would be 200 and 500 euro and you have to you know write some code to extract that this exercise will require little thinking. I have a solution here, and it is very tempting to click on the solution. But I know you all are brilliant, not brilliant but sensor students. So I'm pretty sure you will not click on this solution link. You will work on on it on your own and if you click on it before without trying it, look I have a magic embedded here. So when you apply for NLP engineer job, you know you will probably not get an interview call, if you don't work on the solution diligently, all right? So I wish you all the best! The link of this notebook, the exercise, everything is in our video description below. So make sure you check out the video description. If you like this video, share it with your friends, give it a thumbs up! If you give a thumbs up, you're paying fees of this session okay? So if you want to pay back, give it a thumbs up, share it with your friends on LinkedIn especially, or WhatsApp whatever, and I will see you in the next video. Thank you!