Please welcome to the stage NVIDIA Apparently, there's a huge line out there and this room, but once they come into this room, it you guys, hurry up, hurry up! I'm going to start! and there are two other breakout rooms, that they're taking too long getting in here I just don't understand is once they get in of a sudden chaos ensues. It's very, very nice to largely remained unchanged for 60 years. The year but the year after my birth, the modern computer processing unit, I/O subsystem, multitasking, software compatibility across a family, backwards software engineers. I describe modern computing. the early '90s, the PC Revolution kicked it into Driving the marginal cost of performance we reduce the cost of computing by about 10 times, 15 years a thousand times, 20 years 10,000 times. computing costs reduced by 10,000 times, more Could you imagine if everything in life reduced valuable to you - reduced in cost by 10,000 times, years by the time you became an adult. Something now it costs a dollar. Well, maybe that's and then one day it stopped. Not precisely one It still continuously improved a little bit every we worked on another form of computing called use - there's nothing easy about using to formulate the problem from what is originally you do faster and faster every year. But you have Parallel algorithms as a whole field of science. and we believed that if we could accelerate there are some applications that we can make a We can make something impossible possible. cost-effective. Or something that cost And so we called it accelerated computing. our company's history. And one application we were able to accelerate. The first one accelerated computer graphics and video games games company. But nonetheless, we kept pursuing and games because it was simultaneously a That rarely happens. The confluence of large the ability to drive incredible technology graphics and games. Well, to make a long story that first sighting was Alexnet, the first contact it started our attention - our attention to amazing happened, and it led to today. Today, I'll something led to generative AI. Now, you've heard The fact that software can not only recognize word &quot;cat&quot; and generate an image of a cat. It can prompts, like on a surfboard on a sunny day off a ski hat, you know, whatever. You just keep on AI is able to generate that. We have now taught a these pixels, to recognize the pixel, understand generate from the meaning of something pixels. just about any data is incredibly transformative. this is the beginning of a new revolution, a new that. In this new Industrial Revolution, we're in previous industrial revolutions, the last one, it, this thing called a Dynamo goes off to work, value that we depend on today. Water comes into you boil it, and what comes out the other electricity out. Magic electricity used everywhere it. A new facility creating a new product of great of software, and the software is produced, it, scientists have to go create it. But then, after building with machinery we call GPUs, essentially You give it raw material, this raw material amazing thing comes out. Numbers go in, numbers things, unbelievable things. And they could be you know, but it could be used in healthcare transportation and cause cars to drive, and industry that we know will benefit from this new So, a brand new thing that the industries and it's going to be produced in facilities and factories using and producing AI, and the AI being A new Industrial Revolution. None of this existed front of us. This next 10 years, you don't want to Unbelievable new capabilities will be invented, some researchers. And so today, I thought we would language model called Transformer. And the way we room, and there will be very little moderation. I wish you were there. There were a lot of deep you know, we're going to see if any of them land, of arguments. And so, what I thought I would do is and then we can see where it takes us. And so, of the Transformer, and they were the authors of Okay, so let's have Ashish Vaswani. startup company called Essential AI. Noam Shazeer, called Character.AI. Somehow all of their startups just not in the right order. I knew it all along, and the letter &quot;i&quot; in there. I just didn't know nice to see you. See you, Jakob is also really interesting. Inceptive. Okay, Llion Jones, founder and CEO of Cohere. My goodness, ladies and gentlemen, the only person who is hero. Illia Polosukhin, come on the stage. NEAR Okay, and we have a colleague and friend who and so our hearts and thoughts are with Nikki. first of all, they've actually never been in I got this work-from-home thing has gotten out innovation and invention. So, it's great to have and we're going to talk about the importance transformative capability of the Transformer, and as I was saying earlier, everything that we're the fact that we can learn from data of gigantic as well as spatial data, but learn from just a and patterns and to create these gigantic my first question, and you guys all dive into it, other off, talk over each other, disagree with a whiteboard then. For that, we need a whiteboard, back to the beginning. What were the problems, you What were the problems that you were struggling everybody had a different problem, probably, but answering. So, very simple, you go to Google, you Google has very low latency requirements. And you know, search results like tons of that really quickly. And models at the time, because there were recurrent RNs, and RNs had some to read one word at a time. That's right. We were actually train the most advanced architectures on. just feed-forward nets with n-grams or so as input on, you know, at least in some problems that they basically always outran the much more And so, it seems like a valuable thing to fix. back in like 2015, and you could see that if you here is the best problem in the history of the the next token, and it's going to get so smart and just want to scale that up and make it better. And in the, you know, to deal with, right? And then, &quot;Hey, let's replace it with convolution or with this.&quot; And I think it was, I like to compare it and the Transformer is like internal combustion. on the steam engine, but it would have just been a with internal combustion. So now we have electric right? That's the next wave. Our, I mean, two especially during the time of the Transformer, when I was in grad school, when I was working I'm not going to, I think that gradient descent, teacher than me. So I'm not going to learn these descent to do everything for me.&quot; And the second from the bitter lesson, that general architectures the long run. Today it's tokens, tomorrow it's and they'll sort of start mimicking our activities do.So, the Transformer was self-attention, it had this quality that was extremely general, second thing it made happy was physics because, was that matrix multiplications are a good idea. both those things together, this motif has been of rules, gradient descent will one day learn like, we, like, all of deep learning has been, a GPU, and now we're building an AI model the supercomputer is the model now. Yeah, that's you guys know, we're building the supercomputer Now, what were the problems you guys were definitely. And it seemed so hard five years would translate, maybe it would be slightly wrong. you don't get any data at all. They just learn to it just emerges that the model can translate, led to &quot;attention is all you need&quot;? So, I came up at the point where we were looking for a title, recently started throwing bits of the model away our surprise, it started getting better, including like, &quot;This is working much better.&quot; And that's where the title comes from. Basically, what's started with that barebones thing, right? And then later on, we kind of knocked them out. And a lot were also a really super important piece. if you watched it, &quot;Leon,&quot; where this guy is in a they didn't exist anymore. I was wondering in that universe. I don't know if you watched something like that. Yeah, no idea, sorry. He's So, you guys, well, hey, I think this is were some of the other choices? Who came up with an excellent name, by the way. Um, you, I think Yakob had for it. I was like, &quot;That's a name, the model does, right? And every step actually as opposed to having to iterate over us. That transformed. Yeah, oh, look at that. That's transformed. Before, nobody thought to use the too simple. I was like, &quot;Oh, every...&quot; That's you know, like, I was overruled. Everybody thought what was the name you came up with? I had a lot. something called Cargonet. I had, like, there... One was attention. One I called recognition net. And so, convolution, attention, recognition, That's... That's horrible. I'm glad you were... the reason it became such a general name is that, on translation. But we were definitely aware of something that was very, very general, that really And I don't think we predicted how well when Transformers were being used for images, logical to you guys, but the fact that you could I think that was architecturally there very early tensor library, we were really focused on scaling just for language. There were components in there output side. Lukash said what he was working on himself. All of these ideas that we're starting and being a joint model, it was there day zero, that's what Luk was going after. It didn't work. But I mean, there was this other paper, &quot;One Model Yeah, eventually, it started working, but it was were percolating. And it took some time. Lukas's and they go from image to text, text to image, on everything. And that idea is really what which Writer Team has, you know, succeeded, and I think that North Star, it was there on gratifying to watch that come to fruition. We're interesting that in so much of knowledge, it is text to text. You know, tensor to tensor, this translation idea, is quite universal. And in Or maybe something that we like to call biological software that starts its life as a program that a GPU. In our case, read. Basically, the life specification of some behaviors you want, say, in a cell. And then you learn how to translate That's right, this idea really goes all the into computer code, but also specifications of Any day now, into the actual molecules that we produces all this? You have to run experimentation this. The data does not yet exist. There are tons download, largely available, actually still openly still largely publicly funded. But really, you to the phenomena that you're trying to model something like protein expression in mRNA vaccines Alto, we have a whole bunch of robots and people who were previously biologists. Now we think working on actually creating that data and So, the idea you're saying is that some of the learner, universal translation, were there architectural fixes, enhancements, breakthroughs think are really great additional contributions I think on the inference side, there's been tons efficient. I still think it kind of disturbs think the world needs something better than the succeeded by something that will carry us to a new question to everyone here. What do you see comes is too similar to the thing that was there years how similar it is, like you said. And people do as if I'll just magically know because I'm on the to point out an important fact about how these you have to be clearly, obviously better. Because enough to move the entire AI industry to the despite the fact that probably technically it's you know, everyone's toolset, right? But what better? Context window, you want to make better, We want to make it faster. Well, I'm not sure if much computation right now. I think they're doing that more efficient. Thank you, but actually, it's amount. You need any amount of computation, right? effort and ultimately energy on a given problem. that's easy or too little on a problem that's solution. That's a real example. It was like model, it uses, you know, a trillion parameters, doing that. So I think adaptive computation is one how much computation to spend on a particular immediate follow-up paper that I know a subset of which targeted exactly that. So these ideas prep paper a year earlier, the mixture of experts, kind of now folded into the Transformer, but it if folks here know, but we kind of failed at we wanted to model the evolution of tokens. It evolves. We iterate, we edit, and that allows us but also have them as a part of the process. generating it, they can actually get feedback. all of us read Shannon's papers, so yeah, we were and perplexity.&quot; But that has not happened. And organize our computation well, right? And that the interesting property that they're iteratively And yeah, I mean, this fundamental question of what knowledge should exist outside it, retrieval And also, there goes for reasoning too. What systems, and what reasoning should be done I do believe that large models will ultimately adding up trillion numbers to add up two, that's if asked 2 plus 2, the AI should just pick up a we know, which is a calculator, to do 2 plus 2. 2?&quot; or &quot;Is 2 plus 2 the right answer?&quot; then it That's right, that's right. I'm pretty sure all pick up a calculator. GPT does this right now, is just too cheap right now. It's too small. like Jensen said, you are producing computation something on that order. Thank you for creating so half a trillion parameters and you're doing like like a million tokens to the dollar. That's like a paperback book and reading it. It's so cheap. or more valuable than efficient computation on cancer and that sort of thing is, but even just your doctor, lawyer, programmer, that you pay like a million to play with to make it way smarter. the right word is going to change the world. Yeah, smarter, the right interfaces are essential. How sort of decompose the task that we're doing in a And if we ultimately want to build models that can the interface is going to be absolutely crucial. start a company? You left Google after you Transformer and you started your company. could company and why you decided to start it? Because you know, your company is working on. So, yes, models that can ultimately learn to solve new They watch what we do, they're able to understand doing. And that's ultimately going to change Basically, in 2021, one of the big reasons why I smarter, you couldn't make these models smarter in and put them in people's hands because you kind get feedback from them, and make these models build something useful. Learning does require an was hard to do it in the vacuum of a lab, and at the time. Yeah, that's cool. And Palmyra, yeah. know, the biggest frustration I had at the time is not getting out to everyone. This is like the Can you guys imagine Palmyra being impatient? The people, let them do a billion things with deep learning Zen. This is when he's calm. Zen sitting next to him, like, &quot;Yeah, thank God for Jensen, and thank everyone. The ultimate goal is to go to Character. You've got to go check this let's start by doing this for real. Let's build and get billions of people able to use it. And it just for fun or for emotional support or and it's really working. Just going to grow the really, really working. Congratulations. Thank little bit about biological software, but maybe I co-founded Inceptive. You know, for the need or impact on improving people's lives with this broad, but not very direct. My first child was a newfound appreciation for the fragility of life. out for protein structure prediction, winning AlphaFold 2 and AlphaFold 1 was that they started their model or architecture with that. So it for prime time in molecular biology. And then ethicacy results came out, and it was very clear life. With the RNA World hypothesis, there is no the longest time, it was the neglected stepchild almost a moral obligation. This has to happen, thought of it as drug design, but I love that you biology. It makes so much more sense, actually. this compiler would have to be learned. We're we have to go learn this compiler. That's right. obviously you need a laboratory to test it can't work. I'm pretty excited, but I can see it leave. It's still very early days, but I can tell company called Sakana AI. What does &quot;S&quot; stand for, very weird in English, right? Call your company the Japanese seem to like that. Yeah, so the to be evocative of a school of fish. We want The analogy is that a small fish can be quite together, they become quite complicated. But when we say &quot;nature-inspired,&quot; so I want to dive right? What I try to push on researchers when you go from humans trying to do something to actually just using computers to search through the Deep Learning Revolution was an example of just learning them, and it worked so much better. to remind you that the mad amount of computation we can do apart from gradient descent. We can currently hand-engineering. And actually, I would there's a sort of a time difference problem, of surprised that we have something to announce going to be open-sourcing. And it's very on-brand now, is model merging. But it's being done by how to merge these things. So what we did is we and then used a very large amount of computation the space of how to merge and stack the layers. keep a lookout for that. Wow, okay, all right, actually. I'm also under strict orders to say, ID. Yeah, I think my reason for starting Cohere that I thought could change the world. You know, a new modality. So I thought that should change we work, the way that we interact with all the it wasn't changing, there was stasis, and there were faced with, for those of us in the know, and to close that gap. I think the way that I've in the sense that Cohere builds for enterprises. adopt and integrate it into their product, as the way that we want to push that technology out and help companies adopt it. You know, the thing I he's super excited, and that's what Gnome looks true. Cohere, okay. Lucas, well, I did not find You went on to change the world. Go ahead.Yeah, I as Al Capone was asked why do you rob banks, he time I joined, that's where the best Transformers We know you can take a ton of data and a ton of I can remove the ton of data sheer, we'll just and then I want to ask these guys the next to leave Midway Y. And because, kind of similar to going to make progress towards, you know, pretty learning is eating software, is to teach machines software and transform everyone's access. Now, We did not have as good compute yet at the time. people instead to actually generate more data. have this ability to put something in front of up realizing we need the new basic primitive, money is what allows us to coordinate people at which is a blockchain, which has been running in the blockchain space, with multiple millions blockchain, but they are actually interfacing now we're starting to use that to actually bring And I think fundamentally, I mean, to, you know, probably controversial elsewhere, copyright as We have a new generative age that is in front of now is broken. And the only way to do that is to and blockchain. And so one of the things we're people to contribute data that then models Cohere. hold a new positive feedback system exactly into new economy on top of it. We've got programmable programmable money. I love this. And so one of generation of GPT models have training datasets which represents approximately the size of the internet freely. And so what's next? What kind like reasoning, you know, so on and so forth? And the data come from? From interactions? Like, And that's, like, you need massive platforms people get from this to do this. And then on the to actually become smarter. You can do that to that incredible pre-trained model, the starting Is there a way for models to interact with each synthetic data generation techniques? Like, kind of between all of us, we're working on every I think the next big thing that's coming is this, and a lot of people are working on it. But now. We're sort of writing prompts by hand and that we think they should. And I, of course, through that space and actually learning how to powerful reasoning that we want. Another way of to generate things that we want to consume as should be trained on all the stimuli that we So basically, any type of video, audio, any type spatial information, spatiotemporal information, sure if everyone understands that reasoning and if you have a model that can do reasoning, then processing, why is this thing following it? But it you know, oh yeah, that comes out, and it computation that it puts into reasoning. It's like you can just let it go and then try to build it in design its own experiments so it collects the most continue searching. But I do think that reasoning, reduce the amount of data you need. But then the which is where all the interactions with the real will be a new age where we still pre-train on some like high-quality things, will make it easier to for pretty much teaching machines to become better billion tokens, so people learn pretty well. So right. I would also argue that something that's a of benchmarks and evals too. So like, you know, say, automation? And so breaking down real-world is also important because our models could deploy, get more data, and then once that loop on more complex tasks, one because they're also doing, so that gives them more data, then they can primitives, like do more abstract tasks. So I do progress is also going to require breaking down that we've done with some evals, but the science the science of code generation. And then you can't exactly, really important. Yeah, yeah, yeah. What are the three questions you guys want to off one first question. So, what do you think? Oh, okay, wow. Well, the funny thing about those remember the pre-Transformer age, right? But a lot the paper for the first time, it was very obvious the problems that we were having back when we were in these models. But it seems that because people they have to rediscover all the problems. So my and we'll probably end up with a hybrid model. fun fact I find is that nobody's actually playing for a variable number of steps and train that recurrence, because what this model does is with for every token and resolves things and does steps, you can only do, you know, actually five getting more context. And sometimes you don't need the different recurrences you can do on that? tokens? Like, exactly, that's a pain of I have this personal belief that we recurrent models with gradient descent. impossible. I mean, LSTMs, they did poorly, better because they're structured to it. But with gradient descent. Maybe you need to train in some sense, recurrent. I mean, we but it's not so clear how we're trained with is a way, it's just not gradient descent, and Well, guys, it's been so great spending time with now and then and see what amazing magic can come whole industry that is grateful for the work that you. Thank you. Thank you. Thank you. Thank you. give me one? I'm going to do one, and I'll give Ashish Vaswani. You transformed the world. Okay, we go. Beautiful. Thank you very much. Thank you, Thank you, Lukasz. Thank you, Illia. Thank you