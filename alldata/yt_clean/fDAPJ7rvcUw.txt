There is an enigmatic and at work inside everything from and neural networks, to quantum physics. It's simple enough for high school students to grasp yet so complex that even seasoned This operation is called matrix multiplication. Matrix multiplication is a very mathematics that appears in many and in physics. A matrix is a two dimensional can perform operations like Researchers have long sought to multiply matrices together. So if you even just make that a problems come into reach. Where for now, we would say that's too big to be computable in reasonable time. However, actually finding faster a huge challenge. But thanks to a new tool, a long standing matrix multiplication record, one that's more than 50 years old. What's their secret weapon? Students of linear algebra are matrices based on a centuries It goes like this. Multiply elements from the and the first column of matrix B and add them to get the first element of matrix C. Then repeat for the first row of matrix A and the second column of matrix B, and And so on. Multiplying two two takes eight multiplication steps. Multiplying any two N by N matrices with the standard Which is why applying this matrices quickly becomes If you take a matrix that is have a computation time that is So you can imagine, if you doubled it a eight times more a couple of reach the limits of what a Enter Volker Strassen, a German mathematician known for his In 1969, he discovered a new algorithm to multiply two by two matrices that requires only seven multiplication steps. Going from eight down to seven multiplications and the new addition steps look more complicated. But Strassen's algorithm offers dramatic That's because when multiplying large For example, an eight by eight nested two by two matrices. So Strassen's savings applied to these smaller matrix Applying Strassen's to an eight by eight matrix results in a third fewer multiplication For very large matrices, these savings vastly A year after Strassen invented his algorithm, proved it was impossible to use six or fewer multiplications to multiply two by two matrices, thus also proving that Strassens, with its For half a century the most multiplying two matrices of any them down and apply Strassen's algorithm. That was until October 2022, a new algorithm Strassen's, specifically for multiplying two four by four matrices where the elements are This new algorithm made it possible to by breaking them into four by four matrices So who or what was behind This This new algorithm was artificial intelligence research For more than a decade, DeepMind has garnered to master a host of games, everything from Atari Pong to chess. Then, in 2016, DeepMind's AlphaGo achieved it defeated the top ranked human Go player, Lee Sedol , in This victory shattered the limited notion of what's possible for computers to achieve. DeepMind then set its sights on a problem even more challenging than Go. I was like very surprised that don't even know what's the optimal And at some point, we realized that this is actually a very good fit for machine learning techniques To tackle matrix multiplication. algorithm descended from AlphaGo AlphaTensor is built on a called AlphaZero. So what one needs to do is to go really beyond the AlphaZero and to tackle this huge search to find this, these needles in a AlphaTensor isn't the first mathematical research. In 1976, two mathematicians proved what's called the Four Color Theorem using a computer. The theorem states, you only need four neighboring regions match. The pair verified their proof by processing all 1936 required requiring more than 1000 hours of computing time. Back then the larger mathematical community was not prepared to cede logical reasoning to a machine. However, the field has since come a long way. AlphaTensor was trained with a technique called reinforcement learning, which is kind of like Reinforcement Learning strategically penalizes experiments with different ways driving the program towards an But what kind of game should AlphaTensor play matrix multiplication algorithms? This is where the term tensor in A tensor is just an array of Vectors are 1D tensors, and matrices are 2D tensors. The process of multiplying any two matrices of a given size can be described by a single unique 3D tensor. For example, when multiplying any two, two by two matrices, we can build the corresponding 3D tensor. Each dimension of this cube represents one of the matrices, each element in the cube can be one zero or negative one. The matrix product C is created by combining elements And so on. Until you have the full matrix Now, you can use a process break down this 3D tensor into Similar to taking apart a cube puzzle. One natural way to break tensors down is into what's called which are just products of vectors. The trick is each rank-1 tensor here describes a multiplication step in a matrix multiplication algorithm. For example, this rank-1 tensor represents the first multiplication step in the The next rank-1 tensor Adding these two rank one tensors Here are the next two rank-1 multiplications, A1 times B2 and Eventually, the entire standard multiplication steps is 3D tensor into eight rank-1 tensors. These all add back up into the original 3D tensor. But it's possible to decompose a Strassen's seven multiplication matrix multiplication looks like this. These rank-1 tensors are more complex. And this is still a full decomposition, but in fewer steps, which add backup to the original tensor. So the fewer rank-1 tensors, you use to the fewer multiplication steps used DeepMind's construction of a for AlphaTensor to play, and learn Find an algorithm for this matrix multiplication multiplication steps possible... But it becomes a clearly defined once it's formulated as: decompose this 3D tensor using as few unique rank-1 tensors as possible, It's really hard to describe what, what the search space looks like. In the particular that's, that's quite, it's quite convenient to formulate it in that space, because then we can and our machine learning techniques very large, but formalizable AlphaTensor's play was simple. It was programmed to guess rank-1 tensors to subtract from to decompose it down to zero. The fewer rank one tensors it used, the more rewards that got. Each rank-1 tensor that you is has a cost, has a cost of let's say one. And so we want to figure out what's the way of And so that's what the system is trying to learn how to do it's learning to estimate, well when I'm in this kind of configuration, roughly how many penalties do I think I'm going to incur before I get to the goal? But tensor decomposition is not For even a three by three matrix multiplication with only elements zero or one, the number of possible tensor decompositions exceeds the number of atoms in the universe. Still, over the course of its AlphaTensor started to home in on patterns to decompose Within minutes it rediscovered Then the program went even further. It beat Strassen's algorithm for in modulo-2, where the breaking the 50 year record. Instead of the standard algorithms 64 multiplication steps or Strassen's 49, AlphaTensor's algorithm used only 47 multiplications. AlphaTensor also discovered thousands of other new fast algorithms, including ones for five by five matrices in modulo-2 . So, will programs like AlphaTensor, churning away in server rooms pulling out new mathematical discoveries from lines of code make mathematicians obsolete? Ultimately, I think this will not replace like the mathematicians or, or anything like that. This I think provides a good tool that can help and guide their intuition. That's exactly what happened just days after the AlphaTensor results were first published in the journal Nature. Two mathematicians in Austria, Manuel, Kauers and Jakob Moosbauer used AlphaTensors 96-step, five by five matrix multiplication algorithm as inspiration to push even further. And then Jakob suggested we just take the algorithm that the AlphaTensor people found as a starting point and see whether there's something in the could have an additional drop. And indeed, that that was the case. So it was a very short computation for, for our new mechanism that was able to reduce the 96 to 95. And this we then published in the arxiv paper. The right way to look at this collaboration between a particular kind of technology and mathematicians. The true potential for human and is a frontier that is only now being fully explored. I don't think people can be made irrelevant by any of this kind of work I think it empowers people to do more.