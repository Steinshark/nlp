- Hi, welcome to Generative My name is Emily Webber. I'm a principal machine at Solutions Architect and at AWS. And today you are gonna You've heard about generative AI, you've heard about all sorts of things. The purpose of this class So we have, in fact, no that you're gonna get to So they're broken up Each class is about 45 minutes of slides. So you're gonna get to learn You get to dive really deep and and interests that you may have. And then we're gonna have a hands-on demo. So each of the 60-minute that you'll be able to So you can fast forward, you take screenshots, do and then you're gonna get So all the content you're gonna and then step through on your own. And so with that, let's get started. So today, the session we're this session is what What on earth is a foundation model? Where do they come from? How do they impact generative AI and the end-to-end life cycle for interacting with maintaining, foundation models. And in particular, in we're gonna take a look at and especially on Amazon SageMaker. You'll notice that there on this slide that's 'cause we're all fond of the Llama models. And you'll see them come up quite a bit. And so, hypothetically let's say I asked you to learn which right away, you know, Like, obviously, how could on the internet? But for the sake of argument, so you'd probably look at the structure of the most popular sites on the internet. Maybe you'd map out some of all of the different And then you try to store your knowledge of these things, right? You might store your notes, you maybe, you'd wanna store the files. Obviously they're nicely but in any case, you'd or something, but it's gonna The largest bottleneck here is really just the human time that it would take to, to literally read everything online. And so just to put some there were just under 6 And actually the average time on a website is 52 seconds. So that's pretty short page viewing. But nonetheless, multiply those together, you get right around 5 billion just to view all of the And now realistically, if you obviously you'd spend some time on less pages. But in any case, if you look in a given year, assuming a day, maybe I'm working maybe I'm working 50 weeks in a year, it's gonna take a human to read everything online, Obviously that's multiple human lifetimes. And then on top of that, the There's so much information online, there's so many new creatives, things that are popping up constantly. And so that number of just continues growing. A foundation model can So this is why foundation This is why foundation They're able to, through through distributed training, through PyTorch and scripts and what not, they're able to quote unquote or parse massive, massive massive amounts of data. So this is why foundation You can do a lot with foundation models. Essentially, a foundation model that's designed to cover And so in traditional you would use say, a classification model or a regressive model to And foundation models are they're trained on so many and these massive data sets. And then essentially they learning styles from those files online. So they'll naturally learn classification, they'll naturally learn they'll learn summarization And so today you can build applications using foundation models There's a huge amount of to design and develop to incorporate foundation models Design new applications to code generation, audio generation, video generation, search It's a really exciting space. And interestingly enough, many machine learning tasks where again, years ago we would've looked at this as just a classification task. So you would take a tax, for I'm not that into this house. It's too expensive and it's too far from the train line, right? And we put this into a And then this model to perform binary classification. So it's just labeling positive or negative. And so here the model would return with a negative sentiment. So it's gonna output sentiment negative, and that's traditional classification. Today, you can recast this So you can take this same and then give it an instruction. So in the prompt for your you can just add a prompt that says, classify the sentence into You can have a more You can say like the So this instruction, you can You can word it as, is Are they likely to buy You can use really any type and that you find works And so in any case, you'll as it's called into the model, and then your agent will And so again, whereas these very discriminative that were explicitly trained ahead of time. Now most people are because they're trained and are very powerful. And also because they're so So you can take the same this text or ask it to translate this text or ask it to stylize this text. And so it's very computationally to have everything bundled which you can then use for n And so there are many ways to Once you pick a foundation most people will want to There are core trade-offs in and we're gonna understand those today. So on this X-axis here, you see So complexity and cost with each other in the is a lot harder to do, generally it's gonna You're gonna need more experts to do it and you're gonna need And so that pushes up your cost. At the same time, in many cases So we're gonna look at a number of different customization techniques. We're updating and that while they can be more they usually will increase accuracy. And so the first one is So let's say you pick or your Titan or your any you're trying to work with, you pick this and then just like we saw on the previous slide. You send it some you send it some prompts, and you'll quickly start to realize that when you update your So there's, I'm sorry, you get a better response So when you change and and you stylize the prompt, you can actually develop a That is a way to boost the accuracy and the performance of your model. For most customers and that isn't enough. That's the starting points. The next really common phase to move into is called retrieval augmented generation. We're gonna dive into this So fear not, but retrieval or RAG basically refers your user will ask a question, they'll type in some type of query, and then we'll take an LLM. And so we'll take the to generate embeddings from that query. And then we're gonna search in So we'll look in a corpus of documents, we'll find the most similar document, retrieve that document, and Actually we can stylize the and to the customer based on And so that's retrieval we're gonna have a whole So lots of, lots of ways you So retrieval augmented from prompt engineering. There is another way, however, to improve the performance specifically for a downstream task. So like, let's say I'm in healthcare, let's say I'm in financial services, or let's say I'm in and I like working with Llama, you know, I enjoy working with Vicuna and Falcon and all of these open source But what I really wanna do and I wanna customize so the performance is exactly in line with what my organization wants to see. So again, prompt engineering Retrieval augmented generation Fine tuning is another step, it's another way to take be those large samples in the case of unsupervised pre-trainings, sort of like a continued So you can do that or you you can put them in your That's called few shot prompting. You can also actually of the parameters producing a new model that's then fine tuned on your domain and on your downstream task. Don't worry, we'll learn about all that throughout the class. And then my personal favorite technique, it's all the way out here. You'll notice this jump. So there's this sort of in complexity and cost, And so pre-training really it refers to creating a new foundation model. So it means taking your that you're interested in working, be that in language or be that in vision or some new niche modality that Which by the way, let me know if you are, 'cause that's awesome. But in any case, you're creating You've got your neural custom data sets, your and you're hacking, this Pre-training is unambiguously the best way to get a much more accurate model, now that is contingent on you on certain key steps. And we'll learn how to do So there's a whole lecture on each of these topics actually, so you can learn how to do And then we come to find out are actually built on human feedback. So this is something I in a past life before moving I was a creative writer actually. And so I spent so many literature and having these you know, 10 people can and we all interpret the Or all of us watch a movie and we interpret the movie differently. Like we see different themes in it, we see different characters we respond to it differently. And so generative AI and generative models are really powerful when they at scale. And so we're actually that's just on this. So the technical term here with human feedback You start with a generative model. So this can be a large language model, this can be a computer vision model, this can be a modality star, And then you'll, so you'll start and then you'll send in You'll send in, maybe you have 10 prompts, maybe you have a couple thousand prompts. And these are like directly These are directly from your application, directly from your domain. They really matter. So they can be about in your call center. They can be about answering They can be about generating They can be about generating blog posts. They can be literally any type of content that you're trying to create. You can do this. And so you'll catalog a You take the prompts, you send the prompts to and you'll find out that the model gives, can give you many So maybe you'll get four I'm sorry, four or five different So you have one prompt, you You're actually gonna send to your users or to data labelers. So humans that you'll hire for and ranking these. So the humans will pick their favorites, they'll rank these sort of best or worst, you'll update your training data sets, and then you're actually So again, whole lecture but we're gonna learn how that aggregates this And so this is how generative AI models are able to sort of get around of subjective human preferences, like particularly in There are so many ways to and interpret images. And so using this sort of at scale, we're then able to update and to improve the And so a couple of the we looked at instruction fine tuning, where you take key and then do sort of a basic And then we looked at this with human feedback lifecycle. And both of these are critical and improve your own foundation And so a couple model spotlights for you. So this model spotlight is So in this case, we're sending in a prompt So we're sending a prompt of Paris, rebuilt near the because why not? With great weather, Sandy et cetera, et cetera. And then we get this sort I did this myself. I went into the SageMaker to find good prompts. So I go look up good paste it in to my SageMaker and then I generate this and we're ready to go. You can also, in stable diffusion, you can add negative prompts. So negative prompts are handy because it's a way to I really don't want it to be in a certain category. And so here it's funny and no green, and yet and both green, but this way it sort of So if we hadn't included most likely there would And so this way we were kind of that sunny motif. And then when you're interacting you're sending in So you can set the sort of dimensions of the output image that you want. So here I'm giving it a much larger width, so width of 720 because I want And then a standard height of 512. If you want a portrait or a square, you can do 512 by 512. And if you want the other you just rotate it out. Great and then a couple of The guidance scale is interesting because this is a way to how intensely you want it Like if you want the model about the prompt and do nothing else, then you set a higher guidance scale. You max out that guidance scale. If you want the model to be to have some sort of you know, liberties in how it interprets the prompt, then you reduce the guidance scale. It's more common actually to like this with a higher guidance scale. So in this case, it's interesting that I use such a complex prompt with actually a pretty because this is a lower number. So I'm giving the model more and it still comes back beautiful. The seed is another interesting because setting the seed is sort of a way of giving the model like to explore. So if you set the seed to it will encourage the model to like pick a completely different style different colors, different It will still be following your prompts, particularly depending But when you just change the seed, that's an easy way to just or a different response And so let's take a look at because these LLMs are to talk about in technology today. But as we'll come to find out, they're not that new actually. Foundation models and have been around for a long time. So back in 2017, right actually the transformer whose name I don't remember but no, I'm kidding. So the transformer is a that is designed to operate So the core transformer was actually built to handle translation. So it has two parts, an So it takes in a string of of text originally again done to enhance machine translation. And transformers were interesting because they operated So they set a new state of the but more than anything, there were a net new way of thinking about how to learn sequences. So rather than recurrent neural rather than CNNs actually. So yeah, so transformers became of approaching knowledge using this core self attention mechanism. That's a lot of matrix multiplication. And so in any case, the year after that gave us two models that sort of supercharged NLP. And so one of them was of course BERT, the bidirectional encoding the BERT model and BERT for classification. BERT models are encoder only, which means it's going to a larger output and produce a smaller, as produces smaller outputs. And BERT models are handy and for smaller tasks. BERT models tend to fit on single GPUs or single accelerators, We also saw GPT-1 in 2018, which back then was sort of And so in any case, many years of NLP fascination Year over year we saw of language models where these scaling laws to help us just be bold and throw even more data at these models and really And so you see that now in 2023, there are a lot of foundation whereas previously there were just a few. And so what this timeline tells and large language models There's a very active, mature research community and you too can benefit That's all we're saying here. So one other foundation AI21 is an AWS partner. Their models are available on the SageMaker Foundation model hub. They're also a customer for the record. And so in any case, this jumbo means it's quite large. So jumbo means that it's north And we'll give it a prompt. And so here the prompt I'm about a dog, running down the street and then I click generate text. And this is in the SageMaker by the way, where we that we'll take a look And so we generate this text about a dog named Max. He's a very happy dog, he loved to run. Once he was out for a walk, Mr. Jones, Mr. Jones was holding his leash, Max pulled him down the street, Max was excited, he just wanted to run. Mr. Jones was having a So it's funny because but we don't really There's not really a conclusion, So anyway, there are further but it's still pretty close And then on this side, I'm sort of giving the So I'm seeing like how complex it can go. So if you found two shoes, one for the right foot how many shoes would you have? Now obviously we're pretty sure but we just wanna make this sort of basic common sense. And so if you found two shoes, one for the right foot, you would have two shoes, great. So the model is able to sort of read this, somewhat more complex prompt and give us a reasonable answer. So that's good. All right and so the typical starts with picking a And again, the second is gonna be just about how to So we're gonna dive pretty deep into that. So we'll pick a base foundation model according to the domain, the performance, et cetera Then we're gonna use prompt We'll develop prompt templates, we'll hack the syntax to and then we'll evaluate the We'll actually send the to our users, see if they like it, see what their responses are, store those human preferences, So actually improve those to make the model even more Then we're gonna update that and put it back into our application. And so in the lecture we'll learn about each of these And so with that, let's So in this demo we are as it were. This is gonna be a notebook which is running on SageMaker Jumpstart. And we're gonna interact with this to learn about text generation. So feel free to follow The short URL is right This is already a public or you are welcome to and have the notebook sent to you in your manner of choosing. So now that you have the All right, so here we are. As you can see, of course, we're in AWS, sitting And this is SageMaker, right? So in SageMaker we have these are models that to do all sorts of things, So some of them are open source models. As you can see we have Falcon some using BF16, actually But in any case, instruct By the way, if you're gonna fine tune, feel free to start with models that haven't been instruction fine tuned. If they've already been then you're not gonna wanna But if not, then that's But in any case, what's and especially the playground, So let's say we wanna work of a proprietary model that's available in SageMaker Jumpstart. We'll click view model. And then after we've clicked view model, it's gonna take us to in just a minute here. Great. All right, so the model details page, and we see this is indeed AI21 Jurassic, and what do you know? There is a playground available. And so the playground is really handy. It's a way that you can And so essentially this means you're not of course hosting the model, you're just sending it the prompt and getting the response back. And so we can choose a few examples, actually we can choose, and then let's see if we can make this bar a little bit larger. Here we go, great. So we're gonna write sections, we're gonna write sections for the following title, how blog sections. Okay so clearly we have So this is your few shot prompting, and then now we're asking for this new blog. So let's see what we've got. So we're gonna generate the text. All right and here we have new sections. Great. Okay, so clearly it works. We get content out that seems reasonable. And now let's say that through the playground, we're ready to move into That is over here. So in this view, you can see I'm running What is SageMaker studio you ask? So it's a IDE for machine learning. But beyond that, it actually So every notebook that is actually, we call it a What that means is it's a where it has the ability to And so I can change this out. For example, just in your the stop, right? You can give this a click and then actually select And now remember, this isn't like the visual that you see here that is provided by a Jupyter server, which is actually built And so just to see that So let's say we go out to the AWS console, and let's check out SageMaker studio. So right up here. And then let's say we that's under domains. And I'm using this diffuse domain and I'm running on this Falcon. And you'll notice that there are a couple different parts here. One part is this Jupyter again, built to managed by Amazon. And this is running your visual here. So the Jupyter Lab experience experience that is literally on this Jupyter server. And so every time we run a notebook, what we're gonna do is into the Jupyter server. So let's say I wanna So let's say I create a new notebook and just for fun, maybe I need a GPU, or maybe I wanna run on So I have all of these instances, I can choose from many C-series, accelerated all sorts of things. And so yeah, so what I'm from any of these instances actually lemme just pick one to show you. So for each notebook, again, this is gonna be running And then over here on this left hand side, as the instances come online, you'll start to see And so this will give us an that are available in our IDE, I digress. Let's get back to Falcon. So now that we know where we are, you'll know that I downloaded the notebook from the SageMaker examples. So for those of you who this is the notebook you should see. So SageMaker Jumpstart, text We're gonna go over here and So first off, you're gonna be and then we're gonna point to a model. What's interesting about this notebook is that it actually gives So you can see there are You've got Falcon 40b and then instruct, and then the same for So again, obviously the instruction ones have already been instruction fine tuned, and the base ones have not. And then we have this that lets us pick the model So I'm gonna interact and then we can just that is the right one. And let's just show you here. So the model ID that I'm Yeah, so we've got the Falcon 7b instruct, and then we're gonna do our SageMaker. One line model.deploys. So this is scarily easy because So because this model to be hosted on SageMaker, we can just hit this and then the predictor comes up. Now I've already done this, so I will avoid the wait time here. You see this, this took a good So do be patient if you're And then the notebook and indicated different instances that have been tested And so we see 7b is on the And then the p4d, so And then the 40b as well, And then of course the p4d. Pro tip, make sure you pick If you're new to AWS, that means going with So a smaller T-shirt size that means the instance it's gonna have fewer CPUs, it's gonna have fewer accelerators. And everything about The amount of bandwidth that sees, if there's any instant storage, And so as a corollary, so that 48 that's there, It also means the pricing is heftier with the larger instances and the pricing is smaller And so you always wanna pick generally speaking as a And that's what we're gonna do here. And so they have a couple notes for you about changing the number of And then here, actually I like this. So if you are using a larger instance, which sometimes you wanna do because maybe you're or you're testing that actually need more infrastructure. And so you'll need a larger instance. So if you're gonna do that, just make sure you set this parameter. So my model.environment, and then just increase the All right, and so now theoretically, this model should be deployed. And actually we don't have We can just check. So I'm gonna go up to and let's go down to deployments. Let's see what endpoints we have. I'm gonna close that out. And lo and behold, we do This is great. So this is the SageMaker And then what do you know, Falcon 7b instruct handy. Let's do it. One, two, three, take a breath. Let's roll. Great. Okay, so here's our prompt. This is the prompt we send to the model. Tell me about Amazon SageMaker. You'll see we're putting So our inputs are indeed this texturing and then the parameters. So how the model should Those of you who are playground experiences, you with these parameters and that's okay. Don't stress out about it. But you data scientists out there, obviously you wanna consider but using the default values for starting. So we sent in our prompt, tell and we get back a paragraph So this model is coming This is coming straight I did not write this, nor did I put this in the model myself. This came out from the model Amazon, let's just read it here. Amazon SageMaker, lost developers, create, train and deployment from the infrastructure. Hey, looks good to me. I would check the box on that. Great. Okay and then the notebook about the Falcon model built by TII, and is currently the best via the open LLM And then you'll see a Let's step through this. So we have this nice query endpoint, which basically is a lightweight wrapper around this predictor.predict, and then is gonna give us some So, so now we'll query the endpoint, and this time we're gonna to compute the factorial in Python. See if it does this. All right, here is a Python I am pretty sure this is the factorial. I actually don't remember If you're interested in and then letting us know please let me know. I am pretty sure it does n-1, so hmm. Yep and then it's this recursive function 'cause it takes itself. Okay, great. Onward ho. Next we're gonna ask Hmm, it's funny 'cause I was to me this means, hey, give but yeah okay, here in 10 simple steps. So what we want are the to create a website. So choose your domain name, register, web posting provider, which frankly, arguably you would choose because you can't really before you have the web hosting And then you create your Yeah, looks pretty good. I mean, notably they don't about how you would do but at least it's a good And then translation. So translate English to I will not butcher this for you, but I'm gonna take their word for it, that this is indeed the French or no, I'm, I won't say that one for you. And then cheese, here we go. Okay, so let's run this. Ah yes, fromage, of course. And then we'll do some sentiment analysis. Great, so in this case, here of few shot prompting actually, because we wanna tell the model be it negative or positive, and then we give it this last tweet. New music video is incredible and it's obviously positive. Couple more examples here. When was the C programming Okay, this one we should see. So folks, it's always good So let's see if we can check this. When was the C programming Okay. The most creative period Okay, Bell Labs. All right, looks pretty accurate to me. And then the recipe for a graham crackers, butter, cream cheese, and then the instructions. Okay, great. Into the springform pan, springform pan. Then beat the cream Add these things, bake, and then sprinkle on top. I mean, that sounds pretty fair. We just generated a recipe for cheesecake. And then the last one here So the summarization example, they are providing this extensive input. So the input is this, right? It's basically three paragraphs of content describing the Falcon model. And then we get information Falcon is available, the And then this all comes back, right? So here is the input because of that little wrapper function. So here's the input and then right down here. So summarize the article above, and then the output is right here. TII made state of the Art Falcon model, available on SageMaker Jumpstart, pre-trained models, et cetera. Great. Okay, so accurate summarization. And then a handy guide for And then a couple limits on which is specifically the number And if you're new to NLP, remember as a token is a part of a word, basically tokens are how to feed them to machines. And then there's a little bit more, this is a, hmm, generating Okay, so this is sort of Walking through this range to the model sequentially. And then at the end we'll do a cleanup. Alright. Yeah, great. We've got the list. Multiple iterations, this Hmm. So basically what this is that you wanna send to a if it's too long to fit then just send it through So you can loop through the document or loop through your range and then send parts of the document up and it will generate responses And so here we went through 10 iterations listing a variety of services. This is not uncommon actually to see this kind of degradation. So here we just, I feel like in the model going on But in any case, that was an in order to perform text generation with Falcon on SageMaker Jumpstart. And so that is the end I hope you enjoyed it. And in the next one we are going to learn about how to pick the So I'll see you there.