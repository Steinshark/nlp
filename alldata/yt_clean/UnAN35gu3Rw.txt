Hi, welcome to this session on architects. My name is Sara van de Moose. I'm a Senior AI/ML AWS. Now, my goal for this architects and developers, specialised in machine learning, for your organisation. I will components in an effective these components are necessary details which only data before watching this session, architecting on AWS, and know S3, Lambda, EventBridge, you're unfamiliar with these some of the other architecture topics, then come back to watch if you have a basic learning and how it works. Okay, expect. I'll start off the the challenges many companies learning models and maintaining briefly define MLOps, before architecture diagrams. the architecture diagrams small to large. This will allow point based on the size and And finally, I'll end the starting your own MLOps quickly go over the machine that we're all on the same page. machine learning process because which needs to be solved, and learning is the correct will spend quite a bit of time integrating data from various analysing the data. Next, the process of engineering features, machine learning models and these models. And then based on scientist might go back to additional data cleaning steps. performing well, he or she would model so it can be used to final step and certainly a model that is in production. depreciates in value as soon as machine learning model is out of it. The world is constantly older your model gets, the worse By monitoring the quality of it's time to retrain or perhaps every data scientist will follow when they're just starting out entirely manual. But as learning across their for building, training, deploying innovation. Even a single production needs to be to maintain the quality of its operational practices in place, impact data scientist and costs. So to illustrate what architecture diagrams. Here I science setup. The data to an AWS account within a SageMaker can use Jupyter Notebooks to models. Data might be pulled number of data related AWS the data scientist are then so good. Unfortunately, this is estimates that only 53% of make it into production. And this. Often there's a objectives of the company and being built by the data lack of communication between the data scientists, challenge blocking models from finally, if the company already a data scientist team can existing models, while also if the model does make it data scientist spins up a model. And the developer of an to this model and generate connecting to a Lambda function, endpoint. So what challenges do Well first, any changes to the manual actions by the data running cells in a Jupyter code which the data scientist Jupyter Notebooks, which are difficult to automate. Third, forgotten to turn on auto- endpoint, so it cannot adjust of requests coming in. And loop. If the quality of the only find out through complaints are just some of the challenges proper MLOps setup. So what is operational practices to building training, deployment, governance. It can help end machine learning lifecycle, scientists and MLOps teams, accuracy, and enhancing security phrase in that previous practices. MLOps, similar to of technologies or services. You right skills, following the same successfully operate machine technology exists to facilitate easier for the people. Now in the technology and specifically build a successful setup. But I the architectures provided in you have the right teams, and if establish and follow MLOps learn more about the people and will include links to useful session. So without further ado, architecture diagrams. Now with lots of features and choose to adopt, but you don't immediately. So to start off, minimal MLOps setup. This would or a small data science team of just a couple of use cases. So off with the same architecture reduced in size so I can create scientist accesses Jupyter Studio, accesses data from any stores any machine learning the challenges I mentioned stuck in Jupyter notebooks and automate. So the first step to this architecture. You can based repository to store code. Container Registry or ECR to versioning the environments machine learning models. By environments and the model ability to reproduce models and let's talk about automation. previously is that the data re-training models instead of models. To solve this, you want pipelines. In this architecture, you could also use Step these repeatable workflows. The data scientist or by a machine version code and environments to model training, model save the new model artifacts to to complete these steps or training jobs, EMR, or this pipeline, we need a EventBridge to trigger the Another option is to have pipeline. Both triggers are and I'll introduce more triggers slides. So now that we have an want to introduce another that's the model registry. While object locking functionality, different models, a model models and their versions. to store metadata alongside your hyperparameters and evaluation explainability reports. This compare different versions of a model version for production. still stored in S3, but model an additional layer. Finally, we reach the glance, this might look very earlier in the session. But the I still have machine learning SageMaker endpoints connected communicate with an application. I have autoscaling set up for there's an unexpected spike in up to handle the requests and falls. Now one nice feature of can replace the machine learning Since I now have an automated new models, and a model registry would be best if the deployment as well. I can achieve this by which triggers when a new model and then update the endpoint connected all the pieces and will take advantage of. Not only learning models hosted by the so gradually using a canary small portion of the user new model, and any errors or Watch alarm to inform me. Over sent to the new model will gets 100% of the traffic. So I sense. I started with a very features and services, I now setup. My deployment strategy is scaling and canary deployment, automating model training, and versioned. But as your data architecture won't be slightly more complicated architecture will be more science team of between three to several different use cases at a start with the basics. Our data through SageMaker Studio, sources and versioning their artifacts. This should look automated re-training pipeline. only made it smaller to create finally, I'll bring back Event- re-training pipeline and model metadata and approving model the same as in the previous about deployment? Well, this is I have the same deployment setup autoscaling group connected to users to submit inference deployment services now sit in a account strategy is highly you to separate different separate restrictions for and have a fine-grained view of component of your architecture. managed through AWS scientists should not have account. This reduces the chance account, which directly affects account strategy for machine staging account alongside the models are first deployed to the then deployed on the production scientist cannot access these deployment must happen services deployed into the are set up automatically using CodePipeline in the development up a trigger for CodePipeline. Bridge. So when a model version this will generate an event deployment via CodePipeline. So again, and this is starting to setup. But I'm sure you've left on this slide. So let's add crucial when you have multiple extended periods of time - that's monitoring machine learning detect a change in behaviour or data capture on the endpoints in accounts. This captures the inference results and stores a model monitoring use case, the incoming requests, then you directly on your staging and case, I assume the data needs to data that's on the development to move the data onto an S3 Now, in order to tell if the data has changed, we need That's where the model baseline process as part of the automated generate a baseline dataset, behaviour of the data and the components I need to set up Sage- compare the two datasets and step in this architecture is to of the model monitoring report. event to EventBridge to trigger significant change has been medium MLOps of the same features used in the expands to a multi-account for extra quality checks on the you're now wondering what a like and how I can possibly fit slide. So let's take a look at suitable for companies with or more people and with machine the business. Of course, I start last time but reduced in size still using SageMaker Studio and stores model artifacts and respectively. The data sources now stored in a separate to have your data lakes set up access controls to determine by resources in other accounts. becomes the more AWS accounts through AWS Organizations. So bringing back the automated separate operations account. And as well in yet another account. same as in the small and medium split across more accounts. The used for any automated workflows intervention by the data practice to store all of your account like I have here for an easy way to prevent data changing production artifacts. production and staging accounts is exactly the same as in the reduced in size. The and staging accounts is still CloudFormation in CodePipeline, CD account. Note that I have account structures I have seen organisation might use a that's totally fine. Use this it to your structure and your model registry to CodePipeline the same as in the previous all the pieces connected again. but one of the basic building picture. Hopefully you spotted while. So let's bring it back by account because environments, environments are artifacts which one more change I want to make previous architecture diagrams, were building Docker containers and ECR manually. This process automated using CodeBuild and scientist or machine learning Docker file, but the building container is performed more time, so data scientists Of course, in the previous monitor to trigger model changes in model behaviour were back as well, starting with the production accounts, followed by operations account. As before, baseline to compare performance baseline can be a step in bring back model monitor to trigger re-training if necessary. components I had in the medium more features that I want to Maker feature store, which sits features are artifacts which can basic data science workflow from data scientists will normally before training a model, and it performance. In large companies, there's a good chance that data separate use cases which rely on store allows data scientists to created by others. It reduces consistency in the features that final component I want to Clarify can be used by data phase to identify bias and explainability reports for important for responsible AI. Now can also be used to generate reports, which can then be model in the endpoint. If increasing or the explainability trigger a re-training of the and Clarify can be introduced even the small MLOps on the needs of your business. example architectures to design you. Now, the architecture heavily on different components SageMaker provides purpose- integrations with other AWS practices across your SageMaker, you can build CI/CD management overhead, automate accelerate data preparation, training, monitor the quality of detecting bias model drift and track lineage of code datasets governance. But if there's one from this session, it should be don't have to immediately adopt complicated architecture design. integrate versioning and features I introduced in this according to the needs of your them as and when it's needed. presented in the session are not MLOps, but I hope they'll provide architect. So to help you get useful resources and placed the be able to download a copy of these links. The resources on advice on the technology behind people and processes which we If you're interested in any Cloud, I recommend checking out centre. It offers over 500 free experience. You should also through AWS Certifications. If particular session, I'd Solutions Architect associate as well as the machine learning that's all I have for today. listen to me talk about MLOps, you in upcoming projects. I just that's to complete the session to know if you enjoyed this minute. I hope you have a of Summit!