- Good afternoon, President Salovey, Marta Moret, colleagues, and friends, and welcome to the Tanner hosted by the Whitney Humanities Center. My name is Alice Kaplan and One of the privileges and is to advise the president on the selection of Tanner lecturers. This year, we've had the to invite a speaker working in the field of artificial intelligence. Fei-Fei Li is an international leader in establishing a human-centered framework for artificial intelligence in research, education, practice, and policy. Her visit to Yale is especially meaningful for us at the Whitney, of course, because it's our first Tanner and our first here in the but also because it's into our mission, to blend ways of knowing as we've made contact with working in artificial intelligence and related fields across the campus. We've said to many of you that we are new to the neighborhood of and delighted to be here. I'm especially grateful of Arts and Sciences, Tamar Gendler, to Gary Tomlinson, former Whitney Director and Sterling Professor and to Alan Gerber, Sterling Director of the Institution for Social and Public Policy Studies, and Statistics and Data Science for their knowledge and their support in planning this event. Before we begin, let me say a few words about the Tanner Lectures on Human Values, which were established industrialist, and and his wife, Grace Tanner. Educated at the University of Obert Tanner went on to and professor at the University of Utah, but that is not the whole story. In order to pay for his education, he founded a jewelry company His company, O.C. Tanner, which manufactures and seeks to improve workplace cultures through personalized employee recognition, became an extraordinary success, and it is on this base that Grace and Obert Tanner's philanthropy was built. Yale is one of a number the UK, and beyond to Appointment as a Tanner of uncommon achievement in the field of human values. The lecturers come from many philosophy, religion, the sciences, the creative or, indeed, from leadership The lectureships are and are meant to transcend religious, and ideological distinctions. As I noted earlier, at Yale, Tanner lecturers are and consultation with the director and the executive committee of the Whitney Humanities Center. We are thrilled, though by this big turnout. Yale is obviously delighted that President Salovey invited Fei-Fei Li, Sequoia Professor of Computer Science at Stanford University of the Stanford Institute You'll hear more about and the impact of her work in a moment. Please note that there will be no question and answer period following the lecture. The video recording will be posted on the Yale YouTube and we invite you to check the Whitney Humanity Center website and subscribe to our weekly email at subscribe.yale.edu for updates. Now it is my pleasure to welcome to the podium Professor Marvin Chun, Richard M. Colgate Professor of Psychology and Professor of Neuroscience. Marvin, it's such a pleasure in your capacity as dean of the college to welcome you today as of brain imaging and (audience applauding) - Good afternoon, everyone. Dr. Fei-Fei Li is one and influential researchers of artificial intelligence. If you enjoy the ability to search through your or trust your Tesla to as it races along with then you owe much to Dr. Li. An AI system is only as good and amongst her many accomplishments, she is responsible for a labeled database of images that enabled revolutionary advances in computer vision over Even though I'm not a computer scientist, I'm deeply honored to to introduce Dr. Li because in cognitive neuroscience, especially scene perception a topic to which Dr. Li has even before developing ImageNet. Her broad research interests machine learning, deep learning, computer vision, and AI plus healthcare, especially ambient intelligence systems for healthcare delivery. Dr. Li has published more in top-tier journals and Even more impressive is with a total of over 181,000 citations. Her work attracts more than most researchers gain Not surprisingly, she has been recognized with countless honors and awards, including election to the the National Academy of Medicine, and the American Academy I not only admire her but her contributions to society. She's a national leading voice for advocating diversity in STEM and AI. After serving as a director she took a sabbatical as the vice president at of AI/Machine Learning at Google Cloud. Now, Professor Li is the Co-Director of Stanford Human-Centered AI Institute, and beyond campus, she is of the national nonprofit AI for All, aimed at increasing inclusion Recognized in 2016 by as one of the Great Immigrants: if you consider other past I can describe Dr. Li to or the Yo-Yo Ma of We are excited to hear her and What We Value: AI Please join me in welcoming this year's Whitney Humanities on Human Values, Dr. Fei-Fei Li. (audience applauding) - Thank you. It's quite I really want to thank President Salovey and the Whitney Humanities Of course, I cannot pass the opportunity to also honor another who came out of Yale. That is Dr. Grace Hopper. So for all of us, especially women, working in computer science, we still are following her footsteps and trying to live up to and trailblazing careers for all the women in computer science, and that journey has been We haven't get to where for all of us, but it's to the alma mater of Dr. Grace Hopper. So I'm here to really share with you some of my own work in the field of AI, and I have to confess, as a when I enter this incredibly exciting but not really new field it would not have been my wildest dream that how profound this field has become to the daily lives of and our society. So I really cherish this moment to share with you what we have learned but really hope to engage in a productive conversation with you. I understand there will be a and hope to meet some of So thank you so much. I'll begin my talk. As Alice has said, the topic of my talk today is &quot;What We See and What We Value: AI I'm going to take you back in history, a little bit back in history, 540 million years ago. (laughs) Just a little while back. Life on Earth was very different. It was all underwater. Animals live in the and frankly, there aren't that And life was very chill. They float around. And then something really It was a mystery. In what geologically consider 10 million years, between to 530 million years ago, the number of animal What happened? There were many theories, from climate change to of the ocean water and all this. But there is one influential conjecture that has captured the attention, by a Australian zoologist By studying fossils, he conjectured that by the way, that's what with the animal speciation, or Big Bang of Evolution, actually, the Cambrian Explosion is triggered by the sudden evolution of vision, which set off an evolutionary arms race where animals either evolved or died. So vision, the onset of was one of the driving forces of the evolution of animal species. Furthermore, it's a driving force of the evolution of the nervous system, eventually leading up to very In fact, today on Earth, most, the majority of the animals as a major sensory system to guide its survival and thriving of the species. And for us humans, we're Our visual intelligence with each other, to work, to interact with the world, and it's hard to believe that we can live without much In the meantime, the field of intelligence in addition to what natural evolution is in the field of computer science. AI compared to evolution is a Most people say it's about 60 years. But the history of computer vision and the history of AI Just at the very dawn of AI, back around 60 years ago, there was a very ambitious and the MIT summer project &quot;We'll use our summer workers,&quot; and I suppose they had some smart ones, &quot;effectively to construct of a visual system or a within one summer.&quot; Well, that didn't quite happen, but the urban legend goes of the field of computer vision, that someone or a collection of pioneering computer that one day, we can create a with the kind of visual Fast forward to the second decade or actually the third Vision has come a long way. Just as Marvin was mentioning or visual driving assistance to image classification to the incredible work recently by generative arts, vision has not only become a blossoming, a vibrant academic discipline, but it also has become a driving force in commercialization and industry. So the rest of the talk is my own journey and Where have we come from And also, what are the human lessons that we have learned along the way of developing computer vision? So one thing that's been really fun to hear about this from all walks of discipline and areas, and I'm not going to assume or a student of AI, so let me just give a because a lot of you probably machine learning, deep What are these things? Well, here's a cheat sheet. I'm not going to go over but let's just get some basic definitions. Artificial intelligence, AI, is actually a term coined and Stanford's Emeritus Back in the 1950s, early '60s, he described this field as the science and engineering of making Nature took 540 million years. Let's see how long that to create a silicon-based And within the field of especially as it grows, you start to see areas such as field of computer vision, field of natural language processing, speech recognition, and there are more and more newer fields within artificial intelligence or related to artificial intelligence. Okay. And you also hear terms like machine learning and deep learning. What are these things? Well, consider machine learning the mathematical language In fact, there are other have been developed in the history of AI, but for the purpose of this talk, machine learning is a in computer science that apply math, and even inspired and all this in terms of based on statistical of different kinds of AI tasks. And within machine learning, the latest revolution is based on a family of algorithms called neural networks. And we'll get to that a little more later, but you hear neural networks a lot probably on news these days, and also there is a better name now or less nerdy name for neural network, which is called deep learning. So these are the words Okay, so let's get to the talk. So I want to share with you three parts of the work we have done, and the first one is building First of all, let's just accept this. Humans are incredible visual In fact, our visual system is very robust. This is a seminal study by Molly Potter back in the 1970s with her colleagues to show you that... She show you a video and consider you a human subject in her if you see a person in one of the frames. Okay, I see all. Good. You have good visual systems. We get that out of the way, and... But take a moment to reflect Every frame here is only on the screen for a hundred millisecond, And other than, say, me you don't know what the person looks like, the gesture of the person, what kind of clothes it's wearing. You don't know if this is Yet, your visual system is really quick at recognizing and detecting this object. Furthermore, by the turn of the century, shortly before the turn of the century, neurophysiologists like Simon Thorpe and his colleagues have also put a speed of processing of our when categorizing complex natural scenes. The experiment he did in front of computers and flash them with complex photos of Here are some examples. And the task of the human is to just say &quot;Yes, I see an animal&quot; or Mind you, these are really You don't know which kind of You don't know what the distractor And in the meantime, he is measuring your And as early as 150 millisecond after the onset of the visual stimuli, he sees a differential that tells you the difference between an animal picture Considering our wetware than transistors, this is in terms of the fleeting speed that our visual processing is. In addition to temporal capabilities, we also find neural that are dedicated to object recognition, such as faces or places This is a concurrent set of studies during the turn of the century showing us these kind of brain areas. So all this is to show you that there's something remarkable about our human visual brain, and one of the most remarkable to do object recognition In fact, we believe, and and vision psychologists believe, object recognition is a of visual intelligence. And for those of us who work in AI, especially trying to make machines see, this has become a north star. It's a important ability we need to endow the machines with. So formally speaking, the task of object recognition an agent, whether it's a picture of an object, of output the category of in this case, a very cute You might wonder how hard is Well, mathematically speaking, any 3D object in the world of pictures in a 2D frame, whether it's a camera because of the infinite appearance differences, self-occlusion, camera viewing angles. So this is a hard task, and now evolution has solved it for us. Let's see how we solve it in computers. At this point, I'm going to bring you to the last lesson or diversion of machine It's this concept of generalization. So for the rest of the talk, no matter what project I'm talking about, as long as I'm talking about AI, mathematically, the goal of AI and developing a AI algorithm is to solve the problem of generalization. What is the problem of generalization? The generalization is the following. When we develop an algorithm or a system that is capable of doing something, in this case, recognizing our goal is actually not so that the training That's a little counterintuitive you want the training error That's why you send your kids to school. But your kids will also get out of school and go to the real world, so what you really want is, once the algorithm encounter a case, an example that is not in training but in testing or in generalization time, you want the algorithm You want the testing error to be as small as possible. In fact, the gap between the testing error and the training error, you want it to be as small as possible. And that goal or that process of making this happen is generalization. And in AI or machine learning, is always to achieve the Okay, we got the nerd talk out of the way. So let's continue with the which is object recognition. In the early days, back last century, the pioneering computer there was very little compute, there was practically no data, and we know very little about So pioneering scientists have designed hand-designed features to try to account for how to recognize complex objects. And it was a beautiful... It was a bunch of beautiful but it didn't go very far in Around the turn of 20th to 21st century, a new tool emerged, and that fundamentally including computer vision, of machine learning or So in this time, we start to of using hand-designed but start to use machine learning models to learn the parameters of these features. This might be a little over who are not in computer All you need to know to move towards machine learning and machine-learned Something else important which is actually the Internet start to happen, and the field of AI and to have available pictures So in this seminal early work by a group of European scientists, they created this data so that the whole community can start working on the by using this data set and 20 object classes. That was also around the and became an assistant professor, and something really bugged me, because while it's really cool, we're working on this north star problem of object recognition, but Cognitive psychologists have told us humans have a remarkable ability of recognizing object categories, more than 30,000 object So 20 versus 30,000, it didn't fully compute. So my students and I started in color vision and object recognition. I realized one of the biggest problem we might be facing is not the It's actually the data we're training on. So in a three-year effort, my collaborators and my students We downloaded the entire back in 2006, 2007. More than a billion images we hand-curated through a crowd engineering method. 15 meticulously cleaned Oh, sorry. (laughing) 15 million meticulously across 22,000 object categories. And that was the ImageNet project. And ImageNet was born with the ambition that it will fuel computer vision and machine learning algorithm in a fundamentally, And to our, I wouldn't say surprise, but we're very happy to A particular family of when there is a huge flood is neural network models or algorithms, especially in computer vision, Convolutional neural network is one of the statistical models and it's not a particularly The early onset of the began even in the '60s and '70s and was further refined by people like Jeff Hinton and Yann LeCun. And for those of you not familiar with this neural network model, it is actually inspired by the cat brain and the on the cat visual system. So the fundamental building block of a neural network is a node, a neuron-like node that takes input from other nodes and sends output. And not only they take inputs, they also are layered hierarchically so you can pass information So even 10 years ago, with the availability of neural network models as and hundreds of millions of parameters and billions of connections. And this number is tiny compared orders of magnitude in today's computer science. So with the help of ImageNet and the the field of computer vision started to see success in object recognition. So we're able to categorize, or even find a cat in a or some other pictures, some other objects such as teddy bears or girls or a boy or even small objects like dog and kite. And then a historical moment happened. So for three years after we build ImageNet, we actually open-source this data set. We believe this is an important Not only we open-sourced it, we created an international challenge to invite researchers around the world to participate in the object And we curated a particular task called the ImageNet with a thousand object classes And then in 2012, the winning algorithm of is by Professor Jeff called ImageNet classification with deep convolutional neural networks. With this work, and people because of the first author, or the generalization significantly compared to And many people, and maybe historians will many people now consider the ImageNet challenge and the Alex model, the beginning of the But AlexNet is only the beginning. There has been incredible progress in the field of computer vision, especially through these annual winners of ImageNet challenge And that trend didn't stop. ImageNet challenge and the took the world by a storm, of computer vision just accelerating. And we're very honored of the most cited work in the and also, it spilled over. Computer vision, as well as AI, was no longer a niche academic It became a driving force of industry, a transformative era of some people call the or the deep learning revolution, and the field of computer vision, if you look at some market research, there is projected tens of market available now and from Tel Aviv to Silicon Valley, we're just continue to see the blossoming and flourishing of So back in the lab, my students and I don't believe that recognizing or the ability to label object classes is the end of our visual intelligence, right? In fact, here's a simple example. If I have two pictures with the ImageNet you would think the second telling the same story But the truth is there's a In fact, this is a very different scene from the left one. And psychologists and about this before computer scientists. This is a friend of mine and one of my favorite partially because the He had conjectured that just recognizing object is not enough to understand and categorize a scene. Relationships between Indeed, there is rich beyond just the identity of objects. So my students and collaborators and I actually propose this new work called scene graph representation, where we encode objects, the pairwise relationships, as well as the attributes here in purple boxes. And you can see a simple picture and dense visual scene representation. And by creating a large data set with this kind of annotated data, we're able to do things like in a scene, in this case, or person wearing a hat. In fact, scene graph representation and because of this, we can take advantage of the compositional nature and be able to do something like zero shot learning, which is we don't have to with a ton of images. In this case, our algorithm was able to recognize horse wearing hat, which is a rare event in life, or person sitting on fire And in general, our algorithm at that time using scene graph than state-of-the-art Of course, calling out is not the end of the story. In fact, we humans can when we are presented with a scene. So around a couple of years ago, using deep learning algorithm, my students and collaborators of work pushing the boundary beyond objects, beyond into storytelling and captioning. In this case, we showed and the computers such as a man riding a horse-drawn And if we push it harder, they can write denser captions Even furthermore, the and most of the world So we push the scene graph representation into spatial-temporal on recognizing multi-object, multi-actor activities and relationships. This is an example of an algorithm recognizing not only ping-pong but their movements and the relationship between the table, the ball, So all this is to show you that the field of computer and there are so many Beyond my own lab, our field in 3D vision, in pose estimation, and in generative art of vision. And this journey hasn't stopped. We continue to see the explosion of computer vision development. So for this part, we have observed data, compute, and neural network algorithms ushered in the deep But I'm a firm believer that AI's development has to be inspired by brain science And here are the collaborators who have contributed to the Now let's look beyond what humans can see. I know what humans can see as a inspiration of computer vision, but can we build AI to Well, to start with, we don't know everything In fact, can you name Maybe if you have kids, I'm unable to name all the dinosaurs, and there are tens of and thousands of car, I guess we don't call species, car types. And so this is what we call fine-grain object categorization, and to be honest, most cannot recognize that level of bird species or that many cars. And this is where computer algorithm have pushed the boundary and we have in our lab, as created algorithms to do this kind of thousands of bird species recognition. And this is one of my favorite work because we scrape the to learn 2,800 types of cars ever manufactured since the late, I think, 1970s, and then we we scanned Google Street View images across 200 American cities, and we learned about neighborhoods. We learned about sociology. We can correlate the recognition of cars with census data like neighborhood education level or income or even voting patterns. So computer vision, is a lens not only to but to see and understand who So that was object categorization, but I want to push you a little bit because our human vision And this is a famous visual illusion test that many of you know, the Stroop test, and you know the words, but can you read to yourself not the words itself? So from left to right and top to bottom, I would say red... Oh, God. (laughing) Red, orange, orange, green, and it's hard, right? And we feel bottlenecked And here's a related that we have limited So in a change blindness example, I'm showing you two alternating pictures and there's one change in Nod your head when you see the change. Ooh, this is an IQ test. Okay, so you didn't as Molly Potter's detecting the person. So it's the jet engine. Okay, let me do it again. Right? A huge part, right? And okay, so this is fun, but the limitation of when it costs lives. In fact, medical errors of death in American healthcare system, and we have many situations that we need more visual attention. For example, surgical instrument, accounting for surgical instruments. There are so many of and to this day, it's who have to tabulate and and it slows down the surgery So can we imagine computers to help? If they can recognize 2,800 types of cars or birds, can we account Here is a pilot study we did in actual surgical rooms where the computer vision algorithm is automatically accounting for all the sponges in the scene, and we can imagine to take this further and help the doctors and nurses. So there is a lot more humans don't see that we can ask computers to see. But sometimes not seeing is more profound. This is really one of my and I'm just giving you I didn't even have time to test you. So if you look at the top don't you swear they're of And then you copy this image They're identical gray values. Isn't that crazy? Because of the context, the prior knowledge of shape and lighting, you have a different way of processing this visual information. For those of you who are (audience laughing) Clinton and Gore, right? Of course it's Clinton and Gore. Really? Is it really Clinton and Gore? Look hard. Now, it's But because of your prior knowledge, the context, the hair and all this, you see two different people. You come with your own human visual bias. And this is a fun example, Bias can be hurtful, bias can harm people and then can particularly harm people in underrepresented, So computer scientist put this beautifully in a example in a poem she wrote called And this is to really raise the awareness of AI algorithms, in this case, facial recognition algorithms, failing to recognize women and people of color's faces. So AI can amplify human bias, especially if the data is biased and has issues. But the good news is there that is seeking to address this. I'm just showing you a few examples, and there's a lot more happening now and the awareness level is a lot higher. So we talk about bias, seen bias, but there is something even and it's very important. And consider the situations In fact, it is really important to recognize privacy is and it's part of human rights, these powerful computer vision algorithms, how do we address the issue of privacy? I, for one, do not believe technology is the only solution to addressing human and societal issues, but technology must raise to in being a solution, not causing problems. So there are actually a growing number of technical approaches in computer vision. Here I'm just listing a few, dimensionality reduction, body masking, federated learning, Some of you might not but for those of you who and data science, these that my colleagues and I have worked in. And I want to just show you one work. It's actually not even my work. It's mostly my collaborator and his students. This is a really fun work of doing privacy-aware They recognize that there is a need for computer vision technology in many applications, or safety or a garage or whatever. But how do we do this in They devised a lens that actually distorts the input. So instead of seeing humans, it blurs. The lens gives you images like this. But it's not enough to because you still want to So they actually developed that's coupled with this hardware so that even when the the algorithm is still capable of recognizing relevant human activities. So this is a way to balance the need to recognize what's going on but to protect privacy Of course, all this is just ongoing work and this is a long we talk about building AI to but what we really learned these are the issues AI many profound issues that So we as technologist, along with other stakeholders, must commit to study, forecast and guide AI's impact on people and society in a And these are some of the and postdocs who have contributed the work I talked about in this section. This brings us to the It's building AI to see Let's face it. When we talk about AI today, the most important concern, one of the most important concerns society has is the labor threat. You know, when we are we're taking away human or radiologists or factory workers, and we see these kind of I for one have worked in a industry for the past 10 years that I see if not a, I see a different angle, and or maybe even more is actually the current labor shortage in many of our industry, By next year, that's, America will be missing and our current nurses are overworked and underpaid and over-exhausted. And these are the issues, that we're facing. So how do we balance the concern for AI's labor threat, which is real and we need to really think about it, to many of the current or even unsafe work environments? So we really believe, there is a verb we need to replace, and it's the word replace itself. AI should be augmenting not replacing human capabilities. And I want to show you two lines of work in this arguing for AI's augmentation. One is backing healthcare. I already give you a little that medical error is a one of the leading cause of death in American healthcare system. In fact, there are many where we don't have to ensure quality, and to ensure safety. So more than 10 years ago, my colleagues in Stanford's medical school and I came together and can we, in this new wave deep learning revolution, can we imagine that we and ML algorithms to glean that are important for And this technology is what we call ambient intelligence for healthcare. A couple of years ago, we put our work, 10 years' work, in a &quot;Nature&quot; review. I'm not going to get into but I'll give you a a couple of nuggets of what we talked about. The first one is hand hygiene. Hand hygiene practice, is very important to reduce which causes tens of thousands of lives, three times more than car and whether we're talking or technology like RFID, they are actually more or less ineffective or too time- and labor-prohibitive. So we piloted this project where we put a smart sensor the sanitization stations, and these sensors capture depth images that look like the blue videos. So it's more or less of our clinicians, but in the meantime, our backend algorithms is running 24/7, recognizing proper hand Here, if you see a green box, it means the clinician is before entering the patient that proper hand hygiene Our algorithm performs is a lot better than one, three, or even four human observers. So we also worked in ICUs. Patient mobility is linked but how do we know a patient both in terms of frequency It's hard. We're still resorting on human auditors, and if you know anything about you know they're overworked to even cover how much they need to do. So again, we put these smart sensors in Stanford Hospital as well as a hospital in Utah and start working with clinicians on can we prototype So here, you're seeing that our AI algorithm is of activities or mobility getting out of bed, getting in bed, getting out of chair, And we're showing that our against human-labeled ground truth. Last but not the least, in using this kind of ambient in senior homes and aging in place to predict and prevent unsafe events, monitor patients with mild symptoms, and manage chronic conditions such as infection, mobility, In fact, I pay particular because America is aging, and we're having a real So can we go a step beyond to send alerts to doctors and families when it comes to aging? Here's a little bit of sci-fi. Because of the decrease of, you know, availability of caregivers, can we imagine one day AI in the forms of robots can also help our seniors or aging in place? And here is not to replace humans, human connection, human emotion. Family is important. But there is a lot of tasks And it's also, remember, of home assistance from unpaid caregivers that's not accounted in our current GDP. And guess who it impacts the most? Women and people of color. So as a AI scientist, I'm extremely excited to that can help people at home. Well, don't get too excited too soon. Here's where today's robots are. We hear a lot of robots Those are very structured If we put a robot in a the poor guy can barely And this is... I hope I didn't offend any We know that there's a In fact, most of robotic research today is still more or less limited such as placing something or door or inserting a peg in a hole. And moreover, if you and the papers we publish, our experiments, in artificially small-scale and anecdotal and setup, lacks standard And if we want to connect this to the goal of everyday robot, the real dynamic, and uncertain, interactive and social, So how do we close this gap? Well, if we have learned anything in the past decades of the we have learned that data and benchmark has come a long way in such as computer vision and In fact, with this kind of inspiration, we ask ourself, can we get inspired for a new north star for And this is the last work I want to present in this lecture today, is a ecological robotic and a large and diverse set of activities that is proposed by the which is a benchmark for in virtual interactive It's been so fun to work on this project in the past three years, and we have both cognitive psychologists and roboticists and computer Before, we're talking about, What are they talking about? Let's just take a moment and think. At the end of the day, what kind of tasks are You want robot to help What are the everyday Well, let's do a little bit I'm going to ask you about that you would like a Just, like, nod or say What about cleaning the kitchen floor? Okay, good. Yeah. Good. Shoveling snow in Connecticut? Okay, good, good, good. Folding laundry? Okay. You know, I fold a lot of laundry at home. I actually like that time. But anyway, robot can do that. Cooking breakfast? I'm getting a little Okay, this last one is the best. Opening Christmas gifts? Okay. By the way, I don't know if of the images are AI generated Okay, so that's good. So this is really important. We are very enthusiastic technologist, but before we even start our project, let's focus on humans and So before we started BEHAVIOR Project, we actually did a large-scale user study with 1,400 participants and screened and curated 2,000-plus everyday activities that are proposed by government data and ask people what would they, how much would they benefit Because it's important And then we got the scores. We ranked them. As you just showed, cleaning, we all want cleaning. But people don't like robots to open Christmas gift or throw darts. I guess they're worry robot would win. Or buy a ring. I hope not. So, okay. So if you put the top 1,000 tasks that people prefer robots help, they actually do cluster or some simple cooking tasks or some boring shopping tasks, and so... So that's good. So we decided we're going to take the top-ranked as the baseline or as the for our robotic learning project. But that's not enough. In what environment? We don't want to use simple laboratory-made environment. So we actually scanned 50 from apartments to restaurants to offices to stores and use these as basis for building our robotic training world, and we also obtained across 1,200 object categories, and these object asset models cover 30-plus different properties from, you know, temperature and deformability and so on. Now with all these ingredients a unprecedented large-scale for robotic learning. Simulation environments are very critical. In fact, they're fast, they can be used to they're safe, and they and we can work towards using simulation, and There has been a lot of in creating simulation environments. And these are some of the state-of-the-art by our colleagues in the world. BEHAVIOR Project, we collaborated, we partnered with NVIDIA's Omniverse team, which is a commercial-grade and we aimed at creating to train robots both in physics, perception, and interaction. Here I'm showing you examples You see thermal effects. You see lighting and reflection effects. You see fluid, We also did a user study on We compared BEHAVIOR's environment or simulation environment against other simulation environment and ask people to score and BEHAVIOR came out pretty well. We also try very hard to the realism in physical interaction. Here you're seeing robots doing things with pretty complex materials such as water, fluids, So this is just a very messy, nerdy chart that we put together to show you the quantitative comparisons of BEHAVIOR compared to other simulation environment and how BEHAVIOR is more diverse, more large scale and complex in several important dimensions. Of course, with the goal of creating a new generation for these everyday household activities, we want to start by asking the question, how did today's algorithm do? We picked three BEHAVIOR activities, store decoration, collecting to benchmark against today's this is a little dense, I promise this is the last where we use these three tasks and we change different to see how the Let me just say that if we do not give artificially privileged information to today's state-of-the-art algorithms, the performance is zero. Well, that's actually encouraging, at least that's what is that you want to be challenging and this is where we can make progress. But of course if we relax and give some privileged here, we give it what we which is, for those of you'll know this is a little more, just pre-programed movements, then the performance start to improve, and if you add some it'll improve even more. All this is to say, this heavy chart is to say, that BEHAVIOR is perhaps one of the most challenging benchmark in today's robotic learning and we find it very exciting that we have now this fertile playground to train 1,000 household activities for robotic learning algorithms. And we're not stopping here. We're also taking and creating a Sim2Real possibility. At Stanford, we created and also have a real-world working with its own digital twin in the simulation environment so that we are simultaneously as well as the physical agent. Here's actually Marvin. Marvin's very slow, but Marvin is actually successfully going to pick up this bottle with the goal of sending this bottle to a trash bin. So let's see if Marvin does that. So please bear with me. Marvin in this case to the trash bin and place So but Marvin is not Sometimes, it navigate to the wrong table. Sometimes, it just doesn't Sometimes, it didn't manage to place the cup in the right place. So I'm just showing you this to show you we still have a long way the cleaning kitchen And this is a teaser video to and give you a little more sense of the diversity of this environment. All these are our simulation environments, and with the video, you get of the complexity of BEHAVIOR, complexity of the environments, the large-scale things to restaurants, the kind and of course the website of BEHAVIOR. Okay, so we started by that per robot and our goal is one day, And that's still a long journey, but I'm very excited by BEHAVIOR because it is born with the goal of augmenting people, It's large scale and diverse and it's striving to be realistic and ecological. So with all this work in building AI to see and do what humans want we learn the value that AI must seek to augment, not replace, our individual and collective human and there are a lot of and postdocs who have contributed So this brings us almost to We've talked about three kinds of AI work we have done in our lab. What's most important is in building AI, we have of human lessons, that the by the concern for its human impact, that AI should strive to not replace humans, and that to be inspired by human And with these three Stanford launched three years ago the Human-Centered AI Institute with the support of a across our, at that time, seven schools, now we have eight schools, from law to business to to engineering and natural sciences. And HAI is a very young institute. I'll just highlight a couple mostly during pandemic, I guess. We have launched the digital economy lab to specifically put together economist and computer scientist and technologist to study the impact of of our economy in this digital age. We have launched a Center for All the news-generating AI you see today, whether it's GPT-3 or this DALL-E art, is by these large foundation models, and they have profound as well as AI application, both from ethical societal point of view as well as from technology point of view. So we have professors, faculty, researchers coming together to study this. We also pioneered a new way of reviewing AI research grants called Ethics and Society Review inspired by the human subject IRB review led by political scientist, a humanist, and a computer scientist. We also have education program geared towards undergraduates by embedding ethics in our But our education didn't stop We reached out beyond and start to extend AI education like business executives, journalists, lawyers, and judges as especially in Washington DC but also at the state and local level. And Stanford was very proud to be leading the successful lobbying of a bill called National and I believe Yale was part of the team, and thank you for the successful effort. So Biden administration launched the National Artificial task force two years ago. I'm very honored and humbled to be one of the 12-people task force, and in a few weeks we'll this new idea of how to and higher education sectors capability of innovating and education in AI. So all in all, it has been an exhilarating as well as humbling journey to create AI to enable machines to see, to realize the audacious ambition that evolution have shown to create intelligent machines in silicon. But it's incredible that we've learned more human lessons and understand more human values than machine-learned lessons, This is only in the and I want to thank, of and collaborators and also sponsors to support this research. And thank you for listening. (audience applauding) (soft music)