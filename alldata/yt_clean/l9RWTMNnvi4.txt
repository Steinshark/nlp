(playful music) This is Geoff Hinton. Because of a back condition, to sit down for more than 12 years. I hate standing, I would but if I sit down I have So. Well, at least now standing Yeah, but I was ahead. (laughs) I was standing when they Since he can't sit in a car or on a bus, Hinton walks everywhere. (playful music) The walk says a lot about For nearly 40 years, to get computers to learn like people do. A quest almost everyone thought was crazy, or at least hopeless. Right up until the moment Google thinks this is the Amazon thinks this is the Apple thinks it's future of the company. My own department thinks this and we shouldn't be doing any more of it. (laughs) So, I talked everybody into (playful music) You obviously grew up in the UK, and you had this very prestigious family full of famous and, I was curious what Yeah, there was a lot of pressure. I think by the time I was about seven, I realized I was gonna have to get a Ph.D. (laughing) Did you rebel against that? Or you went along with it? I dropped out every so often. I became a carpenter for a while. Geoff Hinton pretty with this idea of figuring He started off getting into physiology, the anatomy of how the brain works, then he got into psychology, he settled on more of a to modeling the brain, and got My feeling is, if you want to understand a really complicated device like a brain, you should build one. I mean, you can look at cars, and you could think you When you try to build a then there's this stuff that otherwise it doesn't work. Yeah. (laughs) As Geoff was starting to he got inspired by some AI Specifically, this guy: Frank Rosenblatt. Rosenblatt, in the the late 1950s, developed what he called a perceptron, and it was a neural that would mimic the brain. The basic idea is a collection of small units, called neurons. These are little computing units, but they're actually modeled on the way that the human brain They take their incoming data and they actually learn, so the neural net can learn to make decisions over time. Rosenblatts's hope was that you could feed a neural network a bunch of data, like pictures of men and women, and it would eventually Just like humans do. There was just one problem: Rosenblatt, his neural of neurons, and it was Extremely limited. And a colleague of his that showed these limitations. And, it kind of put the into a deep freeze for a good 10 years. No one wanted to work in this area. They were sure it would never work. Well, almost no one. It was just obvious to me that everything was about ready to go. The brain's a big neural network, and so, it has to be that because it works in our brains. There's just never any doubt about that. And what do you think that kept you wanting to pursue this when everyone else was giving up? Just, that you thought it was No, that everyone else was wrong. Okay. (laughs) (upbeat music) Hinton decides he's got an idea of how these neural nets might work, and he's going to pursue For a little while, he's bouncing around research institutions in the US. He kind of gets fed up that most of them were funded by the Defense Department, and he starts looking for I didn't want to take I sort of didn't like was going to be used for purposes that I didn't think were good. He suddenly hears that in funding artificial intelligence. And that was very attractive, that I could go off to and just get on with it. So I came to the University of Toronto. And then in the mid-80s, we discovered how to make more complicated neural nets so they could solve those problems that the simple ones couldn't solve. He and his collaborators developed a multi-layered neural And this started to work in a lot of ways. Using a neural network, a guy named Dean Pomerleau built a And it drove on public roads. Yann LeCun, in the 90s, built a system that could recognize handwritten digits, and this ended up being used commercially. But again, they hit a ceiling. (upbeat music) It didn't work quite well enough, because we didn't have enough data, we didn't have enough compute power. And people in AI and computer science, decided that neural networks were wishful thinking, basically. So, it was a big disappointment. Through the 90s, into the 2000s, Geoff was one of only a who were still pursuing this technology. He would show up at academic conferences and be banished to the back rooms, he was treated as, really like a pariah. Was there like a time when you thought this just wasn't going to work? And you had some self-doubt? I mean there were many &quot;I'm not going to make this work.&quot; (laughs) But Geoff was consumed by He just kept pursuing the idea that computers could learn. Until about 2006, when to Hinton's ideas. (upbeat music) Computers are now a lot faster. And now, it's behaving like I thought it would behave in the mid-80s. It's solving everything. The arrival of super-fast chips, and the massive amounts of gave Hinton's algorithms a magical boost. Suddenly, computers could Then, they could recognize speech and translate from one By 2012, words like neural were popping up on the front page of the New York Times. You have to go all these years, and then all of a sudden, in it just takes off. Did it finally feel like aha, the world has finally come to my vision? It was sort of a relief that people finally came to their senses. (laughs) (gentle music) For Hinton, this was after decades of toil. And for Canada, it meant Hinton and his students as an AI superpower, something no one, and no computer, could ever have predicted.