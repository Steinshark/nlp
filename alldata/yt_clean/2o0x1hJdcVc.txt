You've seen the models and the images we can generate with them. And that's cool. But there's something even better. Today I'll show you how Stable Diffusion works. By the end of this video, you'll see how we can go from a random patch of noise to a very impressive looking dragon. And there's one more thing. I'll show you the code that you can use to generate images and animations between two text descriptions. Let's begin. Hey. If you look at their paper, you'll see this graphic representing their system. Now, a lot is going on here, so let's try to break it apart and focus on the main components of their solution. I'm going to start in the middle green area here and explain how a Diffusion Model works. First, we have a Forward Process, also called the Diffusion Process, as you see here on the screen. Then you have a Reverse Process, also called Reverse Diffusion or Denoising Process. Now, here is my representation of that Diffusion Process. We start with an image, an image of a cat in this example, and add noise to it consecutively, for many different steps until it becomes unrecognizable: like pure noise. This process generates the data we need to teach a model how to remove noise from a picture. And that's this model here, the Denoising U-Net. And it's called U-Net because it has the shape of the letter U. Now, to train this U-Net model, we start with one of the images --let's say it's the 7th image that we created, and you can see there is some noise added to that image-- and ask that model to learn how to get the previous image in that sequence. In this case will be image number six. So basically, we're asking this model to remove one layer of noise from the input image. And we're going to do that over and over with every pair of images. When we train a Diffusion Model like this with a giant dataset, we should expect to get a model that's very, very good at removing noise from pictures. And that's a key component here. We can start with a random patch of noise, run it repeatedly through the model until we get a high resolution picture. And that's great. But something is missing here. What is that picture going to look like? If we start with random noise, how do we know which picture we're going to be getting back? To understand what's happening, I'm going to need this globe. I want you to imagine that we take every person in the world and represent them based on their language and accent. That's the only thing we care about for this example. And that compressed representation of every person, let's call that Latent Data. You'll see that term everywhere. Now, the globe will represent the latent space where we will project the latent data that we just created. Here is the exciting part. It's likely that people who speak the same language with a similar accent will appear here in latent space in close proximity. For example, latent vectors representing people who speak English with a Southern American accent will probably appear here around this area in latent space, while Mandarin Chinese will show far far away, around here, in China. The fact that we can compute how similar two different vectors are based on where they appear in latent space that is a key component of the solution. And I want you to notice how everything that we've discussed so far it's happening in latent space. That means that we are not working with full blown out pictures, instead we're working with a compressed representation of those images, and that's clutch for several reasons. One of them being this little piece that you see here. That's a text encoder. That text encoder is capable of taking any text and converting it, projecting it to the same latent space where our images are. And we can use that latent vector to condition the Denoising Process. In other words. The latent vector becomes some sort of coordinates in latent space. So the image that we're going to get back from the Denoising Process will look similar to whatever we find in that location. Those are the two main ideas of Stable Diffusion, and here is a summary of the process. Of course, there is more to it but we can start with text: &quot;A pretty cat&quot;. That's the prompt we're going to show the system. That prompt will go through a text encoder which will create the latent vector, and together that latent vector with a random patch of noise is going to go into the Denoising Model. Remember. The latent vector is conditioning the Denoising Process. Is guiding the Denoising Process. That Denoising Model will start peeling off one layer of noise at a time from this random patch of noise conditioned by the latent vector. And we're going to repeat that until we get a high resolution image that we're going to show you. Now, I don't know if you can tell but I'm psyched about this process. Is so cool! Let's move this away. Bring the computer. My computer is here, so now we can take a look at the code and see how we can take advantage of Stable Diffusion. Now, I'm running my code on my MacBook Pro that has Apple silicon, and I can take advantage of the Apple's GPU here. If you want to do the same, check the link here that will take you to a video where I will show you how to set up your computer. I also created a copy of this notebook, and you can run it locally or in Colab. And you can check the link in the description below. Now, the code here is very straightforward, and that's what's really nice about it. We start here by importing the libraries that we need. Nothing interesting here. Then, I'm going to display the physical devices that TensorFlow has access to. And you want to make sure, when you do this, to see a GPU here. Stable Diffusion will need a GPU in order to work. Then I have a few auxiliary functions that we're going to be using throughout the code. Very simple functions. One is to plot the images on the screen. The second one is to save an individual image to the hard drive. And the third one is to export that image as an animation. Here is where I create the Stable Diffusion Model. And notice that I'm passing the dimensions of the images that I want to generate. Keep in mind, Stable Diffusion was trained with 512 x 512 pixel images. You can definitely specify bigger pictures, but the quality will suffer. So 512 x 512 is probably the maximum you should specify here. Now, that's it. The next step is to create a text prompt and call the text to image function with that prompt. In my case, I'm using a batch size of three. So, I'm going to be getting three images back. And right after generating the images, I'm going to be plotting them on the screen. You can see here what the Ford truck traveling down a dirt road with mountains in the background looks like. Here I have another example. This time it's an electric car traveling through a city, with a city skyline in the background. And that's what I got. Then here is an example of how to save one of those images in your hard drive. In this case, I'm saving the first image here, but you can save any of those images. Now, one thing that I wanted to add to this notebook here is because the images this model generates are only 512 pixels square. That's not a lot of resolution. So in case you want to use one of those images for something that requires more resolution, check out this Hugging Face space here. On that space, you can upload your image and increase the resolution of that image. It's a separate model that will do that for you. So I wanted to add that link here. Now, in this section is where things get interesting. We can use two prompts, generate their latent representation in latent space of each one of those prompts, and interpolate between them. After we do that, we can generate images for each one of those interpolation points and stitch them together as part of a big animation. And what's going to happen is, depending on how far away in latent space those prompts are, and how many interpolation points you create, you'll see a smooth transition from one image to another. And that looks pretty cool. In the code here, you see that I have two separate prompts. The Ford truck and an electric car. Here you have the number of steps that we're going to generate in that interpolation. And the more steps you use, the smoother that transition will be. Steps are going to be closer to each other, so you're going to be probably generating images that have a lot of similarities. Then we have the frames per second. And when we're generating the image, we want to determine how many of these generated pictures we want to include within a second of that animation. The number of diffusion steps. The default here is 25. The more diffusion steps you add, the better looking your image is going to be. Like, the more quality you are going to get. But, obviously, the process is going to take a little bit longer. The animation file name and the seed. And you're going to see why that seed is necessary. So, in the code here, the first thing that we are going to do is generate a patch of noise. That's wh at I'm using the seed here. The reason I'm doing this is because you're going to be generating multiple images, and you're interpolating between those two prompts. If you start from completely different random patches of noise, they're probably not going to look that similar. Second step here is just creating the encoded vectors, creating the latent representation of the two prompts. Then we interpolate those two representations. And you can see here how we are generating interpolation steps, number of steps. Finally, because we're going to be generating a bunch of images, we don't want to just blow up the GPU memory. So instead, we're going to be generating different batches. So that's what's happening in these three lines. And here is where the real process begins. We're going to go through all of those batches, and we're going to generate images. Notice that here we're passing the noise. So we are seeding the Denoising Process with that random noise patch. We are determining or passing here the number of diffusion steps that we want: and the default is 25. We're specifying where the encodings are. In this case, it's just for that specific batch number. And we're specifying how many batches are within that specific vector there. And finally, when we get those images back, we're going to be adding them to an array that we can later export as a gif. And that's what this line here is going to do. The final line will display that gif on the screen. So that's the code. And I have a small challenge for you. In this video here, I show you how to use a model to transcribe audio. Connect that model with Stable Diffusion so you can generate images with your voice. So go nuts, build something cool, and I'll see you in the next one.