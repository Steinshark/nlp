parallel programming is hard the barriers the barrier to entry is far too high we live in a world where almost all Hardware platforms are parallel and require explicit programming to use that parallelism despite that many users in many code bases do not take advantage of the available Hardware parallelism for those that do many have chosen approaches that are platform specific and not portable for example most code that is written to take full advantage of the many cores on a powerful CPU can't run effectively on a GPU so the C plus committee wants to build an on-ramp to parallel programming we want to give users easy to adopt solutions that are widely portable we're not aiming to expose all the capabilities of each platform but instead to expose enough of each platform's parallelism to be useful we want widespread adoption and normalization of parallelism across the C plus ecosystem there's three pillars to our strategy for parallelism in standard C plus plus we need a corpus of common algorithms that dispatch to vendor optimized parallel libraries next we need tools to write your own generic parallel algorithms that can run anywhere finally we need mechanisms for composing parallel invocations into task graphs today I'm going to talk about the C plus standards committee's plans for bringing parallelism asynchrony and accelerated Computing into standard C plus about how the committee is supporting all three of these pillars I Am David Olson lead engineer for the Nvidia HPC C plus plus compiler and VC plus plus and member of the C plus standards committee my job for the last few years and likely for several years to come has been to implement C plus plus standard parallelism for both CPUs and Nvidia gpus within the NVC plus compiler I would be remiss if I failed to thank Bryce adelstein lilbach who originally put this presentation together I have modified it and updated it to be my own but all of the best parts come from came from Bryce and all the inaccuracies are mine there will be lots of code examples in these slides and I will go through them quickly I don't expect you to understand all the code instead focus on the high level takeaways which I will attempt to highlight and repeat because this is slideware I have used some namespace aliases for ranges views and execution in this thread to save space on the slides those caveats out of the way let's move to the main content we'll start by talking about the first pillar the collection of common parallel algorithms C plus plus has had has had a set of Serial algorithms for manipulating sequences of objects since the first standard these algorithm algorithms operate on one-dimensional sequences initially they were parameter parameterized with iterators but with C plus 20 we've introduced a new more powerful extraction ranges there's about a hundred different standard C plus algorithms for Loop abstractions filters sorts searches rotations reductions scans Etc in C plus plus 17 we introduced parallel versions of these algorithms the parallel overloads have the same interface as their serial counterparts except they take an extra parameter an execution policy which describes what form of parallelism is allowed if any execution policies describe the how of execution but they don't specify where that's left up to the implementation execution policy parallelism could be implemented with a variety of strategies such as a CPU thread pool openmp or GPU acceleration execution policies permit parallelism but do not require it an implementation may choose to not parallelize the execution policy overloads are all Fork joint synchronous or blocking the implementation will wait for the parallel operations to complete before returning from one of these algorithms there are four execution policies seek short for sequential indicates that all operations must be performed within the calling thread and must be indeterminately sequenced basically it says no parallelism is allowed unseek also requires up all operations to be performed in the calling thread but allows those operations to be unsequenced or interspersed with with each other meaning that vectorization is allowed but not required par allows the implementation to parallelize operations at its discretion however it also requires that all operations within each thread be indeterminantly sequenced meaning that vectorization is not allowed power unseq allows operations to occur in multiple threads and to be unsequenced with respect to each other mean that both thread and Vector parallelization is parallelism is permitted again this means parallelism is allowed not required the implementation may choose not to parallelize let me show you one of Bryce's favorite parallel algorithm examples word count we can write it in a single parallel transform reduce Transformer reduce takes both a transformation operation and a reduction operation it applies the transformation to the inputs and then uses the reduction to sum the results of those transformations if you're familiar with mapreduce this is C plus Plus's form of it we'll use the overload of transform reduce that takes two input sequences and a binary transformation function the first input sequence will be the entire string except for the last element the second input sequence will be the entire string except for the first element this means our binary transform function will be passed a window of every two adjacent characters in the string our transformation function's job is to tell us whether the two adjacent characters it's looking at are the beginning of a word if the left character is white space and the right character is not white space then the right character is the beginning of a word the pseudo sequence produced by the transformation will look like this for every word one and only one of the transformation invocations returns true but what about the first character of the string we never actually test it with our transformation function we start at the second character so we account for the first character in the initial value of the reduction our reduction operator is just plus after the transformation we have a sequence of bools summing that sequence into an integer will give us the word count this standard C plus code is parallel and portable you can run it anywhere on your GPU your server CPU your laptop even your phone foreign the standard Library introduced ranges unlike iterators ranges are composable ranges allow us to express a wide variety of algorithms in terms of a few key Primitives used in conjunction C plus 20 ranges and the C plus 17 parallel algorithms are quite powerful standard C plus algorithm algorithms iterate over a sequence of objects for example the elements of a container however sometimes we want to iterate through indices instead of objects this is particularly important in numerical and scientific computing we can do that using Iota a range Factory introduced in C plus 20. it produces a range of monotonically increasing integers for example Iota one comma n will produce a range of all integers from 1 to n minus 1. instead of passing iterators to a container to a parallel for each we can instead pass an Iota range giving us a parallel 4 over indices instead of objects we can also use parallel parallel algorithms and ranges to iterate multi-dimensional index spaces C plus plus 23 is Cartesian product range adapter takes multiple input ranges and produces a range of all the ordered tuples formed by taking an element from each input essentially reduces a multi multi-dimensional index into a sink into a one-dimensional index we can use this in conjunction with Iota to create ranges representing multi-dimensional index spaces for example here we use the pattern this pattern to write a simple parallel Matrix transpose the range V will produce Two element tuples from zero zero to n minus one and minus 1. the iteration order will be row major the second index will be contiguous we can change to column major by switching the order of arguments to Cartesian product the C plus plus 17 execution policy parallel algorithms are for joined synchronous so each invocation launches its work in isolation and blocks until that work completes so for example if we invoke two parallel four each's on the same data data back to back we'll get two parallel work launches the caller will be blocked waiting for the first invocation to finish only then will the second invocation be enqueued and launched this transfer of control back to the caller is a latency bubble and can degrade performance one way to address this problem is to fuse the operations together into a single parallel invocation which can be done with the transform range adapter transform takes an input range and a function and returns a range that is the result of applying the Tran the function to the input this is completely lazy the function is only executed as needed when the elements of The Returned range are accessed for example here f is not evaluated until the elements of V are accessed in the parallel for each there's a big change in semantics here previously we had a guarantee that all invocations of f would happen before any invocations of G we no longer have such a barrier another useful range adapter is filter which takes an input range and a predicate function and produces a range of all elements of the input for which the predicate returned to true again this is done lazily the predicate isn't evaluated until the elements of The Returned range are accessed here we use filter in combination with a parallel reduction to sum only the positive elements of a container let me show you some applications that have adopt adopted standard C plus parallelism Lou Lesh is a mini app for lagrangian explicit hydrodynamics on an unstructured Grid it's designed to stress vectorization parallel overhead and on node parallelism it's been ported to several Frameworks and programming models including MPI openmp open ACC Kuda Raja Cocos and now standard C plus with the standard C plus version of bluelash you can get performance that matches many of the other back ends and you can build that same standard C plus plus code with multiple compilers GCC and VC plus plus Intel msvc and run it in parallel on either CPUs or gpus another example is stlbm aladis boltzmann framework written from the ground up in standard C plus plus it can run on multi-core CPUs and gpus without any code changes it doesn't use any language extensions external libraries vendor-specific code annotations or pre-compilation steps just standard C plus plus Maya a code base for Aerospace and noise simulations has ported to C plus plus standard parallelism they're planning to move from openmp openmp to standard C plus the C plus parallel algorithms introduced in C plus 17 are great but they're just the start of the story they have two major limitations first the C plus plus 17 parallel algorithms are all Fork join synchronous they launch the parallel work and then they wait until that work is completed before they return to the caller now where exactly do they launch that parallel work that's the second problem by Design users have no control or visibility into where the parallel work runs it happens on some amorphous implementation to find execution context it could be a CPU thread pool a GPU stream Grand Central Dispatch Windows fibers Etc implementations have complete freedom today C plus has no standard model for asynchrony and no standard way to express where things should execute but the solution is coming soon to your C plus implementation senders and receivers an asynchronous execution framework for standard C plus let's look at a simple example first we need to get a scheduler from somewhere from a thread pool a tasking system a GPU driver Etc to start a chain of work on the scheduler we call schedule which returns a sender that sender will complete on the execution context associated with the scheduler next we use a sender algorithm then to compose work onto the sender that we got from the scheduler this work will be performed on that same execution context the sender algorithm will return a new sender which we can use to add more work onto the chain finally we wait until the chain of work is completed using sync weight which will return the value sent by the final sender in the chain foreign there are three key three concepts in the model schedulers senders and receivers schedulers are handles to execution contexts schedulers produce senders senders represent asynchronous work that will eventually send a signal they can be composed together with sender algorithms to form task graphs receivers process asynchronous signals from senders okay let's look at schedulers in more detail I said they are handles to it execution context but what exactly does that mean an execution context is a resource that represents the place where execution will happen this could be a concrete resource like a specific CPU thread pool or a GPU stream it could be a more abstract resource like the current threat of execution execution contexts don't necessarily have a representation in code and today they don't have any exposed interface they may have stayed associated with them OS handles memory metadata Etc schedulers represent a strategy for submitting work to execution contexts they are lightweight non-owning handles to context schedulers are cheap to construct and pass around execution contacts hold all of the state multiple schedulers May refer to the same execution context including multiple different kinds of schedulers for example you might have two schedulers that submit work with a different priority you might even have a scheduler that dispatches work to multiple different execution contexts we use schedulers to produce senders that will perform work on the execution context associated with the scheduler once we've obtained a sender from a scheduler we can compose work on it now let's look at senders and how we actually compose them as I said before senders represent asynchronous work they form the nodes of an asynchronous task graph which may span multiple schedulers and multiple execution contexts senders are lazy you must explicitly start them when ascender's work completes it sends a signal to the receivers attached to it receivers are handles that get notified with a signal by the sender there are three different handling paths which are called channels the value channel is used to indicate successful successful completion and may pass one or more values to the receiver the error Channel indicates that the sender's work failed and passes an error object that contains information about the failure the done channel is used to indicate that the sender's work was canceled before it could be performed this is distinct from the error Channel because cancellation is not an error it may happen during the course of normal operations each sender notifies its attached receivers with one signal meaning only one of the three channels is invoked now let's look at how senders and receivers are hooked up to each other let's start with some scheduler which we will get a sender from a receiver is attached to a sender via connect that's a behind the scenes interface that won't usually appear when using asynchronous operations connect returns an operation state which contains the actual work that the sender represents eventually you initialize the work by calling start on the operation state after some time the operation completes and then notifies the receiver with the signal one important note before we move on to the next topic it is unlikely you will ever deal with receivers or operation states where they connect and start functions the public API for senders and receivers is schedulers and senders not receivers the algorithms that I will talk about shortly take and return senders unless you are writing your own sender's algorithms you won't need to write any receivers next let's look at sender composition we can post together sorry we can post together senders using sender algorithms of which there are a few forms the first is sender adapters under adapters take one or more senders as parameters and return a sender most sender adapters are pipeable just like range adapters the semantics are similar to Unix shells send pipe F pipe G is equivalent to G of f of send languages like Haskell and AP APL call this point free style the primary input argument is not explicitly named this index is essential for elegant composition of senders composing senders via nested functions calls is a mess the order in which operations occur is inverted from the order in which they appear in the code the predecessor senders are more deeply nested and thus appear after their ancestors things get a bit cleaner when we instead use a temporary variable for each step of the composition however this is error prone it's easy to mix up one of those named variables the pipe syntax gives us it gives us a clean way to compose chains of senders in the order that they will be evaluated now let's show you some of the most important sender adapters then takes an invokable F which calls uh and calls it with the value sent by the prior sender the sender returned from then will send the results of the invocation of f this is how you attach a continuation to a sender bulk is the parallel version of then it evaluates the invokable once for each index in the shape n in the simplest and most common case the indices are one-dimensional and start at zero and the shape argument is an integer indicating how many invocations to perform the sender returned from bulk will pass along the signal from the prior sender transfer changes the scheduler that will be used for the next sender it doesn't change the scheduler for the prior sender only the subsequent ones some senders can only be connected to a single receiver for example because they move any values or errors they send instead of copying them we call these One-Shot senders senders that can be connected multiple times are called multi-shot senders split takes any type of sender and returns a multi-shot sender that passes along the signal from the original sender split senders represent forks and Center task graphs conversely when all takes multiple input senders and returns a single aggregate sender that will combine the signals from all the input senders senders returned by when all do not have a scheduler associated with them which means they do not promise when they complete where they complete thank you when all senders represent joins in a sender task graph when all is not a pipeable sender adapter as the partially applied piped form would be ambiguous with a fully applied non-piped form and sure started connects and starts a sender returning a new sender that will pass the signal sent by the original if the input sender is a composition containing other senders those senders will be connected and started as well senders are lazy by default they won't run until their result is needed and sure started is used to turn a lazy sender into an eager sender where the work starts right away and happens concurrently in the background sender factories are another kind of sender algorithm they don't take senders as parameters but they do return a sender sender factories are used to start new chains and graphs the senders they return are the root nodes we've already seen one sender Factory schedule which returns a center that completes on the specified scheduler the return sender doesn't send values or represent any actual work it's just a handle you can use to compose work on the scheduler just is another sender Factory it takes a set of values and produces a sender that will send those values immediately when connected this is how values are inserted into a chain of senders the last kind of sender algorithms our sender can send our consumers they take senders but they do not return senders they typically launch a sender graph by connecting and starting it they are the leaf nodes asunder graphs sync weight is a sender consumer and synchronization primitive that blocks until a sender completes and then returns or throws whatever was sent yeah now let's discuss the details of how sender graphs get formed suppose we have a single Link in a chain of senders we have an abstract before sender adapter on the left side of this link a concrete then adapter and an abstract after adapter on the right hand side if we unroll the pipe syntax we'll have something like this we've got a before sender that came from somewhere we created then sender which will contain the before sender and the function f we'll also create the an after sender which will contain the then sender we'll end up with a nested structure of senders at some point we'll connect a receiver to the outermost and last sender each sender in the nested structure will connect a receiver to its child this happens in the opposite order of sender Construction that will give us a nested receiver structure that's the inverse of the sender structure a nested operation State structure is also produced and returned from connect eventually we'll start that operation State as the operations complete they'll begin notifying their receivers with signals when the before receiver gets a signal it will notify the then receiver if it's a value signal the then receiver will invoke f with the value and Sig and Signal the after receiver with the result of that invocation let's look at a more advanced example a generic asynchronous and parallel inclusive scan we'll write it as a pipeable sender adapter so that it can be composed with other sender algorithms sender adapter here is some wrapper type that returns a Lambda into a sender adapter object it'll take three parameters Ascender which we'll expect to send the input as a range an initial value and the number of tiles to split the input into we're going to use the classic two pass parallel parallel scan approach which requires a temporary storage for partial results communicated between tiles we need to allocate this temporary storage asynchronously once the prior sender has sent the sent us the input so we'll chain a then sender onto the prior Center in the body of the continuation we'll create a vector to hold the partial results we'll return both the input range and the vector next we'll need to do the first parallel pass the downsweep we'll use bulk to invoke the body of the pass for each of the tiles the first thing we do for each tile is calculate the range of elements that belong to the tile and then we take all of the elements in each tile and perform a local serial inclusive scan on them which we do right here next we need to propagate information between the tiles the sum of each tile needs to be added to the elements of all preceding tiles we've already computed that sum it's the last element of the local inclusive scan we store that result in the partials vector assignments to the partials from different tiles may happen concurrently but that's fine each tile uses a different and unique slot in partials and no one reads from partials yet so there's no data race here then after all tiles have completed their local inclusive scans and written to partials we need to have one execution agent do a Serial inclusive scan of partials we do this by piping another then sender onto the chain which will perform the the partial's inclusive scan this then sender will again pass along the input sequence in the partials vector the information that each tile needs to add to its element in the partial slot for the tile directly preceding it sorry the information that each tile needs to add to its elements is the partial is the partial slot for the tile directly preceding it now we need to go parallel again to distribute that information within all tiles this is the upsweep pass so we'll pipe another bulk once again over all tiles in the body of this bulk oh yeah can you lower that one foreign we'll need to calculate which elements belong to the current tile just as we did before in the downsweep pass then we use a Serial for each to increment each element in the tile by the appropriate value from the partials vector after that addition we'll have the correct result finally we want the sender returned by our asynchronous inclusive scan to only send the input sequence not the partials Vector so we had a final then sender which only passes along the input the partials Vector will be destroyed when this then Center completes and that's it we're done this is too much code and too small of a font covered much too quickly for you to read and understand it right now but I do want to pause here for a few seconds to point out some important takeaways we have used several sender algorithms some of them parallel algorithms passing values from one sender algorithm to the next using the convenient pipe syntax the result is a generic asynchronous parallel inclusive scan it is a task graph that describes the work to be done we can arrange for the work to run whenever we want to any scheduler that we want or we could combine this task graph with other senders so it is part of an even bigger task graph we've developed a prototype implementation of standard C plus senders and receivers that supports CPU schedulers GPU schedulers and distributed schedulers let me show you some examples and applications that we've already ported to senders and receivers this simple electromagnetic electromagnetic wave simulation solves Maxwell equations on a uniform grid this is the entire solver Loop which is expressed as a graph of senders this code can be run with a variety of different schedulers by changing just one line of code the kind of scheduler passed to the solver we can go from running in line on a single CPU thread to Running In Parallel on the CPU using openmp to a single GPU to multiple gpus within a single node and to multiple nodes scaling up to thousands of gpus with standard C plus senders and receivers you can change one line of code and scale from a single CPU thread up to an entire cluster of gpus palabose is a framework for parallel Computing computational fluid dynamics simulations using the lattice boltzmann method we've forwarded one of its applications to standard C plus senders and receivers this simulation model models carbon sequestration techniques the forest structure you see is sandstone it is filled with saturated salt water which is not visible in the representation the red bubbles correspond to liquid carbon dioxide which is injected at the bottom and travels through the Sandstone because of buoyancy forces using our distributed GPU scheduler we're able to run this application at scale on up to 512 gpus senders and receivers are the next major step in the in the deployment of C plus standard parallelism they deliver the second and third pillars of our plan the framework gives us the tools we need to write our own generic parallel algorithms that can run anywhere and to compose asynchronous task graphs today C plus plus has no reasonable abstraction for multi-dimensional data this is unfortunate as many of the interesting compute heavy problems that benefit from parallelism have a multi-dimensional shape that's why we're introducing MD span a multi-dimensional span type in C plus 23. it's very similar to the one-dimensional span introduced in C plus 20. md-span is non-owning it's just a handle to some underlying data it doesn't manage the lifetime of the of that data MD span is cheap to copy it just contains a pointer and metadata describing the size and shape of the structure metadata such as the extent of a dimension can be expressed either at runtime or compile time allowing for meta programming and compile time optimizations md-span parameterizes how a multi-dimensional index is mapped to a location in the underlying data we call this parameter a layout and it can express any kind of multi-dimensional structure there are some concrete layouts in the standard library for common use cases but anyone can Define their own layout and plug it into an MD span likewise MD span parameterizes how it accesses the underlying data the default is to just perform a normal C plus pointer dereference within with an index with a custom accessor you could instead use a special cast bypassing instruction read from disk or perform a remote memory access mdspan uses extents objects to express the number of dimensions in a space the rank and their length the extents extense objects take a size type and a variatic number of integrals of that type as template parameters for dynamically sized extents the magic value Dynamic extent is used as an integral template parameter and the extent is passed to the Constructor of NDS pass to the Constructor through the power of C plus 20's class template argument deduction when you're working with all Dynamic extents you usually don't need to spell out the entire verbose instantiation there's also an alias d extents for for the all Dynamic extents case it takes a single parameter an integral specifying the rank for statically sized extents the extent itself is passed to an integral template argument no corresponding Constructor argument is needed you can mix static and dynamic extents for example in this case we have one static extent and one Dynamic extent so we'll need to pass just one extent at runtime extense objects in MD spans support arbitrary rank you can have as many dimensions as you want MD span has four template parameters two of which are optional the first is the element type the second is the extent type this must be a specialization of the extense class template the third is layout the default is layout right we'll discuss layouts more in a few moments the final parameter is an accessor which performs element access MD spans of all Dynamic extents have a simple syntax which class temp with class template argument deduction you can construct one without specifying any template parameters if you do need to spell spell the type out it's still concise just use the D extensor alias the elements of an MD span are accessed via the index operator thanks to a recent core language change in C plus 23 indexing operations can now take multiple parameters you can also make MD spans of static or mixed extents the simplest way to construct these is by passing in extense object as the second Constructor argument now let's talk a bit more about layouts the two most common layouts are layout right and layout left with layout right the rightmost extent is contiguous meaning its stride is one and strides increase right to left as the product of extents this is the layout for C plus plus built-in arrays and numpy it's also the default for MD span for example if we had a two by two Matrix the two elements on the first row would have data locations 0 and 1 and the elements on the second row would have data locations two and three with layout left the leftmost extent is contiguous its stride is one and strides increase left to right as the product of extents this is the layout used by Fortran arrays and by Matlab if we had a two by two Matrix in layout left the two elements in the First Column would have data location zero and one in the two elements in the second column would have data locations two and three there's also a standard layout that allows that allows you to explicitly specify the strides for each element all three of the concrete layouts in the standard Library are just implementations of the layout concept generically a layout is just something that Maps a multi-dimensional index to a data location anyone can define a layout layouts may be non-contiguous they may map multiple indices to the same location they may perform complicated or expensive computations they may have or refer to state parameterizing layout is critical because it allows us to write generic multi-dimensional algorithms that can be used with any layout this is an essential component of portability because different layouts may be needed on different platforms to deliver on performance today we have a major vocabulary problem with multi-dimensional types in the C plus ecosystem suppose I write a function that uses a concrete owning multi-dimensional type like eigen Matrix my users will be able to pass an eigen Matrix to this function but what if they have a boost tube loss Matrix or a pet C Matrix or a blaze Matrix or a Cutlass tensor or a multi-dimensional array passed from Fortran that's where MD span comes in by using it in your interfaces your code can work with any demo any multi-dimensional data structure because MD span is just a non-owning handle you can construct one that refers to an eigen Matrix a boost do blast Matrix a pet C Matrix a blaze Matrix a cutlix tensor an array from Fortran a built-in CRA or whatever for example suppose we had our own row major Matrix class we can add a simple MD span conversion operator allowing us to pass our Matrix class to any interface any interfaces that expect an MD span and some more complex cases we might have to write our own layout type now let's look at how we can use MD span this is a simple 3D 7-point stencil inner kernel similar to what you see in a proxy application like mini ghost we have two 3D MD spans representing the problem State using the Cartesian product of iota's technique I showed earlier we build a range indicating the index space and then use a parallel for each to iterate that space if we want to change the layout all we have to do is change the input MD spans everything else Remains the Same earlier I showed you this example of a simple parallel Matrix transpose we used MD span and dealt with multi-dimensional indexing manually that's not particularly portable we we've hard-coded a specific data layout which may not make sense in all circumstances it's also error prone as it's easy to make a mistake in the indexing formula we can improve this by using MD span instead now we can change layout by simply changing the type of MD span that we use mdspan has a powerful slicing interface submd span it takes an MD span and returns a sliced MD span MD submd span is particularly useful for tiling operations where you want to iterate over a small section of a matrix as if it's where as if it were its own Matrix because MD span is non-owning and cheap to copy slicing is also cheap you're just creating a new view of the same underlying data the input MD span is not modified for each extent of the MD span you pass a slice specifier to the submd span there are three kinds of slice specifiers you can specify a single index to be selected for an extent by passing an integral as a slice specifier the rank of The Returned MD span is reduced by one for each single index slice specifier you can also pass a range of continuous interfaces to be selected for an element finally you can pass a full extent to select an entire extent for example here we have a 3dmd span m0 and we make a slice of it by selecting eight indices from each extent starting at indices 15 31 and 7 respectively the MD span returned by submd span M1 will have a rank of three we didn't use any single index slice specifiers which would cause a rank reduction each of the extents of M1 will be eight multi-dimensional indices for M1 will be offset by what they from what they would be for m0 for example m1000 would be equal to m0 1537 31 7. let's look at another example this time with rank reduction we'll we'll slice m0 selecting index 15 for the first extent the entire second extent and index 31 for the third extent the MD span we produce M2 will have a rank 1. and the extent of that rank will be 128 the extent of the second dimension of m0 iterating this 1D slice will be equivalent to accessing m0 with a fixed index for the first and third element the set of standard C plus algorithms we have today is great but not complete what we have is primarily focused on manipulating one-dimensional sequences of objects we only have a very limited set of numerical algorithms such as reductions in scans we don't want C plus plus programmers writing their own versions of common numerical algorithms we want them to use standard interfaces that are backed by an implementation designed and optimized for the platform they are running on so the C plus committee is exploring new new families of algorithms to standardize starting with linear algebra the C plus committee doesn't want to reinvent numerical linear algebra we like to standardize existing practice and the existing practice for linear algebra is blase the basic linear algebra subprograms which is a many decades-old Library originally developed in Fortran but the Blas library but using Blas libraries today from C plus is painful they have low level C interfaces which often have a great number of parameters that describe things like extent scaling factors whatever or not whether or not to transpose the inputs Etc good luck getting that function call correct on the first try or understanding what it does when you come back to this code a year later depending on the platform you may also have to deal with things like setting up library handles transferring memory Etc we want to standardize a modern C plus plus interface that can be implemented under the hood by existing Blas libraries here's that same code written using the new C plus standard Library linear linear algebra algorithms the existing one-dimensional C plus algorithms parameterized data with iterators and since C plus plus 20 ranges for standard C plus plus linear algebra algorithms we plan to parameterize data with MD spans instead we use the md-span parameters to express thing things that appear as distinct parameters in traditional C style blase interfaces for example instead of having scaling parameters on Matrix Vector product we have a scaled operation that takes a scaling factor and an empty span and returns a new MD span that will apply the scaling Factor upon access C plus plus standard Library implementations can see through this abstraction and extract the scaling factor to feed it to a lower level blase interface that expects it another example is transposed which returns a transposed version of the supplied MD span as with today's C plus algorithms we're planning to have serial and Fork join parallel overloads of linear algebra algorithms in this example where we solve a system via upper triangular chalensky factorization we'd want to change the two operations together and launch them asynchronously in the future we imagine we'll have asynchronous sender adapter forms of these algorithms that will allow you to do that I have thrown a lot of information you over the last hour too much to be easily absorbed so that you can come away from this with something useful here are some of the important highlights see the C plus plus 17 parallel algorithms are a good sized collection a fork join synchronous algorithms they use exception policies to indicate which algorithm calls should be parallelized but give the user no control over how or where the parallelism happens the inputs to the algorithms are one-dimensional sequences indicated by pairs of iterators plus 20 ranges especially the standard views that are part of the range's ecosystem make it easier to compose the inputs for the parallel algorithms in other traditional algorithms senders schedulers and senders algorithms provide composable building blocks for lazy asynchronous test graphs users have much more control over when and where the code runs senders just miss C plus plus 23 so they will almost definitely make it into C plus plus 26 in their current form MD span is a non-owning multi-dimensional view of data it keeps the common cases simple to use while providing flexibility control and extensibility over the rake rate rank extense layout and access of the data empty span is in C plus plus 23. sub MD span however will likely miss C plus plus 23 and B and C plus plus 26. the linear algebra proposal is an easy to use C plus interface on top of laws using md-span as inputs it is currently synchronous using the same exception policies as parallel algorithms The Proposal is still under active development if all goes well it'll begin C plus 26 those C plus 29 is also possible all of the features listed here are currently available in the Nvidia HPC SDK which can be downloaded for free from nvidia.com senders MD span and linear algebra are experimental prototypes in some cases very experimental but all of them can be compiled for multi-core CPUs and for NVIDIA gpus turning the C plus committee's Visions for parallel C plus plus into reality there's plenty of other work to do down the road for C plus plus standard parallelism other classes of numerical algorithms asynchronous streams memory model extensions affinity and locality facilities but with every standard revision we deliver more and more components of C plus story for parallelism I want to end where we started with our goal we need on-ramps to parallelism in standard C plus almost all modern platforms are parallel yet a shocking amount of code does not take advantage of that parallelism so we want to normalize parallelism and accelerated computing writing parallel C plus plus code should be easy and natural parallelism should be the default thank you all for coming here and listening [Applause] questions comments criticisms thoughts right thank you so much it was a lot of information uh in terms of when and where there was this example with inclusive skin and uh I can understand about when it's set up through this adapters I guess but how about where how do we set up schedules schedules there maybe just behind it can you please explain it um okay the the inclusive scan example was setting up right a chain of senders yeah for example right but it set it up as as a sender that wasn't attached to any scheduler yet so the result of calling that code is a sender which then you can attach to a scheduler you can get a sender from a scheduler and attach this one for the inclusive scan right after it and so then it'll run on the execution context of that scheduler that wasn't shown in the example the example was just setting up the task graph you can then use that in the way you use any other sender to run it whenever you want or wherever you want so inclusive scanning is indent one big sender that was got from all these senders and then it's passed the schedule yeah okay thank you yes there was one yeah thanks for the talk um a question regarding schedulers you said there are multi-schedules that it can schedule to different computation platforms Maya will it be possible to have like um runtime decision for Optimum or priorities so for example I want to compute ffts but I don't know if the client platform will have a GPU driver available and if not please do it on the CPU but if it's available run it on the GPU instead yes you can you can write any you can put whatever you want into a scheduler into a want so that exact thing of runtime checking what what Hardware resources do I have available which one's the best for this particular kind of problem um yeah people have been asking for that years I'm sure that will be a common scheduler once we get this out there in the wild so yeah that that is that is a use use case we will definitely support