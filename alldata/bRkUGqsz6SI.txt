so what yeah what we got here then what's this playing around with this quite fun uh i borrowed this from our robot lab this is an intel realsense which is a kind of all-in-one depth sensor right so i thought we'd just talk about what it is and why might we want depth in the vision literature which is sort of where i am we would usually term this rgbd right which is red green and blue and also depth right now depth is kind of like 3d in the sense that you know where things are in a scene but actually really you just know how far they are away from the camera which is not as good as 3d because of course you don't know what's behind those things but it's better perhaps than just rgb because it allows you to for example separate objects much much more easily quite a lot of research is going on in rgbd just in general because it makes a lot of problems slightly easier or more powerful you know deep learning applied to rgbd is traditionally going to be a bit better performing than just rgb because the network doesn't have to make that effort to separate where the object boundaries are you could just find the thing that's closest and do that right that it would theoretically make it easier now actually i don't do a lot of real sense only because i only got this yesterday but and i probably have to give it back but you know maybe if i do a really good video they'll let me keep it um but i thought we'd talk about you know what why has it got four things on it you know how does this relate to for example the connect that everyone knows famously that was one of the first things to do this and what's new and interesting about this kind of technology well let's demo it first right so i've got my intel realsense viewer here i've made almost no effort right so i haven't installed the sdk i'm not using this with code although you can do this i'm just going to turn on the stereo camera and now it's booted up you can actually see it's pretty good right i mean you know when you get closer yeah that's working it's working that's not bad i can see i can see the microphone and the i've got my hand in the way now um and then you're behind it over here and you can just generally see that there's a kind of wall behind you that's kind of the shape that i would expect if i turn on the camera as well you can see that this is kind of what the scene is looking at there's sean yeah so i mean it works pretty well now people who are expecting a perfect depth map might be surprised that there's a little bit of missing stuff here and perhaps we can talk a bit about the technology stereo is in some sense to solve problem in the sense that this is very good right very fast it's never going to be perfect right there's a few reasons for this mostly it comes down to essentially you can't see everything with both eyes and so you know on the left here there's a huge gap and that's because the right camera can't see that bit of a scene and so we can't resolve any kind of depth there it's the same thing of when you hold your finger up and you actually see two fingers right if i'm looking at you and i'm looking at my finger through my finger i can see two fingers you kind of ignore that because you're used to it but those things appear as artifacts in this kind of image so how does this work well i'll turn it off um so what we've got is we've got four sensors on here these two are cameras here um the left camera and the right camera all right i am that is correct it's just because they're from my point of view not from your point of view right this is an ir dot pattern emitter which just splashes dot pattern and we'll talk about that and then this is the actual rgb camera right now what the whole point of the real sense is that it does rgb d so it will it knows where these cameras are on its pcb it's been calibrated and that means when it produces a depth map it can also attach rgb values to those exact pixels right by knowing where this sits compared to these now we talked about stereo in a different video and it was about matching between images in general the idea of stereo is you have some pixels in your left image for example and you try and find the corresponding position in your right image and you use triangulation to work out then how far away they are now in this setup that's a lot easier because these cameras are fixed they're very well calibrated and essentially they're level with each other so basically the algorithm just needs to look where you are on the left and look left and right in the right image until it finds the corresponding matching feature and work out the depth and it does that with respect to the left image so it goes it starts in the left image fighting for all pixels it looks in the right image to find out which pixels um correspond to them and then what the depth is and it will also be doing some stuff to do with smoothing and things like that generally i mean we mentioned this in a stereo video but generally scenes like your camera has got a lot of different depths but this wall doesn't right and so it can produce smoother things on the wall right this is not an interesting wall um i should put some artwork up or something anyway so what does this dot pattern do well one of the problems with stereo is that you need features to match you need to be able to say okay i found these pixels that look like this and in the right image they're over here but if your pixels are all a white wall that becomes really quite difficult to do and so what the connect introduced was this ir dot pattern so essentially you can you can you can see pictures of this online with people who've got ir sensitive cameras looking at this but it just splashes a sort of semi-random pseudo-random pattern of dots over everything and the kinect directly used that to work out what the depth was based on the warping of those dots so you'd see the pattern obviously differently on a flat surface to a yeah yes and and for example if the pattern looks small and far away it's small and far away like something like that right the pattern is a of a known structure and so when that structure changes you can say okay that's because of the depth this is not quite the same as this this uses the infrared as an optional additional texture that can help but it's not mandatory right so what that means is that this camp these cameras see both the scene but also a load of dot pattern that's put everywhere if it's there and they can use that to say okay i definitely am pretty sure even on this white wall these two positions match right and we can see this actually if i plug this back in so if i put this in here right so this looks pretty good what this is now using stereo matching between both images and it's also using the infrared dot pattern to provide additional texture if i cover up one of the cameras we lose the depth obviously right it's just a camera now it doesn't work if i take the camera back it comes back in right it's quite robust actually it didn't crash anything i had written to do this would have probably broken um if i cover up the infrared dot pattern which i'm sort of guessing where that is it's over here right then you can see that the depth still works but it's much much worse than it was before and that's because now we only have the two cameras we're just doing the best we can with what we've got so areas like you where you've got quite a lot of texture are still actually pretty good right it's worse but it's not bad the wall over there which is essentially plain white has basically started to completely fail over here on the corner because there's nothing really of interest on that wall if i take my finger off we can see suddenly it's much much much better because on this white wall this infrared pattern is really really helping right so this is what makes this quite a cool little device is it first of all it doesn't crash when i cover up one of the cameras which i like just as the software engineer me thinks that's good but it's optional right so some of the real sensors don't have an infrared emitter but this one has its optional infrared emitter where if it works it can help if it doesn't work you get some depth back outside infrared is much less effective you can imagine against the sun this dot pattern is essentially not visible so when you're outside you're going to fall back to normal stereo vision the other thing to mention is that no processing is being done on my laptop at all right so my laptop the fans aren't spinning up it's having to display this on the screen but it's not having to work very hard and that's because all of a stereo matching all of the use of infrared and all of the rgb you know alignment is being done on this device which of course is exactly what we want right and that was actually one of the first things that connect did and then and since then all these other devices that do much the same thing because if you have to use one of your cores on your computer to do the actual matching then suddenly that matching is going to slow down when your computer's under load it's just a complete pain it's nice to have a camera that just gives you a depth sensing without having to do a load of extra work to get it right but of course other depth sensing technologies like lidar timer flight lasers and things like this which are perhaps arguably more accurate more expensive i mentioned that deep learning for example would be better in rgbd so you actually it's quite common so for example one of the things we're looking at um is object saliency which is how imp you know where are the important objects in an image and in object saliency there are various data sets but there are also rgbd data sets which are essentially the same but they also have depth information and it depends on how the data set was captured and actually this is true of a lot of fields where you've got some data sets in rgb some data sets in rgbd and you know the techniques for rgbd as you imagine are pretty similar right they might vary slightly so for example if you've got a normal three channel image then if you remember what you put into a deep network you would normally have rgb and we represent these as planes of image now actually of course thinking back to one of my first videos maybe my first video on how digital images are stored if i can remember that 255 would be the most of that color that the camera's seen a much younger version of me we actually store them pixel wise rg and b as little bytes within the pixel but we don't do that when we put them into deep networks we put them in as three channels of input like this r g and b or bgr whatever right now this goes into the first layer of your network and is convolved using a convolution and so on right goes through your network and so your network would do some some convolutions and some filtering and then it would make some sort of decision produce some kind of segmentation or whatever your downstream task actually is or your your objective now in rgbd it's exactly the same we just have an extra channel so we have r g b d so this is our depth map and then r g and b that's not a b and you can put this into a network and just you know run it as normal right the network in some sense doesn't care that you've got four channels not three you just have to change the first layer so actually what you do is you have an identical network where you just reconfigure this very first layer to take an input of four instead of an input of three and that will require retraining but it's not a big task to do this and you might find that if you do that you you have some kind of performance improvement right with the caveat that you've got to have gone then had a real sense or some other camera capture this additional data right so you'll find that people do this when it's necessary to do this or useful to do this not just because why not capture depth right there is some there's some headache in doing so also the depth map might be bad right we saw when the infrared drops down it gets less good this is true outside when there is a poor texture it's worth bearing in mind now some techniques separate the depth from rgb so what they would do is they would have an rgb a pathway like this and then they'd have a set put d pathway like this which does some processing and maybe they join these together for some kind of decision making right and that's also quite common so the reason you might do this is if you think that the depth is in some way it somewhere needs to be treated separately to the rgb because it represents some different data that might be what you do right it's that kind of idea but you can see it intuitively not difficult to build depth information into a deep network or into any image processing uh pipeline and you usually get a bit of a boost in performance giving it a depth channel even though technically visa means something different not really that important as long as you train it appropriately doesn't really matter you might find some performance benefit of keeping it separate for a while or some other strategy but in some sense there's no harm in doing it like the it's not this they used to be quite expensive stereo setups or difficult to configure now this comes out of the box i plugged it in and it immediately returned me a depth map there was no calibration there was no faffing about someone must have calibrated it in the factory i guess so i was surprised actually i thought it would work worse than it did it worked really really well and so theoretically it's just there's no big headache for me to just to use this it produces rgb as well so i can stream this directly and put it into my deep learning albums and it's not an expensive device it's you know we order a few hundred pounds for these kind of devices maybe less and so that sort of consumer budget you could see that it might be something that people could actually buy and make use of that's a quantum bit so it's not just zero or one it can be a mixture of zero and one so what we have is this and this and we have some mixture we have some super position machine and see can't see or hear each other they can't pick up each other's transmissions