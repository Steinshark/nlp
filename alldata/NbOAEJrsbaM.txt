every database has its sweet spots and its drawbacks by Design they've been optimized for specific use cases and that's why there are so many kinds of database you get some that have been optimized for analytics some have been optimized for embedding in web pages or Hardware some for distributed transactions or realtime processing some of them are truly web scale you pick which strengths you need and there's probably a database out there that Exel at exactly those strengths but what do you do when your system outgrows that original decision well one answer comes from this week's guest Philipe Noel he had hit the limits of what postgress can do for fulltech search and analytics and instead of doing what a lot of us would do building a hybrid system with elastic surge he dove into the heart of how postgress Works CU I didn't know this before we met but postgress has an extension mechanism that goes so deep you can actually change the way indexes are constructed and the way tables get written to dis so there's a lot of flexibility in there to change what postgress strengths are and they ended up building a custom search mechanism and a columna storage engine to make postgress do everything it already did well and a lot more besides well I had to get him in to explain how and while we're on the topic what what does that mean for postgress going forward I mean how far can you extend it is the postgress of the future going to be a database or a platform on which new databases can be built let's dive in and find out I'm your host Chris Jenkins this is developer voices and today's voice is Philipe Noel [Music] joining me today is Philipe Noel Philipe how are you I'm good how are you thanks for having me good pleasure yeah I'm I'm excited because you are you're um by my standards you're a postgress dark wizard yeah I don't know about that but uh you know certainly know a thing or two you're further down the rabbit hole than I've ever been so that's exciting to me that yeah that's that's perhaps true so we're going to get into building very bespoke features into postgress uh before we do before we go right into the depths how come you learned about this what to what where did you get to in life that you needed to extend postgress we that's a good question um we used to be but we were postgress users for a long time as developers ourselves right me and me and my my co-founder and my colleagues we built a few projects and we know we liked it as from an new user perspective but um last summer we were we were working in the um we were working on a new startup and as we talked to more and more customers it seemed like there was a need to do better search in postgress and that got us kind of interested in in understanding how it works and the internals and why no one had solved this problem before and as we did we kind of realized maybe there was a thing for us to learn there and move from being you know users to developers of postgress and that's kind of how we got started with it right but this this um there's plenty of people that would hit that problem some people would hit it and say okay post Chris has some kind of full teex search is that not good enough other people would say well let's bundle in elastic search you seem like you've taken the hard path yeah well we wanted to do it not just for ourselves I think that was another reason there we um well there's there's there's a couple of points to to your answer to the answer to your question sorry on the one end it is true that it has full Tex search and that's already pretty good you can go very far with it I'm happy to talk to you about it we often recommend people with especially like small to medium workloads to stick with the sort of native functionalities but eventually you need more and um for our use case we needed more and we saw a lot of other people in the space that also need it kind of more complex feature set and at that point we already were kind of drawn into the idea of hacking on postgress ourselves um I don't know if you've seen but there's there's actually an article that was circulating around last week um that said hacking on postgress is very very hard and it's by one of the the main postr committers um it's very hard but it's also very cool and that got us kind of interested in solving the problem for like the let's say like more complex workloads that people had how hard would you say it is I don't know I mean okay this is also a bit of a we don't do the hardest hacking on postgress so let's say we do like we do maybe like a notch before some of the people that work on actual like Upstream source code um they they do the real hard work and I can pretend to be someone that's doing that work because I'm not um it's quite hard I mean the code base is like 30 something years old right it's it's it's enormous really by the and there's so many things that that can change and go wrong and there's like seven or eight people in the world that have Commit access to actual postgress so it's very very uh it's a very select group right okay so you're not doing the you're not trying to hatch postgress so much as extend it right because it has an extension mechanism that's right that's right the only one of those extensions I've ever really used in Anger is like um geospatial I've cly never looked at building my own yeah it's a very popular one geospatial is a big one in fact allegedly this is a non-verified information by myself but allegedly I've heard that in geospatial conferences the majority of people who are there are post grass users that use geospatial for post sort of the primary way people storage your spatial data nowadays oh really so that suggests for a cunning person extending postgress there are latent markets out there that's certainly that's certainly the hope but it's also true in practice um postgress postgress is growing to be more than just a relational database um it already is by by um by you know in a lot of ways and for us we we do search right in postgress but there's other people that focus on it in other ways post is an example where you can modify to store geospatial data um another particularly relevant extension of recent times is PG Vector which allows you to store Vector data and do Vector search with postr which obviously has come to be you know a big deal in in the last year or two super hot right now yeah and so you can kind of transform it into anything and there's even this movement happening in the in the ecosystem that people call postgress everything where they're trying to run your entire data infrastructure exclusively on postgress and variants of postgress modified for these extensions for example okay so I don't we're there yet but well this is perhaps another stepping stone in that in that path because you're going to teach me and therefore everyone listening if I wanted to extend if I imagine a new feature for postest and I want to extend it where do I start where did you to start yeah so great question um so postgress is an extension mechanism where you can create small extensions that can be loaded into the database without needing to modify the core source code that's very very convenient to getting started there's two there's two main ways you can go and build extensions nowadays postgress itself is written in C and so comes with an extension framework to write extensions in see you can look at the post documentation and get started um however the way we started is there's a sort of a meta project a framework called PGX which allows building extensions and rust and so the it's quite a popular project a lot of companies and and new extensions are built off of it the majority of them really nowadays but someone uh by the name of Eric who who's quite he's much more of a prous wizard than I'll ever be um and a couple of his colleagues they created this framework where you can create extensions at rust so you write rust code and it generates the extensions in C basically at the end of it and that's quite a phenomenal way to get started they made it very very ergonomic and they have examples and so on so you can get started in that way and that's how I would recommend people start okay um I'm going to dive into that but what's the firstly what's the um what's the development experience like in that do I have to recompile postgress every time I want to make a change or how painless is it it's it's wonderful honestly it's wonderful so if you use if you I'll speak for using PGX because that's what we use um but if you do it with the extensions in CA as well it's also quite fast you don't need to recompile postgress typically you install postgress right on using your package manager um PGX they can also install postgress for you if you don't have it with a package manager you want them to manage everything and then afterwards it's the pretty standard rust development experience like you know cargo cargo run and it's the way it works is it creates um it packages your extension installs it into a temporary postgress instance that's blank which just your extension install and then you can do some testing with it and they also have like packaging and testing commands and so on so it's quite standard it's very very painless I would say it they make it quite easy if you do it with the framework in C it's not quite as ergonomic but it's it'sit pretty nice too but it's the standard C development experience you know like make make install and you'll have your um your you know your package manager post grass and then it would install there some of the it's it's less aemp poent I would say in that experience so sometimes you might find that you know like your extension is still installed some part of the system which like can cause some unexpected errors or some unexpected behaviors but if you work with the rust version it's it's quite good it's you know it's made for the kids it's it's modern yeah that that's impressive because they could easily stop like Mak rust bindings to the SE core easy yeah they they have taken it so far and I've got the chance to talk to Eric he's told me that he's done it because he needed it himself so for context Eric created this extension called zombo DB um they were one of the early extensions in postgress it's quite uh you know it seems a lot of mileage by now and ironically it's quite related to what to what I work on they were bu building a connector between elastics search and postgress so um the work that I've done and that we've done with my team is we've modified the way ftech search works in post gr to make it more capable and more comparable to elastic what Eric did back um you know several years ago was he had elastic search running separately and he had this postgress and he wanted to be able to run postgress commands and have them be used to uh manage the elastic search cluster sort of a bridge between the two and he first built that it's called zbo DB he first built that in and eventually had a variety of issues learned about rust and thought he could do things better if he could rewrite it in Rust and so he built this PGX package like extension framework initially for himself and when I talked to him um he's told me that you know he works on it because it's useful for himself and so on but I think he's way past altruism at that point and like he could have stopped a long long time ago he says he couldn't I think he could have stopped a long long time ago and now he does it because he's a good guy and this is like improving the ecosystem and so on and so they're really they're really taking it to great great to very far to great quality nice nice okay so it I'm feeling slightly less scared about the prospect of doing this one day but what's actually like so you need where do you start you need to do fulltech search better in postgress you create an index extension yep so so what we did is we created an index extension there's two main ways to poke with storage in postgress one way is with what they call an index access method so a database Index right you can index some data from a table for example um that's what we do for fex search and the existing capabilities for basic fa extion postgress also work in a similar way that's quite that's quite common um so we created an index access method via an extension and whenever you run our post extension it gives you this extra command so in postgress you can say like create index some name on specific table um now when if our extension is loaded you have like an extra option as the possibilities for index and it runs our code instead of running the the native code and um I think we'll get into that later but another way in which you can modify the way data sort and postgress is to do what's called a table access method or instead of modifying the way um the way the index is is an index is created on top of it table you modify the way the table data itself is is stored and that's a somewhat recent functionality came in late 2019 early 2020 really okay a 30-year project I suppose that is recent yeah exactly it's not like recent recent but it's relatively recent when we when we did that in Via PGX earlier this year we were allegedly according to to Eric the author of of the framework the first people to do it in in Rust to modify the table Engine with is with framework so when I asked him about it he said it was supposed to work but he had no idea if it was going to work because no one had tried it before someone else can go first one step back yeah yeah okay well let's start with indexes because I feel like they're going to be easier to understand my guess is somewhere in there there's a hook where postgress tells me there's a new row coming in I've got to write I got to write an update hook and I've got to write a find hook I mean what's it actually like so a lot of this a lot of this is already abstracted away from us so it's quite nice so when you um if if you go and you look at the postgress documentation you can see there's a couple of functions for defining index access method and there's a couple of functions for defining um table access methods the index access methods are quite quite small are are are not very many functions really I could I could pull up like the list of all the functions that you need but you implement these like these handful of functions and then afterwards you're good so exactly like it basically allows you to fetch the tles right whenever a new row comes in which is a topple in postgress um it fetches that and then it re indexes it into the the um like whatever indexing scheme that you use in our case we work with a library called tentv which is a rust implementation of Lucine Lucine is this yeah maybe Lucine rings a bell but it's this um search Eng framework that on which elastic is built and it's pretty state-of-the-art the underlying technology is called an inverted index that's how um that's how you can tokenize the words in such a way that they're easy to retrieve using full teex search and we basically bundle that in so you sort of bridge the two you implement the functions to fetch the topples from the rows that come in in a new table for examp in a table when like there's updates that happen to it and then these get index like pass over to 10TV to be index in those inverted indices and those get stored on disk with um inside the database and then whenever you fetch uh or you make queries against the index it retrieves the information against the the index data instead of it against the table okay what's the your index file is that something that's managed for you by postgress or do you have to I mean do you get some kind of scratch file storage file in which you can work for your index or do you have to do everything manually we do it manually but the index access method framework has a lot of um there has a lot of things taken care of for you but in postgress but you have to mostly do it manually so what I mean by that is we um so tentv that Lucine library that we use they have their own file um extension that they use to store data on dis this is what we use but then afterwards we need to do the wiring up to postgress and really um you know speaking of hacking on postgress and getting started and so on making extensions can be as easy or as hard as you want it to be it can be as easy in the sense that this framework that I mentioned is very very ergonomic and if you do things that are relatively simple it's quite easy you can put something together in like an hour and it's going to work well but for things like us where you modify the file system for example you interact heavily with the tables you need to kind of connect the pieces with all of that makes postgress postgress like you need you know the index to be cleaned up if you do vacuuming for example right um you need you need it to be connected to the right ahead logging system so that you can replicate the index to other postr shards if you want um or have yeah have high availability whatever it is that you might need and so all of these are actually quite a decent amount of work and even we haven't implemented all of these ourselves by now we've implemented under the majority but there's still a few things that are Works in progress right yeah I can see I can see it being a rabbit hole as large as you like but so the essence of your project is you've said it would be better if this data were in elastic search but we want to keep it in postgress so if we rip out the library from elastic search or a close equivalent and all your job will be glue in Rust yeah that's that's a pretty good way to to summarize it exactly okay I can I can see that bringing hope to people that want to spin out another database but don't want to spin out another database exactly the idea really is well elastic has is is powerful right but the truth is for a lot of people you don't need that power but you still need like you need more power than progress offers but you don't need the cumbersomeness and the scale and all the problems that come with doing something that's really powerful right like perhaps you know like you need more than a kitchen knife but you don't need a chainsaw right what if you just needed a butcher knife right or we could actually just give you something like that okay okay take me through a bit more of this API like let me think if I let me see if I can think of a interesting Corner case like you must have to do something for the sake of Optima is a statistics right your custom index has different access pth P it must affect the optimizer yes so um so there's a couple of things I mean one is the the way we make it work so it's actually quite interesting I um the way we make it work is we use various procedural calls to create our indexes in postgress so it's kind of a unique syntax it's a bit of an uncommon syntax and we um when you make queries against it we make sure that you only route the queries directly to it only routes the queries directly to our index So the postgress plan actually I may be speaking a bit out of my depth here but um when we do the scanning like it essentially only scans our index and all of our queries at least as of today they get executed ex exclusively against our own index so one example of the optimizer not I think of it is the way we do limits in postgress so let's say you're making a query against against a search index and you want to say like retrieve me uh you know everything that contains the word red from this list of of clothing and you want to limit to only five items for example um one thing that we do to be able to optimize the way the postgress optimizer and postgress planner runs the queries is there's specific keywords that need to be handled whether by tent Tov or whether by pogress to have maximum push down of the computer so for instance um tentv which is the the the search Library it comes with its own limit capabilities and whether you run those limit capabilities with postgress or with intent to makes a huge difference on performance because there's only so many um so much that the index access method API gives us in our ability to work with the optimizer right and to to optimize certain queries so we recommend for users to we have like some what um somewhat peculiar syntax or mean not peculiar but um pedantic syntax perhaps like it needs to be exactly the way um the way the way that we recommend it to be optimized otherwise you can still run standard postgress queries on top of your of your search index but it's not going to be as fast right so the the integration isn't completely seamless for the end user there'll be aware that they're using something custom here yeah they are they are it's it's pretty good now it's mostly seamless but it's not perfect and that's because of the way the planner optimizes um the planner interacts with the optimizer for example something else to uh to mention here is when you do work with index access method you need to implement a function called the am cost estimate stands for Access method cost estimate yeah and that's another example where like Bas on how that function is implemented it will um it will influence the way the plan planner executes your queries and that might require a little bit of um you know a little bit of poking around in various ways to make sure like it does what you want it to there's a bit of a it's a bit of an art more than a science the same goes for the the table access method yeah yeah do you get um you can I seem to remember in postgress you can get it to deliberately reanalyze indexes for the sake of the query planner do you get a hook for that where you say okay time to update your estimates about how fast you are a certain kind of query you can so so for the um for the index side of things you can actually do it pretty um it's pretty seamless so it's well so because index access methods don't modify the actual storage of the data in the tables um they're sort of like they're not truly in the guts of pogress a little bit on the on the peripheral side um it's kind of connected pretty seamlessly to to the explain analyze commands and so on so you can uh you can EV um look over the plan and things like that pretty well and postgress will do a lot of the optimization for you but when you start to work in the core of the storage engine itself you need to do a lot of that yourself yeah and so at that point you do have like a specific hook that you can use to query the planner to query the executor at various points in times and make sure that it does what you want and you can even come in and do plan optimization yourself um and once we talked about the storage the storage uh the table storage side I can tell you more about how we do plan optimization on that because it's a lot more um it's a lot more intentional and manual than at the index side on the index level we basically try to make post grass do most of it our itself right we want to mess with the ecosystem as little as possible right yeah yeah yeah okay well in that case I think we probably should get into the hairy world of replacing the storage engine cuz I didn't know until we spoke recently I didn't even know this was possible that you could rip out the way the postris thinks about tables yeah I think not that many people know it's possible and it's it's not very he it's not heavily advertised but there's a few people that have worked on it we're not the only one there's maybe three or four teams that have worked on it from various um various angles I'm happy to tell you about it it's it's a cool it's quite cool actually you can write an extension that changes something so critical to postgress so there's enough there's enough rope to hang yourself here yes very much so yes you got to be careful what you do out of curiosity is this part of the reason for choosing rust is it like you don't want to accidentally crash your users postgress instance uh I mean we chose rust because it was rust or C and choose I mean why would you CH see I think at that point but for for from Eric from the team that made pjx in the first place I'm part of the reason for choosing rust was to make sure like you can do memory management um in a in an easier way for sure um but still even like even though we've had to deal with some pretty hairy memory memory issues ourselves despite working in Rust there's sometimes you have to go and work with the postgress functions directly and so it's not fully fully hands off you can still crash your user post instances oh good oh good got to have something to keep you awake at night right yeah exactly exactly but rust it you know it it Pets the pillow you gets you your sleep a little bit better but it's not perfect yet okay so maybe you should tell me why you need a custom storage engine because I understand full Tex search I can understand that needs a custom index why do you want a custom storage yeah so the work we do is split into two categories we are trying as you and I talked about to make the workload that someone might use elastic search for be possible via postgress so you don't need to move data and you can stay within postgress obviously the name is elastic search and when people hear that name they think of search right and so that's the first thing that we did but actually another key functionality that that elastic offers is analytics and it's actually quite common for people to do analytics and full Tex search within the same query as well right yeah and so so that kind of got us to thinking okay could we actually make postgress do analytics and if you're familiar at all with with databases you'll know that there's two main ways databases store data depending on the use case for relational databases like postest they store data in row format so Onis you know the sequences of bite that are stored in row format so like a specific row is stored next to one another on the dis and that makes it easy to do reads for sequential for point in time read so let's say we're trying to retrieve like you're trying to log into your online B in system it's trying to authenticate you against your user account right just trying to retrieve your information the other way that databases store data is in column format so instead of storing rows next to each other on on your SSD it's going to store columns next to each other so the column is sort of flipped into a row format from the dis and then each column so like by data type so we store all the usernames in one big part of and all the ages and other part disc and exactly and this is optimal because when you're reading the data on dis sequential reads are significantly faster than if you need to jump the you know the reader to various parts of the hard drive and so um if you have query patterns where you need to read specific columns for example analytics where you say okay what's the I don't know the total sales across all locations this month right something like that um then you look over like all the TR like the sum up all the transactions for instance and um it's significantly faster to do this over when you sorted in column in column format yeah because we wanted to do analytics alongside our search we were like okay we need a way to do that in postgress postgress stores data in row format that's very inefficient for this type of workload we need to store data in column format and that brought us to saying oh well if we can modify the way data is represented under the postest tables from the perspective of the user we just give you postgress tables right and you don't need to worry about anything but us ourselves we need to gut the inside a little bit and say hey will store data in column format that is the point at which I would have given up with postgress and said well this fundamentally not the way it works yeah it and it is fundamentally not the way it works you're correct but again there's this PO is so extensible and people wanted to use them for so many things it kind of reached a point where we said Hey what if it could be the way it works right there's so many things that are wonderful about postgress if you can abstract away this problem or no this challenge um we do the hard work but you as someone who uses POS you need to worry about it and it's quite seamless really when you create a table and you you know create table my table right and then you can specify the the schema for your table Yeah if you use our extensions you can say create table my table using and then you specify the storage format we use parket which is this very very industry standard colum format so you say create table my analytics table using Park and now it's going to create it in column format and that's it now you everything else you do is the same you can run the same queries with the same syntax you don't need to change anything just the table creation name and then everything is the you know the analytics are 95 times faster than regular postgress and that's because the data well one of the reasons because the data is stored in in column format on this and we've had to go and there's 42 or 4 three functions you need to implement to create a um a custom table method it's a lot more than the index yeah but um yeah once you get that done you it from the end user like the database user perspective it's pretty it's pretty transparent like how transparent like are updates the same are joins the same yes and no in a sense that it depends how you build it out afterwards this is actually a pretty um you're asking a simple question that has a very long winded answer but um okay we the answer is yes and no so the way we do it um it depends how you process the queries by the end of it so the way we build our own custom storage method is um we modify the way we do custom storage and we also modify the way postgress executes queries on top of it so we have more speed ups and we integrate other indust projects for this I'm happy to talk to you about if you don't modify the way postgress queries get executed the answer is yes like everything's going to work the same in our case because we modify the storage engine quite heavily um there's a couple of things we need to do to wire up the pieces together so if we only modified the storage engine the answer would have been no there would be like other things that we would have needed to do we kind of finish those but if you do if you stay within the row structure Paradigm the answer is yes so the initial the initial um motivation for implementing this API in postgress in 2019 2020 in the first place was by Fujitsu I believe and VMware and what they wanted to do is they wanted to improve the way vacuuming is done in postgress okay but it's very difficult to get the seven or eight people that have postgress Commit access to approve your changes because they're you know they care a lot about keeping Upstream extremely small extremely reliable right they thingss to be very well tested so what those people thought is hey we want to rework the way data is stored on disk so that we can have a more efficient vacuuming process if we can create an extension API that allows modifying the table we can offer like an alternative storage engine and people can start testing it without it being committed to the core and that started this project in the first place right and so there's work that's done there to just say like oh like it's still post it's still a relational data model like it's we're not changing the Paradigm we're just trying to make it a little bit better we kind of took it in a different approach and said like no no no we'll now use this to convert postgress for analytics and in our use case there's a little bit more that you need to do to make joints work now that we have things things work you know across the board and the user wouldn't necessarily know but from you know if someone is listening to us and says like I want to go and Implement my custom to table method um just so you know you may need to do some more than just make the data be stored in column or format to like put all the pieces wir together properly right yeah yeah it depends on how fundamentally different to the original assumptions you're going exactly exactly give me some Clues how how does I mean don't give me your Trade Secrets but logically how does the game of joining change when you're in a hybrid row and column format hybrid row and colum oriented data format yeah so that was once we released the the first version of our of our custom storage uh method that was a number one requested feature and so we spent a lot of time looking at it and the so the way we do it well actually before the way we do our our analytic side like our custom storage we also use a separate query engine and that's important because that's what we use to be able to do the hybrid joints we call them hybrid joints or htab joints okay whenever um when you want to do fast analytics there's two important components one is to do column or storage of the data so you can read the data faster but the other is you want to be able to do vectorized processing so when your database goes and fetches data from the storage um typically the way postgress works is it's going to fetch one row at a time or one tle at a time so if I'm retrieving information I'm going to go one by post is going to go one by one and again that's because postgress is optimized for Point lookups right look up your user account look up like your credit card number to make sure your transaction's correct or like things like that big databases that do analytics there there's might be grabbing like let's say doing a account over the entire sales or the entire purchase history right and so you don't you're not interested in a specific value so in that case it's just more convenient and faster if you can grab like batches of T PS at a time to process them that's called vectorized processing right yeah and so you're reading like a large column and instead of reading like a one a time you say like I'll read chunks of the column all at the same time to make things faster so we also needed to integrate that into postgress to make it really fast and to make it competitive with industry standards nowadays our extension is is pretty competitive with some of the fastest databases in the market as well that are not postgress in order to do that we use a project called apachi data Fusion which is a composable query engine for analytics it's also written in r and we also integrated in and so that's why when I was mentioning there's kind of two things that we do from our table from our custom storage method we modify the way the data is stored but we also modify the way the queries are are um are executed so when you run queries on that custom St table storage method instead of running them via post grass we actually transfer them from post grass to this uh embedded query engine that we have in our extension and those get executed against it and that's much faster okay so therefore you you're taking you're not hooking into to as we were talking about earlier you're not hooking into postgress is Optimizer you've got an entirely different Optimizer to play with that's right that's right that's right and you were asking about hooks there's a lot of hooks that postgress makes available so you can intercept query plans at the beginning at the end of the plan optimization you can optimize it at the beginning of the execution stage where the plan gets passed on to actually be computed on top of the data right at the end of the execution stage so on and so forth and what we do is we up we we intercept the plan pretty quickly and we reroute it to that analytics engine and that analytics engine does its own optimization on it that's optimized for specifically processing par colum or data makes sense so as a user I'm just writing a regular query and you're hijacking it sometime after it's passed that's right exactly that's right the reason reason I mentioned all of this is because going back to your question you asked how does it work to do joins between hybrid rows and columns right and so what we do essentially is we do uh we we Federate the query there's multiple tools that exist out there Big Data tools are common for doing this like spark and so on um that allow you to query multiple databases in parallel right um and say like oh and so it's called it's called query Federation so what we've done essentially is we've taken kind of two databases and fuse them into one right but we've kept a boundary between the two like you have postgress of course right and you have like regular postgress and you have our custom storage and our custom query processing on top of it and we make this transparent to user but to do this we want to say hey let's say you're trying to do a join between a regular postgress table and our analytics postgress table we want to be able to say okay from your query we want to build the largest possible sub plan that can be executed entirely on postgress and the largest possible sub plan that can be executed entirely on our custom storage execute those in their optimized ways and then after that join the final result that's how we do it so if I let me try and make this concrete I come along and I query all the sales in my database and I get that total fine now I say let's slice that up a bit and say all the sales for France mhm which is I don't know let's say it's a fifth of my data sure how's that going to play out if you want to join that with something else from your from your Ro tables so I've got the the information that it's from France is in a different table Yeah well I mean what would happen basically right is you would let's say you have one table that contains I don't know like your your your store locations right like you have your store location table um and then you have like your full like like all the sales right sales ID and then item and and price something like that right the way it would work is your join would essentially be Federate let's say let's say your um you know your sales location is a standard postgress Heap table STS in a bro format and then all of your sales are stored in those analytics table so that you can do fast aggregation on top of them the way would work is that plan would be split between finding all of the store locations that have France as their country of origin right all of that would get essentially processed exclusively on the on the postgress Heap table you would have all of the stores that um that have all your purchase orders and have the ID mapped to those specific stores for example and then afterwards your query will get Federated back and then executed on the analytics table so the first part that would happen would be fetching all of the stores that are in France and the associated you know sales IDs for example and then those would get um joined with the data that contains your all of your purchase orders that are analytics and those will get executed on the analytics engine at the end of the day every single joint finishes on the analytics and Joint uh the analytics engine because that's where the bigger aggregations always happen right unless you design your your schema incorrectly um so but the way it works is your Cy gets passed in the plan gets constructed of that plan we recursively walk back from the bottom of the plan to find the largest possible point where it's going to break and requires like the analytics engine before we reach that point we execute everything we can on the postgress side and afterwards the final results gets join with What's um Avail what's done on the analytics table yeah that makes sense do you do uh I'm thinking of things like Apache Pino right where they do a lot of uh precomputing of certain indexes to try and anticipate which analytics queries you'll run are you stretching into doing anything like that we don't we don't do that right now um apach Pino is an example of a tool that's pretty similar in use case to what it is that we're doing but it's um it's a much more mature tool there's there's a lot of optimizations that are done for us by the data Fusion query engine I mentioned that we don't need to but we don't do any precomputing of indexes or things like that today um there's still a few things that need to be done before we start to like really really over optimize on performance what's interesting about the work we do is people are kind of comparing it really against postgress right and against like other industry tools right like our our analytics engine is about 95 times faster than postgress and it's slightly faster than than Pino to today but it's like not significantly faster um I'm sure there's queries that Pino can optimize way better than than we do because we don't do anything specific there but it's typically like good enough right as a start for people and you know you install an extension run two three lines of code and suddenly your queries are 95 times faster without needing to move the data that gets people excited enough that yeah it buys us time to build the the the index precomputing features in the future yeah so it might be coming along in the pipeline yeah there's a lot of things that'll be coming along the pipeline optimization there's a couple of other things we do where um just yesterday we released the ability to query over um over Cloud object storage as well which I'm happy to talk about oh yeah so that's another interesting one where we kind of mess with the storage in a slightly different way like instead of messing with it on dis we actually connect to remote object storage in this sort of Federated way that's another big one oh is this like so I could store my database tables as par on S3 is that where we're going exactly exactly yeah imagine if we gave you like a postgress table you like UI right or like interface in a way over parkil in S3 yeah yeah and I get to do that all through presumably my very familiar postgress tools yeah you know psql Deaver whichever one you like yeah oh that's neat yeah so there's all this to say I'm thinking this in a bit of a different different direction than your core question but uh yes like index you optimization and things like that are things in the pipeline but we I would say a bit further down the line there's still some cool stuff you can do just at the feature set rather than the performance metric and um there's a big discussion happening in the database world nowadays um there was another article by the founders of duck of mother duck the duck DB um commercial side of things where they were saying like performance is a commodity and it's it's quite true like every database is fast basically nowadays if there all fast enough there's other ways to differentiate and really help build a better product for for users yeah yeah I I can I have some sympathy with that by the time you're talking about several orders of magnitude fast becomes a proper debate but for a lot of people convenience is still the far larger Factor right yeah exactly like if we did like I'm sure you can you can drastically speed up some queries with those optimizations but the queries are already pretty fast and so in a lot of cases you're kind of you know it's not bad it's pretty good but there's still like there's a lot of tuning you can do even before we do any uh pre-computing or things like that you can do materialized views on top of those um analytics like on custom store table storage engine like I was mentioning so that's like a typical way people will drastically speed up queries right yeah yeah yeah okay so by hacking into not even hacking into postest by extending postrest you can do things like changing the storage engine changing the query planner changing the indices or indexes never know which one people prefer um I'm trying to think what the limits would be at what point would you say actually I do need a different database and my guess is the fundamental limitation of postgress is that you're still single machine Master database right yes well yes there's a uh well yes I think it's it's an interesting question it's an interesting question there's a lot of people that believe you should be able to use postgress to do anything and certainly there's great work that has been done to make distributed postgress work very well right cidus although cidus I believe is still like single writer node um on their whole distributed distributed postgress I I got to confess I'm not fully familiar with like the latest and greatest when it comes to sharded postgress so um I may be speaking a bit out of my depth but there's a lot of people working on on it like neon is another one obviously Amazon has their own versions today our own custom storage is not compatible with this distributed postgress so if you use or work it is has to be single node you can do high availability right where you have like read replicas and things like that but it's fundamentally single nodee this won't be the case forever the truth is postgress scales vertically very well nowadays like machines can get huge right and it's quite crazy like I've talked to companies I won't say any names because there are companies that you know of and I don't know if I should but there's quite large companies that run enormous internet workloads that I've talked to and when I asked them how they run their PR they're like yeah we have this one big machine in this one you know AWS region and like it's good and I I I thought they would tell me you know like they have this really crazy setup and so on but um you can get quite far with it yeah so the answer to your question is maybe like obviously you know any postgress limitation is a limitation you get as well by using us but there's a big push in the industry to truly get rid of pretty much every postgress limitation and make postgress almost like it's kind of like the Linux kernel in some ways today like it's so it can be taken in so many directions to make so many versions of what a database should be and I see no reason why postgress couldn't solve those problems and Tru TR be like a database for everything in 5 10 years so you get to the point where you download postrest and then you choose what kind of thing it is by which extensions you install yeah right like you'll you'll pick your your Linux distribution based on your preferences or your use case right and there are so many postgress is the same like you want a database well maybe eventually databases get more and more synonym which is the word postgress right and you say okay today it's you know I need this one to be my postes like geospatial database right this need to be my analytics postgress version and you have all these flavors um there's still limitations but you need to start hacking on postgress to realize how extensible it is I had not appreciated that until we started and to anyone listening I would highly recommend you give it a try uh it's it's very eye openening there's a lot of things people think you need something custom for right like a custom data store or things like that but really well-designed abstraction layers and boundaries and like what you can do and can do um allows you to do a lot it's qu it's quite beautiful it's very elegant in some ways like over way to design a system I'm trying to think of um what databases would challenge at um I'm thinking about Kafka but that's actually really simple storage engine fundamentally it's just an appendonly log so that doesn't seem like much of a CH what if I said okay I'd like postgress to be like reddis a really really fast inmemory only key Value Store does that seem feasible I believe so I mean I believe so is I'm trying to think if there's anyone that has done like a reddish like system um there there's a project for doing queuing on postgress um like rabbit mq type workclothes but that's a bit different top of mine I mean maybe I'll I'll out myself there and and this is not a good idea I've thought about it for like five seconds but if if I were to do this I would do something like you can make a custom storage Engine with rocks DB or something like that right which is one of the more battle tested if not the most battle tested key value store and probably um you can Implement something that like can approximate this workload pretty quickly and postgress has in memory table right called virtual tables so you can do all these things purely in memory as well your index access method as well um I mentioned well everything that I've been talking has been with the Assumption of writing on dis right our full Tech search index are written on dis our custom storage engin are written on this but those can also be virtual tables they can be in memory indices um so you can like it doesn't have to be written written it's just the way the data is modeled so you can do things in memory I'm not familiar of a reddest competitive project on on postgress but I don't see why it would not be possible maybe my my suggestion is probably a little bit Limited in this C I need to think about it more but um I don't see why it wouldn't be possible maybe someone listening into this will go out and build a r reddis competitor maybe maybe you can go and and do it yeah maybe maybe um okay so I think there's one other really big issue that hits relational databases and is often something you give up for the sake of going to a different database which is um the transaction model concurrent readers and writers um uh acid these classic relational database Things That No SQL scalable databases often give up first yeah to what degree do you have to give up those kinds of guarantees in order to get uh performance that's a very good question uh this is something we spend a lot of time thinking about so let me give a very simple example uh in the case of our fex search Index right one question that we had was do we offer weak or strong consistency when indexing so you want your transactions to be as fast as possible right as soon as data is committed like it should be available to execute you know SQL queries again but um then you have a dilemma which is okay I have this search index on top of my of my table that does indexing for full teex search you go and you write a new row into your database do you want to wait until that row is also indexed in the full Tech search index before the transaction commits in this case you get purely strong asset consistency right but you get the trade-off that your transactions are slowed down because you need that indexing to happen or do I say hey like we'll give you weak consistency like we're going to commit the transaction right away but it might not be fully done indexing the um the on the full ex sear deex so it might take like a second or however long um to fully commit it depends what you want in our case the decision we've made in this specific example is to hold strong consistency because people come to postgress they want as you said they want asset properties and things like that if you didn't care about those things well there's a lot of tools out there that no SQL databases that give those up yeah right um people seem to care but it does mean transaction a little bit slower so if you were running like a pure transactional workload on on our with our extension installed versus without uh your transactions are ever so slightly slower it depends on the set of trade-offs that you're willing to make but you do have all this to say you do have to give to give up some of those um some of those guarantees if you want the pure Optimal Performance yeah yeah I think um I can see Arguments for both ways but I think you made the right decision if I were if I'm in postgress I expect things to behave in a postgress is way yeah that's that's always been our thinking in the first place like we want to make the the number one priority would for us is to remain purely postgress idiomatic as much as possible right we people come to our work because they love postgress and they want to do more postgress and we should do everything we can to make everything feel as postgress as possible to them right if they didn't care enough about postgress they want to stay in it then they might as well use one of the other tools that exist out there right and they'll probably offer like a good solution for them but we've still had heated debates we've had debates with with like early you know early users of our database as well on this at some point we considered making this uh like a setting that you could choose like do you want consistency or consistency for the time being we decided not to do that to keep things a little bit simple but who knows you know maybe down the line it makes sense but this is something also like post is so extensible you can basically make the user able to toggle fundamental database properties like asset guaranties which is crazy right when you think about it right like way you can design your index access methods or table access methods we could give the users the ability to select some settings and say like no I want strong consistency or we consistency um yeah that's that's that's pretty cool if you ask me the postgress of the future will have uh three switches for the cap theorem and if you try and put all three down one of them flicks back up yeah yeah well like exact that's the thing like you don't you don't want to give people also like you know as you said like rope to hang themselves with right like you you want to be careful what options like you give them because sometimes you know that's not necessarily a good thing but there's there's a couple of things you can do which is kind of cool yeah yeah okay you talked about early users so I think we should talk about taking this into production because it's one thing to play around in the internals of postest and sure make it work in theory but what happened in practice how well does it work how often did it crash that's a great question so for um so we our work is sort of is split into two main categories right the search and the analytics the search side is quite good today it's used in production by um by one Fortune 500 now today uh which is maybe one doesn't sound like a big number but our company is only about eight months old eight nine months old and so um it was quite a big deal for us when it happened and i' say a huge number yeah I we you know we were pretty scared I'll be honest the first time that we started testing that um but they U like we have done testing on on terabytes of data and billions of rows and it scales very well performance is very good um things are quite reliable so on the on the search side it's it's very it's it's very good it's not perfect but in terms of resiliency and reliability is very good uh we haven't had any major crashes the data you know doesn't get corrupted or anything like that some of it is thanks to the fact that index access methods as like I said they sort of restrict a little bit more what you can do which is put a blessing and a curse right yeah um when it comes to the analytics engine or analytics engine is not uh is not production ready yet there's still things that need to be done to make it fully postgress idiomatic it's used by companies um by mostly small and medium companies in Dev in Dev environments or let's say like you know what some companies called production when they don't have like really really you know strong uh uptime guarantees yet and they're just started starting um but so it's probably going to be a few months until like we fully Hammer everything out and I feel comfortable telling people it can go into production the main way people typically deploy us is to install extension on already running postgress databases and that's the that's kind of the pitch right like you shouldn't need another database so we ship a Docker a Docker image with postgress and our own extensions installed but overwhelmingly what we recommend people is say like hey Chris like you run you know the postgress cluster at company ABC you have a Dev you know cluster you have a prod cluster you can download the extension create it make sure it works well and then like your data doesn't need to go anywhere and that's kind of the preferred deployment mechanism or people will integrate it in you know whatever you know devop scripts and tooling they have for managing their cluster yeah yeah as as I've installed like geospatial stuff just WP the directory and tell post where to find it exactly exactly so that's the that's kind of the deployment process can I ask what are those last steps over the next few months which parts are you having to really hammer out good question so there's a few things the on the analytics side the big one is the buffer cache in postgress and um so so when we started doing our work we try to keep things as straightforward as possible and ironically that made them perhaps like less straightforward in that when you're making a a table storage engine there's a lot of pieces to it right and post has this buffer cach where basically when data gets written in it gets buffered before it gets written to dis or things like that and that's used in a very in a lot of ways uh in a lot of places in postgress it's used for like right ahead logging for example um so you can like roll back transactions EAS it's used for like deleting being able to delete data it's used to make sure that the ingestion speed is very good right because you don't want to flush to dis every single time that you write a row or you write a column that would be very very slow but initially we were just getting started and so we did do these sort of um um shortcuts where it's like hey like we're just going to flush every row to this right and it works but it means like ingesting data is very slow right so today if you use your analytics engine queries are super fast they're like as fast as click house or some of those existing state-of-the-art databases but if you compare the ingestion speed between us and click has it's quite a bit slower because we do that flushing um all the time and obviously that's not going to cut it for production Readiness right um we need to wire it up with right ahead logging right now right ahead logging is not yet implemented on our analytics engine if you're just testing it's fine but in production you need this to work with your higher availability and fail over uh toolings and things like that right and that requires rightand logging yeah so those are example of functionalities that aren't I would say like core to just seeing that the value of a faster like analytics engine postgress is possible but they are part of the functionality of saying like Okay like now that this is possible let's make sure it works in a production environment yeah and there's a couple of things that we do to make that as um like to to make that a bit faster uh s to to um to improve the speed at which we develop those features for our our customers that are waiting for them um when it comes to working with remote object storage for example we don't suffer from those limitations because we overlay the postgress um like table interface basically on data stored in S3 we don't store it inside postgress itself so it we don't have to rewire all of these internals so if you want to use our Analytics Engine with the data stored in S3 for example today it's already pretty production ready I I wouldn't go as far as to say that it is but it's like significantly closer than for data stored on disk because we separate storage and compute in that instance right those are examples of things that we have left to do okay very interesting is there anything on the horizon after that I mean is there are there a burning features or perhaps even a burning extension in your mind that You' like to see in post oh there are yeah there are you mean that that we will do or that I would like others to do both both sure so for things that we will do I mean there's the one that we just we just started um so our analytics engine is now being added the capability to do data Lake housing like data Lake type querying right okay so we yesterday we release the ability to query S3 there's a lot of other places people store data that we want to query we ship this as a separate extension so sort of a third one which is a it's like a a half C extension between a completely new one and like a an our analytics extension but it's the reason we ship it as a separate extension is so you have the ability to use it without any file system access if you would want for for security Reason by saying like this analytics like we have an analytics engine that's purely in postgress and we have this one that can be like in postgress and sort outside but you can make it be fully outside uh another extension we want to do down the line will relate to time series I'm not going to say too much um by now but it's going to relate to time series which is going to be exciting okay there's great work that exists in the space but also other things that you know can be done a little bit better perhaps or so we think that's one as for others there's cool stuff that other people are doing I think like the reddest example you mentioned is a cool one um people are working on queing system um that already exists a company called Tempo um they that's a pretty cool one I think there's a lot of things that need to happen as well in the progress extension world to facilitate migrations and I know there's some people that are working on extensions to do migration from that would be cool yeah like extensions that allow you to easily migrate data from other data stores like MySQL for example um there's a company working on on one for but for my squl there isn't something super super good that's another big one um that I'm hoping to see more more energy invested in yeah that's let me see those are the main one I say I have top of mine there's a couple oh I have another one I think um doing like time to L and and simple like database maintenance would be a big one as an extension as well we've had multiple users tell us they want to be able to set ttls for have literally row deleted after a certain period yeah yeah for for um like private and security and today I've talked to bunch of people that have reimplemented that themselves within their own organizations because there's no extension that does it um I think having TT ttls would be like it would be a cool extension if someone wants to build an extension with PGX right that is probably somewhat straightforward I think that would be a cool one that could be done and it would be valuable to people yeah I could see postgress with a custom table type of gdpr one day yeah exactly yeah exactly that would be cool yeah gee so the future of uh postgress is not only bright but it's uh expanding I think it is I think it and it's it's kind of wild when you think about it because a lot of people you know think postgress is like new in some ways like oh it's database everybody's talking about like it's got all these you know all these projects like our projects that are trying to make it do more and so on like what is this cool new database but it's like 30 something years old right it just had sort of a slow ramp up in some ways and yeah it sort of missed the hype train and the MySQL years yeah exactly and so even though it's even though it's an old project I think it's kind of at its beginning of it's like truly growth era right it's like becoming a teenager right now and like there's like a lot of things that are going to happen um the future is very much postgress Centric in my opinion well that that's both a bright not note for technology and a generally happy note that when you're over three decades old you might just be entering your teenage years EXA you're just starting you know it's it's it's just the beginning it's I'll go with that yeah Philip I will leave you to go back and uh finish off that extension thank you very much for joining me no thank you for having me it's been a great time I don't know I I don't know if I'll be able to finish it off today but we'll we'll put some more hours best do your best I wish you the best of luck with getting getting to production and getting to the second uh Fortune 500 company thank you we're working on it we're working on it good luck thank you Philip so if you're in a position where postest does nearly everything you want but not quite maybe it's time to get extending I've put links to all the projects that he mentioned into the show notes and those would be good launching points to start from while you are scrolling down towards the show notes if you've enjoyed this episode please take a moment to like it rate it share it with a friend and make sure you're subscribed because we'll be back next week with another voice from the developer Community but until then I've been your host Chris Jenkins this has been developer voices with Philipe Noel thanks for listening