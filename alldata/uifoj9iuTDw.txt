when you fire up chat GPT you're connecting to a big silicon brain that lives somewhere and contains something so what exactly is that thing what is the hardware that runs the chat bot that you've fallen in love with there's actually a sort of basic building block of the chat CPT infrastructure the Nvidia a100 GPU and if you thought the graphics card in your computer was expensive the a100 goes to around ten thousand dollars a pop roughly the same as six RTX 4090s artificial intelligence applications often utilize gpus because gpus are very good at doing lots of math calculations at once in parallel and nvidia's newer models have the tensor cores which are good at Matrix operations that AIS frequently use so even though the a100 is called a GPU it's built specifically for AI and analytical applications and as such you can't realistically game on it it doesn't even have a display out although you can get the a100 in a PCI Express version such as the one Linus holding up here it's more common in data centers for them to come in this form factor called sxm4 unlike a normal graphics card the sxm4 cards lie flat and connect to a large motherboard like PCB using a pair of sockets with the connectors sitting on the underside of the card although sxm is just a connector and data is still carried over a PCI Express interface sxm4 is preferred over the traditional pcie slot for data centers because the socket can handle more electrical power the pcie version of the a100 can use a Max of up to 300 watts but the sxm4 version handles up to 500 watts leading to higher performance an SX M4 a100 has 312 teraflops of fp16 processing power additionally these gpus are linked up with a high speed nv-link interconnect so that the gpus that sit on a single board can act like a single gigantic chungus GPU now that you know what lies at the heart of the GPU servers though exactly how many a100s are needed to keep the service running for 100 million users we'll tell you right after we thank circuit specialists for sponsoring this video circus Specialists provide tools and supplies to the stem community at competitive prices explore their wide range of offerings including resistors capacitors soldering stations oscilloscopes and more leveraging their technology and sourcing expertise their goal is to give customers access to tools and parts that may otherwise be unavailable or too expensive their commitment to Quality ensures that you receive reliable and High Performance Products suited for your needs so let circus Specialists help you upgrade your electronics toolkit by checking them out at the link below and using code lmg for 10 off it turns out you can run chat GPT just fine on its own on a single Nvidia hgx a100 unit these units typically contain Eight a100 gpus in One machine powered by a pair of server CPUs that each feature a few dozen cores but the issue is that with so many users you need a lot more processing power to ensure the chatbot can answer queries smoothly for everyone openai and Microsoft who are behind the chat you project haven't disclosed exact numbers about their Hardware but given the processing capacity of these hgx a100 systems chat GPT likely uses somewhere around 30 000 a100s to keep up with demand to put that into context it's a heck of a lot more than the roughly four or five thousand they likely needed to train the language model in the first place training is the process of feeding the AI lots of information in order to build it out before it can be used publicly intuitively it might seem like the training process would need more processing power than actually running the model but because of the massive amount of i o chat gbt has to handle with 100 million users it ends up actually requiring roughly six times more gpus to run it and with as pricey as these systems are you can bet this meant a massive investment on Microsoft's part while the actual dollar amount hasn't been disclosed we do know that it was in the hundreds of millions of dollars in addition to several hundred grand a day just to keep the system running and unless you think that Microsoft is gonna stop there the company is also integrating the newer Nvidia h100 gpus into its Azure Cloud AI service which actually dwarf the a100s fp16 performance by a factor of six in addition to adding fp8 support which should prove to be very useful for AI due to how the math calculations involved in running AI models work not only will this ensure that more people can use chat EBT and other AI services but will also allow Microsoft to train more complicated large language models maybe soon you'll be able to completely replace those pesky real-life friends of yours so thanks for watching guys if you like this video hit like hit subscribe and hit us up in the comment section with your suggestions for topics that we should cover in the future