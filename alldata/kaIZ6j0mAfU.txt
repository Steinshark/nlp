this episode is brought to you by brilliant in the 1960s the fear of nuclear annihilation brought on by an escalation of the cold war would lead the united states department of defense to aggressively pursue a communications network that could resist such an attack simultaneously the emerging field of computer science began to explore ideas for sharing computing resources in a resilient and efficient manner forming a network from this a solution to the defense department's dilemma would first be proposed by polish american engineer paul barron in his study of the survivability of networks in the event of nuclear war in which he proposed dividing information into message blocks independent of baron's work donald davies of the uk national physics laboratory began to implement his own experimental local area network based on blocks of information known as packets that were routed in a novel way known as packet switching davies network routed each packet of information independently unlike traditional telephony based switching technologies of the time that required establishing a dedicated routing circuit this resulted in superior bandwidth utilization and response time of the network and it required less resources to function by 1969 the advanced research project agency of the united states department of defense would award contracts for the development of the arpanet project an experimental wide area network that adopted packet switching based on the works of both davies and barrett the first arbanet routed network traffic through four nodes known as interface message processors or imps which were located at three research institutions across california and one in utah within a year the first east coast node would join the network bridging the coasts and by the end of the decade over 200 nodes would contribute to the network by the early 1980s the arpanet was growing at a rate of approximately one new node a month as the arbornet grew during the 1970s several other experimental packet switching networks were also being developed both in the united states and in europe these various networks while fundamentally similar were incompatible with each other in order to merge them an effort to create a unifying protocol was started in late 1974 resulting in the creation of the transmission control protocol or tcp specification in which the term internet was coined a few years later after further refinement and testing a layered approach was devised that split the initial specification into the internet protocol or ip and tcp forming the tcp ip specification the rapid adoption of tcp throughout the 1980s quickly unified the major networks of the world forming the foundation of the modern internet arpanet was initially created to facilitate communications among government agencies and research institutions in fact many of the internet protocols used today were initially created as tools to facilitate the sharing of research however using it for personal communications political purposes or commercial use was generally considered prohibited however by the mid 1980s strong commercial interest paved the way for policy changes that split arpanet into milnet a military-only network and a civilian arpanet that opened up the internet for use beyond research and education the civilian arpanet would eventually be migrated to a more modernized parallel network called nsfnet created by the national science foundation its goal was to promote advanced research through a combination of public funding and partnerships with private enterprise in 1989 the original arpanet would be decommissioned with it being taken fully offline in 1990 leaving nsfnet to serve as the backbone of the internet around this time the restrictions on the commercial use of nsfnet would be lifted and with it came the emergence of the commercial internet service provider industry this shift to commercialization became a catalyst for a massive influx of money technical advancement and the proliferation of access that transitioned the early internet from the military's technological marvel to the massive communications apparatus that infiltrates every aspect of our lives today one of the fundamental principles of both the modern internet and its progenitors that make it so resilient is the interchangeability of the physical medium by which packets are exchanged arpa-net for example initially used leased ds0 digital telephone lines to provide a dedicated 64 kilobit per second link between nodes while some nodes that reached across bodies of water were initially connected via satellite data links as the internet grew these dedicated backbone digital lines would expand in capacity with modern fiber optics achieving bandwidths of 100 gigabits per second per link however the cost and infrastructure requirements for these dedicated connections generally limited internet access to institutional use with the removal of commercial limitations in 1989 the sale of internet access as a service to the public began to rapidly grow by repurposing the extensive telephone network in order to link the internet to the masses this commercial initiative that evolved the internet into what it is today relied exclusively on a single device that moves digital data over a 19th century system designed for voice communication the dial-up modem the concept of using the telephone system to transmit data had far preceded the electronic computer in the 1840s teleprinters were first introduced as a mechanism for transmitting typed data over telegraph lines they worked by transmitting pulses that encoded each key in a binary sequence as the public telephone system expanded during the early 20th century the technology would migrate over eventually forming the telex network these systems transmitted encoded key data by opening and closing the dc circuit of the copper wire pair that formed a standard switched telephone connection early teleprinters mechanically encoded each character into five bits and could print transmitted data at speeds between 60 to 100 words per minute the quantifying of transmission capacity in words per minute was known as a baud rate it was named after emil bodo the creator of the bodel code one of the earliest character encodings for telegraphy the baud unit's definition would be revised and redefined formally in 1926 to represent the number of distinct symbol changes made to a transmission medium per second these symbols are not always binary in nature making it possible for multiple bits to be encoded within them because of this characteristic a transmission medium's bit rate can easily exceed its baud rate by the mid 1950s the emergence of the computer and the prospect of networking them together led to the creation of the first modems for use over telephone lines derived from the term modulator demodulator a modem converts digital data into a signal that is suitable for a transmission medium the first large scale use of the technology started to appear in 1958 as part of the sage air defense system created by bell labs they were used in terminal units that connected commands and control centers to various air bases and radar stations throughout the united states and canada a year later a commercial variant of the sage modem would be introduced to the public as the bell 101 dataset the bell 101 functioned electromechanically and was the first commercial communications equipment to introduce a new 7-bit character code known as ascii however its method of line transmission was still similar to that of a teleprinter and due to these limitations an operating rate of 10 characters per second was chosen with each character requiring 11 bits to transmit seven for data one for error checking in three for control the first modem speed standard of 110 baud was created in 1962 the underlying technology of the modem would split from that of teleprinters with the introduction of the bell 103 dataset by att unlike the 101 the 103 was a fully electronic design and could operate three times faster this also resulted in the elimination of one of the control bits that was previously needed for mechanical functionality allowing the 103 to operate at 30 characters per second or 300 baud because the bell 103 was now fully electronic a new modulation method was introduced that was based on audio frequency shift keying to encode data in frequency shift keying a specific transmitted frequency is used to denote a binary state of the transmission medium these states are known as mark and space and on the bell 103 both the origination station and the answering station had its own pair of tones allowing for simultaneous bidirectional communication or full duplex operation the frequencies chosen for the 103 transmission states were a direct result of the limitations of the public telephone system because telephone systems are designed for speech filtering in later digital processing at phone exchanges limited their band of operation to between 300 hertz and four kilohertz the minimum range needed to understand speech for this reason the two channels of the 103 are spaced evenly within the usable band for ideal signal separation the bell 103 would become a standard for modems well into the 1970s a variant of its modulation was even adopted by the international telecommunications union or itu as v.21 becoming the first international modem standard the bell 103 and v.21 variant modems would soon be incorporated into teleprinters eventually replacing the slower mechanical systems with a more modernized telux network even today the 103s modulation can still be found in shortwave radio use and some niche commercial applications where communication over noisy narrowband data links are required by the mid-1970s the baud rate of frequency shift keying modems would be pushed even higher with the introduction of 600 baud modems that could operate at 1200 baud when used in one directional communications or half duplex mode among them the bell 202 was a notable example one which eventually led to the similar itu-v.23 standard for audio frequency shift keying communications this modulation specification would also be used for the transmission of caller id information in most countries towards the end of the decade the pursuit of faster bit rates over telephone lines led to the development of a new generation of modems that utilize the modulation technique called phase shift keying in phase shift keying a constant frequency reference signal is used with the phase of the signal being modulated to represent discrete symbol states unlike frequency shift keying multiple phase states can be used effectively allowing for multiple bits to be encoded per baud unit while phase shift keying modulation was used on several competing modem products of the time it was popularized by the bell 212a dataset released in 1979 the 212 used four phase states each 90 degrees apart to encode two bits of data per baud unit known as quadrature phase shift keying or qpsk the scheme permitted full duplex transmission at 1200 bits per second at 600 baud the bell 212's modulation quickly became the standard for 1200 bits per second communication throughout the 1980s being favored by many of the early dial-up services and it would form the basis for the itu v.22 specification early on most modems adopted the rs-232 serial port standard for their connection to a computer this proved to be effective due to the popularity of the interface as well as its inherent compatibility with the serial data operation of modems most early modems had no direct telephone functionality and required a connection to be established before their use control of the modem during operation was generally done through additional control lines on the serial port eventually some basic functionality such as the ability to dial and answer calls would be added but the extent of these capabilities as well as how they were implemented varied drastically between manufacturers the solution to this problem would emerge in 1981 with the introduction of the smart modem by haze the haste smart modem was functionally unremarkable as a modem as it fundamentally emulated a 300 bit per second bell 103a however the smart modem introduced a command language which allowed the computer to make control requests that included telephony commands over the same interface used for the data connection this mechanism allowed the modem to switch between command mode and data mode by transmitting an escape sequence of three plus symbols while initially ignored by the market the smart modem empowered hobbyists with the ability to not only transmit and receive data but also perform telephony operations entirely from software this led to the creation of the bulletin board system an often small dial-in server that could be easily created for access by the public where messages and files were shared from this the haze smart modem quickly grew in popularity during the mid-1980s inherently making the command set used by it the haze command set the de facto standard of modem control it would soon be found integrated into the dial up modems of almost every manufacturer with many even adding additional features even as modems transitioned to smaller internal bus expansion cards in the late 1980s and 1990s the fundamental principles of this interface would still be retained as the early 1980s progressed manufacturers started to push their modem speeds past 1200 bits per second however they would quickly hit the limitations of phase shift keying over telephone lines while 2400 bits per second operation at higher baud rates was possible it was limited to half duplex use furthermore adding more phase states to a baud unit also proved to be problematic as the ability for equipment to distinguish between the smaller differences in additional phase states would diminish in 1984 a new form of modulation called quadrature amplitude modulation would be introduced to the market quadrature amplitude modulation is an extension of phase shift keying that adds additional symbol encoding density per baud unit by overlapping amplitude levels with phase states the first modem standard to implement quadrature amplitude modulation was the itu v.22 bis vita 22 bis employed a variation of the modulation known as 16 quam to encode 16 different symbols or 4 bits of data within each body unit using a combination of 3 amplitude levels and 12 phases this allowed for a full duplex operation at 2400 bits per second using only a 600 baud rate by the late 1980s quadrature amplitude modulation would be extended to 9 600 bits per second operation by utilizing a four symbol qua modulation scheme at a 2400 baudery however it was becoming apparent to manufacturers that a bit rate limit was on the horizon many believed that 14.4 kilobits per second would be the maximum practical bit rate possible over telephone lines with current infrastructure an ultimate theoretical limit of 35 kilobits per second would even be predicted by shannon's theorem a noisy channel coding theorem stemming from information theory in 1976 austrian communications engineer gottfried underbook would introduce a new form of modulation capable of encoding and transmitting information over low bandwidth channels with high efficiency called trellis code modulation his concept based on his work at ibm during the 1970s remained mostly ignored until it was reintroduced in 1982 where it was presented with a more focused scope of application trellis code modulation differs dramatically from previous modulation techniques in that it does not transmit data directly it works by first partitioning all possible symbol states into a tree that divides each successive branch by their informational contrast a state machine based algorithm is then used to encode data into a stream of possible transitions between branches of the partition set this transition data is used to recreate all possible branch transitions in a topology that is similar to a trellis from this using a predetermined rule for path selection the most likely branch transition path is chosen and used to recreate the transmitted data while trellis code modulation by nature requires more bits to transmit a symbol than previous modulation techniques its level of resilience to errors was unprecedented it was capable of reducing error rates almost a thousand-fold when compared to simpler techniques this allowed baud rates to be pushed higher easily overcoming the requirements of more bits for simple encoding leading to an increase in the overall effective data rate trellis code modulation was a breakthrough for the telecommunications industry and by 1984 the international telecommunications union would release the v.32 standard the first trellis-based modulation specification designed for full duplex 9 600 bit per second operation at 2400 baud in the early 1990s the final restrictions on commercial traffic on the early internet were finally lifted and the dial-up internet service provider industry began to explode by the end of 1991. over the next several years the proliferation of inexpensive but powerful digital signal processor chips would create speed wars between competing manufacturers leading to further advances in trell's code modulation and a dramatic increase of data rates by 1994 baud rates would be increased to 3429 symbols per second with up to 10 bits per symbol encoding now becoming possible this would peak with the itu-v.34 standard which permitted full duplex operation at 33.6 kilobits per second a rate near the theoretical predicted limit by shannon's theorem the dramatic boosts in data rates created by trellis code modulation directly changed the look and feel of the growing internet the internet quickly shifted from a mostly text-based experience to one that now incorporates images audio and video it became more visual and mass appealing commerce quickly began to dominate the landscape during the second half of the 1990s creating the dot-com bubble and paving the way for modern tech giants in early 1997 the modem would get one last boost in bitrate with the introduction of the first 56k dial up modems pushing speeds above 33.6 kilobits per second proved to be extraordinarily challenging as the process that digitizes telephone audio signals for routing by telecommunications infrastructure made it very difficult for denser data transmissions to survive the digitizing process this difficulty led motor manufacturers to abandon pushing analog end bit rate speeds higher however it was soon realized that internet service providers could obtain direct digital connections to telecommunications infrastructure and utilize this more granular signal control to send a more densely encoded data stream that would survive the conversion back to analog at the subscriber end this technique allowed for an asymmetric 56 kilobit per second downstream rate from the internet service provider to the subscriber while still using a 33.6 kilobit per second upstream data rate initially there were two competing standards for 56k technology us robotics x2 modem and the k56 flex modem developed by lucent technologies and motorola both competing products began to permeate the market at the beginning of 1997 and by october nearly 50 percent of all isps within the united states supported some form of 56k technology however because of the need for matching internet service provider infrastructure the conflicting standards were becoming a hindrance to 56khz adoption and consumer sales with no clear superiority in either implementation the international telecommunications union announced a draft for a standard for 56k modems in february of 1998. called itu-v.90 it merged the two competing standards into an entirely new standard that would receive strong industry support additionally v.90 was purposely designed so that existing non-v.956k modems could be easily converted to the standard via a firmware or a driver update by september v.90 would be formally adopted quickly becoming the sole 56k standard of internet service providers and consumer modems alike by the turn of the century broadband internet access such as through dsl and cable services started to grow both in availability and popularity while moto manufacturers attempted to push upstream data rates to 48 kilobits per second with a more enhanced encoding scheme in november 2000 with the v.92 standard it was met with very little enthusiasm as internet service providers were now focusing on transitioning to broadband infrastructure and not upgrading their dial-up services while dial-up services remained a viable option for a few more years especially in regions with no broadband access the evolution of the internet into a more media-rich platform would quickly extinguish the technology though many of the techniques developed for its use would migrate over to broadband modem technology and the then new emerging connectivity technology known as wi-fi it's estimated that in 2020 the entire tech industry constitutes almost seven percent of the entire global economy most of which rely on the existence of a global network unlike anything ever seen before in human history to function this global network that today empowers the 21st century was first conceived in the early 20th century and quickly flourished into what it is today using just a set of peculiar sounds transmitted on a technology from the 19th century in producing this video what i found most fascinating was that in order for modems to improve in capability they had to become computers themselves and rely on algorithms to implement their complex modulation schemes algorithms have completely changed how even the most trivial aspects of our lives operate and if you've always wanted to learn how they work but were put off by coding languages brilliant is the perfect solution brilliant is my go-to tool for diving head first into learning a new concept it's a website and app built off the principle of active problem solving because to truly learn something it takes more than just watching it to truly understand you have to experience it with this in mind brilliant has recently upped their interactivity on their platform to a new level and they continue improving their courses to add more interactivity to them for example brilliant can help you learn how to program without having to deal with the drudgery of coding syntax through these fun interactive challenges you can simply shift around the blocks of pseudo code and get immediate feedback on your results it's an excellent way to understand how algorithms work and then once you have that down coding syntax becomes far less intimidating with brilliant you learn in depth and at your own pace it's not about memorizing or regurgitating facts you simply pick a course you're interested in and get started if you feel stuck or made a mistake an explanation is always available to help you through the learning process if you'd like to try out brilliant for free and get 20 off a year of stem learning click the link in description below or visit brilliant.org forward slash new mind you