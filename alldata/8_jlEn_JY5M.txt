uh hello everyone welcome to the performance optimization techniques for mobile devices and this is an overview of the techniques and you will see there is a lot of things to learn for people many people will know some stuff many people will learn some new stuff so let's move on to it so about me i'm an ibiza from serbia i'm an application performance specialist and i focus my professional focus is how to make programs run faster by using better by utilizing better algorithms better using the underlying hardware using better programming language standard library or the operating system i work as a performance consultant so when people have problems with performance that i have them out to fix them and i train teams when it comes to performance now why we why why why do we focus on the performance and battery power devices mobile devices so battery powered devices mobile devices are now ubiquitous you can find them everywhere uh mobile phone uh mobile phones but also in terms of things and one of the requirements that is really important that there is to conserve energy now uh performance and energy preservation are somewhat related so you will see how and the topics of today there are five all together five topics that we will cover uh today uh one of the the first one is how is performance connected how are battery life and performance connected how they work how they appear together next thing we'll talk about software design principles for battery power devices some things are bit differently when you're running on battery next topic will be uh performance optimization techniques what to do and what not to do if you want fast software and if you want energy efficient software then we'll give some ideas for optimizing computational intensive codes and optimizing memory intensive codes now don't worry if you don't understand what computational intensive codes and memory intensive codes are we we're gonna talk about it very soon so before diving into the the battery life and performance let's give a very some simplified model of how cpu on a battery powered device so a cpu nowadays is mostly consists of several cpu cores for example my phone has eight cores uh uh one cpu core can either be running or sleeping when it's running it consumes full power when it's sleeping it consumes very little power so this is a very simplified module a model that is sufficient for us the cpus can work uh the cpus can lower or increase the the frequency and with decreasing or increasing frequency you will also increase or decrease in energy consumption but with this simplified model it allows us to write efficient software it's good enough for us so how does that work the cpu core wakes up it does some useful work and then it goes back to sleep so and why is performance important performance is important to maintain battery life if your software is fast the cpu could spend less time in running state and more time in sleeping state and this lowers the memory the energy consumption okay now the first topic is the software design principle for energy efficiency so these are the high level uh high level uh considerations when you're designing your software to make it energy efficient so uh all all your data processing should happen at once so you how the component should be designed it does some useful work then it goes to sleep if you have threads they go to sleep for some time you don't want to wake up your component too often because if you you want to this the longer it sleeps the more less memory it consumes so instead of one milliseconds you want to increase the interval to 100 milliseconds or one second next thing in your api designs you want to avoid polling so what is polling polling means asking other components if the state has changed so you often in embedded system you will see that there is a check if the state if the components have finished something to check if there is something arriving on the input buffer and so on so instead of doing it like this register for events register for callbacks to save power uh next thing you want to do is you want to avoid chatting with other local or remote components so what chatty means chatting means chatting is with the with regards to software designs many components that talk a lot they exchange a lot of information if your if two components exchange a lot of information that means that uh that means that the the the components are chatty when this happens is um when this happens this this has a tendency to increase energy consumption and the last the most important part of of how to how to preserve energy is to strive for high performance processing and this is what we're going to talk about today now um just for the records this is just for the record this is not only specific to embedded systems but also specific to desktop systems even more is that the cpu is almost never used completely hundred percent uh but the cpu's resources due to various bottlenecks and if you know where the bottlenecks are and you can you can help it increase performance this is important for embedded systems because sometimes uh you might be considering to buy another chip because the current the one which you have right now is not fast enough but maybe you should ask yourself another ask yourself another question and is my software performant enough okay so what is the way of the performance on mobile devices or generally on battery powered devices or all devices that are limited somehow limited in energy so in desktop settings especially in high performance computing settings they have like this huge huge uh uh servers with many cpu cores and then they use the the the brute force uh brute force um approach to distribute the workload to several cpus so if you have this like you have some hot loop which is takes like five seconds if you distribute it to four cores if you're lucky enough you can get four times speed up now this doesn't work very well on multi multi-training is not always a good idea on mobile devices because we teach each core consumes energy and this code actually consumes consumes more energy than the single threaded code now uh multithreading multiple in general increases energy consumption it is difficult to do it four cores are not equally fast and on cpus we often see designs but there is there is this imbalance that some cores are faster than the others next things mobile devices often have limited memory bandwidth so the cpu and memory they're connected with the bus but it has a limited limited bandwidth and what happened is that even you all cpu cores are busy but most cpu cores are waiting for the data to be fetched from the memory and this actually increases memory consumption without doing any useful work in my experience a well-written single threaded algorithm often cannot perform an evenly written multi-threaded algorithm but bear in mind this is this is one of the one assumption is the multitraining actually increases energy consumption but if it's done good it doesn't necessarily mean that your your program will consume more more more energy uh if if a loop takes 10 seconds to execute on a single core and 2.5 seconds to x it could execute on four cores then it's it's worth considering using multi-threading for this distributing distributing it to four cores uh wouldn't bring any additional increases in energy consumption so a good indication for multi-threading on mobile devices is when the speed up is linear to the core count if you have four four four cores that means your software should be running four times faster okay this is end of the first topics any topic any questions yes there just came as a question in um as multi-threading consumes more energy is the reason why apple's silicon has extremely high single threat performance but only on average more decided performance so i i'm sorry i just i'm not i don't know how apple designed there i'm one single silicon so i don't know i i don't know the answer to that question i think it's an interesting question but that's outside of the scope of this talk but just to very uh just there is let me just give you a general explanation which is valid for all architectures you have cpu cores and you have cpu speed then you have memory memory and your memory bandwidth the amount of mem data you can fetch from from the memory now if you have a really fast course but the memory bandwidth is not large enough in that case you will see less benefit from mult threading and vice versa if you have slow cores but you have a lot of memory bandwidth then you can you can you can achieve achieve like linear speedups up to 48 or 256 scores that run soft electrons 256 times faster to be honest i was surprised for example i have raspberry pi the board with arm cortex a72 but the same chips is also the same core is also used in a in a high performance cluster the same course so they're i think they're the same silicon what is different is that my my raspberry pi has a really low bandwidth over memory and the software that multi-term doesn't scale very well on that high performance system it scales almost perfectly okay okay uh next we talk about better using the hub resources so if you're aiming for peak performance and that this is what you want to do in case that you are you're aiming for reduction in energy consumption you want to you want you want to write your program so they use other resources optimally now from the viewpoint of other resources there are two major bottlenecks the first bottleneck is if your program is limited by the amount of calculation needs it needs to do on the cpu so cpu has some resources dividers adders uh vector units and so on and if the bottleneck is in the cpu itself we call this code computational intensive or we say that the code is core bound alternatively if the problem is not in the cpu code but the problem is in the memory bandwidth the amount of data you can get from the memory we call these codes memory intensive and we say that the code is memory bound so why do we introduce these two new terms because the optimization strategies for computational intensive are completely different from optimization technique strategies for memory intensive codes okay now the question is is my code computationally intensive or is it memory intensive does it use does it is it does is it's bottlenecking the cpu or its bottleneck is in the memory now there are tools that you can use like windows vtune profiler and the microprof also has some map pmu tools perf or liquid to read the hardware performance counters now these hardware performance counters can tell you if this code is waiting more waiting for memory or it is more waiting doing some processing internally now if you use these tools you will see that memory intensive codes will typically have bad numbers in memory bound section so there is this data cache miss section and it will be bad you will see those numbers bad numbers there now computational intensive codes will typically have bad numbers in the core bounce section that means that there will be high use of vectorization units highers of dividers large amount of branch misprediction and so on now these tools are not so simple to use they have a steep learning curve because you need to know the hardware architecture and so on and it takes some time to master it but luckily there is like a way to determine if your code is computational intensive memory intensive by just looking at the code okay questions until this point yes so right now there i don't see any questions and i think we can carry on okay uh so we have a talk now about uh how to optimize the computational intensive code so um let's the question is if my is my code computational intensive or is it memorizing now if you have a loop that is processing simple data types like integer charge doubles not classes simple data types or maybe smaller classes but not large classes if date and the data is stored in an array or a vector so vectors and arrays are basically the same and you're accessing neighboring elements of the ray sequentially so you're going inside the array you go from zero to n or from n to zero so uh then your code will be computationally intensive it will have it will most likely have a bottleneck inside the inside in the cpu itself now if any of this is not true then your uh code will be memory intensive uh this is important to note not many codes are this there aren't many codes like this so codes that are processing simple data types and data stored in an array and you're accessing it from sequentially but not skipping any elements not doing any random accesses there aren't many codes these codes happen a lot in like uh scientific computing uh image audio and video processing machine learning especially matrix multiplication telecommunications and so on but in the general object oriented programming world they don't happen often now if this is a value of this is not true if you're processing large data type so that is stored in a hash map or a tree or if if you're already storing an array but you're not exiting sequentially but with a stride for example element 0 element 8 element 16 and so on your code will most likely be memory intensive now the question is so this is part of the computational intensity topic is fixing memory intensive codes what does that mean on some occasion there are certain transformation that you can do on your code on your loops to convert memory intensity cos to computational intensive and it's much better to be computational intensive because the memory is much much slower than the cpu cpu is really fast but far too often it has to wait for the data from the main memory to arrive now the first way of doing this so fixing fixing memory intensive code is called structure of arrays so what is it let me give you an example so we have here a class student which has a string which is a name and a double which is a nevermind average mark now uh on there is a vector of student classes and we the the question is how much time do we need to calculate average mark for for all students so this is the the first way of representing it class with name and average mark the second way of representing is that we have class students please notice the plural here and then we have one vector for names and the other vector for average marks now these codes this is object oriented code it's called array of structures so and this is not object oriented because each member of the class has its own vector and it's called structure of arrays and this uh transformation from array of structures to structure of rays actually increases performance and if you want to calculate the average mark for false students original code takes nine 194 milliseconds and after this transformation it takes 42 milligrams now question is why because in this second case we are only accessing this vector and this is a vector of simple types and we are accessing it from zero to the end so this means that we are this code is computational intensive in this case we are accessing classes and as we said classes are not primitive type and these these this code is is memory memory intensive okay this was one way of fixing memory intensive code if you're using structs or classes in your code now the second thing or the second way of fixing memory intensive code is is if your program is processing data in a matrix so you're doing a matrix processing so this happens with linear algebra all it happens with image processing and if you're running through the matrix column wise so if you're processing matrix 0 wise going from left to right your program is and you're using primitive data types this is computationally intensive but if you're going column wise so you're going column wise this code is memory intensive or memory bound now can it be fixed yes it can so here is on the left is an example of memory multiplication program loop so we have three loops now if you have if you have a look at this let's look at the the the side of the loop so there are three arrays c a and b now this innermost loops iterates over k uh with regards to that access to c i j is constant because the k doesn't uh it doesn't involve k access to this element is also constant inside loop we're also always x in the same element x is to this array this array is sequential we're x in neighboring elements okay because the k is in the right in the right brackets but the axis to this element this array is or this matrix is column wise because we are indexing columns not rows so we are moving column wise so we will call row one row two row three or four and but the the the column remains fixed now we don't like that this axis makes this code memory intensive luckily there is this this transformation called loop interchange where we exchange the loop we move the in out in the loop to the outside and now to loop to the inside now if we analyze this code what happens here this is a sequential axis this is just like accessing a vector from left to right sequentially this is also sequential axis this is a constant axis j j the the variable j changes and this is this this axis remain constant i k remains con doesn't depend on j and this one is also sequential axis now we have three sequential axises and one constant axis with this loop interchange earlier we had two constant axises one sequential and one strided axis and this could be much much much faster so with this transformation the program is accessing memory location sequentially and we see that the matrix size 1200 times 1200 the first loops takes 5.8 seconds and the second takes 1.22 seconds to execute okay any questions until this point [Music] i don't see right now any questions in the chat to this okay moving on yes so the previous two the techniques uh the loop interchange and the structure of arrays are used to are used to to fix memory computation memory intensive codes now i'm going to introduce vectorization so what is vectorization vectorization is really important if you aim for peak performance so many modern cpus can process more than one data in a single instruction so they have these vector instructions this model is called cindy single instruction multiple data now the good thing about seeing these compilers can automate automatic these instructions are faster than their scale version so they're vector instructions and scalar instructions these are instructions are generally faster the code which uses them works faster and the compilers can automatically generate code that uses cmd instructions if some conditions are met so here's a simple loop on the left which is a of i equals b of i plus coi when the compiler auto vectorizes this loop what it happens so we see that the loop doesn't do i plus plus it does i plus four so it means it's processing four elements in each iteration of the loop what it does it loads four elements from the uh from the array b then it loads four elements from the ray c into into vector registers so these these are vector register then it adds together b and c two vector registers and and add them together to this uh to this uh vector register a and then when this these results is calculated it stores it back to the memory using this store form now all the time it's working before every each iteration works with four elements simultaneously okay so using vector instruction is much much faster so the vector is called runs typical two to two to six times faster than its non-vector as counterpart and the compilers can actually auto vectorize loops when certain conditions are met if the compiler has managed to auto automatically vectorize part of the code that is a good indication that your code is running at maximum possible speed so even if you disable vectorization or your cpu does not support vectorization the fact that it was auto vectorized earlier means that the code is well written now the question is you want to ask yourself did the compiler automatically vector is my code and you can check that you can specify compiler flags for clang this is the compile flight for clang and for gcc this is the compiler flag for gcc and it will tell you if your loop if you loop if you look it up you will see that if your loop was vectorized and if not then it will tell you the reason okay now the compilers as i said can vectorize certain certain vectorize uh hot loops but the question is what are the conditions for the compiler to do this so in order for the compiler to vectorize uh your code you need first you need a good memory access pattern so you need to go you need to use vectors or arrays and need to go from zero to one you need to access sequentially you erase sequentially so loops which act as arrays but not sequentially don't vector as well well they they don't get any performance benefit from vectorization also the loops that taxes memory in the random access fashion like hash maps binary trees linked lists on they don't vectorize that next thing is you want your loops to have independent iterations so what is an independent iteration then peace that means that the current data you need in the current iteration does not does not depend on the previous iteration if it does we say the loop has data dependency and these loops are impossible to vectorize so sometimes there are loops that are impossible to vectorize next next things you want to make sure is that the number of iteration is known before the loop starts so what does this mean in practice that means that for loops within where the start and end indexes are known and compiled uh i know that before the loop before the cpu enters the loop are good candidates for auto vectorization if you're you have a loop and you break outside of the loop or you're skipping iterations with continued those loops factorize don't vector as well or don't vectorize it all next thing is that you don't have a lot of conditionals inside the loop so why is this important if you imagine your for loop and you see a lot of conditionals when you have condition the nature of vectorization is that your program is performing the same kind of operation on all its data so if you are performing on on all data the same operation vectorization makes sense but with conditionals that means that different operations can be done depending on the data value for example you can use additional positive elements of the vector and subtraction of negative elements of the vector these loops don't vector as well because you cannot generate that the compiler cannot generate factory instruction for them and the last thing is that there is no pointer aliasing now the the the topic of point releasing is huge but what does in in essence mean is that the pointers so the the blocks of memory accesses access through the pointers are independent on one another if they overlap then those loops cannot vector as well okay now any questions about auto vectorization yes we do have two questions yes the first one is how do you identify code sections that need to be optimized for power consumption how to identify optimize uh sections that need to be for power consumption like news profilers those those call sections will appear in the profile with the the longest runtime profilers are tools that that programmers use to see which which code in which which code takes the most time there in the program peter wolf asks regarding techniques like the loop interchange to auto vectorization are there analysis tools which would help to detect classes or cases where the developer could optimize the loop so as far as i know there is one tool which is developed by a penta it's called parallel analyzer and it can detect places where you can use loop interchange clang has like a built-in support to detect places with loop interchange and do loop interchange automatically for you but that code is experimental and in my testing it didn't work so there were examples where you can see clearly there is a loop interchange but it didn't detect anything so if you if you're if you have analyzing for rule properties the main question you should if you want to know if your if your code can profit from a loop integer the main question you should ask yourself is am i accessing this element every time the inner loop increases by one is this the index also increases by one so in this case it doesn't increase at all doesn't increase and all increases by one increases by number of columns for example so this is the the the if you're doing manually appenders you know a pen tool can also do that and but i don't know any other tools that can do that there is one more question about indexes when you are saying access area sequentially do you mean iterate by in the index by index so means that if you're accessing array it means that in if you have a for loop you have the increment is i plus plus and then you you're using this psi so if it's i plus equals two r i plus equals eight that's not sequential that you have a stride which is bigger than one so a sequential means basically going from zero to n or from n to zero that's also fine i minus minus also x but you you should not skip elements if you have array if you skip skipping some elements then you're not taxing it sequentially okay that's on this segment and okay now what are the fixes for actualization problems if your critical loop doesn't vectorize because there are too many conditionals or the loop is too complex there are techniques like loop fission blue fishing means splitting loops into vectorizable and non-vectorizable parts and you can use this uh tool if inside the loop you have conditions but they do not depend on the data if the conditions depend are constant inside the loop for example if if you have a loop and there is if debug prints something and this debug is constant doesn't change inside of the loop then you can move it out the compiler does that for you also it's called loop and switching also if you have uh conditions that depend on the iterator variables then you can do like loop peeling you can peel a few iterations for door loop and rolling there are also some some some ways to get rid of conditionals but the essential conditions where the processing depends on the data you cannot get rid of them so if you're leaving i'm giving you the fixes for vectorization problems but they're not going into details because it will take too much time and this is just an overview but in case you come across a situation you want to optimize your loop which is which is hot takes a lot of time it's this is a good place to start now if you're leaving the loop earlier if you have a search loop so with the search loop you don't know the number of iterations in advance when you find something you break the loop so this loop is uncountable there is a technique called loop section that will allow you to vectorize the loop if you have bad memory access pattern then you're out of luck if you can apply a loop interchange or or moving to a structure of arrays that can help that can help if not then your code will be memory bound and the last thing is pointer aliasing now sometimes the compilers are not sure if there is if there is pointer aliasing they cannot do this analysis they cannot make sure that the blocks or memory are independent of each other now you can use this restrict keywork keyword to mark the pointer as independent from one another and that unlocks the the vectorization now what you actually will do you will take a look at the compiler optimization report then you'll take to the google look it up and see what does that mean and how you can fix it but these are generally recommendations okay questions um looks like we have no questions this time okay i guess we carry on okay next thing is optimizing memory intensive codes so memory intensive codes uh we talked about computational intensive code now memory intensive codes what's that so cpus are much faster than the memory and what happens is and this happens a lot is the cpus have to wait for the data to be fetched from the memory so the cpu design designers did the following they added a small fast memory called the data cache memory and this memory is inside the cpu and it is used to speed up access to the commonly used data so if you're accessing a piece of data which is already in the data cache in the cpu this access will be fast but if you're accessing a piece of data which is not in the data cache that means it has to be fetched from the main memory and this axis will be slow so many programs especially object oriented programs with large classes use memory inefficiently so they have bottleneck and they're memory intensive if you are aware of how the memories work and how data cache works then you can use this knowledge to make more to create more efficient access okay so now question is when do the data cache misses deeply happen so data cache misses are connected to lower performance data cache hits are connected to higher performance so there are certain situations where data cache misses are more likely to happen so what are these situations so inside your c plus plus or c or c plus plus code if you're the referencing a pointer for the first time so it's a very common operation using operator arrow so this is the operator arrow so if you have a classes we have pointers to other classes every time you reference a pointer you'll get a data cache miss if you're accessing heap allocated objects so for the first time so you're accessing something allocated on the heap if you have a vector of pointers which are used for polymorphism like row pointers shared pointers you need pointers next if you have look up in the std map as the unloading map as the delete sd set and so on all three based link based and hash map based containers you will get uh you'll get data cache misses because lookup in these containers actually referencing uh the referencing using the arrow operator lastly iterating through std list not only luca but iterating through std list as the d map porosity set will create will create data cache data cache misses iterating through hash sets doesn't create data cache misses can be make made do not create cache misses the hashmaps store their internal user race to store data when do also data cache misses happen if you're accessing a member of array or another data structure in a random fashion so here is an example of loop we are accessing this right histogram but we are not using i to reference it we use a of phi so histograms are often seen in in image processing and scientific computing also if you're looking doing the binary search so binary search is done in a on a array and but you're not texting right uh sequentially you're asking randomly you go in the middle then you decide to go left to right so this will also create data cache misses uh lookup in a hash map for binary will also create uh data caches now this creates data cache misses the question is when do data cache hits typically happen so there's certain situations where data cache hits are more likely to happen so if you're iterating through a ray or a vector sequentially we already talked about it the smaller the type the higher the chance of cache hits so if the class size is smaller this increases data cache hit rate if the class size is larger this decreases data cache hit rate and means lower performance so rule number one of peak performance is that peak performance is achievable only when working with arrays and simple types or small classes when weather data cache also more likely to hit so if you should if you keep your data set small it has small hash set or a small vector or a small binary tree uh this increases data cache uh hit rate so you look up in a small tree or hash map is much faster than look up another large data structure what also increases data cache heat rate so what is accessed together should be close neighbors in memory so if you have a class and you have two members inside the class that you often access together you should declare them in the class declaration one after another this increases data cache thread what else what is x is one after another temporarily so in in time if you access this element and then you access the next element it should be laid out in memory one after another so nodes of a linked list that are accessed one after another should also be one after another in memory als this applies also for the binary trees nodes of the binary that are accessed one after another should be also one after another in memory okay so this is about data situation which increase data cache hit rates or decrease data caches hit rates now how do these recommendations translate to everyday development so these were our generalized recommendations now let's see a few examples and measure the performance difference but with performance you should all have all always have in mind to always measure sometimes you assume something increases performance but when you measure it doesn't so first example is minimum and maximum in array so we have a minimum and maximum in a vector which has a vector of class i six in bytes and there is 100 million million elements in the vector so if we are finding minimum and maximum in the same loop it takes 207 milliseconds if we are finding first minimum in one loop and then max in separate loop it takes almost two times slower so the question is why such diff difference uh iterating over the same data set two times waste is memory bandwidth so what you should do is when you're iterating over the data set you should always strive to do as much work as possible for example stdl offers std and mid max to find both minimum and maximum at the same time it iterates over the the the the vector only once okay this was the first example so we see like double increase in speed just because we are iterating over the container one sister of tire twice next example is linked list memory layout so we have this linked list which has six nodes six values but we have like this is how it is laid out in memory so we have first it's called a random memory layout so the nodes are just randomly just put in in memory they're just random there is no pattern here the second is compact memory layout so they're compactly packed in memory they take the there is no wasted space between nodes but they're not neighboring so this is the first then we go to the second then we go to the third when we go to the fourth then we go to the fifth then we go to the sixth so they're not neighbors in so the neighboring logically neighboring nodes of the linked list are not neighbors in memory and then we have this called perfect memory layout with perfect memory layout linked lists that are logically neighbors that go after one another are also logically uh physically neighbors in in memory now which led is best if you want to iterate over the list that's the question and the answer is depends so we measured for three sizes of of linked lists so small sizes had 128 doubles medium has 1 million and large has 64 millions and then we use three layouts random layout compact layout and perfect layout now first the small small increase with only 128 doubles the layout is not important the numbers are almost the same but for medium and for large there is a huge difference now the random layout for the medium is 8.2 and compact is 6 seconds and the perfect layout is 0.12 seconds it's almost 50 times faster for a large linked list the random layout takes 15 seconds and the compact 9.7 seconds and the perfect layout doesn't change it doesn't change doesn't depend on the the the linked list size it it remains constant so for a small link list the layout is not important but for a large linked list a random memory layout is 125 times slower than the perfect memory layout now the question is okay this is all nice to know but can we control the memory layout of the linked list and actually you can if you use a custom allocator for your linked list then it's possible to control memory layout and if you're if you put enough effort you can get really achieve perfect layout and and and achieve perfect performance you can find more info on my site in this in this post so the question is why is perfect memory layout the fastest but there are several things to it the cpu has a component called data prefetcher that's part of the data cache the data prefetcher can figure out figure out the memory access pattern of the program and then it can preload the data from the main memory to the data cache before the program even needs it so if the the cpu can predict the memory access pattern it can preload the data for vectors and for arrays it is trivial to figure out the memory access pattern if you're going sequentially or x eraser vector it's really simple and the cpus utilize this so when we are using perfect memory layout actually the the the the pattern that the way the cpu access memory is predictable for the cpu and cpu can preload the data and then you see perfect speed okay questions until this point yes so um i think like your comparison between lists and vectors good but what you should have even was a perfect layout for list and in mind that there is like an overhead in the memory for list because yes yes yes of course i i mean performance is always uh at odds with some other things like maintainability simplicity and so on and yes yes it's a perfect layout for lists the same as vector how are the performance numbers for student vector instead of stood list so the problem with linked list if you use like rolling place from the standard library is that you can you don't have any guarantees of the memory layout it can be perfect but it doesn't have to be and normally you you will see what you will see often is that if you have a small program linked list is almost as fast as array but you have a large program which uses allocator a lot so you have other components that are allocating the allocated memory then you don't get the perfect memory layout you get the bad memory layout and what happens that is you get the performance drop significantly now you can fix this by using a custom memory allocator for this hot data structure that you want to keep perfect uh but with the stl located with the standard library allocator there is no there are no guarantees you might took two consecutive calls who's a malloc might return neighboring neighboring memory addresses but there is no guarantee okay so we are slightly over time i can't give you like three to five minutes more so okay let's see what to skip let's see like this so let's compare vector okay let's let's do it quickly so we have a vector of pointers and again vector of pointers there is a perfectly optimal layout or perfect reliable where the pointers point to the neighboring elements in memory and there is a non-optimal layout where pointers just point to random pieces of memory now this is the same problem as with linked lists and if you compare the numbers uh the run times the perfectly optimal layout will be 384 milliseconds and perfectly knocked up non-optimal will be 5 seconds so it's like 10 times slower or 15 times longer the problem of memory layout becomes apparent again so if you have vector of pointers and you want to use them because of polymorphism then you should consider vector of values and they're much better for performance because the memory access pattern is predictable and the data prefetcher can can can fetch it uh faster also you there are no cost to malloc and free and the other side things that are slowing it down now the other thing that's also another example of how performance depend of the on the mem on the memory memory intensive code is this experiment with class size we have this class rectangle and we're doing some processing on it now how does the speed of processing data depends on the size of the class so we have a class which is size 20 bytes and then we grow 24 28 32 and so on the the size of the class grow and you can see here that there are two different methods one is calculate surface visible the other is calculate surface all but you can see that the performance gets worse as the class size uh the class size grows and when the class size is really large and the performance like two or three times slower than when it's really small and when you have that then good problem the composition you should only if you have a hot loop and you have a class you're processing classes in hot loop then you should have your class should be really small and you should access all the elements of the class all the members of the class if you want maximum performance so this is related to the data class size and and performance um why is processing of smaller class classes faster well the answer is simple the data is brought up from the main memory to the cache memory typically in blocks of 64 bytes it's called cache line size with large classes the cpu is bringing data from the main memory to the cache memory but this data is not used anywhere so the data is brought to the to the cpu to the data cache it's the cpu can really access it really cheaply but it's never used and this is waste as memory bandwidth and slows down your program if you have a code on the hot pad moving all non-stationary data to other class will help the performance additionally data members that are often accessed together should be declared together in the class definition because this decreases increases the chance that the data members will land in the same data cache block okay other ideas for optimizations we talked about linked list we talked about vector of pointers but also other random access structures like trees and cachements will benefit from improved memory layout uh if you have a random access data structures like trees or cache maps keeping the the the the block of memory used by the next structure as content is possible increases performance these kind of structures where all the data is allocated in one block is called flat data structures and there's like flat hash map flat binary tree and so on additionally in case of binary trees you can use a better memory layout there is something called one am the boss layout which is famous you can google it and this using this kind of memory layout for your tree increases performance or you can use energy trees instead of binary trees so anarchy trees are generally more cache friendly uh if you have a hash map that is slow you can switch to a more cash friendly implementation there are hash maps with open addressing and these are faster they can be fast they can be also slower but if you you are careful now they can be faster than the other can than the regular hash maps stl implementation of binary trees and hashmap is not the fastest and their faster alternative example ea the game producer has its own implementation of the stl that aim for performance okay so uh the let's make a summary of the memory performance for good performance it is essential to use the available memory bandwidth in the most effective way possible avoid random memory axes prefer vectors whenever possible avoid vector of pointers prefer vector of values keep things you use together close to one another in the memory keep dating your processing small classes your pressing small i avoid reading the same data from the memory several times with a few exceptions most of the software that you are writing or working on in the object with the object-oriented paradigm is memory limited and mobile devices typically have slower memories smaller data caches less memory bandwidth so this makes situation even worse but with careful programming you can achieve reasonable performance even on mobile devices that's it