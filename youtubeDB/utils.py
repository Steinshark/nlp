import re
import urllib.request 
import requests
import unidecode 
import urllib
import random 
import time
from fake_headers import Headers


_HEADERS            = Headers(headers=True)

ascii_cleaner   ={
        "\U0001f3b8":" ", "\u0171":"u","\u202a":" ",

        '\u0131':'i','\xa3':'L',
        '\xa0':' ','\u2019':"'",
        '\xb0':'degrees','\u266a':'[music]','\u2026':'...',
        '\U0001f60a':':)', "\x2018":"'",
        "\u2018":"'",        "\xe9":"e",
        "\u20ac":'Euro',        "\u2044":"/",
        "\xe8":'e',        "\xf1":"n",
        "\u201c":'"',        "\u201d":'"',
        "\xed":'i',        "\xc5":'A',
        "\xf3":'o',        "\u2014":'-',
        " um ": " ",        " uh ": " ",
        " i i ": " i ",        'Ã§':"c",
        'Ñ':"c",        'Ï€':"pi",
        'â‰…':"~=",         'â„¢':"TM",
        'ð‘›':"n",          'ð‘ƒ':"P", 
        'â„™':"P",         'âˆ’':"-", 
        'Â²':"^2",         'ðœ‹':"pi", 
        'ð¸':"E",         '~':"~",         'Î³':"gamma",         'â€²':"`", 
        'Â¹':"^1",         'âµ':"^5", 
        'Ð²':"B",         'ðº':"G",         'â‚‚':"_2",         'âˆ€':" for all ", 
        'Ð¼':"M",        'âˆƒ':" there exists ",        'Î”':"Delta",         'ðœƒ':"Theta", 
        'â€½':"?!",         'ð›¿':"sigma",         'Å‘':"o",         'ð»':"H", 
        'ï¿½':"?",        'âˆˆ':" element of ",
        'â‚':"_1",        'Î´':"sigma",         'âˆŽ':"[]",         'âŠ—':"X",         'É¸':"phi",         'Î½':"v",         'â„•':"N", 
        '\u2009':"?",        'ð·':"D",         'Â·':" dot ",        'Ã¤':"a",        'Ì¶':"?", 
        'â°':"degrees",         'Ã‰':"E",        'Ã ':"a",        'Ðµ':"e",        'Ð´':"D",        'Ã—':"x",
        'â†’':"->",         'Ã¶':"o",        'ÎŸ':"O",        'ð¶':"C",
        'ð‘Ž':"alpha",        'Ãº':"u",        'Ñ‚':"T",        'ð¹':"F",        'Â½':"1/2",        'â„':"R", 
        'Î¸':"Theta",         'Î­':"e", 
        'Ã´':"o",         'Â³':"^3",         'Ã¡':'a',         'ð“':"l", 
        'Â´':"`",         'Å„':"n",         'â…“':"1/3",         'Ã¯':"l", 
        'ï½¥':"dot",         'â€“':"-",         'ðµ':"B",         'âˆ©':"intersection", 
        'ð‘':"b",         'âˆž':"infinity",         'âˆ‚':"b",         'Â¡':"!",         'Ã¼':"u", 
        'â´':"^4",         'áµ¢':"_i", 
        'â™«':"[music]",         'Ï…':"v",         'ðŸ˜²':":)",        'Ã«':"e",        'Ã£':"",
        'Ä':"a",        'Å¡':"s",        'Å™':"r",        "Å":"o",        "Ãµ":"o",        'Ð¹':"n",
        'Ã¬':"i",        'Ä«':"i",        'Å ':"S",        'Ã¹':"u",        'é¼Ž':"?",'Ð':"?",'Ñƒ':"?",
        'é¨Ž': "?",'å¹¡': "?",'å·¥': "?",
        'æ˜Œ': "?",'çŽ‰': "?",'Ð¿': "?",
        'é€²': "?",'é«˜': "?",'å´Ž': "?",
        'æ‰€': "?",'Ð¡': "?",'æ©‹': "?",
        'æ¢': "?",'æ–°': "?",'æœ¨': "?",
        'Ñ': "?",'é…’': "?",'ç©º': "?",
        '\ufeff': "?",'é›»': "?",'æ˜Ÿ': "?",
        'å’Œ': "?",'è±¹': "?",'æ¢…': "?",
        'æž¸': "?",'Ð½':"?",'è¨±': "?",
        'çŸ¥': "?",'æ±': "?",'æ°´': "?",
        'å‰': "?",'é‰§': "?", 'ç±³': "?",
        'é‹': "?",'å·ž': "?",'å¯': "?",
        'éº’': "?",'Ð–': "?",'Ð•': "?",
        'é€ ': "?",'åƒ': "?",'èŒ…': "?",
        'é™³': "?",'è€€': "?",'èƒ¡': "?",'ã‚º': "?",'ç•ª': "?",'æŸš': "?",
        'å¾·': "?",'æˆ': "?",'æ½˜': "?",'å£¹': "?",
        'è±Š': "?",'ç™½': "?",'æ«»': "?",'å›½': "?",'æ€': "?",
        'ð´': "?",'å­Ÿ': "?",'å€': "?",'å»': "?",
        'ç´¹': "?",'æµ·': "?",'ä»½': "?",'Ð’' :"?",
        'äº•': "?",'ãƒ¼': "?",'ã‚¹': "?",'ç«¹': "?",
        'éºŸ': "?",'ç››': "?",'å®š': "?",'é–€': "?",
        'æ”¤': "?",'æ²³': "?",'Ðš': "?",'çš„':"?",
        'é®­': "?",'ç¾©': "?",'ÑŒ': "?",'éµ': "?",
        'Ì': "?",'Ñ‹': "?",'ç ”': "?",'è‚¡': "?",'å·': "?",'æ—¥': "?",'æ›¸': "?",'å°': "?",
        'ç«‹':"?",'é‘‘': "?",'å­¸': "?",'é™': "?",
        'Ñˆ': "?",'æˆ¿': "?",'é‰¤': "?",'Ð»': "?",
        'å‚…': "?",'  æ˜¥': "?",'è²´': "?",'Ð': "?",
        'è£½': "?",'æ–‡': "?",'å½¦': "?",'æ”¿': "?",'èŠ±': "?",
        'Ð˜': "?",'ãƒ‡': "?",'å°': "?",'ãƒª': "?",'æ·µ': "?",'ç¾Ž': "?",
        'ï¿¼': "?",'ç€¬': "?",'è—¤': "?",'ç›§':"?",'è”­':"?",'ç‡’': "?",
        'å¤§': "?",'éµ¬': "?",'å…¬': "?",'è¾°': "?",'ãƒ©': "?",'è¯': "?",'é™½': "?",'ç§‘': "?",
        'ç¿Ž': "?",'é‹¼': "?",'å¸°': "?",'é™…': "?",'åˆ':"?",' å…«': "?",
        'å°':"?",'å…­': "?",'ç’¿': "?",'Å¾': "?",'åˆ¶': "?",'é›¶': "?",
        'é—´': "?",'å» ': "?",'ãƒ‰': "?",'é›†': "?",'Ð¸': "?",'Ðº': "?",
        'è“': "?",'äº¤':"?",'ç™¼': "?",'æœ‰': "?",'å·': "?",'Ñ€': "?",
        'Ñ…': "?",'åœ’': "?",'æ–—': "?",'é¹¿': "?",'äº‰': "?",'å­': "?",
        'Ð¤': "?",'Ñ‡': "?",'æ›¹': "?",'èµ¤': "?",'åœ˜': "?",'æ¾³': "?",
        'è¥¿': "?",'æœ¬': "?",'ãƒ¬': "?",'ç¦': "?",'æŽ': "?",'ãƒ‹': "?",
        'ã‚µ': "?",'è¡¡': "?",'è£': "?",'å£«': "?",'å¸': "?",'æ¾': "?",
        'Ð±': "?",'ãŸ': "?",'Ð¾':"?",'ã‚£':"?",'è¦‹':"?",'ã‚¯': "?",
        'é€š': "?",'ã‚®': "?",'é¯¤': "?",'èˆª': "?",'å¯§': "?",'ã‚‰': "?",
        'ç®­': "?",'åœ‹': "?",'ãƒ³': "?",'äº¬': "?",'å®¢': "?",'ç‰‚': "?",
        'å¤©': "?",'å­«': "?",'Ð°': "?",'åª':"?", 'Ä›':"?", 'é‰„':"?",
        ' å¹³':"?",'Ð¢':"?", 'æ¬£':"?", 'æ¦®':"?", 'ä¸­':"?", 'Ð³':"?", 
        'é…±':"?", 'æŸ¯':"?", 'é™¢':"?",'æ˜¥':"?", 'å¹³':"?", 'å…«':"?",
        'ð‘§':"z", 'â…”':"2/3", 'Â¼':"1/4", 'Ï‰':"w", 'ð‘¤':"w"
}


def get_transcript(video_html:str):

    #Find url 
    try:
        pre_parsed_url  = re.findall('(https://www.youtube.com/api/timedtext.{1,400}lang=en)',video_html)[0]
    except IndexError:
        #Could indicate its not in English (it sucks)
        with open("dump.html",'w',encoding='utf_8') as writefile:
            writefile.write(video_html)
        print("out")
        raise IndexError
    parsed_url      = pre_parsed_url.replace(r"\u0026","&")

    response        = requests.get(parsed_url,timeout=3)

    transcript      = ""
    for line in response.text.replace("text start=","\n").split("\n")[1:]:
        line = line.split(">")[1].split("<")[0].replace(r"&amp;#39;","'")
        transcript += line + "\n"

    transcript = unidecode.unidecode(transcript)
    # for key,val in ascii_cleaner.items():
    #     transcript = transcript.replace(key,val)

    # transcript  = transcript.encode('ascii','replace').decode()

    return transcript.replace("\n"," ")


def filter_bad_content(video_transcript:str):

    #Make determination on only 20_000 charas 
    decision_window     = video_transcript[:5_000].lower()
    decision_len        = len(decision_window)

    #Reject anything less than 500 chars 
    if decision_len < 500:
        return False 
    


    #Make checks for bad keywords 
    #Reject if font occurs more than 200 times 
    if decision_window.count("font") > 50:
        return False 
    elif (decision_window.count("[music]")*len("[music]"))/decision_len > .1:
        return False 
    elif (decision_window.count("[applause]")*len("[applause]"))/decision_len > .1:
        return False

    return True


def get_url_html(url:str,delay=.2):

    time.sleep(delay)

    #Try to make request 
    print(f"requesting to {url}")
    request         = requests.get(url,headers=_HEADERS.generate(),timeout=1)
    print(f"\tgot back")
    if request.status_code == 200:
        text        = request.text 
        print(f"\ttext back len {len(text)//1_000}K chars")
        urls        = ["https://www.developer-tech.com/news/" + preurl.split('"><h3 itemprop')[0] for preurl in text.split("https://www.developer-tech.com/news/")[1:] if '"><h3 itemprop' in preurl]
        #urls        = [it[0] for it in re.findall('(https://www.developer-tech.com/news/([a-z]+|-)+/)',text)]
        print(f"\t{len(urls)} found")
        return urls 
    else:
        print(f"\treq fail")
        raise ValueError(f"{request.status_code}")

        


def is_transcribed(video_transcript:str):

    #Get ratio of "." to len 
    punct_count = video_transcript.count(".")
    tot_len     = len(video_transcript)

    return ((punct_count / tot_len) > .005)

if __name__ == '__main__':

    html    = open('ex.html','r',encoding='utf_8').read()
    get_transcript(html)