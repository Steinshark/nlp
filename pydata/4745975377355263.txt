import torch 
from torch.nn import ConvTranspose1d,Upsample,BatchNorm1d,ReLU,LeakyReLU,Conv1d,Sequential,Tanh, Sigmoid
from networks import AudioGenerator,AudioDiscriminator

from utilities import model_size

from torch.nn import Upsample

#base_factors = [2,2,2,2,2,3,3,3,5,5,5,7,7] 
#           IDEA 1 -
# Use a ConvTranspose with strides equal to factors of input    
#ncz = 512 
#random_input    = torch.randn(size=(1,ncz,1))


def build_gen(ncz=512,leak=.02,kernel_ver=0,fact_ver=0,ch_ver=1,device=torch.device('cuda'),ver=1,out_ch=1):

    factors     = [[2,2,2,2,3,3,3,5,5,7,7], [7,7,5,5,3,3,3,2,2,2,2],[7,2,2,7,2,2,5,3,5,3,3],[4,5,7,7,5,3,3,4]][fact_ver]
    channels    = [[4096,4096,4096,2048,2048,1024,1024,512,256,256,256],[4096,4096,2048,1024,512,1024,512,256]][ch_ver]

    kernels = [
            [5,     5,  9,  21, 21, 19, 17, 17, 17, 11, 11, 11, 11],                #   LG-SM 
            [3,     17, 65, 65 ,65 ,129 ,129],         #   MED-LG-SM
            [19,    19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19],                #   MED_SM
    ][kernel_ver]
   
    Gen     = Sequential(   ConvTranspose1d(ncz,channels[0],factors[0],factors[0]),
                            BatchNorm1d(channels[0]),
                            LeakyReLU(leak,True))


    for i,ch in enumerate(channels):
        if i+2 == len(channels):
            Gen.append(         ConvTranspose1d(ch,channels[i+1],factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(    channels[i+1]))
            Gen.append(         LeakyReLU(leak,True)) 

            Gen.append(         Conv1d(         channels[i+1],out_ch,kernels[i],1,padding=int(kernels[i]/2),bias=True))
            Gen.append(         Tanh())
            break

        else:
            Gen.append(         ConvTranspose1d(ch,channels[i+1],factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(channels[i+1])) 
            Gen.append(         LeakyReLU(leak,True))   

            Gen.append(         Conv1d(         channels[i+1],channels[i+1],kernels[i],1,padding=int(kernels[i]/2),bias=False))
            Gen.append(         BatchNorm1d(channels[i+1]))
            Gen.append(         LeakyReLU(leak,True))
    return Gen.to(device)

def build_upsamp(ncz=512,out_ch=1,kernel_ver=0,factor_ver=0,leak=.2,device=torch.device('cuda'),verbose=False):
    biased  = True
 
    Gen     = Sequential(   ConvTranspose1d(ncz,2048,kernel_size=4,stride=1,bias=False),               # 4 
                            torch.nn.BatchNorm1d(2048),
                            ReLU(inplace=True), 
                            Conv1d(2048,2048,kernel_size=3,stride=1,padding=1,bias=biased),
                            ReLU(inplace=True),    

                            ConvTranspose1d(2048,2048,kernel_size=4,stride=4,bias=False),              # 16
                            torch.nn.BatchNorm1d(2048),
                            ReLU(inplace=True), 
                            Conv1d(2048,1024,kernel_size=5,stride=1,padding=2,bias=biased),
                            ReLU(inplace=True),  

                            ConvTranspose1d(1024,1024,kernel_size=8,stride=8,bias=False),              # 128
                            torch.nn.BatchNorm1d(1024),
                            LeakyReLU(inplace=True), 
                            Conv1d(1024,1024,kernel_size=17,stride=1,padding=8,bias=biased),
                            ReLU(inplace=True),  

                            ConvTranspose1d(1024,256,kernel_size=8,stride=8,bias=False),              # 1024
                            torch.nn.BatchNorm1d(256),
                            ReLU(inplace=True), 
                            Conv1d(256,128,kernel_size=33,stride=1,padding=16,bias=biased),
                            ReLU(inplace=True),  

                            ConvTranspose1d(128,128,kernel_size=8,stride=8,bias=False),              # 8192
                            torch.nn.BatchNorm1d(128),
                            ReLU(inplace=True), 
                            Conv1d(128,64,kernel_size=65,stride=1,padding=32,bias=biased),
                            ReLU(inplace=True),  

                            ConvTranspose1d(64,64,kernel_size=4,stride=4,bias=False),               # 32768
                            torch.nn.BatchNorm1d(64),
                            ReLU(inplace=True), 
                            Conv1d(64,32,kernel_size=129,stride=1,padding=64,bias=biased),
                            ReLU(inplace=True), 
                            #LeakyReLU(inplace=True),  


                            # Conv1d(32,16,kernel_size=257,stride=1,padding=128,bias=biased),
                            # torch.nn.LeakyReLU(inplace=True),

                            Conv1d(32,1,kernel_size=513,stride=1,padding=256,bias=biased),
                            torch.nn.Tanh()
                            )

       
    return Gen.to(device)

def build_low_hi(ncz=512,out_ch=1,kernel_ver=0,factor_ver=0,leak=.2,device=torch.device('cuda'),verbose=False):
    
    Gen     = Sequential(   ConvTranspose1d(ncz,1024,kernel_size=4,stride=1),               # 4 
                            Conv1d(1024,1024,kernel_size=31,stride=1,padding=15,bias=False),
                            ReLU(inplace=True),    

                            ConvTranspose1d(1024,1024,kernel_size=4,stride=4),              # 16 
                            Conv1d(1024,1024,kernel_size=31,stride=1,padding=15,bias=False),
                            ReLU(inplace=True),  

                            ConvTranspose1d(1024,1024,kernel_size=4,stride=4),              # 64 
                            Conv1d(1024,512,kernel_size=31,stride=1,padding=16,bias=False),
                            ReLU(inplace=True),  

                            ConvTranspose1d(512,256,kernel_size=4,stride=4),              # 256    
                            Conv1d(256,256,kernel_size=31,stride=1,padding=15,bias=False),
                            ReLU(inplace=True), 

                            ConvTranspose1d(256,128,kernel_size=4,stride=4),              # 1024 
                            #Conv1d(128,128,kernel_size=31,stride=1,padding=15,bias=False),
                            #BatchNorm1d(128),
                            ReLU(inplace=True),

                            ConvTranspose1d(128,64,kernel_size=4,stride=4),              # 4096 
                            #Conv1d(64,64,kernel_size=31,stride=1,padding=15,bias=False),
                            #BatchNorm1d(64),
                            ReLU(inplace=True),

                            ConvTranspose1d(64,64,kernel_size=4,stride=4),              # 16384
                            Conv1d(64,64,kernel_size=31,stride=2,padding=15,bias=False),
                            
                            )

       
    return Gen.to(device)

def build_encdec(ncz,encoder_factors=[2,3],encoder_kernels=[5,7],dec_factors=[7,5,5,3,3],enc_channels=[256,1024],dec_kernels=[5,17,25,89,513],leak=.2,batchnorm=True):
    
    #Factors management 
    factors             = [2,2,3,3,5,5,7,7,3,2,2]
    factors             = [2,3,8,9,25,49]
    output              = 529_200

    #Start with 2 encoder layers
    enc_channels        = [64,512]

    #Start Generator Architecture with the encoder 
    G   = Sequential(   Conv1d(1,             enc_channels[0],    encoder_kernels[0], stride=encoder_factors[0],padding=int(encoder_kernels[0]/2)))
    if batchnorm:
        G.append(           BatchNorm1d(enc_channels[0]))
    G.append(           LeakyReLU(leak,True))

    G.append(           Conv1d(enc_channels[0], enc_channels[1],    encoder_kernels[1], stride=encoder_factors[1],padding=int(encoder_kernels[1]/2)))
    if batchnorm:
        G.append(           BatchNorm1d(enc_channels[1]))
    G.append(           LeakyReLU(leak,True))
                        

    #Finish with decoder layers 
    dec_channels        = [1024,512,128,64,2]             #OLD 
    dec_conv_kernels    = dec_kernels
    dec_conv_padding    = [int(ker/2) for ker in dec_conv_kernels]

    for i,fact in enumerate(dec_factors):

        #Add conv transpose layer 
        if i == 0:
            G.append(           ConvTranspose1d(enc_channels[-1],dec_channels[i],dec_factors[i],dec_factors[i],padding=0))
        else:
            G.append(           ConvTranspose1d(dec_channels[i-1],dec_channels[i],dec_factors[i],dec_factors[i],padding=0))
      
        
        if i == len(dec_factors)-1:
            G.append(Tanh())
        else:
            #Add rest of layers 
            if batchnorm:
                G.append(           BatchNorm1d(dec_channels[i]))
            G.append(           LeakyReLU(leak,True))

            G.append(           Conv1d(dec_channels[i],dec_channels[i],dec_conv_kernels[i],1,dec_conv_padding[i],bias=False))
            if batchnorm:
                G.append(           BatchNorm1d(dec_channels[i]))
            G.append(           LeakyReLU(leak,True))

    return G.to(torch.device("cuda"))

#BEST 
def buildBest(ncz=512,leak=.2,kernel_ver=1,factor_ver=0,device=torch.device('cuda'),ver=1,out_ch=2,verbose=False):
    factors     = [[15,8,7,7,5,2,3],[2,5,7,7,8,9,15],[15,5,9,7,8,7,2]][factor_ver]

    ch          = [2048,2048,2048,512,256,128]

    ker         = [
                    [3,61,513,1025,129],
                    [3,5,9,17,65]][kernel_ver]

    pad         = [int(k/2) for k in ker] 
    Gen         = Sequential(   ConvTranspose1d(ncz,ch[0],factors[0],factors[0]),
                                BatchNorm1d(ch[0]),
                                LeakyReLU(leak,True))

    Gen.append(                 Conv1d(ch[0],ch[0],3,1,1))
    Gen.append(                 BatchNorm1d(ch[0]))
    Gen.append(                 LeakyReLU(leak,True)) 
    
    for i,c in enumerate(ch):
        if i+1 == len(ch):
            Gen.append(         ConvTranspose1d(c,128,factors[i+1],factors[i+1]))

            Gen.append(         Conv1d(128,64,factors[i+1]*3,1,int((factors[i+1]*3)/2)))
            Gen.append(         Sigmoid())
            Gen.append(         Conv1d(64,out_ch,factors[i+1],1,int((factors[i+1])/2)))
            Gen.append(         Tanh())

        else:
            Gen.append(         ConvTranspose1d(c,ch[i+1],factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(ch[i+1])) 
            Gen.append(         LeakyReLU(leak,True)) 

            Gen.append(         Conv1d(ch[i+1],ch[i+1],ker[i],1,pad[i]))
            Gen.append(         BatchNorm1d(ch[i+1]))
            Gen.append(         LeakyReLU(leak,True)) 
    
    return Gen.to(device)

def buildBestMod1(ncz=512,leak=.2,kernel_ver=1,factor_ver=0,device=torch.device('cuda'),out_ch=2,verbose=False):
    factors     = [[15,8,7,5,2,3],[2,3,5,7,7,8,15],[15,5,9,7,8,7,2]][factor_ver]

    ch          = [ncz,int(ncz/2),int(ncz/2),int(ncz/4),64]


    Gen         = Sequential(   ConvTranspose1d(ncz,ch[0],factors[0],factors[0]),
                                BatchNorm1d(ch[0]),
                                LeakyReLU(leak,True))

    Gen.append(                 Conv1d(ch[0],ch[0],3,1,1))
    Gen.append(                 BatchNorm1d(ch[0]))
    Gen.append(                 LeakyReLU(leak,True)) 

    Gen.append(                 Conv1d(ch[0],ch[0],7,1,3))
    Gen.append(                 BatchNorm1d(ch[0]))
    Gen.append(                 LeakyReLU(leak,True)) 
    
    for i,c in enumerate(ch):
        
        
        if i+1 == len(ch):
            n_ch                = 48
            Gen.append(         ConvTranspose1d(c,n_ch,factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(n_ch))
            Gen.append(         LeakyReLU(leak,True)) 


            #ker_size            = 5 
            #n_ch_prev           = n_ch
            #n_ch                = 64
            #Gen.append(         Conv1d(n_ch_prev,n_ch,ker_size,1,int(ker_size/2),bias=False))
            #Gen.append(         BatchNorm1d(n_ch))
            #Gen.append(         LeakyReLU(leak,True)) 

            ker_size            = 7 
            n_ch_prev           = n_ch
            n_ch                = 32
            Gen.append(         Conv1d(n_ch_prev,n_ch,ker_size,1,int(ker_size/2),bias=False))
            Gen.append(         BatchNorm1d(n_ch))
            Gen.append(         LeakyReLU(leak,True)) 

            ker_size            = 7 
            n_ch_prev           = n_ch
            n_ch                = 32
            Gen.append(         Conv1d(n_ch_prev,n_ch,ker_size,1,int(ker_size/2),bias=False))
            Gen.append(         BatchNorm1d(n_ch))
            Gen.append(         LeakyReLU(leak,True)) 

            #ker_size            = 5 
            #n_ch_prev           = n_ch
            #n_ch                = 32
            #Gen.append(         Conv1d(n_ch_prev,n_ch,ker_size,1,int(ker_size/2),bias=False))
            #Gen.append(         BatchNorm1d(n_ch))
            #Gen.append(         LeakyReLU(leak,True)) 

            Gen.append(         Conv1d(n_ch,out_ch,3,1,1))
            Gen.append(         Tanh())

        else:
            Gen.append(         ConvTranspose1d(c,ch[i+1],factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(ch[i+1])) 
            Gen.append(         LeakyReLU(leak,True)) 

            ker_size            = 5 
            Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
            Gen.append(         BatchNorm1d(ch[i+1]))
            Gen.append(         LeakyReLU(leak,True)) 

            # ker_size            = 15 if i < 3 else 13
            # Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int((ker_size*1)/2),bias=False))
            # Gen.append(         BatchNorm1d(ch[i+1]))
            # Gen.append(         LeakyReLU(leak,True))
            
            # ker_size            = 9 if i < 3 else 5 
            # Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int((ker_size*1)/2),bias=False))
            # Gen.append(         BatchNorm1d(ch[i+1]))
            # Gen.append(         LeakyReLU(leak,True))  

            #ker_size            = 5 
            #Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int((ker_size*1)/2),bias=False))
            #Gen.append(         BatchNorm1d(ch[i+1]))
            #Gen.append(         LeakyReLU(leak,True)) 

    Gen     = Gen.to(device)

    if verbose:
        print(Gen)
    return Gen

def buildBestMod2(ncz=512,leak=.04,kernel_ver=1,factor_ver=0,device=torch.device('cuda'),out_ch=2,verbose=False):
    factors     = [[2,2,2,3,3,5,7,7],[7,7,5,3,3,2,2,2]][factor_ver]
    ch          = [1024,    1024,   1024,    512,    512,    256,    128]

    activation_fn               = LeakyReLU
    activation_kwargs           = {"negative_slope":leak,"inplace":True}


    Gen         = Sequential(   ConvTranspose1d(ncz,ch[0],factors[0],factors[0]),
                                BatchNorm1d(ch[0]),
                                activation_fn(**activation_kwargs))

    Gen.append(                 Conv1d(ch[0],ch[0],3,1,1,bias=False))
    Gen.append(                 BatchNorm1d(ch[0]))
    Gen.append(                 activation_fn(**activation_kwargs)) 

    Gen.append(                 Conv1d(ch[0],ch[0],5,1,int(5/2),bias=False))
    Gen.append(                 BatchNorm1d(ch[0]))
    Gen.append(                 activation_fn(**activation_kwargs)) 
    
    for i,c in enumerate(ch):
        
        if i+1 == len(ch):
            n_ch                = 128
            n_ch                = 128
            Gen.append(         ConvTranspose1d(c,n_ch,factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(n_ch))
            Gen.append(         activation_fn(**activation_kwargs)) 

            ker_size            = 31 
            n_ch_prev           = n_ch
            n_ch                = 64
            Gen.append(         Conv1d(n_ch_prev,n_ch,ker_size,1,int(ker_size/2),bias=True))
            Gen.append(         BatchNorm1d(n_ch))
            Gen.append(         activation_fn(**activation_kwargs)) 

            ker_size            = 31 
            n_ch_prev           = n_ch
            n_ch                = 64
            Gen.append(         Conv1d(n_ch_prev,n_ch,ker_size,1,int(ker_size/2),bias=True))
            Gen.append(         BatchNorm1d(n_ch))
            Gen.append(         activation_fn(**activation_kwargs)) 

            ker_size            = 63 
            n_ch_prev           = n_ch
            n_ch                = 64
            Gen.append(         Conv1d(n_ch_prev,1,ker_size,1,int(ker_size/2),bias=True))
            Gen.append(         Tanh())


        else:
            Gen.append(         ConvTranspose1d(c,ch[i+1],factors[i+1],factors[i+1]))
            Gen.append(         BatchNorm1d(ch[i+1])) 
            Gen.append(         activation_fn(**activation_kwargs)) 

            if i < 3:

                ker_size                    = 3
                Gen.append(                 Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
                Gen.append(                 BatchNorm1d(ch[i+1]))
                Gen.append(                 activation_fn(**activation_kwargs)) 

                # ker_size                    = 7
                # Gen.append(                 Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
                # Gen.append(                 BatchNorm1d(ch[i+1]))
                # Gen.append(                 Tanh()) 

                ker_size                    = 9
                Gen.append(                 Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
                Gen.append(                 BatchNorm1d(ch[i+1]))
                Gen.append(                 activation_fn(**activation_kwargs)) 

            else:
                ker_size            = 7
                Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
                Gen.append(         BatchNorm1d(ch[i+1]))
                Gen.append(         activation_fn(**activation_kwargs))

                ker_size            = 15
                Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
                Gen.append(         BatchNorm1d(ch[i+1]))
                Gen.append(         activation_fn(**activation_kwargs))

                # ker_size            = 31
                # Gen.append(         Conv1d(ch[i+1],ch[i+1],ker_size,1,int(ker_size/2),bias=False))
                # Gen.append(         BatchNorm1d(ch[i+1]))
                # Gen.append(         Tanh())

            

    Gen     = Gen.to(device)

    if verbose:
        print(Gen)
    return Gen


if __name__ == "__main__":
    kernels     = [15,17,21,23,25,7,5]
    paddings    = [int(k/2) for k in kernels]
    D2      = AudioDiscriminator(channels=[2,32,64,128,256,512,1024,1],kernels=kernels,strides=[12,9,7,7,5,5,4],paddings=paddings,device=torch.device('cuda'),final_layer=1,verbose=False)
    inp = torch.randn((1,2,529200),device=torch.device('cuda'))

    print(f"shape: {D2.forward(inp).item()}")