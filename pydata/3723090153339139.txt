import torch
import math
import random
import numpy
from data_utils import PCA_Handler
import torchaudio 



class ConvResBlock(torch.nn.Module):

    def __init__(self,n_ch:int,kernel_size:int,stride:int,padding:int=0,bias:bool=False,act_fn=torch.nn.LeakyReLU):
        
        super(ConvResBlock,self).__init__()

        self.block      = torch.nn.Sequential(
            torch.nn.Conv1d(n_ch,n_ch,kernel_size=kernel_size,stride=stride,padding=padding,bias=False),
            torch.nn.BatchNorm1d(n_ch),
            act_fn(),

            torch.nn.Conv1d(n_ch,n_ch,kernel_size=kernel_size,stride=stride,padding=padding,bias=False),
            torch.nn.BatchNorm1d(n_ch),
            act_fn()
        )


    def forward(self,x:torch.Tensor) -> torch.Tensor:
        return self.block(x)

    def apply(self,fn):
        self.block.apply(fn)

 

class SpecDiscriminator(torch.nn.Module):
    def __init__(self):

        self.device         = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
        super(SpecDiscriminator, self).__init__()
        conv_act            = torch.nn.LeakyReLU

        
        #1x64x128
        self.conv1          = torch.nn.Sequential(
            torch.nn.Conv2d(1,16,5,1,2,bias=False),
            torch.nn.BatchNorm2d(16),
            conv_act(),

            torch.nn.Conv2d(16,16,5,2,2,bias=False),
            torch.nn.BatchNorm2d(16),
            conv_act(),
        ).to(self.device)
        #32x32x64
        self.conv2          = torch.nn.Sequential(
            torch.nn.Conv2d(16,32,5,1,2,bias=False),
            torch.nn.BatchNorm2d(32),
            conv_act(),

            torch.nn.Conv2d(32,32,5,2,2,bias=False),
            torch.nn.BatchNorm2d(32),
            conv_act(),
        ).to(self.device)
        #64x16x32
        self.conv3          = torch.nn.Sequential(
            torch.nn.Conv2d(32,64,5,1,2,bias=False),
            torch.nn.BatchNorm2d(64),
            conv_act(),

            torch.nn.Conv2d(64,64,5,2,2,bias=False),
            torch.nn.BatchNorm2d(64),
            conv_act(),
        ).to(self.device)
        #128x8x16
        self.conv4          = torch.nn.Sequential(
            torch.nn.Conv2d(64,128,5,1,2,bias=False),
            torch.nn.BatchNorm2d(128),
            conv_act(),

            torch.nn.Conv2d(128,128,5,2,2,bias=False),
            torch.nn.BatchNorm2d(128),
            conv_act(),
        ).to(self.device)
        #256x4x8
        self.conv5          = torch.nn.Sequential(
            torch.nn.Conv2d(128,256,3,1,1,bias=False),
            torch.nn.BatchNorm2d(256),
            conv_act(),

            torch.nn.Conv2d(256,256,3,2,1,bias=False),
            torch.nn.BatchNorm2d(256),
            conv_act(),

            torch.nn.Conv2d(256,512,(2,2),1,0,bias=False),
            conv_act(),
            torch.nn.Flatten(start_dim=1),
        ).to(self.device)

        #512x1
        self.conv6          = torch.nn.Sequential(
            torch.nn.Linear(512,1),
            torch.nn.Sigmoid()
        ).to(self.device)

    def forward(self,x:torch.Tensor) -> torch.Tensor:
        x           = self.conv1(x)
        x           = self.conv2(x)
        x           = self.conv3(x)
        x           = self.conv4(x)
        x           = self.conv5(x)
        x           = self.conv6(x)
        return x



#ASSUMED TO BE OUTPUT OF SHAPE (128,128)
class SpecGenerator(torch.nn.Module):
    def __init__(self,nz,nmel=64,t=128):
        
        self.device         = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
        super(SpecGenerator, self).__init__()
        init_act            = torch.nn.LeakyReLU

        #Start with a (1,nz)
        
        bias                = False

        n_ch                = 128

        self.in_layer       = torch.nn.Sequential(
            torch.nn.Flatten(start_dim=1),
            torch.nn.Linear(nz,512),
            torch.nn.ReLU()
        )

        self.final_layer     = torch.nn.Sequential(
            torch.nn.Linear(512,1024),
            init_act(),

            torch.nn.Linear(1024,2048),
            init_act(),

            torch.nn.Linear(2048,64*64),
            torch.nn.Unflatten(dim=1,unflattened_size=(1,64,64)),
            torch.nn.ReLU(),
        ).to(self.device)




    def forward(self,x:torch.Tensor) -> torch.Tensor:
        x           = self.in_layer(x)
        x           = self.final_layer(x)
        return x

class dumbDiscriminator(torch.nn.Module):

    def __init__(self):
        self.device         = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
        super(dumbDiscriminator, self).__init__()

        self.l1     = torch.nn.Sequential(
            torch.nn.Flatten(start_dim=1),
            torch.nn.Linear(64*64,512),
            torch.nn.LeakyReLU(),

            torch.nn.Linear(512,128),
            torch.nn.LeakyReLU(),

            torch.nn.Linear(128,1),
            torch.nn.Sigmoid()
        )
    
    def forward(self,x:torch.Tensor) ->torch.Tensor:
        return self.l1(x)


class resBlock(torch.nn.Module):

    def __init__(self,n_ch,act_fn):
        super(resBlock,self).__init__()
        self.block  = torch.nn.Sequential(
            torch.nn.Conv2d(n_ch,n_ch,3,1,1,bias=False),
            torch.nn.BatchNorm2d(n_ch),
            act_fn(),

            torch.nn.Conv2d(n_ch,n_ch,3,1,1,bias=False),
            torch.nn.BatchNorm2d(n_ch),
            act_fn()
        )
    
    def forward(self,x:torch.Tensor) -> torch.Tensor:
        return self.block(x) + x 
    

class autoencoder(torch.nn.Module):


    def __init__(self,act_fn=torch.nn.ReLU,p=0):
        super(autoencoder, self).__init__()
        self.device     = torch.device('cuda' if torch.cuda.is_available() else "cpu")
        self.model  = torch.nn.Sequential(
            torch.nn.Conv2d(1,32,3,1,1,bias=False),
            torch.nn.BatchNorm2d(32),
            act_fn(),

            torch.nn.Conv2d(32,32,3,2,1,bias=False),
            torch.nn.BatchNorm2d(32),
            act_fn(),

            resBlock(32,act_fn=act_fn),                             # //2


            torch.nn.Conv2d(32,32,3,1,1,bias=False),
            torch.nn.BatchNorm2d(32),
            act_fn(),

            torch.nn.Conv2d(32,32,3,2,1,bias=False),
            torch.nn.BatchNorm2d(32),
            act_fn(),

            resBlock(32,act_fn=act_fn),                             # //4


            torch.nn.Conv2d(32,64,3,1,1,bias=False),
            torch.nn.BatchNorm2d(64),
            act_fn(),

            torch.nn.Conv2d(64,64,3,2,1,bias=False),
            torch.nn.BatchNorm2d(64),
            act_fn(),

            resBlock(64,act_fn=act_fn),                             # // 8

            torch.nn.Conv2d(64,128,3,1,1,bias=False),
            torch.nn.BatchNorm2d(128),
            act_fn(),

            torch.nn.Conv2d(128,128,3,2,1,bias=False),
            torch.nn.BatchNorm2d(128),
            act_fn(),

            torch.nn.Flatten(start_dim=1),                          # // 16
            torch.nn.Linear(8*8*128,4*4*128),
            act_fn(),

            torch.nn.Linear(4*4*128,2*2*128),
            act_fn(),

            
            #DECODER 
            torch.nn.Linear(2*2*128,4*4*128),
            act_fn(),

            torch.nn.Linear(4*4*128,8*8*128),
            act_fn(),

            torch.nn.Unflatten(dim=1,unflattened_size=(128,8,8)),
            torch.nn.ConvTranspose2d(128,64,4,2,1,bias=False),
            torch.nn.Dropout(p=p),
            torch.nn.BatchNorm2d(64),   
            act_fn(),                                     # //8

            torch.nn.ConvTranspose2d(64,32,4,2,1,bias=False),
            torch.nn.Dropout(p=p),
            torch.nn.BatchNorm2d(32),
            act_fn(),                                     # // 4

            torch.nn.ConvTranspose2d(32,32,4,2,1,bias=False),       # //2
            torch.nn.Dropout(p=p),
            torch.nn.BatchNorm2d(32),
            act_fn(), 

            torch.nn.ConvTranspose2d(32,1,4,2,1,bias=False),        # //1
            torch.nn.Dropout(p=p),
            torch.nn.ReLU()
        ).to(self.device)

    def forward(self,x:torch.Tensor) -> torch.Tensor:
        return self.model(x)

if __name__ == '__main__':


    # import data_utils
    # from torch.utils.data import DataLoader

    # pca_handler             = data_utils.PCA_Handler(from_vectors="C:/data/music/2048_2048_16/",sample_rate=4096,non_pca=True)
    # pca_handler.ds_no_pca(n_samples=8,length_s=8)

    # dataset                 = data_utils.AudioDataSet(pca_handler)
    # dataloader              = DataLoader(dataset,batch_size=2,shuffle=True,num_workers=1,pin_memory=False)

    # model                   = AutoEncoder().cuda()
    
    # spectrogrammer          = torchaudio.transforms.Spectrogram(n_fft=128,win_length=128).cuda()

    # #

    # lr                      = .0002
    # betas                   = (.5,.999)
    # display                 = 16
    # save                    = 16

    # optimizer               = torch.optim.Adam(model.parameters(),lr=lr,betas=betas)

    modelg                   = SpecGenerator(nz=64)
    modeld                  = SpecDiscriminator()
    inz                     = torch.randn(size=(8,1,64)).cuda()
    outs                    = modeld.forward(modelg.forward(inz))
    print(f"out size is {outs.shape}")

