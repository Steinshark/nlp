three key lessons in application server optimization november 15 2023 hot off the presses weeks i've spent most of my time let's see weeks i spend most of my time improving the performance of uh wait something's wrong here some something's wrong here something's wrong here i zoomed in and it got all effed up oh yeah it got all effed up once you zoom in okay over the past six weeks we've i i've spent most of my time improving the performance of graphitees application server affectionally nicknamed subwoofer while this effort is still fresh in my mind i wanted to share three key lessons that help me uh help us find meaningful performance gains in uh and in doing so greatly improve the graphite user experience okay okay i'm pretty excited about this yeah by the way i do love it that i can that i want to be able to zoom in and have all the side stuff go away so i can just look at the article because now it has to be so small you know what i mean it has to be so small all right context uh we started to investigate our app server performance in earnest after noticing a few uh concerning trends in the data over the summer okay at the time we would semi-frequently experience large but unexplained regressions in server endpoint latency all right every three or four weeks a team would get paged because of large regression in endpoint response time think something like p90 shifted up to eight seconds for a few minutes oh that's crazy if you're getting 8 seconds on a p90 that means 10% of your customers are having 10 or 8 seconds or larger request times that's crazy i smell gc issues i don't think so you'd have to be on an old ass version of of node you'd have to be on like node 12 in practice these regressions would render the graphite's dashboard unusually slow for reviewing or merging prs but then again really large amounts of react can be quite slow despite dayong investigations into these regressions each time they cropped up the cause of these incidents remained a mystery have you profile filed as a result we began settling into an uneasy rhythm that we've noticed large performance regression scale up the number of server pods dig in usually find nothing and just move on repeating the exercise again for a few weeks later really being able to run a server isolate a server run it to the ground profile uh usually if you're on a server perf utilities is around pf's a great one to really get a good look at kind of where time in generally is being spent uh between the aforementioned regressions and the outlier uh outlier users we've noticed uh were adding disproportional load our site performance graph were extremely spiky when teammates andor users in our slack community remarked that our site had gotten slower over the past window of time it was difficult to authoritatively verify or disprove this let alone point pinpoint the cause um wait what wait what of course you can verify it you just eff and said your p90 was above 8 seconds you can verifiy verify yes with all authority we're slow right now we're slower than we were and it happens a lot of times we don't know why sorry my p90 69 is under three milliseconds well damn son that's fast anyways that's kind of i feel like you can verify that these things are happening it's just why we have no idea there uh there was a clear user pain to be addressed if only we could get wrangle on the system okay uh hope this investigation let's see the hope for this investigation was two-part learn where those large aggressions were coming from and remove any of them as possible reducing the bad user experience and making it easier to understand app server performance trends over time let's dive in uh typically i'm g to i'm just going to toss out before i look it almost always come down to coping promises react server rendering just going to throw it out there your p90 is 5.7 almost statistically averaged i'm just going to assume the problem isn't with looking up data maybe that's going to be 500 milliseconds but that's usually a very obvious problem when your database is the slow thing okay all right throughout this investigation i used our existing uh p95 server endpoint metric as my guide whenever i saw a spike in this metric i would dig in as proposed to pass incidents where we would investigate thing uh things would return to normal and then we'd move back on to other work this time i didn't rest until i had answers we had a hypothesis of of course hypothesis of course but the results were actually surprising a few things emerged okay one one verify the underlying impact p95 latency can be too blunt okay interesting okay what does this mean this lesson is key for an application that integrates with a third party api in our case api providing experience and or provider experiencing an incident to spike your p or your app's p95 response time okay that's fair this is fair you should put timeouts always put timeouts uh all without any real degradation on your uh side during the six weeks when i was working on performance a large customer onboarded onto our graphite onto our graphite yeah i don't know a graphite i actually i don't think i've used graphite a few days after the customer onboarded sessions we've noticed a search and server latencies okay so we're having some issues hi qps failing server nodes db bottlenecks okay okay exciting graphite is awesome time series database oh super cool i've never played with a time series database i always have to use hadoop at at work or hive a lot of hiving told you and so i've never played i i i've i've briefly seen grafana and you know i almost had to write an adapter for grafana but i've never i've never played with grafana much so looking at a rollup let's see looking at a rollup on the top user endpoint combinations we noticed that a surge in the number of requests was coming from a specific set of users one or when one of these users would try to load the insights page on the graphite dashboard our app server would try to grab a profile info for every github user in their monor repo including their github avatar urls ooh which needed to be proxied through a server resulting in a single network request for each avatar and thousands of requests with a span of seconds this was not ideal but easily remedied okay great this is a great okay so great great stuff right here don't make a thousand requests okay i just assumed that whatever the problems were we weren't going to see these type of problems because these are very obvious problems don't make a thousand requests right like this just bad it's just bad for business okay it creates a thousand promises it has to wait for a thousand things to be done uh it's just it's just not a good thing we quickly stemmed the bleeding uh for this customer and based on the knowledge we had around queries per second the cost of each request assumed that we had taken a sitewide hit in performance because of the spikes over uh an overall server latency however digging in showed a different picture our server actually processed the thousands of avatar proxies with no trouble instead it was the customer github enterprise instance that had become overwhelmed leading to a high endpoint latency for this particular organization but perfectly normal behavior for all other graphite users okay the lesson here sometimes p95 latency can be a bit too blunt and shift and shift as a result of a few outliers i don't get that i honestly don't get that phrase because i'm just trying to understand it p95 means your top 5% of your customers it's not a few outliers it it's the 4.99% behind the p95 that are slower right so if you see a shift in your p95 that means there's like people are hurting right 5% one out of 20 if top 5% is five users then it's an outlier well if top 5% is five users i'd have to think about that okay so the p95 is a the a number of requests and you have power users in which are distributing most of the requests that is that what they're trying to say so that the p95 could just be a couple users if you're if the top 95 is five users then your bottom 95 is 95 users if you have 100 users you don't really need percentiles right you want percentiles always so that's the thing i guess i'm a little bit confused on what does this mean are they saying like because one really good way to make sure that you're doing a good job measuring how users are affected is that you need to be able to take the uh you need to be able to take all of a users's experience and then take that individual user's experience and average it and then take the medians of the user's averages and why i think that is more important is that one user that does like 900 things becomes a singular data point that way you have a more reasonable distribution of what is happening for each user you can have better insights into each user uh i always found that to be a way better way to kind of look at these type of things you don't want to let one power user shift you know your stuff around just something to think about uh we've let's see we've since seen other instances in which p95 latency was pushed up because of a regression in one end point in particular while i was uh while i still use p95 latency as the simplest leading indicator it's crucial to attach it to a chart of p95 is grouped by endpoint yep fair good average average users no you want median average you want you want to do the median always median is good you don't want to just do averages across the board you only take average of the user's experience then then you look at the the quantiles of the averages then it's a little bit nicer i think in the tldr is you are a big client uh so we'll ignore your bad experience well yeah not the median you want quanti but you get the idea you get you get the idea all right p95 p95 by endpoint very interesting look at that look at that purple just blow up interesting interesting for an example you can spot the significant jump in p95 at just before 6 a.m. above is actually due to a spike in response time from the yellow endpoint i honestly can't tell i mean i see these spikes in yellow but i also see spikes in purple blue has been really spiky i i can't quite tell what he means by that now this on the other hand this oh this chart yes this chart seems very reasonable to look at this chart i can i can really see that things were happening and then all the things seem to happen but still i'm not sure how i don't really know what's happening because blue is low and then blue just just skyrockets but yellow's been up for a while been up for 8 10 10 hours on the other hand here's a clear example on thursday july 24th where uh there is a true sitewide regression with all the lines spiking uh each other let's see each color representing a different end point okay okay i still don't really i i don't quite understand the lesson here i don't understand the lesson i just think they're not necessarily measuring their p95 potentially well uh unsure watch out for the event loop blockers in particular big queries okay i like this this is this is good uh with the per endpoint graphs in place i started h uh honing in on the use cases where we'd see sitewide regressions and ass surge across many endpoints uh end points latencies simultaneously some of the regressions where the result of issues we were familiar with high uh database cpu and outlier number of requests servers running out of memory classic javascript on that last one i love that last one that's not a skill issue that's just a js issue and sort of a skill issue uh but for the other regressions all of our variables seem to hold at healthy levels despite a jump in latency in some of those cases we just see a quiet period where everything would look healthy then there'd be a few or no air logs but latency would still be uh skyhigh yeah you're blocking baby uh because the lack of concerning external variables or hypothesis was that there must be something occurring within nodejs process itself enter node event loop block ers oh interesting is there a specific list of event loop blockers don't block the event loop or worker pool well yeah but who's blocking i mean sometimes you have to do a lot of work i mean that's just a reality and that blocks uh the simple mental model here is that node is uh single threaded so when you do heavy synchronous compute you can block the process and thus the server from handling any other inflight requests yes data dog provides a basic outof thebox dashboard here uh though though granted it does have some weaknesses uh it also slows everything else you're doing down so if you're having problems with promises sl like crushing out stuff you also by the nature of observing make things difficult uh namely that once you get down to this service level you can't slice on a per pod basis despite each pod having its a separate node process with its own event loop you can't let's see you can't examine each pod granularly oh that's too bad i assume they can fix that that seems like a fixable issue here this means that a large event loop blockers may get hidden and averaged out if the other nine server pods at the time are doing fine you could also just i mean if you're just doing testing uh you could also just launch a bunch of single pod instances out there right i mean yeah you're being really wasteful with your cpu but for a little while you know be wasteful to try to catch a problem that's that's i think that's fair uh instead i suggested uh via ashby uh in the let's see in an extremely help ful blog post i found logging to be more fruitful approach i do like a good oldfashioned logging o uh as a side note we also tried profiling our cpu using linux perf tools as per netflix suggested let's go is this the yunong one uh i like this one but we uh wound up preferring ashby's approach for a couple reasons adws doesn't expose the underlying settings needed to use linux perf tools on ecs containers okay okay fair fair uh ashby's approach allows us to identify the specific request that block the event loop making it easy to replay uh set request with the exact details to recreate the blocking conditions this would be possible with plain flame chart okay that's fair that's really fair uh here here's what the end result of implementing this logging approach looked like oh very cool okay so you're able to replay these requests and you just put a bunch of logs in that effectively track all the logs and the time spent for each one of these you have like effectively a bunch of performance counters and you have a bunch of things summing it up and then you have the the related request coming in that's pretty cool uh the apm trace includes request start log which allows us to replay the request locally with the manage command that then spits out an associated curl command to replay the request and debug against a local server very cool that's super cool after debugging lots of requests in this manner there are a few common themes half of the event loops blockers were from intensive synchronous work that we were doing rendering react uh in this case we moved computation off the main event loop and into a worker thread the other half came from database queries specifically database queries that returned a significant amount of data when type omm received results back from the database it parses these raw results and transforms them into typed entities that we then interact with in our code base or are they using um is it some sort of input validation like zod uh ssr is not bad some ssr is bad uh not all not all ssr is created equally uh let's see because i'm curious about this because when it says that you have a typed omm is is there some sort of like zod like validation for this because zod is like definitely not known for its speediness now maybe it's gotten better i haven't looked at it in a while but zod is most certainly not fast uh when these raw results are large three megabytes or larger in our particular case the uh this parsing can be extremely expensive valid i verified this by cloning a type orm locally plugging it into a server and adding custom debug logs yes it's blazingly type orm does is od stuff yeah so yeah that's very very typically type validation runtime validation because you're not using a real typed language remember typescript is not statically typed it's a lint okay if you don't think of it if you think of it as a lint you will not be disappointed as much with typescript but the moment you think about it like it's typed like it's enforcing some sort of types no it's linting okay it's just letting you know when your types are out of place from each other that's it and so that means if you want to know you're using a real type you have to do a runtime check on it meaning that you have to crawl the entire object using object doc keys using whatever it takes to go through uh you know go through the whole thing and validation validate its key and its property as being the same thing uh it's just different you can't just use type of right because if you want to validate an object is of a certain shape has a few keys in it you have to go through it all so typically people will use type guarding uh as one of the better ways but i mean there's still a lot of oddities that goes through with type guard stting uh you can type guard not realizing a certain field became uh undefined that's a real thing right you don't realize a field can be undefined from some particular third party service or whatever and then you get this response back out it happens to be undefined you didn't know about it you blow up your server despite your typescript saying everything is good great and awesome so you know there's these weird little audities that will show up if you don't have perfectly correct types uh typescript just moves the problem from type checking in your code via runtime to typechecking via lint it's good i mean don't get me wrong i'm i'm fine with what they do it just still can be difficult it's not free is what i'm trying to say so this is why you run into these situations is that you are type checking i assume that's what typm is doing it's literally type checking against your data making sure that you've adhered to some sort of schema all right the results may be larger because uh we're quering for a large number of rows or because the joined data in the query type omm maps the join in the entity manager to left joins with uh which can result in significantly inflated number of rows hold on typ maps the joins and it's entity man manager hold on it's doing a left join in typescript is that what i'm seeing not free called it pick is that what i'm reading here pick pick am i reading that you're doing a join in not a database typ rm maps its entity managers to left join which can result in significantly inflated number of rows if there is a lot of relations between the query data oh no no it just does the joins itself it does the joint it it it does these left joints somewhere and then brings a bunch of extra stuff down is what i'm seeing okay it's probably that um this means that there are uh there were a few seemingly innocuous queries that turned out to be timing out the postrest serialization time not even event loop blocker related though the concept is the same because of all of the joined data [music] i'm impressed okay so it is it is it is joined in the db okay so that's good it's not doing something that i think prisma was doing prisma had the russ cin doing the joint itself which again i mean again just just a cautionary tale raw dog and squeal typically results in you knowing what happening squeal builders are like the next level of raw dogging maybe it's not as raw dogged maybe it's a little bit you know it's a medium rare it's a medium rare squeal query okay still good you still have a lot of control but at the end of the day you're not crafting the query yourself and then finally the last level is omms which mostly could be good right oms mostly good but sometimes you run into these odd situations okay so you got to know when to raw dog it okay because sometimes your queries come out medium well all right uh what is omm to be honest it's just like effectively your mapping things to like classes just imagine that's how how i like to imagine it is that you have a way to effectively manifest representations that you have from the database into your code and it gives you all the nice little things around it i think front end masters does a really interesting thing they're telling me about it which is they have a handcrafted squeal builder which only allows specific operations for each field meaning you can't just look up by any field in a table it has hardcoded each field which ones are indexed doesn't allow you to do that so when you want to add a new query you have to alter this like this kind of squeal builder to be like all right i want this new query that does this that way they have every last bit of queries very easy to represent in the system but highly optimized in their like back end right because they know the exact query every single time there's no messing around uh it's it's very very fast i love those kind of approaches because it's the it's the middle ground right typically you the dev you're not doing anything uh but everything is still raw but it's protected behind what feels like a a query builder so when you go all right i want from this table all of a sudden you only get like three fields you can select on so it kind of like reduces things down to what you can do oh you can't even sort this table do sort is not a method found on this table sorry you may not do that right like it it makes you think in a different way right it's a specific problem to a specific uh or it's a specific solution to a specific uh problem which i really like i think that's the best way to write any really good performance or really good application as a specific problem and a specific solution uh the more specific you can go the better index scan all the bbs you always index scan all the things at all times uh there are let's see there are some more there are some other interesting but niche f fixes as well as some of our uh pattern of mapping over a list of objects to generate promises blocking the event loop nice uh there could be lots of objects uh in that list or to creating promises took uh some non-trivial synchronous work constructing typo query builders interesting that a query builder would take non-trivial amount of synchronous work i'm confused by that i i must not understand how building a query would take anything more than like a handful of microseconds you know okay you have a slow machine maybe it takes a millisecond or two like it just that that's hard for me to understand this isn't uh this isn't a call to prematurely optimize which by the way again call out to casey premature optimization is the root of all evil is a misquote uh the the creators would just uh and canth would say hey we are talking about hand rolling assembly to make it faster than c don't hand roll assembly to make it to make it faster than c don't do that that's stupid but writing good code you should do that okay optimize your code just don't handroll assembly only do that when you really need to do that uh uh ssl you know i get it that tls whatever it is 1.3 or whatever it is does do that that's fine uh i wouldn't worry about the synchronous cpu intensive type uh of work off the bat but i would rather monitor it as it rolled out knowing that this might later have to move to a worker thread i would say that uh avoid promises you'll find a huge you'll just find a just a giant win uh there are also cases however where there was a logic i thought would be quite cpu intensive which turned out not to be so this is why proper logging and monitoring are crucial in a web application at scale absolutely 100% be very wary of large volumes of data return from databases it turns out that this not only has an uh an expense on the database level and sending it down to the client but can also affect other all inlight requests yeah absolutely this makes perfect sense because when you json code when you walk these objects and cast them into things when you verify or validate their shape at runtime you 100 and 10% spend all that time synchronously in javascript there is no anything else that can run and every time you create a promise remember anything however long it takes to resolve that promise constructor everything else behind it in the event loop gets to have that has to wait until that thing is done and so it's like it's very good to know these things exist uh from this investigation we were able to uh uh to let's see we're able to burning down a number of these event loop blockers however keeping an eye out on performance will continue to be required as we inevitably introduce new blockers absolutely lesson three everything has a cost but only the most expensive things have impact classic completely reasonable statement that you cannot argue with uh on the let's see let's see one of the first things i dug into when i was looking at performance was background fetching uh that our app did um we had noticed in data dog metrics that our server spent a lot of time evaluating these requests by server wall time this was a third highest end point but because of requests that uh that occurred in the background for users we weren't sure whether uh whether this was worth optimizing okay very interesting um interesting but if it's in node.js it doesn't matter that it's in the background for a user because if it takes time it takes resources and if it takes resources other services have to be slow or slowed down right i mean by the very nature we'll find out at the time we decided uh to experiment with unshipping the feature that relied on this background fetching curious to see the impact uh showed up anywhere after unshipping the server cpu and memory stayed at the same uh at the same and we saw dramatic drop off in the event loop delay crazy key takeaway here for me uh with and with much more of this performance work was that nothing comes for free even non-user face facing latency again at some point you have to process it at some point you're taking up resources at some point it has to get in line with other things you know what i mean like of course nothing's free i mean that's this makes perfect sense right you either have yeah you vertically scale you horizontally scale or you get rid of it or make it better for a while we've theorized that some of the large unpaginated endpoints we have are costly however we h haven't seen this show up in our top level performance metrics these queries don't seem to be using a disproportional amount of c cpu or memory or blocking the event loop more than the other endpoints so while we uh while we could staff a large effort to overhall these end points and iron out our pagination scheme i not yet confident that this trade-off will be worth it the so one thing to kind of say about this not all changes are worth it as the first line but making things fast mean you don't have to make them faster later when more things rely on it or at least making it good enough enough do you know what i mean like as you let things that you know aren't good sit more things grow on it which means it's harder to detangle that mess as time goes on you know almost always is it a good idea to try to write the right thing first that you know is going to be as efficient as you can write it that's not hyper ridiculously made right uh i do not like make it work make it right make it fast i've never liked that uh phrase i like to make it like i try to make things performance-minded first as i make the thing right i design my apis around it i think about where and how i'm allocating memory why i'm allocating memory can i reuse objects i just don't i don't first think about making the thing because often it becomes really difficult going backwards you know what i mean it's difficult going backwards and then making something fast because you've made a lot of tradeoffs in how you've designed it it can be difficult uh all right capturing performance trends after fixing many of our outliers and our latency graphs are much smoother and more consistent week over week for example uh consider the following uh time seri graphs that show the number of requests that took greater than 5 seconds greater than two seconds greater than 1 seconds the blue line shows the data from today and the orange tracking line shows the data from the same point from the week before oh nice some week over week graphs okay um it looks like wait is blue sucking more than orange in this situation i don't really see much of a win here let me just make sure i got that blue and blue and orange correct here um the blue line shows the data from today yeah i i don't know if i don't i i'm not sure if i'm seeing like huge winds all the time it's really hard to say that there's a wind you need to do more statistical rigor on this kind of win um you know what i mean you have to do a lot more statistical rigor on these type of things i would never eyeball a graph like eyeballing a graph only works if it's obvious you know what i mean like if if it's like one graph and there's a golf inet twix and then another graph you don't have to do much rigor on that one and be like yeah that's probably better we're like lower at all points by at least 20 milliseconds probably faster but if it's like right there i'd be careful i mean this just looks like an accent right here uh now when we see the performance regressions it becomes easy to con uh uh concretely identify in the grass below for example a regression around midnight immediately jumps out yeah there's like that that jump right there that must be a regression i i i just don't see any difference though look what do you think of these two images they're the same um anyways there's no midnight either uh previously we were using graphs to monitor latency across our entire application but uh the hope is that we can now use this approach to track performance trends on uh key end points the work continues uh through this work we have significantly improved our ability to understand where large performance regressions on the graphite apps are coming from and how to combat them i do like this you know the per endpoint kind of identification week over week graphs those kind of things they're actually super useful because when something does regress at least you know where and when it happened then you can attempt to fix it you know what i mean at least that's good identification is half the problem it is it is a very good thing uh i like that uh i i do like that i let's see understand how overall site performance is tracked tracking over time yeah that's also good historic historic is always really good uh it's really good to know that you're slowly doing it the hard part is that when you creep up slowly over time you you don't do larger regressions with more statistical rigor over time because then the problem becomes over two monon period did you slowly become slower i can fix her every developer uh yes this is th this is it uh while most of this work has centered around uh our understanding at the most extreme outliers we are continuously hard at work continuing to drive down endpoint lane sees across the board and more importantly locking our wins in all in all the this investigation the resulting fixes have greatly improved graphite's app server performance over the last six weeks and i'm confident uh these learnings will only help us continuing going forward you know what i'd love to see them do i would love to see them uh just simply isolate a server be able to run some dev tools on it and look at the performance or the performance have and just see how much time are they spending in garbage collection right like that's such an amazing thing to do is just looking at that also what node version are they on are they on node uh 16' 20 like very very different performance characteristics and note 16 is like intensely slower than note' which is still much slower than note 20 right so it's like there's there's some real wins there that would be great to see uh what like what's going on because i think that if you looked at garbage collection you can also find a huge number of reasons why uh individual requests or a set of requests spike for a while anyways the name is i really like i like this kind of stuff i think this stuff is great i love that they made graphs i love that they're tracking tracking is always the best first step i love they being proactive about it absolutely lovely uh these graphs i don't really buy you should really get some statistical analysis in here at uh at at the startup at places where i've worked we call this uh we call this automatic canary analysis or being able to kind of alert on trend differentiating some good stuff and so i just love to see this absolutely love to see this the name is the prime gen unlike unsubscribe report dmca call my mom do it all you got it a jen