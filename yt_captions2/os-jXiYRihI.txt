if you went to the grocery store, gave them your money, and left with 10% less vegemite than you paid for, you'd probably be pretty ticked off. but what if i told you that happens every day, except with $350 cpus? well, it's true. these 12 chips were obtained over a span of a month from 8 different sources spread across 3 separate countries. and the real-world difference between the best and the worst of them is 8% in factorio and as much as 12% in csgo. this has caused a lot of problems for us over the last 6 months. you see, in april, i gave the labs team the goal of increasing our testing throughput so that we could give you guys more juicy benchmarks in our reviews. unfortunately, the math was not in our favor. you see, the typical turnaround to troubleshoot, benchmark, write, film, edit, and qc our coverage of a new product is 5 to 8 business days. at about 5 minutes per test, times 3 runs for consistency checks, times however many products we want to compare, times however many games or applications you guys want to see, it is easy to run out of time. now, we could shorten each test, but we've found that longer tests are more consistent from run to run and end up being more reflective of the real-world experience of using the product. so, that's out. and i guess we could work through the nights like a bunch of adderalled out master students, but come on guys, we're not that young anymore. automation with markbench does help a lot, but if our system hangs in the middle of the night, which it happens sometimes, we're right back where we started, which leaves us with one real option, building up more of our test benches and running our tests in parallel, except as i already told you, 8% difference in factorio, 12% in csgo. now, when i signed the procurement authorization for 11 cpus, i knew that we were likely to find an outlier or two, but then it turned out that this rabbit hole went way deeper than i could possibly have known. like this deep segway to our sponsor, nexigo. whether you're in need of webcams or vr accessories, nexigo has products that'll make you nexigo. wow, those are cool. learn more about them at the link below. now, before you start sharpening your pitchforks and demanding that amd's executives be turned into thermal paste or something, it's worth noting that most of our chips were within a few percentage points of each other, even in csgo at 1440p, which was the test that saw the greatest overall spread. also, when we expand our comparison to include our full suite of games, that maximum difference falls to around 2.5%. far less outrageous, so you don't really have to worry about buddy in front of you in line getting a way better gaming experience for the same price, but still too big for us to buy any two random 7800x 3ds, use them to test two different gpus in parallel, and then say, well, these results should be comparable. now, the obvious conclusion at this point then is, guys, there's something wrong with your csgo test. i think it's time to get good, but the thing is, there aren't a lot of variables here, and that's by design. we used the same motherboard, same memory, same windows drive and install. we even tested using phase change thermal pads to ensure that our paste application wasn't an issue, and we chucked our bench in the thermal chamber just for good measure. we are very confident that our numbers are valid, and we're gonna have the process doc linked in the video description if you wanna have a look, which is all fine and good, but doesn't answer the much bigger question of why do these cpus vary so much in the first place? amd gave them the same model number and specifications, amd charges the same price, so they should have the same performance, right? well, back in the day that was true, cpus would run at their rated speed, and if they didn't, it meant they were either broken or they were about to be, but thanks to a relatively recent innovation called dynamic frequency scaling, that's no longer the case. you see, no two pieces of silicon are the same, and whether it's through rolling improvements to manufacturing or just sheer blind luck, you can end up with a processor that is capable of better than the advertised clock speed. now, in the old days, you could unlock this extra performance manually through overclocking, but nowadays processors just adjust their own speeds, and they do it on the fly based on a whole host of factors, including user configurable power profiles and thermal limits. amd's approach is to allow the cpu to clock as high as it's able until the cpu die average reaches about 90 degrees celsius, at which point the clocks will be dialed back until it reaches equilibrium. sounds good, right? i mean, why be bound by some artificial performance limiter when i got a golden chip that can go higher? and i actually agree, but for the folks who end up with a lesser chip, can feel a little bit like missing out, even if amd is careful to only guarantee clock speeds that 100% of the chips can hit. and as i mentioned before, it's also very inconvenient for our parallel testing endeavors. so what do we do? testing. lots of it. after throwing cinebench at our very first cpu, we ran into our very first roadblock. the numbers from run to run can be vastly different. i am talking 300 point spreads on a single cpu run back to back. what the heck, right? how on earth are we supposed to narrow down which two cpus are within 1% of each other if one cpu isn't within 1% of itself? as it turns out, software was the culprit and software is notoriously hard to account for. have you ever opened up task manager right when you boot up your system? there's no programs running, nothing should be happening except wrong. what was that? here's the thing. even when you're doing nothing, your operating system is busy managing all the behind the scenes work that keeps your system running, like updating the weather widget, synchronizing the clock with a trusted time server, prepping the next thing it thinks you might need, installing updates and so much more. and we don't really get to decide when that stuff happens, which means that no one result can ever be taken as gospel truth. we do have custom windows images that are intentionally debloated to remove some startup processes to help with this, but it only partially mitigates the issue and it introduces new ones, like making our results slightly less representative of the typical user. we feel this trade-off is worthwhile because it helps us to better isolate our variables in testing, but it's not even enough. to further mitigate the amount of work that windows is doing in the background, we can also increase a process's priority. in cinebench, we went from seeing points varying in the hundreds down to the tens on the same cpu. that's a big improvement and enough to use cinebench for our binning process, but we're not out of the woods yet. you see, with some tests, it's not enough to use the same hardware at the same process priority because the benchmarks themselves have built-in inconsistencies. red dead redemption 2, for example, simulates physics and ai behavior during a bench run. that's a really good thing because if that stuff was canned, our results would not be comparable to actually playing the game. but the bad news is it means that sometimes arthur loses his hat, sometimes he doesn't, sometimes the horse gets shot, sometimes it doesn't, which can impact our run-to-run consistency. can we ever fully account for this? unfortunately not, but by running each test multiple times and then taking an average, we can get a pretty good picture and we can bake that expectation of noise into our data analysis, which finally happens now. sorry for all the preamble. first up, gaming. for the sake of legibility, we named each of our samples after a pokemon. why pokemon? i don't know, because it seemed better than deadly diseases. anywho, looking at the geometric mean of our gaming results, we found a 2.07% spread in average frames per second between the best performing and the worst performing cpus and a 2.46% spread in our 1% lows. this puts all but one of our cpu samples within three frames per second of each other, which gives us confidence that we'll be able to find some close enough cpus, but given that 2.46% isn't 1%, it also tells us that we can't just pull any three chips at random, nor can we simply look at the average. returnal, for instance, is a benchmarker's dream because a, it's actually a good game that people might want to play, and b, it is a stunningly consistent benchmark, which is great for producing results that we can trust when we're comparing gpus. but the real world is a lot messier than returnal, and while most of our other games, both at 1440p and 1080p, showed a similar small level of variance in cpu performance, in a couple of games, notably total war, warhammer iii, and cyberpunk, we found larger variance in the 1% lows. this indicates that, as run, these games are more cpu bottlenecked, which better reveals the deficiencies of our worst chips, but as you're about to see, not all cpu-bound games are bound in the same ways. we went into this process thinking, ah, csgo, what a classic cpu gaming benchmark. it's a shame that it's been replaced by cs2, and we came out of it thinking, ah, cs good riddance. i mean, on the one hand, it does certainly separate the cpus from each other and our slowest chip, corsola, was the slowest-est in csgo. but on the other hand, the overall variance is so high and so different from the entire rest of our test suite that it becomes almost an outlier data point, having an outsized impact on our results. and this could be for a number of reasons. first, csgo uses a game engine that is older than youtube, which has been useful over the years since it was originally built just for single-core cpus, and it can make use of just about all the single-threaded performance that you can give it. but it also means that its performance requirements just aren't very similar to more modern games that are gonna wanna see a number of fast cores rather than just one or two. second, csgo itself is also old, old enough that any modern gaming cpu can run it so fast that no professional esports gamer even could tell the difference anyway, and so fast that limitations in the software itself can start to rear their ugly heads, which adds potential variables. basically, csgo is having its quake iii arena moment. after a long run, it's time to drop it. and when we reviewed the overall variance numbers without csgo, it shows just how much of an outsized influence that it had on our results. the new results show far less variance, closing the spread to just 0.46% and 1.43% for the average and 1% lows respectively, which is somewhat reassuring for you, the consumer, but still doesn't change that our runaway loser, corsola, is still a dud. corsola consistently underperformed the rest of the chips by so much that when we remove it from our results, our overall spread and performance goes from 1.43% in our lows to 0.86%. that is a massive decrease. so what the heck is wrong with this thing? we don't know for sure, but one guess is that the 3d vcache on this chip could be struggling some way because it fumbles pretty hard in our factorio test where most of the benchmark can actually fit on that 3d vcache. another possibility is that it could be the pcie controller, the part of the cpu that communicates with our pcie lanes and consequently our gpu. this idea comes from the fact that when it comes to productivity performance, sure, it still ain't top of the class, but it isn't flunking like it used to. speaking of, we actually found greater variance between our chips in our productivity tests, which kind of makes sense since we no longer have the gpu getting in the way of raw cpu performance. 7-zip brought us a spread of around 3-4% for compression and decompression and blender hovers in the same realm along with our video and audio encoding suites. the biggest contributor to the size of the spread of our sample though is lugia, who takes up the bottom spot in pretty much every productivity benchmark. since it wasn't so bad in gaming, this leads us to believe that perhaps there's a problem with the integrated heat spreader, but amd has made that much more difficult to evaluate now that all of their cpus just kind of run at the same temperature and then adjust their clock speeds to reach their thermal limit. so across our small sample, variance and performance is present, but not egregious. of course, we aren't trying to quantify variance. what we're trying to find is equivalence. so how do we do that? it turned out to be a bit tricky. we ended up using euclidean distance to determine which cpus were the most similar, unconventional, but also kind of cool. here's how it works. first, we scaled our data so that our five digit cinebench scores don't overshadow those low flacking code numbers. then we took those scaled numbers and treated each as a coordinate for a point in multidimensional space. think about it kind of like this. if we took a plane and chose two points, those would each have an x and a y coordinate. well, the euclidean distance is the distance between those two points. the closer together the points are, the more similar they are. and this can be applied for points that exist in any dimension. in our case, a 12 dimensional space for productivity and the 19th dimension for gaming. since we're weighing all of our tests as equal, we can then do a bunch of comparisons to determine which cpus are the most similar to one another. from that, four emerge as extremely comparable. ev, mewtwo, raiku, and zapdos, with zapdos being the least equivalent. so, sorry bro, just the other three. they are outside of our tolerance in csgo. but the issue is that the cpus that did perform identically in that one game were not the tightest across the rest of the suite, meaning that they can't really be trusted on games that, you know, you might actually be able to play. in conclusion, productivity saw these cpus perform within roughly 0.24% of one another. and in gaming, we see a spread of 0.86% in the 1% lows and just 0.1% in average frame rates. now that's tight. tight enough, we figure that it will allow us to directly compare gpu results across our test benches. wait, benches? oh yeah, you see where i'm going with this. we found near identical cpus, but what about the other components? do they vary? time for another round of testing. the main secondary performance contributors in your gpu test bench are going to be your motherboard and your ram. but since those still run at fixed clock speeds, we're not expecting nearly as much variance. with ram, for example, you set the speeds in your bios and then it's either capable or it's not, and your system is unstable and probably crashes. all of our testing is done at the recommended ram speed from amd, 6,000 megatransfers per second. and if you want to learn even more about how we test our hardware, we've got a recent exclusive over on floatplane.com where we have a feature length deep dive looking at the improvements we've made to our testing processes. anyway, to validate our hypothesis, we took one of our future test cpus, ev, and threw it into both of our new parallel benches. in gaming, we landed on 0.45% variance in the 1% lows and less than a 10th of a percent in average fps. that is more than acceptable. and in productivity, we ended up in the 0.13% neighborhood. that means in our upcoming gpu reviews performed on these three parallel benches, we're gonna consider our results to be accurate within plus or minus about 0.25%. of course, that doesn't mean that our results will be identical to your cpu or to other media. and this is one of the big reasons that we have always encouraged our viewers to look at reviews from multiple outlets whenever making a purchase decision. oh, before you ask, by the way, there does not appear to be any foul play from amd with respect to review sample selection. so you don't have to pick a reviewer, for example, that buys their own cpus versus one that gets seeded. at least we don't think so. we'd have to buy hundreds of cpus to know for sure, but it appears that the unit that was sent to us for review, which is raiku, falls somewhere in the good but not exceptional range. so i think we can put that conspiracy theory to rest. another before you ask is, yes, driver updates, operating system updates, and new software that we add to our test suite could change our cpu performance spread in the future. and we're gonna do our best to maintain our data integrity by performing periodic, what we're gonna call equivalence checks, because you guys have asked for reliable, trustworthy information, and you deserve it, which brings us to a big issue. why is this task falling to random youtubers? i mean, the automotive industry, for instance, has government bodies that are dedicated to verifying the performance of vehicles and ensuring that companies aren't cheating on their testing. then they dole out big fines when they inevitably do cheat on their testing. with computer hardware, there's no such oversight. we and our peers are this thin, open-mouthed thumbnail line between you and getting ripped off. and that's a big problem. i mean, for one thing, we don't have access to the types of testing that large tech companies have, and we don't operate at the kind of scale where we can say for sure if an observation is a fluke or if it's the result of conniving suits that are trying to save a quick buck. even buying 11 chips for this investigation, that was a huge investment from our side and not the sort of thing that we can do with every single review. unfortunately, all i'm doing is ranting right now. i don't have a solution to this other than, well, we're gonna keep trying, gosh darn it. but it just struck us as we worked on this project that the fact that these companies don't have to report things like estimated performance in a regulated and standardized fashion is kind of crazy, especially if you consider the kind of money that they're asking for their most expensive cpus. so what's next? well, first is gonna be going through the exact same rigmarole with however many 4090s it takes to parallelize our cpu test platforms and then slowly, but surely we're gonna be improving our automations and increasing our test volume, especially once we get the lab's website up and running. but good things take time and we aren't going to rush a good thing. especially, i'm not going to rush this segue to our sponsor. delete.me, your personal information sounds like it should stay personal, right? i mean, it's right there in the name. well, data brokers and other sketchy companies disagree. so they're sharing your data online like it's a family style dinner. eat, eat your skin and bones is what they're saying to each other. thankfully, delete.me is here to crash the party. they'll find out who's spilling your info and get it removed so that scammers can't use it to batter you with robocalls and spam emails. nevermind that it can also lead to fraud or identity theft because delete.me can mind that for you. now, wiping out data held by hundreds of sites by yourself sounds borderline impossible, which is why this whole time i've been trying to tell you that delete.me can do it. you don't have to. their nifty software and expert squad can sweep it away in minutes, not hours. delete.me averages over 2000 pieces of personal data gone for a customer in their first two years. yeah, go on delete.me, get them. and you should get on over to the link below and use code ltt for a sweet 20% off. if you guys enjoyed this video, why not check out our motherboard turbo nerd edition video where we went into what exactly are all those little things that look like cities and towns on the pcb?