uh how discord stores trillions of messages this is actually pretty exciting let's see let's find out i'm actually curious about that i i have not read this this seems exciting can we all agree this could be exciting in 2017 we wrote a blog post about how we store billions of messages uh checkmate atheists we shared of our journey and how we let's see started out using mongodb and then migrated to cassandra because we were looking for databases that were scalable fault tolerant and relatively low maintenance we knew he'd be growing and we did okay good i like it we wanted a database that grew alongside us but hopefully its maintenance wouldn't grow alongside our storage needs unfortunately we found that not to be our case our cassandra clusters exhibited serious performance issues that required increasing amounts of efforts just to maintain that improve i would love to hear more about that because you know i don't know a lot about cassandra other than it's like that eyeball with like a little for its logo but that's all i know about it netflix uses cassandra for some stuff or did maybe they've changed i have no idea i've never actually used cassandra it could be fun though eventually consistent i feel like you know that programmers are humans too channel where they do like interview with the senior js person it'd be great to have a database guy on there and he just says the term eventually consistent like 17 times great almost six years later we've changed a lot and how we store messages as well okay our cassandra troubles well look at that looks like he looks like it was right there the whole time i should have just read forward a little bit all right we store messages in a database called cassandra messages as the same suggests it ran cassandra and stored messages thank you for thank you for saying that to me i'm just happy i read that out loud in 2017 we ran 12 cassandra nodes storing billions of messages at the beginning of 2022 we had 177 nodes with trillions of messages to our chagrin it was a high toil system you know what never thought in 10 years that reason i'd read this sentence honestly if you asked chad gpt to say what what sentence would prime not think of it wouldn't even come up with this to our chagrin it was a hot toil system this sounds like something somebody would write in like the 1700s or on call team was frequently paced for issues with the database latency was unpredictable and we were having to cut down on maintenance operations that became too expensive to run all right what was causing these issues first let's take a look at our messages all right oh this is exciting uh the cql statement above is a minimal version of our message schema every id we use is a snowflake oh gosh we all know no one likes snowflakes okay nobody likes snowflakes they're always complaining always making a chrono chronologically sortable we partitioned our messages by the channel they're sent in along with a bucket which is a static time window this partitioning means that in cassandra all messages for a given channel and bucket will be stored together and replicated across three nodes or whatever you've replica set the replication factor okay within this partitioning lies the potential performance pitfall a server with just a small group of friends tends to send orders of magnitude fewer messages than a server with hundreds of thousands of people it's true it's true statement can't argue it honestly uh and cassandra reads are more expensive than rights i didn't know that it kind of seems like this would have been an obvious issue going on here rights are appended to a commit log and written to an in-memory structure called a mem table that is eventually flushed to disk reads however need to query the mem table and potentially multiple ss tables on disk files a more expensive operation lots of concurrent reads as a user interacts with servers can hotspot a partition which is referred to imaginatively as a hot partition okay personal pet peeve i never correct somebody on english but i'm not going to lie to you the period outside the quotation marks drives me nuts okay i'm like the worst at english in writing but when i know something like that should happen you should be in you should feel bad about yourself uh the size of our data set when combined with the access patterns led to struggles for our clusters um it is the worst english rule it is it pains me every time but i do it i do it every time and i hate it all right when we encounter a hot partition oh daddy daddy loves odd partition [ __ ] uh it frequently affected latency across our entire database cluster the one channel in bucket pair received a large amount of traffic and latency and the node would increase as the node tried harder and harder to serve traffic and fell further and further behind what a try hard don't be a tryhard everyone knows not to be a tryhard other queries to this node were affected as the node couldn't keep up since we perform reads and writes with quorum consistency levels all queries to the node that serve the hot partitions suffer latency increases resulting in bro broader end user impact i feel like they've already said this are we hearing the same thing again is that what is happening here what's going on here cluster maintenance tasks also frequently cause troubles we were prone to falling behind on compactions where cassandra would compact compact compact i think you can it's called compact i somehow tried to make it into some sort of like action incorrectly i put the wrong emphasis on the wrong syllable um for more performant reads not only are reads but then more expensive but we'd also be seeing consent or cascading latency as the node tries to compact wait compact compact i am reading this i have just i've reached like visual satiation on the word compact uh we frequently performed an operation we called the gossip dance where we take a note out of the rotation to let it compact compact compact without taking traffic bring it back in pick up the hints from cassandra's hinted handoff and then repeat until the compaction backlog was com emptied we also spent a large amount of time tuning the jvm's garbage collector and heap settings because gc pauses would significantly or would cause significant latency spikes all right come on i hope it's rust is it rust our message cluster wasn't our only cassandra databases we had several other clusters uh and exhibited similar uh though perhaps not a severe false okay okay this is getting exciting people here we go this is where we're gonna mention rust here we go i'm ready for some rust in our previous iteration of this post we mentioned being intrigued by sila is that pronounced sila or skila or is it sila uh db a cassandra compatible database written in c plus plus i am visibly agitated right now this is a plot twist no one wanted okay no one wanted this plot twist all right it's promise of a better performance faster repair stronger workload isolation via its shard per core arc architecture you know when i first heard the term shard i thought the person said shart and i just felt a lot of feelings and i was trying not to laugh and i was in a meeting and you know but somehow more disturbed the fact that i just heard c plus plus okay i just heard c plus oh gosh a garbage collection free life sounds quite appealing although sila is the definitely not void of issues it is void of garbage collector since it's written in rust minus minus rather than java historically our team has many issues with the garbage collection on cassandra from gc pauses affecting latency all the way to super long consecutive gc pauses that got so bad that on that an operator wouldn't have to manually reboot and babysit the node in question back to health damn imagine being on call at one in the morning and you are restarting a cassandra node in production oh imagine how bad that would be waiting for it to start back up do all the sinking oh my goodness these issues were a huge source of on-call toil there's that toil word again uh and the root of many stability issues within our message cluster after experimenting with sila db and observing improvements and testing we made a decision to migrate all of our data all of our databases while the decision could be a blog post in of itself the short version of that by 2020 we had migrated every database but one to silo db the last one our friend cassandra messages that [ __ ] how dare you cassandra messages uh why henley migrated yet well to start with it's a big ass cluster what some people call that a cluster that's what i would have said right there okay uh with trillions of messages and nearly 200 nodes any migration was going to be an involved effort additionally we wanted to make sure our new database could be the best it could be with as we worked to tune its performance we also wanted to gain more experience with silo db and production using it in anger and learning its pitfalls what what the hell did i just read what's using something in anger can someone just tell me like normally i use cobalt as like my thing i use in anger um i can't say is this what is this anyways we wanted the to gain more experience with silent db in production using it in anger and learning its pitfalls oh my goodness we were suspicious that slapping a new database on our system wasn't going to make everything magically better hot partitions can still be a thing in silo db and so we also wanted to invest in proving our systems upstream of the database to help shield and facilitate better database performance okay okay with cassandra we struggled on hot partitions high traffic to a given partition resulted in an unbounded concurrency leading to cascading latency in which subsequent queries would continue to grow in latency if we could control the amount of concurrent traffic to hot partitions we could protect the database from being overwhelmed i have no idea what they're even trying to say right now all i know is i'm getting hot and bothered by it uh let's see to accomplish this task we wrote what we refer to as data services intermediary services that sit between our api monolith and our database clusters please tell me you called a cluster when writing our data services we chose languages that we've been using more and more at discord rust let's go all right well we knew we knew it we knew we were going to get rust we knew it we could feel it everybody come on clappies let's go clappies clappies for the rest we've used it for a few projects previously and it lived up for the hype for us it gave us as fast c plus speeds without having to sacrifice safety or say mental happiness of your employees hey kids you're gonna go from java what to c plus plus they're like what what i don't feel i'm not sure things are getting better here at discord at the current moment i'm glad at least there's rust in here at least there's rust for a little bit but literally they're going for cassandra to a c plus okay rust house fearless concurrency i when i hear the term fearless concurrency i just want to just like drop kick you know you can still screw it up i had so many times i deadlocked myself in tokyo because i didn't know what i was doing okay it's not fearless you still have to know things right uh the first language should make it easy to write safe concurrent code it's library also a great match for what we are intending to accomplish i love tokyo is a tremendous foundation for building a system on asynchronous io hey by the way can everybody in here i saw that flip edits was in here flip could you say something for a quick second clip flip edits flip edits could you say something for a quick second flip hey flip hey flippity flip could you speak up for a quick moment okay hey flip now that i have you here hey flip remember that one time we had this idea for a t-shirt that said tokyo what happened flip where did it go did you drop the ball everyone's disappointed right now flip okay we could have all had tokyo t-shirts on we could have had tucked talking we could have been screaming tokyo together instead look at us look at us additionally in front of the uh a joy to code in with the help of a compiler gives you the claire dude i love this we have got we have gone from a database problem into the joys of rust i kid you not people who like rust i ca just every single time no matter what you're talking about we've managed to take the conversation that is about databases and the problem with the c plus plus database and god from that into russ the greatest programming language of all time oh what are you doing over there oh you're using javascript have you thought about rust let's see anyways um the language constructs and its emphasis on safety we became quite fond of uh of once let's see of how once it compiled it generally works most importantly however it lets us say we rewrote it in rust yes yes yes let's go oh this is so good this is everything i've ever wanted in an article our data services sit between the api and our sila clusters they contain roughly one grpc endpoint which grpc is historically one of the worst performing ways to communicate with a rust thing right doesn't ser day saturday have some sort of problem with grpc and all that isn't it like really poor it's because you have to have an underlying source if i remember correctly uh it is yeah if i remember correctly i can't i don't know why but i know it's like really bad uh endpoint per database query and intentionally contains no business logic the big features are data services provide is requesting coalescing if multiple users are requesting the same row at the same time we only query the database once the first user that makes a request causes a worker test to spin up at the service subsequent requests will check for the existence of the test and subscribe to it that worker task will query the database okay this makes perfect sense do you know why this makes sense it makes such good sense because you can imagine that if we were to jump right now into discord right last time i did this someone posted a peen so we're going to be quick here and we say something hi mom so many people at the exact same time are gonna see this thing from happening and so many people going there right now are going to see this thing happening so therefore what happens when we do this right which is well a lot of people are causing reads if a lot of people cause reads why query the database 900 times right it makes perfect sense this is great this is a great great thing right here this is the power of rust in action it made it easy to write safe concurrent code let's go rust oh it feels good to be a rust asian doesn't it oh man you and your pathetic c plus plus or rust minus minus or whatever we call it and java right seriously i'd rather write javascript than java uh let's imagine a big announcement on a large server that notifies at everyone users are going to open the app and read the messages sending tons of traffic to the database boom i called it called it ad everybody called it previously this might lead to a hot partition an on-call would potentially need to be page to help the system recover with our data services we're able to significantly reduce traffic spikes against the database the second part of the magic here is the upstream of our data services we implement a consistent hashing based routing to our yes yes dude i love this okay this is so good consistent hashing for routing is so beautiful we don't use it at netflix which i i honestly think that if we started using this we could save a bajillion dollars hash routing is amazing i think uh at one point it was called ring pop i don't know what they call it now but that's effectively the same idea for each message a request to our data services we provide a routing key for messages this was a channel id so all the requests for the same channel go to the same instance of the server yes yes this routing further helps reduce load yes yes these improvements help a lot but they don't solve all of our problems we're still seeing hot partitions and increased latency on our cassandra clusters just not quite as frequently it buys us some time that we can prepare our new optimal scila cluster and execute the migration let's go people are you ready for it our requirements for our migration are quite straightforward we need to migrate trillions of messages with no downtime and we need to do it quickly because while the cassandra situation has somewhat improved we're frequently firefighting step one is easy we provision a new silent cluster using our super disk storage topology by using local ssds for speed and leveraging raid to mirror our data to persistent uh to persistent disk we get the speed of attached local discs with the durability of persistent disk with our cluster stood up we can begin migraine gosh that step one does not sound easy that sounds like a lot of effort that sounds like years of engineering went into that step one simple we're just gonna do magic okay sure ten years ago people would have never thought to do such a thing but today easy magic step one make it magic local speed persistent data storage raid right okay our first draft of our migration plan was designed uh designed to get value quickly we started using our shiny new sila db cluster for new data using cut over time then migrated historical data behind it okay this that seems like the right move though honestly that seems like the right move and you're fine doing slower requests because within a small period of time people don't i assume the amount of messages being viewed beyond it like a certain time frame gets really small right uh it adds more complexity but what every large project needs is an additional complexity right absolutely but that's why i use rust uh we begin dual writing new data to cassandra and silo db and can currently begin provisioning sila uh db's spark migrator uh it requires a lot of tuning and once we get it set up we have estimated time to completion three months yeah a trillion's a lot you forget how much a trillion is right people forget that if you're doing a trillion from one large db service to another it is like insane the amount of time that time frame doesn't make us feel warm and fuzzy inside and we'd prefer to get value faster we'd sit down as a demon brainstorm ways we can speed things up until we remember that we've written as fast and perform a database library as we could potentially extend we neglect to engage in some meme driven engineering and rewrite the data migrator in rust no shut shut the [ __ ] door no way are we doing it in an afternoon we extend our data service library to perform large-scale data migrations it reads token ranges from the database checkpoints them locally via a squeal light and then fire hose them into silo db we hook up our new and improved migrator and get new estimate nine days let's go oh my goodness rust is coming in again oh this is too good it's too good it's too good if we can migrate data this quickly then we can forget our complicated time bros to a time based approach instead flip the switch for everything at once we turn it on and leave it running migrating messages at the speed of 3.2 million per second several days later we gather to watch it hit 100 and then we realized it's stuck at 99 and a bunch of other nines complete no really our migrator is timing out reading the last few token ranges of data because they contain gigantic ranges of tombstones that were never compacted away in cassandra ha ha ha ha ha you don't mess with oh no oh no when you don't measure things by the space they take but by like how many have you ever done that with git you download something it's like 20 40 60 80 percent 81 and you're like dude i only have five left 85 you're like what the hell is happening and then it just like just stops because it can't count the size it can only count like how many units there are oh ah this is a beautiful problem our migrator is timing out reading the last few token ranges of data because they contain gigantic ranges of tombstones that were never compacted away in cassandra we compact the token range then seconds later the migration is completed we performed automated data validation by sending a small percentage of reads to both databases comparing results and everything looked great the clusters held up as well with full production traffic whereas cassandra was suffering increasingly frequent latency issues we gathered together as a team on site flip the switch to make silo db the primary database and eight celebratory cake i hear cakes a lie okay personally i've heard that i've heard this several months later let's hear this is where everything went bad because they're going to rewrite it and come on they're going to rewrite it they're rewriting it rust here we go we switched our message to messages database over in may 2022 but how's it held up since then it's been quite well-behaved database it's okay to say this because i'm not on call this week we're not having weak and long fire fights nor are we juggling nodes in a cluster to attempt uh to preserve uptime it's much more efficient database we're going from running 177 plus cassandra nodes to just 72 silo db notes man the power of garbage collection this is actually super cool the power of garbage collection is out of this world each style of db node has nine terabyte of disk space up from the average of four terabyte on cassandra node wow our tail latencies have improved drastically for example fetching historic messages had a p99 of somewhere between 40 to 125 milliseconds on cassandra with sila having a nice and chill 15 millisecond p99 she that is out of control and message insert performance has gone from 5 to 70 milliseconds p99 and cassandra to a steady five milliseconds on p wow dude that is insane thanks to the aforementioned performance improvements we've unlocked new products use cases now that we have confidence in our message database wow at the end of 2022 people all over the world tuned in to watch the world cup one thing we discovered very quickly was our goals scored showed up in our monitoring uh graphs this was very cool because not only is it neat to see real world events show up in your systems but this gave our team an excuse to watch soccer during the meetings we weren't watching soccer during meetings we were proactively monitoring our system's performance that period made it inside the quotes people you see that that one made it inside but look at that that's actually that's super cool you can see the goals being scored ah i love it we actually tell the story of the world cup final via our message uh send graph the match was tremendous uh lionel messier i was uh let's see it was trying to check off the last message accomplishment or the last accomplishment in his career and cement his claim to being the greatest of all time and lead argentina to the championship but in his ways to the massively talented ezekillian kylian kylian i don't know how to say this uh um parlez-vous francais i don't watch soccer so i don't know any of the soccer names it's football i don't watch football okay i don't uh each of the nine spikes in this graph represents an event in the match messe hits a penalty argentina goes one one zero let's go argentina anytime argentina can beat france i'm i'm in for it uh argentina scores again let's go it's halftime they're sustained 15-minute plateau as users chat about the match okay yeah i can see that the big spike occurs when i'll just call him bappy when bappy scores for a parlay buffon say it scores again in 90 seconds later to tie it up damn the back to back it's the end of regulation this is a huge map or mash is going to extra time not much happens for the first half of the time but we reach the end users chatting messi scores again and argentina takes the lead bappy stripes again ties it up it's the end of the extra time we're heading to penalty kicks excitement and stress grows throughout the shout out until france misses and argentina doesn't uh argentina let's go i never even watched this this is fantastic this probably was a lot of fun to watch that was cool people all over the world stressed watching this incredible match but meanwhile discord and the message database aren't breaking a sweat we're way up on messages send and handling it perfectly with our rust-based data services let's go rust and silent eb we're able to shoulder this traffic and provide platform for our users to communicate we've built a system that can handle trillions of messages and if it works and if this work is something that excites you check out our career page check it out if you like rust okay uh by the way this bow fella right here hey bo i would just like to let you know personally that i genuinely enjoyed reading this article it was great and i'm very happy that you rewrote it in rust honestly made me happy everyone give them a clap give them a clap give boat clap say nice job bo w for bo