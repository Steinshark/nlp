- when i say we store
and handle a lotta data for a youtube channel, i mean it. i mean, we've built some'n sick, hundred plus terabyte servers for some of our fellow youtubers, but those are nothing compared to the two plus
petabytes of archival storage that we currently have in
production in our server room that is storing all the footage for every video we have
ever made, at full quality. for the uninitiated, that is over 11,000, warzone
installs worth of data. but with great power comes
great responsibility, and we weren't responsible. despite our super dope hardware,
we made a little oopsie that resulted in us
permanently losing data that we don't have any backup for. we still don't know how much, but what we do know is what went wrong and we've got a plan
to recover what we can, but it is going to take
some work, and some money, thanks to our sponsor, hetzner. hetzner offers
high-performance cloud servers for an amazing price. with their new us location
in ashburn, virginia, you can deploy cloud servers
in four different locations and benefit from features
like, load balancers, block storage and more. use code ltt22 at the
link below for $20 off. (upbeat music) let's start with a bit of
background on our servers. our archival storage is composed of two discrete glusterfs clusters. both of them spread across two
45drives storinator servers, each with 60 hard drives. the original petabyte project, is made up of the delta
1 and delta 2 servers, and goes by the moniker old vault. petabyte project two, or the new vault is delta 3 and delta 4. now, because of the nature of our content, most of our employees
are pretty tech literate with many of them even falling into the tech wizard category. so, we've always had
substantially lower need for tech support than the average company. and as a result, we have never
hired a full-time it person, despite the handful of times,
perhaps including this one, that it probably would have been helpful. so, in the early days, i
managed the infrastructure, and since then i've had some
help from both outside sources, and other members of the writing team. we all have different strengths, but what we all have in common is that we have other jobs to do, meaning that it's never really been clear who exactly is supposed to be accountable when something slips through the cracks. and unfortunately, while obvious issues like, a replacement power cable and a handful of failed
drives over the years were handled by anthony, we never really tasked anyone with performing preventative maintenance on our precious petabyte servers. a quick point of clarification before we get into the rest of this. nothing that happened as
the result of anything other than us messing up. the hardware, both from
45drives and from seagate who provided the bulk of what makes up our petabyte project servers, has performed beyond our expectations and we would recommend
checking out both of them, if you or your business has
serious data storage needs. we're gonna have links to them down below. but even the best hardware
in the world can be let down by misconfigured software. and jake, who tasked himself with auditing our current infrastructure, found just such a thing. everything was actually going pretty well. he was setting up monitoring and alerts, verifying that every machine
would gracefully shut down when the power goes out, which happens a lot here for some reason, but he eventually worked his way around to the petabyte project servers and checked the status of
the zfs pools or zpools on each of them. and this is where the kaka hit the fan. right off the bat, delta 1 had
two of its 60 drives faulted in the same vdev. and you can think of a vdev, kind of like its own mini raid array within a larger pool of
multiple raid arrays. so, in our configuration
where we're running raid-z2, if another disc out of our 15 drive vdev was to have any kind of problem, we would incur irrecoverable data loss. upon further inspection,
both of the drives were completely dead, which does happen with mechanical devices and had dropped from the system. so, we replaced them and let
the array start rebuilding. that's pretty scary, but not
in and of itself a lost cause. more on that later though. far scarier was when delta 3, which is part of the new vault cluster had five drives in a faulted state with two of the vdevs
having two drives down. that's very dangerous. interestingly, these drives
weren't actually dead, instead, they had just faulted due to having too many errors. so, read and writers like this are usually caused by a
faulty cable or a connection, but they can also be the
sign of a dying drive. in our case, these errors
probably cropped up due to a sudden power loss or due to naturally occurring bit rot, as they were never configured to shut down nicely while on backup power, in the case of an outage. and we've had quite a few
of those over the years. now, storage systems are usually designed to be able to recover from such an event, especially zfs, which is known for being one of the most resilient ones out there. after booting back up from a power loss, zfs pools and most other raid
or raid like storage arrays, should do something called
a scrub or a re-sync, which in the case of zfs means that every block of data gets checked to ensure that there are no errors. and if there are any errors, these errors are automatically fixed with the parity data that
is stored in the array. on most nas operating systems, like truenas, unraid or any pre-built nas, this process should just
happen automatically. and even if nothing goes wrong, they should also run a scheduled
scrub every month or so. but our servers were set up by
us a long time ago on centos and never updated. so, neither a scheduled nor
a power on recovery scrub was ever configured. meaning the only time data integrity would have been checked on these arrays, is when a block of data got read. this function should theoretically
protect against bit rot, but since we have thousands of old videos, of which a very, very small portion ever actually gets accessed, the rest were essentially
left to slowly rot and power lost themselves
into an unrecoverable mess. when we found the drive issues, we weren't even aware of all this yet. and even though the five drives
weren't technically dead, we erred on the side of caution and started a replacement
operation on all of them. it was while we were
rebuilding the array on delta 3 with the new discs, that we started to uncover the
absolute mess of data errors. zfs has reported around 169 million errors at the time of recording this. and no, it's not nice. in fact, there are so
many errors on delta 3 that with two faulted drives
in both of the first vdevs, there is not enough parity
data to fix the errors. and this caused the
array to offline itself to protect against further degradation. and unfortunately, much
further along in the process, the same thing happened on delta 1. that means that both the original
and new petabyte projects, old and new vault, have suffered
nonrecoverable data loss. so, now what do we do? in regards to the corrupted and
lost data, honestly nothing. i mean, it's very likely that even with 169 million data errors, we still have virtually all of the original bits
in the right places. but as far as we know, there's no way to just tell zfs, "yo dawg! ignore those errors, you know, "pretend like they never happened, "tow easy zfs" or something. instead then, the plan is to build a new properly configured 1.2 petabyte server, featuring seagate's shiny
new 20 terabyte drives, which we're really excited about like, these things are almost as shiny as our reflective hard
drive shirt, lttstore.com. and once that's complete, we intend to move all of the data from the new vault cluster
onto this new, new vault. - [jake] all three. - new new vault. then we'll reset up new vault, ensure all the drives are good and repeat the process to
move old vault's data onto it. then we can reformat old
vault, probably upgraded a bit and use it for new data. maybe we'll rename it
to new, new, new vault. get subscribed, so, you
don't miss any of that. we'll hopefully be building
that new server this week. now, if everything were set up properly with regularly scheduled
and post power loss scrubs, this entire problem would
probably have never happened. and if we had a backup of that data, we would be able to
simply restore from that. but here's the thing, backing
up over a petabyte of data is really expensive. either we would need to build
a duplicate server array to backup to, or we could
back up to the cloud. but even using the economical
option, backblaze b2, it would cost us somewhere between five and 10,000 us dollars per month, to store that kind of data. now, if it was mission critical, then by all means it
should have been backed up in both of those ways, but having all of our archival footage from day one of the channel has always been a nice to have and an excuse for us to
explore really cool tech that we otherwise wouldn't
have any reason to play with. i mean, it takes a little bit more effort and it yields lower quality results, but we have a backup of
all of our old videos. it's called downloading
them off of youtube or floatplane, if we wanted
a higher quality copy. so, the good news, is that
our production 1x server is running great. with proper backups configured, and this isn't gonna have
any kind of lasting effect on our business, but i am still hopeful that if all goes well with
the recovery efforts, we'll be able to get back
the majority of the data, mostly error free. but only time will tell, a lot of time because transferring all
those petabytes of data off of hard drives to other hard drives, is gonna take weeks or even months. so, let this be a lesson, follow proper storage
practices, have a backup and probably hire someone
to take care of your data if you don't have the time. especially if you measure
it in anything other than tenths of terabytes, or you might lose all of it. but you won't lose our sponsor, lambda. are you training deep learning models for the next big breakthrough
in artificial intelligence? then you should know about lambda, the deep learning company. founded by machine learning engineers, lambda builds gpu workstations, servers, and cloud infrastructure for
creating deep learning models. they've helped all five
of the big tech companies and 47 of the top 50 research universities accelerate their machine
learning workflows. lambda's easy to use
configurators let you spec out exactly the hardware you need from gpu laptops and workstations all the way up to custom server clusters and all lambda machines come pre-installed with lambda stack, keeping your linux machine
learning environment up to date and out of dependency hell. and with lambda cloud, you can spin up a virtual
machine in minutes, train models with 4 nvidia a6000s, at just a fraction of the cost
of the big cloud providers. so, go to lambdalabs.com/linus to configure your own workstation or try out lambda cloud today. if you liked this video, maybe check out the time i almost lost all of our active projects
when the og 1x server failed. that was a far more stressful situation. i'm actually pretty relaxed right now for someone with less
much data on the line. - [jake] yeah, must be nice. - yeah, i'm doing okay, thanks for asking. i mean, i'd prefer to get
it back, you know.(chuckles)