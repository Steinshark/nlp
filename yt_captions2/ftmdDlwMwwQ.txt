welcome to mcoding! 
i'm james murphy. let's talk about asynchronous 
programming in python. i'll show you the basic async/await
 syntax, as well as the built-in asyncio library. then, we'll get into the good stuff: 
writing a web crawler. alright, let's jump into the code. i claim that you already 
understand asynchronous programming. suppose today we've got 
three things we need to do. we're waiting on our amazon 
package to arrive. we need to do our laundry. 
and we're also baking a cake. so, do we just loop over those 
tasks and do them one after another? according to this, first, we should spend all 
day waiting for our package to arrive. only after it arrives should 
we start our laundry. and then we should sit 
and wait for our laundry to finish. after our laundry finishes, we start baking 
the cake and watch it in the oven until it's done. nothing wrong with a lazy saturday 
if that's the way that you want to do things. but at the very least, you probably recognize 
that things could be done more efficiently. why not just order your package, then immediately 
put your laundry in, and then immediately put the cake in? all you're doing is waiting. why 
not just wait for all three at the same time? well, that's exactly the fundamental 
idea behind asynchronous programming. just like in real life, there 
are some tasks in programming that are primarily made up of 
waiting for something to finish happening. these would typically be things like waiting on a file to finish 
 writing to disk or waiting on packets from the network. it doesn't matter whether 
using python or c. you can't make facebook.com 
respond any faster. you just have to wait. and that's what asyncio 
is all about. if you're waiting on a bunch of things, you can 
wait on all of them at the same time. alright, so how do we 
do it in python? take any function that might need to
 wait on something and slap an "async" on there. for us, that's "do_work." and then, we also have to mark "main" as "async" 
because we want to do work in the main. "do_work" and "main" 
are now coroutine functions. unlike a normal function, when you call a 
coroutine function, it doesn't run the function. instead, calling a coroutine function, 
an async def function, returns a coroutine, which you can think of 
as a very lightweight task. but the whole point of async is that we don't just 
want to immediately run every task that we come up with. so, calling "main" down here just creates 
this "main" task, but it doesn't run it. use `asyncio.run` to start the 
very first coroutine. this starts the event loop, which is in 
charge of actually running our coroutines. here we're calling "do_work". but remember, this 
just creates a coroutine; it doesn't run it.   if we run the code, we see a warning 
saying the "do_work" was never awaited. if you ever see this error, it
 probably means you forgot to write an `await`. an `await` means 
"wait here until this thing is done," pausing the 
current task if necessary. async in python is purely cooperative; you have to opt-in 
and participate in order to see the gains. here we're still using `time.sleep`, 
which is not an async function. python will not pause and let someone else 
work just because this is sleeping. the only time you might pause and let 
something else run is when you `await` it.   so, replace `time.sleep` with `asyncio.sleep`.
and don't forget to `await` it. this is a kind of silly example because i 
could, of course, just delete the `sleep` altogether. but remember, here, asyncio 
is usually about waiting on i/o. so, mentally replace this with writing to disk 
or waiting on packets from the network. stay tuned for the web crawler 
to see a real use case. in any case, we run the thing again. 
and it still took three seconds. and that's because of 
these lines here. remember, `await` means 
to wait here until this thing is done. but i don't want to wait here until this
 thing is done before i start the next one. i want to start all of them 
and then wait for them to finish. you accomplish that using tasks. `create_task` takes in a coroutine and wraps it in a 
task object and schedules that task on the event loop. but importantly, it does not 
await the task right then and there. this allows us to create and schedule 
multiple tasks before waiting on any of them. then, we can use `asyncio.wait` 
to wait on all of them. and now when we run it, we see all three tasks 
are started immediately. then, a second later, 
they all finish. and it only took one second, not three, because 
we were waiting on all three tasks simultaneously. this is still just one python 
process and one python thread. we don't need multiple cores 
or threads to wait for time to pass. the package will arrive, the laundry will finish, and 
the cake will bake, all without any extra input from us. we just wait. if our coroutines returned any results, we 
could inspect the results like this.   if there was an exception, it would be 
re-raised when we ask for the result. `wait` returns done tasks 
but also pending ones. this is because `wait` 
supports a timeout. and it's possible that not all the tasks 
are done by the time the timeout is over. but if you don't specify a 
timeout, `pending` should be empty. another way to do basically the same 
thing is using `asyncio.gather`.   this accumulates the results 
of the tasks in a list. it can even take coroutines directly,
 which is something that `wait` can't do. in that case, it'll automatically 
create tasks for them. you could catch any exceptions raised with 
a try-catch around this line. or specify `return_exceptions=true` to just get the 
exceptions as elements of the results list. and in python 3.11, there's yet a 
third way to do exactly the same thing. just like a normal `with` statement can ensure 
that a file opened using `open` is automatically closed. this `async with task_group` ensures that 
all of the created tasks are automatically awaited. i know this must be very jarring to have
 three different ways to do exactly the same thing. i can hear murmurs of the zen 
of python even across the internet. but the reality of it is that asynchronous 
programming in python was very much an afterthought. like it or not, that's reality in python, 
so we just have to deal with it. just make sure to keep 
up to date with best practices. it's one of the most rapidly changing
 parts of python with each new release. so, enough with this syntax and the silly example 
where i could just delete this `sleep` statement. let's get to the web crawler. here's a basic but 
fully functional web crawler. it takes an asynchronous client. in this
 case, i'm using the `httpx` library. we take some initial urls 
to start our crawl. then, this `filter_url` is just a function that we're going 
to use to filter out links that we don't want. and we'll take in the number 
of workers this crawler should use, which will determine the number 
of simultaneous connections it can use. and we'll put a limit on the 
total number of pages to crawl. 25 is a really small limit, 
but this is just for testing purposes. we save the client, 
save the set of starting urls. we make a queue to hold 
the work that needs to be done. `seen` will hold all the urls we've seen before 
to prevent us from crawling the same one twice. and `done` will contain all the 
finished crawls that were successful. we store our `filter_url` function, specify the number of 
workers, and the limit on how many crawls to do. then, we initialize `total`, which 
will keep track of how many crawls we've done. everything is centered around 
this `run` function. it's an async function 
that will crawl until it's done. we start by putting our 
starting urls in the queue. this is done in this 
`on_found_links` function. whenever we find some links, we compute 
which ones we haven't seen before. mark those as seen. you could save things to a database 
or file here, but we're not going to. then, for each new url, 
we put it into our todo queue. we do this in a separate 
function to respect our limits. whenever we put something 
into the queue, we add one to the total. if we're beyond our limit, then we 
don't put anything more into the queue. an async queue has a pretty standard queue 
interface, the main methods being `put` and `get`. of course, being an async queue, 
these are async functions. we didn't set a limit on 
how big our queue can be. but if we did, putting something into the queue might 
require waiting until something else has finished. anyway, back to our 
`run` function. the effect of this is to put all of 
our starting urls into the queue. then, we use a pretty 
common technique which is creating a bunch of workers that 
all simultaneously wait for work from the queue. each worker is just another coroutine, and 
we use `create_task` to schedule this many workers. however, we don't wait 
on the workers. we await on what we actually 
care about, the queue. the queue contains all 
the work that needs to be done. waiting for the queue to join is the same 
as waiting for all the tasks to be completed. as long as there's more tasks, 
we want to keep going. but as soon as there's no 
more work to do, then we can stop. in that case, we go through and 
cancel all the workers. `cancel` is a method that's available on all coroutines 
that just raises a `cancellederror` inside the coroutine. so, what does each 
worker do? they just forever try to process one 
element in the queue until they're canceled. processing an element is 
pretty straightforward. `get` is also asynchronous, and we might have 
to wait in order to get work to do. if we do have to wait to get work, 
that means the queue is empty. but that's not the same as 
there being no work left to do. that's because the queue's `join` that we're waiting on 
doesn't go based on the number of elements in the queue. but rather whether the number 
of elements taken out of the queue is equal to the number of 
tasks that have been marked complete. the general pattern is: get work out of the queue, 
do the work, and then mark the work as complete. we use a try-finally to ensure 
that the task is always marked done, no matter whether 
there was an exception or not. this is important because 
if we ever missed a task done, the queue would never join because 
it thinks that there's still work in flight. as far as actually doing the work, now we finally get 
to something that's specific to the crawler. this whole run_worker_queue 
stuff is a very common pattern. so the crawl is the first place where we
 actually do something specific to our crawler. we try to crawl the url, and 
if anything goes wrong, we just pass. if you wanted to, this would 
be the place to add retry handling. put the url back into the queue, and keep 
note of how many times you've tried. we're going for a 
more minimalistic approach. on to the crawl. i'm just outright sleeping 
for 0.1 seconds here. this would be a good 
place to do proper rate-limiting. you want to make sure that you're not sending 
out too many simultaneous connections, both in total and per domain. our number of workers limits the 
total number of connections. but we have nothing to limit 
the number of connections per domain. in any case, be nice, 
rate limit yourself. oh, and if you forget to 
rate-limit yourself, this is a great way to get yourself 
banned from all your favorite websites. many websites will automatically ban your ip address 
if they detect too many connections too fast. after rate-limiting, we use our 
asynchronous client to go out and get the url. once we've got the html, parse 
out the links, mark them as found. and then you're done 
with that url. i left `parse_links` as an async function. 
but really, it's just totally synchronous code. all i do is create this url parser, which is a 
custom subclass of the built-in html parser in python. let's take a look at the parser. did you know python has a 
built-in html parser? so please put away that attempt 
at using a regular expression. html is not a regular language. regular expressions are not 
sufficient to parse html. all we do is override the 
`handle_starttag` from the class. there are other ways to look for links,
 but we're essentially just looking for `` tags. so, if we see an `` tag with an `href` 
attribute, then we go in and filter our url. if it passes the filter, then 
we add it to our set of found links. just a note on what 
this `base` thing is. when you click on a link, where it takes
 you depends on where you currently are. so, the `base` is being used 
to track where we currently are. the `filter_url` function we're using 
for this crawler isn't very sophisticated. i wrote this urlfilterer. it just takes allowed domains, 
schemes, and file types. then, to filter the url, it parses 
where the link would go based on the `base`. and if the scheme is bad, 
or if the domain is bad, or if the file type is bad, 
then we just return `none`. otherwise, we return the 
normalized url. you can really put in whatever you want here; 
those are just a few things i came up with. and that's it for the crawl. we 
got the response, parsed the links.   and then on_found_links, we'll 
add the new ones back into our queue.   and then the worker will just 
get the next element and keep going. eventually, we'll either crawl 
everything within our constraints. or hit the limit on the number 
of crawls that we're willing to do. either way, the queue will 
eventually empty. the todo join will finish. 
and we'll cancel all the workers who would otherwise be 
stuck here waiting for more work. then, all that's left 
to do is try it. let's make a urlfilter. i'm just going to use my own 
domain as the only allowed one. and i'll only search links with html, 
php, or no file extension. we use an `async with`
 to get a client for our crawl. pass everything to the crawler with 
an initial url of the root of my website.  and then await the crawler's 
`run` method. at the end, we'll just print 
out the results. and there you go. it found 
all the pages on my website. finally, a quick tip when 
you're debugging with coroutines. `asyncio.run` takes a `debug` flag. if you turn this on, things will be slower, but 
you'll get way better error messages. if you're looking to test your understanding, 
then try some of these exercises as homework. thank you to brilliant.org for
 sponsoring this video. hone your algorithm skills with brilliant's 
algorithm fundamentals course, where you can learn about 
classic searching and sorting algorithms, concepts like big o notation
 and the stable matching problem. or branch out and find 
something new to fit your taste. brilliant has thousands of interactive
 math, science, and computer science lessons to help you learn something 
new every single day. and there are more lessons on the way every 
month, so you're never going to run out.   visit brilliant.org/mcoding or click the link in the description to 
get started with your free 30-day trial today. and if you'd like, the first 200 of my viewers to sign up 
for brilliant's annual premium subscription will get 20% off. that's brilliant.org/mcoding. as always, thank you for watching. 
and thank you for making it to the end. asynchronous programming 
is a huge topic. so let me know if there's 
anything you'd like me to cover. as always, thank you to my patrons 
and donors for supporting me. see you next time!