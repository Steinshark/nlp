nvidia's rtx 4090 was a massively powerful gpu with massive power consumption to match something that's been making headlines since before launch with or without combusting adapter cables now the rtx 4080 is here and nvidia claims that it's going to double the performance of the previous generation which would put it dangerously close to the rtx 4090s performance level and what they say will be a much lower power draw and yet nvidia's claims were far higher than the power draw we actually see the kind of lie that i don't mind hearing the question now is can this thing live up to those performance claims or at twelve hundred dollars well the 40 series inflated price tags make it a skip-it generation like the rtx 20 series cards it's only one way to find out and there's only one way to segue to our sponsor karma save money while shopping with karma's new pay with k feature with it you'll get the lowest price and get cash back bonuses on all of the best brands learn more about karma later on in this video thank you the rtx 4080 16 gig or i guess i don't really need to call it that anymore the 4080 has roughly 60 percent of the cuda cores compared to the bigger 4090 with slightly lower clocks to go with them this is coupled with a 256 bit instead of a 384-bit memory bus which means effective bandwidth is cut to just two-thirds given that the starting price is roughly two-thirds of the 40-90 the math is mathing although the rated total graphics power is a bit higher than you'd expect given the spec cuts still it's the same 320 watt tgp as the outgoing rtx 3080. so if we're gaining performance like nvidia says we are we're not paying for it there at least in theory let's test it in practice we grab the rtx 4080 along with its predecessor and what we consider to be the closest competing gpus in terms of price and performance and slap them into our 13th gen core bench to give them the best chance to stretch their legs starting with non-ray trace 4k the rtx 4080 pulls a respect 25 lead over the rtx 3090 ti in forza horizon 5 and nearly reaches 80 percent of the rtx 4090s performance which puts it on solid footing out of the gate f-122 sees the 6950 xt pulling close thanks to the monstrous traditional rendering performance that cart has and the 3090 ti closes the gap substantially thanks to its greater memory bandwidth the 3090 ti's performance carries over to cyberpunk somewhat but the other cards fall behind nvidia's 40 series cards hitman 3 sees the rtx 4090 pull significantly ahead of the 4080 again likely thanks to its memory bandwidth as the 3090 ti and 6950xt both trade blows in this title modern warfare 2 pushes the 4080 back again compared to the 4090 but it's still commanding that roughly 50 lead over its predecessor and sits comfortably ahead of the 3090 ti and 6950xt older titles like red dead redemption 2 and shadow of the tomb raider both display similar results across the board with the rtx 4080 clearly topping the last generation's best cards but trailing the 4090 by about 30 percent 1440p results are unsurprisingly much tighter thanks to the lighter load it places on the gpu although not in the way you might expect while the 4080 closes in on the 4090 the other gpus remain where they were relative to the 4k results suggesting that it's only the rtx 4090 that's cpu bound in these tests there are cases like cyberpunk and modern warfare 2 where the extra horsepower still makes a big difference but for users of 1440p displays and especially in older titles like red dead redemption 2 and shadow of the tomb raider the rtx 4080 may be more than enough to achieve close to 40 90 level performance while costing substantially less money and drawing substantially less power which does also cost money we'll get to the power draw but first ray tracing this is amd's achilles heel and it shows even in shadow of the tomb raider where the 6950 xt is only roughly equal to the lower tier rtx 3080. even in this older title nvidia's new rt cores appear to perform slightly faster than the 3090 ti's giving the 4080 a broader lead than it had in traditional rendering f-122 by comparison had the 4080 lose relative performance against the ford 90 versus when ray tracing was turned off but it pulls ahead substantially from the rtx 3080 making for frame rates that are playable at 4k rather than cinematic cyberpunk unfortunately remains cinematic on all of our cards at 4k with ray tracing enabled though it's here that we really do see the rtx 4080 pull over double the performance of its predecessor and amd's 6950xt and while dlss 3.0 is still early days it appears that the two cards handle its frame generation roughly the same way which is unsurprising but we had to test it to be sure for the same reasons as 1440p rendering performance the rtx 4080 seems to benefit more from dlss overall than the 4090 does thanks to the latter becoming more cpu bound and this is something we should see more of as the rest of the lineup continues to launch just like we recently launched these various nazi channel hoodies on ltdstore.com they feel like stealth hoodies but they're anything but moving on to productivity the rtx 4080 is nearly double the performance of the 3080m blender rendering thanks both to the additional and upgraded rt cores and it seems to come within three quarters of the speed of the 4090. davinci resolve favors the 4090 by about a minute with the 4080 coming in just a little slower than the 3090 ti however its addition of av1 encoding means that any 40 series gpu i mean any of them is going to make your pc sub substantially faster at this kind of rendering compared to any of the other gpus we've tested here matlab gpu bench is nvidia only but we can see that depending on the algorithm the rtx 4080 can come within spitting distance of the 4090 in single precision or be a good 40 behind in double precision either way it's consistently well ahead of the rtx 3090 ti and 3080 making it an excellent choice for gpu compute it's less clear-cut for topaz ai where our test suite finished soon around the 4080 than the 3090 ti and within 10 minutes of the 4090 but amd actually pulled ahead here suggesting that topaz is geared more towards raw gpu compute than amd's ai accelerating tensor cores or at least the ai model we used was finally specviewperf puts the rtx 4080 in the good overall position handling outperforming both the rtx 3080 and 3090 ti although amd pulls off some wins and katia creo energy medical and especially siemens nx though the latter is likely because that version that's used in spec viewperf doesn't support acceleration on nvidia yet power consumption has been a hot topic since the earliest rumors of the rtx 40 series launch and the rtx 4090 proved our concerns were valid however the rtx 4080 is surprisingly tame yeah during our testing it almost never reached its 320 watt rated total graphics power instead hovering around the 300 watt mark or usually lower we were so confused by this that we asked nvidia what was up and they responded by saying oh yeah we actually we're changing what tgp means sure okay so what did they say basically tgp has popularly been thought of as a power target until recently it's the power level that the gpu tried to adhere to when under load opportunistic boost meant that gpus would often spike higher for short periods but usually settled in around their tgp rating now tgp is being looked at more like a power limit the maximum the card should hit under normal conditions and nvidia's built themselves a buffer igor's lab recently published an article showing this in greater depth with the rtx 4090 which i'll have linked below if you want to check it out nvidia themselves claimed that the rtx 4080 draws roughly 251 watts on average while gaming though it's closer to 300 watts at 4k depending on the game and yeah even when running a power virus we couldn't get the 4080 to go much higher than the rated 320 watts despite the gpu load remaining high and the core clocks remaining above 2 gigahertz this leads to some interesting questions because each and every rtx 4880 you've tested has had a massively overbuilt cooler that resulted in the card barely ever getting warm like seriously under full synthetic load we're looking at around the low 60 degrees at worst and in games it sits around in the 50s the fans never even began to approach 50 percent let alone higher even the internal case temperature was the lowest of any gpu in our test suite with a delta t over ambient of between just 10 and 11 degrees unlike the rtx 4090 this gpu would be well suited to a small form factor chassis i'm wondering if nvidia made this adjustment to ggp latent development and nobody got the memo in time the question now is why though nvidia didn't volunteer that information when we asked when we overclocked the cards we noted that there wasn't a lot of headroom for the gpu core clocks our cards could only push between 110 and 150 megahertz over stock and while this is obviously going to vary from card to card that's only about one percent which is much lower than the roughly 15 percent you might find back on the 10 series cards this might be one reason why nvidia dialed back the gpu power a bit in fact even when we pushed the memory and the core clock as high as they would go we still didn't get substantially higher frame rates however we did substantially increase power consumption letting further credence the idea that nvidia saw no reason to push the gpu to its rated 320 watts under the normal operation what we've observed is its sweet spot with that in mind what are you getting for twelve hundred dollars u.s you're getting a gpu that's much faster than anything the previous generation can muster with dual envy encoders that support av-1 a thermally very tame design with boatloads of thermal headroom that could potentially be cut down to fit lower profile chassis and official support for deep learning frame generation to boost perceived frame rates where it needs to however unlike the rtx 4090 which is a halo product with no price pair currently the rtx 4080 has to compete with those last gen cards and while the rtx 3090 ti is close in its current retail price it's not as close in performance or power consumption you're paying an extra hundred dollars for the 4080 and getting in my opinion more than a hundred dollars in value the rx 6950 xt and rtx 3080 however those do represent a threat in terms of value if the extra features simply don't matter to you some of these can be purchased for as little as 800 brand new with used cards routinely going for less sometimes even 600 at half the price they definitely do not represent just half the value based on our testing bear in mind the amd card draws substantially more power so if that's a concern go with the 3080 for now we'll have all of those linked below none of which is to say that you shouldn't buy the rtx 4080 it's a solid card and it's only just getting started just maybe wait for reviews of amd's rdna 3 gpus to drop before pulling the trigger who knows you'll probably even go on sale if amd does a good enough job competition is beautiful and hopefully i did a good enough job segueing to our sponsor karma karma is a browser extension and mobile app that's designed to help you be a better online shopper and save you money it has features like coupon scanning price drop alerts real-time stock updates and tons of other tools are included to help you get whatever product you want you can even organize the stuff you want into wish lists to save for a later time and get cash back bonuses on all the best brands karma's newest feature pay with karma or pay with k allows you to connect several different payment options like credit cards and debit cards to apple pay and google pay so you can shop at all your favorite brands once your info is loaded up you can experience one swipe shopping all within karma for this week only sign up with karma using our link and get 10 added to your account instead of five dollars and don't forget to also check out their commercial linked below it will blow your mind how it works thanks for watching guys go check out our response to amd's upcoming rdna 3 graphics card cards for more on how the gpu landscape is changing nvidia launched first but they might not keep the lead