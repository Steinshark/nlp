all right all right let's get started on this thing okay so a few days ago we made a video uh on this blog post that i saw by modular um and it's mojo is 50% faster than rust right and i read it and there are some like skeptical remarks and hey you're doing this and hey you're doing something else and hey you're not correct on this and they literally came back and first off they quoted me i will say one thing um i was an individual i wasn't a group of people okay i not an and second off i'm not sure if i would call myself an expert okay i use rust regularly wouldn't call myself an expert but i still agree with my state with my statement uh any who so followup to the articles they actually wrote a response to everything and they're saying that they're actually is it actually faster this quote has been flying around everywhere for mojo i think it's the best qu i honestly i fully agree with my quote so for those that don't know what my quote was uh or didn't really understand when they saw it it's very very simple if python can remain python you have to learn a little bit extra like some extra apis a little bit of extra syntax for some basic declaration and all that and you can have effectively the benefits of performance of rust then everybody that's in ai that is doing these fast small experiments throw away code and everything they're going to stick with python or in this case mojo they're not going to make the migration over to rust and i think that that's still absolutely true anyone that thinks that somebody who's trying to do these really fast really gorilla experiments and throw away everything they make why would they ever try to go and learn rust and spend this incredible amount of time getting good at it just to do that one thing right it just doesn't make any sense it it it doesn't it like it it doesn't feel like a good a good argument anyways so let's go into this one this is their response because obviously they got called out saying hey you did all these things wrong so let's find out what they have to say mojo is built on the latest compiler technology in mlir an evolution on llvm which rust lowers to and so it can be faster it depends largely on the skill of the programmer and how far they're willing to go with optimizations classic skill classic skill issues isn't that all performance skill issues and uh tradeoffs because your boss wanted it yesterday isn't that like most skill issues dude right now i'm dealing with the skill issue uh involving performance which is literally just a massive performance black hole and simply that's just because things had to get done and they had to get done quickly i mean that's honestly that's that's such a huge part of the professional world is just people ain't got time to make the greatest thing ever and so if it's if it's a pretty fast easy path to make something good enough a lot of people are happy mojo's goal as a language is to meet python developers where they are and allow them to learn some new tricks to optimize their code to the performant limits of any hardware see this is what i mean i don't see how you're ever going to be able to win this argument if you could just immediately get benefits why would you change anything right dude i would program if mojo ends up being as good as they as they are if modular has perfectly sold us a like assuming everything they say is great hell i'm g to probably start looking into using mojo all the time you know what i mean over the weekend netflix engineer and rust advocate hell yeah hell yeah that's me let's go um released a re reaction video to the community guest blog we published outperforming rust dna sequencing parsing parsing benchmarks by 50% with mojo uh by the way i do i mean i i do advocate for using rust i think it makes like honestly if you write an lsp and you don't use rust i think you are a bad person or you wrote go please which is an incredible is the best lsp ever created if you write an lsp in in in javascript you're you're being a bad person stop it stop it stop it um they're blaming me the blog post stirred up some contra controversies uh rust uh is being positioned as the potential successor to the dominant language in ai which is currently python and c++ this is the briag take on mojo okay yes if mojo is i did not i didn't imply legitimate okay i think i said i thought i said if mojo is like giving us okay whatever if mojo is legitimate i think mojo will win just hands down the reason mojo will win if you by way this is live streaming i don't i mince my words maybe a little bit while live streaming is you don't change the paradigm of any acclimated or professional individual you just have to learn a bit more and you get amazing performance if mojo compiles fast it looks like a language you're already familiar with and it's really close to being at the same speed i just don't see how you're going to make that sell for rust pre-read pre-stated and then even luca yeah luca agreed with me by the way great book i already he said thank you because i i promote his book on the regular but i do mean it here hold on one second literally what probably the best one of the best rust books if you're vaguely familiar with rust and you kind of already understand how the borrow checker works buy the book it's fantastic i won't even give i i literally will not give anybody a amazon affiliate code just go buy it i'm not going to make money off that one because it's too good of a book all right catching on the prime stream from yesterday mojo versus rust he's right if mojo delivers he did he did he did make some specifications that they're not saying here if mojo delivers we'll never see rust in user space for people working on ai the value proposition is just too good if you're an ml engineer or data scientist who knows python well it is a great yeah it is the value proposition is just simply too good but by user space i mean data scientist data analysis ml researcher tinkers all folks who need to validate an idea and or build a prototype that's good enough to be shipped they're uh they are the lion share of the ai world a bit like compiler engineers versus web devs depending on how much mojo delivers rust might struggle to settle in ai infrastructure space too that's the big one is if if mojo good enough to also settle into ai infrastructure that's going to be sweet by the way you could uh you could you could by the way you can just link you can just link i i i have zero rules in my chat okay i just ban people for being okay you can link i i assume you're i assume most people aren't and they'll just do the right thing and then there's a couple  and you just you make a game out of banning them right it's fun all right so mojo our goal our aim is to be intuitive to learn for python developers as muhammad showed he was able to learn mojo and optimize an algorithm using simd in a matter of weeks as a side project while this article is focused on performance differences the points that primagen and luca uh palmary made are important to us we we are heavily focused on ai where a three language problem exists ooh the three language problem a minaj a language uh we are heavily focused on ai where there is a three body problem uh and where cpu plus gpu programmability is so important across hardware but let's not forget the real goal of mojo is is lifting the world's most popular ai language in python and empowering developers everywhere with incredible performance hardware uh portability and programmability is mojo faster than x language uh raised an important question rust is known for low-level performance how can mojo provide better performance out of the box than rust or c++ i i assume it can't i assume by the very definition that it should not be able to but it will be able to or it will be able to compete on par with specific things right so a very simple example would be a for loop a for loop doing some amount of work they should be able in some sense one would assume you'd be able to do that but at the end of the day i assume this is still there's still garbage collection going on right it's just like go will never be as fast as rust but go can be really fast if that makes sense we're going to let them cook don't worry we're going to let them cook i'm just explaining i'm explaining a very simple principle that people seem to not realize about programming languages which is very very simple there's like a pyramid of languages there is like your uh there's like your p pretty much your like interpreted languages right and so you got things like lua lua if you don't use lua jit right um then you got yourselves like the slow interpreted languages like python and ruby then you got yourself the google has paid $10 billion to make it fast language javascript right then you got yourself compiled but still garbage collected languages such as java and go right these are these are faster languages they tend to be a lot faster java kind of falls java's somewhere between these two cuz java has the ability to kind of compile but it also bite codes so it's also interpreted but then it's also you know java's in a weird spot we're we're going to put java in between these two but go is like a purely compiled language but it has it it has no jitting right but it does c i think falls under the same platform as java right so c right it it is both compiled and there's some sort of interpretation thing going on there i don't i don't know the exact lines in which things happen uh but i know you have to warm up end points and like that uh but go on the other hand there's no warming up it's just fast but it's garbage collected so it's not perfectly fast then you got things like c uh rust and c++ this makes total sense that these guys are always on the tip just the tip because they are compiled and you manage the memory so assuming you don't have skill issues you should be going fast does that make sense oh assembly it falls under the c is just fancy assembly okay sure you're right assembly fortran people don't cobal cobal falls under this we're not going to talk about those ones okay because those aren't interesting very few people do assembly uh okay and the people that are doing assembly and should be doing zig is okay shut oh my goodness yes we get it the compiled managed memory are are the ones in which fall into the most elite category of speed okay because they have the they have the most possibility of being fast it doesn't mean they are the fastest in all situations they just have the possibility of being the fastest but you suffer from skill issues way more frequently the the line of potential skill issues go up really really fast and so like there's a graph that exists right so when it comes to uh ruby uh this is like uh this would be like speed and this would be uh skill issue right so this is speed and skill issues ruby goes like this cuz there's no skill issues it's just slow right that's just how it goes javascript you know the speed and skill issues there's there's definitely like a line here you can make javascript fast but most people just suffer too heavily from from skill issues actually the line would be like this right it's really hard to get right whereas like go you can have skill issues and you can be pretty fast rust though tends to be like this as well to make rust fast you have to be pretty good same with c++ same with those other languages because you have to like not suffer from skill issues to make it good so i always find that go just has like just the right amount still can goof yourself up but you can make it pretty fast so i'm hoping that mojo falls right here honestly this is what i hope i hope mojo falls on the go line if mojo falls on the go line then it's out of this world right it's a great great great thing a common question when you uh users first during the discord is how much faster is mojo than x language there's a lot of consideration surrounding any benchmark implementation you can't choose any one benchmark to say x language is faster than why language this is by the way very good on them to do this people constantly say these dumb things about languages being faster and then what they what they do is they show you like an http request that does empty echoing of like hello world and that is just totally not totally not good totally not good um a better question is how much overhead does mojo introduce compared compared to x better a major goal for mojo is to allow you to push hardware to the limits of physics while remaining ergonomic and familiar to python developers compared to the dynamic languages language like python compiled languages allow you to remove unnecessary cpu instructions such as allocating objects to the heap reference counting in periodic garbage collection we covered that mojo takes lessons learned uh and best practices from c+ plus rust and swift to provide direct access to machine without these kind of overheads okay mojo and rust both allow to optimize at a lower level but in rust for example you can still wrap everything in an arc mutex box okay please i mean this is how many people write a bunch of rust and make it really awesome and do all these great things only to end with an arc mutex hashmap come on come on come on don't lie to me you know you know you little sons of  end with an arc mutex uh hash map okay don't even don't even lie explain to me like i'm five what an arc and arc and a mutex iss yes so arc stands for automatic ref or it stands for reference counting at the atomic level meaning that you can pass it between threads because what they count h or effectively what is cloned is is is pretty much a pointer to where your data is and the counter for how many people are holding on to that data is atomic so that means if two threads try to increment at the same time it will appropriate appropriately increment it by two whereas if you had just an rc and you tried to do that you could very well have two people incrementing uh two at the exact same time thus it's three even though two people have ownership so it's a reference counter so it'll count how many people hold on to an object a mutex is like a single lane road only one car can pass at a time that's the easiest way to think about a mutex and so that means only one person can hold on to the mutable access to an object so if you had a hashmap and you wanted to uh mutate it you'd want to use a mutex a semaphor is just a mutex that has a wide road if that makes sense it can allow four people to to do something at a time or eight or 12 or however many you want can now explain to me like i'm four because i'm too dumb to be five no uh to avoid fights with the borrow checker at the cost of performance if you're writing an application code this might not have any significant impact but if you're writing a library or performance sensitive code the overhead can add up quickly it's up to the pro programmer how much they care about reducing over head and optimizing performance fair fair take this is a very fair take both uh let's see both of the languages can use llvm for optimized coden and both languages can let's see allow for the use of inline assembly but of course no one can afford to do that what is this tweet agree you can match perf with inline assembly argument is funny it implies that the language doesn't matter at all but obviously the world can't afford to rewrite all software for chip sl vari even more true with shift uh shift to accelerators which mojo built for yes this makes sense so someone obviously made the argument uh that you can do you can be faster if you use handroll assembly and it's just like yeah you can do that but that's dumb and it's true it's true it is dumb like there should be only like i know tls does it with their crypto stuff there's like some handrolled uh crypto algorithms out there and that makes sense but most people ain't doing that so they both have the same potential on traditional hardware right well sure but the real question is how does the performance of idiomatic normal mojo code compared to normal rust code written by someone who isn't a world expert writing in assembly for every chip and doesn't know the details of how the compiler works this is a much better question by the way what does the what does the six monon writer of code look like and when i say idiomatic i don't you know idiomatic is funny because unless if you have a very strict way in which all code is formatted and expected to be written there is no really such thing as idiomatic there's like localized id idiomatic idiomatic right so if you look at uh jav if you look at javascript there you could see these idiomatic lines i can look at a library and tell you which five-year period something was written in and so if i look at certain things it's very e like early javascript it was all about using that extends and classes and prototypical inheritance and you can be like oh this was idiomatic java script at one point for some set of people anyways this is a really great thing so what would the six-month person look like reduced meem copy with borrow by default when a new user is learning rust one of the first pitfalls they run into is that function arguments default to taking an object by moving it this means that you pass something into function and try to reuse it you get a compiler error correct but it's also beautiful i love this so uh i actually really this is what i i actually do enjoy this level of the borrow checker i find it to be very very useful in how i think about things the line with debug throws a compile error because you've moved foo into bar function uh this th because debug uses foo so that's why this thing breaks whereas bar uh bar also effectively takes ownership of fu because he didn't pass it by reference uh for those that don't know rust rust is a little bit wild okay javascript is a bad example there is no idiomatic uh only different levels of id idiotic yes the m copy can be optimized away by llvm in some cases but this doesn't always occur and is hard to predict unless you know how the rust llvm compiler works mojo simplifies this concept for a standard use case right main foo string uh bar print foo mojo uh arguments are borrowed by default not only is this much more gentle when learning mojo compared to rust it's also much uh more efficient due to no implicit meem copies if you want to get closer to rust behavior you can change the argument to uh to owned i didn't even know that this was like a thing owned what is owned i even i i i'm not i'm clearly not an expert i i've never used this as a keyword i never i've never once used this as a keyword have you guys used that as a keyword i like i have never seen it i've never seen it in any arguments oh this is mojo oh no wonder i was just like dude i've never seen this what is this rust no wait this is not wait this is this is mojo dude i can't tell the difference you can't use f you both can't use fed they're both using fed okay this is mojo this is mojo okay nice nice sorry my bad my bad oh this is cool by the way i i do want to say something about this so this is very oam meaning that you're sending an explicit message about the variable but you're not tying it you're not tying a lifetime as part of its type does that make sense so one thing that rust does that's very you can call it frustrating is that a reference is effectively part of its type which means that you have colored types whereas this whereas o camel and now apparently mojo is that they're orthogonal which is really nice it's very very very nice like this is i i think this is one of the greatest things ever because that means the you know the outside can make a bunch of uh assumptions about this meaning that okay this thing could be stack allocated due to how it's being used really cool whereas all like all dynamic languages you usually can't make that and everything has to be heap allocated but this one's being very explicit about it uh this still works because string implements a copy constructor it is able to move into bar and leave behind a copy under the hood this is still passing a reference for maximum efficiency but it only creates a copy of f if mutated to fully opt into so that's called a cow copy on right uh or to cop a fully opt into rust default of moving an object and losing ownership you need to use the operator whatever this is cool honestly this is cool now you finally get a compiler error for uh trying to use fuo after a move and you have to work much harder to fight the borrow checker in mojo this is better default behavior not only is it more efficient it does it doesn't uh roadblock engineers from dynamic programming backgrounds they still get the behavior they expect by default with the best performance possible i mean to me this is a little bit of a weak argument in the sense that uh you're kind of like people would just use a reference here dog they' use anster right they' aner that son of a bee so you know what i mean you know what i mean like kind of feel kind of feels a little you know boilerplate rust though but then again uh but then again you also have a lot of these types that you have to think about blah blah blah blah and then if this thing has to be held on to longer for some amount of time then you run into lifetimes and anyways all right no pin requirement in rust okay every time i've asked somebody that understands what pin is they can't ever explain what pin is they can just tell you how you should use it it's very very funny how pin is like one of the most loosely understood concepts of rust as far as i can tell pin is it's saying whether or not it can be moved in memory is all it is so there's unpin and pin i believe that's like the the fundamental basis of it a pin equals can't move exactly but to explain that to why you need it and and all those things and how it actually performs it and all that that stuff it feels very confusing in russ there is no concept of value identity for a self-referential struct pointing to its own member the data can become invalid if the object moves as it'll be pointing to the old memory location this creates a complexity spike particularly in parts of async rust where futures need to be self-referential and store state you must wrap self with pin to guarantee that it's not going to move by the way this might somehow mojo just made one of the best explanations of of pin or of pin they're doing a pretty good job in mojo objects have an identity so referring to self. fu always returns to the correct location in memory without additional complexity required for the programmer uh there is a nice blog titled pin and suffering which we've read that takes you on a journey of a ration working through the implications of pin these are complexities that a mician i didn't mean to press that i was just so disappointed by people who program mojo calling them magicians okay i'm not i'm just saying i think it should come up with a different word okay just saying i'm i am i am furious right i'm fur i'm i'm fuming i'm fuming right now um all right rust was started in 2006 and swift was started in 2010 um and both are primarily built on top of ll of vm i mojo started in 2022 and built on l mlir which is mo more modern next generation compiler stack than llvm ir approach that rust uses okay so there could just be but this is like a uh one could argue that this is a eventually fixable problem mojitos there you go i like that mojitos this is a an eventually fixable problem there's a history here our ceo chris lner started llvm in college in december 2000 and learned a lot from its evolution and development over the years he then led the development of mli at google to support their tpu other ai accelerator projects taking that learning from llvm ir to build the next step forward described in this talk from 2019 okay so he may know a thing or two as it sounds like mojo is the first programming language to take advantage of all the advances in mli both to produce more optimized uh cpu code generation but also to support gpus and other accelerators and to have much faster compile times than rust let's go get dunked on i think actually this is a very important part if you're doing a bunch of really uh if you're doing a lot of really fast iteration rust can become just annoying and i don't want to sound like a diva which i will sound like a diva compile times can take the joy out of programming sometimes i get like compile allergic sometimes you know it's one reason again why my go arc i've been loving my go arc because there's just like you just they're just compile times are transparent you know it's just really enjoyable uh this is the let's see this is an advantage that no other language currently provides and it's why a lot of ai and compiler nerds are excited about mojo fire uh they can uh build their fancy abstractions for exotic hardware while us mere mortals can take advantage of them with pythonic syntax uh great simd ergonomics cpus have special registers and instructions to process multiple bits of data at the same time known as simd single instruction multiple data but the ergonomics of writing this code has historically been very ugly and difficult to use yeah wild every time i see simd code i'm like du uh these special instructions have been around for many years but most code is still not optimized for it when someone works through the complexities and writes portable simd optimized algorithm it blows the competition out of the water for example simd jon that's real that's what uh v8 uses and that's why often i mean it's amazingly fast uh mojo's primitives are natively designed to be simd first u 8 is actually a simd dtype u 81 which is a simd ele or simd one of one element there is no performance overhead to represent it this way but it allows the programmer to easily use it for simd optimizations that's actually pretty cool okay okay that's pretty good okay so maybe magicians might be okay maybe magicians mojo has too many types what can you ever have too many types for example you can split up the text into 64 byte blocks and represented as simd type u and 864 then compareed to a single new line character in order to find the index for every new line because the simd registers on your machine can calculate operations on 512 bits of data at the same time this will improve performance of those operations by 64x i don't know if it's like literally 64 i mean like yeah like in in like the the the peer scope of that yeah but there i mean i'm sure it's not like a pier 64x always win there's probably some some coefficient sounds like a skill issue it does sound like a skill issue but also not having to like write out all that also sounds really fantastic uh so more examples of simd type float 64 824 whatever you can multiply a float by float let's see by let's see you can multiply it by float uh two improving performance by eight on most machines compared to multiplying each element individually so that is like i mean this is super cool honestly this is super cool in the sense that you're able to do very simple operations you know i mean just think about matrix multiplication right that's i mean i think that's what you got to have in your mind when you think about this like vector operations and so if you just just say they're int 8s and it just does it for you like there is something that's really good about that right it's really really good about that well it's not just great for ml these same ideas can also probably be i assume they can be translated to some game programming math as well uh lovm and therefore rust has automatic vectorization optimization passes but they'll never be able to reach the same level of performance as the programmer expressing exactly what they intend because llm cannot change memory layout or other important details for simd mojo has built from the ground up to take advantage of simd and writing simd optimizations feels very close to writing normal code all right eager destruction russ was inspired by ry resource acquisition is initialization resource acquisition is initialization resource acquisition is initialization don't forget they also go inverted uh so you know your destructors go backwards uh the interesting part about this is that they aren't focusing on uh solving the automatic parallelism problem they're going straight for uh to instruction parallelism yeah it's it's it's interesting preach dude c++ doctrine yeah thank you for coming to my c++ sermon and implicitly rust sermon uh from c++ which means that once an object goes out of scope the application developer doesn't have to worry about freeing the memory the programming language takes care of it this is really nice paradigm and you get ergonomics of dynamic language without the performance drawbacks of a garbage collector exactly mojo uh takes this one step further i am curious about something though with this like is it always faster and here out on this one this is why i assume they do like arena arena allocations and and you you you pretty much write your own allocators to avoid some of this where if you're always deallocating as it's running is that better or worse than having something like a garbage collector that can deallocate a bunch of things at once maybe it's like you know there's obviously a trade-off we all know the tradeoff i'm just saying i wonder what the distributed efficiency versus the you know point efficiency is i have no idea i just assume allocating is more expensive it's allocating is more expensive in gc's because you have to allocate bookkeeping memory along with the thing itself and then deallocating can be more expensive because it has to crawl a bunch of stuff better throughput but worse overall that's what that would be my assumption as better throughput but but worse overall right like maybe that's if you take a time period it's better but then if you take a time period long enough it becomes worse and that makes sense mojo takes one step further instead of waiting until the end scope it freeze memory on the last use of the object this this is advantageous in the field of ai where freeing an object early can mean dealloc deallocating a gpu tensor early therefore fitting a larger model in gpu ram this is a unique advantage for mojo where the programmers gets the best possible outcome without having to think about it the rust borrow checker originally extended the lifetime of everything to the end of its scope to match the destructor behavior which has some confusing consequences for users rust added new features to simplify this for developers with non-lexical lifetimes due to mojo's eager destruction we get these simplifications for free and it aligns with how objects are actually destroyed so we don't have uh confusing edge cases i'm not going to lie to you mojo is making really powerful arguments here i really like that this is no longer like a 50% faster article but instead this is like a here are our tradeoffs we are making and there's reasons why this is better and worse and i think i mean always garbage collection will always be worse than not garbage collection i think everybody agrees with that statement but i i really like some of these things it's saying i think it just makes it it it means that it's it's going to be fast now is it going to be absolutely faster than rust my guess is probably not uh if you have like you know expert in both categories but i' again i really think a great measurement is take somebody with six months of familiarity and both person gets thrown at a problem and see what comes out of it right then i want you to also take i want i want a boss to be over each one of of those two people and i want them to also be in like hey you need to hurry up hey we got deadlines hey we got people that are waiting for you hey we got this and i want to see what the outcome of that code is you know what i mean because anybody can make the world's most optimized code given enough time and effort but can you do it under pressure under you know real world working conditions all right tail call optimization oh yeah here we go another piece of overhead is the way that drop works in rust it tracks if an object should be dropped at runtime with drop flags rust can optimize these away in some cases but mojo defines them a way categorically to eliminate the overhead in all cases okay okay okay there let's see there isn't much of a trade-off being described right they're explaining how a fast mojo is because tech trademark but it's not describing what trade-offs they are making well there i mean their claim was that representing something as a simd int contains no overhead change but has the like opens the door to easier use of simd can we stop for a moment and recognize that this is also free good python compiler facts that's basically hotpot elimination mojo is using well mojo is going to use a if it's if it's pythonic in nature it's going to use a garbage collector it has to uh due to the previous i assume it has to i may be wrong there but i just assume tail call optimization and elimination tco and tce uh due to previous point about not having eager destruction and rust if something is allocated inside a recursive function that d structure won't run until the end of the scope which means that tail call optimizations and eliminations are off the table here's a minimal example that you can run yourself uh first function new cargo rust looks like this again i don't know how true these numbers are but that is wild i mean that is wild so so hold on so rust doesn't have tail call optimization is that true it does let's yeah i know then then what i don't get it uh personal hold on is there is there a cargo nit can i provide a name say what what what what just happened here oh what hold on what what is this supposed to what hold on what is happening here like what what are they asking for remove these generics okay why are you why are you why are you being like that i i thought you could turbo fish that side are you not able to turbo fish that side expect a type what is this type what are you asking what are you even asking me uh with capacity oh i left those in i'm a stupid person okay so they didn't have a typee on it so it it blew up is that like some new feature that i'm not that i'm not not aware of okay so i can do that cargo run release uh just make sure i i i did see that uh yeah all right oh my goodness i i actually went to a different neovim instance uh i32 there we go okay so it it's definitely running these things i mean even if i throw a little time in there i guess i don't i what what are they using to run and do the uh hyperfine i don't have hyperfine so i don't have hyperfine yeah uh i don't have hyperfine just pseudo app install right am i right am i right am i right just pseudo app install super pseudo app install hyper fine isn't that how you do it let's find out if that works uh what am i supposed to let's see uh cargo build release i just wanted to make sure that i was actually running the thing okay release hyper fine and what do they have here here uh build target uh cd target oh here i'm going to take that uh hyper fine uh target release uh foo okay well i'll be damned i don't so so the problem is is i don't know what why this thing can't um i don't know why this thing can't be is it is it is it because this thing is allocated on the stack and therefore it can't be tail call optimized is that what they're saying i don't i don't really the problem is i don't really understand exactly how tail call optimization works other than it it literally it literally like unrolls it into a loop uh instead of having to allocate a new stack frame per function call uh read the text above that's why they say why yeah yeah i saw i saw that because they don't have eager destruction therefore it won't work i don't have mojo installed so i can't actually i can't actually prove this to be true but you know what we can do since we're already here we i mean we might as well you know we might as well just go like this uh index.js let's let's try this out let's uh let's try this out in the old um let's see how let's see how javascript does huh yeah yeah yeah if uh if x equals zero oh yeah i was like why can't i see why this is a problem there we go fantastic uh all right let's let's see how let's see how node does uh let's go like this let's go node um source why why why can't i what just happened there that was weird the  okay that does that can't that like literally can't make sense okay that can't make sense yeah i was about to say there's just it that can't actually make sense okay that was just launching node okay how do i make this do i have to like okay here do i have to create like a i have to create like a run script is that what they're asking me to do uh node index i mean this is this is at this point there's no way that this will be faster right there's just no way right there we go there ain't no way that could be faster right it has to be it has to be orders of magnitude worse cuz we're starting up multiple processes it's going to it has it has to by the very definition be like way worse why command terminated why why why why why why i hate bash sorry sorry bad cop i hate bash okay i hate bash i hate bash i don't i don't even want to hear about it maxim okay well node didn't win this node did not win node did not win the argument increase ram maximum old space increase okay well can you try drop stuff before okay yeah yeah yeah yeah yeah yeah yeah yeah what what let's let's do yeah okay we'll do one more little one more little search drop stuff let's throw that in there okay uh let's go like this let's go cargo build release hyperfine appears to be no difference here if you aren't using vec with capacity but normal vc new it's fast i have no idea why there must be there must be a reason why that is there's some there's something that goes on where you allocate an array with capacity that changes how how it goes on there so i can't say i understand that with a capacity pre- allocates sure but so i mean new new allocates what five slots but look i mean they're doing they're doing something very similar they're saying hey create an int with 42 slots in it right so i mean they're they're also creating a similar sized thing so it's not like so it's not like it's not like they're cheating i don't think new allocates one i don't believe you on that i don't believe you at all new does oh v new does not allocate until push is called i wonder what they're doing so that so that's actually a really so that would actually be super interesting it's it's just super interesting to think about which is what does this actually allocate because if this actually allocates and allocates everything to that size then that's really impressive right but then why use with capacity they're using with capacity to show you something right you do with capacity if you want to create a vector that does not resize itself it's called dynamic vector for a reason it doesn't allocate that's not true it's dynamic allocator or a dynamic vector and it has a size associated with it i'm i one would assume that its capacity is respected okay yeah i with capacity resizes itself yeah i would assume it resizes itself to the size that you pass in but starts at 42 again i would assume that all happens uh anyways okay so this is very interesting there's something going on here that i think we don't understand but we do know that when you say with capacity rust clearly respects the fact that it's with capacity but it has implications if you if you don't do that then it does not allocate therefore for it's just effectively a stack whatever and it can go by pretty quick i'm just curious what they're doing we don't know what they're doing try vex 042 i i i just don't think that really like i think we're kind of we're i don't think we're getting anywhere on this you know what i mean i don't think we're we're i don't think this changes okay it does change a little bit i don't is that because this thing has the same this thing has the same behavior does this thing have the same behavior as new except for when it's first pushed to it then it makes its first initial all a first initial push become allocation that sets its size to 42 we know that i have a faster pc i do have a faster pc uh the compiler must ensure that destructors are called an appropriate time uh which for rust is when a value goes out of scope in the recurs in function the vect has a destructor that needs to be ran after each function call this means the function stack frame can't be discarded or overwritten as it required for tail call optimization i am very cur like is that is that true like are we seeing something that's true because you know the obviously you guys the audience uh have shown something different that it may be a misunderstanding that's going on here how do i know these things are happening um uh let guarantees a stack allocation in rust what there there are magic so now i'm i'm just curious how mojo what they're doing differently right uh anyways we can just move on uh as is required for tail call optimizations because mojo destructs early uh it doesn't have this limitation and is able to optimize with tco even with heap allocated objects i wonder if there's other ways you could kind of test this with rust with heap allocated objects and expecting of destructors and all that kind of stuff being called such that you're able to get out these results in a meaningful way right uh this is 126x the result for my m2 mac give it a try yourself if you don't have mojo you can install it here why does okay so can we just do a quick quick question why does my multi-year old lemur pro apparently produce results twice as fast as whatever the hell is going on here what is going on here uh conclusion we all love a rust at modular and we are inspired by it the tooling is great the tooling is great the tooling is fantastic install mojo right now i don't want to install mojo right now and it currently has one of the best highlevel ergonomics for any system programming language absolutely this is probably the most tru statement ever been uttered absolutely rust for being a systems level language is feels like a highlevel language in a lot of cases right there's also simd for rust yeah but i if i remember if i if i'm not mistaken uh dehydrate mango um simd for rust is much more cumbersome uh but it has pointed two out major problems in the field of ai uh it compiles slow and ai is about experimentation and rapid iteration most ai researchers are experienced with python won't take the time to learn a new language from scratch i completely agree with these two things uh magicians uh but they the rust wizard that's why uh members of our team tried to solve this problem with swift for tensor flow at google which didn't catch on due to the issues mentioned with ai researchers not willing to learn a brand new slower compiled language interesting this is actually like this this point i find actually to be the most compelling or interesting part of the entire article is that at i mean google's a weird place i'm not saying google's fantastic but the fact that they struggled to get people to make changes and that google was using swift for tensor flow all right two is also an ar argument against uh mojo uh it's hard for me to say it's an argument against mojo in the sense that it's it's it's mostly python right it's mostly python and so you're already like if you're already at 85% understanding the language then that's not a huge jump if you could if you could 100x python's performance let's just call it a 100x i know it's not 100x but if you could py if you could 100x python and you had to learn 15% more syntax to do it but keep everything else about python the same would you do it of course everyone's going to do it right that make it literally would make no sense right it's like typescript typescript is literally the same argument would you want to add 15% more syntax to your javascript but get notified when your dumbass thinks you're using something that's a string but it can also be an undefined yeah we're all dumb asses of course we want that now me personally i use i use a js doc which is like 40% more syntax but you get the idea geico can even save you 15% no based got i use i use js do not typescript because i'm actually based uh yeah uh free free your build pipeline stop using typescript anyways members of our team tried to solve this problem uh with swift at google blah blah blah blah we love python uh c++ russ swift julia etc but over the decade of the industry hill climbing these technologies we believe that a fresh start with mojo embodies the only way to make a dent in these ag old problems mojo has already optimal performance for system engineers but still has a long way to go for all the dynamic features that python programmers expect optimal performance for system engineers maybe the phrase would be like good enough so easy a caveman can do it shut up optimal is kind of you know optimal might be a little a little aggressive in this situation uh maybe it has optimal ergonomics and performance cross-section right rust is an excellent choice if you need to put something into production right now uh if you're curious and looking towards the future and want to be early with a language that could be instrumental in the next 50 years of ai give mojo a try we'll be adding ai specific libraries to the package that comes with mojo soon which will uh which we're working on as the killer app to show the world what mojo can do keep an eye out for max in the coming weeks and netflix used to have something called max we killed max so son of a max got taken out the pasture i believe it's on the playstation only playstation 3 something like that anyways uh money money worked on it great guy by the way love that guy at netflix um i'm curious if mojo will go like the go route and have a really strong standard library because that's what it kind of sounds like is that they're they're creating an environment for you to have everything you need like with go effectively with go you could write a web server with with with zero dependencies like it's actually possible for you to write a web server with no dependencies and it will be reasonably pretty good so like can that happen with this i don't know kind of neat okay shut up about my hair okay we'd love to see you guys did this to me uh you did it to me uh we'd love to see you in the mojo community here are some links to get you started there you go anyways um i actually like this this is a great article this is a great article uh i liked what they were trying to talk about and the things they were trying to say uh the pin thing is kind of an interesting one because this is more just like this is purely skill issue this is purely skill issue right uh and i suffer from this this this particular skill issue of rust um and so very interesting this is a very interesting thing here um i'm curious how much this makes a different and if if this is something that that rust is in the process of quote unquote transferring to or whatever like is it actually that much better to the point where everyone is going to be adopting this over the next so much time because if this is the case where it's like this is going to become the new standard then it's not really like an argument for why this thing is good it's just good right now does that make sense it's good now it's not good forever this is not a competitive advantage uh assuming that mli is open source and everyone you know everyone's movie there's there's a lot of assumptions going on there i do like the simd ergonomics though that seems really exciting come to brazil i am going to brazil anyways okay hey good job mojo keep on don't lose your mojo okay austin powers i mean she uh i was a discord with some uh larger streamers and someone mentioned brazil and so then i did the classic brazil meme with brazil mentioned and then nobody said anything about it and i felt stupid