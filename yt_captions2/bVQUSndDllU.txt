hi everyone in the series we're going to be exploring a bit of machine learning with neural networks and in particular we'll be trying to get our network to correctly classify handwritten digits in this first video i just want to introduce neural networks a little bit by way of a simple example imagine we have two types of plants one with red flowers one with purple flowers and one day we decide to measure the width and length of some of their petals i'll call this two attributes x1 and x2 for short now since we have just two attributes we can of course plot this data in two dimensions looking at how the red and purple flowers have formed two separate clusters we can draw a straight line to separate them this is called the decision boundary and if we get some new data and it lies on one side of the boundary we predict that it belongs to a red flower while on the other side would predict it belongs to a purple flower this decision boundary is essentially what we want to on your network to compute so let's start designing a simple network it will have two input neurons for the two attributes and then two output neurons to output the likelihood of the flower being red or purple respectively we can then connect the first output neuron to the two input neurons and the strength or weight of these connections will be the values w 1 and w 2 so then are the likelihood of a fellow being red is given by the equation x1 times w 1 plus x 2 times w 2 we can also connect the second output neuron to the two input neurons to calculate p which is equal to x1 times w 3 plus x 2 times w 4 returning to the graph i'll color the region where r is greater than p in red and where p is greater than r in purple as you can see the weight parameters control the slope of the decision boundary in order to properly separate the red flowers from the purple flowers though the boundary needs to be able to shift vertically let's go back to our network design quickly and add a bias term to each of the output neurons we can see that the biases and the weights together give us enough control over the decision boundary to separate the data however there's one huge limitation at the moment which is that our boundary is always a straight line that's fine for this particular data search but let me now swap this out for something that be separated linearly clearly we're going to need a more complex network for this so returning shall design let's insert a new layer called the hidden layer we can connect the first hidden neuron to the input layer to get a value which i'll call a 1 equal to x1 times w1 plus x2 times w2 plus b1 which is of course our old read output value we can do the same thing for the remaining two hidden neurons to get a2 and a3 next we'll connect the first output neuron to the hidden layer and this will give us an output of a1 times w7 plus a2 times w 8 plus a3 times w 9 + b4 and we'll do the same thing for the second output neuron as well back to the graph i now have a lot more weight and by a slightest to play with but disarmingly i can still only make straight lines this is because all we're doing with our network is adding together a bunch of linear equations the result of which is just another linear equation so we need some way to introduce non-linearity one solution is to use some sort of sigmoid function the one you see on screen is f of x equals 1 over 1 plus e to the negative x for this function small inputs results in an output of 0 while large inputs result in an output of 1 we'll incorporate this into the design of our network by passing the values of each neuron in the hidden and output layers through the sigmoid function by the way this is often referred to as the neurons activation function so now we're changing the weights for a single neuron changes the slope of the activation function while changing the bias shifts the function from side to side coming back to the graph of our networks decision boundary we can see that the introduction of this activation function has allowed us to create a nonlinear boundary which means that with the correct weight and bias values we can now properly classify the red and purple flowers in the starter set of course i'm just changing these weight and bias values by hand at the moment but the entire point of neural networks is to figure out the correct values automatically as you can see happening here we'll look at how this learning process works a bit later on in the series though for now the main thing to understand is how the weight and bias values determine which regions of the problem space correspond with which class red flowers or purple flowers in this example so that it can correctly classify new data when we don't know what class it belongs to remember that we're only dealing with two attributes in this example which is why we can look at it in two dimensions if we had a third attribute though it's not so hard to imagine finding some decision boundary or decision surface if you prefer in three dimensions with four or more attributes ok it's suddenly not so easy to imagine anymore but the concept remains exactly the same so i hope this video has given you firstly an understanding of the structure of a neural network with it's input hidden and output layers along with all the weights biases and the activation function and secondly a general way of thinking about what the network is learning for classification problems in terms of this decision boundary next episode we'll begin creating the network in python so until then cheers