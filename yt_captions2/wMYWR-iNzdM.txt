are lists evil by bj strr creator of c++ magnify from the from the frequently asked questions according to some corners of the web i'm under the impression that vectors are always better than linked lists and that i don't know about other data structures such as trees and hash tables obviously that's absurd yes the creator of c++ probably knows a thing or two about these um the problem seems to be uh be an interesting little exercise that john bentley once proposed to me insert a sequence of random integers into a sorted sequence by the way the reas why i wanted to do this article is we actually watched this youtube video where he explains this and it's extremely interesting extremely interesting uh then remove those elements one by one as determined by a random sequence of positions uh do you use a vector a contiguously allocated sequence of elements or a linked list for example see software development infrastructure okay we're not going to see that i use this example to illustrate some points encourage thought about algorithms data structures and machine architectures concluding don't store data unnecessarily keep data compact and access memory in a predictable manner what was going on here is that it's actually really difficult to do this problem with a linked list now that seems uh unintuitive right it actually seems really unintuitive because you like oh you're removing random elements from within a list use a linked list that seems obvious but it actually might be better and is and you really can kind of prove this out to actually walk the array to the position remove that and move over all of the other elements i know but you still have to rem you have to move over all the remaining elements right so you still have to cost up to that element in both uh in the linked list but you have the other side being less expensive sounds uh sounds like obviously bad to me it's crazy that like if you haven't thought about these problems it is rather crazy to think that it is faster to keep things more compact all right like if you're just not thinking about it which is totally okay right most people just program in javascript right where everything's on the heap uh where you just really have no no idea where stuff is stored you don't even know like how is an object actually stored in memory an object in an array what does that look like right you just you really don't have a good like conception of what that probably is underneath the hood it's probably a contiguous piece of memory that has memory addresses that point off to containers that contain pointers to the object or some weird thing like that you know what i mean like i don't exactly know what goes on but learn a little bit of c and you kind of have a better idea right note the absence of list and vector in the conclusion please don't confuse an example with the examp uh with what the example is meant to illustrate i use the example in several talks most notably going native keynote all right this video has been popular it has been downloaded more than 250,000 times plus another 50,000 times at various other sites my impression is that many viewers fail to understand that the uh the purpose of that example is to illustrate some general principles and to make people think initially most people say list of course i have tried asking that question many times because of many insertions and deletions in the middle lists are good at that the answer is completely and dramatically wrong and it is good to know why again this is a really hard concept right like because we've been taught our whole life that that's that's that like that's why you use linked lists right but it's dramatically incorrect i've been using this example for years i had graduat students implement and measure dozens of variant uh variants of this exercise and different exercises examples and measurements by others can be found on the web of course i have tried maps they are much better than list but still slower than vectors i have tried larger element sizes eventually lists come into their own uh i have used binary search and other uh direct insertion for vectors yes they speed up even further i have checked my theory i'm not even violating any big o complexity rules it is just some of the operations can be dramatically more expensive for uh one data structure compared to another absolutely i have preallocated lists that's uh better than a standard list but traversal still kills performance i have used singly link list ford lists that doesn't make much difference but makes it a bit harder to ensure that uh the user's code is 100% equivalent i know and say that 500,000 lists are not common but that doesn't matter for my main point we use many structures largest small where there is a choice between linked and contiguous representation i know for insertion push to front is faster for standard lists and push the back is faster for vectors you can construct examples to illustrate that but this example is not one of those honestly this is super super good things to think about because i think what this goes to show is that often we simply kind of use these guiding principles right and let me show you something that i think will probably drive this point home really really well uh here we go so what i have here is the following i have some ways to control this data structure i have a string that has a bunch of unique characters in it and then i have an iterator effectively that creates a string that does not repeat until certain amount of characters have been added uh that certain amount of characters of course is going to be 800,000 so at 800,000 it starts producing a string that has a longer amount of unique uh in here what we're going to be doing is we're going to check we're going to see can we can we find x amount of unique characters in a row so i've created a string that produces the worst case over and over and over again and then at 800,000 will produce a string with that many unique characters right and so i just keep on going through and i do a little uh check for the uh for a set and i do a little check for an array same thing right and i compare i can compare them so i'll run one of them 40 times and run the other one 40 times if i jump in here and say um what the hell is this thing uh let's just say that we start with 10 so we do a a size 10 so we we produce a string with nine unique characters up to 800,000 characters and then produce uh a 10 unique character string at 800,000 let's see how long it takes for a set it takes approximately a second for an array even though i'm doing index of checks it's like half the time and you can just do this over and over again and see the exact same thing it doesn't really change right an array is faster at finding uh elements within it than a set is when there's 10 elements within a list okay so maybe that's kind of confusing to a lot of people a lot of people probably wouldn't actually um wouldn't actually think that to be true right we can move it up to 20 it's still the same thing and your you know your computer will produce different results uh you go up to 30 and now you start seeing like the real difference start happening right now you're starting to collide into something that makes sense now arrays are the same speed maybe slightly uh maybe slightly slower right we had one where it was slightly faster we had one where it was slightly slower we're getting different results all the time obviously i'm not trying to be very precise here right i'm not trying to to do something good this is just me doing a quick check right and it's clear that there is things underneath the hood that you may not realize it's not javascript that's not javascript that's any language do that with any language right you can do this with any language ever for a big enough one and a small enough uh n o of one is slower than o of n greg oh my goodness we have the creator of leptos here i absolutely love seeing this by the way leptos is probably the greatest templating system currently in rust i hands down agree with that uh absolutely love it uh greg i was actually uh doing i was working with shuttle rs and uh doing more things with uh uh htm x and leptos i love it absolutely love it but big o of one can be slower than big o of n right because remember when i'm doing this when i'm doing this check right here i'm checking for that character every single time right so i'm doing a a linear walk through the list looking for this character whereas i'm doing an o of one check by adding the character to the set and then rechecking the length right all o of one means is that performance is independent of input size correct and this is a key point that a lot of people miss o of one can still be slow because really o of one o of one is an incorrect way to state it oh it's a correct way to sayate it but it's it's incorrect it's o of c1 where c is some constant right it's some cost of the actual operation we drop all constants in big o representation but they actually have meaning so you can think of something that is n^2 versus uh o uh o of one or o of n and you forget that maybe in front of the n square it's actually just a three but in front of the n it's like a 10 it's like a thousand and so n squar is actually faster for some amount of time than n it's just always good to think about that's the whole quick sort versus bubble sort thing sometimes it's faster to bubble sort than it is to quick sort just something to think about even if bubble sword yeah look at that uh it's faster than merge sword well merge sword is just just slow merge sword is a very silly algorithm uh don't use merge sort merge sort creates new memory you quick sorts merge sort without the creation of new memory effectively that's how i think about it anyways um hey great nice to see you i always appreciate seeing you uh i'm i greg so for me in the programming world greg is kind of like my hero if you're wondering when i look at people that i really look up to greg is one of those people um all right my point is not about list and such they have their uses but this example isn't one of them please don't confuse me or please don't confuse the example with what the example is used to illustrate the example is about use of memory we often create a data structure do some computation on it requiring often traversal then delete it the order sequence is simply an example of such use and an example is presented to get people to think about what matters in such cases my suggestion is don't store data unnecessarily keep that data compact access memory in a predictable manner i emphasize the important uh importance of cash effects in my experience all but true experts tend to forget those uh those when algorithms are discussed and yes my recommendation is to use vector by default i actually think this is a great great piece of advice just default to using a vector or this is called a dynamic array effectively it's a contiguous memory space holding stuff always default to using that until you know you should be using something else all right more generally use of contiguous representation unless there is a good reason not to like cc++ is designed uh to do that by default also please don't make any statements about performance without [applause] measurements [applause] love it really don't you should just not ever think something's faster until you've measured it right real talk even if you believe what you're creating will be faster start by measuring first to know that you're fixing something that's slow right uh i've seen a case where changing a 0o to2 uh element list to a 0 to2 element vector made a factor of two difference uh to an algorithm i didn't expect that nor did other experts looking at the code yeah again beautiful i absolutely love this article this article was incredible um i know it's old but these are good things to return to right these are great things to return to great things to really think about great things to actually consider when programming which is like you don't have to be clever you can use a set because they're easy to use but if you're looking as a performance first thing just consider using a vector right it's as simple as that and then when you know that it's slow do something else dick measure absolutely i just read everything you say big o is about velocity not about speed isn't it uh that's not that's not what a big o is big o is the i mean you could say that i mean if you're i guess yeah in a sense you're correct bigo is about uh is not necessarily even about velocity because even in that example of big o uh big o n squ that is not actually true because it's actually big o n s some c of n uh plus some plus some d right it's actually it's it's more than that it's about what dictates the growth of the algorithm that's what big o is and so yes you can call that velocity fair the first derivative yeah it's kind of like whatever is the biggest element in the first derivative that's the one you keep fair i'm solving every problem with the vector well just i mean there's complications with doing that as well but it's just like a tradeoff you have to be you have to understand that you're making i think that's probably the better observation to make is understand the trade-off you're making don't just think something is better or worse you know what i mean it's not a great analogy though it's not a great analogy okay just drop the whole acceleration versus velocity versus all that okay the best way to put a big o of n of course is that the it's it's the amount of expected growth in the algorithm's runtime or space based on the size of the input if you're input doubles and you have an n squ algorithm you can expect your runtime to quadruple or your memory to quadruple and that's in rough sense it's not perfect the name is the prime but