- oh, is this new whonnock?
- this is new whonnock. - oh no, the dell is new new whonnock. - yes, well no. the dell is (beep). - you called it new new
whonnock. confirmed! maybe i'm just a sucker for punishment, but after this thing kicked
my ass for over a year. when i was trying to deploy it as our main video editing
nas at the office. it's back baby! new new whonnock! - [jake] whonnock two. - but this time, i'm gonna be deploying it at home thanks to our friends at kioxia. who sponsored this video. sending over, that's right. the intel drives are out, and these kioxia cd6 drives are in. they sent over twelve of their four terabyte cd6s. which is gonna make this the ballinest nas, on the block. - [jake] not four, eight. - eight? they sent over
eight terabyte drives? - [jake] well, 7.6 i think? - holy (beep). this is gonna be like a hundred terabyte nvme nas, for my house. how did i get this out of the office? (upbeat electronic music) the first thing we're
gonna need to do today, is downgrade the sever because as... (beep) because as much as i would've loved to take off with 256
gigs of ecc ddr4 memory. i think jake would've crapped a brick, when all of a sudden he goes to like, perform maintenance on
a server, upgrade it, and all of our high spec memory is gone. so let's get all that out of here. and while we're at it, our thirty-two core epyc 7502p. that's gonna have to go too. the good news though, is that the lowest-end chip that we have to replace it with, sitting in the office, is a 7402p. which is still really frickin fast, and twenty-four cores i think? - yeah. - nice. epyc rome is such an
amazing platform because even the lowest end chips. still have 128 lanes of pci express gen 4. that means that, even if i populated all
twenty-four bays on this chassis, could have access to the full bandwidth of a pci express gen 4x4 link. now on this particular board, four of my bays are gonna be running at pci express gen 3 speed. i think it might be the
ones connected here? yeah, it's this card right here that's in the gen 3 slot, but that's not a limitation of the cpu. these things are obsoletely nucking futs. honestly, by the time we've populated all twenty-four of those bays with gen 4 nvme drives. we would run into memory
bandwidth bottlenecks, just trying to read or write to them so. it's a nonissue for us. especially because i'm not even using 3200 megahertz memory which is the maximum spec for epyc rome. i ended up with these, just about the most (indistinct) ddr4 ecc that you can get. these are eight gig, 2666 modules. they're from crucial so i expect them to be reliable. but they are not going to be fast. next order of business, finally, is an actual upgrade. one of the things that
my server needs to do, that most store servers wouldn't, is transcode video on the fly. i've been pretty upfront about the fact that i have an extensive blu-ray collection, but i consider the act of, grabbing a physical disk and putting it into a drive to be a very last-decade kind of thing to do, so. in spite of the fact that, ripping your own blu-rays is kind of a legal gray area, i consider it to be morally okay. and besides, what am i gonna do? have a bunch of blu-ray players all over the house and physically go and get the disk to put in the... like come on. all of it is gonna be served from here, but, because not every device is capable of playing back a full quality blu-ray. (tongue clicking) that's where your gpu comes in. so this is just a pretty basic gtx 1050 from msi. that happens to be low-profile so it will fit in our case. and this is gonna handle, downscaling our original
4k hdr whatever to, something that's a little more, smartphone-friendly, or whatever it is that you need. of course, in order to get it in here, we're gonna have to do a little bit of re-conjigering. with step one being, yup, removing the fans. now, this might seem like a bad idea, but given that we're installing
this card in a sever, and these heat sink fins are actually oriented the correct way. i would expect this card to run just fine. even without the fans on it. so we're gonna go ahead and pop it in here, and obviously, we'll find out if we're wrong about that later. next, we've got to upgrade
the onboard networking, because believe it or not, this server only comes
with gigabit ethernet. but that's perfectly normal, because while gigabit could ship it with onboard ten gig for example. anyone rolling a server like this is gonna be running a hundred gig, 200 gig, or even higher, in order to take advantage of the insane speeds of the drives, that you can hook up to it. so those ten gig chipsets would be completely wasted cost because they wouldn't be used. even for us, this ten gig card is going to be a placeholder because we still don't know exactly what kind of switch or switches we're gonna use, so we haven't bothered sourcing any fiber-optic transceivers yet. now one quirk of this
particular motherboard is that this top slot is labeled optional in the manual, and as far as we can tell, in its current configuration, it is not working. we think that's because this mezzanine slot here takes the bandwidth that would otherwise be allocated to it. so four of my slots at the front of the case are actually not going to be usable so i'm just to electrical tape the connectors here and tuck them away in the chassis. that's okay because remember how i said that if we actually populated, all twenty-four of these bays, with the drives that we're using, we'd run into memory bandwidth bottlenecks before we actually were able to use up the full speed? well, that's true. so there's no point filling
all the bays anyway. of course, what we end up using depends on what we decide to do with the storage in this server because the sky is the frickin limit. now you know, blu-rays they're fifty gigs, they're big. but that is nothing for an nvme 1.4 compliant drive that is capable of throughput in excess of six gigabytes per second. so we can do everything from use it as a storage server, which obviously i'll do, and i can stream my plex movies to as many devices as i freaking want. but, i can do more than that. for example, all of the computers in the house, instead of actually having
drives installed in them. they can all network boot off of this array, so all of their storage is safe and centrally stored in one place. that'd be really cool, because not only are the drives built to an enterprise standard. that means full power loss protection, a variety of different security options available, and depending on whether you go for the regular cd6 series or the read-intensive cd6 series. anywhere from three to one drive, full, full! drive rate per day endurance. but because we're gonna be running them in a zfs array. we get access to all kinds of cool zfs features. like, for example, the ability to really quickly and easily create snapshots. say if i wanted to back up some or all of my data to a server at the office, i could totally do that, or file system level compression which would allow me to effectively stretch the approximately
ninety-something terabytes of raw flash storage. that's a lot, but, hey if you can have more and it costs you literally nothing. then, that's definitely the way to go. you ready? let's power this puppy on. these (indistinct) they're gonna go. just give em a second. there they go! see that's why we weren't worried about taking the fans off of the gpu. cus, they're going! there's your gpu fan. look at this baller with his like a hundred dollar bills. - ridge wallet. - actually no, but should
mention lttstore.com. hey, got this new cpu reflected design. - whoa! new lanyard what? - whoa, new colors of
lanyards? lttstore.com - while the cli side of things might sound a little daunting. fortunately for people like linus. you can get zedfs on linux really easily, with a ui. it's actually available as a package in the community app store. so all we gotta do is search up unraid community apps, and you can copy the url. paste it into unraid in the plugin installer. give that a few seconds. - literally a few seconds. - it's already done. - yup - and then bam! you have a new apps tab. now there's a lot of really cool docker stuff, there's a lot of really cool, you know, unraid plugins in here. but for us, all we gotta do, zfs, return. hit install, and now we have zedfs. just that easy. - sweet. - it's downloading still actually. - give it a sec. - okay it's done now. - because of the limitation of unraid we still do need to have an array. so we just grabbed a couple of old sata drives that are, like i wouldn't put
anything important on them, they have relocated nand flash already. like they're just there to exist. - yeah. according to the unraid guys, they're actually going to fix that, in a few versions from now. give em, you know a couple months
- yeah. - and theoretically that
should be a lot easier. - and there's gonna to be a
ui for as zedfs as well so. - which would be sweet. - yeah, but zedfs is
pretty simple actually. if we go into our command prompt, we can just type zedfs list. see no data sets, is equal list. no data sets, we have to make our data set. - yeah. are we just
gonna do a single vdev? - so that's where zedfs can get a little
complicated for some people. how do you want to configure it? - i'm probably just gonna go single vdev. - i'm thinkin a single vdev with raidz2, so that would give us - yeah. two parity drives. - yeah. - so we're gonna lose fifteen terabytes or whatever. - i think one parent v
drive is probably fine. - you think? - this is literally like data center drives, and i'm gonna back this
up to the office anyway. - fine, fine. okay, we're gonna call her z pool lambo because. - because it's (car revving noises) fast! woo! - so we're gonna go down to raid z one, which means one parody drive. - yeah. - and mnt/lambo is where it's gonna be stored. ba da bing ba da... - if you don't know what it means to have one parody drive, it means that one of these twelve drives could outright fail. which is pretty unlikely, and all the data would still be completely intact. in fact, the speed of the array would probably be, not even degraded to the point where it would be affected.
- probably wouldn't even notice, no. - you probably wouldn't notice. - there we go,
- oh! - 83.8 terabytes raw. - so sexarific - but after parody it'll be a little bit less than that. you get 72 terabytes. - that's crazy, so...
- that's a lot of storage. - yeah, what was my old one? - okay, while he does that, we're gonna set up a few other things. so one of the nice things about zedfs, is we can create multiple data sets, within our pool of storage. and those are useful
for a number of reasons. primarily, that we can
define different settings for different datasets. they're almost like folders, but i can say my movie folder doesn't get compression because video doesn't compress very well, but then i can say my vm storage folder, or
data set in this example, does get compression. - so my old unraid server was sixty-four terabytes of spinning hard drives, and i had two terabytes in raid one as a cache. - that's pretty nice. okay, there we go, so those are all our data sets. now we have to do the not so fun part of moving all of the stock unraid things, over to the zedfs pools. - some of it we can probably leave on those crappy drives like isos, come on. - you could, but now that
we've already made the thing, we might as well just change it. it's not actually that hard, we just have to go into
vm manager, turn it off. - you know what the most fun part of building something totally overkill like this is? trying to find a use for it, because you end up. no i'm serious though! you end up exploring all these cool new use cases that you would never have had any reason to explore. - yeah. - like just the idea of having all the storage for all the computers in the house. just on this one box. - yeah, why would anyone do that? - once we figure out how to do it. you don't have to do it with such crazy overkill hardware. - yeah, well that's a whole other video so get subscribed so you don't miss that.
- yeah, don't miss that - cool, so now we should see some usage on our zpool. haha! used six megabytes! - nice. - zedfs is not really designed for nvme storage, it works, but there are some caveats and things you kind of have to do to make sure it plays nicely. the people that developed zedfs never had in mind, that your storage was going to be as fast as the memory on the system. like in what world? - storage now is probably faster than cache was on the cpu when zedfs was created. - yeah. so there's a few
things we need to do. for one, the arc cache in zedfs, which is really great for accelerating hard drives. we're going to set that to be metadata only, so it doesn't store
any actual files on it, just the metadata of the files. if you use arc with nvme. it's probably going to hurt your write performance. - especially at this speed. - yeah. - these drives are what you might use as an arc cache on a hard drive array. - on a level two arc, a level two arc. - yeah. so level one arc would be ram, but like jake was saying even ram is not
- yeah, doesn't even... - that much faster enough. - another thing that's important, we have to enable auto trim, i think a lot of the times now zedfs is smart enough to do it on itself, but we're gonna do it just in case. - if the drive is in trims, there's a lot of wasted extra writes that can happen it's called write amplification, with every write. so you want to keep your house in order effectively. - we're also gonna disable access time which is something you might use for very specific use cases. for us, it doesn't matter at all. that's gonna save us some reading rights. you know what? i'm just gonna set compression on across the entire array, because the nice thing
about lz4 compression, on unraid, is if its got a big file, and it's not compressing it will just give up. - oh cool. - so chances are it's not really going to hurt our performance at all, if there's problems down the road we can set it to off, for
say, the plex library, and then on for the vms, like i said before, - yeah. i still can't believe that i ended up with what was supposed to be new new whonnock at the office. - no new whonnock, not new new. - new, new new whonnock. oh, is this new whonnock?
- this is new whonnock. - oh no, the dell is new new whonnock. - yes, well no. the dell is (beep). - you called it new new
whonnock. confirmed! - oh cool, do you see em? - [linus] yeah, everything's there. zfs docker zfs media system. copy a file over there shall we? - you got a big file? - i got a big file, i got a hundred gigs screencap, that i accidentally forgot
to stop recording so. here you go, there's your big, (bleep) media file. oh! - ah, ah, ah. - i need permission. - oh yes, i gotta... - whoops. so normally
this would be managed through the unraid gooey, but... - well, it would just work by default but. mnt/lambo, i think we have to show nobody users. let's just say everything in this folder, sure. cool. now try. - all right, try again. - haha! - aye, there it goes!
- whoa! not bad. wow, man what a big improvement this is gonna be. - look - oh, well there's your limiting factor. - it's probably smb actually. - yeah. so, - it's not multi-channeling. - it looks like we're single
thread, limited on here. - either way, this is way faster than we have right now. - we should find out, how much performance we still
have left on the table here. just by doing a quick...
- what? - just benchmark. - oh! - i've got about a minute left transferring these files and then you can go ahead and benchmark it. - what if i just do it right now anyways? - sure, i mean, i guess that's what we want to know right? - i mean we're writing at, like, fifteen to seventeen gigabytes a second. is that fast enough for you? - [linus] i don't even feel it. - [jake] you don't even feel it? - [linus] yeah! - [jake] well it went down to like, ten, twelve, twelve, fourteen. - that's literally a hundred times faster than what i have now. one hundred times! i'm gonna need one hundred
gigabit networking at home. - well it's a little late. - we're not gonna do that. - we could do that. - we're not gonna do that. twenty-five! - should we do that? - now obviously ten to
fifteen gigabytes a second is a lot less than the sum of the speed of all of these drives. in a normal data center environment, you might be able to get a lot closer to raw
speed from these drives, depending on how they're deployed. somewhere in the neighborhood of more like sixty to
seventy gigabytes actually. one of the big challenges though, one of the best benchmarks of how good a plex server is, is of course, whether
you can enable subtitles. - [jake] groans meekly. - okay, i'm on the
outdoor access point now. we're playing original quality with subtitles baked in. (loud thumping) so i'm just gonna leave
this here, alright. epyc gives zero fs about this workload. we could conceivably have a dozen people doing the same thing at the same time. - (laughing) basically. - and the best part of all of this? zero funky behavior between these drives, and this server. match freakin made in heaven which leaves, no not that one! i was gonna pull the one that had the nice cd6 logo on it. anyway, jake is doing the demo where we show that, video playback is
completely uninterrupted. even by the complete loss of one of our drives which is very unlikely to happen. - [jake] there we go, look unavailable. degraded, but it works. look, it did have a little hiccup but. haha hiccup get it? - i get. - but look now they're working again. - i think i might be in love, just like i love telling you about our sponsor, kioxia! you can get all the details on their cd6 series enterprise drives, at the link in the video description. but the main things you need to know, high quality nand, lightning fast pci
express gen 4 interface, and of course, their long time proven track record for delivering reliable
enterprise grade drives. (exciting electronic music) if you guys are looking
for another video to watch, you can maybe check out the last time we tried
to deploy this server with someone else's drives when it didn't, it did not go well. - (laughing) that's a
bit of an understatement.