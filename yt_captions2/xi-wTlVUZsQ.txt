if you didn't know any better you would think that gpus and cpus were the same they'd both take in data from the user they both do some kind of math operation on them and they both returned them to the user either in the form of graphics or computational results i made a video about the physical limitations of cpus and why cpus can't have thousands of cores but a lot of the comments were well hey man gpus have thousands of cores so in this video we're going to talk about why gpu microarchitecture makes cpu cores and gpu cores a little different than each other your modern gpu has on the order of 5 to 10 even some 15 000 gpu cores modern gpus such as the latest nvidia 4090 is able to perform 49 teraflops that's 49 a trillion floating operations per second the reason that floating point numbers are so important in graphics processing is that regardless of how beautiful your graphics look at the end of the day all your gpu is doing is crunching numbers these number numbers are information from a scene based on lighting information like texture values and shader values that are all getting put together to determine what color they should make a pixel on your screen this is all based on very complicated matrix multiplication and vector math that we're not going to touch on on this video but just know that most of the time these values are stored by very high precision floating point numbers that your gpu needs to know how to process to determine what color to present on your screen compare this to the intel core i9 13900k only able to perform 849 gigaflops per second that is 50 times less than our nvidia 4090. with these numbers you'd probably think the gpu has a computational advantage and the numbers do play that way but there is one slight caveat if that's the case why are we not using gpus for everyday computing why not replace your cpu in your computer with the processor running in your gpu this is because gpu cores aren't exactly a core let me explain gpus are able to perform high throughput high bandwidth floating point operations because of some very delicate design choices that made them good for high-speed math operations but bad for general purpose computing at the core of a nvidia gpu is well a core this core is the execution engine for doing the algorithms that is responsible for giving you cutting edge graphics inside a cuda core there isn't much the cuda core has four primary components first an fpu or a floating point unit to conduct floating point operations an int unit to conduct integer operations on scalar values a dispatch unit to receive the data from its higher level scheduler and a results queue to give the results back to the higher level scheduler while that sounds simple there are some very strict limitations with this design when a gpu receives an instruction to run that instruction is received by a scheduler that scheduler hands that task out to a thread to run that instruction and then the thread makes use of the cuda core to do the math operation in the classical cpu example think of a thread like the control unit and the cuda core like the alu now these threads are organized in groups called warps that's w-a-r-p like warp speed and warps contain 32 threads per warp to make execution fast warps use a design principle called sim d that's single instruction multiple data by doing this a warp will have all 32 threads run the same instructions on different data as they run doing this enables the gpu to do high bandwidth operations on large amounts of data like a graphics process by running the same instructions in parallel on bulk data the gpu can outperform the cpu in terms of floating point operations this does create constraints for your program though if threads in the same warp take a conditional branch only threads on the same path of execution will execute the rest will block let's say for example that a warp is executing this block here of nvidia ptx pseudocode all the threads are parsing data and if the data indexed by the thread id is even then condition x happens otherwise condition y happens let's say for the case of the example that half of the threads meet condition x and half of the threads meet condition y because of the sim d principle that the warps are designed around the y condition threads will not begin to execute until the x condition threads are finished executing and return to the common part of the execution path which is the area after the conditional jump in the if statement simply put a warp can only execute one instruction at a time therefore limiting the computing power of your gpu to the number of cuda cores divided by the warp size which is 32. cpu designers on the other hand treated their cores much differently each core is able to run an arbitrary set of instructions organized into an arbitrary set of process sees that are constantly context switching inside of the kernel each cpu core is designed around the fact that these instructions may randomly branch to any instruction or randomly access memory at any time during the process execution each core therefore has multiple layers of caches and branch prediction engines that protect the core from encountering delays during execution gpu cores have neither of these at the end of the day cpus are designed to process programs that run on arbitrary input everything from your word processor to the game you played earlier today to the tcp stack that brought you this video were all executed by your cpu cpu cores are therefore more generalized the gpu on the other hand when designed had a much more narrowly scoped task because of its narrow scope the gpu designers could focus on this specific task and make it more efficient at doing this than everything else cpu does so do gpus have thousands of cores yes they do 100 they do these cores are a amazing and can do math at unfathomable speeds but are these gpu cores that do exist in the thousands the same as cpu cores no not at all cpu cores because of their design to handle well everything are just that a jack of all trades but a master of none so to compare cpu cores to gpu cores is questionable at best oh [music]