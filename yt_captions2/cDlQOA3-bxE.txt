my node.js is a bit rusty replacing an internal node.js module with a native rust module made a 25x purse boost let's understand why september 3rd 2023 and by the way it's only september 6th three days away from the big day you know what i'm talking about the big day you know what day i'm talking about this blog post has been residing in draft mode for quite a while now i finally decided to just publish it let's see let's see as such a few things might be a bit off but that's life yeah always just share stop worrying so much about if it's the perfectest back in 2020 my team at wix launched a new internal product sorry i lost my place called a cdn stats as the name suggests cdn stats is a platform that displayed displays and aggregates data and statistics derived from the wix cdn okay uh that same year i delivered a presentation about an experiment i conducted this experiment which involved rewriting a single module into a native node.js add-on using rust resulted in a staggering 25x performance improvement awesome awesome awesome this platform empowers front-end developers at wix by providing real-time data on the usage of their compiled assets by our customers including platforms downloads uh per platform response size transfer side these metrics allow front-end developers to identify which bundles need optimization and the urgency of these optimizations it helped us rectify a critical issue that we might have otherwise overlooked due to a human error one of our primary javascript assets ballooned to 33 megabytes that's so much shitty code like how do you even get that many code units like minified javascript per character is like one of the most dense semantic languages ever how do you even accidentally do that because it inlined uncompressed a savage without oh okay well that i didn't read the next sentence i didn't read the next ends this actually makes perfect sense without externalizing it or even dynamically importing it okay that's a big savage why do you have a 33 megabyte savage that that also doesn't make any sense or maybe 32 megabytes this project seems fairly straightforward doesn't it all we're doing is counting and summing up values toss in some indices in a database present it with the data table in viola but is it really that simple nothing is that simple even the most simplest projects aren't simple how does that work in order to populate the data table we need to sift through the cdn logs these are tab separated values oh nothing like a good tsv if you're not getting patted down by the tsv what are you really doing in your life you know what i mean what are you really doing let me guess dot split considered harmful is that the answer because you know what they're gonna do you already know for a fact they're gonna like dot split decompose i mean the amount of garbage they're going to create is incredible uh we're talking about 290 000 fi uh files per day which can amount to 200 gigabytes okay so every day we download the tsv files from the previous day parse them into meaningful information i'm surprised you don't just stream them out you know seems like a good data just to just to stream out those sobs uh just because the downloading them all at once seems like that would just be the largest part i'd be shocked if downloading them took less time than processing them uh so every day we allowed the tsv files from the previous day parsed them into meaningful information and store this data in our database we accomplished this by enqueuing each log file into a job queue the strategy allows us to paralyze the process and utilize up to 25 instances that's an odd like hard number to put in there to parse out the log files within approximately three hours each day i also have another question why not just have one big ass instance with a lot of uh with a lot of cores and instead of having like 25 individual instances why not just run parallel you know what i mean we just learned about parallel the gnu utility what's an ass instance um pick i'm not sure where you got that from to tell you the truth i was i was ready to follow that joke to the end i was going to take it and run with it as fast as i can and really lean into the ass instance but i just couldn't i couldn't see it i could not see the joke and now i'm emotionally scarred then every few hours we generate an aggregation for uh for the previous week this process takes around one hour and is primarily executed in the database using aggregation that's me uh shedding a tear for aggregation aggregation don't get your things in the wrong order because we're gonna screw you uh mongodb query was pretty intense okay let's let's check it out let's check out anytime you can tweet out your query you know for a fact it's going to be great okay that's this is i mean that's just what is what is this tokyo night you're just showing me your tokyo night theme okay not impressed not impressed uh so what's the problem parsing gigabytes of tsv files might seem like a tedious task sounds like a computer task to me investing so much time and resources on it well it's not exactly a joy to be brutally honest having 25 node.js instances running for three hours to parse 200 gigabytes of data seems like an indication that something isn't quite right agreed completely agreed with this because i i'm still i can't figure out why that's required like someone said well isn't something about i o tasks blah blah how long do you think it takes for your disk to shell out 200 gigs how long do you think it takes for two computers sitting on the same network to shell out over the network create the space and update you as they come in like it's gonna take a while but not three hours i'll tell you that much it's gonna be in the order of seconds 10 sacks oh this is an arch user channel okay we don't we don't talk about sex around here 6.6 6.9 for 20 seconds right the solution that eventually worked i i would just assume in the order of seconds maybe one minute you know what i mean like it's gonna be a short amount of time i would not expect hours like if you saw hours something's just wrong the solution that eventually worked uh was not our first attempt initially we tried using internal wix serverless framework we process all the log files concurrently and aggregated them used you and promised at all isn't that delightful however to our surprise we soon ran into an out of memory issue really you're telling me you didn't see 200 gigs being a problem even when using uh utilities like p limits to limit ourselves to just two jobs in parallel p limit why not just parallel dog uh so it's back to the drawing board for us our second approach was involved uh involved migrating the same code verbatim to a wix node.js platform which runs on docker containers in kubernetes cluster we still encountered out of memory issues leading us to reduce the number of files processed in parallel eventually we got it to work however it was disappointingly slow processing a single day's worth of data took more than a day clearly this was not a scalable solution reasonable uh so we decided to try the job q pattern by scaling our servers to 25 containers it's weird that they just you know shouldn't you just put n a configurable amount of containers we managed to achieve more reasonable processing time but we can truly consider running 25 instances to be reasonable not on this this just seems crazy every part about this seems crazy i'm glad that he recognized right away perhaps it's time to consider that javascript may be the problem you know what you could do a really simple thing to do is here i made a i made something last night i made something last night that i've kind of been working through that i think is really good way to look at it this is a server in which it's very very simple what is happening um let's see uh what is it called shooter shooter okay it's very very simple i have a cargo task that runs and i can say how many parallel jobs that's the q parameter and how many total games should be played uh then i run the server over here and the server just logs out some basic data or i can run it with the inspect flag to see what's happening when i do that this is what i get so no matter how much i scale how many concurrent games being played i have a whole bunch of idle time on here and not only do i have a whole bunch of idle time i also have huge gigantic uh garbage collection uh things that happen and so you can kind of see that you're running up against a potential issue here and the issue is not anything other than javascript itself i know how to get rid of most of these idle times and guess what guess what the answer is don't use a promise you have to be a little bit more clever about not using this isn't that shocking it's shocking to me at least anyways uh perhaps it's time to consider that javascript may be the problem there might be certain issues that javascript simply can't solve efficiently especially when the work is primarily cpu and memory bound areas where node.js does not excel but why is that so node.js is a garbage collected vm node.js is a remarkable piece of technology it fosters innovation and enables countless developers yeah all right uh can i can someone get me a flag flag on play unnecessary dick riding uh v8 determines when to free up memory this is how many languages are designed to optimize the developer experience however for each application explicit memory usage is unavoidable uh let's analyze what approximately transpires when we execute our simplified tsv parsing code for a way ooh nice and async iterator well done uh cons line read line of stream const field split you've just created a bunch of pointers http status numbers that if status is less than 200 or greater than 299 continue hate your lack of score release but whatever records push path name this refer that okay yep we see what's happening here the quote is quite straightforward by using a read line we iterate through the lines in the field and uh let's see in a stream to circumvent memory bottlenecks caused by reading an entire file into memory yep for every line we split it by the tab character and push into a push an item to an array of results really you're not really circumventing pretty much anything here you're going to take most of that line you're going to recreate it into these files and you're going to hold on to it and guess what's going to happen you're going to still blow your stack you're still going to use too much memory but what's happening behind the scenes in terms of memory line dot split is invoked and we get newly allocated array containing multiple items each item is a newly allocated string that occupies memory so this is not technically true what likely is happening is because it's using a substring it's actually going to be a it's going to be a rope string right so it's going to be it's going to be a rope so it's going to be a pointer it's going to be a fat pointer you're going to get an offset and a length or not even a fat prone yeah it's like a fat pointer you're gonna get an offset in the length that's all you're going to get for this and that's effectively what's happening underneath the hood uh this isn't a way that array.split works it creates a new array in strings every time it creates new array and new string references every single time absolutely the intriguing part here is that line and fields won't be cleared from memory until the garbage collector determines its time to do so we find ourselves with ram filled with redundant strings and arrays that needed uh need cleaning it's hardly surprising that our computer wasn't thrilled with the situation yes you'll still use a lot you'll still use a lot a lot of stuff finding comfort in rust as an intermediate restation i do have a fairly successful open source project written in rust called phenomenon i'm well aware that memory management is one of rust's key strengths the fact that rust does not require a runtime garbage collector makes it suitable for embedding within another language and virtual machines positioning itself as an ideal candidate for implementing such a feature this is true anything that manual memory management is ideal especially for this environment because node requires you to have some sort of lifetime an implicit lifetime to your memory whereas rust requires an explicit one so it's able to manage all the memory it uses kind of in inside of its own little world and just give out what it doesn't need to hold on to anymore and and the the the neon api is actually really it's really slick the neon api is super super slick i've shared a few tweets expressing my beliefs building applications and rust is powerful but the ability to embed rust in a battle tested vm without altering the rest of your application is a true game changer i agree this is actually a really great point right here i love this point creating a service in node.js and only using rust when necessary can help you accelerate development and replace modules with more efficient ones as uh as and when required using node.js is the entry point of our application enabled me to leverage wix standard storage database connectors air reporting logging and configuration and management without the need to re-implement these on the rust side this allows me to focus solely on what matters most optimizing the slow part let's go let's go uh fortunately embedding rust in node.js is a breeze thanks to the nappy rs an excellent rust library i think now it's called neon i think neon's the one that people use at this point uh that facilitates seamless integration between rust and node you simply write a rust library use some fancy macros and viola he loves that term uh you have a native a node.js module written in rust all right let's see what we got here uh correct restations and wrestling so one thing that's different about rust uh i don't think you're getting like i i honestly don't think you're actually using that much less memory with rust here i think it's i think it's getting rid of the memory you're not using that's really helping out here uh and plus there's a whole bunch of like pointer cost in uh node.js that's just not present in rust let's see ruskin i can begin by declaring my structs as irrelevant data structures i want to parse into a typed driven development workflow i start with a record the most basic line i want to parse from the tsv record set instead of designating every part of the record as a string an owned string i can force into a reference yep there you go bada bing bada boom this is just a this is a fat pointer right it contains an offset in the length uh note by making record contain uh contain slices of string with an a lifetime we're essentially stating that record is a derived data structure that doesn't alter the original data that implies record cannot outlive line an advantageous feature it aids us in developing our application while being mindful of optimal memory usage all right so let's see what they do there because i'm wondering if he does something different here is there some sort of some sort of thing oh someone's saying that i'm pronouncing something very wrong viola is french actually the way he's pronouncing is uh it is something really wrong in spanish oh really well viola these nuts have you thought about that one all right to maximize our capabilities of a browser caching static assets and deployments of oh hold on oh yeah that's where we're at however merely deriving the raw data isn't a sufficient for our use case to maximize the capabilities of browser caching static assets are deployed with a content hash as part of the file name instead of having artifact.file.js we have artifact.file.abcd uh hexadecimal.js files allowing uh the browser to cache them indefinitely okay nice uh for our aggregations we want to remove this content hash furthermore we want to infer the artifact name and the path name this is where we introduced enhanced row enhance let's say okay path name refer i'm surprised you didn't use a path buff uh artifact or a path or whatever one is the equivalent of a that file name a cow ooh nothing like a cow you know for a fact that once you get to a cow you've reached new levels of rust stationing they could use some power peg in this code they could as we can see enhance row incorporates everything that a record a does but it enhances it with a few values artifact a reference uh part of the path name file name is a copy on right just as we mentioned we attempted to remove the content hash from the file if we don't find the content hash there's no need to copy the string if we do find it copy we copy the string this type allows us to be incredibly explicit with our intentions yep after defining these two structs we implement a try from okay i love a good old-fashioned try from and a try from record oh beautiful i love tri-froms this is like one of my favorite features of rust is the the trade system and it's a and and it's like ability to include on any type and just make things feel very standard i love this part i love it i love the fact that there's very like there's a good rail type programming when it comes to basic operations and tri-from is one of those like kind of like here's the rails just go on the rails and use what we've already provided i love from and try from i use it all the time uh next we create a new struct called resource counter which performs aggregation for a given files it's a simple wrap around hash map with our key composed of artifact file name okay resource counter allows you to provide enhanced record and only clones the data if necessary and okay i want to see what that is can we see what that is because i because what i'm seeing here is something much much different actually are you guys seeing this are you seeing what's happening right here uh so what's happening is that enhanced row relies on the lifetime of a line but if you look at this javascript right here the lifetime of a line is actually the entirety of reading the stream right so the stream itself goes on and on and on which makes me think that what i'm seeing is two very different implementations i think you could have got a a much better performance in javascript doing what they're saying right down here which is to add a map as your like counter and so then you're only counting on certain specific items you're not recreating a record every single time this is like a classic javascript oopsie daisy which is that it's so easy to create memory that you write awful all the time right and so if you were trying to build the exact same thing in javascript i don't think it's hard i think that they're just simply missing uh this one this one little thing so let's find out with this our code was complete however since we implemented a performance optimization feature we knew we couldn't call it a day without benchmarking it against the current javascript implementation this this benchmarking would require us to use node.js module rather than using a rust as our cli or something similar to ensure we included the overhead of rust to js communication i fired up our internal javascript benchmarking tool called perfor and wrote a simple benchmark native parse async some big feature okay some big fixture run async native parser okay benchmark node create file stream some big fixture cost values ohjs parser uh let's see run async this thing all right it ran and the results were impressive okay that looks nice that looks nice now but again what's this thing right here why is this happening again i'm telling you it's because of this line right up here it was a simple wrapper around a hash map and our keys was composed of artifact file name and our value had a counter for the request based on the platforms based on the platform i think it's literally this i think this is one of the big differences is that you're only storing unique combos of artifact and file name and an associated int so you're really storing a very few amount of pieces of data right you're storing of some megabytes right yeah uh okay so someone's saying like show me what's going on here so this is how i read what's happening right here is that we have something that looks like this so right they had four or they had four await cons item in stream right or sorry of stream right that's effectively what they had and then they would do something had something like this const out equals an array and then they did out dot push some object in here right can we all agree that's effectively what they were doing right this so what they're doing in rust looks a little bit different they're doing something that looks more like this kant's out is this and then four oh wait well really it's it's i mean it's a new map right let's just be fair here four away cons item uh of stream and then doing some sort of out dot set or i'll i'll dot we'll call it some sort of set uh you know item let's see some sort of combo right item.id item dot path or whatever it was i forgot the exact name hash i forgot what they had in there uh equals out dot get this thing uh plus one or whatever it was or one right they had something that looked like this right right that's effectively what they're doing and so these are very very different notice that you're recreating the line every single time and storing all the information versus just storing all the information using a hash map so that way they didn't have to create an individual object every single line instead they created they just mutated a variable or created one new entry or x new entries into a hash map it's an apple store yeah there's definitely something going on here that i think is vastly unfair to the comparison at least that's how i read it when you say things like that and i think that's why it makes sense you see these two vast differences is that rust should not reduce memory by by 300 fold as far as i can tell when it comes to a bunch of string and number uh storing it can it can usually reduce the size by like up to like four to five times right it can be it can be like definitely smaller because you just don't you you can just store things in such smaller amounts but this is just like straight up saying no way no way not believing it right also node itself takes up 40 megs right like just the environment uh what i mean by that is if you like this um uh go node e while uh while true right um there we go that thing's just running vmrss like it just takes up 40 makes right so this is how much memory it's taking on my system right now so it's like a forever running item it just runs forever if so if if you're testing if you're testing yeah you have to be careful what you're testing i have never seen something do this i've seen people change stuff but they said they launched rust as a module from node so the overhead would still be there yeah they must be memorizing or they're they're just uh measuring the memory used by this i'm not exactly sure how they're doing the memory just by the native module because it like you said it should be wrapped up inside of here it should be a minimum of four to three megs right there's something going on here that i think we're we're goofing up a little bit you know what i mean something's wrong i don't know what it is let's look at these ones js parser let's see runtime all right uh memory awesome this i'm actually another thing that uh i'm actually a bit surprised by is this right here the standard deviation i feel like should be a lot larger this seems reasonable i guess anyways uh the results were as follows jf uh javascript parser uh a lot of milliseconds this much time that much memory russ parser that much this much and this much yeah again i think it's just simply the data structure you chose while there's a significant difference in times the most glaring disparity lies in memory usage the difference enables us to process more files in parallel and that's a huge advantage uh the wix i i like them using rust though i'm thinking rust was a better move no matter what but i'm just saying this uh this comparison might be unfair uh the wix deployment uh preview model is based on an ephemeral kubernetes pods remember when eazy-e tried to describe kubernetes because of that cameo payment i've never been able to find the video again but i loved it if someone can find it for me i would be so appreciative uh pause that can be manually requested for a specific comment uh the ingress proxy can direct traffic to these uh special pods given a specific header this is great for testing something as innovative as this project so we decided by running a few uh preview deployments using the rust parser instead of the javascript ones this is also great it sounds like you can also do shadow traffic comparison prove that your new feature written as a whole new rewrite can also you know it's effectively creating the same information it also is really great i i really love the idea of being able to uh compare things in a real environment you know what i mean not just some benchmark ran on some person's computer but like a benchmark as in we have all of our instances running this and we have all of our instances running this let's see which one's faster let's see which one uses less memory let's see what we can do here i love that idea we ran a benchmark a few times and processed the same 190 000 log files using the following infrastructure javascript implementation with 25 instances which finished in three hours rust implementation with one deployed preview instance completing in 2.5 hours this does not include any database i o since the deployment preview instance is read only but there are fewer uh insertions in the rust implementation because we can process more files simultaneously this enhances our aggregation reduces the number of records to be pushed to the database we could modify our implementation to store data directly in a simple storage like s3 and then have a separate job that transfers the data directly to the database this would allow us to uh to separate computational work from network work nice assuming the times were the same the rust implementation used only 125th of the resources available to the javascript implement station that's 2500 percent performance boost even if we did uh do need to add the mongodb insertions which may take extra time we would still be looking at a boost of twenty thousand or two thousand nineteen thousand percent gosh i keep saying twenty thousand ninety thousand two thousand nineteen hundred percent either way that's a significant win i agree i completely agree on this plus you can still parallelize this so what's the takeaway it seems that choosing the right tool for the job is essential also choosing the right data structures okay that data structure call out it's very serious okay i think we really gotta focus on that data structure there some tasks are well suited for javascript while others are not although rust is a language that's more challenging to master you can uh we can still encapsulate the logic and distribute it in such a way that users uh don't mind using it we can see this trend across javascript ecosystem next.js replace their use of babel with swiss and may use turbo pack by default to fingers crossed i assume they're going to be doing it soon es build a popular choice for bundling and transforming typescript or modern jazz to javascript is written in golang it's simply faster which is always beneficial yeah it's massively faster than than tsc it's beautiful i suppose that uh my takeaway here is uh use whatever makes you happy and if something is too slow profile at first then encapsulate it and then perhaps consider writing it in rust i do like the idea of profiling it we never knew what was slow so he never actually profiled it right he only timed it right he said okay it takes 25 instances and takes three hours that's not profiling that's timing it right what was slow why were you running out of memory like none of those questions were answered i would highly recommend always asking those questions i think it's very very good and lastly i want you to go on twitter and if you would like you can give this person a follow oh he follows me look at that hey that's a good article though i really did like it i would really love if you could whisper me if you could whisper me what happened and what you changed i would love it i would love to know if you did change data structures and if it's fair to say that still at this point anyways the name is the prime imagine