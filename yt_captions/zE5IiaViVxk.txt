uh my talc the foundation of c plus plus atomics uh the knowledge you need to collect lucy plus boss atomics is a series of thoughts about atomics and today i will present the first in this years of talk here we have the anatomy of c plus class atomics this is how i see c plus plus atomics just put my timer this is how i see c plus plus atomics uh i divide them in four key components that one needs to know in order to understand atomics need to understand atomics as a class in c plus plus for example how atomics interacts with the other parts of uh of with other features of the language need to understand the guarantees that are given by the c-class plus member model need to understand how atomics interacts with compiler optimizations and need to understand how atomics interacts with hardware optimizations today i will focus on harder uh the first in this series of talk uh will be about hardware i would also like to acknowledge the work of honor motlu he's a professor at 88 zurich and i used to watch a lot of his lectures on computer artists and some of the slides present uh in my presentation today were inspired and others directly taken from his uh from the slides of his lectures on computer art as i said uh the first out should be about hardware so that's why it is important to start with a brief description of our computing system our computing system has three key components computation communication and storage and memory and these three key components are used to create different platforms that have to achieve different goals or metrics as you can see in these pictures in the slides there is no need for atomic operations if uh if there is no need for atomic operation if there is no parallel computers so that's why it's also important to understand why parallel computers their original goal was to improve performance and there is also other goals uh like to reduce our consumption to improve cost efficiency and scalability and to reduce complexity and there is also the goal to improve dependability i'm going a little bit faster because all of this information just to give the background and context to understand the rest of the tile and there is two types of multiprocessors there is loosely coupled with processor and there is tightly coupled with processors and for the rest of the talk i will only cover a tightly calculated processor where we have a network of of course that are sharing a global memory address space and the c plus plus standard does not support programming for lucid coupled mood processors there is a lot of issues with technical mode processor but today i will mainly focus on memory consistency so here we have a processor-centric design and in this process of centric design we have a network of calls that are connected by a shared interconnect and these calls are sharing memory so the question is why do we need atomics and we and the answer is we need atomic operation for two reasons first is atomicity with atomicity we want to ensure that we want to ensure that only one core at the time is operating on a particular memory location and the rest of the course cannot see this operation partially done we also need atomic operation for synchronization or visibility and with that we want to ensure that the result of the computation of course can be visible to all the cores when needed that's why we need uh atomic operations uh so now let's understand uh the difference between memory consistence and uh cash coherence because it will also be important for the rest of the talk consistency is about ordering of all memory operations from different processor that is to different memory locations is a global ordering of access to our memory locations and coherence is about ordering of operation from different processors to the same memory location a local ordering of access to each cache block so with consistency we care about operation that we care about the order of operation on our memory locations but with coherence we only care about the order of operation on a particular singular memory location so that's the difference between coherence and consistency uh now let's talk about memory ordering in a mid mid processor it's a multi-construction multiple data processor uh here each processor memory operation are in sequential order with respect to the trade running on that processor and we assume that each processor base uh the vanilla model and what i mean by vanilla model here is that the attitude state are updated in the program order specified by the program right so the the microarticular can perform optimizations that preserve the semantics of a single thread and also updates the attitude of state in the program order specified by the program and also here multiple multiple processors can execute memory operations concurrently so the question is how does the memory see the order of operation from our processors in other words what is the ordering of operations across different processors so let's understand why this question is important let's understand why this question is important uh using this example can the two processors be in the critical section at the same time given that they both obey the vanilla model the answer is yes and the other question is what does the second question what the second question asked what the second question has to do with the first question what the second question has to do with the first question is that these two threads can be at the critical section at the same time because the threads don't agree in the order in which the memory operations took place now let's understand uh how this example works so here we have two threads and both of the threads have uh global variables that i use it uh as a flag to communicate uh to other threads when the threat is in the critical section or not and here we also have some letters a b uh c and d that i just use them uh to indicate instructions so when i when i say uh instruction a i mean the global variable s1 being set to one now let's understand how this example worked so uh when a threat wants to enter the critical section first the threat set his flag to one and in this way the threat is communicating to other threats that he is in the critical section and after that the thread checks the flags of the order thread to see if the other thread in this case the thread number two is in the critical section and if the flag is equal to zero it means that the second thread is not in the chemical section so this thread can enter the kit cross section but if the flag of the other thread is one uh this thread cannot enter the category and the same behavior and the same behavior applies to the second thread so before we go into detail into details why on how these threads can be at the critical section at the same time let's understand why it's important to protect shared debt threats are not allowed to up their chariot data concurrently for correctness proposed and access to shutter data are encapsulated inside critical section or protected via synchronization constructs like logs smartphones and condition variables and only one thread can execute a typical section at a given time this is the motor exclusion principle and a multi-processor should provide the correct execution of synchronization primitives to enable the programmer to protect shared that so here i just illustrate why it is important to protect sharadat now let's go into detail and uh in a sequential uh manner how and why this threat can bear the critical section at the same time so uh here i have the the two threads and here i have time that just describe uh the time in which event happens so let's start at time one the third one set his global variable to one but the thread one is not updating the memory directly instead the third one is right into a local buffer but for the point of view of the third one this operation is complete even though the thread will update the memory later and the thread to do the same with respect of the uh of his flag the thread the the the the flag s2 like the the variable s2 instead of update instead of updating the memory directly also updates to a local buffer and will update the memory later so at time 2 the third one checks if the if the checks if the flag of the check number two is equal uh okay at time two the thread one checks if the thread number two is in the critical section right but while while loading the flag of the trade number two it's a cache miss so i need to send a request to memory and so the trade start loading the global variable of the trade number two and the chart number two that's the same here with a respect of the flag of the trade number one so a time tree the request that the trade number one sent to memory arrives and the third uh one okay and the thread two and the third one sees that the flag of the thread uh number two is zero why it's zero because this values still didn't propagate to memory right the vowel that was writing to the to the flag by the to the variable s2 by the second thread still didn't propagate to memory still on the local buffer so that's why the thread number one here is c zero and the same thing happens with the trade number two with respect of the flag of the trade number one and because of that at time four both of the pros board of threats entered the critical section and we saw in the previous site in the previous slides that two threads cannot appear at the critical section at the same time because if two threads are indicated cross-section at the same time there is the probability that the suit threat will be operating at this uh at the same memory location at the same time and this is a problem so at time five the instruction a completes uh the thread one updates the global s1 directly to members sent to memory and the instruction c also completes and the variable is too easy memory but it's too late because the two threads are already in the critical section at the same time so what's the problem here the problem is that uh threads don't agree in the order in which uh or memory operations took place for the thread one uh point of view the instruction a happens before instruction c but for the point of view of trade number two the instruction c happens before a so there is no consensus here and both of threads cannot be correct the problem two processors did not see the same order of operations to memory and the happens before relationship between multiple updates to memory was inconsistency between two processor points of views as a result each processor thought that order was not in the critical section so how can we solve this problem right how can we solve the problem an idea is to use sequential consistency and here i have a formal definition of sequencer consistency but i will use a more abstracted way to explain sequence of consistency so in sequential consistency memory work as a switch and when the switch points to a thread or processor where the guarantees that all the memory operations performed in that thread or processor will be visible will be visible to other threads or processor in the system in that way we can create a total order in which all the processor thread agrees in how memory operations took place and here i have an example of sequential consistence presentation gaps as i said uh it is a series of talk about atomics and the focus of today is uh hardware so there is not really uh enough time to cover to go through this example but i would like to mention here is that even though we are even when we are using sequential consistent memory model uh the execution of a program can still have uh multiple orders of execution right a program even though he's using sequencer consistency can still have different orders of execution and which order is observed depends on implementation and dynamic latencies so now let's talk about issues with sequencer consistency sequencer consistency is a nice abstraction for programming but have two issues two conservative ordering requirements and limit aggressiveness of performance enhancement techniques and we have a question is the total global order requirement too strong why because the ordering of operation is important when the order affects operation shattered that that is when processors need to synchronize where's the cuter program region let's use an example here so in a single thread we can have local local variables that we are not sharing with other threads or processors right for example we have an array of 100 positions and we are operating on this array uh do we need uh to make this our way the operations that are done in this area visible to all the threads of processors in the system well we don't need because we are not uh sharing this array there is local to a threat but in sequence of consistency we need to to to distress but in sequential consistency the operation on this local array need to be visible to all the trade even though no one needs to see it so uh it rises the question uh how about a global order only across our stores like the total store memory model for example that say that i use it in x86 altitude or how about enforcing a global order only at the boundary of synchronization like acquire and release consistent model for example so for example when we have uh when we are using locks right we know we when we are using locks we only need to ensure that the operations inside the kid cross section do not exit the crypto station but uh but member operations outside the critical section and inside the critical section they don't need to be executed in a sequencer consistent manner and so on also because a sequencer consistency is uh because sequencer consistency affects performance uh computer scientists computer scientists and and engineers have proposed uh techniques uh to speed up sequential consistency memory model so here we have a proposal at the hardware level and here we also have a proposal at the compiler level to speed up sequential consistency and also the consequence of consistence affects performance uh there is also proposals for a weak consistent memory model like the acquire and release memory mode again i will not go through uh the memory models today because uh i will have a special talk that i'll be covering this uh and and also because the the focus of of of the talk today is the hardware right so how techniques used by computer acted uh can affect the memory model of an architecture so uh the c plus was uh programming language also offers uh supports or also provides support for a memory uh ordering acquiring release as we can see here again i will i will not go through this and the zero this and the c plus plus programming language also offers even a more relaxed uh memory model that is the relaxed memory model and if you one want to understand this these execution graphs mathematizing the c plus plus concurrency is the paper to read so uh we saw in the previous slide that two threads can be at the critical section at the same time because instead of updating because the threads instead of updating the memory the the memory directly destroys were updating a local buffer this outcome may be surprising to some of us but this outcomes actually expected from almost all more than computer artists today so uh some artichokes so the memory model of some architectures are affected by the buffer and also other mechanisms that are used at the microarchitecture level but the tco memory model the memory mode also used by the x86 aperture is only affected by by this buffer so the rest of this talk i will be covering how they reel the buffer also this buffer is also called the rio de buffalo store queue uh affects uh affects uh or influences the x86 total store memory mod so the goal uh of the tao is to is to to understand how the real the buffer within e36 course influences the the x86 total store memory model so uh the let's uh start by making uh by comparing the tco member model with sequential consistency because we also cover sequencer consistency and the tco memory model is is is close to sequential consistent memory model but there is a difference and we are going to see this different now the difference now so uh sequencer consistent execution are a proper subset of total store execution and our sequence of consistent execution are total store execution while some type of store execution are sequentially consistently efficient and some are not with that we can conclude that uh total store member model is weaker than sequential consistent member mode why again uh because our sequence of consistent execution are tco while some pco execution are not sequential consistent education so here we have another example that will help illustrate the difference so for both sequencer consistency and tco loads and loads cannot be reordered loads and stores cannot be reordered and stored on store cannot be reordered but in sequential but in tco a store can be reordered with a subsequent load but under sequential consistence it cannot be read so that's the difference between a total store memory model and sequential consistency member mode right in total store a store can be reordered with a subsequent in the program order load so systems that support pco do not provide ordering between a store and a subsequent program or the load although they do require the load to get the immediate the immediate value of the early store in situation in which the programmer wants those instructions to be ordered the product must the programmer must explicitly specify that ordering by putting a fence instruction between the store and the subsequent load again for the rest of the talk we covered harder we start to understand uh how the real the buffer within e36 cores influences the x86 total store memory model so uh before we start talking about hardware so why is it important to understand the harder when we are studying or trying to understand memory models first it's the knowledge that one needs to verify if the compiler is providing an efficient compilation to hardware and i have some questions here to help illustrate that do we need transits to implement sequential consistent memory model on each aesi's aperture the answer is yes but why do we need fences to implement the acquiring release memory model on x86 the answer is not but why why the compiler and hardware change the order of execution of instructions the answer is performance but why it's possible to gain performance by reordering instructions right if one won't really understand this question that are very much related to atomic operations it's important to understand the harder especially if you care about performance and you want to verify if a compiler is really providing efficient efficient compilation it's important to understand the harder so the other question is why why to keep her why to keep a buffer inside of a code so why this question is important well this question is important because almost all modern articles is the right buffer so this buffer has influenced the memory mode of almost all modern computer architecture that we have today so that's why it's really important to understand this buffer and as i said this stuff the series of stock is called the foundation of supraspace atomics and understanding hardware is really part of the foundation so uh the answer for the question why to keep a buffer uh inside of a core uh the answer for this question is different for for aperture to articulate but uh some architecture use the right buffer to provide precise exception and to help uh solve the problem of memory disambiguation and going forward we see why you don't need to understand all of this right now going forward it will be clear and the other and the other reason that we have this buffer inside of uh uh of of inside of the architecture is to support right uh high high performance execution of programs and we will see also why going forward why and how going forward so it's imp for the rest of the talk it's important to understand data dependencies and we have three types of data dependencies there is a flow dependencies that's a read after a write here the first instruction is writing to reason number three and the second instruction is reading verse number three so we have a true dependencies over here because the second instruction cannot execute until the first instruction completes right because he needs the value that will be writing by the expression number one and there is also anti-dependency that's a right after a wheat and here the first instruction is reading register number one and the second instruction is writing to address the number one we don't have true dependence over here the problem that we have here is that we don't have enough resistors in our aperture and if there is a because if there is not enough resistors in our architecture we can replace the race number one at instruction one with verse number 15 and the semantics of the program will still be preserved there is also output dependency there's a right after the write here the first instruction is right in question number three and the second instruction is reading version number three and the tip instruction is writing also to raise the number three right even though we have a true dependency between instruction one and instruction two there is no true dependency between instruction three and instruction one the problem that we have here is also because we don't have enough resistors in our architecture because if there is enough registers we can replace the list number three at this number three with register 20 sorry with uh we just number 20 and this amounts of the program will still be preserved so that's not good for high performance processor because all these dependency will affect uh the performance of the processor so uh in order right to pull in order to eliminate there is a lot of techniques that i use it uh to there to various techniques that are used to eliminate the anti and output dependencies for example that i use it uh to eliminate this dependency we use uh acted computer assets use uh buffers to perform renaming and the the the flow depends cannot be eliminated it can only be tolerated and it's tolerated by uh i have by doing uh restricted data flow machines so uh we don't have enough time to cover all of this but you can uh watch my cpp cp uh talk that will be available at the youtube channel of the cppcon where i will go into detail how these dependents are eliminated and how modern machines tolerate flow dependencies but it's important to understand uh the out of order it's important to understand out of other execution also dynamic scheduling because uh it's it is crucial right to understand atomic operations so that's where for now we are going to understand that also this is an example where i just illustrate how all of this works again we don't have time to go through this i described all of this in my cpu cpp contact but it really doesn't have enough time to cover all of these so i will just give an abstraction an abstracted explanation of out of order execution so in out of order execution we fetch here have an an illustration of a pipeline so we fetch an instruction after fetching the instruction what we do we allocate an entry of that instruction into the router buffer it's the right buffer the buffer that we are talking until now so now we are going to see one of the use of this buffer at the micro aperture level so at the code stage we allocate an entry for that instruction into the real buffer and we do that to provide precise exception right we don't want to update the program uh we don't want to update the state of the program out of order in order to update in the program order and to provide precise exceptions we we write we allocate an entry to that buffer so that this buffer will we will update the article state in the program order specified by the program so and also after that what after we allocate an entry into the rail the buffer uh what we do we perform register renaming and we perform pressure renaming or just renaming because we do we do that for registers and also for memory operations and we do that to eliminate right we do that to illuminate mp and output dependencies right renaming is the technique that is used that uh that is used to eliminate anti and output dependencies so that's why we perform renaming so after the winning and instruction what we do we move the instruction to further versus stations and the result station is the technique that we use again sorry for that to tolerate flow dependencies right the reservoir station is the technique that we use to tolerate floor dependencies because we are now uh uh making all the instructions wait into a special place that wait for the operand in a special place called uh dresser vegetation so here we also have a move for execution units so uh at the reservoir station when the the instruction we uh will go after the operands and when the operands of the instructions are ready we can move the instruction into a functional unit right and we have uh multiple function units we have integer addition uh we have integer multiplication floating point uh multiplication and so on but it's important to understand that this function units have different latencies right for example the integer addition takes one cycle to complete uh one instruction is a and the integer multiplication takes uh four cycles uh to complete an instruction and because of that this this this this instruction can update the aperture state out of order and it's also important to mention that here the only requirement to execute an instruction is that both of the operands or all the operands of the instructions need to be ready when all the operands of our instruction is ready we can dispatch the instructions for functional units right we are completely executing instruction out of order right we don't care about the program order we don't care what the programmer have specified in in in uh has specified the only thing here is that we will right respect the dependency the flow dependencies the true dependency will be respected but we will execute instruction out of order we will not follow the problem order and the only requirement in this machine is that the all operands of the instruction need to be ready to be executed on obviously and when the instruction completes it's out of order instructions complete out of order but don't update the actual state out of code they update the reorder buffer out of order and the real the buffer updates the actual state in the program called specified by the program so we have a an e-node dispatch and out of all the execution and you know that update of the artichoke state so here is an example of how uh modern machines i will perform rest renaming and provide precise exception so it's an example of the painting four we have these frontiers alias table but together with the real the bar i use it to perform register renaming to eliminate anti and output dependencies and the retirement versus alias table with the buffer i use it to provide precise exception in other words to update the the architectural state in the program order specified by the programmer so now let's understand a little a little more about out of all the dispatch so the the benefit of an output execution is that we can tolerate latencies it allows independent instruction to execute and complete in the presence of long latency operations here we have uh an example that we that we illust that we help illustrate are the different of the i would say the normal way right we see oh we think uh computers execute instruction right we the first instruction that is the computation of x plus y and the second is the division of y plus uh y divide 10 is the division of y by 10 and then the third instruction is the a times b so and when normal we as a problem we see uh a program uh like that we think that the processor we execute the first instruction and then the second and then the third instruction well but modern computers more than high performance uh computers processor are restricted data flow machines and now we are going to understand here what is a data flow machine in a data flow machine the only requirement here the requirement to to to edit the second instruction is that the first instruction needs to be need to complete but here in a data flow the requirement to execute right an instruction is that the operands of the of the instruction need to be read as soon as the operands of the instructions are ready we can execute an instruction now let's see so the the first we transform right these uh instructions into nodes for example the node edition that depends on two operands that is the x and y and the node addition to be executed we need the x and y to be ready and we have the node division that depends on uh and y and the quantum stand in order to execute so let's see how this instruction can be uh can can execute out of order so when the y operand is ready because the the the another operand of the division node is 10 and it's fast to get because 10 is a constant well this division we can execute but the addition cannot why well because both of the operands are memory and to to to to access memory takes time so that's why even though y is ready it's not ready but both operands of the additional web so we execute the addition but it's contradict the program order that we see here because now the second instruction is being executed while the the second while the the first instruction didn't complete so this is here we have an example for data flow machine so the time doesn't allow to go through all of these uh i would suggest you to watch my sleeping pick on because we need to go to the most important and here is a my mental model of out of order or data flow machine here instruction files when they open and already right we preserve data flow semantics right we eliminate uh anti and output dependence by performing renaming and we fire when the operands of instructions are read so that's my mental model of our data flow processors so that's good because now we eliminate anti and output dependencies and we also can provide precise exceptions and also we can tolerate uh memory latencies of data flow uh dependencies so now let's talk about handling out of all the execution of loads and stores and how this affects the total store memory model so first let's understand the difference between physicists and memory because it's really important for the rest of the talk so the first uh difference between presence and name and memory is that prejudices of the dependencies are known statically and memory dependence are determined dynamically rest state is small and memory state is large rest state is not visible to auto threat or processor but memory state is shared between threads or processor in a shared memory mode processor so here we really have the goal of the talk right we will see how the way uh modern processor handles execute a lot of stores affects the visibility of of stores operation and how so why uh in e36 for example we don't need fences for store uh for store for stores and store operations you can store and stores so uh we need to obey memory dependencies in a out of order machine or a restricted data flow machine and we need to do so while providing high performance so the observation and the problem memory address is not known until a load or store executes let's understand this a little bit so when the operands of an instructions are only registers we can uh we can have we know right which register we need to access right we know the ids of the resistance at decode stage statically we don't need to compute anything to get the ids of registers and the only thing we need to check uh the dependencies between instructions that only use register is just the the id of presence but memory operations are different right we need format operation first we need to compute the address we cannot dispatch a lot at all before we compute address so that's the one of the difference so let's understand uh the problems that we have uh with uh member operations first uh the naming memory address is difficult and we will see why second determine the dependency or independence of a load or store has to be handled after the partial execution as i just state that we cannot dispatch loads and store before completing the address and the corollary number three is that when i load also as it's ready there may be older or younger stores or loads with unknown address in the machine and distilled coloring is also called the memo examination problem and here we are we are we are going to understand uh this problem so the first question is uh when to schedule a load instruction in an out of all engine the problem is that a younger load can have its address ready before and all the stores address is known it's also known as the memo examination problem or the unknown address problem so to understand this example to understand this problem i have an example here but before i would like to mention that loads and stores have two different types of latencies there is others calculation and there is also uh the latency of of going into memory to load or write now let's understand the problem here as i said before before with this before we dispatch load or store uh instruction we need to compute the address right so to compute the address of this store is take 100 cycle is take 100 cycle to compute the address of the store and to compute the address of a subsequent load in the program order only takes two cycles and the question is okay we have the address of this load can we dispatch this load right can we dispatch this load well no why why we cannot because maybe this load is dependent it depends on that store maybe what is load is going to access the memory will be right by this store right well but to check if there is a dependency between the loads and store we need the address but the address of the loads take two cycles to complete and the address of uh and uh uh okay but the address or the computation of the address of the loads take only two cycles complete while the computation of the address of the store takes 100 cycles right this is the problem a younger load can have its address ready before and all the stored address is known so this is the minimum disintegration problem and here we have another illustration that will help us understand that the problem is even bigger so we have the first stall that takes uh that the address takes 900 cycles to complete and we have a subsequent store that the address takes 500 cycles to complete and after that we have a subsequent load that the address takes two cycles to complete and it's important to mention that this load is into access four bytes in memory and that means that this loads here is going to access four different locations in memory and for that memory location we need to check if there is no a preview store that's going to write to that to these locations for example the first store here maybe is going to write to the last location of the fourth location that this load is going to access and the second store maybe it's going to access uh maybe it's going to write to the certain location that the store that this load is going to access right now think uh in modern architecture for example we can address bytes two bytes four bytes and eight bytes and for our uh the location that a lot is going to access we need check if there is no if there is not a predictor that's going to write to this location so that's make the program worse the problem works it's not good so a lot dependency status is not knowing until our previous store address are available and the question is how does the out of all the engine detect dependence of load instruction on operating stores the option number one we can wait and our credit stores are committed and we don't need to check for us as much so for example if we have 15 or 20 stores right we just wait and we we just computed the address of a load we just wait until these 20 stores completed by the update the memory so that we can dispatch the load but this is not good why this is not good because maybe the load right do not depend on any of these stores so that's why it's not good for performance and the second option is to keep a list of pending stores in a store buffer and check whether a lot address matches operators uh store address so this is a more this is a better approach and going forward we will see why and and how this works so another question is how doesn't uh how does the out of order engine treat the scheduling of a load instruction with regards to previous stores and we have three approaches here the first is the conservative approach we can store the load and to our previous stores have computed the address or even retired for the machine this is also not good for performance and the second approach is the aggressive approach and here we assume that a load is is independent of no address and we schedule the loads right away this uh this is good but for this we need to do after we need to check right because here what what we are doing is predicting we need to check if the position is right or not and if the prediction is not right we need to execute the load again and there is another approach that is the intelligent approach is also prediction and is similar to the aggressive approach but the difference is that here we use a sophisticated predictor inside of the pipeline that we make prediction if a load is dependent on our previous store based on the behavior of the program and here we also need uh to check if the provision is right or wrong and if it's wrong we need to execute the load again and here we have an illustration of the performance of these three approaches and the perfect or intelligent approach is better in terms of performance and predicting stores and loads dependencies is important for performance so now let's understand data forwarding between stocks and loads we are going out of time we go a little bit faster we cannot up the memory out of the program order we need to buffer our stores and load instructions in instruction windows and another uh point that is important is even if you know all the address of past stores when we generate the address of that load two questions still remain first how do we check whether or not it is dependent on a store and the second how do we forward the data to the load if it's dependent on a scope so modern processor uses a load uh q and a stall key for this and can and can be combined or separated between loads and stores a lot searches the store queue after it computes its address why here we are assuming that when a load when a store finish the computation of its address the store writes its address in a buffer that's called store queue so that when a load finishes the computation of each address the load search this uh this this buffer to see if it's dependent on a page source a store searched the slot queue after it computes its address why so here we are assuming that uh we are we are using prediction to dispatch loads right if we predict that the load is not dependent on previous store we still need to check if it is true or not and to be able to check and to be able to check we need to use a buffer or the structure in which before we dispatch the load the load write its address to that structure so that when a preview stores finish the computation of its address the store will search the this buffer that is called load queue to see if there is not uh a subsequent load that was already sent to memory that depends on a pretty store so now let's talk about uh out of all the completion of memory it's going a little bit out of time because we are going really to understand how it affects the memory model in a couple slides just just uh just hold on so when a store instruction finish execution it's write its address and data in uh in each router buffer or ins it's write its address and data in each router buffer entry or the store q entry and when a letter load instruction generates its address it first shows search the store queue with its address and access uh the memory with the address also why we need to access the store key on the memory well if the value is not at the stock at the store queue well we need to do the memory right because the value may be at the store queue or maybe at the memory so uh uh we need to receive the value from the youngest all the older instruction that's well to that address either from the real debate or store queue or from memory this is a complicated search logic implemented as a content addressable memory and content uh here uh is the memory address but we also need to take in consideration the the size and the age going forward it will be clear and it's also called the store to load forwarding logic so uh i think we don't have time to go through this but here i would like to illustrate why renaming memo ads is difficult but the when when we are performing renaming of resistors we can using direction because the state uh is small right we can have uh the entries inside of the uh that area stable can point to an entry in the real debacle so begin using direction but we cannot do that with memory because uh we use uh 48 bit of memory address and with that we can directly address every byte of 256 terabytes of storage and if we try to use in direction with uh when renaming memory address well we will need address either stable oh no tracer we need a structure with 256 entry and in total it will be 256 terabytes because each entry will be one byte and we cannot have it inside of a pipeline so that's one of the reason why remaining memory is difficult because we cannot use uh in direction and i again i encourage you to watch my vidcon talk to really understand this so now let's understand why uh store to load forward is complex right because we talked about okay unload when have its address need to search the start queue now we understand why this search is complex well the service is complex because the content address was searched we cannot use in direction because memory state is large is also a range search because for every for example for every uh member location that a load is going to access we need to check if there is not a preview store that's going to write in that location it's also h based search because we need to uh to get the value that was writing because we need to get the youngest value that was writing to that address and the load data can come from a combination of multiple places right one or more stores can be in the realtor buffer uh and others maybe we need to go to memory okay and if that is not a red buff we need to go to memory so here i have an example illustrating the realtor buffer right i'm here i'm only uh illustrating the content addressable search but in real modern computer we need to take in in consideration also the range church and the aside basically but here i'm only taking consideration the content addressable search so when when a load finishes the computation of its address we use the address of the load right to to check every entry of the real debate to see if there is a match so we need to search out right we cannot use the duration right we cannot go directly to a special entry where we need to check if there is an address or not we cannot do that here we need to search right because it's a content addressable search right we need to check every entry of the buffer to see if there is a match and if there is no match we need to go to memory and here we can spend we can spend a lot of time searching this buffer and if there is not much well we need to go to memoir again and for example when going to memory we can meet at any one we can meet certainly so we can miss a teletree and we can miss a dram and we need to go to disk so you see why this search is complex and here we're only talking about the content addressable so now let's uh see how this uh rail the buffer affects uh the visibility of store operations and also how it affects uh the total store memory model so for that let's see how this buffer uh works a little bit so we have the first instruction that is the store and it takes 100 cycles to compute the address of the store and when the store arrives what we do we allocate an entry for that store at the realtor buffer and the stores becomes the youngest entry in the realty buffer and we also have the address beat and the valid bit and the value in each entry so for the scale for the store this address take 100 cycles and because of that the the the address and the value are not ready so the valid bit is zero and the address bit is also zero and after that the second stores arrive and to compute the address of the store only takes two cycles so okay the second store arrives we allocate an entry for the second store and the second store becomes the youngest uh instruction in the rail de buffer and to compute the address of this toronto takes two cycle and the value of the the stock was already produced by another instruction so that it's ready so we can we have the address we have the value we can update the real debuffer where we set the valid bit uh the address bit 1 and the valid bits to 1 and the value that we need to write to that address is 25 so we can see uh it is 25. so now we have completed we didn't wait for the first store to complete right well we still this this the first store still computing its address but we already finished the execution of the stores of the second stop right but what's important to mention here is that the store the second store we only update the aperture state we only update the memory when the faster compete when the first or complete why well because the real buffer is a front out structure and this is how we provide precise exception and this is how we don't need fences between stores and store in e66 attitude why well even though we we completed the execution of the second store the second store cannot see cannot update the memory the values of the buffer and if a local reload tries to access the value that was produced by the second store we can access that for the for from the rear buffer but the second store cannot cannot exit the reorder buffer until the first store completes because the second store can only exit the road above when it becomes the oldest instruction in the real debate philip sorry sorry for interrupting just to let you know that the ordinary time is over so if you can please catch up and then we have a couple of questions but i think you can you can get them into into the lounge okay so can you take a few minutes yes two minutes yeah for sure okay so uh here we saw why we don't need uh store and store uh fences right because we have the guarantees that the subsequent store will not update the memory until uh the first store completes right so the real the buffer inside of the micro aperture of s86 gave us this guarantee so we don't need fences for that we have the guarantee that a subsequent store we only update the memory the result will only be visible when the fastest completes so again and another thing important to mention that is the because we have the buffer it affects the visibility of other cores because the value of this address for example when this the the the third instruction that's allowed is going to access right this value that the value that we're writing by the previous store that's the second expression it will find the value at the road above but if another instruction in another call try to access the same address right try to load a valid from the from this address well the problem is if we need to go to memory and if you load a different value and here i have a bag here the lord should be reason number two to be equal with the second instruction sorry for that so this is how two threads can see different can have different point of views of what on how memory operations took place because one have directed access for the relative buffer but another one has not accessed the router buff i need to go to memory and this value is at the render buffer and we only exit until the stores becomes the oldest instruction in the road above so i think i will complete i have more here but how for example how they thought of store uh how they rio de buffer interacts with hyper trading for example but i don't think we don't have time to go through this so thank you very much for attending my talk and see you next time and thanks a lot for your for your session philippe i think we can uh on context over here but that's us thank you very much thanks a lot