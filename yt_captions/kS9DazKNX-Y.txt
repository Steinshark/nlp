Vector search and semantic search are today's topics on developer voices how do you take a large language model which is all the rage at the moment and teach it about your data set what does it even mean to take a large language model and get it to search through the meaning of your codebase or your documentation or your product catalog or whatever data you're dealing with how do you teach a computer to understand it it seems really hard because large language models seem to come pre-baked from the factory as these trained fixed things how do you teach it new stuff this is one episode where we get to go delightfully deep on how this all actually works what's a large language model really doing what does it need an auxiliary database for and if it does what's the flow of data back and forth between them what is that auxiliary database actually doing what's it doing in the pipeline but also how is it it doing it what are the data structures how does it organize data in memory on disk what does it mean to search through meanings how much work is it how do you make it fast how do you make it cheap enough to be used by your users I am interested in all of this stuff and I really wanted to find someone who could take me right into the guts of what's going on here and I think I lucked out with today's guest Zan Hassan he has both the depth of knowledge to go right down into the internals and the clarity of explanation to really bring it to life and make it makes sense to me now it really does and I want to share that with you we go all the way from clues about how you can improve your prompt engineering through index design and computational complexity and optimization to where Zan thinks the future of semantic search is headed I can promise you just from the first 5 minutes of this interview you'll have a much better understanding of how this is all put together so let's get going I'm your host Chris Jenkins this is developer voices and today's voice is Zan [Music] Hassan Vector databases is the topic today and we've got Zan Hassan as our expert to talk about it how you doing Z everybody I'm doing well I'm doing well good good you've just got back from filming a course for LinkedIn so you're now a Hollywood star right yeah exactly makeup and everything in fact I did my makeup before this so you're looking fabulous so we should probably jump out of the unfamiliar world of Hollywood and straight into the tech as fast as we possibly can we want to talk I wanted you to tell me about Vector databases in as much depth as I can extract from you but we should probably start on the common understanding of the state of ml right I think everybody everybody has chat GPT as a reference point right even if they haven't looked into this much and I have this impression of chat GPT as a pre-baked neural net and you can ask questions of it and it can answer because neural Nets are magic but it's pre-baked so I can't add any data to it I have a vague sense that that's where needing a database comes in yeah take me to a proper understanding yeah so if we um zoom out a bit and we think about what chat GPT is uh chat GPT is a chatbot that's built on top of a a base model and the base model itself is just a sentence completion tool it's a you can almost think of it as a fill-in the blanks tool so it's studied essentially pretty much all the data on the internet to learn about the cooccurrence of words and which words are more likely to be used with other word so if you say uh something like I don't know the um the monkey ate the dash banana would be a higher probabil higher probability completion compared to I don't know cabbage or something like that or car that would be a very low probability completion and so on top of that you can um you can build chat bots so you can actually force a base model to act like a chatbot if you just prime it correctly so you can say speaker one said XYZ chatbot answered speaker one questioned XYZ chatbot answered and then you the question that you wanted to actually answer you say speaker one asks a question what's the color of the Skype and then chatbot and you leave it empty and then it will autocomplete based on the high the highest probability tokens that it's been trained on um the chat GPT is just a finer and refined version of this where we control the quality of its Generation by uh fine-tuning it on uh higher quality data points that have been extracted by contractors so you give contractors a question and then they generate high quality answers and you train uh train the base model to Output those uh higher quality answers um all this to say even if you take chat GPT and all of its fine-tuned versions the issue is that it doesn't know what it doesn't know it only knows what it's been trained on so it knows the probability of words or it knows the concepts that it's been trained on right where a database comes into this is if you want to provide external knowledge to the system um whether that's because the data wasn't available at training time so it's future data that's uh that's occurred after the the the training process or more realistically it could be data that's private right so if if I'm a company I have proprietary data um and I want to use the reasoning power or the generative power of a large language model on this data I'm not going to send this data over to a thirdparty company that's going to train the large language model um I that's my private data but I still want to be able to reason over this data um and that's where the database comes in you can run the database locally that data base can have your private data and then you attach that database to the large language model as almost like a um a secondary uh Bank of information or it represents State for the large language model really and it can then retrieve information in real time before it generates so it uh it gives it the ability to read context ground its answer in that context and then generate as a result of that so that's the high level picture a a lot of people call this uh retrieval augmented generation because you're retrieving context and then you're augmenting the generation with that retrieved context um and you can use any database for this a vector database naturally fits in because as we'll talk about later you can uh query a vector database using natural language you can you can talk to a vector database basically and so it's easier to get a large language model to query the database and then retrieve cont text from it so that's the larger picture of how the database fits in it's essentially just a knowledge store okay take me one level deeper on that when I type in my query what's the flow of data through the large language model and through the vector database and how is it recombined to do something yeah so retrieval augmented generation you can you can implement it in uh in multiple ways and there's different complexities of it but the simplest way that you can Implement retrieval augmented generation is think of going to chat GPT and asking a question you can take that question and you can turn it into a query for the vector database so let's say your uh question is um what type of condiments go well with a hurger right right so you can take that question and rather than send it off to the large language model immediately you turn it into a query for a for a vector database that query is can itself be the question you send it to the the vector database and now you ask the vector database retrieve for me the five most relevant documents that I have in my knowledge store that are to do with this question so in this question I've got condiments I've got uh Burger I've got ham so it's going to retrieve for me things that are related to those Concepts and then those five things that come back can then be stuffed into my prompt and we can say the question here is useful information that uh you might find relevant and that's what you send off to the large language model to generate with okay so I could literally do this manually open one window with chat GPT I go and query for five web documents related to my search term and then I mash all of those into it and prompt now answer my question yeah exactly in fact before retrieval augmented generation was popular um I was at a Meetup uh here and um this was back in March of 2023 so uh chat GPT was taking off and people were hearing about it um and back then uh a lot what a lot of people were doing was they would search over their PDF and they would say okay this chunk of text is what's relevant to this question type out the question and then copy paste the huge chunk of text dump it into the into your um chat bot and then say answer the question here's relevant information you might need to know um that that's basically what's happening but the problem there is scalability what if you have a billion documents or hundreds of millions of documents and then right you can't really do that but that's where the vector database comes in really I see okay that that that's uh I feel like you're you've just given away a magician secret okay so then um I get how that works then we'll keep um d llm as our magic box of neural networks let's take a look at the vector database side when I type in which condiments go well with a hamburger what exactly is happening to that sentence yeah so this is a little bit of a look behind the scenes of why it's called a vector database so anytime you query a vector database a vector database understands vectors so it understands groups of numbers now whether they're three groups of numbers or a thousand groups of numbers um that's determined by an underlying machine learning model but the whole idea here is that if I type in what condiments go well with a hamburger that is a human understandable version uh of a sentence right so I can I can ask people that and I can get coherent responses back but if I ask that to a computer it has no idea what these words mean the only way that it can um that it understands meaning is if I capture this question or this sentence in numbers and so or or groups of numbers so we want to we want to kind of give every word or every token here an ID and then we want to analyze you know which words here from my training set commonly occur with other words um and I need to be able to transfer the concepts in this human understandable version of a question to a computer or machine understandable version of a question so essentially I need to go from this sentence to now a vector representation of that sentence and there's so is my intuition right here that you're going to end up with a vector that like very broadly it's a a vector of floating Point numbers and the first one is the probability that we're talking about cats and the second is the probability that we're talking about fast food and the third is the probability that we're talking about Germany uh so the first part I think is correct but the second part is not so the when you capture when you capture the uh sentence as vectors it's not clear what the uh individual dimensions are it could be that the first one is we're talking about cats what's the probability we're talking about cats but we don't really know what the latent space is represented we we don't know what each Dimension actually means um it so this is the whole point of optimizing a neural network you initialize these weights randomly and then you say I want you to correctly predict the next word and optimize all of these millions or billions of Weights appropriately so that I can predict the next word uh better and you get better and better as you optimize them so we're not really sure whether the First Dimension is are we talking about cats second dimension are we talking about Germany uh it's whatever optimized the answer it's whatever got us the lowest uh loss right so the vectors are still sort of a black box but I would expect two paragraphs that talk about burgers have some similar subsets of vectors is it something like that exactly so you would expect I almost think of vectors as a barcode right so you can think of let's say you have a 100 dimensional Vector there's just 100 floating points in in an nump aray let's say and the higher the vector let's say it's uh more black and if the lower the vector let's say it's white so now you can almost think of a 100 dimensional barcode where you have a color it's a strip for every number and the the higher the vector the um the the darker the color whereas the lower the vector uh you get a um a white color so now you have essentially have a barcode so the barcode for a sentence that's talking about a burger is going to be is going to light up in different places whereas a barcode for a sentence that's talking about a cat is going to light up in different places and you can actually calculate these barcodes and say okay this is a sentence about a cat this is a sentence about a dog and this is a sentence about a burger the cat and the dog barcode would be a lot similar than the uh the burger and the the cat dog uh barcode right yeah and if you've got a barcode with all three of those lit up you're in a really dodgy restaurant exactly so that that's that's the whole idea behind Vector search you're comparing these barcodes and you're saying well how close is this barcode to this other barcode and the idea behind the barcode is that it's just some uh semantic it's a capture of the semantics behind the human understandable version of the data okay so I still need some kind of model that's going to turn my data whether it's text or a PDF or an image into a vector exactly that that's the key Point here um the reason why they're called AI native or machine learning databases or AI first databases there's a lot of buzzwords but the reason why vector datab are affiliated with machine learning and kind of interwoven with machine learning is because they search over these vectors that are spit out and generated by ml models and most of the times they're neural networks so that's why um semantic search is also known as neural search sometimes right okay so I okay I understand up to that point so then we're what we're really talking about underneath all this once you finish neural networking things is a data space that's good at storing vectors of floats and then searching for similarity between vectors of floats yeah how does that work so on a high level intuitively understanding it it's it's what we talked about if you if I give you a barcode for a question your job is to say which barcode let's say every single object you have let's say every file you have on your computer can be captured into a barcode right so that includes uh text documents but interestingly also images audio files video files uh and we can we can take that and we can talk about multimodality later but let's just say we only have text documents for now and we have the ability to turn every uh text document into a vector or a barcode Vector search or semantic search or neural search is effectively saying if I have this question and I have the barcode for this question what are the five most similar barcodes or vectors to this question vector or question bar code um intuitively that's what's happening one level deeper what's happening is you can because you have Quantified everything as a vector you can actually take the distance between these vectors using multiple different metrics the easiest one let's say is ukian distance where you can actually measure the shortest distance between this vector and this Vector um if you had threedimensional vectors or two dimensional vectors you could actually plot them out on a grid and you could measure the shortest the direct uh line from one vector to the other Vector okay I remember enough Pythagoras to calculate the distance between XY points and I guess you just scale that up right exactly so that for for multi-dimensional vectors you can just take every Dimension subtract the other corresponding Dimension and then you can you can put it into to the um the ukian distance formula and you get a measure of how different two two vectors are and that's just one distance metric there's lots of other distance metrics that we can choose from but that's the main idea right so now we can take this concept of a barcode and we can quantify how different one barcode or one vector is from another vector and essentially what you're doing is quantifying how different the question Vector is from every other Vector that you have in your database and then you're saying now that I have a dist between this question and every other Vector I'm going to organize them from smallest distance to highest distance and I'm going to cut off at the top K let's say the top five and these five objects are the ones that are the closest or the most semantically similar to what my user is interested and then you return those okay so I can almost literally give you the ballpark exactly like I I put your 10,000 documents into a imaginary space and I can say which other documents are near this one space exactly exactly and actually your query is kind of the 10,000 and1st document isn't it yeah so the query goes through the exact same pipeline because the query needs to be translated using the exact same machine learning model into Vector space um and that Vector space has to be the same Vector space um otherwise it's like all of your documents are stored in German and then you're asking a question in English and if the system doesn't understand both languages it you won't be able to extract relevant vectors so the the vector language or the embedding space that we're talking about has to be generated by the same model okay so I'm going to push you a bit deeper on that so I can I can imagine if I actually want to use this in Anger maybe at work I guess we're talking between 10,000 and 100,000 documents that kind of order of magnetude as soon as we get anything really interesting I know enough from game programming that calculating Collision detection on 100,000 objects is horribly slow yeah so how are you going to make this efficient calculating the distance between all these documents yeah so that's the that's the question that gets to the bread and butter of every Vector database if you if you think about Vector search as I have some question vector and I have 10,000 object vectors or data points that are um that I'm interested in and I to retrieve the five most similar data points you can't actually perform this K nearest neighbors uh this Brute Force K nearest neighbors algorithm because well at at a scale of 10,000 objects you probably could at 100,000 objects now you're slowing down at a million objects you're slowing down even further and the reason why is because let's say you have uh a question vector and you're in order to find out which five objects it is closest to you have to calculate the distance between this question vector and all 10,000 objects and then you have to sort them from lowest distance to highest distance yeah so that is uh that is a um a complexity a runtime complexity of n where n is the number of objects that you've stored in here um so that's Big O of n let's say so as that n goes from 10,000 to 100,000 you get a uh a a 10x slow as that goes from 100,000 to a million that you get another 10x slow um the other problem here is that not only that but because the ukan distance calculation is also a function of how many dimensions the vector are the vectors are uh right yeah you also have that component in the in the runtime so if you have a vector of 10 dimensions then you have a runtime of 10 times the number of objects if now your vector is more interesting if it's capturing a lot more um lot more features it's if it's capturing more Concepts now it might be a thous dimensional so now your runtime scales up by the dimensionality of each Vector times the number of total vectors so it's a it's a MN right yes exactly so it's so you you get this kind of explosion where the more interesting data types you want to search over you'll need bigger vectors and the more of that type of data you have you'll have more vectors in in total so now you're really slowing down so what you need to do instead of just to make that concrete before you go into how to optimize that what sort of numbers are we typically talking about what's an ordinary number of documents and an ordinary number of um Vector components yeah so if you look at um the dimensionality of vectors commonly we get anything from a th000 dimensional to 2,000 dimensional um some of them are about 700 dimensional but in that 1,000 dimensional ballpark I would say is average okay on the higher end there's also models that generate for 4,000 dimensional vectors and a little note on this I guess as we have models that are multimodal and that can capture all sorts of Concepts I imagine we'll have larger and larger dimensionalities growing as a model needs to understand different concepts not just text documents but also videos or images the dimensionality will only grow so we're starting off with a th000 2,000 but 10,000 is not uh not out of the question okay your future 10 thou so that order of magnitude multiply by what kind of document the number of objects so the number of objects usually um if we're talking about social media uh applications if if you're thinking about like um Facebook Twitter Instagram you can have objects in the trillions easily if you scale it down a little bit let's say you're talking about recommender systems um Netflix for example has hundreds of millions of users it has a catalog of let's say 20 20,000 unique um TV shows movies all around the world so if you take that you can easily get up to the billions or tens of billions of documents um realistically a lot of people are using this so right now A lot of people are trying proof of concept there's very few companies that are um at scale moving into using Vector databases but we've tested uh we8 with a billion documents officially and then there's users that have tested it with even more so I would say even three to three to four billion um and that's that's the kind of the the upper end that we're talking about and of course that's changing day by day but if I had to give you a ballpark I would probably say 100 million to a billion if you're on the lower end probably uh 10 20 million documents yeah okay that's that's got me firmly convinced thousands multiplied by billions that's firmly into the place where we need to optimize yeah so you can me an idea I've got uh my my fictional service has been very successful and I've got 100 million users now and I would like to say who are users who buy stuff similar to xan because I want to recommend stuff to xan yeah right um how are you going to make that work fast yeah so the trick here is not to do Brute Force kers neighbor search uh it's impossible to scale up when you have a runtime complexity of D dimensionality multiplied by n where D is a th000 and N is 100 million there's no way you can scale that up uh in real time so you imagine having an app somebody clicks a product and then you trying to retrieve the 10 nearest uh objects to that the person would be sitting there for days um yeah so the idea is to do approximate nearest neighbors and this is what all Vector databases uh really um uh use and so the idea is that instead of at a high level you want to give up uh accuracy for performance so you're going to say maybe I won't be able to retrieve the best kir neighbors sometimes I'll miss a few of the right neighbors but I will be able to you know giving up a small amount of accuracy or a small amount of this recall of the right neighbors so let's say you give up 5% of that recall so you up 10% of that recall so 10% of the times you might not get the correct nearest vectors you might get an incorrect nearest Vector but for that 5 to 10% recall and of course that's um you can fine-tune that you can you can say well I I want a 99% recall and I'm willing to uh uh give up performance for that but usually the tradeoff is uh is not direct so you give up let's say 1% recall but you gain a lot of uh performance so then you can run in real time you can run thousands of queries per second um and that's what approximate nearest neighbors does right it gives you the ability to trade performance increase performance decreasing recall or decreasing accuracy um there's multiple algorithms that allow you to do this right um but one is probably the most popular one that uh supports all of the other functionality that databases also need are we saying like just on the accuracy point is it like if you dropped me into a crowd and said who are your 10 nearest neighbors I probably personally wouldn't get exactly tape measure out the 10 nearest but I'd still get roughly the right I mean they wouldn't be wildly far away yeah so essentially what you're what you're saying there is if you if you look in uh in a vector space if you think about it in Vector space and you draw a bubble around your query vector and you're saying I want to draw a bubble that's large enough to encapsulate five nearest Neighbors in Brute Force search you'll always get the nearest neighbors because you've kind of from beginning to end calculated all the distances and sorted them yeah but here you don't have the luxury of calculating all the distances so you need to say I better be smart about which distances I calculate and then of the distances that I've calculated I want to sort them out from lowest to highest and because I haven't calculated all the distances out because that would take too long I might have missed some distances of nearest neighbors and so now I'm just overlooking that nearest neighbor um and I'm picking one that I think is the nearest so yes they'll be close together yes they'll still be close but they won't be the closest yeah that's the that's the idea but I'm not going to suddenly end up with someone completely the other side of the room for instance no no no so it's it's highly unlikely that you would is because you're still calculating distances you're still organizing them uh to a degree and we'll talk about how the uh approximate nearest neighbors hnsw algorithm Works uh uh in a second but the main idea is that you get let's say you have the nearest neighbor but you fail to calculate that distance now for all intent and purposes this isn't this neighbor is invisible so now you're going to say what is the nearest neighbor that I calculated so it's not going to be like you'll get all sorts of wonky distances you're still going through the same uh sorting them based on um distances in ascending order and pick the top five but this top five might not have the correct five just the five that you calculate the distances for right so I might get five out of the top 10 for exactly yeah exactly okay that convinces me to give up a little accuracy for a lot of performance yeah so that that so the performance that we're talking about here you go from uh runtime complexity of O of dimensionality times n to now runtime complexity of logarithm of N and that that's a very very uh scalable um kind of algorithm and how this this works is essentially intuitively when you're searching uh when you're searching for uh vectors that are close to your query Vector what you want to do is structure your search such that you make big jumps earlier on right so let's say you have your uh your you have your database of vectors you come into a random object and you say how close is this object to my query what you want to do is initially when you come in you don't want to have all of your vectors being searched over because that would kind of fall back down to Brute Force search you want to structure it nicely so one way that was proposed to do this is uh called the hierarchical navigable search uh hierarchical navigable small worlds model and what this does is it takes your vectors and it makes a graph out of them and it makes a hierarchy of them and it what what it does is when you enter enter the search you enter at a top level and at the top level there's only large distances that are available so you take vectors that are very far from each other so now you're almost like uh taking a highway from one vector to the other vector and these vectors are quite far away so you're saying of these vectors that are really far away which one is closest to my query Vector so you're almost saying I want to quickly localize which region or what type of thing my query Vector is asking about so rather than search for every other Vector I'm just going to say out of these 500 vectors and I might have let's say 10 million vectors these five million 500 vectors are further apart they allow me to explore Vector space efficiently I want to find out which Vector is closest to my query vector and these are bigger jumps in Vector space right I I have mental image that this is a bit like if you looking for a house in a country let's say in England houses aren't uniformly distributed across England they're clustered together in cities so I start by indexing which is the nearest city exactly so you would instead of instead of saying well if you're interested in this house let me show you the next house the next house the next house the next house instead of searching locally uh exhaustively you search in different neighborhoods and you say okay this is one house in this neighborhood are you interested in this this is another house this is another house so you find the global region that you're interested in and then You Dig Down Deeper within that Global region so you do more you you do a course search and then a more fine grain search within that neighborhood exactly so the higher the highest hierarchy here is going to perhaps show you one house using your analogy from every neighborhood one of my university university lecturers would be proud that I can now see how this starts to become log in exactly because you're you're building a hierarchy that gets gradually more and more yeah detailed is it multi-layered yes so there's you start off at the uh the highest level and then you go down levels and you can have 15 levels five levels however much um you want but the highest level has uh an exponentially low number of data points so you can almost think of starting off at the bottom level and that level has all of your data points every Vector let's say you have a 100 million vectors all of the vectors exist at the bottom level and then as you go up one level these data points start to drop down so by the time you reach the highest level you only have a exponentially decaying number of data points so they've only survived um with a certain probability up here so you have a very few number of data points to search over and then as you drill down you get more and more vectors yeah it's the exponential decay on the way up that gives you a logarithmic search on the way down yeah exactly okay is does that mean this is expensive to create the index are you then having to um are you doing that search to pre-calculate all the Clusters or is it smarter than that no so uh it is it is a lot more uh intensive to create the index than it is to search the index because searching the index is just logarithm of n because you come in with a query vector and then you pick a random Vector at your highest level and you make these big jumps on which neighborhood are you interested in and then depending on this neighborhood let me show you more locality more locality you keep on adding vectors and then you keep performing searches all the way down to the lowest level where now you've got your nearest neighbors and you return that but building this index up is a lot more difficult and so usually if you're building an index it'll take um anywhere from uh hours whereas searching is you can perform thousands of searches per second okay is it um just try and get a sense of this so if I've got a large index pre-calculated and I want to add something in how expensive is that if I add one new document so adding is uh relatively quick so this is one of the um the plus points of the hnsw algorithm and this is one of the reasons why um we V8 uh uses the hnsw algorithm it's one of the few um approximate nearest neighbors algorithms that not only is quick is a log event but it also supports insertion and deletion because every database needs crud operations um you need to be able to add data points and not have to reconstruct the entire index because then every uh update or every insertion would be hours long yeah so you can simply add a data point and that data point gets added at the bottom level because every data point exists at the bottom level and then again you have that exponentially decaying probability of whether or not that data point survives at the next level at the next level at the next level so at some point that that Vector object is going to die off and it's only going to reach up to a certain level into your into your hierarchy into your index so that's all insertion takes yeah that's that one other small detail which I'm going to ask you about are you saying as there's less and less probability of it surviving are you saying at the top level where we've maybe got let's say a dozen surviving points they are actual documents they're not like average centroids no no they're actual documents okay okay yeah there is um so we can we can talk about this but there are algorithms that allow you to approximate vectors using centroids so using product quantization or using like K means clustering but just hnsw by itself doesn't um average the vectors out it uses the actual vectors to to search over yeah okay I wonder if that means you could like if your index got sufficiently large you could shortcut you wouldn't have to go all the way down to the search but I guess you're saying it's fast enough anyway yeah so yeah because you have this log N I don't know if you would you could you you mean cut it off at a certain point with before it reaches the bottom yeah you get the five levels down you say well that's good enough for now yeah I I think you could do that but I'm not too sure I guess you could but you wouldn't need to bother yeah because it's it's already real time so even if you have like hundreds of millions of documents you're getting um you can search hundreds of queries per second right so unless you're trying to push recall uh to a degree where now you've now you're kind of falling back onto um Brute Force search uh then I think you would have to say Okay uh maybe we don't go down to the the bottom level but then again you would have the program the problem that recall would be um would be hindered as a result of that right so okay okay give me anide idea of the size of this index once you've built it is it um because there's got to be huge compression from the original documents down to the vectors but then is the index presumably the index is larger than the vector set because it's got multiple copies at different hierarchies yeah so the if you just look at one vector the index if you have 10,000 documents you'll have 10,000 vectors but then the index has um yeah so this is a good question I'm not sure if multiple vectors are actually stored or you just say that this Vector is also at this index just refer to it uh here but it also depends on how many vectors you have and so for example we when we tested this with um the sphere data set which has around 900 million objects when we vectorized that and we created the index for that I I believe we had um thousands of gigabytes uh so the index was thousand or hundreds of gigabytes uh large I can't remember the exact numbers but we've written a Blog on this that people can refer to so the index was that large and this is where the compression algorithms that I was talking about come into place because the vector database runs in memory and so if your data set gets very large it can get very expensive so that raises the question this is all memory but is there how is this stored on dis is it's you're storing lots of lots and lots of lists of floating Point numbers is this some kind of column or thing on disk so on disk I'm not so everything all the vectors when they're indexed and you're searching over them this is happening in memory it's all in memory it's all in memory there are some uh algorithms um which propose uh doing read wrs from disk um and this is what in order to compress and reduce um memory usage that's what you would have to do so we've recently announced this um where you can keep in memory uh of compressed centroid representations of vectors um and then on disk you store the full representation so what you do is you do this core search over k mean centroid vectors and then you say these are my 100 closest vectors that I'm interested in Now read them all in from dis and perform the uh the finer search over the 100 correct vectors so that's that's one way to uh to balance between you know storing the whole uh index in memory versus storing a compressed compressed version of the index in memory using that to identify which uh centroids I'm interested in and then reading the those vectors in and then rep performing or rescoring th those distances so you get back the recall yeah and presumably there'd be hot pages in that so you could keep the most important things still in memory yeah okay exactly okay so let let's move on to another topic which you've hinted at which is one of the early things I remember seeing in this space was you can do cool things like you take the vector for very broadly I'm hand waving you can fill in the details but you take the vector for Bridges and then you take the vector for bridges in Germany mhm and you take the vector for Germany and you subtract Germany from Bridges in Germany and add in Mexico and you get documents about bridges in Mexico yeah and you've been doing this with like going from images to text to sound files tell me about that yeah so this is this is a field that I'm really interested in this idea of multimodality because a lot of people are doing doing Vector search over text documents right now but I think the future is doing this Vector search over multimodal data because the idea is that there's no reason why we should limit these neural networks to just text data and in fact before we had this revolution in natural language processing we had a revolution in computer vision the original paper that was published by um Hinton uh in 2012 was on imet and he he used uh you know uh Nvidia gpus to train this uh this alexnet uh model that got state-of-the-art on uh solving imag net so these models can understand not just text not just sentences not just documents but also images video which are just you know evolutions of images uh they can also understand audio files they can understand all sorts of data so that why limit the vector representation to just text um we're more comfortable with text but I think now that uh people are getting used to Vector search I think the next thing that is going to be very popular is storing images in the form of um vectors storing videos storing audio files in the form of vectors and because the vector database all it's doing is searching over vectors it doesn't care whether the input was an audio image video a vector is vector and I can just as easily build an hnsw index over a vector of audio files as I can a uh a um you know a vector of uh text documents so now the idea is that now that if we have models that can represent this multimodal data as uh as vectors we we can search over them it gets really cool when you have models that can capture not just one modality so you might have a model that understands text and a separate model that understands audio which is interesting because they allow you to perform you know text to text search audio to audio search individually within modality but what gets me really excited is when you have models that can understand multiple modalities so a model that could potentially understand images and text like um clip from open AI for example where now you can search for a concept like cat and you get back images of the cat and there you're not really um matching words with images uh in terms of metadata but rather you're matching the vector of the word with the vector of the image and those two vectors happen to have a lower uh distance between them and that's why they get retrieved as nearest neighbors right so they both get indexed into roughly the same space so of course you to and presumably if you index lots of these different things any any text query would land you in a place that was surrounded by documents and and images waveforms videos exactly but you said earlier it's it's like one model will encode things into English another encodes them into German and you can't just mix and match vectors yeah so do I need a neural net that's been trained to do all these things yeah so that's that's a great question right now there's two kind of um development in this field of multimodal neural network modeling one development uh is more practical and this uh goes in line with the ones that we've integrated so far so if you look at clip from open AI or if you look at um image bind from meta that was released earlier this year these types of multimodal models so clip for example understands images and text and um image bind understands text images audio video um and there's a couple of other modalities which are not as interesting but these are the the four main modalities that it understands right this model uh image bind is actually six independent models so there's one model that's a specialist at identifying images another one that's a specialist at identifying words okay and so you've got six separate models and now the problem is what you said each one of these models speak speaks its own Vector language so if I take the image of a cat I'm going to get a barcode and that barcode can be very different from the barcode of the word cat itself yeah they compatible standards right exactly exactly so now the trick is how do I unify the vector spaces such that the barcode for a cat the image of a cat and the word cat and maybe the video of a cat and the cat meowing are all Landing me in the same kind of approximate Vector space and that is the that is the trick how's that trick done yeah so what they do in the image bind paper is they take they use contrastive learning so they say here is one data point this can be and they do this across modality so let's talk a little bit about um within modality contrastive learning so for example if I take five images of dogs and then I have five images of other classes then what I can do is pass these through the same model and I get Vector representations for five dogs and five other classes so now I can say in my Vector space I'd like the data point for dog to be sufficiently close to the data points for other dogs but far away from the negative examples far away from the non- doog uh data points right so through this contrastive training I can actually push and pull data points in Vector space so once I've generated them then I can kind of put push and pull to my to my laking based on these positive and negative examples right so hang on how does that actually translate into V how does that translate into vectors and how does that help like yeah so for example if let's say you have uh let's say you have a model that understands images yeah you're going to pass your images through and the model generates the vectors so it one way that you can train this model is just to classify let's say you have a model that takes in images and its job is to Output a probability per class and you train this classifier and then you take some slice some representation of Weights in between before it gets to the classification part and that becomes your vector representation okay so sort of doing a centroid for dogs is that the you can [Music] uh yes so the the idea is that in order for it to if it's trained in order for it to correctly classify uh among the different or distinguish between the different classes it has to be able to identify what's unique about dogs versus what what's different from a dog than a cat versus a cake so it has to uh it has to nudge the the the weights such that it can identify dogs from cakes from cats and in doing so it identifies or it localizes different parts of your training set in different parts of vector space and then you take that part of the Vector space and you say now these are my embeddings this is my Vector representation for what a dog is and I'm going to use that with my Vector database and so if your model can give you this you can then go forward with this idea of contrastive learning that that I talked about okay so I'm still not entirely getting how that takes me between different modalities yeah so that's the that's the trick where let's say you have a model that you've trained independently to identify um vision right identify visual features it can take in images you've got another one that understands text and another one that understands audio if these are all generating vectors of the same dimensionality or you've kind of um modified them to generate vectors of the same dimensionality now you're getting a image representation of 500 dimensions a word representation of 500 Dimensions now what you want to do is say you take the image representation of a dog and you take its Vector so you get 500 numbers you get that bar code yeah what you want to do is then train it further to say this uh Vector representation of the image should be close to the vector representation of the word dog of a dog barking of the video of the dog so you pull together the vectors across modalities in this multi-dimensional Vector space and you push apart the concepts that shouldn't be close together right that's the part I'm missing when you get those five dogs there actually five different modalities of dogs exactly so now you've got you've got a concept you you know how um you know how babies when they go when you explain A New Concept to them they're not just taking in that concept like a machine learning model would they understand a concept across all these modalities so they when you explain to them that's a dog they see the dog they can hear the dog they can kind of see the the fur wave so they understand the motion dynamics of what happens when the air interacts with the dog so they they understand all of this they might even get smell yeah so now you've got this multi-dimensional Vector space where you you can pass in a query and then you can retrieve the uh the uh nearest neighbors across multiple modalities and so that's what this image bind model is doing so you're kind of forming a Gestalt of all the different Vector types exactly exactly okay and so this is one approach and this is the Practical approach so the last module that we released with uh with wv8 was the um image bind uh multi-c image bind where you can take any type of multimedia that the model understands it translates from that multimedia type to a vector and then you can search within this Vector space um and you can do some really cool things with this um but the the other approach here that um I haven't seen work as well I don't know if it's um I don't know if it's as advanced as uh this approach of training one model per data type and then fusing them unifying Vector uh spaces is to like you said take a model and just train it from scratch to understand all of these modalities so there's no different model it's just one model that understands everything and it has to learn to differentiate and then group itself yes that seems like it would be a lot more difficult that it is a lot more difficult but it is a lot more scalable because now think about what would happen earlier um you would have let's say you want to encode 25 modalities you have 25 models that you're training and then you have to fuse them how do you fuse them well you need to hand curate these are positive examples these are negative examples pull these closer to this representation push these Concepts away yeah um it doesn't scale whereas this one because one model understands it you don't need to unify um the the model itself in in the uh optimization process unifies for you um but it is a a more difficult training process for sure which is why I don't think we have practical implementations of these second types of multimodal models there are proposals but I haven't seen one that works as well as uh as this image bind model from meta so this is slightly ahead of The Cutting Edge that's the future you're hoping for exactly but I think this is the future this type of one model that understands all of these modalities if not all the important modalities like audio video image and text um I I think if we can get a model that understands these types of modalities I think they will function better but but for now we have this kind of duct tape solution of six different models unify them so that they each speak the same language and then now you can perform uh very interesting multimodal search cross modal search um given words you can search for audio files video files images super cool okay that does sound very cool I um so the people watching us on YouTube we able to see there's a lovely painting behind you and we we have a future where I can search for medieval poetry that's closest to that painting exactly well not a future I mean you can do it now so we have a we have a module with weeva and we've um we've actually created a demo of this where we seed the database with audio files with video files images um and um then we can search over it with any of these modalities so you could say today I'm feeling like this song that could be your query and you say here are some paintings that are are close to that query or here's some videos that are close to that query and you can do that right now this this kind of cross modal search is completely possible that seems like the most mad fun I could have since Markov chains mashing together you know heavy metal lyrics and Alice in Wonderland yeah it's amazing we're actually having multiple um hackathons around this concept of multi modality because once you give people this machine learning model that can understand all these types of data and the ability to scale up to millions or hundreds of millions of these data types then really the your imagination is the is the limiting factor right what can you put together are you going to search for audio over audio like a Shazam type of application or um you can do all sorts of interesting things with this okay this sounds like fun so to wrap up then why don't you give me the uh very high level overview of how I could do this at home and I'll I'll allow you a quick plug of we8 for this yeah so if you wanted to so all my uh demos are open source so if you go to weate tutorials um that repository has a bunch of these implemented if you go to weate examples there's a bunch of uh implementations of this as well starting off with basics of how do you search over text documents um all the way up to how do I search over these multimodal uh documents as well so you can look at examples there um also so we're implementing newer and newer modules um pretty much every month so if we see a really new cool uh model that we think would be helpful in vectorizing data like this multimodal uh image bind one from meta we integrate it so that if you take your data you can simply just point us towards that data choose which module which module we're going to use and which model we're going to use to vectorize it and then we store all the vectors and then you can come along and say this is a new file this is an image in audio we handle all of the vectorization all of the going from data point or data object to passing it through the model generating the vector and then searching over it so it's a in terms of how many lines of code it's relatively simple I think it's only 50 lines of code to creating this multimodal search engine um and there's an example of this that you can uh check out as well um outside of this uh if you're interested I would join the slack Community we've got a forum as well if have questions when you're playing around with this um so yeah that's that's where I would look uh also if uh we're we're getting feedback from the community as well so we've recently announced a new um python client so if you do try to make this multimodal search engine I would recommend you use the new python client and if you have feedback to you know uh tell us about it if you uh think that we can improve things definitely reach out with that uh and yeah would love to would love to see see what people build cool I'll get some links from you offline and we'll stick those in the show notes yeah for sure and so the last question I wanted to ask you really brass tax um is weate it's it's one of these models where it's an open- Source database plus you can pay us to host it for you exactly that's that's the very high level view which parts are open source and um what's the license just so I know yeah so it's fully open source um and this same uh the same code or the same source code that paid customers get is the exact same code that you can access on on GitHub uh we8 and so the base model the the say the base code is exactly the same if you deploy it locally on your computer it's exactly the same if you deploy it into your Cloud it's exactly the same essentially what we um charge for is managed um instances of we V8 so if you um deploy we V8 on WCS wv8 cloud Services then we charge for that um or if we manage your uh database in your own cloud environment then we charge for that um so that's that's what we're charging for but the code itself is um is exactly the same so if you have the capabilities to run the database then you get the exact same functionality and performance uh and also we're improving it daily so it's not that one performs better than the other there is only one version of wv8 the free and the paid version is exactly the same cool I like that business model it's like you you can use exactly the same at home until it becomes a headache to manage and then we'll do it for you at a price yeah that's fair okay well thank you very much I finally feel like have an understanding of how the computer science under the hood Works awesome yeah so this was very interesting because as a data scientist I understood the K near neighbors and The Brute Force approach and then I was wondering myself how this would scale up and so I I did a deep dive it's been almost a year since I joined the company but um it was very interesting to get an engineering understanding of how this is implemented which is very new to me so all of the the database implementations and how you do the approximate nearest neighbors that learning all that was very fun coming from a data science background and um yeah and the technology is moving so fast that uh not only is the machine learning component moving fast but the engineering stuff is also moving fast so we're pushing out uh updates monthly around upgrading the um the performance upgrading the features that you have within the database as well and then the machine learning world is moving so fast that we're also integrating multiple modules so that you can represent data better and better in these in these uh Vector formats as well well with the world moving so fast I should probably leave you to get back to uh keeping up awesome thank you very much for joining us great fun thank you Chris Zan thank you very much I've been playing with those ideas for a few months now but that's the first time I got a sent that I can trace the bites from the keyboard through the data structures all the way out to the screen again you know that sense that you can see how the data's processed every step and I love that feeling I love getting a new architecture inside my head feeling a certain sense of Mastery over it you know thank you hopefully listening at home you've had a similar sense of revelatory clarity I certainly hope you have if you have I'll just simply take a moment to remind you that the like and rate and share buttons are there waiting for you in your app and we will be back again next week with another look into the world of programming through another developer voice so you might want to click subscribe and notify to make sure you catch it and until next week I've been your host Chris Jenkins this has been developer voices with Zan Hassan thanks for listening oh