so I saw dr. Holden's video and enjoyed it the videos I feel like was kind of framed as a counterpoint almost but actually watching it I really didn't disagree with very much of it at all I think probably the area in which we diverge is that I think the default artificial general intelligence if you were to build one without any particular concern for safety would be a bad one would be one that we don't want to build that would try to do things we don't want it to do and I talked about this in the earlier videos when I was talking about the space of general Minds or the space of motivations this is the kind of thing that the stamp collector machines creator was expecting to happen human values are complicated right they are complicated and we don't really know what they are fully and anything that has values that aren't very tightly aligned to ours is not a thing we want to be building yeah I talked to to dr. Holm about this but almost everything in that video I agree with I agree about the timescales I agree that we are a long way away from the kind of thing that I'm talking about or with that we're probably a long way away right in the previous video I stand by the video but I think I came across as overconfident just because of the way that it was framed like I didn't choose the title the deadly truth you call something the truth capital T you know it was a thought experiment it was a I wouldn't I would have called it an interesting thought experiment about general artificial intelligence or something like that there's an argument in mind I've got quite so many views yeah I'm not going to try and tell you how to do your job you know but no the actual content of the video I stand by so there was really only one thing in doctor Holden's video that I didn't agree with which was like one sentence he was talking about how we don't pay enough attention to the the Friendly AI possibility the positive outcome which i think is true but then he said which i think is more likely I don't know how likely it is I think it depends on how seriously we take this problem of how do we actually make sure that any artificial general intelligence we create is friendly friendly you know I don't think that it's a problem that's solved and I don't think that it's a problem that will solve itself I actually think it's a problem that's very difficult to solve and we are very far from a solution or quite far from a solution it's very difficult to say with any confidence how far we are but it's comparable to how far we are from general AI itself we had a brief conversation with Professor Brailsford on exactly this and he said maybe this is like cold fusion 50 years ago they said hey in 50 years we'd be able to do this and it's always in 50 years right yeah and the thing is with historical examples you can go back and find historical examples endless historical examples of people claiming that something fantastic is right around the corner when in fact it isn't but the thing is you can also find a lot of examples of people saying that something fantastic is definitely never going to happen and it's actually right around the corner because before we talked about efficient self-sustaining fission reactions Cinda in the AI video that is an example of a situation where you have Ernest Rutherford Nobel Prize winner extremely eminent scientist so he split the atom right well he was part of a team that's forgotten and he was on the record saying some people think that you could use this as a source of actual energy that you could reliably get energy out of this that's they called it moonshine said it was completely absurd right and then you have Leo Szilard who eventually did it he had the idea now we don't know exactly but as far as we can tell the idea for using neutrons to make a self-sustaining nuclear reaction occurred to him on the same day that Rutherford was dismissing it as nonsense this kind of thing can happen I don't I'm not saying that it will that's my point right you can find situations where people are overconfident in predicting something will happen you can also find situations where people are overconfident in predicting it won't happen and it's just a function of the fact that predicting is really difficult my central point actually doesn't rely on anything to do with time scales really right I think we're a long way away from all of these things but at the same time I'd be very surprised if it were more than 100 years or something you know just because and again this is what this is as dr. Holden said it's it's kind of inescapable if we continue advancing as we are we're going to get there sooner or later we know it's not impossible unless we wipe ourselves out some other way or destroy our technological capacity some other way or we discover that the brain is literally magic and not like Roger Penrose quantum magic like actual magic then we'll get there sooner or later right my point is that the problem of AI safety is not solved is not going to solve itself and is not easy to solve and most importantly we have to solve it for we solve the problem of general AI that's that's all I'm trying to say really is that sooner or later we will probably get general AI and when we do we have to know how to make safe general AI and currently we're we're a long way from that over a long enough time scale I think that human level artificial intelligence is completely inevitable then asked was that a human you were talking to or was it a machine if you can't decide that it's not human then it's passed the test