so uh hello everybody thanks for coming in for my talk about uh a smooth introduction to Cle um Let Me Maybe introduce myself and uh our company so I'm Jo Fu I'm an associate professor at the uh well the computer science lab of R which is near Paris I want uh buture the uh very French acronym so my research topics and uh general interest is basically doing par Computing and doing that in z++ with proper uh interface proper abstraction and to do so I usually try to U torture compilers um for various reasons because why you know so I've been authoring and commenting a bunch of cace Library U on value subject including CD programming um and other stuff and I'm also a co-founder at codon which is a company centered on giving um helps trainings uh on C++ and HPC in general in various way and we try to integrate whatever we do and whatever we we need to be doing in the company into various uh open source projects so that's for myself uh what are we are going to speak about today well uh we are going to speak about a bunch of tools to deal with this you know very recurring subject about the fact that you know computers you have more of them every year you are they are more complex every time you look at them you know uh more Calles more new system stuff like that and uh it's very cool and dendy because it makes you know like um GPU vendors or CPU vendors to do uh cool Keynotes with fine benchmarks and curves that go up but uh the issue with that is that at some point um someone somewhere has to write code for these machines and before one it was basically a matter of juggling with a bunch of threads in in your head or having to think about how to vectorize a bunch of codes or doing both at the same time um well it's not going to be easier and easier because now we have like thousands of cores in gpus you have a systems you have reconfigurable systems that starts to be um a major uh player in a in a in a bunch of fields so it's not going to be easier for for the random Joe developer that have to write meaningful you know um business related code on those machines and you have the problem of choosing which vendor which Hardware which way of handling all of that and usually you have to deal with issues about how can I actually Express whatever I want to do in on those complex systems and what what are the ergonomics of all the existing Solutions um so that's normally where I should have had you know this very wellknown is kcd uh web comics about standards you know so you have 10 standards about something and nobody is happy with that so someone decide to make a new standards to you know unify everything and then you have 11 standards uh that's kind of what happened somehow uh with the cycle standard okay or CLE and I I should learn to pronounce that it's Cal like you know the the round curvy things okay and just to you know uh answer this question before it came up there is no armor project related to that so it's just the SLE um so CLE is what it's a standard it's uh mostly proped by the Kronos group uh that you may know for its work on open C among others and uh um it also deal with stuff like SP V and stuff like that which are open industry standards around all these Technologies about accelerators gpus whatever uh the idea behind CLE is the fact that we want to have something that looks like regular C++ with regular C++ em and construct being built in so you can actually write C++ looking like code on some stuff as you as you may see a lot of them and and um the stand up being open the implementation is well rather free for anybody to try and do some things and you have a bunch of actual implementation by different actors uh being companies or open source initiatives uh that targets different subet of uh machines and systems and uh basically you can write secal code okay and uh well you can Target a bunch of Intel machines you can Target a your bunch of gpus uh style machine including IMD and Nvidia you can Target fgas stuff like that and so the idea is to keep this standard as close to the language as possible and that's why we try to play around a bit with it uh to see if we can well if we can actually understand what's going on can what can we do with that the basics of the uh idea the vision behind the standards um we worked with a bunch of people on some you know actual subject uh well related to high level um sorry high energy physics and uh we will see how we can actually you know take all of that and can we actually fit that into an actual library with you know a decent API or whatnot and can we actually get something out of that so CLE Open Standards for uh ause um you know um accelerator based Computing okay okay that's the um that's the goal of the it's supported by a bunch of companies the one version we used is the Intel one which is called 1 API that gather a lot of things uh a bunch of them are very Intel specifics you have a bunch of libraries uh like the old mkl stuff for data analytics you you got TBB all punched together and everything is associated to new version of the Intel compiler which is called ipx uh this slide is actually a bit old now um which is actually well it's a new version of the Intel compiler which is based on nlvm and that also give us access to this uh SQL supports you can actually try it I will give you some information but if you are more into you know like using nonproprietary software for building your own um clang starting uh clang 50 I guess also support CLE so you can try it with with clang uh or you can try one of the other um you know version of that you can try iol also which is open source which is based on openmp somehow uh so you have a lot of you know um how to say that variation of Cal implementation laying around um the Intel one is great because now they have this you know push for more open source software and you can basically download the with everything inside and it just works so it's rather easy to um to do this so that's the version we used just for this closure so whatever result we are going to present is based on this version of CLE okay so what's uh what's the big things how how is this this thing is working so um the SQL programming model in some way if you are familiar with um pring model for gpus like Huda or stuff like that has some um similarities with this we will see that we will have this notion of you know um active threads over multicores that do stuff into blocks and so and so on and so forth the one m one main difference is that a lot of things in CLE are actually very explicit in term of uh Construction in term of selections so it may looks a bit more like you know verbos at the first time but we find find out that in the end it's actually easier to reason about what's going on and to build stuff around that so that we would try to to see that well it's put a bunch of uh functions and objects to find the device uh get an action Quee on that which is the the way we would have to pass computation to the device uh there is a bunch of way to express different par operations and different way to handle memories uh either manually or using the buffer and accessors um abstraction that will be as we will see far more C++ like that uh than what we have in other systems and we have a way to build task graph with uh an implicit or explicit endling of dependencies between operations so we have all of that playing around and we will try to see how everything works again if you want to try this there a bunch of link you can get later if you want to try that so let's go and see what's going on so the first thing we have to do uh when we write a CLE program is to connect to a device this is done through the Q object which represents um the medium of communication between your program on the host machine which is usually your main your main machine your main CPU to the whatever device you are choosing to and the queue will transfer in um operations data to the device and it can also retrieve information from the device so you can know what you are currently doing uh what kind of um specificities your device can support because you can actually choose whatever you want as a device and this will be the main uh how to say that the main uh intermediaries between uh the CPU code and the accelerator codes so you can just build a queue and by default you will get whatever the implementation decided to give you uh it could be the best device you can find like if you have an accelerator it will give it to you and if not you will access to your CPU or it can be always a CPU it's it's implementation Define but uh we can as we say that we can get the device back from the queue and ask him some information so we can say what's your name what's your uh what's your version number how many parm level do you have whatever and of course we can also um select a device based on some uh Well Properties so you can you have a bunch of pre-existing selectors that you can use to say Okay I want my I want GPU I want a CPU I want any accelerator if I have M of that you can also select device by choosing aspects so give me whatever you have but I want something which is the bable and support 16bit floting points for example and you can also write your own logic and say okay this is a device I have and uh if it has this or that properties I will rank it higher than if it only have that or that one and depending on where you run your code it will fetch the device information rank them and will give you the best one depending on your logic so you can embed a lot of how to say that um tricks into the selection process because for example uh you can actually say oh you know what uh if my um I I'm building a queue for doing some operation on a bunch of data in the matri or something and if I have the size of my Matrix I have the number of operation I want to do I can fetch a device uh fetch the information about I don't know like it's memory bandwidth or something and if it fits my execution model then I will take it and if not I will just take the CPU and you can just run your code and it will be aut automatically be placed on the correct places uh depending on what you want to choose so it's very flexible so but the thing we have to to see there is that it's explicit we start by building a queue okay but the cool thing is that you can have multiple cues you can have multiple device at the same time and you can feed the device with different cues okay and uh every action on any number of devices on any number of cues are all asynchronous as long as they are sent to different cues so you can actually build a complex system where you will be pumping data into a queue while you pump operation on the other one you can synchronize those so you can build a classical you know pipeline structure so you can send the data while you are Computing and getting some back and everything will be working basically without doing anything else but that because everything in cql is asynchronous by default that's also something we need to care about because obviously at some point we will need to do some synchronization or to do some um precise code to ensure the cooperation of the different actions and cues and whatnot so let's see how we can actually start those par operations so this is a q things okay uh so we have a string which is badly encoded of course don't do that um so this is the most basic things we we can do okay so that's using uh something which is called the unified memory model in which we will be just allocating memory in a place where both the CPU and the device can actually read and write inside it it's very crude it's not very performant in some cases but it help us just you know make something works and from that we can you know um refine and optimize later so this is just so that it fits in the slide basically okay so what do we do there so we have this string over there we have this Malo shared things that let you allocate a bunch of data uh in this shared memory block if you are familiar with Cuda terminology okay uh this is not the same shared memory as in Cuda that's something which is across the CPU the host and the device it's not the shared memory inside the device so we can just M Copy the data inside that and it will trigger um some um data transfer at some point uh and then we can ask the queue uh to start some operation and we know it's parallel operation because it's like in in the name actually that's a parallel for takes a number of repetitions and it takes a regular Lambda as as a kernel function so there is no you know like underscore underscore Global kernel syntax whatever it's a regular Lambda and actually it can be any regular colable objects so you can have function object somewhere you can have regular non uh non-template function if you want as long as it's calable as per C++ definition it's okay so there what do we do we take the uh input uh which is the index on which the Kel is currently replicated and we do something at this place we just substract one from this uh from the current characters and then we wait we wait so we wait on the completion on parallel four okay so we can do it this way we could do it like waiting on the que or we could have um stored the event object at the parallel for return and wait for it later okay event in CLE it's a bit like a pance promise future things okay so you can just you know move the things around and when that's done as the memory is shared somehow we got the result there uh directly and we can uh send it back to the system afterwards so that's the basic things okay it works but it's not very you know neither efficient nether very C++ issue know like Mal free things we don't like we don't do that anymore well we shouldn't be doing that anymore you probably should not because I don't and so yeah that's the basic thing we will see that we have better uh a better system for that now the the interesting thing is that the Malo shared things do this um allocation on this special memory block which is a bit kind to the Pine memory Ina so we do the M Copy things it's damed into the uh into the device and whenever it's done because it could take some times over there and we run that we are basically waiting implicitly on the result copy to be done we start the um the kernels on every SZ um Computing element we go we do the results to wait wait for the finishing and we know that the result is back and we get the thing that's just that but as I say we could actually have done something else there wait for later and so on and so forth so now um let's do that in a bit of a better way because at this point what we did was just allocating a bunch of data and call it today what we can do is we can use buffer and accesses buffers act as a view like array stuff okay that Maps an existing part of memory on the OST okay and uh that can be later accessed on the device using an accessor so this basically say this is a piece of memory I care about is it's that big okay then I can use this submit things which is a bit more po than just calling parallel for it takes a endler reference which is some kind of a medium between the CPU code and the device code and what we do is that say oh you know what on this device endler I want to have an access source that will look into the datab buffer and this will create a relationship between the data on the host and the device and then I can use the accessor directly as a array like you know object to access the data through the buffer so it looks like exactly like what we did before but with extra steps and non-trivial abstraction for no reasons but we will see that it's actually important and what we can do later is we can have an host accessor which is an accessor on the host machine that look up and read the data back for us to get into our um on the CPU and display and we see that we have this uh red only thing there which is an access modifiers we can have an accessors and that's very important because using this accessor and the order in which the accessor inside a submit call are constructed the comper will be able to infer the task craft dependencies between all the buffers and when the point where you are using them okay so this is a way to do it and uh it's a bit better than before because we just use this buffer thing on top of the of the data for that we don't have this Malo shared and we don't have the free it's a more a bit more you know U ER to I like okay and we can actually do even better because if we let's zoom on that a bit the same thing okay but look at what we do there we build the buffer into a scope and when the buffer is destroyed okay whatever data it's tied to which is all already on the device and not on the on the host yet will be transferred automatically so we can get rid of this strange O accessor notation just by scoping our buffer doing operation on them Wai for them to be destroyed the destructor gives the data back onto the host machine and we got our results okay so we could actually use these buffer access of f as well as the an actual r2i enabled transfer system which is very interesting because we could actually put that into a function okay we start building the buffer we do whatever we need on the device and we go out of the functions and we know that whatever the data we are Computing on the device in the function is back on the CPU when we go out of it okay so that's one way to do this we can we could also have a storage for buer elsewhere so we can keep the data in the GPU up to the point we need them later on okay fine fine fine fine now uh yeah we send stuff on on the GPU on the accelerator uh we did very simple computation for now what what if we want to actually extract some amount of fism from that so we have another model which is very close to what we are used to have on all the GPU like systems which is this uh block things that we call work groups there and so when you work on some amount of data okay uh you can actually slice them into work group and the world group is a bunch of work items okay it can be one two or three dimensionals and inside this world group you have subgroup which is onedimensional uh bunch of work item this is this green things over there in which you can actually access every work item the way you want and we have a way to either just work with work group and work items or we can work with the your hierarchy of work group group and work item as we need to express more or less nested parallelisms and by doing that it will also map on the actual Hardware on the different level of fism you have access so if you have a GPU you probably go onto the MTI processors and into the internal threads but if you have for example a CPU because you can access CPU system with CLE uh the war group will probably be threads like in in CPU threads and the subgroup will probably be mapped onto CD registers or stuff like that or multiple CD registers and by doing these hierarchical parallelisms we can actually exploit um different level of um performance you know um sources so uh let's write a small uh algorithm right let's do a four all things which is basically for each uh you take a function okay uh you take a vector and you want to process that this function on on the device so what do we do well uh when we build the queue we map the data from the vector inside the buffer okay and look that in this case I don't even have to specify the size of the buffer because buffer know what the range is and he knows how to get to the size of the range in C++ 20 directly so just know it's that it's a vector or it's a range or it has a data it has a size and it worked with that directly without us doing anything what do we do next well we have an accessor on on this buffer we compute the size and we run this parallel for this time with a ND range that will tell us that we want to work on block on 64 blocks of eight work items so this is block size and the work item size and what we have we have these item things instead of Auto there uh we see that we have this one over there which is the number of Dimension we work with and we do the classical oh well if my uh index of my walk item is in the zone of my data I just call the function over uh over the data and I'm taking a function from outside that I wrote as a Lambda on the CPU directly I mean this is a regular Lambda and you get transfer over there get ship to the to the GPU or whatever without us having to do anything else so it's a single Source single path we don't have to think about oh it has to be a device function it has to be whatever it's a Lambda just works okay so uh this is a very basic things we can probably do that better and some people did okay uh if you want to have a a look at that we can have a look at the parallel STL implementation from Kronos which is basically uh a sqle uh based um execution policy implementation you have a bunch of algorithm on GPU using this and it's it just works this way okay it just you just take a function you you run your parallel for you submit the things you capture the fun there uh by a reference over there and then again there and just walks just walks don't have to think about that and uh despite the fact that it's very uh explicit oh you have to get the queue you have get to to the endler you have to submit the thing you have to wrap stuff into a buffer in the end it just I mean it just look like regular C++ and you can just do whatever you want okay and one thing what we be having a look at later is that what if I push that to a point where I have a very complex C++ 20 library and I just do that somewhere and it just it just works okay that's in term of languages that's the main interesting point Point okay so what do we do with that well we do some science okay or we try I mean some someone with a science degree do science and we do the computation but that's deal of it okay right so you may know about the LHC the larger drone colliders uh they take protons and they smash them together because that's how physics is done right now uh and in these collisions we expect that the this ey energy Collision we just spring out new particles or new phenomenon that we don't know about or we know about and we want to be sure that it works the way we think it works and uh the atlas experiment is basically a bunch of detectors different kind calorimeters um electromagnetic detectors a lot of things okay uh that detects um how to say that not particles themselves but ins that somewhere somehow particle was there at some point and by analyzing those Trace those measurement in space and time we can guess what kind of particle was there because if your particle is evier than what you think it has a different you know Parabola like trajectory if it just go straight you know that it's not magnetic stuff like that okay physics um and there is software which is called the ACs software uh which is obviously as all use software as a recursive acronym of course so CS ISS common tracking software uh it's a bunch of algorithm of detection so we pre-process the result of the row detector data right and we try to find traces of particles so there is a bunch of um detection um reconstruction and tracking of particles based on different uh algorithm so some people do machine learning some people do old school uh stuff like you you know calman filter stuff like that so we can group a measurement somehow in the uh in in some energy level and when we have that we can say oh yeah you know it looks like we go this way you know and by aggregating those small ins uh in a in a in a process called seaing okay these blue things as soon as we have something that looks like yeah it kind of looks like a trajectory okay we can extrapolate it and try to find you know um where actual detection fits and do some statistics and so we know that it's actually whatever uh we were looking for okay and uh there is a lot of such traces in in a single experiment it's multiple hundred of gigabytes per second in every Collision they do so they have a huge amount of data to process so they want to do that fast so they try to use whatever they can to accelerate those computation so gpus fpgas whatever okay it has to be fast so what do they do well they implemented a bunch of icts um core algorithms using different techniques um on Nvidia machine using Cuda using CLE and basically uh we have a rather huge speed up compared to the um basic CPU versions and it quite performs okay if we compare to more manual uh proper CA Cod so all in all it was some kind of a good experiment because we we were about to see that um yeah it works and it works as much as good as any other uh GPU targeting things so if we do Sal stuff it works okay so what about we do less s stuff okay and more C++ stuff um yeah because you know some point um yeah we work on some oh sorry some bunch of Library uh one of them is Kaku which is the um a C+ 20 storage library of multidimensional data and uh the question is why doing this because you know MD span and stuff okay uh well it's not MD span for different reasons uh first because we wanted to have a single you know places where we have the both non non non-owning and owning data structures and we wanted to play around and try to you know uh play around with API with the way we want to Define algorithm the way we want to Define um interfaces for people to specify what they want into their container so we want to do that and uh as multi multiple Dimension data processing is complicated for many reasons we also provide different uh algorithms and execution context that let you go on different Hardware without doing the wrong thing with your data we are not in the business of computing in linear algebra stuff we are notun we don't do expression templates whatever we are just storing the data in a meaningful way in a controllable and configurable way and so we also provide way to process the data in a Curr way so we want to do use C++ 20 as much as we can which implies bunch of template meta programming a bunch of Concepts a bunch of a lot of thing like that and uh we use something that looks like execution policy it's the same idea but it's done in a different way because we wanted our context execution context to be something that some people can actually uh Define themself which is not easy with execution policies and to do so we are based on something called algorithmic par skeleton that give you a bunch of simple functions and as soon as you have that you can you are good to go and we can use them into our algorithms so let's have a look uh how does it work so we have views obviously that's the easiest things to do and and the first thing we are going to look is what the E is this definition of v okay so we have this um named the parameters based in the face where you can just say okay this is my size this is my data sources this is my option of storage and whatnot and uh you don't have to think about the order uh which parameter is a template or not we just infer everything from the uh definition of the of the object in a complex uh deduction guide but you just make a view with whatever stuff we that so this is a view of some size over the data I have there and I can pass them through to the square each things and as the type of the view is very complicated because we need to turn that into something that fit and keep all the information it's not easy to take one as a parameter so we have also parametric Concepts where you can say oh yeah I want to view or I want to view with some Dimensions or with some base type or B in any order you want or any other uh test compile time testable properties so usually we do this we have a concept of view and we can say okay give me whatever A View which is one dimensional and floating points and I can do something like this uh one what why 1D and not exactly the size because you can have this size is dynamic we can have static size we can have hybrid static compound time runtime size so we just say oh I have just want one Dimension and you can just work on it like you know it's an array so you you can just you know iterate with using index but of course you don't really want to do that so uh you can actually uh have algorithm on that and uh this version is a bit different so we build a table which is the owning version of view so we allocate some memory there and we copy whatever is in the source thing uh and we have this parent thing that let you build a sub range from a sub range descriptor so if you ever worked with mat lab or stuff like that it's basically based on this notion of I want to go to there from there it's also a bit like numai so this give us a view so that's a view between the F the second element and the one up before the end and uh well I can transform this view okay and uh if you look at what we do that we have the function first okay uh this is the output and this is the input and the input are actually viic so we have a viic amount of input at the end so we don't have to rely on having an implementation on of zip or whatever which is not natural for a lot of people and then we can print the table and we are done so we have a bunch of algorithm can also do more complex slicing okay this is a 2d view of some data and I make two sub view oh right that's the second one should be a z sorry that's that's a z that's what I get for not re reviewing my code before uh we have this slice things which is a bit more complex stuff where you can pass information about from where to where how many elements you want how many elements you want to jump through and we have this underscore things that basically means you take everything along this Dimension so we basically take a view of the data and we make two sub which are the two half of them okay along the outer Dimension we make a table which has the same size as w and we transform w and z and store them the result into T and we can return T and everything is basically just working like that and if you want to go further all our algorithm is about to take a context so we have a basic CPU context which is regular computation or you can have cycle context and uh which is basically buildable like a que like a SQL Q so there I'm asking for having a GPU and this is the Lambda I want to to walk over my table and my view and be done okay so that's basically what we want to have as an API and the question is or far can we go in term of performances well um that's a result of a complex computation think like STD power of AR tangent of x divided by cinus of whatever a huge compute compute bound uh computation right um so the great thing is the time um on on the CPU context so the basic things and second one is a CLE timing using uh the CPU as a Target okay so this is a huge uh 24 cores or 16 cores I don't remember 16 cores hyper threaded uh IMD cores uh it's double so we expect to get something around 2 * 16 as a speed up okay two from the CD level and 16 from the course and the best speed up we have is like 29 over 32 which is quite okay okay okay and now we can take the same the same code and we just change I mean we just change the way we uh initialize the selector for the CLE context and we can go over some Nvidia things okay and then again we got these things speed up is a bit less because the GPU is a bit less interesting than the CPU but that's another discussion but we got some performance out of our code and the the the the thing that we want to achieve with that is to be able to if we go back a bit um let's just provide proper implementation for all these algorithms and a bunch more um especially um being able to work on subtiles working on complex stencils and Maps up onto the proper uh device code each time and by using this uh simple implementation of the context that we just have to have a map reduce and a scan base uh operation and we can rebuild all the algorithm from that uh we can quickly have a bunch of of first running and for more complex algorithm that doesn't fit the simple model think about sort or uh search stuff like that we have a way to specialize um algorithms based on on an actual concrete type of a context and so we can write the proper things done so it was a cool a cool experiment um the non cycle version of the library was quite already advanced when we start doing that and uh the CLE um implementation to us like what two weeks something like that of just finding the correct you know uh way of doing the things wrapping the uh the elements in the correct way so it was quite a nice experience because we had this existing C++ 20 code based uh we came back with CLE just show the thing where they should go and it was just working so that's something which were actually a good you know um positive return on that okay so uh before concluding [Music] um want to say that there is a lot of things going on into the C API um there is an extensive document on the konos websites that give you all the information about how it's supposed to be working okay and usually okay I I should have make a real solo that because 10 bucks I will pick something and it won't work uh where is it section whatever yeah got some pretty extensive documentation and even sometimes you got some decent yeah not this one of course got some decent examp or maybe for the r you got some decent examples some places um it's uh it's updated quite frequently whenever something changes uh you have an extensive um list of stuff that change from the version that was not C++ 20 based they offer information about how to um simplify old sqle into new sqle and stuff like that it's a very useful uh document it's pretty much interesting to write and for the people that do what is it um they want to go further than that they also explain oh you have a special hardware and you want to support CLE this is the thing you have to write uh so we can actually support your whatever into SEC directly so it's a very uh open-ended things uh it's very interesting to to have a look at that if you are into this so let's conclude on that so um moving forward well if you want to use accelerators or whatnot uh you have to have tool for doing this because uh you are not going to be able to find people that are able to think about your business like business side algorithms and then know how to work on all those machines so you have to have tools uh and having uh standard like C which is CA vendor CA machine is actually a step into the right direction so you can quote me on that that's my personal opinion on that uh it's easy to use uh it's very simple to deploy um so I cannot do anything else but telling you to try it if you want to go into that um it's C++ 20 compatible uh you know what the range is uh it knows how to Endor stuff like you know topples it has a very extensive uh way to detect um trivial types regular types and know how to handle all of that it's concept compatible so it's actually very interesting um it's also cool to give feedback back to Kronos they are pretty open about that so you can actually try the things report bugs and try to get things uh sorted and I would just want to make a special thanks to uh s which is actually my PhD student that work on that and which is responsible for all the graphs and the explanation about the uh Atlas experiment that we worked on with uh Adan gon and David shamon from Labs that are also working on on this project so um thank you very much and uh see you next [Applause] time and and in a in in a quite surprising time of event I still have time for questions this time so any questions come on uh yeah I have a few so like I can alternate with other people let's go uh so uh first of all like I didn't you said oh okay it's going to Target this device it's going to Target that device right and depending on what's available and stuff when is the compilation happening um of the of the code of the sqo code for the device uh so you actually have the choice uh basically uh the the uh the main way of doing that it's all compiled um ahead of times in something that looks like a lot like PTX for uh Cuda which is a some kind of an abstract pre-compiled stuff that at run time we get adapted to whatever happens okay as the very beginning now uh you could also if you um if you use uh a preset selector like if you if you say oh I I build my queue with a CPU selector or I build my queue for a GPU selector that's something the compiler detect and we just compile for the correct so for the correct um sorry the correct device ahead of time now you have also a way to ask for just in time compilation so when the application start whatever your colel walls are going to be compiled so you actually have the choice and the compiler on it side try to infer from whatever you wrote on the Q definition whatever it should be trying to compile for and uh this compilation can be done either when you compile or it can be done just at the beginning of the application you you actually have the choice on that so I can do all ahead of of time all all just in time or I can do partial compilation that's a good question I I I think the partial compilation it was working at some point okay okay let me rephrase that oh God and I'm and I'm actually uh you know recorded um so in tell people you know just forget that uh it was supported at some point uh and you still find trace of that into the document mation now I will be very Frank with you uh we usually just you know do whatever the basics things do so we are probably just using the ahead of time things um I'm not sure the hybrid thing is still supported but I have to go back on you and that because I don't want to say something stupid but we we have this opportunity to change that I know the hybrid things was working for a while uh when C was basically trying to just ride over open C and all the pre-existing um implementation I probably have to check if it's still the case thank you hello I have some basic questions just about selecting uh uh the requirements on on on your yes where the Q should the selector yeah you can uh put there as many condition as you can yep and it will choose the best one and can you pass their condition that you want different device than the other Q has yes so what you can do is you can select so you have this which is this is basically a NM okay this one the CPU selector GPU selector accelerator selector it's an enem it's basic things to do and we just pick one then you can uh use what they what we call aspects which is a list of stuff you want on your device or stuff you don't want on your device you have a low deny list systems so you can actually say oh I want this and that but not that and that and the the system will try to find you something but if it doesn't find anything that works it would just say I didn't find anything I'm just quitting and then you can pass an arbitrary function that take a device as a parameters and return an integers and what it does with that is that you can test for uh information about the device or whatever you want next to that because you can just construct this function or object function the way you want or you can pass additional information and what you do is that you say okay this device I can ask for information about it and I can give it the score and you will pick the highest score possible and you have the choice to say if I return zero uh just pick one because that means I I didn't find anything fancy so just make a choice or you can return minus one which give you the say okay if you didn't succeed into finding this Lo something with this logic I want you to fail so instead of just saying I want this one or that one based on information you can actually rank um uh properties of the device and the ranking will be uh used by the system by okay this is all the device I have I will rank all of them and I we pick the best ranking or I will pick nothing or I we just fail so you have a you have a bunch of how to say that um flexibility on that uh considering that for example this get for things it basically has like I don't know like 20 30 something information you can grab from the device so ranging from the kind of memory it supports the kind of operation it supports is it debuggable is it emulated or whatever I mean you can get very fine grain selection process or ranking process and usually it's enough and you have this opportunity to say if I don't find anything I can go back and you know give you a default oper systems so you can B Bally write whatever so in the beginning you had this slide that showed the bunch of implementations that are available so I'm wondering if there if I'm writing an open source library is there some kind of fallback implementation if a user of my library doesn't use one of these compilers that will just always oh you you can you can use a sorry you can use a Clank cycle implementation yeah but say someone is using visual studio and they don't have it is there like a library implementation that will just always schedule everything on the CPU and if you use another so that's probably you probably want to Target three cycle which was the uh old vers the oldest version of all of that and it has a default mode where you just do CPU things no I'm thinking more along along lines of I'm making an open source Library I want my users to be able to use whatever compiler they are using yes and so they will compile my open source Library themselves and they will use probably visual studio and I have no control over that is there like a let's say you do that you your target user studio and you want people to be able to use your things that you CLE then uh you should tell them to use a three cycle Library version which is basically CLE as a Library without any compiler based so just the most regular you know uh no special things you can have and so they just you can just say oh if you are on these things and you don't have any of this fancy uh you can use that can use just tricycle okay thank you again um thank you uh so Joel um I know you're very familiar with uh CPU CD stuff and the question is so this seems to from what we talking about this seems to be a lot GPU based so how close do you think you can get with CLE to the handwritten C code that targets uh CPU and CD and threads okay um so for the CPU supports um on the threading front uh it's basically using uh it's usually used Stu like as open m or TBB in the back so the trading support is pretty good uh for simd depending on the uh back end so I'm I'm speaking from my own experience so I may be wrong so take that with a piece pain of salt uh people usually either rely on pragma SD from open MP or they try to reuse internally the uh autov vectorization process of the compiler so that basically mean that the IMD quality is depending on what kind of um system they use for that so I guess basically that um if you compile using say the Intel versions you will get result that we get pretty close to whatever the auto vectorizer from Intel is going uh if you use any other one probably Clan Clan is probably relying on its own autov vectorizer on or on open the SD and in all cases that means that if you have a piece of code that is not not um how to say that um that amenable to being vectorized because it use some you know like some cmat functions or it use a complicated memory access pattern that the auto vectorizer doesn't know about is probably subar compared to what you can do by end now uh what could be done but my days are quite full already it should be possible to make a a platform back end for Cle that actually use an actual proper vectorization system uh I didn't look into that but it's probably fible um but then you will have to uh be careful because you cannot you have a lot of information in this platform back end things that is probably too run timey uh if you want to have a perfect you know CD code generation so it's usually as good as whatever the autov vectorizer of the comper is which means that if you do how to say that um Regular computation I don't know if it's a thing you know like you are not going to uh to do very fancy complicated uh matte things or you have very complicated run runtime based and in size or whatnot it would probably be pretty good uh I mean as as much as good as you could have with the auto rizer but it's probably some not below what you can be doing with manual or simply if you want to you know write everything yourself and and do all these complex things all by yourselves it's probably in between okay hello um so you had these questions with a secret string what's the decoded version oh uh I don't remember uh I think it's something like H World from SEL or something like that uh yeah stuff like that yeah it's probably a world this is this is cycle from one API or something like that uh the funny thing is that that's actually a that's actually nint example and they didn't check that I mean I mean the end of the uh the end of the secret is actually okay sowhere no yeah it's a h World thing I mean it's not very very funny and we have another question from the audience so what implementation do you recommend to get get started with cycle and can you use actually clang directly uh normally clang just work out of the box uh as long as you have like some recent versions uh we used to use the clang trun that it's on compiler Explorer to do our early exploration if you don't want to you know go into oh I need to install complex stuff on my machine or I can't install whatever uh just just give a try to the 1 AP Docker it's I mean it's a bit big it's like 10 GB something like that but you have the Y you have the Y Bonanza so you have um you have the compiler for SQL you have the uh libraries you have the um what's the name of these things uh it change all the time uh vune Slash uh parallel advisor SL I don't know how it's called today uh but you see this kind of thing so uh it's it's R trial you can just Docker it up and it just works um so depending on what you want to have as an experience you go either with a claim version or with that and if you want to go a bit further than that and you need stuff like you need to support uh Cuda you you want to Target sorry Cuda and stuff like that uh then uh you have an extensive uh setup setup documentation on the 1 API Pages where you can have you can find the exact uh packet list for your own Linux distribution uh both for the compilers the tools and the um the Cod playay Cuda plugin if you want to support Cuda it's rather well documented and uh from experence it usually just works so it's something which is also actually quite cool because you don't have to deal with a bunch of you know uh trashing your display because you forgot to update your graphic driver or whatnot so then thank you again thank you [Applause] again