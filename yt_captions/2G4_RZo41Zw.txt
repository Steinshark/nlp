Early in 2023, TSMC presented some details about 
their new set of N3 - or 3 nanometer - nodes. One   of the things that jumped out at me and 
other people is the static RAM, or SRAM. TSMC has two N3 nodes - N3B and N3E. The 
former is a dead end reserved mostly for   Apple’s use. The latter, N3E, is the one most 
other customers like Nvidia and AMD will use. TSMC’s presentation said that their N3E SRAM 
bitcell or just cell will be the same size as   that of their N5 nodes. In other words, no SRAM 
scaling from the N4 to the N3 node generations. There has been a lot of concern and 
ink spilled over this revelation. But   what is SRAM anyway? And how big 
of a deal is this? In this video,   we take a look at one of the most 
important parts of the integrated circuit. ## Beginnings Before we talk about SRAM, we must 
talk about the Memory Hierarchy. There are a whole lot of memory technologies 
out there. The hierarchy sorts them by their   response time. At one end, we have 
very fast memory - Level-1, -2,   or even -3 caches which are oft right on the chip. This is where SRAM often is on the hierarchy. 
It is very fast - it can pull out data within   a few nanoseconds. But the drawback is that it 
is expensive to manufacture, and if that SRAM is   "embedded" - meaning that it is placed on the chip 
- then it takes up precious space on the silicon. On the other side, we have things like Dynamic 
RAM. And even further down - flash memory,   and hard drives. These memory systems are 
far cheaper and have much more data capacity. But they are "off-chip" - usually sitting 
on the motherboard - meaning that to access   their data takes more time 
and will cost more energy. Modern systems like those on an IC require fast 
access to memory. As one of the memories closest   to the system, SRAM is hugely important to an IC’s 
overall cost and functionality. It's a big deal. ## History The first SRAM design was patented in 1963 
by Robert Norman of Fairchild Semiconductor. He designed it on his friend's breakfast 
room table in a few hours at IBM's request.   IBM demanded that Fairchild sign over 
their patent, but the latter held firm. That first design used bipolar transistors. A 
year later, Fairchild patented another design   using MOS transistors, a different 
structure that is far more scalable. In 1970, a 256-bit TTL SRAM produced by 
Fairchild was used for a computer main   memory for the first time - a Burroughs 
Illiac IV. A year later, IBM shipped a   System/370 Model 145 computer with a 128-bit 
version bipolar SRAM. SRAM has since been one   of the most widely manufactured memories since, 
and a default choice for integrated circuits. ## How It Works Like its cousin DRAM, SRAM is 
a volatile memory. This means   that when you lose the power, they lose the data. Unlike its cousin however, SRAM does not use 
a capacitor to store its bits of data. Thus   there is no need to periodically refresh the 
capacitor's charge to maintain data integrity. The most common cell design is the 
6-transistor SRAM cell. It uses what   is awkwardly called a "bistable 
latch". "Bistable", because it   has two stable states representing 
the 1s and 0s of a bit of data. The latch is made up of a pair 
of inverters. An inverter is   a single input and output device that 
toggles a signal to its opposite state. So if a low signal - a "0", in 
digital parlance - comes in,   then the inverter device turns it into 
a high signal - a "1". And vice versa. We put two inverters together to 
create a feedback loop - the output   of one inverter connecting to the input 
of its lovey-dovey partner. An inverter   is made up of two transistors each, 
so that is four transistors in total. The other two transistors in the SRAM 
cell - formally called the "pass gate   transistors" - are for reading or writing 
the value of the data bit inside the latch. Pulling back, we have many memory cells 
strung up in a row. Each cell in the row   is connected to a horizontal line that we call the   "wordline" - as well as two vertical 
lines that we call the "bitlines". The wordline is used to select 
a row of cells. Once selected,   the cells can interact with their two bitlines. ## SRAM Shrinkage
Modern high performance system on chips or SOCs have demanded increasingly 
better embedded memories. In some cases, embedded SRAMs consume a major 
portion of the chip's area. Back in the mid-2000s   for some high-performance CPUs, SRAM percentage 
took up as much as 71% of the whole die. Today,   there are some systems with 90% of their 
surface area covered in embedded memory. Note, that high percentage isn't always 
the case. Portable devices for instance   have less embedded memories. 
Not only for spacing reasons,   but also because SRAM cells use power and we want 
to minimize power draw on the limited battery. But I digress. My point here is that systems are 
always craving memory. But that memory doesn't   always scale down as fast as everything else. Why 
is that? The SRAM cell is made up of transistors,   but shrinking it is far more challenging 
than just making those transistors smaller. ## Power Leakage The first major challenge 
has to do with power leakage. Traditionally, SRAM cells use the most 
energy when they are switching - "dynamic   power". And that certainly was the case 
back when the transistors were larger. But as cells got more dense 
and the transistors shrank,   focus shifted from reducing dynamic energy 
usage to leakage. Power leakage is when the   charge flows through the gate in the 
transistor, but the gate is closed. This happens for several reasons but the two most   significant are sub-threshold 
leakage and gate tunneling. Okay, sub-threshold leakage. An 
open transistor gate closes when   its voltage hits a threshold. That 
threshold is low but not quite zero. So when the transistor hits the 
threshold voltage and closes,   it can still conduct a very tiny current even 
in its "sub-threshold" state. Kind of like a   faucet that you close but it keeps on dripping 
because you did not turn it tightly enough. As we scale down the gate size, the threshold 
voltage lowers to reduce overall power   consumption. But as that threshold voltage 
lowers, the sub-threshold leakage rises like   yeast in a warm room - as in, drastically. 
About 10 times for every 0.1-volt decrease. The second is more intuitive. This 
is where the transistor gate has   gotten to be so thin that the charge 
carriers like electrons and holes can   quantum-tunnel right through the 1 or 
2-nanometer thick gate oxide layer. And as the gate gets smaller,   it gets thinner. Which in turn makes the 
quantum tunneling effect more prevalent. These two effects were not really a 
problem going up to the 130 nanometer   node, but suddenly took a big leap 
starting at the 90 nanometer node. ## Going FinFET This power leakage is a problem 
for anything not connected to a   fusion reactor. But a particularly thorny 
issue for things depending on batteries. There are a few things that designers do to help 
eliminate some of the leaks. A notable one is the   gated VDD technique, where we add more transistors 
to shore up the gate and prevent further leakage. Another major solution has been the 
introduction of new transistor gate   designs like the FinFET. I have mentioned it 
before, but the FinFET is a type of 3D gate   that covers the channel on three out of four 
sides, giving it more control over the current. FinFETs do indeed offer better power leakage and 
density. The old ITRS roadmap on semiconductors   predicted that if nothing changed then by 
2014 some ICs would be 94% covered with just   SRAM. FinFETs helped avoid us this dark future by 
introducing more miniaturization and efficiency. However, FinFETs are harder in general 
to produce. There is greater risk of   getting it wrong and ending up 
with defective products. That   leads right into our next major 
challenge - process variations. ## Process Variation
There are two types of yield - functional yield, which represents the fraction of 
ICs that work in the first place. And then there is parametric yield, which measures 
the variability in how the chip performs in speed   and power. As a fab like TSMC or Samsung goes 
through its process steps producing the chip,   very very small variations from the recipe can 
eventually impair the product’s final performance. These variations are extremely minute, but 
can generally be traced to difficulties   in controlling lithography lines 
and the roughness of those lines. So the channels' width, length, 
or threshold voltages being just   a bit off can lead to performance 
deterioration. And unfortunately,   process defects do not scale down with size, 
making their impact on the product far greater. We measure this using something 
called Static Noise Margin,   or SNM. The SRAM cell is very busy, 
subject to a lot of electrical noise.   Sometimes that noise can cause the cell 
to "flip", losing its stored bit of data. SNM is a simple measure of how resistant the 
SRAM cell is to flipping. In other words,   its resiliency and stability against noise. We can plot variations in length, width,   and threshold voltage in each of the 
six transistors inside the cell against   the resulting SNM to get what is called a 
"butterfly curve". Which look pretty cool. For the most part, the best way to avoid these is 
for the foundry to simply do better. Of course,   easier said than done. But hey, that's 
why we pay $20,000 a wafer right? ## Alt Designs
Okay, so if the DRAM cell with 1 transistor has been so successful. Then why keep 
the 6-transistor SRAM cell design? There are other designs with more or 
fewer transistors. The problem is that   each design offers its own tradeoffs. 
And density isn’t always the ideal.   That’s the problem with memory - the dual 
mandate between capacity and performance. For instance, there exists a 5-transistor SRAM 
cell design, having just one access transistor   and one bitline connected to it. It is more geared 
for density - taking up 15-20% less space but at   the cost of less Static Noise Margin. In 
other words, the cells are less stable. Way on the other side of the spectrum, we 
have the chonk 10-transistor Schmitt-Trigger   SRAM cell design. It replaces the traditional 
inverters with what is called a Schmitt trigger,   named after Otto Schmitt who 
came up with them in 1934. It takes up twice as much area as the 5-transistor 
design but gives you a far more stable SRAM cell.   10 transistors might be too much, but we might 
be seeing more Schmitt trigger-based designs   down the line. Its additional stability 
makes it suitable for very advanced nodes. There are also 8 and 9 transistor designs. But,   the 8-transistor design gives you great process 
stability but suffers from unacceptably bad   power leakage. This hasn’t been a 
good trade off for manufacturers. The 9-transistor gives improved 
power leakage and stability over   the 6-transistor design. But at the cost of 
more area consumed and a fairly complicated   design. Same as with the 8-transistor 
design, this is not a good tradeoff. In the end, the 6-transistor design is most often 
used because it is simpler, has acceptable power   leakage, consumes less area, and has fewer noise 
issues. It strikes a balance in the criteria. ## Stacking? I will be derelict in my 
duties if I do not mention   anything related to advanced packaging at the end. The aforementioned challenges with SRAM 
cell scaling have caused companies to try   stacked RAM arrangements. AMD and 
other chip companies are adopting   stacked SRAM solutions where we put 
the SRAM die on top of the logic. For instance, AMD's 3D V-cache which involves a 
bunch of SRAM dies stacked on top of an existing   Level-3 Cache. This allows us to add even 
more Level-3 cache. And as we all like to say,   more memory is as good as I remember. Just 
kidding, nobody says that. I made it up just now. The more SRAM gets harder to scale down, 
the more people will be pushed towards   advanced packaging solutions. And I think 
that's the right direction going forward. ## Conclusion But can SRAM itself keep scaling? 
It looks like TSMC hit some limits   on what can be done using the 
current FinFET architecture. Observers have posited that TSMC bit 
off more than it can chew with N3B,   which is said to have had yield 
issues. I feel like reversing   the SRAM density numbers like they did 
with N3E is a good indicator of that. But it is not and should not be the end of 
the line. Something about an IC being 90%+   SRAM just doesn't sit right with me. Logic 
dies should have logic circuits on them. I   hope I am not offending anyone when I say 
that. Figure out a way around the SRAM,   and there is still so much more headroom to go. And as for the future of SRAM itself. Next we have 
the move to Gate-All-Around FET gate or nanosheet   designs. This is where the gate wraps around the 
channel on all four sides for even more control. Samsung is already making these. Intel,   TSMC and SMIC are working towards it. A 
recent paper hints that these structures   might improve SRAM performance in other ways 
but shrinkage itself might not continue. Imec seems to suggest that forksheets, 
another type of transistor gate with a   different structure, would be better suited 
for continued scaling down. These are kind   of like the Gate-all-Around but adds an 
additional layer of dielectric cutting in   between. Needless to say, these are 
going to be very very hard to make. SRAM will always have a role in the IC. But it 
is looking to me quite clear that squeezing out   more fast cache memory with SRAM is a steadily 
losing game. We will need alternative solutions.