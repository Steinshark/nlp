in 2016 humanity took another loss when google's alphago defeated lee sedol at the game of go but in the midst of that loss some people comforted themselves with a small observation in terms of energy use the human brain is far more efficient than the computer the 2016 alphago ran on 48 tpus each consuming about 40 watts the human brain on the other hand with its 100 billion neurons runs on a meager 20 or so watts neural networks have gotten increasingly bigger over the years and with that the amount of energy we need to run them has also grown scientists have been looking for ways to bend the curve on this cost trend and a very promising approach has been to use silicon photonics in this video i want to talk about the growing effort surrounding neural networks running on meshes of light before we set out on this i want to thank phd student alex ludz from mit for not only introducing me to this world of silicon photonics but also holding my hand and walking me through this ultra complicated technology i will inevitably make some errors in this video which will be a little more technical than most forgive me for them these errors are mine alone not alex's let us start at the beginning much of our recent advances in deep learning have been enabled by two pieces of silicon technology gpus and ai accelerators the gpu first allowed us to train large and accurate neural networks in a reasonable amount of time a big deal but an arguably bigger development was when google founded the multi-billion dollar ai accelerator industry with the tensor processing unit or tpu these ai accelerators have helped us bring the benefits of machine learning to the general public by allowing us to run these models on our data at scale with these tools like google photos and the lot are possible for us to use yet despite their massive impact these accelerators are conceptually pretty simple that is because they are geared to do one thing really well matrix multiplication in practice neural networks are represented as mathematical matrices when someone wants to use a pre-trained neural network model like to identify what is in the picture they use matrices the pre-trained model is a matrix the image data is a matrix and roughly speaking when the model is being run you are multiplying those matrices together in search of a final answer matrix multiplication is a simple math operation that requires the summation of the products of each of the matrices entries over 90 percent of running a pre-trained neural network a process that the industry refers to as inference involves matrix multiplication operations so inside the tpu is a 2d array containing tens of thousands of multiply accumulator circuits or mac units each mac unit is geared to do one thing multiplying two numbers at high precision and then adding that to an accumulation sum when in use the mac units retrieve a matrix of predetermined values from the pre-trained neural network model referred to as weights so that they can multiply them with the image data matrix knowing this it makes perfect sense why the tpu outperforms a big nvidia gpu it's the semiconductor equivalent of a guy who only pumps iron with his right arm because he's a professional arm wrestler one of the big problems with this computing paradigm has been energy consumption like i mentioned at the very start of this video the brain is capable of doing what it does with far less power why interestingly enough if you crack open an ai accelerator like the google tpu and look at how it uses its 40 watts of power you will find that about 80 percent of that power budget is spent on connections and data transfer every time the mac circuits move data around like to load in the weights from the system memory energy is used this is because the connections are electric a flow of electrons negatively charged subatomic particles circulating around the circuit electrons can interact with other particles and when they do they generate losses in the form of heat furthermore ai accelerators are very parallel each of those macs are running side by side on separate tasks which lets them finish the whole project faster but at the cost of using more energy companies and designers are aware of this and have designed their systems with it in mind for instance they might design the system to bring the model's weights physically closer to the chip's mac circuits as a result of these efforts ai accelerators have gotten to be 20 times more power efficient on a per mac basis than a gpu ai engineers are also doing more with less hardware the alphago of 2016 used 48 tpus across a broad network but a year later google debuted alpha 0 a better performing ai that uses just 4 tpus in a single machine despite all that we still want to find ways to disruptively improve the energy efficiency of the hardware running these neural network models it stands in the way of achieving artificial general intelligence and it also cost companies a lot of money so what can we do this is where silicon photonics comes into play i talked a little bit about silicon photonics in another video so you can watch that if you want the whole tamale here's the season recap silicon photonics chips replace the electrical connections used by traditional semiconductors with light based ones using the same principles that allow us to send data through fiber optic cables you can send light signals around the chip with hardly any losses broadly speaking the benefits of silicon photonic computing devices over traditional silicon devices are twofold first very high bandwidth 100 terahertz and above you can transmit optical signals at super high frequencies because those signals are moving at the speed of light second and more importantly there is the potential of very low power consumption the only energy used would be that for sending or receiving the light itself that 80 percent or so of power that the tpu was using for data connections gone of course that's only the theory in reality there are limits to what can be practically achieved in the energy efficiency category but the overall gain can still be a potential 10x improvement over the market status quo okay so how does this look in practice the simplest implementation is a hybrid approach that fuses silicon photonics and traditional semiconductors the traditional semiconductors will help store the inputs the weights and the intermediate results between neural network layers that data would then cross over to a photonic circuit through digital to analog bridges or dacs where the crazy light bending stuff happens the output comes back to the rest of the circuit through adcs analog to digital let's take a look at a company called light matter the mit spinoff startup has raised over 100 million dollars from venture capitalists and big enterprises for their approach to silicon photonics enabled deep learning their photonic circuit which they referred to as a nanophotonic processor replaces the google tpu's 2d array of multiply accumulator circuits with a mesh of silicon photonics components called the max zener interferometer or mzi the maxzediner is a basic building block of photonics if you apply a voltage to it then it can split and then recombine light in a specific way when this happens the nature of that recombined light changes from its input state the magnitude of that change can be mapped to a multiplication result yes really with this your weight and input matrices can be converted into arrangements of light roughly speaking you then execute the matrix multiplications computations by sending that light through the mzi photonic mesh since light travels so fast this calculation happens quickly about 100 picoseconds the photons that come out at the end can be detected collected and then mapped to a value that represents your computation result this photonic mesh produces working neural networks albeit smaller ones with only a few neurons and layers and as predicted these small networks are three orders of magnitude more power efficient than their electrical peers so this stuff actually works with all that being said however there are still a few other issues let's talk about the two biggest first the accuracy isn't quite there yet the light matter team notes in their 2017 paper that their photonic neural network had about a 76.7 accuracy rating in recognizing vowel sounds simulations suggested that this should have done about 91 percent a large part of this has to do with how the system encodes and decodes the data from light in other words the conversion from analog to digital photons are weird and sometimes you get measurement errors there are ideas to improve this for instance improving the light's contrast to make it more detectable but more time and investment is needed this brings up another point when it comes to running on photonic meshes and other analog computers of this nature because you are dealing with analog signals you are unlikely to get the same accuracy that you can get with digital signals as in you cannot get as many places in your numbers for this reason silicon photonics will probably remain a technology for running inference rather than training companies will still have to rely on big nvidia gpus for that purpose all that being said there have been some recent interesting efforts one group from cornell has been able to train up unusual neural networks that lack the digital accuracy i just mentioned while using traditional digital methods so perhaps this accuracy obstacle can be overcome another issue is that of size and scale the system profiled in the 2017 light matter paper had just two layers and 56 mach zedner interferometers in the 1980s bell labs produced optical transistors with the intention of making commercial computers but the effort didn't live up to its promise in part due to the bulky size of the optics the advent of silicon photonics has helped us shrink these components somewhat but mzi's are still rather bulky they're usually about 10 000 square micrometers which is big in the semiconductor world to compare the nvidia a100 has hundreds of cores capable of trillions of floating point operations each second in order to be commercially competitive can a photonic mesh system scale up to far more layers and neurons while retaining its advantages in this particular space the problem remains outstanding what light matter has done to tackle it was to produce bigger and bigger silicon photonics chips in 2020 they announced their mars chip which has a 64 by 64 matrix of mach zedner interferometers fabbed on a 90 nanometer process companies like global foundries and intel have been investing a lot of money into their silicon photonics platforms so it is very possible that these chips reach commercialization scale simply based on the fabrication improvements the foundries make if they don't however there have been some theoretical approaches to scaling without needing to fab out all those interferometers for instance it is possible to get a 1d row of interferometers to perform similarly to a 2d mesh by essentially replacing one of the dimensions with a time dimension this offers a theoretical pathway to much larger neural networks without needing the silicon photonics fabrication technology to catch up accordingly silicon photonics powered neural networks sounds like a marketing phrase but the ideas scratch real world itches googled created the tpu because they realized they needed some way to make it cheaper to run neural network models on real world data the same economic rationale applies here energy usage is a direct operating cost for these massive data centers if light-powered neural networks can provide similar performance but with 10 times less energy usage google and amazon will snap them up like hotcakes that is of course if these photonic meshes can perform competition in the space is heating up with a number of startups in addition to light matter jumping in with their own products it shows that the technology has potential it just needs to deliver on that promise alright everyone that's it for tonight thanks for watching subscribe to the channel sign up for the newsletter and i'll see you guys next time