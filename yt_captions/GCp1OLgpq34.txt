okay hello everyone welcome to this to this lecture and the title of this lecture is my program was running fast six months ago what happened or in the essence why do programs tend to get slower over time i'm sure everybody has uh have he has had uh a situation when something like this happens so let's dive into details uh before before we start a short note about myself so my name is evita bogusalich from serbia i i do application performance so my professional professional focus is how to make your programs run faster faster and i mostly focus on cnc plus plus programs so we use better algorithms use we try to exploit hardware better we use the standard library in a better way you use programming languages better way or the operating system i also do work as a performance consultant so when people hear problems with debugging performance issue or help them and also help training with teams that that develop performance as the software so introduction uh as i said uh as software develops and you add more features to it it's some some somehow it feels to get slower even though uh in a way you wouldn't expect that to happen now there are several reasons so there is not one reason why this happens there are several of them and i grouped them into five categories and we'll talk about them in this lecture the first categories architectural issues so issues in how your software is designed uh there are algorithmic issues so these are not these are the issues in how well is your algorithm how well is your algorithm working then there are issues with memory allocations uh memory issues with compiler optimizations and there are also issues with uh with hardware which prevent your your program from running from full speed at full seat okay so the first topics are architectural issues so when it comes to architectural issues uh architectural issues can be in essence divided to divide it into api design issues so architecture general deals with api design with internal component design and how different components work together to achieve a certain goal so the problem with architectural issues that make your program run slower is that they can require a lot of rewrite to improve so you need to coordinate several teams and you need to do many changes in many places so if uh for performance sensitive systems you need to take uh you need to take into account the considerations related to architecture if you want to get uh if you get want to get to do good performance and you have to do it from the beginning you cannot do it later or you can do it but with much higher price but luckily with careful design most of the time you can you can uh you don't have to hit the wall so the first architecturalist issue is generally known as static components chatting means just speaking without any need so as i said the software consists of logical components or modules we call them in this lecture components and the components exchange information on the cells to get some work done now if you are uh interesting into into making components work as fast as possible you need to think about a little bit include performance in the in the design decisions for your api uh there are two crucial things you should do one is to minimize the number of time component a has to communicate with component b and the second thing is you need to minimize the size of the messages exchange between a and b okay if you follow this you should generally achieve a good api design uh which is streamlined from performance however if components talk a lot they exchange a lot of messages that they are called chatty uh for example a typical when you will see chatty components is when you hear you have two components they communicate some help and the one is adding data to the other one by one instead instead of sending data in bulk instead of setting one million data it will change just date the bundle so one piece of data by one now why chatting skills why why is the reason why such design kills performance so there are several reasons for it the first is the overhead of function calls so function calls is a function called just takes some time if the function in component b which you're calling is really short the overhead the function calls can be severe that's the the number one reason luckily compilers are most of the time now good at doing inlining but cross component in lining is not so in lining when you have a compilation unit uh one and compilation unit two so the functions from completion two to unit two generally don't get inline compilation unit one also if you're using dynamic libraries this also doesn't happen the second reason why checking chatting skills performance for cell is that it inhibits compiler optimization so by inhibiting if you don't have inlining the compiler generally cannot optimize valve functions uh which are not in line it can only optimize inline code and this can make in certain certain occurrences it can make your function like running two or three times slower than it actually needs to be next we have the overhead of critical section protection so if you have a component and it's critical it has a critical section so several threads are accessing it and you're adding mutexes you need to lock unlock matrixes if the function shows the overhead of locking and unlocking mutexes will can be can be large and the last thing is instruction cache misses so we'll talk about instruction cache misses later i'm just smashing if you have chatty components you will have those as well the price of chattiness if you're running in a single process is also but if you were if with one point you need to move components for whatever reason security often to another process then the price becomes higher so this uh chatting which crosses the border between processes is more expensive and so is if you have two processes that are running on the on to two machines so the price of chattanooga's can be really really high uh there are some additional problems uh these these three or four problems are generally related to chat in chattanooga's and there are additional problems uh if the component is not chatted that means it's sending data in bulks and there are several of optimizations compiler optimizations or composite optimization chattering is that require that you process data in bulk not just one by one only in that case you can achieve some additional speed up if that is needed so fixing later is difficult because it requires a large rewrite now the example of chatty components is the example of system allocator the implementations of molecule free so malik the function malik uh the second the function memo malacalic is just one block of memory and function three just releases one block of memory but many times you have a program that you'll have data structures that need to allocate one billion identical objects so this in some naive implementations of the system allocate that this can require uh one million calls function calls one million billion locks and one million unlocks and relatively short work inside it so the the overhead of allocation can be really large so what would the alternative be malloc that can allocate variable number of blocks so this is a malloc which is over loaded and it has the size but also has a count and then it returns the chained blocks pointed pointers as blocks and we can also have a free function that can take this chain version chain blocks of memory returned by malloc so performance benefits in these games there are several of them first the allocator can organize the memory better if it it knows it will give out one million blocks so this is one important consideration if the allocator knows it will be giving one million blocks then it can allocate from a dedicated memory pool which has additional good implications for performance there is decrease in overheating function call decrease overhead of multi-threading synchronization and improved instruction caches so we talked about this a little bit later okay questions not so far okay not so far here not so far moving on to the next topic related to architectural issues that can cause performance problems is data copying and data conversions so as your the complexity of your system grows the the need for data copying or the conversion increases and but the problem with both of them is neither data copying or data conversion to any useful work so data the essence of computing is processing data moving from one form or of or to another and data copying the data conversion just change the memory layer but they don't essentially modify the data and with your component design you should avoid them as much as possible um sometimes data copying it cannot be avoided i mean if you have a component which you have a component which has a method called get bucket you need to get bucket and then you need to write to it uh so if you want to communicate to other component that produces output on its it produces output in its own buffer and you need to do some kind of data copying additionally if you have uh two components which weren't designed with a similar intention in mind you will have to have some kind of data conversion in order for them to to talk and these things can be avoided with clever design so the data format should be agreed before either of components is designed so one of the problems with component design often is that the people think about the api but they don't think about the data they're their component is processing the first thing you should try to specify is the data what it comes in and what comes out of the component and when you have that in place the other teams can look this up and you can create a good system but if you first design the api and you get the data if you don't get the data right you get this problem of data copying the data conversion also if you're using external library as a component you can use the data format expected by those libraries that you're using okay question about this no questions yet okay moving on to contention so what is contention resource contention contention generally means getting stuck in line and waiting for something to happen so there is a resource context contention when you are waiting on a resource now this resource can be anything i mean for in the computing system can be cpu memory can be the disk it can be the network data from the network in the world of components contention often happens on waiting to enter the critical section so this is where these getting stuck happens so this is where where the components get stuck their weight try to get the the into the critical section but it doesn't work now if you look at one mutex and you look at its utilization percentage how much of the time is it locked if the utilization is high like it's blocked 90 percent or 95 of the time then the weighted time times tend to explode they they grow uh exponentially so there is a mathematical explanation for this called king man's formula and this is how it looks on an example system it's not uh it's not the it's not the this diagram is from from productions so from factory production so it's not from the development but the line looks exactly the same so what happens when the utilization of your mutex is low like 20 30 40 percent then essentially no component is waiting to enter the critical section but as soon as the utilization grows above like 70 80 90 95 percent then the the waiting time just go goes mad so it really jumps uh over the the over the roof now uh the shape of this curve exact shape of the this curve depends on several parameters but this is the essentially this is the shape it can it can start to to rise up a bit earlier or a bit later but essentially this is the shape uh when the the components when the mutex is hundred percent it's it's luck hundred percent of the time the waiting time becomes infinite so uh example of contention that you will often see in your system is the logger so the logger is the components which writes logging data to a file and many components other components are using to logger to put logging data into files so the logger works with one file so it has to have a critical section protected by a mutex now what's problem with the with this design so when the utilization is large so many components are waiting to put data into the logger to write data on the disk and the system the program the system becomes much slower and it's getting slower as as as as the number of components increases the work of the lava has done it can really really become become the bottleneck and there is a domino effect here so uh components few components are waiting for the logger to complete the other component components are waiting for this component to complete now the whole system is stuck stuck and basically everything everything depends on the the the amount of time the the speedy input output of the input out of the disk so the program will work better if you buy a faster disk now this effect described by kingman's formula can be seen everywhere so devops people who work with the development with servers if the server is overloaded that means it will stop being responsive so if you have that the disk usage on a server is 95 that this can that server cannot process any more requests happens on the database if the database is a component either system is more than 90 or 95 busy to 99 percent of the time busy that means that it will your system will wake on the database that happens in the bank if you go to the bank if the bank teller is working at if his shift is eight hours a day and he's working eight hours hours a day that means there is a huge line waiting for him so he at no point is he free also you see that in an overloaded highway so in the highway if there are many traffic it gets slower and slower slower okay questions about contention yes we have one we have one questions from jonas he's just is asking what's your preferred approach to measure mutex utilization uh what is my preferred uh so i don't do that normally i don't optimize that much in the architectural level so i don't have a clear preference so there is uh if you if you are if you are trying to investigate contention if you want to find which mutexes are locked but it doesn't measure utilization rate of the mutex is it imagine how many times it got locked so you can see that there is in the perf tool there is a event for that and you can set up counters with that uh and then you can see how that happens okay so the perfect the tool on linux that i use for that but it doesn't give you utilization rate of the mutations this gives you how many times this need to get locked and if you see there are like million uh logs to the mutex but nine nine hundred thousand happen on this particular lock you'll see that lock is like a hot lock something is going on here other questions not so far not so far okay thank you thank you uh so we're done with architectural issues now the problem with jackie when i've got this article normally i don't do that architecture and this high level architecture anymore so uh i guess the people also have some experiences related to architectural patterns that that create code that is slow so after the meeting if you have some time if you have some experience to share with me because i have these cells as an article and i would like to keep it up to date okay so next types of issues are on the rhythmic issues and algorithmic issues are issues especially algorithm uh algorithm centered so the algorithmic issues appear because the data set size grows the amount of the the data that the system has to process it grows so if you profile an application and providing means to determine which functions of each loops or which functions take the most time if you have a small data set you will see many loops with different complexities in performance profiles some loops can have oh fan complexity some caps have all on analog n some can have complexity of o of n square but when the large with the large data set the the the the loops with highest data complexity you will see that the in the data in the in the in the performance profile only the loop with high data complexity so in this case the loops that are previous oh final or and or find log n will disappear completely in your left arm with only with one square and this the most algorithmically most complex loops eats up all the runtime programs runtime so what is the general conclusion is uh that programs with complexity that is larger than and logan do not scale well if you have a if you have uh uh if you have a let's take example on image processing if your cpu and you have an embedded cpu and your cpu is doing well to process image which has a dimension 1 000 times 1 000 pixels now if you increase the dimension the size of the image by 2 so 2000 by 2 times 1 000 pixels then in that case the the the loop has to the cpu has to be four times faster and if you did it three times then the cpu has to be nine times faster so at one point the the software stop scaling if your if your program has a hot glue which which has a analog n higher than logan it will stop scaling um [Music] yes uh okay that's all about algorithm algorithmic issues so if you have o and twos loop and a large data set either you need to find a way to decrease the complexity of the algorithm or you need to move to some kind of high performance computing or with gpu computing that can handle this this kind of stuff questions no no no no questions no questions so far so i think most people were experienced about the algorithmic and it had some sense he had some natural understanding of algorithmic issues and the architectural decision of going a bit deeper now we are talking about memory allocation so what about memory allocation system allocator in your program is a shared resource so it's a shared component so implementations of malloc free new and delete go through the system allocated and imagine you add to your program a new component that uses the system allocator a lot so you have a new component that allocates many blocks of memory so what happens the data fragmentation rate memory fragmentation rate for the data increases for the all components not only the new code for everyone and increased data fragmentation memory fragmentation means lower performance also the data cache hit rate also decreases so data cache heat rate is really important we talk about that a bit later but having a high data cache hit rate means that you have good performance and as a result all components that rely on heavily on the system allocator get slower so this doesn't happen if you're processing your if your component is processing data in an array this you don't have problems with memory allocation but if you have a component that uses binary trees linked list based data structure some hash maps all types of pointers it will generally create a lot of cost to the system allocator and actually has a tendency to slow down slow down the performance in other components of your system so there are a few mitigation strategies so the system allocator the implementation of malaquin free is not something there are several other implementations which are not from the glyph c on linux so there is the google has one implementation of the system allocated microsoft has another facebook has a third one all of them are in open source and typically they are much faster than the building allocator because they do some some stuff differently so one way is to use a different system allocator and often the programs that are with their memory bound that have problems with memory the memories the bottleneck really benefit from changing the system allocator you can get like 10 or 15 increase in speed the second mitigation strategy is to have per component allocator so each component allocates its memory from a dedicated block so there is no mixing of memory between different components now this is also a good approach it increases memory consumption a bit but uh generally uh with this approach you would get the benefit of good you won't have problems with memory fragmentation you won't have problems with increasing data cache miss rate so everything should work faster okay questions no we don't have questions so far okay next thing that can slow down your program is our compiler optimizations now how do compilers optimization what's the problem there so the problem is that compiler optimizations are fragile so to optimize your code to some perfect form the compilers rely on pattern matching and heuristics so and these things break so pattern matching means sometimes the pattern will not recognize then the compiler needs to emit an efficient code if some heuristics finds that the certain optimization is not worth the effort it can de-optimize that code and make it slower so the two classical examples of of of compiler optimization that can break the performance of your code is vectorization and inlining so first about inlining function inlining meaning instead of having a function call you take the function body and copy it instead of the call now inlining itself is not that important by itself but within lining this opens the door for many compiler optimizations without inlining just function calls are really zero optimization possible for the compiler and in some if at some point the the compiler decides not to inline a certain function you can see a sharp decrease in performance inlining is one thing the second thing is vectorization so a vectorization is another compiler optimization technique it relies on vector vector instructions in the cpu cpu has some special vector instruction that can process more than one piece of data in a single structure for example you can process four integers or four doubles now for the compiler to emit vector instruction in this process called vectorization there are certain uh certain preconditions that needs to be met and sometimes you can add just one bad line and you can break vectorization and when that happened it happens you see a sharp decrease in performance a code that is well optimized might get easily optimized which you can add just one additional light and the loop hot looping record can get de-vectorized or one additional line in your function and that function becomes too large and stop stops being inlined so also sometimes when you change the compiler so you're going from program to for example from gcc to client or you upgrade the compiler version from gcc 7 to gcc 11 sometimes this also makes optimizations now unfortunately there is no generic solution to that but most of the time writing simply easy to maintain code helps because the compilers are generally well adapted for analyzing that that kind of code okay questions no no we don't have questions so far okay uh i hope it's because uh the topic is interesting and not boring okay moving on to hardware issues now hardware issues so they're not issues in hard work of their issues in software but they happen because the software grows so what happens is if when your program becomes larger or your data set becomes larger somehow it becomes less harder friendly so what happens there are three things that happen three let's say big things that happen larger program means more instruction cache misses we talked about that a bit later larger data sets means more data cache misses we talk about it a bit later also and this is a failure to use the cpus vectorization unit this is closely correlated to what i was saying in the previous slide so if the compiler doesn't emit vector instructions it emits standard scale or slow scalar instructions the cpu vectorization units are idle and this code will run slower and you can try like two or three or four times slower it's not like 10 percent slower so when you add new statements to your loops it gets they get more complicated compiler has problems recognizing uh the compiler has problem doing the the the optimization and this code ends being slower okay now question uh next topic is if you have more code that means you have more instruction cache misses now what are what is an instruction cache instruction cache is a special memory inside the cpu so memory and cpu are separate cpus one cheat memory is another chip and there is a bus connecting them now inside the cpu there is another memory called instruction cache this memory is really small like 32 kilobytes or 64 kilobytes and this memory is used to speed up access to instructions so instructions are stored in the main memory and instruction case can speed up access to the instructions so how does instruction cache works if the instructions you are trying to execute is already instruction cache you can get fast access but if instruction is not the in the instruction cache that means it needs to be fetched from the main memory and this is slow access you need to wait for the instructions the cpu cannot execute destruction it needs to wait if the instruction has not been used by the cpu for a long time it will get removed from the cache it it goes out so the the instruction cache is 32 let's say that the instruction cache is 32 kilobytes in size and your program is 16 kilobytes a small program so what happens it fits completely the instruction cache so if it's running long enough time the whole program is in instruction cache and that means that that means that there is no delays in the coding instructions but when your program becomes bigger and bigger and bigger and bigger and bigger then your instruction cache doesn't hold the whole program it just calls some parts of the program that are currently being executed and the instruction caches uh instruction cache misses start to appear the bigger the program the you will you will expect more instruction cache meshes so some programs suffer more from this issue so if your program is moving quickly from one function to another function to another function so you have loop that goes through several functions and goes back these programs will typically experience more instruction cache missions if you have like a small look where everything happens inside a block of kind of instructions you don't have problems with instruction cache masses so are there any ways to ways to mitigate these problems so there are there are three ways generally so the first one is called bolt bolt is a special tool i think it's facebook so what does it do you run your program and this with a profiler and profiler collects information about which function called which function how much time did each function execute so it tries to create like a call graph and it tries to create uh to establish which functions are hot and which are not so both take this information as an input and then changes the placement of functions inside your binary so it puts those functions that's called one another close to one another in memory and this thing decreases the data cache misses so facebook people claim that this gives them like 10 performance on their servers next thing is the profile profile guided optimization so profile guided optimization is another compiler technique and this technique is used to um [Music] it works similarly as bold so you first you collect data about functions call graphs and so on but and then you feed this information to the profile to the compiler now the compiler with this information compiler can also optimize the memory layout of functions of functions in order to avoid instruction cache misses but it can also apply aggressive optimizations to hot code and leave most of the a called code that is not exited often it can leave them to be short and to be short but unoptimized because it doesn't influence program performance almost at all so that's another way to do it the third one is called the link time optimization they also decrease instruction cache decrease instruction cache miss rate how they do it they allow link time optimization allow the compiler to inline functions from another c file if you have two c files generally they're individual compilation units but with link time optimization you can inline the function from file a into the into the function in file b okay questions no no questions okay checking now okay next thing is uh why do programs get slower which is a harder issue is that with larger workloads with larger data sets there are more data cache misses so what is a data cache a data cache is a small memory on the cpu very similar to instruction cache that it uses to speed up access to commonly used data so instruction cache speed up speeds up access to instructions data cache speeds up success to data if the data is the in the data care piece of data is in the data cache you get a fast access you see if it's not then you need to get data from the main memory the axis is slow so there is a problem with the random access data structures so these are trees cache maps linked lists but not arrays arrays of vectors are not random access they can be random access you're accessing randomly but if you go from left to right or after right left you're not texting them randomly so there is a pattern and the cpu can take advantages the problem with the random access data structures trees hash maps links list so how does it look like in the graph how does it look like in the graph larger work cloud means more data caches so we did a measurement uh to see the difference so look up in in a small data structure is faster than lookup in a large data lash one and we measured the time needed to do eight million searches inside a hash map okay now we do 8 million searches so we would expect that the runtime of the program doesn't depend on the cash map size it doesn't matter if the cash money has 64 entries or 1 million entries or 4 million entries but if you look at this graph here you see that the runtime is about 0.1 0.2 for 64 entries and then for 260k entries it goes higher to hundred milliseconds and with 16 million entries it goes hard to one second and with 64 million it goes even higher you see that the even though we're doing it million searches the runtime depends on the hashmap size uh so why this does this happen this happens because again the the the hash map the data from the hash file doesn't fit the data cache so there is no reuse of data you accidentally for the large data hash map you access data once and then it gets evicted from the cache because some other data is needed and when you're acting the same data again it's not in the data cache so the performance suffer there so are there any solutions or strategies to work around this so there are no general solutions you always you will have the this this this uh function this this numbers will always look like something like that but there is also ways to do is that you can get these numbers to be smaller so instead of having 1.5 seconds with this hashmap from the stl there you can you get 0.7 with another cache map this is more data cache friendly so what are the mitigations if you're using hash maps there are open addressing it has open addressing hash maps that are more data cache friendly although they're not they also have their own problems so they are not like uh out of the box replacement solution it doesn't work like that you need to test them next you have instead of binary trees you have energy trees so a tree where inside a single node you don't have pointers to two but you can have pointers to three or four or five so and inside a single node of the tree you hold two or three or four values so this generally makes the increases data cache hit rate and also you can use binary trees with a good memory layout so there are several memory layouts and then and i wrote about them in my blog so i didn't write them it has name of some scientists but i write down i can look it up later if you need it but if you optimize for memory layout you can generally get better numbers for the same data structure size okay questions about uh data cache misses and data set size yes we have two questions okay one is the first is does a vector slow down as much as a hash map with increasing size i imagine it might be faster due to less fragmented data uh it's yeah so yes you're on the right track the the the the the person asking questions is on the right track so all these stocks about data cache misses applied to hash sets so for vector if you are going on a vector and you're going from zero to n and it doesn't matter if vector is one megabyte or 100 megabytes the time you need to go through that vector depends on the vector size so it doesn't it's not slower because you the large the data structure is larger okay second one is from sasha uh well okay at the end of the actually this is at the end of the talk we have a note uh so if you want to take this at the end sasha recommends to to take that sorry i i haven't read it before so maybe we can take it to the end maybe we can take it okay so we're almost at the end so um so we're almost there so this is the last slide so the last problem is with the problem of processing large classes so you have structures with class it doesn't matter that they're large have many data members the larger the class that means more data cache misses now why does that happen so this this problem is not related to the random access data structure this is problem that happens with larger classes if you keep them in array you'll have problems the bigger the data the class size discover the processing so what happens is that the data fetched from the memory to the cache instruction data cache is in blocks you don't go to the level just to pick one byte you go there and you pick a block of 64 bytes and this whole block is switched to the to the data cache so what you have if a large data class if you have a large class but you're accessing only two or three data two or three members they have like class with 10 members you're asking only to a three you're bringing from the main memory to the data cache the data that cpu is not using and memory bottleneck is in most systems the memory bottlenecks is in the memory not in the cpu and when you do this when there's like when you bring this bring these large large classes to the to the data cache um this increases data cache miss rate and directly decreases performance so here's an example that we measured how much time do we need to process 20 million 20 million elements from the rate so this is some some function called calculate surface and this is its runtime so we created the test class which has variable size and the size of the class goes from 20 bytes to 504 bytes and we're doing the same kind of processing the function doesn't change anything inside we're just adding some padding data there padding data means that the means that uh this padding is unused but it will be brought from the memory to the data cache so these are the padding bytes actually the the members of the cuts that you're not using in your hot function so what happens runtime in the class is smallest 20 20 bytes is best it's about 20 milliseconds or 30 milliseconds and now it starts to grow and at one point it goes to 130 milliseconds so the difference between the class size of 20 bytes and class size 184 bytes is a substantial it's like four or five times slower so you see a large slowdown as as as the the system is the class size is bigger or to put in another way it's not that the class size is bigger is that you're not using the all the members of the class if you're using all the members of the class then you have them you don't have any problems so what are the mitigations to this problem so mitigations is one of the way to work around it is to decompose large classes into smaller classes so this is a general approach if you have a hot function and it's processing classes and the classes are large if you remove unused member you'll get speed improvement we guaranteed so the second way to do it is called entity component system and it's just going additional it's decomposing large classes into smaller places in a different way so this is a paradigm like object-oriented paradigm this is another paradigm and is uses is used in c plus plus so plus plus supports this paradigm and it's used by game developers because they are after speed they want the games to run fast they don't want to bring unused data from the memory to the data cache but this directly influences the performance of the game so entity component system is about different way to think about problems in different ways how to decompose problems into classes but as a result you'll get smaller classes and this will have positive impact on performance that's it we are at the end questions yeah yeah yeah yeah thanks for for the session we have a lot lots of questions so far so okay first of all since we are at the end we can take finally sasha's questions about uh any recommendation book website or other resources uh for digging into the optimization topic and also so i do i write a lot about uh i have a lot of a lot of myself i write a lot about performance so this is the site johnny softer lab and they're like i think now there are about 20 or 25 posts that are specifically talking about all sorts of optimizations c plus plus low level optimization high level optimizations compiler operating system and so on so there's a lot of interesting material there but there is also one free book by dennis bakula you can download it from this site or you can you can you can download it from his site or you can buy it on amazon the book is free but if you buy it you can get to keep the copy and it helps the author so that's the the second book this book is most about it's called performance analysis and tuning on modern cpus and it's mostly about low level optimization how to better use the hardware and there is also a third book it's called high performance plus plus so this book is by vic ferzer and bjorn andrist so high performance zippers plus high performance and it's a really good book so i have these two books in my in my in my library that i use for foreign thanks for the recommendation um okay we have other questions one is from jacob what's the difference between allocating a continuous array of one million objects and the alternative monologue you presented so i don't understand what he means what is the difference between uh if you're always getting a vector of one million elements you get one block of memory and that's completely fine it's not a big problem but if you're allocating a binary tree with one million objects then you get one million calls to the one million calls to the system allocator and that is the problem that increases the data fragmentation that decrease the data cache the data cache data cache hit rate and there is a lot of space to improve on this so if you're good with memory layout i talked about that in my blog if you're good with memory layout you can get very decent speed improvement okay okay another one is from matthias does memory alignment play a large role uh but not anymore so it used to play a memory alignment used to play a large role in speed performance but not with modern cpus so with modern cpus generally you you i've you cannot get like you can get maybe with proper memory alignment 10 or 20 speed improvement but you cannot get like two or three times so there is some speed improvement if the data if you're using a line instruction then you get a guarantee if you're using underline instructions on intel cpus then then if the data is itself aligned but the instructions are online they work exactly the same as the aligned instructions if the data if the date is not aligned then you get some performance under some conditions you can get like performance glitches but nothing's nothing spectacular okay okay another one from jacob is is stuck data generally more often in the cache than heap because surrounding variables are often accessed as well yes so stack stack data and allocating the stack it's faster for several reasons so one of the thing why stack is faster is because the allocation on stack is just increasing one counter so it's one instruction whereas in the allocate in the dynamically memory you have to go and look for the block of appropriate size and then bring it back so that's one thing and the second thing is the stack is always used the top of the stack is always in the data cache because these variables are used often and they're keep they're reused often the same data is reused you leave the function you enter another function and this is belongs to the same piece of memory so this memory is almost always in cache okay okay then we have another one it's regarding memory optimizations what are your thoughts regarding optimizing data structure layout for better caching for example putting hot data first minimizing padding etc etc so yeah yeah these things are actually used a lot in those industries but when there is a performance performance is really important like games or like high frequency trading or or maybe some data processing also they they manipulate the memory layout in order to get better performance because manipulating memory layout actually increases data cache hit rate with binary trees with hash maps with linked lists everywhere and it can have a dramatic it's not a small sometimes the increase can be dramatic okay okay then we have other two okay this is the most upvoted is in your experience with your clients which of the mentioned optimization areas seem to be the most relevant so it really depends on the software it depends on the software basically you have software you can divide software into two into two into two groups one is the cpu bound software where the bottleneck is inside the cpu and the other one is the memory bottleneck so to where the problem is in the memory so the the bandwidth between cpu and memory is not large enough the memory is not fast enough it doesn't respond fast enough now in general object oriented paradigm you most of the time get the memory limited software you don't do a lot of computation but you do a lot of fetching data from memory fetching classes from memory on the other hand scientific computing image processing video casting audio processing they all work on the data which is in vectors or arrays and they use simple data types and this kind of processing is mostly the bottomless bottleneck is mostly in the cpu okay okay then we have one last i think it's not really a questions it's mostly an observation and probably requires your comment about this is david says i know with cuda programming we group threads into blocks to act on a block of memory it seems like we could use the same model with the cpu and arrange our data in vector blocks that match the size of the cpu data cache and then our code would act on one block at a time yes so what you are saying basically is exactly the the although cpus and gpus are different things the the optimal memory layout optimization that work on cpu also work on gpu on gpus the same the same patterns vector are restoring data and arrays or vectors and accessing each element of the array is better that that's true also for gpus and for cpus i don't know so i'm i'm not an expert in gpus i just have some knowledge i don't know about the random access data structure is it faster there or not but on cpus memory memory all these attempts to to use reuse the data from the data cache actually help the software performance okay nice uh we don't we don't have any other questions so far uh we still have a few minutes so folks if you have any other questions for image please use the q a tab in the meantime we have a comment in the chat great talk now my brain is full thank you and uh also david saying thanks and all the other people i think you'll be available in in the lunch don't you yeah i'll be available for in the next if if anybody comes in the next 10 minutes i'll be there but if nobody comes then i'll leave in 10 minutes okay okay okay thank you very much hey thanks thanks and see you around thanks a lot for also for attending bye