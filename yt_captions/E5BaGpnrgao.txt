all right all right people I will do this generative AI video I know I keep getting asked to do the generative AI video you guys keep telling me to do it we'll do it so we looked at cliping beddings right and we've talked a lot about using generative AI to produce new sentences to produce new images and so on and so to understand images all these kind of different things and the idea was that if we I I I don't know what we're talking about I feel like I just came into the middle of a video I I must have missed the previous context for this but okay we'll figure it out we look at enough pairs of images and text we will learn to distill what it is in an image into that kind of language so the idea is you have an image you have some texts and you can find a representation where they're both the same the argument has gone that it's only a matter of time before we have so many images that we train this on and so and such a big Network and all this kind of business that we get this kind of general intelligence or we get some kind of extremely effective AI that works across all domains right that's the implication right the argument is and you see a lot so this just how he's just explaining the fundamental basis of an llm right effectively he's what he's saying is that once you get enough you can generalize over any data set this is just MLPs driven out to The Logical fullest extent right in the sort of tech sector from the from some of these sort of um big Tech he's not explaining LMS he's explaining neuron Nets yeah yeah yeah so so just an MLP right companies who to be fair want to sell products right that if you just keep adding more and more data or bigger and bigger models or a combination of both ultimately you will move Beyond just recognizing cats and you'll be able to do anything right that's the idea you show enough cats and dogs and eventually the elephant just is implied there someone who works inside I mean that's I mean that's the problem about extrapolation extrapolation is is really really hard I don't I don't know I don't know about that I don't I don't know about that business we don't hypothesize about what happens we experimentally justify it right so I would say if you're going to if you're going to say to me that the only upward trajectory is is going to you know the only trajectory is up it's going to be amazing I would say go on then prove it and do it right and then we'll see we'll sit here for a couple of years and we'll see what happens but in the meantime let's look at this paper right which came out just recently this paper by the way he is correct uh in the sense that um extrapolated banam uh something that's really important to always remember is that just because there is a large change in a short period of time there is no reasonable evidence to believe that a large set of change will happen again in a short period of time there's only so much of that that can possibly happen and so to always like you know you always hear this phrase that right now today is the worst version of AI that will ever exist you're you're absolutely right I'm sure they'll iterate and get it better than it is today but the problem is is often people paint that in their head from the time in which there was no co-pilot to this new magical tokenizing guessing machine that gives you out uh something that's really amazing and you're just like holy cow it like guessed what I wanted into again that level of transition I don't know if that next level of transition is going to happen again like from nothing to something was huge from something to the next level I don't know if it's going to go on this is toddler level Ai and and and we may not get better than toddler level AI under this current iteration under this current uh you know local Optima like this may be this may be as good as it gets is saying that that is not true right this paper is saying that the amount of data you will need to get that kind of General zero shot performance that is to say performance on new tasks that you've never seen is going to be astronomically vast to the point where we cannot do it right that's not only that a really good quote from CS Lewis there's this there's this CS Lewis quote out of out of the Silent Planet I know I've mentioned this one specifically a few times where ransum lands on Mars and he has no idea where he's at and he looks out at the Seas the mountains and the trees and because it's something new to him he's never seen before he is unable to actually see the thing like even us in our much more advanced amazing human brains we can't even see the forest or the trees on this new world because and I'm sure you've experienced this before have you ever been to a new place where when you look around it just looks different and then as you spend time there it's almost as if things get smaller and they they they get you all a sudden you see them for what they are versus this first initial um like experience and so there's something very very true to that to where given an AI a brand new experience is it able to actually you know is it able to actually do it is a human actually even able to do it it's very it's a very weird experience if you've ever been to uh like a if you've ever seen something brand new it's like you can't even see it like you don't even have the ability to filter it this is a a reason why exploring is fun exactly right that's the idea so it basically is arguing against the idea that we can just add more data and more models and we we'll solve it right now this is only one paper and of course you know your mileage may vary if you have a bigger GPU than these people so on but I think that this is actual numbers right which is what I like because I want to see tables of data that show a trend actually happening or not happening I think that's much more interesting than someone's blog post that says I think this is going what's going to happen so let's talk about what this paper does and why it's interesting we have cliping beddings right so we have an image we have a big Vision Transformer and we have a big text encoder which is another Transformer bit like the sort of thing you would see in a large language model right which takes text strings my text string today and we have some shared embedded space and that embedded space is just a numerical fingerprint for the meaning in these two items and they're trained remember across many many images such that when you put the same image and the text that describes that image in you get something in the middle that matches and the idea then is you can use that for other tasks like you can use that for classification you can use it for image recall if you use a streaming service like Spotify or Netflix right they have this thing called Netflix mention let's go let's go recommender system a recommended system is where you've watched this program this program this program what should you watch next right and you you might have noticed that your mileage may vary on how effective that is but actually I think they're pretty what they have to do it is quite crazy what actually what actually happens there's there's so much to that specific part of the business uh man it is it is absolutely wild how much goes into it and to just gloss over that it's just it's it's banam what what what is happening inside these engines but you could use this for a recommender system because you could say basically what programs have I got that embed into the same space of all the things I just watched and and recommend them that way so there are Downstream tasks like classification and recommendations that we could use based on a system like this yeah what this paper is showing is that you cannot apply these effectively these Downstream tasks for difficult problems without massive amounts of data to back it up right and so and the idea that you can apply you know this kind of classification on hard things so not just cats and dogs but specific cats and specific dogs or subspecies of tree right or difficult problems where the the answer is more difficult than just the broad category that there isn't enough data on those things to train these models in an effective way I've got one of those apps that tells you what specific species a tree is so is it not just similar to that no because they're just doing classification right or some isn't that I mean is part of this what Capt is is this what captures have always been just da data aggregation just getting us to classify for them as proof that you're a human isn't that what it's always I mean isn't that I know that when Google used to do Google Books I went interviewed at the Google office in Boston or in Cambridge Cambridge and uh during the Cambridge time they had Google B books still in there and what they would do is they'd actually chop off the spines and then manually uh you know uh what's it called scan every single page and then all the ones they couldn't parse out with a high degree confidence they'd actually put into captas and then have the captas solve it for you like that was one of the strategies back in the day was Google capture uh figure ing out banam is is this banam or is this banana the human computation Google tech talk yeah it's it's pretty smart it's pretty yeah that's history that was that was real real history happened there a problem they're not using this kind of generative giant AI right the argument has been why do that silly little problem where you can do a general problem and solve all your problems right and the response is cuz it didn't work right that's that's that's that's why we're doing it um so there are pros and cons for both right I'm not going to say that no generative AI is useful or no or these these models incredibly effective for what they do but I'm perhaps suggesting that it may not be reasonable to expect them to do very difficult medical diagnosis because you haven't got the data set to back that up right so how does this pay the rare event problem uh so you can have a really large system that how do you know that you've released a canary that is actually better than the software that you're currently using and this comes down to something called the rare event problem where you have an event that happens only one out of every 10 million requests like how do you know that you have an increased or decreased that problem it's very difficult you don't know what you don't know it's it's very it's it's it's hard uh is that the peaking problem data is always collected and is always getting more detailed I don't know if this is the peing problem um I mean the problem is is that that what you need to describe everything becomes exponentially larger and so you have to continuously have more and more data to get more and more precise and each level Precision is in some like linear amount paper do this well what they do is they def they Define these Core Concepts right so some of the concepts are going to be simple ones like a cat or a person some of them are going to be slightly more difficult like a specific species of cat or a specific disease in an image or something like this and they they come up with about 4,000 different concepts right and these are simple text Concepts right these are not complicated philosophical ideas Cal down I don't know how well it embeds those and and what they do is they look at the prevalence of these Concepts in these data sets and then they should they they test how well the downstream task of let's say one zero shot classification or recall the recommended systems works on all of these different concepts and they plot that against the amount of data that they had for that specific concept right so let's draw a graph and that will me make it more clear right so let's imagine we have a graph here like this and this is the number so they're just saying the more data it has the better it's going to get sort of it just goes like this though some sort of a log curve of examples in our training set of a specific concept right so let's say a cat a dog something more difficult and this is the performance on the actual task of let's say recommend a system or recall of an object or the ability to actually classify as a cat right remember we talked about how you could use this for zero shot classification by just seeing if it embeds to the same place as a picture of a cat the text a picture of a cat that kind of process so this is performance right the best case scenario if you want to have an all powerful AI that can solve all the world's problems is that this line goes very steeply upwards right this is the exciting case it goes like like this right that's the exciting case this is the kind of AI explosion argument that basically says we're on the cusp of something that's about to happen whatever that may be we are 11 months into 6 months away from AI stealing all of your programming jobs we're starting to get about a month into this tweet okay we're on the cus people it's it's like right there it's like this close we we're going to have it it's almost there they took our JBS it's about to happen at any second Elon implied but it's joke right jam on please Devon is not even public it you don't even know I mean that's what I hear with these things is that we're we're on the of this great giant change and I think there's going to be a lot of great change like I'm not some AI Doomer right I like co-pilot I don't use it anymore but I like co-pilot you know I think some people think I'm a Doomer that that I I think that it's all it's all bad I think there's plenty of great I like the idea of it but I often find that I just find it annoying the problem is is that I'm so much better than it so it's like why do I want to use something that I'm so much better than do I really like do I really want to spend my days trying to control uh an intern that's not good at it that only does I mean the problem is is when I'm not good at something co-pilot allows me to write code at a very fast rate using my assumptions and mirroring kind of my coding technique for the for some amount and steers me into my already bad idea at a rate that's so fast where the scale is going to be such that this can just do anything so I don't really like then there the perhaps slightly more reasonable should we say pragmatic interpretation which is like just call it balanced right which is but it's a sort of linear movement right so the idea is that we have to add a lot of by the way there's no technology that's like linear movement you know the speed of computers are not they have not linearly moved Tech does not linearly move it it's like an inverse tangent it goes slow it goes fast it goes slow examples but we are going to get a decent performance on SC keep adding examples will keep getting better and that's going to be great and remember that if we ended up up here we have something that take any image and tell you exactly what's in it under any circumstance right that's that's kind of what we're aiming for and similarly for large language models this would be something that could write with Incredible accuracy on lots of different topics or for image generation it would be something that could take your prompt and generate a photorealistic image of that with almost no coercion at all that's kind of the goal this paper has done a lot of experiments on a lot of these Concepts across by the way those were very different answers which is kind of surprising because the difference between generating an image from a prompt versus generating a problem solve from a prompt are very very different and I I think that's something that people don't talk about a lot because when it comes to an image of anything whatever you want I want a star and I want the star to be very pretty Bing you know everyone's all very happy about the star like the amount of stars that could possibly exist within this thing is just gigantic right there's just so many possibilities but if I say I want you to sum numbers there's there's there there's not that right there there's not that many possibilities there's like a reduce or a fold there's like a boomer Loop right there's a boomer Loop and then what else maybe recursion that's really it the amount of possibilities are very very very very tiny because there's an exactness to this answer there is an a level of impec impreciseness that makes this thing work out and so you find that these ones you always get the right answer in this one you don't get as many right answers a boomer Loop it's just my that's just that's literally just a for Loop 4 I equals 0 up to 100 right uh Boomer Loop Boomer Loop detected you have to use I right um that's that's the problem is that this is a precise answer there is not any variation of being incorrect on here and so this makes it a fundamentally hard problem you can be incorrect in all sorts of reasons here and this works right this is okay you can be in precise all over the place and it's perfectly fine this part of our visual inspection right it's okay to be incorrect my wife is in the uh uh paramedic school and she's absolutely certain that chat chippity is going to kill uh people because because Precision yes Precision is very very very very is is a really huge problem and because these two are fundamentally different problems they're like so different that it's it's like they're world's away right they're the world they're they're literally two separate problem islands and you can take this one you can take this one and you can move it into this one really easily and how you move it from here into here I did almost creade boobas at this point is very very simple the all-h canvas problem right tell it to generate a problem that's all white all black all whatever right and you will actually you you will bamboozle it because the problem went from any variation of these are correct two there is a specific precise answer yes I sorry I had nipple ring Bobs on there for a second a lot of models across a lot of Downstream tasks and let's call this the evidence what you're going to call it pessimistic now that it is pessimistic also right it's logarithmic so it basically goes like this right flattens out it flattens out now this is just one has anyone ever trained handwriting uh so one of my first requirements when I was getting my masters in AI was uh training handwriting so we'd receive a handwriting of data and along with its classification and then a second set of data that is just handwriting and we would go out there and you'd have to like learn how to train it right and so yes the minst data is that is that what it is is that is that the classic one the 10,000 points of handwriting data classic minc yeah okay yeah yeah so I we did we did RBF and we did uh MLPs and RBF did really really really really well because RBF was interpolating between a known data set whereas uh the MLPs you had the you had the train a lot longer and eventually if you train it enough it would do a pretty good job and so there it was kind of this like really weird experience where you realize that okay I've trained it on this thing and now I'm going to give a new piece of data that's just slightly different and all of a sudden it just completely like it cannot classify it at all and this was really my understanding of AI at that point which was the amount of training you have to put into it for it to be able to even interpolate between two of the same uh sets becomes huge RBS stands for radial basis function it does not mean AR resting [ __ ] face uh generalization is ex like that level is extremely difficult right uh there's also mind mins which uh is some uh which is someone's brain waves while looking at mints oh really that's pretty deep it's kind of cool it was a very it was a very cool experience I I absolutely loved that on paper right it doesn't necess that's by the way this is why I think llms are so incredible is that it can solve really hard programming problems this is one of my favorite ways someone describe it to me you don't use chat GPT or these new llms their power is not in solving easy to program problems make me a UI let's make this button let's call this function let's make a rest endpoint those are easy to program problems which are actually somehow excessively difficult for a GPT but let's solve difficult problems is this a beer bottle is this a shot is this a shot bottle yes or no now an l might be able to look at this and go no it's not it's actually a vitamin it's a B12 bottle right they might be able to say it's something different so gypp actually has the power to be able to see these kind of things much much different than us and that's like a super hard problem to program where as is it's not a hard problem problem to to jity on and so that's a shiv right hot dog or not hot dog and so that's kind of one of these incredible kind of experiences oh what's that my my wife gave me some grass-fed beef jerky mango habanero let me try it let me give it let me give this trim what a nice wife flip take it out flip take it out necessarily mean that it will always flatten out but the argument is I think that and it's not an argument they necessarily make in in the paper but you know the paper's very reasonable I'm being a bit more Cavalier with my wording the suggestion is that you can keep adding more examples you can keep making your models bigger but we are soon about to hit a plateau where we don't get any better and it's costing you millions and millions of dollars to train this at what point do you go well that's probably about as good as we're going to get for technology right and then the argument goes we need something else we need something in the Transformer or some other way of representing data or some other machine learning strategy or some other strategy that's better than this in the long term if we want to have this line G here or this line gar here that's that kind of the argument and so this is essentially evidence I would argue against the kind of explosion you know possibility of of that just you just add a bit more data and we're on the cusp of something we might come back here in a couple of years you know if you're still allowing me on computer file after this absolute embarrassment of of these claims that I made um and we say okay actually the performances improve improved massively right or we might say we've done I would bet that it's not going to improve massively I would be shocked honestly I'd be so shocked if it improved Mass massively double the number of data sets to 10 billion images and we've got 1% more right I I'm I'm more Curious how this is going to go because you know one thing that was very interesting is that Sam Alman made some sort of Claim about the difference between jippy 4 and jippy 5 and how he's pretty sure jippy 5 is going to be better than jipp 4 like that wasn't necessarily like a like a winning endorsement I I think I think we're already hitting some of those things cuz I I mean we're in this weird phase where we're we're hitting just how amazing these things I bet those orgies help yeah the the weird CNC parties with heavy uses of LSD as they say uh that's wild U shouldn't even say just crazy right there crazy four is already not much better than uh 3.5 yeah I mean it's better but it's not massively better now the real question is going to be compute power power versus uh Machinery right we have a whole problem machine time and we have a problem of compute power sorry people people are saying what uh for those that don't know there's this whole thing that went on on Twitter just recently let's see if I still have it up somewhere shut up Warren uh where is it oh yeah um to the journalists contacted me about AGI consensual non-consensual CNC sex parties also heavy use of LSD among some of the elite AI researchers blah blah blah blah this is spawned from uh this Yan fella doing ml research on open AI so I don't I don't know I don't know it's just crazy it's just crazy tweet okay that's my only response to it was what what oh we would have lost you to the AI sex parties you would have lost me to the AI sex parties in LSD if I wouldn't have changed that would have been it I would have been an AI cummer instead I would have been but now I'm not but I am curious like can we even sustain the amount of power because if we need to 10x the amount of power and compute like that's a lot on the on on the classification which is good but is it worth it I don't know this is a really interesting paper because it's very very fough right there's a lot of evidence there's a lot of Curves and they all look exactly the same it doesn't matter what method you use it doesn't matter what data set you're training on it doesn't matter what your Downstream task is this this port curve actually went down little flaccid on the curve this one's like this you know this was feel a little I mean I can't quite tell what they are I don't know what any obviously we haven't read the paper but I mean still you don't want to be this guy you don't want to be this you don't want to be this data set where they just they pre-trained look at that they 10x the the conceptual frequency training and it went went downwards this is also pretty crazy 100x the difference with almost no with almost no like big wins here is the vast majority overfitting it could be overfitting you you are right it could be overfitting uh I don't know I I don't know the extents but I would assume that the people doing the research I mean pre overfitting is like your classic AI 101 course you build your first MLP they give you the Rosen Brock function you do the Rosen Brock function and then you realize you've done too much you know it's kind of a classic it's kind of a a big time classic one this kind of problem and the other problem is that we don't have a a nice even distribution of classes and Concepts within our data set so for example cats you can imagine are over um emphasized or over represented over represented yeah over represented in the data set by an order of magnitude right whereas AR specific planes in other words what he's trying to say is that people really think cats and dogs are cute and everybody's like yo screw those shrimp okay put them in the cocktail and we'll have a party okay you bring out Mr you you bring out Mr Bigglesworth and we'll give Mr bigle worth a pet and then we'll just absolutely eat all the shrimp no one cares about it no one has the cute shrimp photos specific trees are incredibly under represented because you just have tree right so I mean trees are probably going to be less represented than cats anyway but then specific species of tree very very underrepresented presented which is why when you ask one of these models what kind of cat is this or what kind of tree is this it performs worse than when you ask it what animal is this because it's a much easier problem and you see the same thing in image generation if you ask it to draw a picture of something really obvious like a castle where that comes up a lot in the training set it can draw you a fantastic castle in the style of Monet and it can do all this other stuff but if you ask it to draw some obscure artifact from a video game that's barely even made it into the training set it's starting to draw something a little bit less quality yeah and the same with lar Lang little less quality okay is that is that is that how we're referring to that draw a centaur um yeah yeah that makes perfect sense it just does langage models this paper isn't about large language models but the by the way obviously the number the reason why cats are the most prevalent is actually a very interesting intersection that you may not have caught on to but there is this really unique intersection of arch user cat girls cats and llms and I think that because they all have been put together cats are just the most recognizable creature from llms second most being cat girls actually being the next most recognizable creature same process you can see actually already happening if you talk to something like chat GPT when you ask it about a really important topic from physics or something like this it will usually give you a pretty good explanation of that thing because that's in the training set but the question is slightly better Google search let's go it lit literally Chad GPT just summarized the Wikipedia page for me that's all that is this is just Wikipedia just without all the Wikipedia right like I don't need to be like well the very the very first you know phases you're just like dude just tell me just tell me what it is is what happens when you ask it about something more difficult right when you ask it to write that code which is actually quite difficult to write and it starts to make things up it starts to hallucinate and it starts to be less accurate and that is essentially the performance degrading because it's under represented in the training set so I actually disagree with him on this take I think that it's it's it's not about that it's about the it's it's about the preciseness um because I think the preciseness is just like copy posting a simple like you know key words from a Wikipedia article right it's just like this is such a strong tied one and maybe okay maybe he's right maybe I'm maybe I'm understanding him maybe I could be wrong here but I feel like the difference is is that you can't yeah okay actually I do agree with them the under represent under representation in the training data meaning that every time you come across any novel thing or the thing you're trying to program in context of what you're trying to program has never really been done before so it's kind of between these two points of extrapolation you know it's trying to like Island hop between these two concepts and so you just get this less precise you know less accurate version with hallucinations okay sure yeah I mean okay I can buy it you know what I'm wrong I'm wrong he's right I don't know why I doubted the computer file guy I mean he even has like a British accent he got British interet all right isn't that pretty good one um right now the only accent I can do is Kano that's it the argument I think is at least it's the argument that I'm starting to come around to thinking if you want performance on hard tasks tasks that are under represented on just general internet texts and searches we have to find some other way of doing it just collecting more and more data right par whoa I don't think I can Doo beond Aro Kung Fu well morphus Mor morphus but what is the Matrix wow because it's incredibly inefficient to do this right on the other hand we they you know these companies will they've got a lot more gpus than me right they're going to train on on bigger and bigger corpuses better quality data they're going to use human feed back to better train their language models and things so they may find ways to improve this you know up this way a little bit as we go forward but it's going to be really interesting to see what happens because you know will it Plateau out will we see trap GPT 7 or eight or nine be roughly the same as chat gp4 or will we see another state-of-the-art performance boost every time I'm kind of trending this way but you know it'll be exciting to okay I got one more I got one more impersonation request which is the shop sales girl from kakar Rico Village that sells both the uh that sells the arrows uh you're welcome that's that that that is the End by the way this this thing that he's saying is very interesting because it also is kind of glossing over the fact that training data is becoming more sparse more and more the internet is comprised of data generated by llms right so now you're getting the average of an average which is not going to be good not safe for yeah you could don't hey flip don't take it out right which is like that's going to be a real problem e girl primagen is not real he can't hurt you can can can he GPD [ __ ] 40 is gonna have like zero hype it is I'm actually very curious what's going to happen to the point where like can gpt7 even exist if 80% of the internet is GPT traffic I I I know they're trying to solve it but it's a really hard problem to solve it may not be solvable I know they're pooping in the well there they go [ __ ] in B again people to see if it goes this way take a look at this puzzle devised by today's episode sponsor Jane Street it's called Jane Street mentioned did we just get Jane Street mentioned did we just get o camel mentioned did we just get teach streams by the way teach DV everybody teach DV bug I by the way I cannot believe that TJ somehow got mentioned even in this video this is upsetting that somehow TJ even wormed his way into this one right inspired by debugging code that world we're all too familiar with where solving one problem might lead to a whole chain of others we'll link to the puzzle in the video description let me know how you get on and speaking of Jane Street we're also going to link to some programs that they're running at the moment these events are all expenses paid and give a little taste of the tech and problem solving used at trading firms like Jane Street by the way Jan Street's pretty cool company like I mean I know like typically we're people are not big fans of of of trading stock Financial companies and generally you're probably not far from Wolf of Wall Street going on but man they also like prop up the O camel community and there's like really smart Engineers there a lot of Giga Chads on the keyboards pretty cool like pretty cool stuff Citadel mentioned yeah it's a little they they don't seem to have um as much of a negative outlook as Citadel uh but they're like what they produce is pretty cool I I am very very excited about the cool engineering they produce I think it's pretty it's pretty amazing are you curious are you Problem Solver are you into computers I think maybe you are if so well you may well be eligible to apply for one of these programs check out the links below or visit the Jane Street website and follow these links there are some deadlines coming up for ones you might want to look at and there are always more on the horizon now thanks to Jane Street for running great programs like this and also supporting our Channel and don't forget to check out that bug bite puzzle okay thanks computer file and thank you Jane Street for sponsoring computer file that was great that was a great video it is a very interesting by the way there's me with a microphone in Brazil wearing a Brazilian jersey Brazil mentioned by the way I do I do like this whole I do like this whole thing this is very very interesting like area I of course I'm you know I think it's it's not it's not shocking that I'm am on the uh I'm on the probably not going to move as fast side of things and the only reason why I'm on the probably not going to move as fast side of things is that you know like I've just been around long enough to see that technology goes you know um it's not as exciting right things always start super exciting and tend to fiz all out so I'd be personally very surprised if this thing actually you know doesn't do that energy is huge yeah I mean there's there's a lot of hype cycle same with Co I I read things before I say before I actually process them in my head um let's see hold on I completely agree with all your reviews on ML and AI really I'm surprised I mean some of them must be wrong just an interesting question about its progression I always ask myself how is this technology different from the other ones the hype over let's see the hype over is understandable crazy uh but people were also doubtful about cars and other stuff in the 1900s would love to hear your take well I mean there there's a huge difference though there's like there's so many different things about especially with cars one hard part about cars is that there there was also just not like infrastructure that was required for people to have cars uh that's much much different whereas we already have all the infrastructure for people to have AIS you know to be able to have access to the AI it's just that we have a compute problem right we have a we have an infrastructure problem on a different side so you the consumer can use it but you the the producer well you you're the ones that actually have to struggle a little bit more on that like whole topic and so it is it is fundamentally different uh but like all adoption of all technology it takes time even if AI were exceptionally better than it is now we would still be about 10 years off before it's actually in every single company it just will cars don't require new physics yeah that's also true too uh it's just it's just a fact of life no matter and that's assuming AI in like its most amazing stance yeah exactly the I uh the ey stands for intelligence for those that don't know this article it's just about the horrors that are happening on security uh and open source and just the issues that are coming in and so that's like a huge problem is even if it was amazing today right now and solved a huge classification of problems and actually was a junior engineer that you could use like a real Junior engineer someone that was full of energy and was able to solve all the problems and with mild like actually come up with new Solutions actually be a junior engineer it would still take 10 years before everyone's fully using it because the first iteration of companies using it there's going to be a whole series of things that go terribly wrong who knows about the lawsuits from what kind of uh training that has been uh used all of that there's like all sorts of things that are going to happen the huge amount of risk management that has to like go forward there's all these companies that just forbade the use of co-pilot because the use of co-pilot is also selling your information there's like all sorts of just like company based hurdle so making the assumption that AI is amazing today we're still looking at a huge gap before people would even start using it everywhere so now let's pull it back and say where AI is today the amount of companies that can really take advantage of it is going to be on the individual level right can I use copilot at my job yes I can I will use it and see if I like it or don't like it right that will be like a whole thing it'll be an individual one there's one advantage of gypp versus human jippes are in uh infinitely patient are they you can talk rudely to a gypp and you get worse results you can talk nicely and tell remind it to breathe and it'll do better like this is a well-known kind of weird weird aberant behavior in llms is that they they're mirrors right they're mirrors of people you treat an you the more tur you talk to an llm the the worse your answers are the nicer and Kinder you are the better your answers are also time of the year yeah yeah it turns out if you change the time of the year you get worse answers you just make sure it's always in July uh yes but you can also restart a thousand times with the same discussion or code task sure we're just going to say hey you're right you win in this one invert it you talk to a person who doesn't have patience who doesn't have all these things right that this magically amazing AI has and you ask it a question you know what it's going to do it might tell you you have an XY problem you're asking for X but what you really are asking is why you're off you're wrong you're actually you're incorrect on your entire fundamental presupposition meaning that what you're trying to do is the wrong way from the get-go AI is not going to spot that for you AI is going to give you all the best answers to do what you're supposed to be doing incorrect right the XY no the XY XY problem it's kind of like a well-known problem in in in in our world which is a very obvious problem where you want to solve problem X so you figure out you need to solve it by solving problem why you go and ask about problem why and you're completely and and you're going off on a completely wrong direction and so certainly you are totally correct here's the version of right the x-wife problem the XY problem it's very very good I love when someone learns about the XY problem and tries to tell me that when I'm clearly searching for my ex yeah I'm clearly searching for my ex XY problem should be solving X but you're solving problem y yeah yeah effectively that's the the gist is is like let's say that you have been given some task and the task is like you have to remove all the uh you have to prevent people from gosh what's a good what's a good way to describe this I can't think of good way to describe it but you get the idea you're you're asked to do some sort of task and you realize there's a roadblock in doing your task you find a way to work around your task you start working on that you run into a new problem in which seems and appears very small so you go and ask how do I solve this one thing now people will go well wait a second this kind of seems like an odd way to use our system and they don't and they're trying to work through that problem with you not realizing that you're trying to solve a completely different problem the dung goof problem anyways the name is uh it's it's very interesting so I just please just just take a moment and re realize that things they they typically don't grow linearly and they typically don't grow exponential they typically grow like that and it just gets real sagge in this area and so it' be very interesting if for whatever reason AI breaks this this this typical thing that you should expect you should usually you expect law log law you usually expect log you don't expect um the other way I drew that all wrong and it really bothers me it should really be like this and then log should be really like that right you usually this is usually what you you're seeing right and so I think we're just right here on on on AI I think we just we we started with nothing like three years ago there was nothing and now we have something and so where are we what line are we on is what you really have to figure out you know we could very well be right here and we just don't know we just don't know okay a Jen