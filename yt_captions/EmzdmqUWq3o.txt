Here, I have a bunch of different sorting algorithms implemented in C++.   We have a quick sort, a bubble sort, a merge sort and an insertion sort.   Let's go ahead and run a benchmark to see which one's actually fastest. Here, I'm using the google benchmark library to do the benchmarking. Here's the guts of basically what we're timing here. Which is that we're going to create a copy of some initial vector. And then see how long it takes to sort it. Which sorting algorithm do you think is going to bench to pass this? Quick sort, Bubble sort, Insertion sort or Merge sort? Well, here are the results. I did include standard sort here just for comparison sake. But of the four that I mentioned before, it looks like insertion sort was the winner. In this case, insertion sort and bubble sort performed better than quick sort and merge sort which is probably the opposite of what a lot of you might have expected. Yes, insertion sort and bubble sort are n-squared algorithms. Whereas, quick sort is expected nlogn and merge sort is nlogn. So, why did the n-squared algorithms beat out the nlogn algorithms in this case? Well, of course, it's because I didn't tell you that the size that we're sorting is pretty small, only 16 elements. But on a fixed small size, like, 16, Big-O time complexity of an algorithm is often not the most important factor. In fact, cache locality and branch predictability often play a much larger role. Caches and branch prediction don't have as much to do with C++ itself as they have to do with your physical processor and memory. In a real computer, accessing something off of disk is going to be thousands of times slower than accessing from RAM which is going to be slower than accessing from cache. Of course, the catch is quantity. You're going to have way less RAM than you do have disk space. And likewise, you have way less cache than you have for RAM. On my machine, I have 32 gigabytes of RAM but only 8 megabytes of cache. Here, I have a struct S that has two members x and y which are going to be stored contiguously in memory. When I call a function that takes an s, then if I want to access s.x, the computer will load that memory where this little s is living and put it into cache so it's available. But it won't just load in s.x alone. It'll actually grab a whole chunk of memory that's nearby. So that later, when I very likely want to access other attributes, like, s.y, it will already be in the cache. If you try to access memory that was already in the cache, we call that a cache hit. And if the thing that you're looking for isn't in the cache, then it's a cache miss. So, misses are very expensive. Hits are not expensive. Here, I have a function for summing up elements of a vector. I take in the indices that I want to sum And then I just loop over them, grabbing that element out of the vector and adding it to the total. Addition is commutative. So, I'm going to get the same sum regardless of the order that I add up the indices in. However, the order matters a lot for cache locality. If I access element 0 and then 1, 2, 3, 4 and so on, that's really easy for the processor to predict. But if the indices were a random permutation of all the possible valid indices of the vector, then the processor is going to have no way to predict what memory to load next. It's just random. And here's the results. The blue line shows the in order access of the vector. So, just going index 0, 1, 2 and so on. Whereas, the red line is for accessing things in a random order and adding them up. The x-axis here is how many bytes long the vector is. So, approximately four times the length of the vector because the elements are integers. I chose to use bytes for the units instead of length Because, from this picture you can practically read off how big my caches are. The biggest kink in this graph is right here around 8 megabytes. And I have 8 megabytes of L3 cache. That means, to the left, the computer could potentially put the entire vector into cache. Now that doesn't actually happen because it wouldn't really know to do that ahead of time. And there's other things using memory on my computer, of course. But to the right of the line, the entire vector won't fit into cache. So, if the computer isn't able to predict what memory you're going to access next ahead of time, then you're going to pay that penalty of cash misses. On the other hand, for the in order access pattern, it didn't seem to make any difference at all when we crossed the 8 megabyte cash threshold. That's because this access pattern is so predictable the computer can just put the data that you need into cache before you ever need it. Okay, accessing a vector's elements in a random order is a pretty uncommon and silly thing to do. How about a more realistic example? Matrix multiplication Here's a straightforward way to implement matrix multiplication. I take a and b which represent N by N matrices. So, they're both vectors that contain N times N elements. The purpose of the function is to put the matrix product of a and b into the output vector. We start by initializing the output to all zeros. Then we just do a simple double for-loop over the rows and columns.  If you remember the ijth entry of the output matrix should be the sum over k of a_ik times b_kj   This third for-loop is to implement the sum. As we loop over k, i and j are fixed. So, this term isn't changing at all. This next term we are changing. But it's just moving with k, one element at a time. The problem is with this third term. Here, when k increases by 1 because we have N times k, we're now jumping N elements ahead. So, this is bad for cache locality. Here I have the exact same function, except the only difference is that instead of looping i, j, k, I'm looping i, k, j. Now let's take a look at the inner loop now that it's a loop over j instead of k. As j changes, this element is moving one element at a time in order. That's fine. i and k are fixed. So, this isn't changing at all. That's also fine. And now the third term is also okay. Again as j changes, this is just moving one element at a time. So, all three terms are good for cache locality. So, I went ahead and wrote a variation for all six orders of i, j and k. Let's compare the performance. And here, we have it a huge difference in performance just based off of the loop order. As we reasoned before, loops where the inner loop was j. So, i, k, j and k, i, j, those performed way better than the others. So, in this case, having good cache locality gave us over a 2x performance improvement. That's pretty good. These times were for 64 by 64 matrices. And when the matrices are that small, cache locality is pretty much the dominating feature. However, this is not the end of the line for bigger matrices. If you're working with bigger matrices, of course, doing block matrix algorithms across multiple threads is going to be faster. However, for smaller matrices, you can probably compute the entire answer in less time that it would take to spawn a single thread. The next factor that you should really consider when thinking about the performance of your code is branch predictability. A branch is just something that affects the control flow of your code, like, an if-statement or a for-loop or a while-loop. Whether or not the if statement turns out true or false determines what the next instruction for the program is going to be. Modern processors are really amazing in what they can do. Gone are the days where your program is executed line by line. A modern processor will happily execute things in a different order than you asked for. And potentially, even executing multiple things at the same time. If you want to add say 3 to some variable x and then 5 to another variable y, there's no reason the processor should wait in order for the store to x to be finished before starting the store to y. They can execute at the same time. There are many many situations like this that your processor will try to take advantage of. As long as you couldn't possibly tell the difference that anything funny was going on, your processor is free to do them out of order or in parallel. But what happens when you get to a branch point? Your processor can't know which instruction to execute next until it's fully evaluated the condition. In situations like this, your processor may just guess. If the last 10 loops your processor found that x was bigger than threshold and this one was executed, then on the 11th time, it might just start executing this ahead of time. As long as it predicted correctly, it gets a big head start on the next computation. We say that a branch is predictable or has good branch predictability if the processor is able to correctly predict which of the outcomes is going to occur a lot of the time. For this function get_product, I'm doing something kind of artificial. But it will allow us to really see how good the branch predictor is. I start with a product of 1.0 and then I loop through all the elements. For each element that's bigger than some threshold, I multiply by 2 times the element. And if it's not bigger than the threshold, then I multiply by 1.5 times the element. In this case, the predictability of the branch depends a lot on what percentage of elements in the vector are bigger than the threshold. If 90% of things are bigger than the threshold, then the branch predictor could just predict true and be right 90% of the time. If only 10% of things were bigger than the threshold, the branch predictor could just predict false all the time and still be right 90% of the time. But if half are bigger and half are smaller, then the branch predictor can't do any better than 50%. And here's the graph that proves it. Here, I used a uniform random number generator to generate numbers between 0 and 1. and then set a threshold between 0 and 1. In both cases, when the threshold was either very low or very high, the branch became predictable. And we got a nearly 2x performance improvement from the branch predictor. Of course, the worst performance is dead center at 50% when the branch predictor can do little more than flip a coin. I do want to warn you, though. Compilers are really really good. So, if you try to do this benchmark yourself, depending on the compiler and optimization settings and the time of day or whatever, you might get different results. There's a good chance that if you try this with your compiler, you might see a completely flat line. And if that's the case, what probably happened is that your compiler completely optimized the branch away. There's a good chance your compiler would optimize it to something like this that doesn't involve any branching. And if that's the case, you would just see a flat line. Well, that's all I've got. Thanks for watching. If you liked the video, don't forget to like comment and subscribe. Thanks to my patrons and donors for supporting me.