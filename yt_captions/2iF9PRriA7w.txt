today I wanted to talk a little bit about the the models and the algorithms we use for planning under uncertainty for robots so how robots make decisions when the world is against them the world's always against robots Yeah Yeah It's Tricky sure probably the best place to think about or the best place to start is a shortest path algorithm so I think that's I'm going to try and draw it and just to warm up the drawing um so you can imagine that you're starting off at home and you're trying to get to work and I should say that this is a an example of stolen from a talk I saw a few years ago and I'll give you the the acknowledgments for that later but you've got let's say the simplest shortest path problem you've only got one action so you can go by bike and that might take you 45 minutes and you can go buy car and that might take you let's say 30 minutes and you could go by train and maybe that takes you 35. this is a kind of a standard decision-making problem right we've got two states so our states are being at home or being at work and we've got three actions so we can go by bike we can go by car or we can go by train we use these kind of sort of State transition models everywhere for robotics so if I'm programming a robot to pick something up or navigate between in a map the last time we were together we're looking at robots navigating through these graphs and we we turn those graphs into these kind of decision problems as well so this is a kind of a model you could use to decide well what route do I use to get to work and in this case if we're trying to minimize time or minimize cost then we would take the the car option which is 30. I'm going to do my teaching what's the problem with this model too simple it's too simple so it doesn't reflect the fact that life is just yeah the world can be against you so the world can be uncertain so the standard thing we do in robotics is we take these what we call deterministic models so this is a model where the outcome is fully determined by the state you're in so you know where you're going to be and we can say well we can actually we can make them more complicated we want to reflect the uncertainty so I'll draw a kind of a big version First and we can think about that so we imagine we take our action which we might call car and that's still going to cost us 30 or it's going to take 30 minutes right that's my planning model this is how we're going to model the world but then what we're going to think about is actually there are going to be different outcomes so when I execute this action so when I perform that action in the world I can still only end up in one of three different states but they might be different traffic levels so it might be light medium and heavy that's kind of amazing I've got in my car I've chosen to take my car to work I've driven a little way and I hit the motorway or whatever and it's it's one of these traffic States and then I can go to work from that but what we could imagine is when we move out of those States it's going to take us a different amount of time so what we might do is we might use that to change our model so this is the basic idea and what we would do is we would assign different probabilities we might say 20 of the time or 0.2 the traffic's light 70 of the time it's medium Temps at the time or probability of 0.1 there's heavy traffic so this is reflecting some of that uncertainty some of that complexity in the world this is kind of the standard action model we'd use and when we use action models like this this is called a Markov decision process so a Markov decision process is States and actions just like a shortest path problem except now when I take my action there's a probability distribution over the the the outcomes I can reach I think we did Mark off sometimes talks about the text isn't it right yeah so Markov is a probabilistic assumption when you've got a state it doesn't need actions right it can just be State transitions excuse me State transition systems and what we do is a mark of assumption says the probability of an outcome only depends on the current state so it doesn't matter the history it doesn't matter what I was doing before I chose to leave home the only thing that determines that probability is the action I'm taking right now to be more formal you can think of that as a sort of first order Markov system and you can have a second order Markov system which says the only the current state and the previous state um determine my probabilities but in general if you think of someone like a Markov chain if you're doing text processing you can think about word generation following probability distributions and there you can think of like one step so if I give you the there's a probability of getting capped or if you if I give you I am the then you get a probability of getting something different and if you take that whole history if you make there'll be a third order Markov assumption you'd get a different probability distribution over the word you'd get next that's desperate says um right so that's our kind of action that's our action model and so we can take this action model we can stick it into the the graph we have before the process we have before let's set this up again and we'll draw that little area at the top that means this is our initial state so just like a shortest path problem we're going to start from somewhere and we typically do this in robotics because our robot has to be someone when it starts thinking and we're gonna make our whole world a bit more complex so I can either go to the railway I can go by car I can go by bike so these these actions now really are reflecting the choices I'm going to make to start with so not necessarily getting on and doing that but sort of making the decision that this is what I want to do and maybe go into that form of Transport so now car is going to take one this is me leaving the house and going to to my parking space and finding what car Railway maybe I have to walk a bit further to get to the railway so that'll be a cost of two and then we can start to put these these um actions in so actually these arrows should be dots and I can put in the the action that we had before where we have different traffic levels so light we've got medium I'm going to start abbreviating these things because my handwriting is atrocious and will take forever just watching me write words or heavy and then down at the bottom down here we're going to have work and I'm purposely doing two circles here because we cook this this state transition system we have our initial state with the Arrow coming in and I do these two rings to mean this is where we want to end up we call this an absorbing State there's no there's no way to get out of it when you're there and as this is work that's a little bit depressing but there we go and then we might have an action called Drive so our Drive action is going to be deterministic in that I can take it and it always takes me to work but I can only take it in certain States so I can only take it once I'm in my car I'm only like to drive then and we can use this now to model the fact that we might get different durations of travel so let's say the heavy traffic it takes me 70 minutes to get to work in heavy traffic 30 and medium 20 minutes in light and I think we have these probabilities of 0.2 0.7 and 0.1 so this shows me that if I go by car I get some probability of these different traffic levels and once I've experienced that traffic level there's no turning back I have to follow the car I have to follow that action choice so use the car to get to work and it might take a different amount of time we're going to say here the bike is deterministic so if I'm on a bike I'm fully in control of the duration it takes me so that will take 45 minutes so it's further by bike but there's no uncertainty and that could be interesting later in this example we can say that when I go to the railway station well maybe 90 of the time it seems quite unlikely the trait the train is there it's ready to go and then it takes me I've got an action which we'll call relax because once we're on the train we're happy and that's going to take me let's say 35 to get to work and then 10 of the time I have to wait so I'm in the waiting room and then from the waiting room I get to wait and then 90 of the time the train comes ten percent of the time I get to wait again so what's interesting here is there's a kind of notion of a Time step you can start to imagine that I'm I'm looping um I wait the train comes I wait the train doesn't come and I repeat that and of course if I get stuck in the waiting room for too long I could also choose to go home and what's nice about that is that I get to make another choice so that might cost me two my waiting maybe I wait for three minutes each time so now I've got this big graph and this is the Markov decision process this is a process because I'm going through a sequence of steps at each step I have to take a choice over what action to perform and then that evolves the state so it changes the state I'm in and we have different probabilities of reaching reaching different states that's kind of still in some sense we can think about this as the shortest path algorithm but instead of a shortest path we call this a stochastic shortest path because what we want to do is reach our goal so we're still trying to reach our goal but now we're dealing with the uncertainty as well this is a model this describes the choices your agent could have so this is the choices you might have when you're going to work we could imagine I'm building an autonomous vehicle and this is you know it's an autonomous taxi system and it has to decide how to take you to work these kind of models really capture what we the decision you know a decision-making part of artificial intelligence whether that's robotics whether that's chat Bots all sorts of stuff um so the next question is really how do we how do we solve this and there's there's two answers or there's two parts that answer the first thing is what does the solution look like the second bit is is what algorithm do we use when you say what does it look like you've been as in is it is it just a simple number or is that yeah well so in the okay what structure does the solution have so if it was the shortest path problem the the solution is a path right it's because it and that's a sequence of the actions you will take from start to finish um but in in a in an mdp in a markup decision process we can't use a path because when I take a step the world changes probabilistically stochastically so I I don't know what world I'm what state I'm going to end up in so instead of a path we have what's called a policy so policy is a lookup table that just says when you're in this state take this action and that state could be the States from the problem or we could even augment them so as the as these models get more complex we might add extra information into the state we could add the time of day we could add um you know how much time is remaining before some kind of deadline um how many times we've waited in the waiting room might have been interesting you can add extra to your state to help make decisions but in general the policy rather than just being a straight line plan a sequence of actions it's going to be a lookup table that says typically what is the optimal action so what's the best action to take to achieve a particular specification so a particular goal that you're trying to reach um when you're in this state and you can think of for mdps there's all sorts of there's a very rich space of specifications the most common one for something like this for a stochastic shortest path problem is to minimize the expected cost to goal so the the expected cost is the average cost so if I get if I'm going to work you know every day for the rest of my life and I have these these choices um then actually having the expected so the average cost kind of makes sense because I get to make this I get to execute the policy for this this problem lots and lots of times sometimes I do well sometimes I do badly but all of that kind of evens out over the time um so we could we could think about that to start with um so my goal is to get to work my cost is the time so what I want to do is minimize the cost to get get to um to get to the goal and so I need an algorithm that's going to compute the the cost of the so we think about the cost of the expected cost of the whole policy starting from the initial state which is going to be similar and it affect thinking about the average cost of a path from the start to the end or the cost of the average path not the average cost of the part the other way around because the paths change because of the probabilities the way we solve this is with something called a Bellman equation so the Bellman equation describes how good a particular action choice is so we need to think about the different action choices available to us in a state and we use those we kind of value those choices and those that we basically pick the best action which in this case will be the action with the lowest cost and we put that in the policy which is also about why we can't use things like a star or dijkstra a star and dijkstra deal with deterministic problems so they deal with State transition systems that might look a little bit like this but they assume the actions are purely deterministic so they produce past from start to finish there's no probability there's no probability yeah so they they assume that when they take an action the world changes in the wave of action behaves there are kind of analogs for some of these things in the probabilistic world then the algorithm we'll get to is is it has similarities to the way dijkstra's algorithm Works um and actually a star is a heuristic search algorithm there are also heuristic search algorithms for Markov decision processes so they're classes of algorithms that fit well together and but for this we can't take dijkstra or a star and say you know off we go if you're thinking about just the average cost then everything gets a lot easier and we what we're basically going to do is take all the the different we're going to multiply the cost by the probability of getting that cost sum them up and that tells us really how good that action is so it's just kind of like it's almost like taking something like a normal shortest path algorithm and collapsing all of the actions into an average action but we have to do that recursively because that each action takes me to another state where I can apply a different I have to make a different action choice so we have this recursive problem of my actions are only as good as the states they reach that's the kind of recursive part but I also have to think about the fact that that happens probabilistically which is where this sort of averaging comes in for the expected cost we can also think about trying to limit the probability of success or failure or the probability of certain extremes I think we can talk about that later um the algorithms are much much harder but but but super interesting and actually don't those are the things you care about when your boss says you have to be here by 10 o'clock for a meeting or your patience runs out if your commute is longer than 45 minutes so real world problems are much I think real world problems are better captured by those richer specifications I mean it looks to me like the waiting room is the sort of is the kicker here because it's potentially you know quite a bit more complicated right yeah so if you went down the train route then you've got multiple kind of nests of that right so it depends on what problem you're trying to solve so if we're solving expected value if I build a policy to only optimize the average cost of a journey then the best choice is car I think it's something like 33 or 34 for minutes which is faster than bike which is 45 and I think train comes in somewhere over 35 or over 37 on average so it's more expensive than car cheaper than bike but yeah the average thing is interesting and this is where these models become fun for Robotics and for problem solving which is your by choosing the car 10 of your Journeys take you 71 minutes right which would make you extremely late late for work if in general you should be taking 30 to 40. so you you if you only solve for average value then 10 of the time you're very late and so what you can think about in these kind of problems is different what we call either specifications or objective functions so if you're doing some optimization you typically think about the optimization the objective function the thing you're minimizing or maximizing and in when we do planning or decision making with these kind of models we often phrase it as a specification that which is a mixture of the goal you're trying to reach and the constraints you're trying to put on it yeah so for instance you're trying to do it in the shortest possible time but it should never be longer that yeah that that's a perfect example so if you try to do it in if you if you say it can never be longer than um yeah I'm going to say 60 because the mass I remember the mass of this so if you say it for 60 well you always take the bike because every everything else there's a chance the car takes 70. um there is a non-zero probability that the train also takes longer than 60. so there is a non-zero probability in the if you take the train you're waiting forever effectively um but the bike is deterministic but the bike is long so it's guaranteed to be less than 60. but it's guaranteed to always be 45 so you're always going to be later than you want to be um the the kind of optimal strategy I think to get in under 60 um guaranteed to get under 60 whilst minimizing time is to go to the train wait for three Cycles wait and then go home and get the bike so effectively you can think about the chance of getting the train within three cycles and then if you go any longer then it's going to take you longer you know you'll violate your deadline so then you go back to get the bike but that's a those kind of solution mechanisms are really I've actually been in that scene yeah exactly um it happens a lot particularly kind of post-covered where you're like well I could work from home but I should be there oh the Train's late I'm just going to go back and put my pajamas back on but you need to think about what we call an augmented State space you'd put the time that you've waited into the state and that allows your policy to include actions you know I you think about actions that I'm in the waiting room and time is now at nine minutes and those things that you do in your head going oh god I've waited this long maybe the Train's never coming you can Factor those into the the decision making as well if you know exactly where you want the robot to go you take the 3D map and then you manually annotate it with the points in this case we're doing it entirely well the graph is built autonomously using that that Lego representation so you say what we call submitted Computing in particular what we want is the