lm's large language models are really powerful and can be used in lots of different ways in your projects you can self-host an AI model or you can work with AI as a service and the most commonly used one currently at least is open AI API when you use open AI API you get an access token and that allows you to interact with the API but it's not Plug and Play there's several things that you need to think about some cfats when you actually start using an AI API like this in your production environment so today I want to give you three practical tips for working with a tool like openi API before I start if you want to learn how to design a piece of software from scratch check out my free design guide at Aron blood codesign guides it contains the seven steps that I take whenever I design a new piece of software new minds also find it helpful so iron. codesign guide link is also in description of this video now let's dive into the first tip which is that you need to work around token limits now you may say that this is actually a temporary thing because the token limit of these AI models goes up and up and up but at the same time we might also want to use AI apis with larger pieces of text at the same time so you still always have to be aware of what the limit is in terms of content that you can send to an API so what I have here is simple main file that I'm going to use as an example in this video it initializes the open AI client uses open AI gp24 model then it reads a query from a file called short story. text which is actually a short story that I can show you right here so this is a short story that for example we could ask the AI to translate this to another language and similarly I also have a much longer version of this story that's for sure going to hit all kinds of token limits if you want to send this directly to an AI and from my own experience dealing with token limits is one of the main issues that you need to think about when working with an AI API and it even depends on the model that you're using for example in the case of open ai's GPT model so GPT 3.5 turbo has a token limit of about 16k tokens whereas GPT 4 has 8K tokens and now it's going to be increased to 100K which is a lot more but still you might run into that limit at some point and what's sort of annoying is that you don't always get back an error from the API that you've exceeded the token limit that only happens if you send more input tokens because that token limit is input plus output so it's very well possible that you do get a result from your API call but it's just truncated and that's problematic because it means you only get a partial response now if the AI just responds with General text that might not be such a big issue but if for example you ask it to respond in Json format you're going to get back half of a Json structure and then you can't parse that and you can basically use any of it so that's really problematic and that means you need to think about how to deal with token limits but then you need to somehow estimate the number of tokens that you need you can't predict the number of tokens in the output because that's what the AI generates but you do have some control over what you send as input now you can kind of guess what the number of tokens is by simply counting the number of words and the text that you send but that's not really accurate the alternative is that you use a tokenizer to make an educated guest and my go-to library for doing this is Tik token so here's one of the functions that I'm using in the script called handle request that gets a query that gets the open a model and the open a client and this computes the number of tokens from the query using this function and that's in the encoding file that uses tick token to get the encoding and you can specify various options here so you can pick the one that most closely matches the API that you're using but then when you encode it you can return the length of the tokens and you get a sort of educated guess of how many tokens this text actually is and now because we have the number of tokens we can decide whether we need to cut the request up in different pieces or not and sometimes you can't do that because the request is needs to be one thing in that case you can return with an error but in some case like for example text translation you can try to cut the text out in smaller pieces so that you then separately translate each of the pieces that's what we assume here so one thing that we've done is that we've defined token limits for each of open ai's model like gpt3 turbo or GPT for and then we can use these token limits to determine in how many pieces we need to chunk the content and that's basically what this function split query into Parts is doing and in this case I created a pretty basic chunker that splits the text into sentences by using a regular expression to match sentence endings I won't dive into all the details here but you can find the code for this example in our example repost story I put the link in description of the video but basically what happens then is that we have all the different parts so we chunked the input text and then for for each part we send a request to open AI API which is what is happening here and because we're handling multiple requests that also leads to the second tip which is that we need to think about rate limits if you make too many calls to an API within a short time frame then you're likely going to hit some sort of rate limits and that means your request will simply return with an error and of course you need to be aware of what the rate limits are of the various apis that you using and then make sure when you're sending out requests that you're within the ranges that are allowed here you see an example of how a rate limiter could work so I've modified the code here to add a decorator that limits the rate at which you can send request and I've put a maximum number of calls here within a certain period so this is all in seconds and then when you call the function then it's going to respect these rate limits and this is just the basic decoror for rate limiting that we built ourselves of course this doesn't work if you have multiple surfaces calling the same API then you also need to coordinate that between your surfaces but for a very basic setup this kind of function actually works fine now I wouldn't really recommend that you build this from scratch yourself you can use rode or you can use an existing Library I haven't really found a great rate limiting Library yet in Python if you have a suggestion please let me know in the comments the third and final tip is that you should use the appropriate model for your project depending on what you actually need you don't always need the highest performance or most expensive model at the moment the high highest end model that open AI offers is GPT 4 which is really good but it's also a lot slower than for example GPT 3.5 turbo and depending on what you need you don't actually need all the capabilities of GPT 4 for example if you just want an AI to review some text then perhaps using GPT 3.5 turbo is more than enough it's more than what you need and that save you money and time because GPT 3.5 is way faster than GPT 4 and it's also way cheaper here I have a main file where I try to translate this short story and then I'm doing that for each of the different open AI models so when I run this code and now it's actually calling the different apis to perform the job you'll see that the time that each of them needs is going to be very different so here you see for example that GPT turbo now succeeds in 16 seconds whereas GPT 4 is currently still working so it takes way longer so overall it's always a trade-off between speed accuracy and price so in short be aware and take into account token limits make sure you don't hit rate limits and the third tip is to make sure you use the appropriate model for what you actually need we've been applying these ideas to learn tail which is an AI quiz tool that we've been developing you can try this for free at learn.com if you want to learn more about how we set that up and how the architecture of such a system works check out this video next where dive into the details thanks for watching and see you soon