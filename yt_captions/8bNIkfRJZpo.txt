hi everyone last episode we looked at how a neural network is able to learn complex decision boundaries or by changing its weights and biases let's now see how we can represent the neural network using matrices I'll start by creating a weight matrix for the connections between the input and hidden layer each row will hold the weight values from the input layer to a single hidden neuron so in the first row we'll have W 1 and W 2 connecting the inputs to the first hidden neuron then in the second row W 3 and W 4 for the connections to the second hidden neuron and finally w 5 and w 6 connecting to the third hidden neuron so obviously the size of a weight matrix in this case 3 by 2 because it has 3 rows and 2 columns is determined by the number of neurons in the two layers that it's connecting ok now the inputs to the network will be stored in a separate matrix which I'll call X since this matrix only has one column I'll usually refer to it as a column vector now what we'll do is take the weight matrix and multiply it by the input vector following the rules of matrix multiplication we start by multiplying the values in the first row of the weight matrix with the corresponding values in the input vector and add them together this gives us X 1 times W 1 plus X 2 times W 2 we can then move on to the second row of the weight matrix giving us X 1 W 3 plus X 2 W 4 and then finally X 1 w 5 plus X 2 W 6 so we now have a nice compact way of representing the way the input values are fed through to the hidden layer we're still missing the bias values though so let's make a column vector for those and add that to our equation matrix addition is very straightforward we just add the corresponding elements together so b1 gets added to the first row B 2 to the second and b3 to the third finally we need to pass this through our activation function f of X this just equates to passing each of the values through the activation function individually what we've ended up with here is a 3 by 1 matrix or column vector which of course corresponds with the size of the hidden layer these values are the circled activations of each of the neurons in the hidden layer and so only in the vector a1 okay now to feed these activations through to the next layer of the network we'll need another weight matrix this time with the size of 2 by 3 as well as another bias factor and then we simply do the same process as before giving us a 2 is equal to F of W 2 times a 1 plus B 2 since we've now gone through all the layers in the network this is our final output note that this output is exactly the same thing as what we arrived at last episode the only difference is that it's now in matrix form ok so let's now implement this in code I'm going to be using Python for the series version 3.6 in particular and we'll be using numpy arrays to represent our matrices so make sure you have numpy installed as well okay so I'll create a new file call this neural network dot PI and save that in my digit recognition folder and here I'll start by importing numpy call SNP for short and then let's create a variable called layer sizes this is going to be a tuple and if I put in here say 2 comma 3 comma 2 that would mean a network with 2 input neurons 3 hidden neurons and 2 output neurons and if I added say a 5 here now we'd have two hidden layers one with three neurons one with five neurons all right and then go to create weight shapes this is going to be a list holding the shapes of each of the weight matrices so the first one should be 3 by 2 then 5 by 3 and finally 2 by 5 so in here I'll create a shape a comma B so a should go from the second element of layer sizes to the end and B should go from the first element to the second last so you can say for a comma B in and then the values of a will be layer sizes and I'll use slicing to say from element with an index of 1 to the end and then the values of B will be layer sizes from the start to the second last element or element negative 1 now in order for this to work we need to zip these values together just using the zip function like so all right so let's just print out weight shapes to make sure this is doing what we want so I'll run this and you can see we've got a list with these shapes 3 by 2 5 by 3 and 2 by 5 let's now go ahead and create weights which is going to be a list holding all of the weight matrices we can just fill all these matrices with zeros for now so I'll say numpy dot zeros and then we need to pass in the shape so I'll just say s for s in weight shapes all right I'd like to print out all of the weights I'm just going to do a little for loop for W in weights print W and I want these each on a new line so comma newline character and then I'll run that let me just expand the console a bit sure and you can see we've got our three weight matrices now initializing all the weights to the same value is actually a terrible idea because it turns out that when the network is adjusting its weights during training if they all have the same value then it will change them all by the exact same amount and it won't be able to learn anything interesting so instead we'll be using random numbers what often works well is to draw these random numbers from the standard normal distribution which as you can see is roughly in the range negative 3 to positive 3 with most values being closer to 0 okay so back in the code instead of numpy dot zeros we'll be using the random dot standard normal function so if we run this now you can see these matrices are filled with random numbers we can then move on to creating the biases so remember these are just column vectors with the same size as the and there's no bias for the input layer so you can create biases is equal to a list and for these we can just set them all to zero so I'll use numpy dot zeroes and we need a tuple for the shape so this will be s comma 1 for s in layer sizes not including the first layer let's now put this into a class so up the top here I'll say class colors neural network and then I'll define an initialization method so that's two underscores in it followed by two underscores and then we need a self parameter for the instance of the class and let's also then have layer sizes as a parameter so I'll delete layer sizes from here and I mean just indent this correctly and I'll delete the print over there and in front of weights and biases I'll just add self dot like so just that these variables can be accessed from elsewhere in the class next up I'd like to create the activation function if you recall from last episode we're using this sigmoid activation function equal to 1 over 1 plus e to the negative x where he is just the constant 2.7 something so of the final function called activation this will take an X and return 1 over or 1 + num PI dot exp of negative x so the exp function just returns e raised to the given power I want this to be a static function so I'll just add the static function decorator here like so I now want to create a method for feeding inputs through the network the output of this is going to be the network's predictions so I'll call this predict it will take in self and a which will initially be the inputs to the network we then want to loop through all the weights and biases so I'll say for W comma B and it will zip together self-taught weights and self-taught biases and then we can say the activations for this layer are equal to self dot activation of the matrix multiplication so numpy dot matte mole between the weights and the previous layer activations or if this is the first iteration of the loop than the original inputs to the network and then on to that we just add the bias factor finally we can return a all right I'm going to save this and to test it I'm going to create a new file which I will call maybe just program save that in the same folder as the neural network and I mean to import a neural network call that may be NN for short and I'm also going to important numpy once again as NP and then go to create layer sizes over here just set this to whatever may be 3 comma 5 comma 10 and then I want to create some dummy input to use so I'll just set this equal to num PI dot ones and the size of this needs to correspond with the size of the input layer here so I'll say layer sizes 0 by 1 and then could recreate the network this is equal to n n dot neural network passing in the layer sizes you can then say that the prediction is equal to net dot product passing in our input finally let's print out that prediction so I'll run this and since I set the size of the output layer here to 10 we've got these 10 values being printed out so you can imagine for doing digit recognition then we could treat this first value as the likelihood that it's a zero the next is the likelihood it's a 1 and so on so here it looks like this point 9 is the highest value so we treat this one as a zero all right I mean to delete this print out over here and go back to the neural network script and I had just to demonstrate something quickly I'm going to create a variable called Z and we set that equal to this stuff here that's being passed into the activation function and I'll just print out the first value in that factor for each layer so I'll go back into the program and if I run this you can see getting values like negative 2.6 noir point 3 I'll run this again one point seven two point two the point is the values are relatively small but now what happens if I increase the layer sizes say to a thousand inputs and 500 hidden neurons if I run this now you can see I'm getting values like 39 negative 19 I'll run it again negative 34 18 you can see I'm getting lots of large negative and positive numbers one thing to keep in mind here is our activation function we prefer our inputs to the activation to correspond with a relatively large slope because as we'll see later the magnitude of the slope actually affects how fast the network will learn so if the input to the function is too small or too large you can see the slope is really flat and so the network will learn more slowly so going back to the neural network class or we want these weight values to be smaller when there are more inputs to a layer so we can divide each of the values in the matrix by s1 which is the number of inputs to that layer now perhaps a little unintuitive li it turns out that we actually want to take the square root of this number which we can do by raising this to the power of 1/2 I'm not going to go into the reasoning behind that but there will be a link in the description if you want to read up on it the point though is that now if we go back here and run this you can see we're back to our nice a relatively small values point six eight point one seven negative point nine nor point eight and so on and for change these back to the original sizes and run this nothing's changed they're still in the same sort of range okay you will basically done for this episode I just quickly want to go back here and remove this print statement save that and that will be all so see you next episode Cheers