it's a great pleasure today to welcome back an old friend professor Brian Kernighan now of Princeton University previous live Bell Labs for those of you would like to know how we met in the first instance we have already prepared a computer file video on this very topic you can refer to the great - OH - jailbreak which is all about computerized typesetting in the very early 1980s so welcome back Brian it's only been 30 odd years since you last spoke to us here and we're delighted to have you here in the more intimate computerphile context perhaps I could begin by asking you just a little bit about what was it like to work at Bell Labs right so first let me say it's actually a pleasure to be back at Nottingham and this is not the same digs as you had 30 years ago so it's a definitely fun to be back what was it like a Bell Labs well I first went to Bell Labs I think in 1967 I had I was at the time a graduate student in computer science except it was before computer science it was Electrical Engineering at Princeton and I was lucky enough to get a summer job at Bell Labs and so I spent two summers there actually working on a couple of different things one of which became my thesis but it was a wonderful place because there were an enormous number of really good people doing really interesting things and nobody telling you what to do that's kind of a rewarding environment and so it was so good that when I graduated from Princeton I didn't even interview any other place I just said ok I'll go to Bell Labs they offered me a job why not do it and that was a decision that was extremely lucky and I've never regretted it oh yeah what it was like was I would guess that in one single largest building there were probably 4,000 people of whom about 2,000 were probably PhDs in various forms of science physics chemistry materials and then on the call of the softer and mathematics and the relatively new field of computer science at that point large number of really really excellent people and the environment was you can do anything you want because the revenue source for Bell Labs was very stable it was part of AT&T which in effect provided telephone service for the whole country and the way it did at that time AT&T was a regulated monopoly which meant that they had in effect a guaranteed rate of return and they peeled a tiny piece of that off for research to improve telephone service in the country that was the quid pro quo and because of that there was no shortage of resources and no management direction that said you have to do something that will save the company and the next quarter that was really very satisfying and so most people worked on things that were in some way long term or at least whose immediate application wasn't obvious hard to beat that environment I think of and I think I was there for over thirty years and I was never once told what I should be working on the way it worked is at the end of each year you had to write down on one side one piece of paper what you had done during the year and they used that to determine how much they'd pay you next year okay yeah so I you know I'm all in favor of this benevolent monopoly environment because after all he gave his eunuchs and as we all know eunuchs changed everything it really really did in the computer science field but one of the recurring themes of all your work over the years was if you like little languages and software tools in the - OH - jailbreak movie I think I referred to your pick you know a little preprocessor for doing line diagrams within types of documents but that's by no means the only one so again was this just you or was there a lot of mutual interest in the little language is things joined together with UNIX pipelines that sort of approach I think that the interest was perhaps latent there and then when the pipeline mechanism was invented which was some combination of Doug McIlroy with an idea and Ken Thompson with an implementation when that came along people started to think I more consciously of how you could combine programs so that this program could do something and produce output that went to this program and the combination would do some richer and more interesting I think it's hard to in particular that showed up in some of the document preparation tools fairly early on because among other things there were limits on how big a program could be this was hard to think back that far but a big machine at the time would have 64 K bytes and K bytes by the way not an bytes or G bytes and so that meant that any individual program could not actually be very big and so there was a natural tendency to write small programs and then the pipe mechanism basically input output redirection made it possible to link one program to another and certainly for the work that Lorinda cherian I did on eqn which was a language for typesetting mathematical expressions the existing formatting program tear off sort of the precursor of things like tech today that was already a big program by the standards at the time it just barely fit in the memory for user programs and so adding more to it was not feasible you couldn't actually do it and so what we did instead was to create an entirely separate program connect them with a pipeline and then if the program is going to be separate you can have it represent recognize a different language it doesn't have to actually do the same language because it's a separate program and I think that that was a place where perhaps necessity was the mother of invention that it actually worked out quite well and then further document preparation programs you mentioned pick a moment ago further document preparation programs had that same property that up they were separate programs separate languages tailored to the particular job that they were trying to do and so that separation of concerns worked out really well forced on us in some ways by not having very much in the way of computing resources so although it's postponed you was a great sense of surprise about wow you actually gained power and flexibility doing it this way yeah and I think people started to think I know I remember fairly clearly at this point that when the pipe mechanism was first invented and it was literally not more than a day or so of existence in some sense people started to say hey wait a minute this is very easy we can connect programs together but we may have to change the way that programs are implemented in a trivial way so that they read from their standard input guaranteed they write to their standard output guaranteed and then you can glue them together and there was this burst of making straightforward changes to the program so that they would fit in pipelines and then people doing interesting pipelines with them in inventing crazy combinations that did things one of the finest examples was one that Steve Johnson did for finding spelling mistakes potential spelling mistakes in a document you take a document you convert it into words that's one program you sort the words that's another program you compare them against the dictionary that's another program and you print the ones that are in the document but not in the dictionary and that's another program and so this multi element pipeline was I first caught at a spelling mistake detection program yeah I suppose the thing you also have to learn as I found my cost by experience is you've got to be somewhat careful when a thing has to be in a pipeline but has to be transparent you know when you've all always got to think about how like what I'm sending down the line fouled up by being picked up too early by something that I didn't expect but there again small price to pay but there's a there's a nice example of that in sorting if you think about it as if you have a sort program in the middle of a pipeline it's a full stop because it can't the downstream programs can't do anything else until the start program has done its job because it has to see all the input before it can sort but the fact that it's a full stop doesn't matter to the user it looks like it's just a continuous process and so the packaging of that is actually quite important to do it well I think when I first came to UNIX I was pretty familiar with the idea of what used to be call the job control of return so there was no great surprise in encountering the UNIX shell but again by the time we're talking about late version six UNIX version seven late seventies early 80s all of a sudden it was as if from nowhere you've got the idea of other flexible interpreted languages and I'm thinking of course primarily of all so how did that come about right so I think part of it you mentioned the shells I think part of it comes about because the shells were used to combine programs and you could do other things with the data as it was passing through in a very simple way but the shells were not very good programming languages and so you wanted to do more with them aughh itself had three or four different inspirations I think one of them not as well advertised at this point I guess was a language done by Mark Rock and which he had designed a language for validating data that was came from telephone switchings systems or something like that where all it did was to look for patterns of input and when it saw a line that matched one of those patterns it printed it because it was died or perhaps didn't match it it was some kind of error and it printed it so it was in effect a very simple streaming program so that would be one influenced from the outside and I had been interested in somehow having a programming tool some kind of tool where I could manipulate text and numbers more or less together kind of equally easily see programs were good at numbers but very bad at text and there and the shell wasn't really very good at text and terrible at numbers so I was kind of interested in that sort of thing Alejo who was in the office next to me I was very interested in regular expression technology he had just finished egress extended version of grep had much richer set of regular expressions and very efficient implementation and Peter Weinberger who was on the other side of Al was very interested in database related things he had just finished doing a tiny relational database for UNIX so here's this combination of three people write together interested in things that sort of lose we are related to each other but not much and so we started talking about what what do you want what could we do what could we build here that would be an interesting tool and then some weird way is a generalization of CREP and said it includes a much richer set of regular expression and stuff and there's this database flavor Peter knew about RPG the report generators thing and so the I don't where you call it the great combination or the mishmash of these things put together led us to designing awk and then doing the first implementation wasn't a great deal of work I think at this point my memory is right Peter did the first implementation of it over a weekend server three or four days at most a lot of it was easy because we had the right tools at hand in particular we had yak which was the parser generator that Steve Johnson and L hey-ho built that made it easy to define the syntax of a language so that you could parse statements in that language and then you could hang semantic actions on it and that existence of yak led to a whole bunch of different interesting tools in the UNIX environment eqn which we talked about a few moments ago was done with yak for example pic which you mentioned done with yak so it was a very very effective tool for building new languages and so that's another reason why I think people thought in terms of language and I have a tool it makes it easy to build languages so let me just do it my memories of a feeling of the liberation of setting up a sad and all pipeline driven from the shell I just thought it was absolutely great and yet the scene out in the big wide world although there's a lot of admiration for the work he did that some people felt that the integration might be a positive advantage I'm thinking of course a pearl and what's your take on this sort of trade off if you like between having everything integrated so that all the communication between what's happening is internal and therefore perhaps more efficient in quotes I don't know versus doing it the pipeline way I think it's a trade-off I think it's precisely a trade-off that there's no single right answer it depends on the application there's some things for which as a quick and dirty sequence of separate programs works really well you just do it you're done you don't have to think about it if you're doing something more complicated perhaps something which is going to last longer then integrating it into a single program is probably a better choice but I without looking at a particular situation I don't think I would know which one to use Perl was definitely built as a reaction to awk awk had a lot of limitations at the time and some of the limitations were artificial because the version that we were using internally was not the version that was available outside 80 and its wisdom decided that they wouldn't release the richer version and so some of what Larry wall did would have been obviated by a newer version of our but then of course he added enormous number of other very useful features to it and made it a much richer scripting language and one which you could build bigger programs I think one of the issues for all of these is how big is the ultimate program going to be or even the one that you're building right away if you're doing something that's only a couple of lines long awk is great and it really is I don't think anything beats it for one-liners and where one is a metaphor for two or three but when you get bigger than that it doesn't scale very well because it doesn't have any of the things that prevent you from doing something stupid or at least finding out when you did and so for bigger programs you want something that has more going for it and I think what's going on in modern programming languages is more and more of that kind of yeah I think I see a little bit of that what I loved about of course was this pattern matching a bit and then match from that pattern you took that action but again I am the Hardaway that what can be sometimes rather awkward is if in the combined script you have two distinct routes through it for two distinct purposes it can be very difficult sometimes disentangling those when things go wrong as inevitably they do the pattern action paradigm is really really good it is a great model for certain kinds of computation grep is a pattern action program said as a pattern action program yak is pattern action program flex all of those tools have that same model here's some patterns and you read the input and if pattern matches do the corresponding action so they're all great that but there are plenty of things that don't fall into that if you want to make two passes over something or if you've got to remember complicated state doesn't work as well and what I discovered when I write lock programs is often rather than trying to figure out how to keep track of state from one liner to another on the input I read the whole input into a giant array and then do traditional you know indexing computations on the array it's just easier and again having the associative facility in matter array that you can index into it via strings and so on that's another big plus as well but this business of you having a powerful version of what you're doing at Bell Labs but the powers that be deciding I supposedly I guess under very great stress of external pressure as to exactly what should be released with a UNIX release to the outside world on the usual basis from what I gather that it's just not fair so there the computer companies were all these bright people producing this stuff but they cross-subsidize from our telephone bills we you as a research community aware of that problem all the time we were certainly aware of it I don't think we'd paid a tremendous amount of attention to it but it came up from time to time it came up I think the reason that the arc version where the official mechanism didn't want to release a newer version of lock was a perfectly legitimate concern with standardization you want to have something that other people can count on its behavior properties if you put out a new version now you have two versions and there's potential confusion in the market and they weren't ready to do that so it was a combination of trying to present a unified version of something and then wanting to actually own what it was that they were putting out and supporting so I think that one was legitimate the cross-subsidy issue I think for people in research basically didn't really show up I was much more of a legal concern and I think most of what was going on in our particular domain was sufficiently down in the noise that it didn't affect us the it did probably affect the people who were doing unix commercially which was not the research people but the research people started with what had been done and sorry the commercialization people started with what had been done in research and then of course added lots of things that made it a better commercial product and they definitely I had various times had to worry about this cross-subsidy issue comes back to the fact that 18 T was a regulated public monopoly getting a guaranteed rate of return on their investments and so yes are you cross subsidizing by competing using money but other people would forced to buy your product with yes I remember well going to presentations about our licensing terms for UNIX and being warned about how careful we had to be not let commercial usage get mixed in with our that you get into usage and so on but um do you think in the end that had that not been so restrictive and owner respond AT&T that it that the need to develop Linux might not ever have occurred that is an excellent question I don't think I have an answer I certainly got an informed or accurate answer I think probably if there had been the equivalent of open source at the time really open source then perhaps there would be less different versions of unikz floating around and there might not be one done by this really bright student in finland yeah I think possibly that would have changed the game because I know that some of the software development that went associated with UNIX broadly was people trying to go around the ATT licensing considerations yeah because we encountered that in our typesetting adventures that basically the whole business of reverse engineering jailbreak and whatever you want to call it that you did was really motivated by the fact that the whole concept of openness just wasn't there in the seventies notice you just had to have closed system there was no way out of it did you realize at the time how pioneering you would be in a way about all of this no I don't think so certainly not on the typesetting front where I don't think it was pioneering it was simply trying to remove an impediment we couldn't use the typesetter with the software that came with it was unusable and so necessity again had to do something to make it so that we can actually do what we wanted to do because their software systems weren't up to what we wanted to do more broadly the openness and that you see today in language design and operating systems and so on I don't think we predicted that in any way either it's not clear how you make money doing that and of course a lot of systems at the time certainly in the 70s and 80s this was the lifeblood of the companies that provided the service and so they couldn't just give it away and make it back in what do you make it back in public relations ah now you have to make it back by turning yourself and saying you can get the free version but if you want really good backup where the people to give it but of course that's not a stance that companies were prepared to take in those days again I mean as we both know it happened with computer hardware so they have McGarry typesetting hardware they could not any visitor scene where they didn't sell the hardware but little with the software yes that certainly was the model for a long long time and in fact in early days the IBM days let's say in the 60s and 70s IBM made all of their money on on the hardware and the software was kind of given away as part of it yeah there's all sorts of ramifications over the South Atlantic about when will I be n before was to unbundle I remember the worst pretty clearly indeed anyway after all I get impression that then over the next few years you moved into other areas of interest to be around for a while and things like total TK which I've never used so could you say something about that yeah tickled TK is it there's really two pieces that this this is just wonderful work by john ousterhout who at the time was at berkeley now at Stanford and what John did tickle TCL the tool controlling which was basically a an interpreted language of a scripting language if you like that made it very easy to extend by adding C functions so that the essence of it was that you could write code in this but you would be mostly calling functions that have been written in C as a language it's bizarre that I've told John that on a number of occasions but it was very very powerful you could actually do a lot with it and then TK was an instance of a set of C functions that would do something interesting and it was basically a set of graphics routines that worked with x11 so that you could very easily draw things of real complexity of richness on a screen and I think as a graphics package it is a great improvement of all of its successors it's just a remarkably efficient effective and rich environment so I used that for a number of years doing quick and dirty user interfaces for a variety of things and even one that was used for a while inside Bell Labs there was a project ill-conceived I think at least in retrospect for doing wireless systems that would work well inside buildings especially big buildings like say hospitals and big stores and so to do that you need something that will tell you how electromagnetic radiation propagates within the building so you have an elaborate model for e/m propagation you have to know where things are in the buildings where the walls what are they made of what are the floors all of that kind of thing and then from that you can say if I put base station equipment routers if you're like in various places the building then I can figure out what the coverage in the building is but that's all kind of low-level radio sort of thing how do you make that accessible to somebody who might be you know an engineer trying to design this stuff or somebody with not any of the real radio background and so what we did was to design a system that had all of that prediction of propagation the models and on top of that a user interface which would show you slices through the building show you where the walls were and things like that and say if you put base station there this is what the propagation is going to look like over the building and then in addition we could even optimize that so that it would tell you this is where you want to put the base stations given the number you proposed to do and I wrote that user interface that was that was my part of it I wrote that user interface in tickle TK and it was really really effective and surprisingly easy I remember being on a conference call with some people who were the ultimate users intended users of the system and they said you know we really need a feature to do something I don't remember what the feature was and they kept talking about the feature during the conference call and while they were doing it I was sitting in my office I implemented it in TK because it was just so easy it's really really great so this was your translation into the world graphics terms in a way because early in those days yeah because we finally had terminals that were just text exactly and I think one of the things that was of interest great interest was you could see the huge strength of Unix in the days of dumb terminals if you like but how was it going to cope with the great graphics onslaught in the late eighties now I've been looking back on it were all sorts of internal projects on your own terminals and so on were they not yeah we had we had weird terminals the the one that I remember most clearly was the Tektronix 40:14 some random number I can't remember there's lots of random numbers in computing but it was tech a big thing big green green screen and um but it had a sort of vector graphics kind of thing where he could draw lines you couldn't do much beyond that no shading or anything and so there was some graphics done with that and then eventually we started to get color terminals that were bitmap not vector aw and so there was a lot of interest in software to drive those sorts of things I didn't do very much of that by that point I was using other people's libraries because that's a fairly specialized technical area to do that well but in that sense that was really where collaboration outside of Bell Labs had to come in for things like X windows yes so to become a standard yeah yeah so we reasonably cool about as what Bell Labs you didn't feel that you ought to be totally dominating this field I don't think I didn't care in the slightest um there were people around me who I think thought that X was very big very complicated and perhaps unnecessarily so I know that Rob pipe in particular did some really good work with overlapping bitmap graphics so that if you have you know two windows on the screen and you do something with them that you get a very efficient rapid update and he did some of the very first work on that very efficient algorithms that I guess he eventually found their way into X and so there were people interested in a bit blit the technical thing for bit mapping and but I had drifted out that at that point and so I didn't pay much attention to it I think I've hold a lot of questioning for far too long Brian so actually cameraman Shawn the computer file people will know he is expert at shouting questions as well as doing the filming so over to usual we took his reference manual which is a really excellent reference manual as a big piece of it took that verbatim and then we feel job for a couple of summers and that paid off both in future context but also given something to do for my thesis which I think Brian probably muttered under his breath what you don't know is I've got a secret weapon