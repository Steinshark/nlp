In 1970, Intel introduced the first 
commercial Dynamic RAM product. Its success and unique memory cell structure 
kicked off the race for larger memory capacities. This was done by shrinking the 
memory cells. But in the 1980s,   shrinkage got harder. So 
things had to get ... creative. How is it that Memory makers always 
have all the fun? In this video,   we look at the Dynamic RAM industry's ridiculous 
journey from 2D to 3D structures and beyond. ## Capacitor A Dynamic RAM memory cell stores 1 bit of data 
in the form of a charge within a capacitor. A capacitor is a passive device capable 
of storing electrical charge within a   field. Passive, meaning that it does 
not use electricity. Basically two   conducting plates - electrodes - separated by 
an insulating material called the dielectric. An electrode-insulator-electrode hamburger. This charge can be mapped to a bit of 
data. If the capacitor holds a charge,   that is a 1. No charge? That is a 0. The memory cell has a single access 
transistor that writes and reads   the charge. After that read operation, 
the transistor also needs to "refresh"   the charge because reading the 
capacitor destroys said charge. Refreshes are also done periodically 
since the charge leaks the capacitor   anyway during normal operations. I was surprised   to learn that these refresh periods can 
be as short as a few dozen milliseconds. This one-transistor design - invented by 
Robert Dennard at IBM - is very bare bones. By contrast, a typical memory cell for Static RAM 
or SRAM has six transistors. Two cross-coupled   inverters - each two transistors large - storing 
the bit, with two more access transistors. But the bare bones structure of the 
one-transistor Dynamic RAM memory   cell is also its genius. It lets 
you stuff together as many of them   as possible - setting the stage for 
years of aggressive memory scaling. ## Intel and Moore's Law Intel revolutionized the semiconductor 
memory industry in mid-1970 when it   released its 1-kilobit Intel 1103 chip. It heralded the first semiconductor   memory with high storage and good 
performance but at a cheap price. Accompanying this, Intel set its product 
cadence to Moore's Law. What Moore's Law   means nowadays depends on whoever is 
saying it. But in early memory history,   it meant increasing the number of bits on a 
memory chip by four times every three years. Being on the leading edge, grants 
the manufacturer a cost advantage on   a per-bit basis. And since memory is a commodity,   that edge on cost gives that manufacturer a 
significant economic advantage over its rivals. ## Capacitor Problems We scale capacity by shrinking the 
1-transistor memory cell. But one   key design area of concern in 
doing this is the capacitor. When we read out data from the memory 
cell, we send the charge out of the   capacitor through the access transistor 
into a wired connection called a bit line. The charge enters into the bitline and then 
changes its voltage. We read the change in   the voltage of the bitline to determine 
whether the capacitor had held a 1 or 0. Voila! Data. And also why the read operation   is destructive to the capacitor’s 
charge and thus requires a refresh. The charge tends to be very small even in 
the best of times. But it needs to be big   enough to be detected even in the presence of 
various random background radiation sources. So the capacitor's ability to hold the charge 
- or capacitance as it is called - is, like,   really important to the cell's data integrity. But in order to align with Moore's 
Law, we need to shrink the memory   cell three times over per generation. A 
significant portion of that is granted   via photolithography advances, but 
the rest has to come from design. ## End of Planar At the start, the industry used planar 
transistors for their capacitors and   memory cells. Here, the capacitor lies flat. As the industry moved from 64 kilobit in 1982 
to 1 megabit in 1985, they mostly scaled by   shrinking the size of the planar memory 
cell. But this eventually caused an issue. The capacitor's capacitance value can be 
calculated using the following equation. E = MC squared. Just kidding. It's really ... C = K * A/d Where K is the dielectric constant 
of the material between the plates; "A" is the area of the 
capacitor's electrode plates; And "d" is the distance between the plates; As the capacitor physically 
shrank throughout the years,   the size of the plates decreased, 
hurting the capacitance. But the memory makers managed to keep 
the capacitance high by also shrinking   the distance between the plates. 
However by the 1 megabit generation,   that tactic hit its upper limit for 
reasons I won't get into right now. Ergo, to continue the shrinkage they 
either needed to change the dielectric   material inside the capacitor to one with a higher 
constant. Not an easy materials science problem. Or find some other way to increase the area of 
the plates. So scientists realized that a new   3D transistor structure would be necessary for 
the next generation of Dynamic RAM - 4 megabit. ## Stacked Capacitors At around this time, two designs 
emerged as contenders for the 3D   step forward - the stacked and trench capacitors. The stacked capacitor was first conceived in   1976 by Mitsumasa Koyanagi of Tohoku 
University, then working at Hitachi.   We essentially "stack" the capacitor on top 
of the access transistor and the bitlines. He worked through the various practical issues   and in 1977 fabricated the first test 
chip using a 3 micrometer process node. Two years later in 1979, Koyanagi 
fabricated a 16-kilobit Dynamic RAM   using a stacked capacitor cell 
- proving that it can be done. ## Trench Capacitors A few years before that in 1974, 
another Hitachi researcher named   Hideo Sunami was attending a 
conference by Texas Instruments. In it, TI demonstrated a solar cell 
with deep trenches. The presenter   predicted a forthcoming issue with this 
trench structure involving capacitance. Hideo Sunami's work at the time was in 
photoemission spectroscopy and so had   little to do with memory. But he was a radio 
enthusiast and recognized a trimmer condenser. A trimmer condenser is a small 
variable capacitor that we use   to adjust the capacitance in a circuit. 
You screw it with a screwdriver and it   varies the overlaps between two conductive 
surfaces - ergo changing the capacitance. Sunami put the ideas together and invented 
the trench capacitor memory cell. Essentially,   the key with the trench is that you put 
the capacitor below the access transistor. Sunami applied for and received a Japanese 
patent in 1975, but did not try for any   other overseas areas. After Hitachi 
won the 64-kilobit memory generation,   they decided to focus their R&D on new 
structures to replace the old planar cell. After several years in 1982,   they presented a trial 1-megabit class 
Dynamic RAM memory using the trench cell. ## Stacked versus Trench As I mentioned, we needed a 3D capacitor 
structure for the 4 megabit generation,   scheduled for the 1986 to 1990 time frame. Each of the major memory makers had to figure 
out which 3D structure they wanted to adopt.   Stacked or trench? Put the capacitor 
above or below the access transistor? Trench capacitors offered certain advantages 
over the stacked capacitor. First,   because the trench has four sides and a 
bottom, you can increase the size of the   plates. The capacitance might be far 
better than with stacked capacitors. Second, because the trench is just a hole 
in the ground, you can put a lot of them   close to each other. So there are considerable 
upside benefits with regards to cell density. But there are considerable downsides 
too. First, manufacturing. The trench   is harder to mass-produce. We need to etch 
them using a special type of anisotropic   etch - reactive ion etch. Which was 
still an immature technology at the time. In addition, there were issues maintaining 
the uniformity of the insulator layer and the   whole trench’s stability. And also because 
the thing was just a hole in the silicon,   it was difficult to measure progress 
on the trench manufacture - metrology. On the design side, the trench had two serious 
data integrity concerns. When we bunch the   trench memory cells too closely together, the 
current in one trench can "punch through" its   dielectric insulation or leak into its 
neighbors. This damages data integrity. And then there was the soft error data 
problem. Alpha particles emitted from   random natural radiation sources - 
like cosmic rays from space - can   hit the capacitor and change its stored charge. This causes a "soft error",   damaging data integrity. And while this 
happens to planar Dynamic RAM cells too,   the first variants of the trench capacitor 
were particularly susceptible to it. ## Two Camps Anyway so there was the dilemma. The trench potentially offered long term benefits,   but also suffered short term costs. The stacked 
capacitor got you to the market faster but density   benefits lagged that of the trench. So you 
might need to switch to the trench anyway. The industry split into two camps. Fujitsu and 
Mitsubishi went with the stacked capacitor. As well as Hitachi, despite their employee 
Sunami being the trench capacitor's inventor.   The reason for this had to do 
with the soft error problem. Being a vertically integrated company, the 
memory they produced was consumed by their   mainframe computer business. Hitachi's 
mainframe people were concerned about   trying to figure out the trench's potential 
soft data errors before shipping to meet   the release date. And so, they went for 
the more conservative stacked capacitor. The Korean memory makers for their 
part - Samsung, Gold Star or LG,   and Hyundai - all went for stacked capacitors 
for their 4 megabit generation memories. Samsung in particular chose to go 
with the stacked capacitor because   it was the easiest to manufacture and 
for them time-to-market was paramount. ## SPT On the other hand, NEC, Toshiba, IBM, Siemens and 
Texas Instruments went with the trench structure. This was because IBM had created 
an interesting new variant of the   original trench capacitor: The 
Substrate Plate Trench or SPT. The Substrate Plate Trench filled the etched 
trench with a layer of polysilicon - making   it one of the capacitor's two plates. The 
second plate is the silicon substrate itself. Thus, the charge is stored inside the 
trench itself. This is in contrast to   the first trench capacitor, which stored 
the charge within the trench's lining. The SPT simplifies the wiring and helped IBM 
overcome some of the trench's aforementioned   challenges. Siemens and Toshiba adopted the 
SPT through their research alliances with IBM. Despite IBM's innovation, the stacked 
capacitor's superior manufacturability   benefits eventually won the day. Most 
of the companies that adopted the trench   structure eventually dropped out of the 
commercial standalone Dynamic RAM business. But IBM and others have continued working on 
trench technology for embedded DRAM modules   or eDRAM. Embedded, meaning that these memory 
cells are put right on a system-on-chip or SOC. There, it can offer more capacity and data 
transfers than traditional on-chip SRAM cache.   Since the SOC is space-constrained, the trench's 
advantages in density are more relevant here. There are also some manufacturing 
benefits. Because you dig the trench   right into the silicon substrate, the 
access transistors can be placed on   the same horizontal plane as 
the SOC's logic transistors. The fancy industry term is "fully planarized". It 
helps with lithography. A flat wafer topography   helps lithography machines with shallower 
depths of focus do a good image transfer. ## Fin Capacitors The saddest thing about Moore's Law is 
that as soon as you ship one generation,   you got to do the next one 
in just a few short years. Soon after the 4 megabit generation 
in 1985 came the 16-megabit in 1988.   Different parts of the industry had their 
own way to get to this very advanced stage. For instance, IBM was using the 
SPT capacitor. For 16-megabit,   they added a thick - as in 100 nanometers 
- insulating collar of silicon dioxide   around the trench to prevent them 
from affecting their neighbors. As for the companies that took 
the stacked capacitor pathway,   they followed a 1988 proposal by Fujitsu 
and modified their stacked capacitors to   create this interesting "Fin capacitor" - no 
close relation to the FinFET logic transistor. Its cross section kind of makes it look 
like a palm tree. The "fins" are made   from polysilicon and serve as the plates. The 
dielectric goes in between the fins. Building   this is very complicated. It involves 
a number of what we call "sacrificial   layers" - temporary layers that we have 
to first lay down and then etch away. As the memory-makers worked on building 
this new fin capacitor structure,   they also began integrating new materials 
to improve their cells' capacitance values. ## HSG The most critical of these materials 
innovations during the 64-megabit and   later 256-megabit generations was the 
Hemi-spherical Grain structure or HSG. This was implemented by adding 
these polysilicon grains onto   the stack capacitor's bottom plate to 
make it more "textured" or "rugged". In doing so, we increase the plate's 
surface area - which per our equation,   increases the capacitor's capacitance value. HSGs were a critical innovation win for 
the stacked capacitor. Trench capacitors   could not implement them because it 
required a heat baking step - which   compromised the trench's structural stability. NEC first invented HSG in the late 1980s or 
1990, but it was Micron Technology which first   ramped it up in 1997. The next year, NEC 
and Samsung introduced it into their own   64-megabit memory products. By 2001, 70% of 
Dynamic RAM memory makers had adopted HSG. ## Hitting 1 Gigabit As we turn into the new millennium 
and the era of 1 gigabit Dynamic RAM,   the industry sought new ways to increase density. Two major innovations emerged: 
Cylindrical capacitors and exotic   high-K dielectric materials. 
Let's talk about the first one. In the late 1990s, a new set of 
lithography machines - 193 nanometer   DUV light - introduced higher resolutions 
into the semiconductor engineering process. This in turn allowed us to 
mass-produce new structures   like the cylinder or cylindrical capacitor,   the concept for which was first unveiled at a 
1989 VLSI symposium by a team at Mitsubishi. The cylindrical capacitor is an evolution of the   fin capacitor and allows us to fully 
maximize the plates' surface areas. It is a ridiculous evolution, to be honest, and 
it seems to have been quite difficult to produce. The following details come from 
Samsung's technical sharing of   their 90 nanometer node back in 
2005 - a 512 megabit Dynamic RAM. The capacitor's cylinder is made from polysilicon. 
You etch it out of the silicon wafer using Deep   Reactive Ion Etching - an improvement of 
the Reactive Ion Etch I mentioned earlier. You then apply a two-layer dielectric - made from 
aluminium oxide and hafnium oxide - onto both the   inside and outside of the silicon cylinder. 
This is one of the capacitor's two plates. The second plate is found inside 
the cylinder - which is filled with   crystallized silicon and - I had 
a double take when I read this - a   layer of Titanium Nitride. Applied with 
I presume to be Atomic Layer Deposition. Oh, and we also need to apply the HSG, 
so we can stack on even more surface area   for the plates. Gosh this is ridiculous. And 
Samsung introduced this nearly 20 years ago. ## Exotic High-K Dielectric Notice the presence of these strange 
materials. It's not a fleeting trend. I mentioned all the way back that insulating 
materials with higher dielectric constants can   contribute to a higher capacitance. The 
industry started with silicon dioxide,   but abandoned that back at 
the 1 megabit generation. From the 4 megabit to 256 megabit generations, 
they experimented first with a stacked dielectric   of silicon nitride and silicon dioxide - thin 
composite oxide-nitride (ON) dielectrics. But for the gigabit era, they 
moved to entirely new things.   The first exotic substance used for the 
gigabit era was Tantalum pentoxide. This   was because the industry was already 
using tantalum in other bits of the   semiconductor manufacturing process - and 
so we knew how to deposit it at nanoscale. Unfortunately Tantalum pentoxide's dielectric 
constant was only 25. Which was about 3 times   the 6 to 7.5 values of the previous composite 
oxide-nitride dielectric layers. So it would   only be useful for 1 generation of 
bit shrinkage - remember 4 times. Therefore, industry engineers started working 
on super-exotic materials like Barium Strontium   Titanate which will also require developing brand 
new deposition methods like metal organic CVD. With EUV Lithography unavailable for much of 
the gigabit era, the industry heavily leaned   on these strange exotic materials to grind 
out more memory cell scaling improvements. ## Conclusion I have to be frank, this video 
spiraled out of control near the end. I am rather taken aback by these 
rapidly evolving 3D structures   in DRAM. It is different from things on the 
logic side, where we only moved to the FinFET   transistor in 2011. And now 15 years later, 
we are only now moving to Gate-All-Around. I suspect that DRAM's nature as a commodity 
forced the memory makers to aggressively pursue   these ambitious 3D capacitor structures. And after 
that - these exotic high-K dielectric materials. It is a tough business, and it makes me 
appreciate the ridiculous complexity of   these advanced memory semiconductors. 
Memory is often the chopped liver of   the industry. I should spend 
more time on it. And I will.