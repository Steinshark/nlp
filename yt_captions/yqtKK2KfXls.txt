so I have to confess something to you when all the noise started to kick off about chat GPT I kind of dismissed it as hype you know it was fun sure it was a lot of fun but it was another AI toy like those image generators that was here today but would drop off the radar in six months but then I couldn't help but notice how many of my friends kept talking about how they were using it not how exciting it was not how cool it was not how it was going to change the world oh my God we're all going to the Moon hooray they were actually talking about being users how it was helping them to get stuff done day to day and that's when I started to pay more attention and wouldn't you know yeah I've become a day-to-day user myself take the number one thing I use it for it rewrites my LinkedIn posts it's just way better at that kind of corporate to use tone than I am so yeah I get AI to leverage my synergies these days while I've started to see it as an actual useful tool until recently I assumed it was a tool that was owned by people with access to supercomputers or at least massive AWS cluster budgets not so I recently got talking to Toby Funk heinle and he's opened my eyes this stuff is now firmly going into the hands of regular Developers we're about to see personalized AI on our desktops because we're about to have the tools to hack them together ourselves and I think it probably won't be long before you know when you check out a new programming language you end up installing a language extension that ships with syntax highlighting and LSP and debugger support and a custom AI model that's been trained specifically for The Language by the extensions developer I can see it coming I think by the end of this episode you'll be able to see it coming too and if it excites you as I hope it will Toby's got some explicit tips on how to get started building this stuff so let's figure out how to put AI in our hands I'm your host Chris Jenkins this is developer voices and today's voice is Toby funkina [Music] joining me today is Toby funkinel Toby how you doing oh pretty great thanks how are you okay I'm good I'm glad to see you we saw each other in person last week which is a rare treat for me as a podcast host and the week before yeah yeah we did actually you were in England the week before and I was in Germany the week after so who knows what we'll do next week I I one of the things we got to talking about when we first met was um openai right I've always thought of open AI as this enormous very clever database of billions of whatevers that is the state of the art and completely out of our hands and this tool we use and then you mentioned to me this blog post from someone at Google um we have no motor I think it was called exactly and it basically said to paraphrase oh God we're being hosed by the open source world yeah and like this is what I want you to tell me about how can we run our own open apis the open source side is sufficiently Rich that we can get involved yes so the state of the technology technology today is that uh it's possible even to run llms on Hardware that do not even have a GPU that's uh that's how far it has progressed uh first the note of caution that memo it was written by a Google employee and it was said to be a leak it was maybe not meant to be published and also it's a it's a solitary opinion but still it gained a lot of media coverage and at the time it was published meta had uh recently released uh some of their first generation large language models the Llama models yeah uh like I think spread like the animal only with a slightly different capitalization and large language model yeah of course uh meta is a competitor to Google in in some ways but uh what they had done is they open sourced the models they published them under an open source license and then um if I'm not mistaken they only limited the access to the model weights which are needed to uh to really use the models and uh they they provided researchers access to that according to a waiting list and then after a while like two weeks I think after they started in the beginning of March this year uh the weights also were uh well it said leaked they appeared uh to be uh people could download them via torrent at first and then the whole thing really took off yeah because suddenly there were reasonably capable models not at the level of gpt4 by far but capable uh and some of them could be run on consumer grade Hardware from the start and so everybody could suddenly get into llm research without needing a cluster without needing like some some membership in a research group or some some access that was limited to that point and it seems many people were eager to get their hands on that kind of technology and So within the span of a few months yeah since March multiple things have happened so let me just check I've got this right first so the model is like how do we configure this big clever neural network right and then the weights are like once you've configured it you have to run vast amounts of data through it to train it and you end up with the magic multipliers that set the weight of the neural net rate so you need those two pieces to do anything interesting uh yeah I I think so yeah yeah I need to be as with those llama based models at home but the thing I haven't done is to try to fine-tune it or to try to try to train a model myself from the ground up and um so I I might just be saying nonsense here but uh the yeah so the model is something that needs to be trained and then um what happens there um the state of the art in terms of llms it uses uh Transformer based architecture and uh it goes back to a paper from I think 2016 or something uh that that is titled um attention is all you need and basically um it's uh it introduces an attention mechanism in the way uh into the way that models are trained and then it's possible with an existing model to fine-tune it and I think what you receive there uh that's probably the way it's much better look it up as I say I'm gonna get into that at some point to work on it myself but right now uh the simple use cases they are interesting enough when it comes to applications and so um I'm I'm just retelling basically what happened um it started to be multiple things that made it easier for people to run um run their models on more and more Hardware um and uh lower and lower spec hardware and so at first there was a way to kind of if I understand it right modularized uh the fine tunings uh called Q Laura and that was even before I got into uh working with those models and by now it is um what has happened is the models have been re-quantized firstly yeah so right quantize quantization maybe um I quickly say something about that I think um the original llama models they were quantized with 16-bit numbers yeah as far as I understand it yeah so you are dealing with Vector spaces which have like a few hundred or a few thousand dimensions in current Generation Um large language models and um then words are represented or tokens which might be a bit shorter than words or sometimes might be multiple words in a few cases they are represented as vectors and those vectors they consist of numbers yeah like in a 500 dimensional Vector space you would have uh one token and it is assigned a vector which has 500 numbers and those those numbers they need to be multiplied a lot here those vectors are multiplied with matrices to do the whole processing of the large language model and basically that's something that gpus are specialized to to do they perform well on this so essentially you're downloading a giant Matrix database base of 16-bit floating Point numbers yeah yeah that's the way I imagined it and the quantization then um what what happens there is initially you have 16-bit Precision numbers a floating Point numbers and it turns out if you just uh decrease the the the well exactitude I don't know how we would call it English conversation Precision yes the Precision of the numbers uh to eight bit or nowadays even to four bit and then people are going even lower than that the llms still perform and deliver useful output and that of course means it makes a huge difference if you're multiplying a 16-bit number with a 16-bit number or if you're multiplying like a four bit number with a four bit number and then what you need to save in terms of uh what what needs to be stored in memory what what you need in terms of storage then uh how often the memory needs to be swapped in and out and then of course what kinds of Hardware are capable of running these models yeah yeah and so this is a big part of the recent progress that has been happening just decreasing the quantization and by that enabling more and more people to run those things on their own Hardware so people have been like down sampling these databases so you can run it on more commodity hardware and see if it still gives you a good enough results yes and some of the research in the academic field that's going on is also dealing with this and testing and comparing according to various benchmarks if I decrease the quantization further or if I try to become a bit smarter about the whole Vector space and maybe uh I'm a bit more exact in some areas and a bit less exact in other areas how can I get close to the 16-bit original models output quality with much much lower resource usage yeah yeah you would think uh as soon as you've published a useful database of 16-bit floating Point matrices someone who knows about compression will get involved right yeah yeah plenty of people uh got involved and some some really specialized into that there are some people who who are regularly uh like re-quantizing new models uh or fine tunings of the original llama models that appear then by now it's not only the Llama models that are there and since uh bless you and since the I think since the middle of May um there's a this Library that's called llama CPP which is based on C plus plus it's been around a bit longer than may but what happened in May is it started to offer a way to run uh the models in a mixed mode between CPU and GPU and people can just set a config I want to have this number of layers which are processed in by the GPU and are living in the video RAM and all the rest can live in the just the normal what is called CPU Ram in this context just the normal Ram of the computer and then if you have a model that is maybe a size of uh 60 gigabytes and you have a graphics card that maybe has a 10 12 gigabytes vram or 24 is pretty common with uh with the top of the line you know gaming gpus is a fairly widespread then you can just say Okay I want to have a part of it running in my GPU uh allowing in my GPU and then if it's maybe just a bit too big for the GPU Ram just a few layers need to be computed by CPU and then you get already very good performance and it's very flexible okay yeah because that kind of network neural network topology does lend itself to having multiple stages in different places yes but okay so here I am I recently bought myself a new Macbook and it has way more gpus than I think I need so if I want to put that to an AI what kind of I want you to tell me how to do it but first what kind of results can I expect on my little laptop yeah it depends on the specs I would say yeah so um I mean you nowadays you could basically I think with uh if you just have 64 gigabytes of RAM normally a ram in in that notebook uh then you can run all of the Llama based models um in in some quantizations like 4-bit or five bit I think even well with 65 billion parameters that's the biggest llama model that has been released I'm not quite sure but I think even that should fit the thing is the more that the CPU gets involved the performance uh in terms of token output token token frequency it decreases but the quality of the output does not decrease yeah so it doesn't matter yeah yeah in case you would think right logically we've spent all these years building these dedicated circuits for multiplying matrices so it can display polygons on the screen it's nice to know there's another useful reason for it yes yeah these specialized Matrix multipliers so what does this actually translate to I'm going to you're going to tell me how I can run my own local version of open in the API because one thing I would like to do with something like this is a problem with that open apis it never has that specialized data set you're interested in [Music] um yeah that's true and uh that's uh that's where Vector databases come in that's a it's another component that can be used in the stack uh just now you are saying open API I think uh you're referring to open AI right and sorry yeah yeah exactly so um I think openai also has various ways for people to uh to just post their their data to them and they will then host it and give give people a way to use it with their models I've never tried that yet but the exciting thing is that actually if you have your own data and it's just not part of any model that has been trained or fine-tuned and also you're you're not getting into fine tuning because it's still fairly expensive or maybe not that easy then you can just use a vector database together with an embedding model that's today a very very low threshold uh threshold of Entry very low entry threshold a way to uh to use your own data with llms and there okay how does that work what's the architecture there yes so there are embedding models um which are the purpose of them is to take some input which is a natural language uh most commonly and to translate it into vectors and yeah so for example I have a big book or I have a collection of documents then what I do is firstly I will do some processing on them they should be nicely formatted they are not not too many uh errors because then the the retriever would also suffer later on and then I split up that book into chunks and those chunks are individually processed by an embedding model and so each each of those document chunks they might be like 500 or 1000 characters long or something like that is also assigned a vector and those vectors they um they often if if the embedding model is well suited to The Domain and to the language that that the book is uh is published in um then those vectors they uh correspond somewhat to the semantics yeah so I can uh I can do when I have done this kind of embedding as it's called the connecting the Snippets with the vectors I can do natural language retrieval which means I can just write a question in natural language and um then this question will also go through the same embedding model and then uh will also be transformed into a vector and then uh what's what's fairly simple is to find uh what what is the distance between two vectors and so um then using that query um the document Snippets from before that are closest to the query Vector they are retrieved from the vector database and often it turns out they are cement centrically close to the query so you can have a big big document and quickly search it using natural language somewhat fuzzy and it's it's a lot of trial and error for people getting into it but also nowadays there are open source embedding models for download there's two tools like the language chain to to connect it all up and then Vector databases are also something that people can run on their local systems they are open source Vector databases so let me check I've understood this um so I get a vector database like maybe I the extension to postgres or something like wavy8 or something like that and I chop up my let's say my product documentation Apache Kafka documentation there's loads of it it's really hard to search I chop that up it gets turned into vectors of floating Point numbers in the database then I say how do you configure a partition in Kafka it turns that into a floating Point vector and then finds other vectors near it yes by the magic of indexes um yes indexes are important as well because of course if you have lots and lots of documents in the database and need to compute like the distance of the query to all of the other vectors that would take a long time so there are clever ways to do indexing and to like put for example the the closest vectors to to one vector that's already in the database in the index so that the search can happen quickly but yes that's that's exactly what happens and this is also exactly one of the use cases that could be treated by that that's cool okay so I want to get into how that relates to things like large language models but let's just check first is that language agnostic I mean human like if I want to do that in German would I have exactly the same experience uh probably not so there's uh there's different embedding models and uh there's this platform called hugging phase where you can find lots of uh different machine learning models in general they have embedding models they have large language models for download and for trying them out doing research on them and hugging phase also hosts a leaderboard and a leaderboard for the embedding models and then you can find uh which are the top performers according to various categories and most of the embedding models they are primarily uh focusing on the English language and so the availability there is much better but then the creators of those embedding models they I remember the one I'm using I I read the comments and there were was positive feedback for one of the English language specific models according to training anyway that they also perform well for East Asian languages for example and that's true yeah and I I tried German as well and it seems to work as well um I haven't looked into the details of that but I I would imagine maybe the training data of that model also just contains some amount of of content that is not in English yeah you have a book and maybe that book is for some reason multilingual and it's used to train that model or you have data from the web and then people post in forums I don't know what they use in multiple languages and then so coincidentally that also works yeah and for German uh I've tried it out with a model that is meant to be used with English and also it worked for me the output made sense and it's trial and error in that case okay that's curious so is that the piece I'm missing then is the embedding model that does that convert this chunk of text into floating Point numbers yes and then the vector database does the indexing and searching yes for similar factors okay so that's where I need to download this llama download this llama I love that phrase yeah um so when you have the vector database set up and have uh decided which embedding model to use and then maybe linked that all up for example with link chain which is a powerful open source tool to link various uh components that are related to uh machine learning models especially large language models then there's another leaderboard for example where you could start out of Open Source large language models and they they have been benchmarked according to various metrics and um yeah most of the models on that leaderboard are based today on the Llama models because there's those are just the models that have had the most optimization that perform the best on low spec Hardware or uh or like normal consumer grade Hardware that that many people have at home and so those lava models they can normally be recognized by by their size yeah that's the biggest one is 65 billion parameters and they have uh some are called 30 billion parameters but it's actually 33 so it's slightly inconsistent you will find some naming using 33 billion 30 billion and then there's 13 billion and 7 billion parameters that's what the Llama based models are but today uh the leader or right now it's it's also constantly changing uh the leader of the open source large language models is actually a 40 billion uh parameter model that's called Falcon and that's completely unrelated to uh to the Llama models it was released by a research institute um in Abu Dhabi and as it has a significant advantage over the Llama models uh when it comes to licensing the license is much more permissive yeah meta has published their first gen models with a license that says you can do research on it and better do your own research I'm not giving legal advice but the the Falcon models they they are very capable and have a more permissive license but right now as we speak the drawback is still that for the Llama models just much more optimization has been done yeah Falcon right now uh would perform not as well as uh similarly capable llama model but this is like breaking news when we're talking this has all happened in the past few months it has happened in the past few months yes and Falcon is I think uh not more than two months old so you've done this locally with your own data sets right and I want I want you to tell me what you've been using it for personally but also like what performance Which models do you choose what kind of performance do you get out of it so at first I just wanted to try out uh what what can I actually do with those models yeah because I'm really really excited about gpt4 and what it can do with the the models behind that they are very powerful very capable but also there's a drawback um sometimes I'm not sure if I'm writing a query will it be considered an appropriate yeah I would I will I get flagged and just the horror of of maybe losing access to that kind of resource because some some value system that has been used to uh to filter the prompts doesn't uh exactly match my own value system yeah so uh yeah so so it's a it's basically a matter of of freedom and just being able to to just put my unfiltered thoughts into a large language model and of course if it's running locally then then I I have nothing to worry about regarding that and so um yeah but mostly I've I've really been uh doing benchmarking and comparing and linking it all up recently I've posted something to my GitHub as well which is I I cleaned up my own Lang chain code which has this tool chain that that we've been talking about with a vector database with the embedding model with a large language model and uh is somewhat easy to use at least for me I I don't know if anybody else has had a look at it yet when it comes to there's a new model that's being released and I just want to drop it in quickly I have that model is llama based then I can try it out rather quickly and uh yeah so I've been using it for various documents and uh we've been posting about it I'm I'm trying to only use a public domain data when I make a LinkedIn post or something like that yeah for obvious reasons um but it works quite well with books yeah recently I for example I've used the Bible which is one of the public domain books that just comes to mind yeah it's a fairly big and then I just wanted to know how long does it take to to create embeddings from it and turns out on my system it's like less than a minute and I have all of the Bible uh subdivided into Snippets and those Snippets uh created embeddings from them yeah we'll start we'll sidestep the whole religion discussion of course of course that's a nice large open source collection of books right yes that makes sense yes and there's lots of Open Source data out there and you can just yeah you can use the yearly reports of companies you can use whatever uh you might have put into the data Lake of your own company yeah so it's uh it's suddenly becoming much much more exciting to uh in terms of what's possible with with all the data that has been just assembled in some cases or many years so I I give me the number so how long on your laptop which presumably is like a reasonable spec laptop how does it take you to index if that's the verb the Bible and then what kind of query performance do you get from it uh yeah well I I've recently uh because in part motivated by that that Google memo I've recently bought a fairly beefy machine and so it's not the laptop anymore it's a desktop and it has a 40 90 and that's uh which is uh currently the top of the line Nvidia Graphics graphics card and when I create embeddings from the Bible uh on that machine that takes less than a minute but also I wouldn't expect it to take much much longer on uh lower spec machines yeah I would need to do testing on that but currently uh I'm saying what's the point I want to use the best I have available and yeah I might move into clusters if I hit some limits uh but I don't if I'm doing this at home I can expect minutes overnight so what I've seen is I've had a fairly large Corpus of text uh 1.6 gigabytes and I split that up into documents that are 1000 characters long and that took a few hours on that same machine but I've also noticed that uh that is much quicker if I use documents that are only 500 characters long yeah so it seems to be non-linear to me um take it with a grain of salt I haven't done a study on this it's just my impression from what I've been doing currently I much prefer document length of 500 characters just for the performance reason oh so this is another decision you've got to make going into you need to choose your embedding model you need to choose your chunk size do you end up iterating a lot and just tweaking parameters seeing what's going to happen yeah the reason I've uh I've decided I want to publish something on GitHub is because I noticed I just started out scripting something with python and with Lang chain and I I cannot actually python so uh I it became uh something like uh unwieldy when I wanted to extend it further and so I thought okay now I need to do some design paths on it and so I thought okay let's let's take open source as a motivator if I want to uh well publish something that another person might theoretically be able to use than what would be an interface yeah and then from that perspective I arrived at a design that is now uh again much much more extensible and configurable uh and more reality again than what I previously had um yeah and I think uh it's precisely what you say I want to uh fiddle with the parameters I want to drop in different models of different quantizations try different document sizes compare and there's so many variables yeah and so it's really important if I want to have meaningful uh knowledge that that I'm methodical about it and and systematic and so it's a it's important to design it as well yeah this is cool you're a home AI researcher yeah officially yeah I mean okay and then so the the the really big job is getting this language model which you do from someone else the Lesser but still quite chunky job is indexing the Corpus in this case the Bible and then you've got this final Vector database output right yes and that's a thing you can deploy to someone else and they can start querying yes I mean I can of course host a vector database anywhere or I can also use just um some Cloud API if it's not data that I really need to keep on my own systems then there are Cloud providers there are also Cloud providers which uh which uh care about privacy and uh so there's there's many options there and so this this whole process yeah you can you don't even need a local embedding model if if you don't care about the data really remaining private you can use an open AI embedding model they have a good offering that's uh it's becoming less and less expensive in terms of per token cost um so there are many many options okay I think we're in getting to in this is I can imagine a future where I've got an AI trained on the data I'm interested in maybe I've trained it on all the emails I've ever written and all the blog posts are never written and then I would like to deploy that to my phone where it could write emails for me on the go yes are we approaching that future there are people who are already doing that uh kind of pre-writing their emails based on what they can do with AI and what's what's popular for people who are doing that who are not that familiar with coding for example they they have access to low code or no code tools that are also provide a way of for example using Lang chain connecting the Gmail API with uh with the open AI API yeah so right now it's just uh maybe a combinatorial explosion of what's possible yeah that's also what do you describe it's uh depending on how much you're willing to do yourself it's not the future it's already possible and it's already being done oh so we're in the actual productionizing phase of this stuff yeah of many many things it will take many years until the potential of what's currently out there has been uh well uh exploited okay where should I be looking I mean if you've got any recommendations well uh I think right now uh it's there are just so many possibilities um and the development is progressing so rapidly that I would say um what's a good starting point is precisely this uh this architecture where you're using a vector database to use be able to use some data that's just not been used in any training or fine-tuning process for for models and then use an embedding model use a large language model and then look for example what I've been doing recently look into more of the length chain features Lang chain has really a lot of ways to deal with the limited context size of the large language model to do compressions using large language models again to extract data from various formats of documents and so even there yeah and then combine it with a business use case maybe uh something something that you care about personally something that maybe if you have a business if you if you're no business owners talk to them uh what what are their challenges and then just yeah connect maybe connect the dots and I would say due to the sheer number of possibilities of combining and also to due to the speed of development um it's for me the question is not where should I be looking but what filter should I apply so that I end up actually doing something and not only being overwhelmed by by all the things that are there to be used yeah the the classic kid in the candy store problem if you've got too many yes okay is there something in particular you're working on other than slicing up the Bible yes of course um So currently I'm just starting out to look for a job and obviously there are applications there yeah so um in the beginning I just take a somewhat Global Perspective and I I think about what what are the fields that I would most like to be working in and so my current challenge is to just find out who are the major players in each of the fields in the domains and um so obviously uh when you've got a big company you also have to publish lots and lots of stuff and then I can just create embeddings of the yearly reports maybe maybe even of job postings yeah and uh then um ask natural language questions about it just so I make progress on my skill set when it comes to llms at the same time as I as I narrow down uh where where I would like to go next you know so this is right now what what I'm thinking about regarding llms that's brilliant you you employing AI for your job service that's great fun but how's that working are you like you pick five companies you like and you index them all or you just as they're like you here's a particular company I'd like to learn about so yeah I'll query that there are definitely some companies that um uh eager to be working for potentially uh no matter uh what what they are currently doing and Publishing and in that case it's easy I just try to find out what they do in some cases uh I'm just uh I'm just joining a Meetup where there's somebody from that company yeah that's that's a low-tech way approaching it um but yeah so there's still a place for natural intelligence and it's a much more efficient way yeah than taking the public routes taking the information that everybody can access and then uh yeah I think a big company that's hiring is always also has a Spam problem and then uh yeah the the public routes to to hire us uh is often the one that's much more resource intensive as well but then yeah so uh I one thing that I have in mind is I'm going to take the Fortune 500 and then I'm gonna see how do they publish their their yearly reports if they publish them in English um and uh then also gonna see like uh AI what what are they writing about AI what what maybe the subsidiary of this is uh working during work related to that field then other other important Technologies I'm uh I'm really passionate about the potential of Robotics as well of Fusion Energy and I can just uh yeah browse their Publications and their their reports uh according to well are they connected to that then in what way are they how are they focusing it where should I turn then maybe I will just uh look at the website of some subsidiary instead of the of the main corporation yeah in that case and narrow down uh what's the most efficient way to approach a potential employer who I know nothing about yet now I'm only passionate about the domain they are engaged in yeah this is the coolest and definitely the nerdiest approach to job searching I've ever heard so four marks we should talk a bit about the um the other side of AI which is prompt engineering when you've got all these models what do you actually say to the thing um so the thing is if you have a model that doesn't have that many parameters a small model then prompting and prompt syntax is really really important so if for example there are some small models uh where if you like leave out a space at the wrong point or where if you don't use exactly the The Prompt syntax that was also used in fine-tuning them then they will not produce useful output or the usefulness of their output is really decreased they have this big disadvantage that a very small model has for example versus what we know from uh the the big uh models and gpg files especially where you are really free to just prompt in any way and um so recently I've mostly been using the 30 billion size models or 33 and they they are really much better at that already they provide people more leeway they will be more more accepting of free form input still it's important when I build a pipeline to have some some structure that makes sense for a prompt yeah if I know okay I will put lots of documents Snippets in the prompt together with my query and then maybe I also have a conversation memory which is another very important component if you want to do conversation and not just have like a zero shot I think it's called prompting and just expect the desired output within just one one interaction yeah then the the prompts sometimes they become somewhat complex here and then it's important to have some formatting which will let the large language model so to say make sense of what's the prompt yeah and then they have like right 10 Snippets from some report and then they have the what has been gone what has happened previously in the ongoing conversation and then there's a actual query and yeah so maybe the next few days I'm going to experiment also with smaller models and see uh what's the complexity that I can still expose them to while expecting output that makes sense output that's useful and then of course they they perform much better yeah so this is if I want to run stuff in parallel or if I just want to uh do async stuff then using smaller models is very interesting and attractive but I I need to know what can they do and what can those generalized models do where do I use specialized models options options okay so do you are you saying that you with a smaller model you you have to be more careful about talking to it but it still gives you very high quality results back potentially yes so it depends very much on the fine tuning that the model has and um then how it relates to to my use case and um yeah so uh it's where where I really need to do some experimenting recently as I say I've you've been using 30 billion parameter models for just about anything but it's a waste of resources for for some use cases here if I just want to sum something up here I I give it a prompt and I just want to use the language capability of the model to give me a summary of what I've presented it to be used in The Next Step of like a chain of language models uh then I might imagine a smaller language model is already capable of that but I need to test it and also see what works and what doesn't work for for the use cases that I in mind right so do these language models do they ship with like here's how to talk to it yes or is that trial and error yeah well uh there's um for the models on hugging phase there's something called a model card and every model should have a model card which uh which lists its uh prompt syntax in the best case yeah sometimes yeah sometimes uh whoever uploads a model there is not too tidy about it and then uh people have to figure out okay I have this model there's nothing in the model card but maybe there's a link to the model that it was based on or that it is a quantization off and then sometimes it's also a matter of okay this uh it just says this model uses the vikuna 1.1 prompt yeah and then uh if you if you know where to find that that's a good thing in that case there's the uh I think uh foreign GitHub I think it's called Uber buga or something like that and they have like just a folder of common prompting syntaxes um where if you know where that is you can look into that and it's fairly likely if a model card says I want that prompting syntax that uh you can find that prompting syntax in that repo this is from I've just started reading like it's an old sci-fi book called the Moon is a Harsh Mistress oh I I don't know that and the lead character has been trained in this special language on how to talk to AIS um they've got a name for the language but it's about loglum or something but it's really reminding me of that indeed yes that's superb so um I think I think you give me enough to get going actually where's the first place I should go and download after this conversation yes so I think for I I would personally recommend to get started with length chain and Lang chain is basically this big toolbox which which enables you to link uh lots of components together whether it be uh those apis from openai from anthropic from Google probably also other providers and then you can just start out maybe maybe you get free credits from some of the API providers and can experiment there they have lots of tutorials they have descriptions of basic use cases getting started page yeah so um just start out with the simple use cases and then once once uh once you've figured out some of the components then you can you already know you can run your vector database locally you can download an embedding model to to actually use the vector database with your own documents then for example if you're using python then you will need a component like Pi PDF but it's easy to install it's just a python package yeah and then okay yeah then you have on hugging face you have those leaderboards of the open source models yeah the for the embedding models and for the open source large language models as well yeah um yeah and then yeah then there's a llama CPP python I think it's a very good package right now which is the one that enables the mixed mode between CPU and GPU for the Llama based models and also there's some preliminary work on the Falcon model that the one from Abu Dhabi that's also right receiving their first optimizations and you are actively blogging about this so we can also check your blog first tips yeah well I'm I'm writing on LinkedIn and I'm blogging about it and uh yeah so I don't really know what uh what I will discover next but uh that's the joy of research and you've got to publish it spread the message about what's possible today yeah because uh basically if if you're a student at University you can just uh try it out get into the field and there's lots of potential there's basically uh lots of uh lots of really early uh bug yet to be done yet to be discovered publish about then talk about and it connects to just about any domain that that is in some way cognitive yeah so uh it's a it's a thing that will go on for at least years uh if not longer and who knows wherever go yeah I can totally see that shaping the next few years of the discussion because it's like a rich ripe field that's we're only really months into open source bedroom hackers exploring right yeah yes indeed yeah great fun well in that case we'll have to have you back in the podcast in a year or so and you can tell us how far you've got I look forward to it great Toby in the meantime thank you very much for taking us through it and thank you Toby thank you very much that was truly enlightening and I think I now know enough to achieve one of my real life goals which is to download transcripts of all David Bowie's interviews over the years and get something that could rewrite my LinkedIn posts in a tone that I really respect I suppose the floor in that plan is it assumes there was one David Bowie when really we got a different one every few years he was a great chameleon as they say over here we're much more stable than that and we will be back next week with another developer lending their voice to the global conversation so make sure you catch it by clicking like And subscribe and follow and notify and rate and all those good things and you know drop me a comment as great as AI is there's still no substitute for hearing from real people that's the very raison lecture of this podcast isn't it so check out the show notes if you want to get in touch with me check out the show notes if you want to get in touch with Toby because at the time of recording this he is available to hire on your marks get set go get him and he also gave me a list of links that you'll want to look at if you want to learn more about this field there in the show Notes too all of which I think brings us to the end of this episode I've been your host Chris Jenkins this has been developer voices with Toby funkina thanks for listening