this episode is brought to you by brilliant the ability to perform billions of mathematical calculations per second electronically has become such a triviality that it often goes unrecognized and even ignored however long before the era of inexpensive yet powerful processors the pursuit of even a tiny fraction of today's almost disposable computing power was seen as the solution to an immense spectrum of design challenges this is the story of five cases of fully functional Computing machines realized in very unconventional ways in 1928 Vladimir sergovic lucianov an engineer with the central Institute of Railway engineers in Moscow while working on the construction of trois orisk Railways struggled with the durability of the reinforced concrete structures as temperatures dropped below freezing in Winter despite attempts to mitigate cracking by pouring only in the summer the winter condition still proved to be problematic lukyanov believed that if a thorough examination of temperature changes within the concrete Mass was conducted taking into account factors such as its composition the cement used how it's utilized and external conditions cracking could be understood better and prevented however while attempting to conduct this very study he had hit a wall of computational limitation existing calculation methods simply could not give a quick and accurate solution to the complex differential equations used in his analysis Luciano's attempt at overcoming these limitations led to his discovery that the flow properties of water possessed similar characteristics to the behavior of heat distribution from this he envisioned a computational device based on water that could be used to not only visualize thermal expansion but also perform the needed computation to analyze it by 1936 his first prototype for the concept would be constructed at the Moscow Institute of Wei and construction called the water integrator his first design was composed of tin roofing panels in glass tubing however despite its crudeness it became the only computer in the Soviet Union that was capable of solving partial differential equations the water integrator consisted of several interconnected pipes and pumps that performed mathematical operations by the precise manipulation of flow rates within the system with the results being plotted on a graph that was actuated by the resultant water flow the system took in stored numbers as data that was represented by the water levels in a collection of Chambers the Precision of this information was only limited by the operator's ability to discern the water levels this was typically within a quarter of a millimeter while leukyanov's first integrator could only solve one problem at a time by 1941 a more modular design would allow for the stacking of water integrators to produce a machine that was capable of solving far more complex multi-dimensional problems further refinement throughout the decade would improve the modularity of the design allowing each unit to interface with each other in more varied ways further enhancing its ability to be used on a broader range of problems ultimately leading to it being mass-produced for use in Laboratories and educational institutions across the Soviet Union from the Sands of Central Asia to the icy terrain of Antarctica the ingenious water integrator found itself at the Forefront of resolving a diverse range of Soviet civil engineering challenges its versatility was truly remarkable providing effective solutions for design problems encountered in extreme environments such as studying the temperature regimes of Antarctic Ice Sheets mine construction Metallurgy and even contributing to rocket science research remarkably lukyanov's invention maintained its relevance for decades outliving the introduction of the electronic computer and continuing to prove its value well into the 1980s it was only with the emergence of smaller high-speed digital computers that the capability of the water integrator was ultimately superseded in the 1940s a curious realization had occurred to electrical engineer turned Economist William Phillips that monetary flow and stocks of money could be thought of as tankfuls of water from this idea the concept of using Hydraulics to model the workings of the British economy was formed creating a peculiar niche of economic analysis called hydraulic macroeconomics while Phillips was a student at the London School of economics in 1949 he had collaborated with his Economist colleague and future monetary theorist Walter Newlin to write a paper that outlined the operation of a hydraulic macroeconomics device construction of a machine based on phelps's paper would begin in 1949 called the folks machine or moniac the core functionality was based on a hydraulic system where the upper pipe represents the flow to the market while the lower pipe represents the flow from the market thus altering the level of the box tank the opening of the two valves are determined by the supply and demand curves a demonstration of the prototype machine was given that November before a distinguished audience of Economist at the London School of economics and it was met with great success this led to the funding and development of a more sophisticated type 2 machine that would be built within a year the type 2 machine used colored water flow that represented total national income and was entered at the top of the machine taxation revenue is diverted to one side and savings to the other with the remainder being consumption expenditure similarly flows into other pipes represent imports and exports while integrator tanks represent various other Financial balances inflows and outflows between the various computational sections are controlled by a series of valves actuated by floats pulleys and servos through these adjustable Valves and other mechanical controls various mechanisms of the economy could be simulated including feedback introduced by government fiscal control action plotter pens driven by the water levels representing key variables such as interest rates imports and exports were also used to generate plots Against Time much like the water integrator Maniac was in effect a machine that solves differential equations simulating the complexities of years of economic Cycles within minutes overall Maniac was never seen as an accurate system but it was considered a breakthrough in the understanding of the complex dynamics of Economics with its visual demonstration of the process of change in a multivariable system in which the variables are functionally related and it did this with the capability that would not be matched until the proliferation of practical digital Computing decades later since the introduction of the modern torpedo in the 1860s Engineers struggled to design an effective guidance system that would allow the torpedo to intercept with a moving Target these early Torpedoes call straight Runners simply ran at a preset depth in a straight line for the next 80 years well into World War II the vast majority of torpedoes used in Warfare were straight-run Torpedoes their Fire Control was a manual task with the intercept course being determined with the aid of slide rule angle solvers mechanical calculators and Lead targeting sites World War II had prompted an acceleration of the development of various forms of ordinance guidance computers with torpedo guidance being an ideal candidate for automation by computer while each major power had developed their own analog computers to fill this role in 1932 the development of the torpedo data computer would be initiated by the United States Bureau of ordinance in collaboration with the armor Corporation and Ford instruments resulting in one of the most complicated mechanical computers ever devised for ordinance use in most basic torpedo designs a gyroscope-based fire control system is used to set a course relative to the launch course of the submarine this is known as the Torpedoes gyro angle when done manually this gyro angle is derived from the trigonometrically calculated firing solution for the Target any changes in the course of the Target or launch submarine required a recalculation of the firing solution the torpedo data computer dramatically improved upon both the speed and accuracy of targeting by providing a continuous solution Based on data from the submarine's navigation sensors and the computer's Target tracker the system also had the capability to update all ready to fire Torpedoes simultaneously without any manual intervention manual gyro angle setting was done mechanically while the torpedo was in the tube a spindle engaged into a socket near the housing of the Torpedoes course gyroscope was used to rotate the gyroscope setting its course angle after the torpedo was fired it traveled a straight course of a known distance called The Reach This reach is set by a threaded shaft designed to create a delay in the activation of the torpedo's gyro steering mechanism once engaged the steering mechanism brought the torpedo to a new course based on the angular offset of the gyroscope the torpedo data computer consisted of two sections the position keeper and an angle solver the position keeper tracked the Target and predicted its position using coarse information from the submarine's own gyro compass and the speed from the pedometer log a device used to measure a vessel's speed relative to the water the position keeper could be set using hand cranks on its face that set the target distance estimated speed and angle relative to the bow on some variants this information can also be sent from calculations based on Sonar measurements the position keeper would mechanically solve the equations of motion integrated over time producing a continuous prediction of where the target was at any instant these tracking measurements could be compared against observation and adjusted for with the hand cranks the predicted Target position became more accurate as further measurements made the corrections smaller it was typical to get an accurate track on the target after about three or four observations under good conditions the angle solver would simultaneously take the target's predicted position from the position keeper adjust for the properties of the torpedo and siding equipment and determine the needed gyro angle for the torpedo these calculated values were then fed back into the position Keeper in a feedback loop the calculated gyro angle is automatically routed to the linked torpedo rooms and set in all Torpedoes continuously the torpedo data computer effectively provided a system that would Point all Torpedoes at a Target as the Fire Control solution developed functionally all variants of the torpedo data computer were electromechanical devices with core functionality being purely mechanical information propagated through the computer using a combination of gear trains with calculations being performed mechanically by differential mechanisms while bevel gear based differentials were most commonly used various other computational differential mechanisms such as shaped rack and Pinon and helical differentials were also employed specially designed gear train configurations were also used as mechanical integrators alongside other simple mechanical mechanisms designed to set the parameters of operation introduced in 1940 the torpedo data computer outperformed this systems used by both Germany and Imperial Japan as neither competing system could track a Target this distinctive Advantage led to four iterations being developed with the system being used well into the 1950s until it was ultimately superseded by fully electronic systems the next strange computer is more of a conceptual demonstration than a purpose-built machine however it does function and was constructed on several occasions the Domino Computer is a peculiar configuration of falling dominoes that can perform logical operations and even simple math it's part of a class of unconventional computers known as physical object computers for a system to perform digital Computing it must exhibit two key characteristics the ability to conduct a signal with amplification and possess a noise effect low enough to exhibit distinct digital States a sequence of standing dominoes exhibits both of these properties amplification can easily occur as a single falling path can trigger multiple subsequent paths and standing dominoes possess two clear digital States standing and Fallen a domino computer is based on the basic signal path of a normal Domino chain Its Behavior is analogous to a digital electrical signal though much slower and with the key distinction of being a one-time use signal event by designating regions within the Domino chain where the falling Domino chain can be selectively allowed or stopped input bits are formed by combining multiple Domino chain paths with these inputs the elemental logic gates of digital Computing can be performed this for example is a not gate an or gate an and gate and an exclusive or gate much like a traditional electronic digital computer from these Elemental logic gates more complex functional units such as a half adder and a full adder can be constructed which in turn can be used to construct an arithmetic logic unit a core element of a processor however unlike electronic Gates Domino logic gates have to contend with the physical constraint of the paths of The Dominoes signal pass must also be designed with enough length so that the delay is introduced at key points in order to keep linked logic units properly synchronized in 2012 at the Manchester Science Festival mathematician Matt Parker and a team of volunteers constructed a domino binary Adder which could add two 3-bit inputs and produce a three bit output the next day the team had attempted to build a 4-bit Adder though a complication between two chains of dominoes mixing signals and a timing issue made the attempt unsuccessful in 2018 at bank Muscat headquarters in Oman a team of American British Academy students was able to construct a functional fifteen thousand Domino 5-bit Adder setting the current world record for the largest domino computer with an integer capacity of up to the number 63. today producing Motion Graphics rely on the raw power of modern Computing to calculate the mathematical interaction between dozens of screen elements to produce a seamless sequence of pixels that form the visual content we consume the sheer level of computation needed is unlike anything ever seen by the film and television industry of the past decades in fact just to produce this video around two quadrillion calculations were performed long before digital animation was feasible a purpose-built machine did much of what immense computing power does today with a clever mix of analog Electronics called scanimate the system produced much of the video based animation seen on television and film between the 1970s and early 1980s scanimate was the brainchild of Lee Harrison III founder of computer image Corporation in Denver Colorado the earliest escanabate blueprints date back to 1969 as a simple two-rack unit system that was able to synthesize animations in real time this real-time capability is what made it stand out from other animation techniques the scanamate process started with backlit light tables onto which high contrast artwork was mounted these were then scanned by a progressive scan monochrome camera which ran at 60 Fields per second in ntsc format video or 50 feels in pal format video which was more than the 24 frames per second that film used this camera also ran at a higher resolution of about twice the scan lines of traditional video the image was displayed on a 5-inch Precision monochrome CRT that displayed modified video signals generated by the analog graphics processor the processor section was programmable through patch panels and a mix of switches and dials that controlled the analog effects imparted onto the original image signal the animator was in effect an analog signal composer patching together a series of ramps sine cosine generators bias and gain potentiometers and multipliers and summing amplifiers to produce an animated sequence on the CRT the generated animation sequence is either filmed or fed into a colorizer a device that converts these Shades of Gray into color along with a transparency level complex animations could be produced by layering graphic sequences stored on videotape using high quality video recorders animation sequences were either started manually or queued to an embedded time code on videotape as the animation ran the knobs could be adjusted resulting in immediate modification of the end product much like graphic animation today it was not unusual to observe an animator re-running a sequence over and over while adjusting a handful of parameters until the whole animation behaved as desired while scanmate was a powerful tool it was extraordinarily difficult and expensive to operate with only about 22 total machines ever being built over the years additional modules and racks were added to allow more functionality and flexibility however this brought with it a problematic lack of repeatability a typical animation might use a hundred patch cords and require extremely precise adjustments of an equal number of knobs Each of which had nearly an infinite number of possible settings with no way to save work even the smallest change in this chain could destroy dozens of hours of work the training of scanimate animators was also very difficult animators were expected to keep a reference of popular effects and be able to wire up and tweak them into place in just a few minutes typically an animator would work in conjunction with a graphic designer with clients paying as much as 2500 per hour for the scanamide expertise despite this well into the early 1980s scanimate was seen as a powerful and effective tool for animation its output was colorful extremely smooth moving and surprisingly flexible and diverse in the variety of its effects and at its peak popularity it was operated in Japan Australia Luxembourg London New York Hollywood and Denver while the mid-1980s saw the emergence of fully digital and digital analog hybrid animation systems that were more versatile and had the capability to store animation parameters two scanimates are still in use today at zfx studios in Asheville North Carolina though these installations have been upgraded to work with HD video and digital recorders for layering it's a fascinating observation that long before modern Computing the need for a computational device was seen as a Magic Bullet across a broad range of fields Looking Through The Eyes of industrial history the digital Revolution seemed almost inevitable have you ever wanted to build a strong understanding of the fundamental principles of computational thinking that's where brilliant.org comes in brilliant.org is my go-to tool for diving head first into learning A New Concept it's a website and app built off the principle of active problem solving because to truly learn something it takes more than just watching it you have to experience it brilliant.org is constantly developing their courses to offer the most visual Hands-On approach possible to make mastering the key Concepts behind today's technology effective and engaging in fact one of my favorite introductions to computational thinking is Brilliance computer science Essentials learning path this intuitive progression of computer science courses allow you to build examine and self-discover the critical abstract concepts of coding Through The Eyes of algorithmic thinking using interactive exercises with brilliant learning depth and at your own pace it's not about memorizing or regurgitating facts you simply pick a course you're interested in and get started if you feel stuck or made a mistake an explanation is always available to help you through the learning process to try everything brilliant has to offer free for a full 30 days and start learning stem today visit brilliant.org forward slash new mind or click on the link in the description below the first 200 of you will get 20 off Brilliance annual premium subscription