nvidia's gpu evolution kicked off the neural network revolution however while gpus run neural network algorithms quite well they're not specifically designed for it so companies have started to develop hardware customized for running specific ai algorithms dubbed ai accelerators today the ai accelerator hardware market is estimated to be worth over 35 billion dollars venture capitalists poured out nearly 2 billion for ai's trip startups in 2021 tsmc considers ai accelerator hardware as one of their top secular drivers in revenue for the near future in this video we're going to look at what these weird things actually are about but first i want to talk a little bit about the asian armature patreon i'll make it quick early access members get to see new videos and selected references for them before their release of the public it's not a lot of money and i appreciate the support thanks and on with the show in 2011 a team from new york university and yale university presented a paper at a 2011 conference called cvpr the paper discussed a scalable hardware architecture for big and deep neural networks by now the ai research community had broadly accepted gpus as the de facto standard hardware to run and test their algorithms but the paper points out that custom hardware can improve performance and power consumption by two orders of magnitude traditionally deep learning models are shown processing images with a sequential pipeline of steps within a neural network it reminds me of how your phone processor turns the raw image data it receives from its sensor into the finished photo that shows up on your screen watch that video if you haven't already the 2011 paper proposed an architecture that processed each individual step in parallel and applied it to a computer vision algorithm they implemented the data grid architecture on a z-link's vertex 6 fpga a type of chip that allows for custom configurations and saw substantial improvements the interesting thing to note is that fpgas are development and testing devices so they tend to be slower so the team also presented a hypothetical 45 nanometer chip design fab with an ibm process and it blew its competitors out of the water we should discuss neural networks and what they are before we go too far i talked about this in my video about nvidia's gpu evolution which i recommend you watch but let us refresh ourselves on neural networks strip away the exoticness of the name and what we find is that a neural network is basically a titanic mathematical function it uses simple processing elements to model complex relationships between many inputs a neural network is represented with matrices you would first turn the input image into a representative matrix of data then you send that input data through the neural network's many layers this is done by multiplying the input matrix with weight matrices then you apply a special math function referred to as an activation function to the resulting values yeah this is a massive simplification of what companies are doing in production but there we go there are two roles that ai accelerators have here first this is an aforementioned training phase where we use large datasets to determine the weight values that go into the network's many layers the second is in the inference stage that is when the trained neural network function is being used to infer a result from the input data in other words when you turn the image into a data matrix and start multiplying weight matrices and applying activation functions this is the moment when you show the neural network a photo of a dog so that it can tell you that you have a dog in the photo hopefully at least neo network accelerators work better than their generalized cohorts like cpus and gpus because they are tuned to handle matrix multiplication and convolution operations which take up some 90 of the work and there are a lot of calculations to do google back in 2016 over five years ago surveyed how many some networks had up to 100 million weights i think it makes sense then that you would build your own hardware to handle this but we will get to that later the concept caught on and soon a small community of ai hardware projects sprouted up i'll mention just a few of these forerunners here they include the 2015 project now run by the chinese academy of sciences institute of computing technology sure diana is a piece of hardware tuned to run a specific type of neural network one suitable for computer vision applications their 65 nanometer accelerator consumed 4 hundred times less energy than a gpu and was small enough to be embedded alongside any commercial image sensor in a product cas would expand their hardware into four lines for basic multi-configuration training slash learning and computer vision purposes another notable project is the iris flexible accelerator for mobile devices running neural networks an mit project with some funding from darpa iris's chip was for running inferences on images using a pre-trained model furthermore it did it with 10 times more energy efficiency than contemporary gpus and finally one notable startup was teradeep a project founded by members of the original team that presented at cvpr 2011 their processor the nnx wanted to revive old webcams and other hardware by giving them deep learning powers the three projects that we have just discussed have one big thing in common their chip products have generally focused on inferring results from pre-trained models hardware specialized for inference but they differed in where they perform those activities and dislocation is important in classifying the chip so in addition to this training versus inference split the ai hardware industry has another classification that depends on the environment in which they operate edge or server edge or server it kind of sounds like a fancy hipster cafe somewhere in brooklyn where the lattes cost eight dollars edge ai chips are placed directly into the device smartphones cars iot devices or wearables as a result designers have to take into account the power and size restrictions of this form factor for server chips companies are most concerned about the cost performance ratio power is a substantial portion of the chip's total cost of ownership so data centers want the best performance they can get at a certain cost as a result these chips are higher end products near the leading edge developing such designs costs a lot of money and resources which raises the upfront investment if needed in the end all ai accelerators are classified using these two axes that we have talked about they're either doing training or inference and they are either doing it on the edge or in the server slash data center up until then ai accelerators were mostly still research projects percolating in small startups or universities the market would not see wide prominence until google made a big announcement in 2016. google had thought about building their own chips since 2006 but the effort really kicked off in 2013 when they realized the immense computational demand training and deploying neural networks would have on their servers so they started working on the tensor processing unit or tpu quickly designed and fab with the 28 nanometer process the first tpus entered google's data centers in 2015. they publicly unveiled its existence a year later in may 2016. chips like the tpu are referred to as asics application specific integrated circuits and that application is matrix multiplication at its very heart is something called a matrix multiply unit it contains 65 536 multiply accumulator circuits or mac units arranged in a 256 by 256 array the multiply accumulator circuit is designed to do just one thing it multiplies two numbers and then adds it to an accumulation sum that's really about it when the tpu is in use the matrix multiply unit receives and processes two things the first is the subject image data from the tpu's host cpu or main memory this is the thing that you want to run inference on the second thing is the neural network secret sauce the millions of weights which are loaded from the attached ddr3 dram memory into the matrix multiply unit other server chips of the time include the 18 core intel haswell cpu and the nvidia kepler k80 gpu the tpu has far more multiplier accumulator units than either of those chips and more on-chip memory for storing intermediate results of course cpus and gpus are designed to handle far more generalized tasks than the tpu i don't think this dpu can run video games or photoshop for instance the design's overarching goal is to ensure that the matrix multiply unit is in constant use more recent iterations of the tpu are capable of trillions of floating operations each second stripping it down to the bare bones the tpu is not terribly complicated a massive linear algebra machine but it is very specialized because of this specialization google's first generation tpu can run inference 50 to 30 times faster and 30 to 80 times better energy efficiency than its contemporaries google filling their data centers with better performing tpu hardware keeps them from having to buy cpus and gpus and that saves them millions of dollars for their data centers good for them it is important to note that google also has their own machine learning software framework tensorflow which lets them exactly customize the hardware to what the software is doing with these two google built out an end-to-end machine learning system that lets it do amazing things like object searches on 8 to 10 years of uploaded google photos google didn't make their tpus available for sale rightfully considering it a competitive advantage of theirs but people can rent its computing power through their google cloud service google essentially created the server ai asic industry but other companies have rushed to do the same amazon is the biggest cloud hyperscaler out there with aws they have already proven their chip design chops with their internally designed graviton chips which are arm-based cpus that have started to become a real alternative to intel server cpus in december 2020 they announced their own trainium devices ai training accelerators these options are a challenge to gpus and other hardware makers with aws hosted instances like habana habana was founded in 2016 with a data center product for training machine learning models they were acquired by intel in december 2019 for two billion dollars considering the amazon announcement i think they timed it right nvidia has their own custom ai hardware too the tesla v100 this is not necessarily a gpu but an ai processor with over 20 billion transistors and 5 120 cores their software competitive advantage is quite strong so they are a formidable player for startups targeting the ai server market this intense competition between billion dollar startups and multi-billion dollar tech giants has made navigating dc's rather challenging when the biggest buyers start making their own ai chips and positioning its instances more favorably there is some reason for concern so the data center market is becoming increasingly competitive for outsiders yet things on the other end aren't getting any better pitchbook noted that half of the ai chip market is in specialized ai chips for mobile phones my first and uneducated guess would be that these chips handle the increasingly computation heavy operations of image processing but the mobile cpu processors are trying to eat up that market opportunity themselves by growing and expanding their chips on board neural networking hardware apple again led the way in 2017 the iphone x a11 bionic chip had dedicated neural network hardware built into it the neural engine the first iteration had two cores and was capable of 600 billion operations each second again apple's control of the entire software stack makes this sort of integration ideal the first neural engine was only used for face id and animojis but the next year apple opened up the second version of the neural engine to users of its core ml api other chip vendors are adding dedicated ai functionality to their chips too for instance taiwanese fabulous chip maker mediatek and their demensity 5g mobile processors there are still many opportunities left for inference on the edge iot devices autonomous driving and what not but few mobile devices have margin like phones and the technical effort behind those low power and low latency use cases will be a bit trickier but as my dad used to say challenges are just opportunities in disguise even with these specialized architectures today's ai accelerators can take hours or even longer to train a single machine learning model for production one suggestion for what's next has been to implement new ai chips using silicon photonics servers in today's data centers operate much like any other computer there is a cpu that processes information and a separate memory bank that stores the data and instructions this setup is called a von neumann architecture and it has been around since the 1940s neo networks challenged this architecture beyond the fact that they are highly parallel the separation between the processor and the memory slows things down a great deal it's like as if the brain stored its synapses and neurons in two separate locations silicon photonics products use light rather than electricity to send signals light can travel through optical fiber at the fastest speed possible and without additional heating concerns i discussed silicon photonics in a previous video but only briefly discuss the technology's data processing potential i plan to have a more detailed video on it later down the line it is interesting to compare the ai accelerator hardware industry with the bitcoin mining hardware industry those guys have taken a similar path first with cpus then gpus and then increasingly more powerful asics for the bitcoin miners the next step seemingly has always been to go to a more advanced node but i don't know if future ai accelerators have to go down that same path if so then the big chip companies like nvidia will always have greater access and resources for that the more interesting ai accelerator approach will be those taken by companies finding new ways to achieve similar results on inference or training without an advanced node whether that be silicon photonics or something else like advanced parallelism we shall see alright everyone that's it for tonight thanks for watching subscribe to the channel sign up for the newsletter and i'll see you guys next time