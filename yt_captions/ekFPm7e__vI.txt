and we arrived with the next talk admin C plus plus 2022 a little bit of surprise we have four speakers on this talk uh Park Here Andrew Jules and Joel farku um and we'll be swapping through those slides and it'll be an experiment so stick with stick with us um looking forward to the talk um let me bring up the title slides for this talk um Joe you have the word take the take it away sentient uh thanks for having us at PTC plus 2022 uh always a pleasure so uh we will be talking about um making a context for standout Library uh among other thing uh I will be uh speaking together with a bunch of co-workers and colleagues from all around the place namely Paul care which is a lecturer at uws uh Andrew gazilian is former PhD student which is now working at AMD she's my own PhD student right now and myself uh Joel which is a [Applause] um associate professor at University of Paris actually so we will all be talking about doing these concepts for experiment around the standard Library so let's let's start by having a bunch of contexts around the work being done there um as you may know template meta programming has been a subject for a very long time uh it's probably one of the older streets in the C plus development box it does being you know a large majority of the time be something that people were looking at as a kind of hack things uh where a lot of ID code needed to be written and since those old times uh temperature programming evolved quite rapidly first through uh tools like boost and Pierre boost visions that were later um upgraded updated replaced by other version of those like boost mp11 boost Anna and a lot of others but the main change in how we look at templates with roaming is the fact that the context for keywords and Facilities that were introducing C plus 11 and usually upgraded uh in simpler 17 and 20 completely changes the way we have to think about doing stuff with templates and trying to force our compilers to behave in a some you know an orthodox way the cool thing about using contexter as a main tool for temperature for me is that it turn then Place meta programming into actual programming as we will be seeing uh in a few moments we are now writing codes that may pass as a regular code okay but it's actually doing the meta programming somehow and it push meta programming into what it was from the beginning and even if it was not obvious it's a template data programming and meta programming in general it's a tool to write automated code generation and this automation level is now more uh obvious to everyone thanks to the simplification brought by Constructor yet well uh if the the benefits from construction are mostly obvious as I say meta programs should be better we have less needs than before of specific libraries uh to write complex temperature programming and we can write the code in the runtime session debug it and then turn this phone's expert uh we are still missing a bunch of elements um even through a lot of efforts that we'll be discussing uh we are still lacking a bunch of elements to get to a perfectly massively constructible standard Library we have a lot of things that will be discussing that are new and we will be using them but we are still missing things there is a problem of compiled times which is always an issue and we will try to see how we can find well a creative way together around that and we will try to focus on how we can actually turn all of this into a proper continuation process and uh in the time that we uh I was now uh I will let Paul speak about this first expert standard Library uh Jules will be speaking about compilation for Generation trade-off and techniques and Andrew uh we tried to give us an Insight on how uh those compact times can be tackled on uh holding all what we want to achieve with this talk is try to get a large Road spectrum of interesting techniques uh around const experts and all they can be structured in a meaningful way I will now uh pass the stage to Paul that we'll be speaking about uh our implementation of a constable standard Library components for up to you thanks thank you um so yeah I'll I'll give a little introduction to say here before moving on quickly for the moment I'll just say that say is a a non-standard version of the C plus standard library that we have um with extended support for compile time evaluation its purpose is to facilitate Research into larger compile time program engineering and it should also help to Port existing projects which use the C plus standard library to the compile time concept grid domain um but before we have a closer look at say and a relevant porting project that we've we've tackled I want to First review some recent const extra features that have made it so that says now possible so I want to talk about first of all dynamic memory allocation which is now since C plus plus 20. um accommodated within the constant evaluation domain so we have for example The Familiar new and delete operators available to us and accessible within context for programs um a context for function needn't run that compile time which is is great it can or or may not depending on its context and so it will be that certain scenarios will compel a contextual function to attempt to run that compile time for example it can be within a static assert it can be initializing a context for a variable declaration or it can be that it's provided the function call return values provided as an argument to an nttp parameter um and the way that we're in general making use of this at least in our testing which I think is is nice to look at the general scheme on the right hand side so we've got um a constant extra function called all okay we can see the since C plus plus 20 what we can we can do is is called the new operator dynamically allocate uh space for the integer there initialize it we can then test to see that that happened so we can initialize that Boolean there and hopefully that'll end up true but we need to also delete and then we return the value which is hopefully true that came from the Constructor function and then we can kind of doubly test it both at runtime and compile time so we call the uh the runtime assert function on the same function with the same parameters none as we do on the line afterwards here at the bottom on the static assert and you know so if we can compile that the static assert has worked if we can then run it then the runtime assert has worked this dynamic memory allocations should be transient allocations only um which can have a little bit of an explanation here essentially all the allocations that we have obtained and during compile time evaluation must be freed before constant evaluation concludes so if you now look at the right hand side again I've made a a change here so I've removed the call to delete but that will now produce a compilation error if we try and compile that um and that's due to the static assert here if we remove that we can get rid of the compilation error but the static assert will produce a compilation error because I am not freeing memory that I've allocated some quite reminiscent of a vowel ground memory leak chair um and C plus plus 20 we also have vector and string they've appeared fairly recently in the standard libraries and um that's that's Louis Dion's p0980 and P 1004 proposals so now vector and string can be used and there's an example of how it could be used around trivial example below you can see that we can create a vector have some values in it we can create a string and set it to something we can then check that the values are as we expect essentially forming the bill that we looked at earlier on and if that goes according to plan then static assert will be happy with the the True Value being returned from that and all is good so that's vector and string I'd like to bring this uh towards the point of [Music] how you should use the vector and string it's not the case that you can just declare a const extra vector or string which is sometimes surprising because when you hear the vector and string are available or with const extra support it might be the first thing you would write is maybe something like maybe the the bottom two lines I guess the point I want to make here is really that this this will produce an error this um well actually this code will produce three errors if you try to compile it and the first is is probably the more obvious as to what has gone wrong um the the memory is not um being used transiently here um the memory that has been allocated at compile time for the the double so the new has been called and has returned space for the uh the Double and initialized it accordingly but the free is not called there so that this can't work um this variable P would otherwise be available within a function or if there's a global variable um it would the user would expect that that would have the value 0.577 Etc but that's not going to happen we have to free that memory and it's clearly not happening there so the point is that it's also not happening here with the vector and with the string so this form is not allowed unlike the previous slide where we we see that the Constructor and the destructor of vector and string are both called so at the end of the call to string Vector okay the destructor feeds that memory and we are allocating memory transiently as we should foreign just for argument's sake we can access a standard Concepts per container at runtime when it doesn't allocate so here trivially we've initialized the P pointer const extra to null pointer fine no allocations um so too for the vector v here no allocations made string much the same although uh Clank will give an error on that currently and visual studio and GCC are happy enough the code is as below um it's just a test out trivially so we can check the pointer is is null pointer the vector's empty and the string does not contain nope yeah when we're working with um standard Library we have to be working with allocators as well so all containers of the C plus standard Library support custom allocator from the user so we have to respect that the containers allocate storage with default construction um and by default that will be the STD allocator class which can be used and we can show some example code with that to get us up and running um so if you've not used allocator before um it's going to be a little bit like malloc I'm not sure if that's any more familiar to anyone right enough but it's a little bit like malloc in the sense that it's it's essentially just allocating memory it's not actually constructing or um or calling the Constructor in any of the um the memory that's been allocated and much of STD allocator is now marked const extra since C plus plus 20. so it goes along with the new and delete being available we also have allocators and member functions are largely also context Pro as well so here you can see uh simple uh program making use of the standard allocator as well it allocates 1024 integers and then deallocates them immediately afterwards so that conceptual function is fine and uh yes void so all good um but one thing to notice is that um when you're writing to allocated memory of simple types for example of extended this example a little bit here to right to the 42nd element of the the um the memory which was allocated and targeted by T and it can be common practice to start writing to that memory given that it's been allocated perhaps also by something like malloc but this actually causes undefined Behavior no end object was actually created and its lifetime never started and this was noted as a somewhat long-standing defect in p0593 proposal from Smith and vitalena entitled implicit creation of objects for low-level object manipulation and that proposal was subsequently adopted into C plus plus 20. so code stays the same but just to say that that adopted paper then defines implicit lifetime types and these are types where creating an instance of the type runs no code and destroying an instance of the type runs no code and so such types are given some leniency on this observation about the undefined Behavior Uh and and the alternative usage or the prior usage because undefined behavior is not permitted within uh or it's not permitted during constant evaluation so there's an issue there and this is has now been identified as undefined Behavior but for these implicit lifetime types some um some leniency has been provided so such types are are now permitted to be implicitly created in codes such as you see above um but not during constant evaluation so section 3.5 of The Proposal makes that quite clear and it's also in the C plus plus standard so as well so consequently let's just uh one more thing to consider when you're doing your dynamic memory allocation at compile time um it's not so relevant really for Library level development but uh for legacy codes that might exist and it's being ported this can often be an obstacle um so since C plus plus 20 we've got STD construct app to provide some appropriate syntax to let us use the memory that has been allocated so here we can see the Constructor call has been been provided with the address of the 42nd element of P just before we use it or at least sometime before we use it and so now that code is good and it will it can be part of constant evaluation now um we could put something like that around as well which would just ensure that that only ran during constant evaluation and then moving to code that just looks a little bit more like you would find within the standard Library we typically would require to construct all array elements after creation and so we need two to as Adam mentioned as I mentioned earlier respect custom allocators provided by a user and that can be done using STD allocator traits and so the code becomes something like this which is not much different it's just it's not Constructor it's construct now uh coming out as a as a member of the allocator traits specialized for the allocator that you're interested in um so that now initializes all of those integers um so certainly no problems using the for our second element um there's a recent proposal which may end up in C plus plus 23 that's P 2674 and it offers an as implicit lifetime trade which might allow us some more Precision here to ensure that it's um only the uh only the types that are implicit lifetime that are um may be constructed if you want runtime and compile time code to be exactly the same that can be quite handy um I wonder if it would be good to now in in C plus 23 or 26 to um remove that restriction on the implicit object creation um that that has gone from runtime um but still exists during constant evaluation and perhaps more more simply I wonder if construct a and might be a nice addition rather than destroy or which would go along with destroy n which does exist um yeah of course we're important programs to concept explore these details will require some care um and GCC and visual studio are lenient official C plus plus I should say but Clank throws an error which you know I believe is the correct Behavior but nevertheless I I also appreciate that uh well with GCC and visual studio we don't receive an error which can well ultimately it's got to get resolved um before your codes can be good so now turning back to C again and how it might be used so the C plus plus standard Library as it is at the moment C plus plus 20 standard Library it may in time um become entirely constant extra that's uh debatable and but meanwhile the sale Library can be used today so we've created this version of the standard Library incomplete as it is but which attempts to create a compile time versions of all of the functions entities um classes that you know from the standard Library it's quite a way from completion to say the least but we have made some progress and we want to let you know how we're getting on um for us we're doing things like verifying string based embedded dsls and we're also exploring code generation um we have support and complete support for forward list list set map DQ q i string stream unique pointer SharePoint or exception and function um say is not Standalone uh lib stdc plus plus is required and its code is also used within say as well [Music] um established const expert entities from lib stdc plus plus are naturally just wrapped within C so now we're kind of at the point where we can wrap um vector and string but we've got our own implementation of vector and string in there as well but but the parts which we've we've never implemented ourselves because they've always been available um at least when we've turned our attention to them but we've got algorithm uh numeric the contents of most of the of those headers as I said Vector string we've got array inside the say name space we've got optional here and variant see now supports recent versions of GCC and client and the GitHub repository is linked as shown here's a small example code of say it fits on a slide it doesn't do much sensible but it tries to make use of a fair few features that we've got so you can see we we have a string we can initialize it Vector has been initialized with one two three dq234 we've got a set with nothing in it at the moment we call set intersection intersects the the DQ um with the vector so that's going to be the elements it have that are in common which is two and three they're used with the inserter to initialize our two add elements to set um then a function is is created an STD function style function um it's created from a Lambda within the Lambda we call accumulate we Traverse across the set um X is then initialized by the value obtained from calling F and so finally we can well note that this line is something that will just execute a runtime as I point out at the top um the i o commands we've chosen to make them do nothing when they're executed at compile time so we don't get an error um but at runtime they will still work so runtime what you get in the screen there is Hello World 5 which is basically uh two plus three and we can also fit this within a static assert as well so we can check that 5 is equal to X and if it is then if we've got that within our static assert that will run fine um so we we looked at a project um Within but it's included within quite a large amount of work that is comes along with the metamath project so um the URL for metamath is there it's a good project um it has a few things it's a small formal language to express Express math theorems um and it's accompanied by proofs and tools for the verification of those proofs there's over a dozen proof verifiers listed at the metamath website which you can have a look at but for us we were interested in the main I think it's the only perhaps C plus plus version that is there that's been there for uh well over 10 years it's um written in C plus plus by Eric Schmidt it's called check mm um when you look at it there's a lot of things that are ideal for our endeavors here we'll get 1400 lines of C plus plus and one source file and makes extensive use of the C plus standard Library um 14 headers at the top there and we've got containers being used queue string set DQ Vector pair and map so um a nice selection including some that are Now supported with an STD namespace but others that require the say namespace so for us it's a focus on say and we also have in in that project we've got the the C plus standard algorithm Library set intersection and find functions being used and we've also got i o operations involving stdco and Sierra and other sorted Standalone functions so our project just put CT at the start of that CT check mm and the URL for that is there the changes we had to make to check mm are listed below at least the highlights um certainly we added the constant extra qualifier to all the functions that were involved but the next thing we did for step two is we changed all the global variables to class members of our newly created struct called check mm that we created to get that um working for us um because we we chose that approach in preference to you know adding a const expert before um all of these variables it seemed like a a lighter touch to the the alterations that then free functions uh which is what they all are in this project uh at least before we got to it there I've changed to members of that simple struct that I mentioned um we had one static function scope variable which was also changed to a class member and handled appropriately um compile time file i o is not possible in particular we had to make an alteration to one of the existing functions there read tokens um it now accepts a second string parameter which is used if it isn't empty and that allows us the flexibility to maintain um the code so that it still runs at runtime and at compile time file includes within the mm database files that will be parsing are not supported when processing a compile time an exception is thrown if this occurs so we don't handle that yet and we don't have that and the projects that we've been looking at um and the exception being thrown you know and a a thrown exception is illegal within a const extra program that is very useful because you can have your a message that explains what has gone wrong in that exception um and uh yeah that's that's quite convenient uh we use say headers so anything that was something like hash include Vector is now hash includes say vector.hpp and so on and we also have a script because uh the input files the the metamar database files that we're using need a little bit of alteration to allow us to Hash include them basically so we have a script that places C plus plus 11 style raw string literal delimiters before and after the mm file contents and after that when we're using uh that file we then access that file's contents using a preprocessor macro mm file path and that's then set to the scripts output so the script input of piano.mm we'd have an output of piano.mm.raw perhaps and then we would just pass that extra component to the command line invocation this is just a glimpse into the mechanism of that um this just shows the the main function essentially in one other function to give you an idea um if we look at the main function over here um you know the parts that are runtime are a little uh they're probably far closer to what was originally there so what's happening here is it's just check and see if the argument count is as expected it should be that uh um a metamask database file is provided to be checked by this this program um and otherwise returns an error then we've got the static assert run time you know that that's there's no cost at runtime it just comes down here at runtime and the app was created because we created this as a as a struct now or a simple class we then call app.run and we give the the parameter provided by the user to check and the code runs at runtime as it did it's the static assert that's been added and so over here you can see a little bit more about the function it's being brought in here CE app run and here we've got a check mm app being created and then we've got a little bit of micro magic going on here so we've got a C string text which has been initialized um by hash including the the mm file path and we use x SDR here which you can see defined there and there is required to make the macros happy and now we call app.run here and we're over here we were passing the the file name as the the first argument to run which has got two parameters but one is a default but here we don't we passed an empty string for the first parameter we pass in txt here which is now a C string and we have a little bit of handling of of the fact that this is not a default value coming in to the Run member function and all if all goes well then we're going to hopefully end up with having exit success but if the user doesn't provide mm file path we also accommodate that with the L stair and they just the return is then exit success and that means that the static is here is not going to get in the way of anyone who's somehow using this code to verify uh runtime um metamath databases and my last slide is just looking at a little bit of benchmarking um I'm just using the time uh function uh the Unix type function here there you can see the spec of my old machine I'm using that version of GCC in that version of clang that's identified there we have bigger files than these you can see we're working with something called demo zero.mm which you can find on the metamath website and same with piano.mm you can see the size of these then you can see how long it takes to actually run them piano a little bit longer it's obviously a more substantial file compile times so for the small one that's with GCC it's 2.3 with piano X 8.4 with clang on the on the demo amm file it's 2.45 seconds um piano dot mmm it's up to 14 seconds which is um certainly a bit longer than piano Sorry then GCC is taken there's the the invocation commands which which might be of interest um as I see I was just putting the time command in front of those two invocations and that is uh that's that's all from me for the moment so I'm going to hand over to Jules now uh so can everyone see my screen already yep okay perfect so um in my part of the presentation we're gonna see how to go from uh const expert programming so essentially um going from say or standard containers at um in in contacts per uh context to actual code generation through templates um so to go from dynamically allocated values uh to code we can uh well stick to trivially typed results and use dynamically allocated containers for other computations and then just return of course stud Ras or purely types Etc um in the future we might be able to use other techniques like reflection and splicing which are essentially um procedural uh code generation where you can reflect expressions and splice them back into a function or a structure or anything else um there is there might be uh the prop const uh qualifier which might allow for um data transfer from Concepts per context to um runtime contexts um and if you're a bit more adventurous you might also try to use Circle which has a lot of meta programming features um but we're going to stick to C plus 20 um to what's available in C plus 20. um so we are going to see how to use um non-trivial types such as stud vector or save vector and actually freeze them into a stud eyes um there is a technique that allows you to um to to get stellarized on the right size um or we are going to see also another technique which is which consists in um wrapping your results in constex per lambdas which allows you to explore let's say uh pointer trees and so on um so we're also going to see how this technique how these techniques perform and for that we have a case study which is the meta compilation of a very basic structured language which you might already know it's brain fog so the spec of the language is like very simple um on the left you have the tokens so there are single characters tokens um and on the right you have the C equivalent so we just basically have a chunk of memory a pointer that we can increase or decrease we can increase or decrease the value pointed by the pointer we have very basic AO uh IO sorry and we also have a very basic while loop so this language is string complete so you might be able to do any kind of computation you want with it although you might not want to do it but you you can um and I have a meta compiler implementation available on GitHub if you want to have a look at it this is the one I am this is the one I used in the test uh that that we'll see later um and the language is actually parsed from a context per string uh into an AST with uh three uh sorry uh pointer trees that rely on uh say uh unique PTR so we're actually using the kind of data structures that you would like to use for a classical uh compiler one that's one that's uh sorry does not run at compile time so the first technique to um bring your data over from const expert to nttp is um converting them into a vector I mean basically serializing them into into a vector and then um converting that Vector freezing it into an R so here we have [Music] um a code that allows us to essentially evaluate a function into an array so we assume that phone returns a stud Vector so we can just get the type of the value from it and then we get the size into a a constex per variable so this is doable because well the the return type of fun is not um is not uh sorry it's not trivial but fun dot size is Trivial so we can store it in a context per variable so we can then use uh this context per variable to give a size to the ra template type and um all we have to do from there is uh copy the content of fun uh into the RN and then we can return it and because this is a trivial trivial title we can then store it um in a context per value and use it as a non-type template parameter for code generation um so yeah the only convitz is that uh it requires data serialization and by the way you can take a look at this code in action uh on this guidebook link uh so you do need to serialize your data so you must have um a uh a trivial equivalence of your data so all you have to do is uh to use a stud variants to take care of the polymorphism and uh replace pointers with indexes uh within uh the within the the vector so the trick uh to convert a vector into an array uh was shown to me by Luke Alessandro so thanks a lot to him and there's his GitHub in the in the slide okay so another technique that's uh way trickier but um I would say maybe a bit more powerful or at least it doesn't require you to serialize your data is to wrap your results into uh into lambdas so here again we get the size into a const expert value um and we use it to generate an eat index sequence um and from that index sequence we can unroll um template parameter sorry um we can unroll a bunch of index and just gather all the data into a topo so these techniques allows you to eventually um call evalastopol into it recursively to um to explore pointer trees uh 3.03 sorry and eventually you have a tuple tree uh it allows you to have uh polymorphic values uh without stud variant so you might actually um unwrap the the types from uh from the variance if you if you use them um and um yeah you basically have a an expression template at this point with values stored in the in the Tuple uh so again I gave a link to a code example showing how it works uh in uh in compiler Explorer uh so yeah the advantages of these techniques of this technique is that you don't need to serialize your data um your values can have different types uh unlike uh with uh uh study ra but study can be uh can use I can use variant too so that's pretty much a known issue um and the huge downside is that uh this will basically blow up your computer because um well the complexity of this task is actually quadratic because you have to call um the function every time you want to access to access an element so this is a huge problem and we can see it in performance measurements so here we have um the two backends of the brain fog compiler uh benchmarks so in one case uh so we have two benchmarks we have um a benchmark where we try to compile consecutive loops and we have another one where where we just implicate the loops uh to make um you know to make just a bunch of Loops stacked together um and then we have the two backends we have the ET backend for the president Island technique and the flashback end for the serialized AST technique so the two curves that explode the The Violet one purple sorry and the green one are the ones that rely on uh the Best Buy lamp techniques so you can clearly see that it dramatically increases um like actually very fast and uh the two back and I mean the the two um benchmarks uh that rely on the uh serialized AST back-end um are much more like they grow much much slower and they're actually pretty much linear uh and this is again something we're gonna see uh with other benchmarks and uh by the way if you want to extract data um and compile time curves from from benchmarks you can use my city bench tool which is a suite of tools cmake tools and C plus tools to um run the compiler a few times uh gather sample sample data and aggregate them into into this kind of Curves um okay so this was actually done automatically automatically using a CT bench and we have also other benchmarks that rely um that are based on more sophisticated cases so we have a Hello World program and a manual broad display program uh so the hello world program has a 106 tokens so that's about 106 ASC notes because well one character in in Brent work is one AST node pretty much and when we look at the serialized AST back end um the timings scale pretty well so we have 1.6 again for the hello world program if we repeat that so if we just change them together we get 1.9 seconds of conversion but when we look at the Lambda wrapping technique it's it becomes pretty um pretty much sorry nightmarish as the size of the program increases so at first it's 6.8 seconds which is still okay but when we double the size of the program we affect actively quadruple the compile time so again we're pretty much in we have pretty much quadratic compatimes so we're pretty much stuck with the Lambda and wrapping technique and when you look at very large programs such as the mandelbrot's um visualization program which is about 11 000 um uh tokens or astinos the serialized AST back-end can just compile it in less than 40 seconds and the Lambda wrapping back in uh simply won't compile on my machine so by the way this is the machine I run these tests on is um his laptop it has a 6300u processor and 8 gigs of RAM so it's not a big one but uh still I try to test to compile this overnight and I had a I had a clanger Clank timeout and uh if we double the size of the program in the serialized ASC version it's uh it folds up pretty well and again the lens are wrapping technique just uh uh just fails uh at this benchmark so to sum it up um for the I would say the the meats of of your program I would simply recommend to just use any kind of data structure you want uh thanks to the say library um and then just serialize it into a vector uh to freeze it into an stdra and then just use this third array as an nttp for code generation it's actually fairly easy try to stick with a template layer of code AS minimalist as minimal as possible as you really don't want to write too much template code and um you can test your contacts per functions at runtime is actually very useful this is what I'm doing uh whenever I can in my uh in my meta programs and you also want to make sure that your functions still run at compile time so you actually once you have some kind of context per compatible test to make sure that everything's in check um okay so that's all for me I think we can switch over to Andrew okay so hopefully my slides are viewable so I'm Andrew Andrew and I have been working on a parallelization compiler for coin stick spur um so that you're saying this is the as coin sticks becomes more weighty you use the performance of all these features are going to become a bit more of a concern as compelled time features already sort of raise compel times significantly when they're used um as you can see in some of the larger projects where uh compilation takes a significantly long time when templates and things like that are used quite um vastly so the idea is that we'd like to take a look at increasing the performance of these kind of features through parallelization in this case it's through concept expert so it works for runtime then we only can see if it can it can be extended to compare them and work there just as well um so client always was created um by me um it's a common compiler which adds extensions for parallelizing context for for loops essentially uses um intrinsics to partition workloads from for Loops across multiple CPUs um and then run them in parallel but this kind of means that the parallelization is explicit rather than implicit it requires users to sort of understand their algorithm and how they want it to be split across the CPUs and paralyzed rather than implicit where it would be essentially automatically done by the compiler so that is all about the work got to be required to be done by users and so here is the GitHub link to the cloudhouse compiler so it is accessible usable just now and although the readme is not quite all there yet um but as long as you send me an email or so I can always point you in the right direction um but the readme should be up there soon enough um but as a research compiler so um don't be using it for any production uses um so essentially let's have a look at the compiler magic behind this parallelization process Evanescence in essence it is using intrinsic functions but not actually clang intrinsics they are essentially function wrappers that the client compiler has been taught to recognized at the moment um by the future hopefully they'll be converted densics um but here is a for each from well it's slightly modified from lib C plus plus um so it's a bit neater that's always changed and some um intrinsics have been added to this um function so it's your standard standard library for each um in this case it's just going to iterate over a bunch of elements from a container and provided and the function gets provided a first and a last parameter which is the beginning and end of your container and a function which will be applied onto each element of your container in the next case we have to intrinsics the forces begin and a Trader pair which essentially takes um two arguments the force and the last saturator and is specifying to the compiler that these are the beginning and alien elements of the loop that you will be paralyzing and this is so the compiler knows how to calculate the partitioning and appropriately separate all the um Loops across each CPU then you've reduced variable which I have stated sum is the final values of course which isn't necessarily the right way to put another reduce necessarily or possibly the best name for it um in essence it's just going to assign or recombine all the partitioned elements so when my or when the compiler is splitting um the container of the air across it's going to clone each segment of data that is going to pass across to each CPU or yeah each thread essentially and uh these then need to be combined back into the original um container essentially um so this is what that's essentially doing in this case we're just variables basically saying um here is the container which is first as a beginner Trader motivated by the begin iterator first and here is how we want to then recombine these slash reduce these uh using partitioned order design which is essentially just going to assign each element back to the container in order that was um looked over and then we have pre-ink which is basically saying this was the order that these were um looked over essentially and it's the same as the plus plus first of the for Loop so it's basically just saying process and Order essentially um going from left to right when you're assigning everything back and so it's not too complicated and it is somewhat reminiscent I guess of openmp if slightly more complexity added to it due to the reduced variable but the beginning and that repair I think is sort of similar to in concept at least to openmp in the general ideas um so how do we sort of hate this basically compiler magic because um people don't really or not everybody is going to want to necessarily deal with um compelling physics to get their algorithms or the parallelization from their algorithms um so it's good for Library users to be able to hate this behind something and so they can then give these paralyzed algorithms to users who are less interested in the details they're more interested in actually getting the results from it um so we in this case that's possible but in this case we've sort of used um executors The executors Proposal um because we kind of think it standardizes quite well with um the idea of parallelization of course expert um elements and it sort of also works to hide the serial or to discriminate or to separate serial and concept expert function or paralyze concept for functions from each other is quite nice um so you see below in these this code segment there is two standard for eaches and the first is your basic just the serial standard for each and we're going to pretend that these two four issues are inside of a coin sticks per context essentially so they'll be evaluated in coins to expert well um just a runtime and so yeah bearing with that um and essentially the first stand for each is just your standard for each that's going to actually over every element of array and multiply it by two and then the below one is your coin sticks were paralyzed one uh which is going to iterate over each element and in parallel and multiply it by two essentially of the whole array in parallel so it's going to split it into let's say four if you get four chords essentially and then multiply them and that's sort of a very simple distinction um to separate the serial const expert and the paralyzed concept expert you're essentially just passing an Executor argument to the standard for each and we have um 30 or so algorithms at the moment incliners um and a modified cxx um that basically just uh paralyze and accept these executors that nlum context for parallelization um so what kind of results can you expect from this um actually it's surprisingly quite reasonable results um so it's up to 75 Peak efficiency um for some of the benchmarks we've tested on most of the benchmarks we've tested on are very HPC specific um so they're not very I wouldn't expect these programs to necessarily be used in a production environment for quantity expert necessarily we're still kind of looking for the ideal um scenario for it um but in this case we used five traditional sort of HPC benchmarks um which is basically Black Shoals manual Pro inbody and swaptions and sickle agitation in the case of second large detection the agitation important is perhaps more HPC Benchmark it's just obliged detection algorithm but the second component is not necessarily um your traditional HPC I suppose um so we basically have a not fully complete um quantity expert sickle implementation the um allows parallelization of or context for parallelization of secular kernels and competitive which is quite nice and we tested that um it's performance essentially using Soul Edge detection and so all the days from a four core i5 um 7600k so it's a sort of old CPU um rather than a neurogen um CPU with a ton of performance I know that there is a visuals to find the executions um but from the graph you can essentially see the Azure performs quite well in most cases with endbody essentially sort of reach new speed up um but first and foremost and this is a speed up graph essentially so at the bottom you've got a number of threads and the neglective speed up essentially so for each thread you're essentially one thing or you'd hope the ideal scenario is that you get exactly 100 performance or a one speed up per thread um and in this case you can see that it is not exactly the same all the way across so we do not reach the idea of speed up in all cases but I believe in most cases that is very unlikely um even at runtime or even on runtime code and in this case as essentially showcasing that the end body is actually very close to the normal speed up and then all the other ones sort of fall a little bit short but they still actually have very significant performance increases um across the board and that is it for me so I'm going to pass it over to you so Saints Andrew so as a quick conclusion um we wanted to show that there is a lot of things going on in the context Pro World especially around the fact that contextual allocation and the abstraction inside a standard Library container style of code like CEST is doing uh shows that it can actually get great results on none to your use cases and the discovery and refinement on all the runtime to comprise some information schemes also help a lot to go through complex piece of code the management of compile time azerbai actually going you know all the way to 11 by paralyzing the compiler which is quite a noble approach to the problem or by trying to reach the Gap into a compile time programming tools like City bench does shows that we need more tools to make compile time programming a moment stream a way of dealing with all all the other issue that still are to be solved all in all uh We've shown that we can manage complexity uh in context sphere by using those tools and those libraries uh up to getting a non-trivial uh workload to be uh optimized and run it from fight time and the final word uh there is we want to send CRC International collaboration awards that make this work possible and also shootouts to a bunch of cool people like modulebots for its compared extra things that helped us a lot and generator's work is which was able to give us access to a bunch of binary package so we can actually work properly thanks for your attention I hope you enjoyed this talk and see you next time thank you for your talk um so let me get this straight so no okay thank you for your talk that was really interesting um I think there are a lot of questions um which some of them probably have been handed in the chat during this talk um if you have further questions there is a table in the lounge to you know handle that and the speakers will be at the table for you to have questions and a debate with