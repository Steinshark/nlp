um hello everyone welcome to my talk thank you for being here ever since I talked about um how to rentify your code people asked me about the performance of ranges and uh in my last talk I benchmarked the code a little bit but I didn't go like into detail um into um the performance of all of this and so I thought it might be interesting to use um some tools and try just to analyze the the ranges code a little bit and so here we are so this talk is an outline I'm first going to start um by telling you um how the setup is so how I generated my results um then I'm going to show you a code example and um because we need to compare the ranges output to something else um I wrote three versions of the code um the one that I called C style C++ um where I don't use like any standard Library stuff um I have the C++ 17 version where I use stuff from the algorithm header and algorithms from that also standard containers and then I have the C++ 23 version where I use mainly ranges and with ranges in this case I mean mostly views and um I'm going to analyze all of the three versions or compare them um for just using Google Benchmark then cash Grint and the visual studio profiler and the profiler will be a live demonstration scary and um so I used a visual studio profiler before but I'm fairly new to the Linux tools um because I am a Windows developer always have been uh I grew up with Windows PCS and um so for you the experience might be the exact opposite and um after we look at the output of the tools we are actually trying to optimize all of the three versions of the code and then we run the tools again and in the end um compare all of the results so let's first start with a setup so as I said before I'm a Windows user and I always have been so um also at work I mainly DEA develop on a Windows machine and um also use Windows compiler and so I never really had the inclination to learn Linux or the Linux tools um so I wanted to have the setup as easy as possible for me in order to use the Linux tools that's why I used awsl um that's the windows subsystem for Linux um which is a virtual machine that runs on your Windows computer and it's installs Ubuntu on uh default and then you can use it like um a Linux computer and the nice thing for me is that I can access all of my files um that are on the WSL from the windows and also the other way around so the um the drives are mounted to each other so that makes it actually extremely easy to use especially for me and what also is interesting the computers that I'm using so this is my laptop and we will run the visual studio profiler on this laptop um because I don't have anything else here and as you can see it's not built to do any kind of hard work it's hardly built to do any work that's why I need a new computer I'm saying this for a while now but I I really need a new one um but I also of course have a main PC and this is my ridiculously overpowered gaming PC um which I love it's super fast and as you can see like it's uh huge difference between the setups and this is where I um generated the cach Grint output and also that the Google Benchmark on so so that you know you know if the um profiling result looks a little bit strange it's because I'm doing this on this laptop so now we have arrived on the code I wanted to find like uh a real world example um where I can use as many ranges and Views as possible and compose them all together but it turns out that's actually pretty hard to find and um so I just made up some code I made up some nonsense code where I use as many different range adapters from C++ 20 and 23 as possible I'm going to show you the code I'm not spending too much time on the code it's not meant to make sense it's meant to be compared um for the output um like in a real world application you would have a mixture of all of these um probably but now I wanted to have like a pure output of the ranges and the algorithms and the C style C++ so don't worry if the code does not make any sense it's not supposed to just wanted to um put in all of the features that I found and in order to mimic some kind of real life functions I made three sub functions and in the end one function that calls all of them um if you're it's I don't know why but if you're interested in the code um it's on my GitHub um so you can look at it later and even uh look at the optimized version um uh I will set say it later again um this is only after one cycle of optimization so if I would be really out um to do as high as a performance as possible I would not stop here here so I would do more optimizations this is just where I stopped for the talk and also so the first versions that I show you is what I would write naively without caring for like any performance so um again as you know um you should only just um optimize your code if you know it's too slow if you benchmarked it you know it's too slow then you should start to optimize it so at the stage where you first write the code there's no reason to like um try to optimize everything from the beginning so I'm going to start with the C++ 23 version as I said I wrote three subversion uh F sub functions this is the first one and um so this function takes a uh standard Vector as input and then I'm going to use one of my favorite views which is the cartisian product and what this actually is it is a nested Loop so it combines all the elements of all the ranges that you put in with each other and um so this first range that you input um is the outermost range from your nested Loop then the second one is the in in this case I'm using three is the one in the middle and the third range that you put in is the innermost range of your nested Loop so this is a nested Loop of three um of of three containers and as you can see I'm just using the input range and modify it a little bit for the the innermost range so um cian product as I said is a view and it creates a view of all of these combined elements so you get references to all of these elements and you get a tuple with these references that you then can um address and do something with the values so that means at the end of these parentheses here we're not doing anything we created a view but um I mean it's called cesium product you might think there's like some computation there some multiplication because you get a product no it's just a combination of all of the ranges and then you need to do something with it so I call transform transform gets a tuple of the references of all the elements I love structured bindings so I always use them um I hate the get syntax and then I just do something with the values again it does not have to make sense I just need to use all the values at some in some way and I'm going to return as you can see I'm going to return this View and um when you were in Nik talk and also if you know ranges you you know that this does not execute anything Rangers are executed lazily so um we can return this view here but it will only be executed once the value from this view is needed so once we call the star operator on the first value of this range and if you want to write this function in C++ 17 you have no other option than to actually use a nested loop I love to create vectors and then return vectors in the end so this is what I'm doing here so um I need to have the result stored somewhere it's not like the ranges view where I have um all of this in a lazy way and so I Loop um over the outer range the middle range the innermost range do the calculation um this is the calculation that we did um before in the transform View and I push back the element that I calculated and then I return the vector in the end so this is the exact same code than before just written in a different way and of course we have uh C sty C++ um as you can see it's Ste style so I'm using pointers and uh because it's dynamic memory I need a size um to um for my range I have a new in there which is super scary and then um I need to use indexed Loops um as you saw before I used Range based for loops and now I'm using index Loops because I'm in the C style world but um the calculation itself is the same and here I'm returning this Dynamic array from the function which I need to remember later because I need to delete this so the second function is a short one I really really wanted to use zip with um which is called zip transform in the standard library and I haven't told you that um this stuff is completely implemented in msvc currently but one crucial function is missing in TCC which is ranges two this is why I'm using range 33 here and not the standard Library I would love to do this with the standard library but I'm using just this one I'm losing needing this one feature from uh TCC which is missing so that's why I'm using zip width here which is called zip transform in a standard library and um zip width um combines all the ranges that you input and you step through them like at the same time so so um yeah in Unison at the same time um you have to put the function first for zipwith because um like for internal reasons this is a um it takes a parameter pack for the ranges so you can import as many ranges as you want but then you need to have the function at the first um as the first argument it's not that pretty but it's it's the language and then you input all your um ranges that you you want to combine again here I'm returning the view no work has been done yet nothing has been executed it's the instruction that we're returning here from the function doing this in C++ 17 we can use um St transform um again this a C++ 17 version so there is no range based version yet um of the transform so I'm using the iterators and I'm doing everything in the transform that I did before um also in the transform view but I'm returning a vector here and for the cstyle C++ you you know the drill gets some memory from the Heap have a index Loop to your calculation and then return your memory and the third function we're almost done um the third function just creates two Temporaries here I use partial sum because because I like it um and for the second temporary I use the sliding view also because I like it I think it's very um common that you want to use something like that and um then I can just do something with the values I always call them transform on My Views and then do something with the values and in the end I wanted to have one output and to get one output I'm using inner product which gives me one double in the end that I can compare that I'm actually doing the same thing um for all of the versions and um yeah that's basically it so um because I'm using the sliding View for the temporary 2 um view here um I'm getting one element less than I get from the input range so I need to drop one value which is um what I'm doing in the inner product here for the first range again it it's nonsense code but it uses a lot of features as you can see so we can actually write this quite nicely in C++ 17 um all of this has uh an algorithm in the algorithm header so I'm using partial sum as before with the ranges um I'm also using um here adjacent difference uh which is actually quite nice it is a terrible name for for this algorithm because with adjacent difference you can do anything to the adjacent elements you don't have to calculate the difference that's the standard Behavior if you don't put in any other function um but you don't have to use it so you can do anything with the um adjacent element so this is what I'm doing I'm giving it a Lambda and I'm just doing what I want so name is terrible function is great and then I'm calculating um the inner product and also like a need to shift the um the iterators a little bit because also for the adjacent difference um the first element is not what you want so the first element um is um not the result of the calculation it's the first element of the input range so you typically don't want to use it I actually don't know why they did this it would be very easier to have like the last element be different because then you could just cut your range at the last element if you cut it on the first element then you know you need to copy all your data uh uh in memory and you know everything is bad so for um C style we of course don't have any algorithms here um I should have used some comments in order to make this a little bit more readable but to have it like fit on the slide um this is the parure suum this is the thing with a sliding view with the two elements and uh this is the inner product and in the end we need to delete the two Temporaries that we just created now nothing goes wrong on this point so it we will actually reach the delete so again um this is just a very quick overview over the code if you really really want to look into this in more detail it's it's on my GitHub you you can get it later yeah and in the end of course I return the Su and so the last function is just calling all of them um so the first function I call with the input range that I get overall the second function I call with the result from the first function and then I call the third function with both of these temporary results same with C++ 17 uh note that we could make these cons in the standard Library those ranges but it's non-c in um range with three um I actually don't really have an answer for that uh I guess Nico has um yeah and doing the same with the c style but then again we need to delete all of our temporary because um it does not do this on its own so the first tool that I'm going to use to compare these three versions of the code is Google Benchmark because Google Benchmark is uh a platform independent tool and it's extremely easy to set up and use and I can compare all of the three versions of my code at once which is also really nice so to use Google Benchmark you have to Implement one function um um that takes a benchmark State as an input parameter then you prepare all your um data or your input that you don't want to be benchmarked then you have a for Loop over the state and in this for Loop body you do the thing that you want to Benchmark um I'm using um do not optimize here um from the from the library because compilers are kind of intelligent and I'm calling a function here that returns a double that is never used anywhere and so it's a high probability that the compiler optimizes the entire function call away because I'm not using the result anywhere and do not optimize prevents that so it actually executes the function that you want to uh um Benchmark um then there's this macro where you have to register your benchmark for um um in order to be executed it I changed the output here to milliseconds um by standard it's NCS and it's just a huge number and I don't think it's very readable so I like milliseconds here and then there's a benchmark main which is also a macro and then it calls all of the registered functions and um calculates The Benchmark for this so this is the entire setup I think it's super easy super fast to do and then you run this Al so you run your your program and then you get some output and so the first number that we have here is um for the SE style and it actually runs for 25 um milliseconds then we have the C++ 17 version which is a little bit slower with 35 milliseconds and then we have the Rangers version which is super slow something happened there um don't worry there's actually a problem in the code we're going to fix it in uh in the optimizing round and then we're Benchmark again um but this is an arrow that is not easy to spot I think so um we will see this then in the uh in the optimizing round so next um I thought I look at Cash gr because why not so um V Grint is around for I don't know how how long um and I never really looked into this I mean by why should I have um and cint is used um to um to do performance profiling and analyze and optimize the cache usage of the program so I never saw this like in any talk that someone like um analyzes the actual cache usage of the of the um of the different versions so I thought I just do it and then um yeah with the output we can try to optimize or we can try to see bottlenecks um in the cash use and try to optimize that in the program so if you never used cashr before um you will be uh uh a little bit overwhelmed by the output that you get so I'm going to go through the output like line by line just that everyone knows what we're talking about here and so um if you run this you get this kind of output um I mean Cashin is is a command line tool um for for Linux systems if you run this you should build in um release mode so optimized but with debug symbols especially if you want to analyze it any further because without debug symbols you cannot read the additional output that cach Grint produces um so the first block is the instruction cache statistics and instruction cache um contains all of the instructions of the program when the processor needs a construction it first checks the level one cache um which is the fastest cache and if the instruction is found in this cache then it's called a cash hit and if it's not found in this cach then it's called a cash Miss um and then uh it may have be fetch may have to be fetched from memory and this is what makes your program slow so everything that runs in the in the caches especially in the level one cache is super fast and the further um up you go in the cache until you reach the main memory and or the hard drive then it gets super slow so that's what you want to look out for when you're uh looking at this output so next we have um the cash misses for the level one cach uh and then we have the cash misses for the last level cach in my case um I have the level two and three cache combined here in the last level cache and then um it shows you also a Mis rate which is in this case 0% I call this pretty good and then we go to the next block which is um the data cache data cache is quite similar to the instruction cache um it contains all the data that your program needs in order to be executed so um data that you access frequently will be in the level one cache and data that you don't access frequently will be um stored far away away in the caches like in the higher level caches or on the main uh on the on the hard drive in the main memory so um if the process needs to read or write data it first checks in the level one cache and if it again finds the data there it's a cash hit and if it doesn't find the data there it's a cash Miss um also um cash Quin shows you the read and write operations separately as well um um again if it doesn't find the data in the in the cache it needs to fetch it from Main memory and this takes time so you always want to have your Mis rates low and then you get the percentages again um so in this case we will go over the numbers um later so in this case um the cach misses are actually quite High um and then the next block um is the last level cache statistics where the last level cache like combines the um level two and three cache here on my computer and um it refers to the highest level of cach in a multi-level cach hierarchy um which is what you all have in modern processes and um it also in it contains the data cache and the instruction cache and this also plays a crucial role if you want to optimize your program um because uh the more data you can do in the higher level caches lower level caches sorry um the faster your program is and then in the end we get also statistics about the branches that were created in the program it's um split into conditional branches and indirect branches where conditional branches um are um changes in the execution flow of your program this happens in four loops um in if statements so every time you have a condition then you create different branches and then you jump um according to the output to those different branches and indirect branches are jumps in the address of the register um from the memory location and this happens for like function calls virtual function dispatch jump tables and all of the situations where the address of what you're calling is not uh known at compile time and so the next lines indicates the branch Mis predicts that are occurred and then the Mis rate a branch misprediction happens when the processor's branch prediction mechanism predicts the wrong Branch outcome and mispredictions are bad for the performance of the program because that means that the program fetched the wrong instruction and has the wrong instruction in the um level one cache and it cannot use this instruction it has to throw it away and get the correct instruction so um this is also what what can make your program slow so we haven't really looked at the numbers yet um but I will compare them shortly with all of the versions so you don't need to remember all of the numbers that I just showed you for the C style um this is here for to have the slide deck complete so if you get the slide deck later on and you want to look at all of the numbers yourself um you can do this here also for the C++ 23 version but what I'm going to show you is the comparison because even I can't remember all of these numbers so um as we can see in the first line uh in the first row that um the C style C++ version has the lowest numbers of overall instructions created and C++ 17 um is in the middle and C++ 23 is actually quite High what I found interesting is that the total number of Misses are the same for all of them and um but Mis rates are zero for all of them so it may not be a problem just because you have a lot of instructions doesn't mean you have a problem if the Mis rate is high then you have a problem um also for the data refs um same direction here um what is interesting to note is that we have um very very low data misses in the um Rangers version and that's because for all of the other versions we created temporary vectors that live somewhere in the memory and they have to be stored somewhere they have to be um written and they have to be fetched again if you want to read it this does not happen for this version of the C++ 23 code because once you um request one of the values in very low than in the inner product where you actually use all of the results that that you want to create then it fetches the memory from your one input source and that's actually um quite good in order to like manage all the the data um the data caches because you don't have like any additional memory stored um that you need to f fetch so you only need to fetch this one or two values that you need for the actual execution of the um of the algorithm this is why the number is so low and also uh I said before the Mis rate is super high I think for the um C sty C++ I don't have that much of experience but just looks like a high number and um so last level cash also is very low for the C+ first 23 version which is the same reason um than um we solve with the data um refs we just don't need to do this much um stuff in in the caches because we only have this one input range that we are using and um yeah we only need to access this and then do all the calculations in place branches are quite high for C++ 17 version this does not have to be a problem um as long as the Mis predicts are low and they are almost the same number again for all of the versions and um yeah but it it's it's not a problem unless the Mis predicts are like high but it's just um something that I noticed so this is what I would focus on um just by looking at this output and trying to lower these numbers so now for the fun part we're going into the virual studio and we use the profiler has anyone of you used a visual studio profiler before oh quite a lot actually that's that's good um so in order to use the profiler um I mean you can use it with almost anything um as you can see you can use your startup project which is a visual studio project in my case um but you can also attach it to a running process and executable um oh oh sorry how do I get this wait this is not working wait wait wait we will get this I think so too yeah oh what's happening is there any tech support here yeah [Music] attached yeah better yes okay so um yeah I just wanted to show you yeah you can use your profiler with your startup project this is which is in this case a visual studio project because I think it's the easiest to use but you can attach it to a running process an executable um running app like anything um probably and um it also works with um uh cake projects I'm going to use the CPP usage I'm not looking at all of the other tools I'm building in release mode and um x 64 and I'm just hitting start so this is all the setup I'm doing for the for the profiling you can do this computer perfect and then once you um start start of the profiler you get um like this um top view of the diagnostic session with some kind of statistic here um where all your execution of the C++ stuff is actually in kernel because we don't really have any runtime that we need to start like in C you have your um net runtime that also does a lot of stuff and memory management and we don't have that here and I'm always just clicking one of the functions here don't really care and I'm going to go to the flame graph so I also don't really care for for these ones I want to use um I want to see um where the time is spent so I like to use the flame graph and there you can um click into your program and it highlights where you are in the program and how uh how much time we have spent in that uh function and then also Al um the the size of these uh yeah of these uh of these thingies is uh according to how much time we spend in the function so um I'm not really spending too much time here so um of course we spend a lot of time in the loop um this is what is expected um also for the second one well we're in the loop nothing interesting here and we can see we spend the most time um in the third function where we have these two Temporaries and there we spend the most time in the in the first um function um if I wanted to uh optimize this um the profiler doesn't really give you any hints on what to do now with the results that you get so um you need to have some like optimized um techniques in in in your head already or you need to learn them and so for this version what I would do to optimize it and what I'm going to do is um I'm going to get rid of the two Temporaries here and combine all of the three Loops into one Loop because um I'm iterating over the same data like all of the time and I really only need one Loop for this so this is what I'm going to do then with the optimization um for the C style and this is the first round of optimization then of course you can go back and profile again and see where you spent the most time and try to optimize the next bit of code so if we do the same [Music] for um for the C++ 17 version um we can actually um do relaunch performance profiler which will launch the performance profiler with all the um uh settings that you did before I mean we didn't do any settings but it just saves one click it's nice so this is how the output looks like for the C++ 17 version again um not really spending too much time on this overview here I'm also not using this like extremely regularly and again I like to look at the flame graph and wow we can see we spend most time in Maine that's interesting and here we can also see that we spent again the most time in the third function where we have the same problem that we have in the C Style version that we use um like these three algorithms which are Loops um in the background and we could combine this into one it will lose a lot of readability because um this just tells you step by step what you want to do um but it will lo you will lose readability if you move it into one Loop um but I mean it's it's the trade-off if if you're interested in the fastest code then you will lose some readability that's that's the game um yeah so we look at the same so now this is actually the interesting part because this was the um slowest function so now let's see if we can spot something in the profiler and see where we spend all of our time yeah that looks um fun so let's click one of these again look into the flame graph and as you can see you see nothing um again we're in the main function this is what I expect and then if you click here this is some memory address um we are in range V3 numeric inner product which also makes sense because all of the work that we're actually doing everything that we're executing is in inner product so before we had all the views which are not doing any work um this is the feature of of ranges that are executed lazily and um inner product is the first point where these values are requested and that's where we spend that's why we spend all of our time in in a product so that makes sense but it doesn't explain why we're this slow so we're still in inner product we are in uh basic iterator which I think is called called by in a product so this we in invoke and um what is well hard to read Because this is the type um it yeah does not help me like even a little bit um when in twole algorithm does not help me again and here you can see the um really readable type again um the only thing that I'm noticing here um well is the cartisian product so you it's like all of the combinations of all of the views that we did okay going back to main maybe there's something here we are in triple algorithm [Music] again yeah I I I cannot work with this I don't know about you I cannot work with this I cannot tell you why my program is slow by looking at this output so I defaulted to something that I usually don't do and I um did this diagnostic session in debug mode in hopes that the debug mode doesn't optimize away this much you will get a warning from Visual Studio like not a not an angry warning but you will get a warning that the results are not reliable if you do this in debug mode and I prepared this beforehand because you can see it takes um four minutes on this computer and uh I don't wanted you to wait for 4 minutes so let's see if the deug mode helps us a little bit it's a little bit better so I mean at least shows you some of the functions but again um it's also not super accurate as you as you can see so we actually spend all of our time here and not at the destructor of anything um yeah but now you can see like way more things but we are still all in range with three and we don't know where the actual time is is spent um like we we cannot see it like clearly we see that we spend the most time in inner product again this is what we suspect now we actually see it here in the output and then again if you just click through we are in inner product this is expected inner product and then you have like all of these locations again basic iterator um partial this this again is basic iterator so I I still think that's extremely hard to use so but how I optimized this code but how did get behind what I needed to optimize and for me it is just the type hints so um in this case I saw the cesium product here at the very top and I saw it again here and I saw it here and I saw it here and like all the way down so all of the branches that are executed in my code are calculating the nested Loop which is like really costly like you have a nested Loop over three ranges this is really costly so I thought to myself how does this happen and how does all my my code like execute the cesium product over and over that's actually a problem in the code and um let me see um yeah I'm I'm going to show show it to you later in in the slides so that's actually a problem in the code we're going to fix this this has a reason this has a good reason why we're going to fix this so now let's see yeah it works so now we are optimizing this as I said for the C Style version I just want to move all of these three parts that that I created here with the um Temporaries into one function so that I don't have the Temporaries I have less data that I need to store I have and and address um and also just one Loop instead of three Loops so it should speed up the entire process and you need to trust me a little bit I cannot go into like the details of this you need to trust me a little bit that this part is here in the shorter version the second function which is um like the the sliding thing over the two elements went over here and the third part is the inner product which went over here um so again the code itself is not that important I just moved all of the um Loops into one and I did the same for the C++ 17 but because I don't like raw Loops I used accumulate and did all of this in the um accumulate um in the Lambda of the accumulate so um this is the exact same thing that you saw before just wrapped in accumulate and now the interesting part I think is optimizing the Rangers version and what we can see here um is that we create the range one here by calling the first function and then putting that into the second function and the third function and so the second function now depends on the output of the first function and now I'm inputting this into the last function so now I have two separate functions where where I only have the instructions how I want to um calculate the values and both of those contain the cesium product and this is the problem so I'm accessing both um values and in order to be uh for it to execute this it needs to evaluate range one with a cesium product and range to again with a cesium product and this is what makes it slow and this is I think something that can be easily overlooked but it can be optimized and I optimized this in a um very easy way um I need to force this code to be executed and stored somewhere this is I I needed this to be only executed once so I use my favorite feature which is range two and I store the output in a vector note that I don't have to change anything else of this function or any other functions so because the way I wrote this code I'm returning Auto here so now it does not return a range anymore it returns a vector it just works works and the other function um uses um I think it's only Auto as as input and now it also it just works again so um I should have constrained um like the auto of the other functions um but I'm I don't so um this is all the optimization that I needed to do so now let's run the Google Benchmark again let's see if we improved so for the Sear style um uh C++ we actually doubled the speed it's quite good I think same for C++ 17 it's still slower than C style but also performance is doubled and yay my ranges version is now super performant which I like of course so I also did cach Grint again um just to see if anything like changed um for the output I don't expect you to remember the output that you saw before this is again if you want to get the slide deck um you have all the numbers but um here I'm comparing the optimized to the non-optimized version um instruction refs went down um myth rate is still zero cannot get any better than that um the um data refs also went down which makes sense because we have two less Temporaries that we create and need to write and read um so that also makes sense um the Mis rate is still high um not a not an expert in um how to manage this memory so might be a trick to this um also last level cach went down so again it's the same reason because we have two Temporaries Le less and we do all the work in one uh loop for the last function and so we need to read and write less data from memory so this is what speeds up the program and also branches went down so I simplified the program a little bit so Mis predicts a zero um percent so good program this is the output for the optimized zus plus 17 version and if we compare it we see again that the instruction refs went down a lot also mispredict is 0% um we also um decreased the data refs which again is the same reason than we did for the uh SE sty version we have less Temporaries that we need to write and read and store and fetch um so this is what makes the program faster and um also the last level cach same reason went down and um we also um reduced the branches that we created which makes sense the program is a little bit easier it's a little bit simpler um but still mispredict is not high so it's not a problem that we have these branches at least I think so and now this is the optimized C++ 23 version and if we compare that we simplified the program a lot for the instruction caches because now we have to execute way less um what when down um is the data refs um they went down a lot but the data misses went up and this actually makes sense because now we created a temporary Vector for the other versions We delet we um got rid of two but now we in uh introduced one temporary vector and now we need to write it we need to read from it and this is what increases your data misses um but still I don't think like this rate is super high it's of course like the percentage got up but U this is what we're living with and also the exact same reason why we do now more work in the level two in the last level cach and um yeah we also brought the branches down a little bit and now if we compare all of these so I have a winner for all of the categories so C style wins for the instruction refs um has less refs overall um but all of them have like 0% Mis rate so um for the data refs um Cy creates the less uh the least data refs but um percentage wise um C++ 23 um wins so lowest numbers overall um also last level C we do the Le the least work in uh C plus 23 and branches so the simplest program from the um computers view um is the C sty version but again this does not have to be a problem so um what would be next so next I would try to optimize this again like um run the profile again try to optimize it again if I need it so if this is fast enough then I would be done here but if it needs to be faster then I would go back and optimize again and then um run these tools maybe use some different tools if I'm not seeing the output that I'm expecting and um yeah for the for the conclusion uh I learned a lot about um how to set up WSL um which I actually now really like um it's very easy to to use um even for a Windows user um uh if you want to do like crossplatform it's super easy to use it's integrated in visual studi so you can um uh um compile your code using WSL GCC in WSL in Visual Studio on Windows that's cool um there are things that you need to look out for for all of these coding Styles um Rangers have their traps um you need need to learn them but I think you need to learn like the traps for all of the coding styles that you're choosing um for Rangers it's not that well known yet because not a lot of people are using ranges yet so uh look out for Niko's talks I think he will explore more of these things in the future um as you saw um the performance is really good um in the end um without sacrificing readability I know readability is a touchy subject because some people think that Rangers code is not readable and raw Loops are super readable but I think it's the other way around I like to see the intention of the code I want to see what the program is supposed to do I don't need to see the how it's actually executed but I know this is um yeah people um look at this in a different way um yeah so I like the functional um style of this where you express your intent but I cannot deny that um the profile of this code was not fun so it does not give you any hints on what's actually going wrong at no point in this did it tell me that it's executing um like the cesium product multiple times so I needed to kind of infer this from the types that um were huge um if it's not that obvious and if it's maybe not that obvious to you that cesium product is a nested Loop and this is what it's slow um this is super hard to profile um maybe there are better tools for profiling or we will get better tools to profile this in the future um or some static analyzers that are um analyzing these kinds of things specific to ranges but right now um I I really didn't like the the profiling part and um but then again it helps to use different tools um combine them use different tools um see which output you can get from which tool how you can combine it how you can explain your programs behavior um with these different tools and then if you need to optimize then maybe you can see something in one tool that you didn't see in the other one [Applause] questions hi thank you for showcasing profiling uh it's always nice when somebody struggles it said of you uh I'll just mention one thing so I think you were trying to Intuit it what those counters mean and why the issues occur like I might be wrong like I was half asleep I'm sorry but uh if you want to really understand it there is this person Dennis bahala from Intel who does uh training he has a book and has 1 million things uh goes a lot into what does it mean to have a branch Miss project and all of that stuff uh so I think if you're interested you'd find that useful yeah that's super interesting as I said I'm new to these tools so I learned the tools um with this talk um but also the talk is not about cash so I could not go into too much detail also for this okay so if there are not any more questions thank you for coming have a nice evening