Starting in the 1970s, the world moved 
from communicating through copper to   glass - and that demanded 
a series of breakthroughs. The optical fiber network connecting our world 
is a technical marvel. Light bouncing from one   end of the world to the other, encased 
in some of the purest glass in the world,   modulated to carry billions or 
even trillions of bits per second. In this video, we talk about how 
optical fiber helped connect the world. ## Beginnings In 1933, a boy named Charles Kao was born in the 
city of Shanghai inside the French Concession. Fleeing the chaos of the Chinese Civil 
War, his family moved to British Hong   Kong. Charles then traveled to the 
United Kingdom to do his studies,   eventually receiving his PhD in electrical 
engineering at the University of London. Then in 1960, Charles joined Standard 
Telecommunications Laboratories - the   research arm of Standard Telephones 
and Cables - in the United Kingdom. At the time, the world and their telecom 
carriers sent electrical signals through   copper wires. But the world - even just through 
phone calls - thirsted for more bandwidth. Telecoms thus turned to millimeter 
wave transmitters. Millimeter wave   transmission uses extremely high frequency 
waves - some 35-70 gigahertz - to send data   at high rates. It is today used in 5G 
networks, with somewhat iffy results. The telecoms were installing chains of relay 
towers beaming microwave and millimeter wave   signals from one point to the next. But 
people soon realized that the atmosphere   absorbed too much of the waves to 
make a good point to point signal. You needed a waveguide - a medium 
that can facilitate the travel of   those waves. Like how a paved 
road makes it easier to drive. ## Light's Potential At around this time, news arrived 
of the invention of the laser. One of the first uses people envisioned for 
the laser was in communications. Theoretically,   they can replace all those 
point-to-point relay towers. Light held so much potential for data 
transmission, if not simply on the basis   of its frequency - 3,000 terahertz 
compared to microwaves' 3 gigahertz. Yet Charles' cohorts quickly tried the technology,   only to dismiss it. Glass fibers and the 
term "fiber optics" have been around since   the 1920s. They were even used in 
1954 to convey images with light. But when scientists tried firing laser 
light through fiber optics to send data,   it didn't work. Unless the fiber was perfectly 
straight, there was too much light lost. The fancy term for that is called "attenuation", 
and it is measured in "decibels per kilometer". Imagine a glass window. You 
can easily see through it,   right? Most glass windows have an 
attenuation of 200 decibels per   kilometer. Make a glass window a few meters 
thick, and you could barely see through it! ## Charles' Discovery Charles decided to stick with it and after many 
simulations and experiments, a breakthrough. He concluded that attenuation was largely caused 
by external impurities in the glass - particles   of copper or iron - rather than the 
glass's inherent material properties. He published his findings in a 1966 paper 
- "Dielectric-Fibre Surface Waveguides for   Optical Frequencies". In it, he said that if 
you get the iron and copper impurities to less   than 1 parts per billion, then the attenuation 
would not be 200 decibels per kilometer but 20. In other words, if the glass is pure enough then 
you can see through a window many miles thick. Few people noticed Charles' paper 
at the time of its posting. Most   people were focused on satellites as the future of   communications. But Charles persisted 
in bringing his idea around the world. It did not take long for him to be 
proven right. A team at the glass   manufacturer Corning led by Bob Maurer took on the   challenge. Corning has made optic fiber 
before, but never with glass this pure. Through experimentation, they stumbled across 
a method. You place a mixture of precursor   gases inside a tube, and then spin it around 
quickly while running a heat source over it. The gases react together to create silicon 
dioxide gas. And then that silicon dioxide   gas crystallizes onto the tube's inside 
as a solid, pure fused silica glass. This procedure is now known as Inside Vapor 
Deposition and it is still used today. After enough time, we got a solid 
glass rod called a "preform". Then we take the preform to a drawing 
tower, which can be many meters tall.   It heats the preform - just the tip - and 
then pulls down a fine strand of pure glass. In 1970, Corning announced the creation of a 
pure fused silica fiber with attenuation of   just 20 decibels per kilometer. Further 
attenuation improvements were made by   other teams. With that, optical 
fiber began connecting the world. ## The Fiber Modern optical fiber is a flexible 
strand of plastic or glass capable   of transmitting light from one end to the other. We want to send some data - like 
a digital picture of a Corgi in a   bathroom - over a fiber line. The digital 
picture exists in the form of 1s and 0s. We then take a laser's light 
and modify, or modulate it,   in a variety of ways in order to encode those 
1s and 0s. Then that light goes into the fiber. The fiber is made up of two components - a 
central core that is about ten to a few tens   of micrometers wide; and a 125-micrometer 
thick cladding that surrounds it. The fiber's core has a higher refractive index   than the cladding that surrounds 
it. So once inside the fiber,   that light internally reflects back and forth 
at the boundary between core and cladding. That modified light will basically internally 
reflect throughout the optical fiber - ideally   its whole length - and then exit without any power 
loss. Even if the optical fiber bends and curves. These light pulses not only travel at a very 
high speed - 70% of the speed of light - but   they also do not suffer heat losses, nor 
latency delays due to electrical resistance. ## Fiber Networks That first ultra pure glass that Corning made back   in 1970 could carry 65,000 times 
more information than copper wire. In April 1977, General Telephone and 
Electronics installed in Long Beach   the first commercial telephone 
line powered with fiber optics. That single 1-inch wide optical fiber carried as   much data as a 2,100-strand copper 
cable, four times its diameter. But first generation optical lines had 
attenuation, causing the light signals   to deteriorate over distances. So 
they needed "repeater" stations to   "regenerate" the light signal. This 
involved receiving the light signal,   converting it to electric signals, and then 
re-converting that into light again - or OEO. Then in the second half of the 1980s, 
a team at Southampton University in   the United Kingdom was studying 
fiber sensing - how variations in   the temperatures surrounding a fiber 
affects how light travels through it. From there, they had these 
experiments where they mixed   rare earths like neodymium and erbium into 
fiber cores, which caused them to emit light. Add some mirrors to the fibers 
like with traditional lasers and   fire some light through these cores 
- and you get some very long lasers. Which was cool because it's lasers and all,   but nothing particularly useful. It took some 
time before they saw the real killer use case. Take off the mirrors, and the team realized 
that they had boosted a light signal within   a fiber by 30 decibels. Unwittingly, they 
invented the erbium doped fiber amplifier. In 1986, Bell Labs adopted the amplifier, 
allowing them to amplify the light and send   signals through long stretches of fiber without 
needing to have so many repeater stations. The Southampton team won the 2008 Millennium 
prize for their invention. It was a significant   breakthrough in optical fiber adoption 
that not only greatly improved fiber's   economic prospects, but also 
exploded its capacity limits. ## Wavelength Division Multiplexing These amplifiers not only boosted a 
single wavelength, but multiple ones. This made it practical for the 
telecoms to transmit even more   data through the same fiber using 
different light wavelengths that   won't interfere with each other - 
"Wavelength Division Multiplexing". So we have multiple lasers modulating data in 
those different wavelengths. Then before sending   them into the fiber, we combine them together 
using a thingamajig called a multiplexer. The telecoms have long known 
about wavelength multiplexing,   but never before implemented it because the 
repeater stations would also have to split   all those wavelengths again before 
regenerating them. Not practical. The optical amplifiers 
changed all that and by 2001,   the industry was regularly putting 80 
wavelengths on a single fiber line. If each of those wavelengths has a 
bit rate of 10 gigabits per second,   then that whole line suddenly has total 
bandwidth capacity of about 800 gigabits.   This was a major breakthrough, allowing 
us to do far more with the same fiber. In 1980, we used optical fiber to transmit images   for the first time - carrying pictures 
of the Winter Olympics at Lake Placid. Fourteen years later, optical fibers 
were carrying video images for the   first time - carrying video back 
from the Winter Olympics in Norway. ## Submarine Cables Most of the world's data 
infrastructure on land is modular. This means information might hop from one city to   another. And that modularity makes the 
network easier to build and maintain. But it is a different story for the 
cables that must plunge under the water   and span entire oceans - undersea 
cables. It costs a lot of money   to lay down and maintain an undersea 
cable, sometimes billions of dollars. The first submarine cables used coax technology, 
using a copper core to carry electrical signals.   But as voice bandwidth demands grew, 
the technology strained to keep up. Notably, they were getting fatter - which made 
them harder to produce and maintain. For instance,   the 1976 TAT-6 cable which went from the 
United States to Europe was 2 inches thick.   And at around $180 million, it also cost 
twice as much as the previous cable, TAT-5. Worse yet, the cost curve valued on a 
per-voice-circuit basis was not trending in   a particularly good direction. TAT-6 cost twice 
as much as TAT-5, but its $45,000 per-circuit   cost was more than half that of its predecessor - 
indicating slowing coaxial technology progression. Facing a significant challenge 
from the satellite providers   and with landline fiber optics 
already in the market in 1977,   AT&T took a chance on fiber optics for 
their next cross-Atlantic submarine cable. They spun segments of fiber - 5 miles 
each - and spliced them together to   20 mile long segments which was how far 
apart the repeater stations would be. Then, work had to be done to adapt existing 
submarine cable housings for the new fiber optic   material. After a series of extensive tests and 
sea trials, AT&T laid down TAT-8 in 1988 - with   two optic fibers, carrying 280 megabits per 
second or about 40,000 simultaneous phone calls. There were some initial issues with 
sharks attacking the cables near the   Canary Islands - the cables were not properly 
insulated and the electric fields attracted them. But the $335 million cable drastically 
cut per-voice-circuit costs - tilting   the weight of the industry away from 
the satellites and back to cable again. ## The Boom
Then came the Internet boom of the late 1990s. Data rate needs skyrocketed. 
Throughout the decade,   internet traffic in the US doubled every year 
into 2001. That’s amazing growth of course,   but nowhere near the oft-cited 
statistic of doubling every 3 months. But like any good urban legend it 
did once have a basis in fact. It   was apparently true during an unusually 
fast period of growth from 1995 to 1996. That was a different time and going 
off a far smaller base. Nevertheless,   the phrase circulated like a chant - even showing 
up in a 1998 US Department of Commerce report. In response to this perception of massive demand,   the telecoms spent billions of dollars to build 
and light up national long haul fiber networks. Leading companies during the boom include Level 3,   which at its peak was installing 
about 19 miles of fiber each day. Or Qwest, which began as a telecommunications 
subsidiary of Southern Pacific Railroad. They   took advantage by using railroad "rights 
of way" to cheaply lay down fiber. Or Global Crossing, a telecom company 
founded by a bunch of bankers. They   were founded in 1997 and just two 
years later, were worth $47 billion. Another two years later, they were worth just $2   billion - eventually filing for 
bankruptcy. That was a wild ride. And of course Enron and Worldcom, 
which are known for being some of the   largest accounting frauds in corporate history. At the peak in 1999, the telecoms spent 
$120 billion in 2000 dollars, worth about   $213 billion today in capital expenditure. Much of   this was funded by billions of dollars of 
debt, which eventually became a problem. After the Dotcom bubble burst, a 
long-lasting glut hung over the   industry. Four years later in 2005, it 
was estimated that 85% of the fibers   were totally dark or inactive. And 
just 5% of capacity was being used. This "dark fiber" was a great opportunity 
for the rising tech companies like Google.   One of their big initiatives in the 
early 2000s was to buy "dark fiber"   to connect their own server farms 
and save on long-haul data transport. For a better overview of the fiber boom and glut,   I highly recommend Doug O'Laughlin's piece 
on the fiber glut in Fabricated Knowledge. ## Spatial Division Multiplexing Today, most installed fiber can handle up 50 
terabits per second over 10,000 kilometers. That's great. But thanks to the 
rise of social networks, the Cloud,   and streaming, current capacity is 
straining again. And unfortunately,   current fibers are nearing their 
theoretical capacity limits. So we either need to build 
a lot more fiber again or   do something entirely new. And one of 
the more exciting recent developments   in the latter is Space-Division or 
Spatial-Division Multiplexing or SDM. Like Wavelength Division Multiplexing, 
which we talked about earlier, this is   where we send multiple signals over the same 
fiber path. The difference is that with SDM,   we send the signals over multiple spatial 
paths in the fiber, rather than wavelengths. Light has different ways of moving through space.   They can move the same way, 
but in different frequencies. Most fibers right now installed are 
called single-mode fibers - meaning   that the light can only travel through 
such a fiber in one way or mode. So we can create special fibers called multi-mode 
fibers that increases the number of modes - maybe   even 100 or more. This is done through making 
changes in the fiber core's refractive index. Or we can make single fibers with 
multiple cores sharing the same cladding,   which was previously avoided because it tended 
to encourage what we call interfering crosstalk. And since light can have the same 
mode but different frequencies,   we can layer on traditional wavelength 
division multiplexing for even more capacity. There remain challenges. Multi-mode fibers 
right now suffer from distortion and power   loss issues at longer distances, so right 
now they are best suited for shorter spans. And the cores need to be thicker, 
to accommodate the light's differing   spatial paths. But the gains are big, with the 
potential to 10x again the bandwidth capacity. ## Conclusion In 2009, Dr. Charles Kao (高錕) received 
half of the Nobel Prize in Physics for   making the discoveries leading to 
our modern optical fiber networks. He passed away in 2018 at the age 
of 84 after a long struggle with   Alzheimer's Disease. A life well lived, indeed. Nowadays, fast internet data access is a 
critical contributor to the betterment of   human life. It is worth some time to 
look back and consider the remarkable   infrastructure undergirding this 
massive transfer of data across   continents and under oceans. And we 
are still pushing its limits today. It is a stunning technology. Leaves 
me a little ... light-headed!