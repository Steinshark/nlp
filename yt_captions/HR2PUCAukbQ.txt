At the heart of it, a Large Language 
Model or LLM is just two files. The first file is like about 
500 lines of C-language code. The second file is just hundreds of billions 
or trillions of seemingly random numbers,   the "parameters". But this 
is where the magic happens. Based on current evaluations - which have their 
shortcomings, yes - the more parameters the   model has, and the more tokens they are 
trained on, the more capable they get. The models themselves are economically 
valuable. They carry proprietary trade   secrets and - when separated from their safety 
systems - can exhibit malicious capabilities. The data that helped train those 
models is also valuable. Nowadays,   good and useful LLM data is produced at 
considerable cost, often by educated workers. If more and better data creates better models,   then there is significant commercial incentive for 
state actors, smaller and less ethical AI labs,   or even just hacktivists to bootstrap their 
performance by stealing from a leader. What if someone stole GPT-4? We should be 
talking about this risk. In this video,   a few thoughts about protecting 
these LLMs from theft. ## Big The first and most defining trait of these 
LLMs and their training data is that they are   extremely large and girthy. That size 
defines their cybersecurity profile. In the case of the models, GPT-2 
has about 1.5 billion parameters,   and comes in at over 5 gigabytes. GPT-3 has 
175 billion parameters and is 800 gigabytes. OpenAI hasn't released GPT-4's parameter 
count, but it is probably over a trillion.   Do some crude math and the model easily 
works out to be a terabyte or two. The training data set for these models 
are also just as chonk. CommonCrawl - the   largest publicly available training set on the 
Internet - crawls the web and provides data   archives for training. Before compression, 
the whole archive is about 45 terabytes. For the attacker, the "big" challenge is getting 
these big models out the metaphorical door. ## Data Exfiltration Techniques Exfiltration attack techniques are well studied 
by cybersecurity professionals. One of the most   popular ways for doing it is the attacker 
encoding the stolen data within sent content. You embed sensitive information within images 
and video files. The encoded image looks the   same as the original, thanks to techniques 
that hide the stolen data inside pixels. You can hide a 50,000 line CSV into a 5 MB 
PNG file, which adds about 21 MB to the file's   size. Zip that, and you can easily attach 
it to an email and send it out to someone. Or important data can be encoded 
within the headers of an email;   Or within the options field of the 
TCP/IP protocols - which is used   to optimize the connection. The max 
size that can be encoded is 40 bytes. To prevent these exfiltration activities, 
companies can employ heuristics scanning   to observe the behavior of traffic passing 
across the network or even look into the   data packets themselves - matching against 
targets and flagging anything suspicious. There have been situations where gigabytes and 
even terabytes of data have been exfiltrated   outside a network. It happens. But 
to be frank, it might take a lot of   time and effort to exfiltrate even GPT-3, 
let alone GPT-4, through email headers. Something else might be needed. And that’s where 
the second major nature of LLMs come into play. ## Deeply Embedded
If LLMs are to fulfill their promise, then they must be deeply embedded 
into people's everyday lives. This means wide distribution. Microsoft is really   at the leading edge of this - rolling out 
LLMs across the whole organization with   GPT-4 powered products like Microsoft 
Copilot, Copilot Studio, and so on. Per a comment by CEO Satya Nadella in 
a recent shareholder conference call,   these AI products all run on top of 
the same core foundation GPT-4 model. Now because those models are too 
large for any single GPU machine   to store in its secondary memory, it 
needs to be split up and distributed   over two dozen or so GPUs. Regardless, 
there are a lot of copies of this model   floating around inside a data center. Such 
devices are vulnerable to exfiltration. ## In Use
Data can exist in one of three states. First, the data can be at rest, meaning that it is 
being stored somewhere on a disk - doing nothing. Technically, we know how to secure 
that. You can encrypt the data with   some very strong encryption algorithm 
and then split it up into different   storage spaces - all of which are 
isolated from the larger network. Second, the data can be in transit, 
meaning that it is traveling from the   disk to another location - either 
over the Internet, or otherwise. The security risks here are greater. For 
instance, a man-in-the-middle attack as we   send the model's data from its providers 
to the devices we are running it on. But we also largely know what to do for that. The 
de facto standard for connecting remote machines   is ssh, or secure shell. The transmitter 
and receiver use automatically-generated   key pairs to encrypt the connection. The data 
can also be encrypted before transmission. So for the most part,   I think we have widely-accepted ways 
to protect data at rest and in transit. The final state however - data in use 
- is one that has not been so much   explored. How do we secure the LLM and 
its data while the model is being used? ## LLMs In Use The work of running these models in the wild 
is called "inference". During such a job,   you run the model over and over again - 
generating and then appending new tokens. During inference, the LLM's data is 
primarily stored inside the GPU's   memory - moving in and out of the register. 
There, it is often out in the open. Someone   with physical access to the GPU can pull out 
the model's data using a variety of attacks. In a memory bus monitoring attack, the adversary 
is trying to directly intercept the data as it   travels along the memory bus between the 
GPU's memory and its processing cores. Similar to that is the memory probe, which makes 
me think of alien abductions or spy movies. This   is where the attacker tries to illicitly 
retrieve the model out of the system's RAM. We never give much thought to the 
idea of attacking and pilfering   valuable data while it is in use 
because of the fact we need to   have physical access to the hardware. Just 
keep people away from the devices, right? So for a long time, no one really worked 
on this problem except for those really   concerned about someone pulling out IP 
from a piece of hardware. Most notably,   the movie studios or game console makers, "DRM". ## Going Cloud But things have changed. In the era of 
cloud computing, the model's providers   often do not actually own the servers and 
GPUs that they are running their models on. More likely, they are renting cloud compute time 
from a third party vendor like AWS, Microsoft,   or Oracle. Often, it is the only way for 
these smaller companies to gain economic   access to otherwise prohibitively 
expensive AI accelerator hardware. Now, malicious state and non-state actors won't 
have to breach the lab's actual premises in order   to get access to their data and models. They 
breach the third party cloud compute centers. One might argue that large cloud 
providers have more incentive to,   are more capable of, and have 
the resources to follow the   best security practices. That is true. 
But data centers are big organizations. Employees - even low-level ones 
and temporary workers - can end   up with unsupervised access to 
customer servers. Particularly   during special and chaotic events like the 
installing and decommissioning of servers. ## Tradeoffs Protecting data while it is "in use" 
is tricky because the most obvious   and widely used protections 
negatively affect performance. For instance, your iPhone has something 
Apple calls a "Secure Enclave". It is   a dedicated subsystem inside the 
iOS system-on-chip - creating this   trusted environment that runs alongside 
the main system but isolated from it. Isolation works. But what people quickly 
discovered is that marching so much data   in and out of isolation creates 
bottlenecks. And we already have   enough trouble running data fast enough 
through the Von Neumann bottleneck. Another way to thwart something like a memory bus 
monitoring attack is to encrypt the GPU's memory.   But if you do that, you substantially 
hurt latency - sometimes by up to 50%. People already complain about 
ChatGPT being too slow. If our   cybersecurity theft measures 
restrict performance too much,   then the product itself starts losing value. 
I don't think companies will accept that. I want to do a more detailed dive into 
the problem of physical access attacks   sometime down the line. So stay tuned for that. ## Confidential Computing With all these considerations in mind, 
it has been interesting to look at a   new consortium launched back in 
2019 - Confidential Computing. The consortium's founding members 
included Alibaba, Arm, Huawei,   Intel, Microsoft, and Red Hat. 
Google, Nvidia, Samsung, Meta,   and others have since joined. The project and 
community sits within the Linux Foundation. The consortium seeks to bring 
out hardware-based solutions for   handling data in use across multiple 
environments. At the heart of this   solution is what they are calling the 
"Trusted Execution Environment" or TEE. The TEE does two things. First, it 
can host the application inside an   environment isolated from 
the rest of the hardware. Something like the aforementioned Secure 
Enclave for the Apple silicon chips. This protects the application from external access 
by those even with high levels of privilege. The second aspect of the TEE is that it 
can issue verifiable "attestations" about   the programs running inside itself - kind of 
like a letter of credit. Outside parties can   use them to gain trust in the application, 
knowing that it has not been compromised. These aspects all extend from a single piece 
of hardware sitting on the silicon die - the   root of trust. It is a standalone module 
containing the cryptographic keys that enable   a secure boot. Kind of like the foundation of 
a building, everything else flows from there. Most of the first TEEs were for 
CPUs. First from Intel and AMD. Later, Arm produced a specification of their own. These Confidential Computing CPUs can separate 
entire virtual machines. Their memory management   units are configured with an encryption engine 
that isolates certain pages of memory. If anything   other than the right virtual machine tries 
to access that memory page, it page-faults. GPU adoption of the confidential computing 
model came later. The A100 had some aspects   of confidential compute, but it would 
not be until the now-legendary Nvidia   H100 that the company first provided 
a confidential compute GPU solution. The H100's memory is split into protected 
and unprotected parts. When the GPU is in   its Confidential Compute mode, nothing 
can enter the protected memory area.   And programs inside the TEE cannot easily 
write outside of the protected memory area. This requires the cooperation of the CPU. So 
if you want to run a confidential workload,   then you need to use a compatible 
CPU that can connect to the GPU and   verify its TEE's attestations before 
starting the workload. Once done,   all the communications between 
the GPU and CPU are now encrypted. Impressively, the H100 can do all of this without 
a significant loss in performance. In fact,   the H100 showed a massive step-change 
up from the prior A100. Many security   experts especially those at AI Labs have 
high hopes for Confidential Computing as   a way to shore up what has long 
been a critical vulnerability. ## Model Extraction There is a subset of security 
concerns unique to LLMs and their   peculiarities - attackers remotely 
extracting the model's parameters. The goal is to extract enough information in 
order to recreate the target LLM. In other words,   you are basically training a student model that   learns from the master through a 
series of questions and answers. Many commercial foundation models have API 
endpoints. The attacker can fire inputs to   the API and use the result for the training set, 
building out its student model as time goes on. Now, replicating the whole model might take 
a long time and many queries. Access to the   API might get cut before the attack is 
complete. But if you know what you want,   it does not take a whole lot of data inputs 
and outputs to get a decent approximation. I read an interesting paper from Lancaster   University discussing the 
concept of Model Leeching. With this, the researchers selected a 
specific topic and generated a question   and answer dataset using a far smaller 
LLM - about 100 million parameters. Over the span of 48 hours, they ran 83,335 
API requests at the cost of $50. With just   76,000+ responses, they were able 
to generate a model that generated   answers with about 75% to 87% similarity 
to the original model, which was GPT-3.5. ## Extracting Training Data
Somewhat related to that, you don't even need to extract all the training data 
to properly attack the LLM. There is a class of attacks that try to 
determine whether a certain data set is   present - assuming that said data was not 
internally or automatically generated.   This type of attack is referred to 
as a "membership inference" attack. Since it is known that many LLMs memorize 
their training data and can spit it back   out despite themselves, attackers can 
craft attacks to pull out snippets. On the basis of AI model theft, knowing 
the composition of the target model's   training data can be enough for 
another party to replicate that   model's performance. You can go buy 
that data set or scrape it yourself. But membership inference attacks can also be 
serious privacy data problems. Imagine if we   deploy a model to diagnose a particular 
condition, trained on private medical   data. The right attack can reveal whether a 
person's data was included in the training set. And lastly, it can also bring copyright headaches 
for the LLM provider. It is somewhat of an open   secret that training sets are augmented 
with illegally acquired copyrighted works. The laws around such data use remains murky - but   lawsuits have been filed over a 
certain work being in the dataset. ## Insider Attacks Insider attacks are always a concern,   particularly when we talk about 
assets of interest to nation states. When attacking these organizations, 
nation-states are likely to build on   current systems of asset recruitment. This means, 
recruiting individuals they have leverage against. These insider attacks have happened. For instance,   the FBI's 2019 case that highlighted 
how the Saudi authorities recruited   two Twitter employees to access and pass over 
internal company information on dissidents. Similarly, I want to call out government distrust 
about Chinese-Americans in academia and government   organizations - with stories of individuals 
being questioned about where their loyalties lie. Insider attacks are a concern, but I 
want to be clear. America and American   companies are strengthened by the 
contributions of immigrants. Many   AI leaders were born in China, have family 
in China, and are rightfully proud to be   Chinese. We want things to be built in 
America, not necessarily by Americans. I should also note that the 2023 Data 
Breach Investigations Report released   by Verizon notes that 83% of data 
breaches are instigated by outsiders.   The goal for amelioration is 
through vetting, encryption,   and good cybersecurity practices that don't 
place inherent trust in any individual. ## Conclusion
The cybersecurity best practices and measures specifically regarding LLMs 
are still being built out. But if you look at the history, one of the big AI 
labs - Anthropic, OpenAI, or someone else - will   eventually get hacked. All the cybersecurity 
elites have been hacked at one time or another. The CIA. The US Government (a lot). 
The Israeli Army. Everyone. China and   Iran won't talk about any attacks, but 
they probably have suffered them too. There is nothing special about AI. Data will be 
pilfered. Perhaps some or most of the weights   or training data. And the LLMs themselves might 
make it far easier for attackers to do their job   at scale, enabling greater productivity 
of existing practices like phishing. Because such a scenario is virtually inevitable, 
we should consider the ramifications of such   an event and what havoc it might cause to our 
society. What can happen if GPT-4 or something   like it is stolen? If it is openly released? The 
time is probably coming sooner than you think.