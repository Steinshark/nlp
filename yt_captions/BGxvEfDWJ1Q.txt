recently nvidia announced a synthetic data engine for training artificial intelligence synthetic meaning that researchers can use it to generate fake images of the real world for training self-driving ais the concept of leveraging modern open world video games and their engines to create huge amounts of fake data for your ai might seem like a disaster in the making but recent events seem to imply the otherwise using computers to generate computer data so that we can train other computers is a trend that seems to be working in this video we're going to talk about training autonomous driving ais with data from a synthetic world but first let's talk a little bit about the agenometry patreon i'll make it fast if you like the channel you can support the work by joining the early access tier early access members get to see new videos and select the references for them before their release of the public it's not a lot of money and i appreciate the support thanks and on with the show deep learning models are changing the world around us but in order to work correctly a model has to be trained with data a basic pipeline for applying machine learning to a problem goes like this one you collect raw data related to the specific problem two you label the data according to the problem parameters for instance semantic labeling of pixels this is where you assign each pixel a value so that the ai can recognize what exactly is in the image at a pixel level view three you train the machine learning model with the labeled data sets you validate that model's results using subsets of the data specifically set aside for test runs this is to avoid a phenomena called overfitting a failure mode when a model can't generalize or fit well to a brand new data set then finally you deploy the model into the real world at the heart of this entire process is data frequently this data comes from real life and you need a whole lot of it but many people are uncomfortable with the idea of their data being collected shared and used especially without permission more so when it comes to ai applications in certain sensitive areas for instance companies are training machine learning models to identify cases of skin cancer but that means feeding real photos of patients skin markings into the model another example has to do with the finance world financial services companies want better working models but financial data loan applications customer service logs transaction data are some of the most sensitive there is thus researchers are increasingly generating and using synthetic data the practice started has simple augmentation making slight edits or recombinations to real data to improve performance but it quickly evolved into a way to get the data they might possibly need without spending an immense amount of money or violating someone's privacy probably the most influential example is that of alpha zero its predecessor alphago was largely trained on actual go games played by professionals but alpha zero took it another step by generating its own data using self-play the model simulated games between itself for data to reinforce and improve its own future performance in another example researchers use generative adversarial networks the same type used to make your pictures look like a van gogh to generate realistic photos of cancerous skin lesions so all in all it might seem a little bizarre at first glance but training machine learning models with synthetic data has become a generally accepted practice thus naturally researchers have looked to apply it to one of the most challenging tasks in the industry self-driving cars self-driving cars have world-changing potential most car accidents are caused by human error replacing every sub-average human driver like myself with a highly automated ai can save countless lives with additional environmental and mobility benefits people have been dreaming about robotic driving since the 1920s but one of the first real stabs that autonomous driving using neural networks was alvin which stands for autonomous land vehicle in a neural network first introduced all the way back in 1989 at carnegie mellon university the little car drove a retrofitted army ambulance around the campus alvin's network took in two types of data a 30 by 32 sized video of the road and an 8 by 32 sized rangefinder to properly train the system the alvin team needed road images under various conditions due to the challenges of finding enough consistent real-life road images the team generated them artificially side note alvin's network had just three layers by comparison gpt-3 is said to have 96 amazing how far things have progressed ever since the days of alvin the technology has vastly improved automated driving systems are today capable of amazing feats for instance a kuka robot can autonomously carry a payload to a certain position with millimeter accuracy but these autonomous robot navigators do their navigating in gated neighborhoods with little else going on drivers drive out in the real world one quarter mile at a time a self-driving ai needs to have the computer vision to see and recognize everything an alert human driver is capable of but this requires having access to a lot of labeled data previously that means a real person has to look at scenes of actual sites and trace it out for the ai to consume there are a few public data sets that actually did this for instance cityscapes which provides up to 5000 annotated images of real streets in germany but 5000 in the context of machine learning is not that much and getting significantly more would be prohibitively expensive thus researchers have turned to video games and game engines as a creative alternative the video game industry spends hundreds of millions of dollars to create games with more photorealistic scenes and situations that closely hue to real-life physics researchers can leverage their work to rapidly and affordably create labeled relevant imagery for training grand theft auto 5 is a game released in 2013 and is the second best-selling video game of all time the game is known for its highly realistic environment and driving scenarios shortly after its release researchers started hacking the game to generate synthetic data for self-driving ais they used two open source plugins to capture screenshots of a particular scene in the game just as importantly the plugins allow them to capture the auxiliary information about the scene for instance the pixel bounds of an object's position in the scene like a car or a stop sign or something with this researchers have been able to generate tens of thousands of images of realistic road scenes under a variety of conditions rain fog overcast clear this variety helps the driving ai respond correctly if it were to encounter such situations in real life the initial paper notes that their gta trained models actually perform better than the ones trained with real-world images because there is so much more data it is cool that researchers are using a video game engine to train their self-driving ai but the reality is that gta 5 was meant to be a game first leading to some crucial differences and inconveniences first a video game has to run on a relatively weak computer which limits its capabilities gta 5 first ran on the playstation 3 and the xbox 360. by now this hardware is over a decade and a half old we can do better than that second you need to hack the game using scripts to collect data from it these scripts aren't supported by the original developers and can occasionally cause problems third gta 5's los santos is a twin of los angeles which is an urban environment suburban and rural scenes are not as well represented in the game and last a video game is meant to be fun more than it is meant to be physically accurate and making something fun sometimes means having to occasionally break the rules of physics so researchers have sought virtual environments from which they can have photorealistic images but also have complete control over the objects extensibility and greater ease of use that is the idea behind nvidia's omniverse replicator nvidia released this tool in november 2021 it is part of their omniverse platform initiative which the company describes as an open platform for photorealistic simulation there are two implementations drive sim which is for training outdoors driving agents and isaac sim which is for robots the goal is to allow developers to create synthetic data that otherwise would be too hard to collect in the real world it is built on top of a software platform first developed by pixar universal scene description or usd this is a common language used for rendering 3d scenes kind of like how html is a common language for rendering web pages in some ways omniverse replicator is like a video game it has a physics engine with accurate lighting effects and more there is a world in which you can see and interact with things yet at the same time it is not a video game it is a real time simulator meant to be physically accurate to the real world nvidia published a case study of a situation faced by boston dynamics which faced onboarding challenges when deploying new industrial robots these robots ai have to be tuned to deal with these unfamiliar indoor environments one way to do this would be to send a team of photographers into the deployment area to take pictures of the whole environment but that is expensive potentially dangerous and requires a lot of pre-planning what if you were to generate the photographs with code the team imported into omniverse isaac sim a bunch of 3d cad models of various indoors environments like an office building or a factory from then they can generate segmented training images complete with boxes depth and more a key point is the ability to change the conditions of the indoor environment an ai has to react to a wide variety of different visual parameters scale perspective color lighting and so on all this was possible with this custom tool usd is pretty powerful in this aspect it lets you customize the various attributes of the assets in the scene you can transform the data randomize it and so on such complicated tools can no longer be built by individual or even a small team of developers so nvidia releasing something like this for their use is a big help you know so long as those guys keep dropping thousands of dollars on big honking a100 gpus that is using photorealistic engines to generate training images is cool but self-driving ais have to do more than just recognize the scene around them for instance they have to track and predict where things are so to adjust the driving accordingly this means a simulated world and there's been a few interesting ones out there for such use first is the open racing car simulator or torx this started off as a linux game in the late 1990s but evolved into a popular virtual simulated world for driving agents torx is admirable but it lacks pedestrians intersections and other aspects of urban driving and like gta 5 it was originally a video game which brings up all the other issues we talked about earlier so a simulator that got quite a lot of attention during its 2017 release was car learning to act or carla this is a free and open simulator for urban driving built on top of unreal engine 4 a video game engine developed by epic games carla is a philosophical descendant of the cynthia dataset which provided thousands of labeled synthetic scenes for training self-driving ais the synthia was created with a closed source engine carla's license on the other hand allows it to be shared with everyone carla allows people to put their driving ais into scenarios far too dangerous to recreate and practice in real life like a child running in front of a car for instance researchers can plug driving ais and all their sensors too lidar gps rgb cameras and so on right into carla so that the ai can respond to various scenarios like as if it were in the real world outdoor simulated environments like carla also have real applications to training ais for autonomous aerial vehicles unmanned drones can use simulators to train on search and rescue scenarios and military ones too the entire concept of synthetic data just fascinates me it is another example of today's game in visual engines advancing far beyond their original purposes to the point where their image output can actually transfer over to the real world and perhaps even do more than that it makes me think in an ieee interview nvidia vp of simulation technology rev liberty and says we believe that if you can simulate the real world closely enough then you gain superpowers i wonder where the pathway takes us down the line in the future what happens when a world simulator gets powerful enough to simulate entire alternate futures and make optimizations to them what might that be able to do for us so cool all right enough science fiction that's it for tonight thanks for watching subscribe to the channel sign up for the newsletter and i'll see you guys next time