one way of detecting an facial expression automatically is by using what we call geometric features so these are features that look at the shape of a face for instance the shape of your mouth um the distance between let's say your eyelids obviously when I blink the distance between my two eyelids will go lower and lower and lower lower until it's zero and then after while it goes up up up and my eyes are open again it's a very straightforward direct relation between the feature the distance between two facial points and the expression whether I'm blinking or not um however finding those facial points in the first place is not so simple you have to again turn these pixels in an image into locations of these facial points traditionally what people did is they built little facial Point detect so they were really models that could tell for a given little image patch of let's say 16 x 16 pixels whether that was the location of the eye and then what you would do is you would scan through every possible region in the phase until you found the location that set with the highest confidence IM amdi it's called binary classification or at least it's it's optimizing a single value until you've got the the highest value and that's where you want to be that is problematic in multiple ways it's it's a problem because you have to search everywhere it's also a problem because you can can get stuck in what's called local Optima where you think you're at a maximum but actually it's not because if you have a local Optima let's say we want to maximize the probability of this area being a phase and we're scanning through all the different possible pixel locations and as our prob goes up because we're near to what we think is an I then it goes down now normally we'd be doing something which is called gradi in descent so we'd be fing steps in that direction until we get a flat line so the gradient is zero there and we're saying we're done we found the point this is the location of our facial point however we've stopped here maybe this was not the real of them but there's later and even bigger Optimum and we've missed that because we stopped searching so that's a local Optimum and that's a big problem when you do search searches for facial points like this in a classical binary classification search the alternative solution is to search everything but that's generally too costly to do and costly in terms of processor or resources yeah what we did is we turned this idea around so instead of asking every location in the phase are you the we're going to ask a location hey um where is the I right so it's going to tell us well you have to go 15 pixels in that direction in the in the X Direction the horizontal direction and you have to go five pixels down in the vertical direction and you can do this because there's something called regression which is again a machine learning technique and whereas binary classification only tells you yes or no regression actually gives a real uh a real valued output so you could use it to predict temperature or in this case displacement in terms of the number of pixels in One Direction or the other so we get a little Vector that points from the patch where we asked so let's say I'm looking for my inner eye Corner we get a little pixel here around my eyebrow because there's a relation between of course the appearance of an eyebrow and where my eyes are because my eyes are always below my eyebrows it can tell me well you have to go in that direction you're using data you already know you know about eyebrows right yes so to build suchar models you need quite a lot of data so we generally tend to use th 2,000 3,000 faces where I painstakingly and and my colleagues have painstakingly located 68 different facial points on uh on that phase and we learn from that data exactly what the relation is between the appearance of the eyebrow and where my eye corner is and we do that for every possible variations so we have 68 facial points that we want to know where they are but actually to train we're going to use millions of locations in the entire phase to give the relation between every possible area where we might possibly test and where the facial point is so you might be able to recognize say I don't know a cheek but it will tell you where the nose is exactly so every part of the phase can sort of vote for where the facial Point let's say the eye corner is and then we combine all those votes and you can imagine that a vote coming from a bit further away we'll have more error we'll be less confident but as we are voting in one general direction you get closer and closer to the real point also when you make errors errors tend to be random they seem to go in all Direction so they cancel each other out whereas all the correct predictions will tend to predict in the same direction they will go towards my eye corner so they add up and all the errors cancel out because they're in random directions that's a a really nice way of doing it and we keep repeating that until we um until the confidence is high enough or whether we've reached some maximum number of iterations and we apply a little uh shape model on this so we're not we're using a particular shape model which looks at whether the constellation of points let's say the constellations of my mouth whether they are feasible whether they are possible we're not going to look for the maximum likelihood for the most probable constellation cuz the most probable constellation of my mouth is a neutral face that will mean that when I'm smiling it would say well yes that's possible but it's less probable because normally you have your mouth shut okay not for me my mouth is usually open but in general it's it it would tend towards a neutral face so we're not going to look for the most probable shape but we're just going to look for whether a shape is feasible my my upper mouth my upper lip uh mouth point will not be below my lower lip yeah that's not really possible um there your eye points will not be below the mouth points so those kind of constraints will'll see what is possible and what is not possible and if it's impossible we replace it by something that we think is the the best location but that's let's say that's an error detection filter that we apply afterwards now once you have these facial points you can now start doing some reasoning let's say we want to look at the facial points of the mouth and see whether somebody's smiling so if you have a neutral face you would have mouth points perhaps like this we have the two mouth corners and two points on the upper lip and two points on the lower lip and you get sort of a mouth like this that will be your neutral face now when somebody Smiles these mouth Corners will go up and out and that will result in a little bit of exaggerated perhaps your mouth Corners go up it's quite a joker smile but as you can see you can just look at let's say the angle here and compare that with the angle here Alpha 2 and you will see that Alpha 2 is much smaller than alpha 1 and therefore you can use that somebody's smiling in general we don't actually learn a set of rules like this but these become features in our machine learning techniques again we usually use support Factor machines neural networks deep learning uh whatever is most suitable for the types of data that we have and that's how we detect smiles from geometic features and and this doesn't uh matter if people are I don't know completely different in ethnic terms or in look or in sex or yes there will be differences between gender for instance or ethnicity in sort of the the global shape but the change es in shape the fact that these mouth points go out and up and the angles go down they are the same for everybody for everybody because everybody has the same underlying structure of facial muscles and we all smile in the same way I'll show you the big machine that's uh so this is a 100,000 processor spin machine