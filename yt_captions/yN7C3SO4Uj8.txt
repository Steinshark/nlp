okay hey welcome to the talk and thank you for joining me so this talk is about optimizing multi-ring performance so basically I would want to talk about the factors that actually affects multi-threading performance uh and in general I really would look into the fall sharing and the something came from the standard which is Hardware destructive interference for the specifically fault sharing so before that I just uh want to introduce myself so my name is Shivam and I work for kab uh and I contribute to llvm compiler infrastructure recently I involved in lipy X6 stuff uh in llvm I was Google summer of code student this year too working with llvm as well and yeah I am a student by the way uh this is my final years for bachelors's and I'm really fascinated about particle physics and cosmology so if anyone would like to talk about uh these thing to me please feel free to talk about yeah the outlet of the talk is a bit of introduction of formaly threading and when and when not to use it because and I really feel when not to use it it's m much more important than when to use multi-threading uh and the factors that affect performance of multi code and then have we'll look into F sharing much deeply okay let's start with having a fun analogy so the busy kitchen imagine you are running a restaurant and imagine you are a single Chef in the restaurant so and you have a lot of customers so customers will uh there are many customers in your restaurant and you're the single Chef so you have to prepare all the dishes yourself and how would you do that you will proceed sequentially so you would would make a dish serve to your customer then move to the next dish prepare it serve to the customer and it will take time actually so your customers will be in the end unhappy and they may leave you and they move to the some of Resturant because you're not able to provide them with a quick service so you're the shev like the computer CPU if you're are single then you could do your work but it's not efficient it would take time to prepare all the dishes and serve to the customers and multi thring is having like a multiple chefs in the restaurant so each working on different dishes preparing different dishes and they can serve it very quickly so they can separate uh so we can separate all the task at the same time and they will be doing all those uh preparing all those dishes simultaneously and then you can serve to your customers and customers will be happy yeah so what happen if you do not have enough customers oh in a worker in your restaurant customers running out of a restaurant due to the slow service okay what is formally in computer terms so it's the ability of the operating system and software application to take the advantages of the available CPU course on your system uh basically by splitting the workload across the CPU uh across the CPU course that is available on your hardware and Performing all the calculation mostly independently but it's not really because if something is independent then it's more sort of a paralyzation rather than multi-ring so there are there's a difference when there is a shared resources so you're much more like having uh multithreading we call it multi-threading and when there is I would like to call when there is no independent uh things out there then it's more sort of a paralyzation because then there is no shared resources and then you need to don't need to be care about things what's going on okay let's have an example of a 10 million elements uh having an array so you would like to calculate the sum of an arrow which is having a 10 million elements so you would like a program you would like to write a program something like this which is not multi- theing actually so it's a simple program which calculates the sum of of an array and it goes uh sequentially calculate uh all the uh total sum of an array just 10 million elements but the good thing about this this problem is this it is Trivial to paralyze it is Trivial and it's trivial to concurrency you can you're able to split this problem into multiple threads right something like this you can spawn your thread uh between two halves of an array and then each thread would work on half of the array and calculate the 5 million element sum on an array 5 million sum on different thread and then you would wait for them to complete something called wait to complete which is not anything by the way it's just a joint but just it's Express well so it's wait to complete and you would wait till they complete and you uh basically Mudge your overall result and you will get your total sum so this code is bit paralyze you can see the performance difference in between these two uh the single thread sum and the multi- thread sum and see the difference uh in in their time so you really see that how multi theing improves performance so it's something like this you have single core if you have a single core you can use you can run process sequentially process one process two process three and then process four if you have dual core you can split two process in a single core then to process in a a separate core if you have a quad Cod processor then you can uh place all those processes in different CES so you could run them in parallel what are the generic way to use multi-threading for performance Improvement so you would divide your workload based on the number of available Hardware threads so you partition your tasks uh into n smaller independent action and is the hardware thread so you so you really like to take the advantage of the CPU Hardwares that you have and you would launch those in software threads with each handling their specific tasks and you would wait until they complete their task and if needed you will combine all the results so this is a generic way that how to use use it for the performance Improvement and yes it isn't simple as it seems spawning number of threads is not cheap it's a resource intensive task starting a thread is never a cheap process it takes CPU Cycles so therefore multi theing is much more suitable for more intensive tasks and not a small set of datas a small set of tasks so that's the reason why it is more suitable for the more intensive tasks so you could outweigh the the benefits of it and other things to concern about is imbalance in handling the workload so sometimes one or more thread may take longer complete their task and they delay the overall CPU time so what do you think what's the solution for this so so if a thread taking much more time to doing its work specifically so how do we think what is the solution for this anyone would like to say yeah threadpool might be an option or you basically split your workload into more pieces than there are threads so as soon as a thread completes its work it can fetch other tasks to execute from the que so B basically it's more like a thread Q and other concern is obviously synchronizations when there are shared resources and and just believe me your all CPU time in multi- theing is just related to the synchronization problem because if you have the shared resources everything is concerned about synchronizations and yeah complexity in dividing the workloads during the computation process or dynamically what are the use cases so you would see use cases and web servers where you have to care about all the clients requests simultaneously similarly the database system where you have to answer all the queries gaming you you really care about user experience in gaming so you do not want that your graphic rendering blocking your UI thread so so you really want that each and all process happening on separate threads scientific Computing in the scientific Computing you have the calculations you have the complex calculations that can be think trivially to paralyze so you can paralyze them possibly so it's very important in scientific Computing Financial trading obviously you you have to care about real data analysis so quickly and advantages is it enhance your performance GUI responsiveness obviously because you really want that something happening intensive in the back end should not blocking your UI thread so for that you can think of multi-threading and yeah it's paralyze your occurrences of tasks better use of cash storage by utilization of resources and better use of CPU resources disadvantages that you have to really care about it complex the debugging and testing process debugging is not easy in multi-threading switching of context the over hard switching of context and it increase potential for deadlock occurrence say uh and multi-threading is not easy it's not easy to write multi- thread code so it increase the difficulty in writing a program and you would see bunch of unpredictable results when to use it you just want to use it multi theing for two reasons separation of concerns and other is performance what are the separation of concerns consider a music application for a smartphone or computer and this application fundamentally has two primary responsibilities so it must stre music from the server so it decode the audio files and play them to you without any interruptions and simultaneously it should response your inputs play pause next previous adjusting the volume so you would like to separate these concerns to two threads just for an example by the way streaming thread and the UI thread so one thread is fetching all the audios from the server and other thread will taking your input and behaving based on it second improving performance so second thing when to use multi threading is you would like to improve the performance obviously and there's two thing task and data parallelism task parallelism is basically dividing a single task into subtask so they can execute them in parallel very simple data paradism is each thread performing the same operation on different parts of the data so you may have you may have seen that we have 10 million elements in in an array so you really want here data parallelism something like this which is uh it basically calling a process from taking each element from the data array it's just four elements it not worth it here but in much more complex scenario we have millions of element in an array it would show you some results uh and something to notice here is this is a 4 uh so it would call the process with each element from the data and and there is something special is execution pair what it is it's a execution policy from the C++ 17 it's so it it generally implies that execution for standard Library algorithms in parallel so we have bunch of standard uh Library algorithms such as find count f for each so they can leverage this execution policies in from C++ 17 and that means all the execution of different processes will happen in parallel safely so it says safely but still you have to be care about synchronizing your shared resources so it's it does not care about synchronization when not to use it complexity versus benefit just take these two parameters for your first basis of when not to use it as I said the concurrency adds complexity to your code so it making it harder to understand and maintain so if the expected performance gain isn't worth does this increase complexity give you a benefit over the if you have a very less performance gain you really do not want to use this multi because it really complex your program so if your performance isn't worth the complexity please avoid it performance overhead so starting a thread is not cheap it has its inherent overhead so if task complete very quickly the overhead of launching different number of threads might out with the benefits remember you have limited resources threats consume system resources too many threads can slow down your performance it will exhaust memory because each thread have its own stack space and it even led to Resource contention because of the exhausting memory so know know your uh Hardware that how many codes that you get and please do not outweigh that zone at least uh but sometimes you really want it but you're not able to do that because of something known as over subscription problem uh because it can L to you more exhausting performance so it's very unpredictable Behavior if you if you try to go outside this uh outside the number that you get in your Hardware potential for more bugs so the multi- thread code is a new new world of bugs actually so it's a you have race conditions Deadlocks tiering fall sharing and what not let's look at the factors that affect mully threading wait let's have a look at something very basic caches but why because synchronization mechanisms such as those involved in locking are closely tied to your coherence in the consistency of memory and many optimization techniques try to manipulate this cash quency protocol so you should know that how important cach uh is in this multi-threading scenario to enhance your performance performance because directly related to the caches when it came to multi-threading performance for instance basically a locking mechanism May initially perform a read operation on a lock before attempting to change it automically so it should not invalidate all the caches in different CPU cores so it helps to conserve some latency so there is different kinds of optimizations that uh the take this benefit okay let's have a look this pictures which have which says L1 L2 L3 cache and then memory in the desk so you know you go away from the processors so you're getting a lot of time from fetching the data from there so and their sizes or also gets increases as you go go away from the processor so L1 is the smallest of the size 100 KB L L2 is 8 MB so and yes these all caches are managed by hardware and your memory and disk managed by OS that's why because the thing you create in your software are software threads that's basically handled by operating systems so you should be very careful handling them okay so some terminology that I would like to discuss is cash it what is cash it so when CPU retrieves a requested memory value and it sees that okay this is a already available in the cash it's a cash hit if it's not then it's a cash M so it go all the way to the memory fetching the data cash M and even though the overall capacity you see that we see that it's 100 MB it's live from KB to MB and so but by the way the all the thing that's uh getting from memory to the caches uh it's not come from Whole KB and MB so it it just it just a whole block I would say it's a block or a cache line that's uh get from the memory to the cache so it's around 64 bytes in all the modern processors something like this act tray all right so generating cash hits so you would like to see cash hits in your program then two assumptions that should be true first is temporary locality so when program access a memory location is likely to exess its neighbor locations very soon or in the very near future so one thing that if you design your data structures you design your application in a way so it can uh take advantages of your near memory blocks so you will generate a lot of cach hits spatial locality spatial locality is pretty much similar it's the same memory location that's going to access very soon in the in the near future and the if these two assumptions are not true then sorry to say there a lot of cash misses and cash misses are really really very performance inefficient and there's lot of things that's reloading the caches a lot let's let's have a look with the classic example how how bad is this cash Miss so two dimensional area how does this layout in your memory it would go like 0 1 2 3 4 the first row and the second row then the third row then the fourth then the fifth that's how it plays out in memory so it's by at a time so for a larger arrays thought this for a larger arrays what would be better to be to calculate the sum of this by row or by column what do you think which will avoid the most Cash Cash misses any idea right so let's look so you have rows columns with a few th000 size I initialize the AR with with a single element one is very Simplicity and we calculate the sum by rows and the sum by columns we write it volatile In Sum what is volatile here doing volatile is just for that it says to your compiler that please do not do anything with me uh I'm just here please do not touch me and do not try to optimize me just here not to optimize me please see the difference between the column and the rows or sum you would see the difference in their time and it's really horrible that how how cashm can impact your performance with the overall code Yeah so basically design your data structure so it can take the benefit from the sequential memory order that would avoid your cash misses and just believe me your Hardware loves sequential memory orders so as you can take advantage of sequential memory order please take it because it would provide you with lot of cash hits sequential memory order something like AIG J and if you're jumping around in the memory it's not worth it okay how different codes keep track of The Cash Line that's very important from there we will go to fault sharing so that's very important so we need some sort of organizations around cor so each core is aware of what's happening in in each course or what's happening in each cach lines so basically every single course have its own copy of caches something called Mesi come here with the uh with the definition is modified exclusive shared and invalid let's try to understand just I just keep it on a single slide to understand it quickly so let's suppose you have two coures and they have their they have their copies of cach one is Cash Line a another is Cash Line B in the core B so first Cory going to fetch the data into its cash and first of all its data will be in the ex exclusive State and the cach line status marked as exclusive subsequently now core B cames up and says Okay I want to access some memory location maybe it's a very similar location that that has cach line a currently having but so even even you do not have the same memory reason that xess by core B but but your variables are so much closed in memory so so they are lying on the same Cash Line so so when when when core B try to access some uh some data from that similar cach line and that have the cach line a also currently accessing so now they both are in shared state so Cash Cash Line B will be in shared State and Cash Line a will transition from exclusive to shared and now Cash Line B modifies the data in its cash what's happened with The Cash Line a it gets invalidated because it sees that okay someone modifies the data and I do not know what what's that modification is so in between the in between the fourth step which where you see the shared and shared because they both will be in the shared state after that but before that cash line B will buffer its updated values to the memory first thing after that cash line a will reload it its cash to to get in the shared state so this reloading and the buffering to the memory by cach line B it will take time and it will take some few uh memory uh CPU cycle so that's not really efficient but but to be aware that this saves you from in getting incorrect results so cash coherency protocol is there for saving you from getting incorrect results because each each caches should have the updated values uh what's there if if a cach line updates it last thing you just see is Fault sharing because even if you are not accessing the same memory reasons but but they're on the same cach line in the 64 BYT zone so in this 64 by Zone uh even if even if cach line a is accessing some different memory location cach line B is accessing some different memory location but they're lying on the same cach line and and to to be aware that when when a cach line B modifies it the whole cach line will be invalid for this because Cash Line B will uh basically block this and or lock this until until it buffers it to the memory and it will gets unlocked it by after that and then thread EG gets again in shared State pretty much in this diagram also so so it's the so the last thing just is false sharing and it is called false because the cours aren't actually needing to share this data they just happen to located in so close in memory to end up in the same cach line which have a huge Performance Set and if you believe in compiler you would say okay it's not actually I'm seeing fall sharing anywhere because compilers are good to optimize fall sharing sometimes and to be noted that fall sharing prevents you from getting incorrect results so you should thankful to fall sharing to because uh it preventing you from incorrect result so it means that uh yeah so okay let's let's have a look at the factors that affect multi- thread performance now yes the number of processors so be aware of the number of processors because the performance of your multi-threading code can significantly differs on whether your Hardware has a single multicore processors or multiple processors with few cor so a program with a proog your program will must spawn a number of threads that align with the available course to fully utilize the hardware without leaving the processor on the processor power on the table by the way so so let's have an example to to look what what I'm saying is uh let's suppose you have eight threads that came run simultaneously on your server and if your application creates 16 threads so you think that okay we have a lot of threads then it should give me some potential performance benefit but it will not it might uh wordss your performance due to due to the context switching in between threads because as I said spawn SP in thread is not cheap so spawning multiple thread is not something that you would like to do okay so you so if even if you like to do it you can do it with hard workare concurrency that's coming from the standard in such cases to correctly identify the number of available concurrent threads on your system so You' basically calculate it by hard workare and currency then you you can do your work then you can spawn your thread based on the hardware concurrency return Hardware concurrency it's not uh as simple so it requires caution as it simply uh returns you with the hardware threats that is available on your system so it doesn't con so it doesn't consider other running threats or your applications so it potentially lad to your problems like over subscription where you have where you find your situation in a you have lot of number of threads than then should be on your system in this case you you you can look through using async so s async is basically avoids this issue of over subscription by aware of all the a synchronous call in your system and it can schedule tasks more effectively with creating more threads uh that can handle it efficiently the data contentions and the cache quence traffic we we we have looked at that multiple threads when multiple threads try to read and write the same data they can interfere with each other and there's a lot of cash reloads as you see so that's that's also called Cash ping pong happening around and cash quency mechanism that a change in basically a change in a one cache should reflected in all the caches so that might slow down your performance but it's preventing you I'm saying it again it's preventing you but slow down your performance as you said you seeing this there's Atomic variable shared counter it's a global by the way and it's uh and it's using in these uh two threads T1 and T2 and we are using fetch add even if you think that we using the memory order relx with saying that oh please do not synchronize any other data but the thing is f fetch ad is read modify WR operations so it would eventually doing a lot of uh cash reloading each time when it sees okay I have to do something with shared a shared counter in each thread so it should always reloading its uh cache to see okay what's uh what's the new value of the shared counter that's coming from different threads locality of the data so the data that is accessed frequently should be kept at close in memory to take the advantage of the cache locality as we have discussed cache locality as well so if data is spread out in your memory so the more time will be uh spent retrieving the data which can slow down your performance so you would see that uh basically provides a thread with a chunks of data so so it would lie on the same Cash Line as well and it would provide you with some performance benefit because because uh it's so Clos in memory so if if a thread wants to use the similar memory locations or the closing memory locations so so you would expect it to be better in performance excessive contact switching so if there are too many threads in the relation uh in your in your program operating system may spend very so much amount of time so much amount of Cycles to switching in between which can reduce all efficiency of the applications so you would see that we have intentionally create 100 threads and we performing some tasks with a million of uh iterations so it would not give you some benefit uh it might watchs your performance let's look at fall sharing so this as we have discussed that this occurs when threads on different processors modifies the variable uh that that that are independent but they're lying on the same cach line so this can cause unnecessary invalidation and synchronization of the caches which means that cash have to reload after a cash updates a value leading to this performance degradation and it's very important to know consider this example which having some one Su two two threads uh function and they just calculating the sum of an array what do you think where is where is the fall sharing happening here any idea nope yep it's somewhere in some to it's very close in memory so they will end up in the same cach line so it sums the value thread one and thread two functions summing the value and since some one and some two are defined next to each other the compiler is likely to locate on the each other in the same so basically it's very close in memory to each other in the same cach line basically oh let's look at the what's what's happening with the with the cach line thread one reads someone into its cache since the line is not present in any other cache state so thread one gets it in exclusive state if thread one gets executed first next uh if thre 2 comes in and it tries to look what's what's there and it's Tred to execute something so it would now now thread 2 will be in the shared state if thread one modifies the data basically even the sum one it's not dependent you can see the Su someone and some is not a shared resource but still they lying on the same Cash Line so even thread one tries to modify the sum one thread 2 gets invalidated and you like what because sum to even not the same location it's just about the same cach line so it's get invalidated everything will reload again so the thread one the thread one modification would buffers it to the memory then threat two uh thread 2 is invalidated of of course this time and they will again be in the shared State and after that if you see the threat two modifies the data now thread one gets invalidated and this similar thing will happening again and again and again and again and they they will find each other in shared situation then after then uh one one are modifying one is invalid state so see this someone and some is very close in memory causing everything related to The Fault sharing and just to be noted the previous slides that memory accesses may not inter leave as we have described because uh there might be thread one might might gets completed before thread to so it might not inter leave as such but just to show you that what could happen and and yeah it's not about the coherence misses and coherence right backs also what not like similarly like that so in simple example as I said the compiler might allocate someone and some two on uh to the registers and not in the memory so compiler are smart enough nowadays so they they are able to allocate them onto the registers and they avoid the memory accessing they avoid this these fall sharing issues generally for more complex programs the compiler might not able to do it and it keeps the someone and some two to memory and leading to potential F sharing uh those complex things could be pointer alizing okay to fix a problems uh to fix a problem that's having a false sharing so we need to make sure that the data that we have accessing are lying on separate cach lines so or you can make them to the local a local to a function so they separate they have their separate stack space so so we can update the sum one sum to variable like this to basically use of alignance with 64 bytes so we can align the sum one to 64 B 64 byte boundary and sum to two other 64 byte boundar so now they are on a separate cach lines and now thread one is operating on separate cach line thread two is operating on separate cach line so they are not uh talking to each other no more or you can use a something coming from the standard again Hardware destructive interference size so it's typically provide you the minimum offset between the two objects to avoid the false sharing is generally 64 between the two things but you should look in more detail that so you can use it with a Linus as well so it's something like Hardware destructive interfer in size inside the Lance I don't know what to do with this name and so it would be basically if you if you see this uh structures in this uh data 1 and data 2 would lying on the separate cach line right okay so let's let's have a look because the talk is more about fall sharing so what are the causes of fall sharing what the programming patterns that uh you would find your situation in Fall sharing per threaded data arrays so locating an array where each element is used by a different thread so basically you have number of threads that you build your AR is based on your number of threads can lead to furing due to they very close in memory again something like this so this is a typical programming pattern which is having number of threads uh of some are and you're you're calculating some Anonymous we are is sum and you're placing that into the some uh array so it it will lie on very close to the memory and that will cause a potential fall sharing over here as well and other is metric spiralization pattern so basically it cause FSE sharing so if you have if you have multiple algorithms that work on multi-dimensional arrays so which is having fine grain divisions which means that uh one thread is accessing some elements then other thread would accessing other element then other thread would access some other elements so it's something like this where suppose if like this uh green box is handled by a particular thread this blue is handled by a particular thread so this is not a way this this is actually not a good way and this this cause a lot a lot of uh cash reloading and then then obviously fall sharing so a more nice division would be to divide your metrics something like this so each thread operating on some number of elements in a two- dimensional array so you can you able to AV the fall sharing as I said that you have too much patterns like this in your code which is a struct field accesses with u some fields in an structure either you can place this whole structure over some different cach line or you can you able to do that field a and field be on a different cach line with uh aligning them onto the separate cach line or you can pad it so the that struct would lie on some uh separate cach line if you have multiple structures in your code yeah it's a dynamic memory allocations and F sharing so when a program dynamically allocates memory for the small object so it it basically risk placing the data by concurrent threats very uh closely and so obviously in the same similar cache line and what is the way to mitigate this problem is to to allocate larger memory blocks for a threads exclusive state so instead of allocating each small object separately which might scatter them across the same cach lines allocating a single large area can localize a threads data reducing the chances of of Cash Line Collision or the fall sharing and else if possible aligner objects with the Linus or the hardware destructive interference size all right so we have we have a lot of algorithms in standard that enabled multi-threading so many algorithms in the standard Library have their parallel versions what including sort find replace counter for each Etc and to use them you will call it in the same way but just with a new ex uh just with a new parameter called execution policy for example how do you sort parallely just use execution par in your array from beginning to end this range yes so that's all final summary would be from my side is multi-ring is indeed not easy it's not easy to write multi- thread code you have to concern about a lot of things other as spawning thread is not cheap so synchronizations as well it's not cheap even there is no contention in your program even there is no shared resources so even uh you do not have multiple multiple thread accessing the same data and multiple thread try to lock and unlock the same m M but still the mutex have the high contention so you you have to think uh you have to think the other solutions for that might be might be shared mutexes which uh Victor presented in in in his last talk or you might be using some better synchronization Primitives like uh latches barriers but you need to be very careful about it because with synchronization uh Primitives you try to to use or this point is very important best synchronization is no synchronization avoid synchronization if you can it's not my words by the way so avoid synchronization because everything related to your multi- threading performance is synchronization again and if if possible use standard library to paralyze uh for example the transform and reduce because these algorithms are trivial to paralyze you can you can use your different uh algorithms with the help of this transform and reduce and you can see that okay uh this could help me paralyze my problem or you can use uh C2C Linux perf tool cash to cash Linux P tool to detect such problems like fall sharing and at the end you could do nothing but just believe in compiler that okay this will optimize it I'm not going to care about it so be it like that all right Richard fman my famous uh my favorite scientist by the way and you can keep in touch with me on Twitter just a disclaimer that I do not post much about tech but more about science so yeah thank you so much for the listening this talk okay any questions hi hey uh just to clarify one thing if I have an immutable data structure so constant C++ words and I share that with multiple threads can this alone incur false sharing or is it okay if it's read only it's okay if read only okay uh thanks yep I've got some questions from our online listeners um first question is if there are any points with respect to avoid cash to avoiding cach misses to be aware of while designing data structures that are going to be residing in heat memory sorry can you repeat yeah um are there any points with respect to avoid cach misses to be aware of while designing data structures that are going to be residing in Heap memory so is there anything we um if I understand that correctly the questions asking whether there's something we should um consider when designing data structures for heat memory for the alignment um regarding cash mes I'm not entirely sure it's the online question so I might be misrepresenting it I do not get it actually but what I think is uh so you so we try to see that cash misses to be less right I believe so yeah yeah so it's just about to use your data structure that it can take benefit from the sequential memory order so as so see your data structures if it if it provides your sequential memory locations uh so yes it would take benefit and it would avoid cash misses in such cases so yeah okay thank you okay thank you okay any others okay thank you so much [Applause] much