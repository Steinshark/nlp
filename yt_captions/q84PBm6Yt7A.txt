on developer voices this week we're talking about machine learning and from a novel angle I think because there's plenty of talk out there about what AI can or could do for you is it going to revolutionize your business is it going to take over the planet is it just going to crash your car but for all that talk there's not actually that much said about how we get from an exciting idea to practical production grade software well thankfully this week's guest has been thinking about it a lot over the years it's the fabulous Eddie in the past she's been found assembling production ml pipelines at IBM and Akamai and she's just released a new book about it with O'Reilly called scaling machine learning with spark her book takes in the whole pipeline from enabling data scientists to build their models to how you might deploy the models to monitoring them and setting up feedback loops so they improve and all that stuff and I think it's interesting not just because AI itself is interesting and the current industry Hot Topic but because there's a good chance that the Practical problems are going to cross your desk some point in the next few years whether you're the person instigating AI in your company or you're just the person being told to support it so let's get learning in fact let's go meta and do some machine learning learning I'm your host Chris Jenkins this is developer voices and today's voice is a d [Music] joining me today it's Dee a d how you doing I'm doing great I'm super excited to be here with you today Chris it's great to have you back although the last time we talked in a podcast we were physically in the same room and now we've got to do it over the web yeah yeah different times I guess yeah occasionally we get to travel now but so you have since we last met you've just released an O'Reilly book called let me get this right scaling machine learning with Apache spark right right and the more I've thought about that title this morning it's almost like a haiku you could almost unpack every single word scaling machine learning Apache Spark three big topics and you've got to put them all together in one book let me so where are we going to start on this let me let me tell you my understanding of machine learning and then you can start to talk to me about how it scales or where I'm wrong right you apparently sounds good right so okay the way I think about machine learning is it's a two-phase process you've got a big bunch of data you want to learn from you do some fancy maths on it and you get a model out and that model is essentially a function you take that function somewhere else and you say hey function here's a picture and it's and the model says there's a 70 chance that's a picture of a cat is that roughly how it works out roughly yeah well it's very it's like a specific scenario that you know we're discussing of image classification there are multiple scenarios also but yes it's their process the pipeline is that tends to be naturally a two-phase thing which you've got to think about how to manage and how to scale as well as the individual components in that pipeline well we can split it into two sections like you mentioned one phase is the development itself and the second one is the after deployment when it's in production um of course each one of these would have their own um kind of sub uh tasks or sub phases that people need to build in order to get through the different stages um yeah but roughly like one is the development research um and so on and the second one now that it's in production uh what do I do or how do I bring it to production and now what's what's next and you cover both like you try to cover all of that pipeline fairly thoroughly in the book right yeah it starts from the very beginning of you know what can machine learning do for your business with some examples from big companies uh what other companies are doing to drive more business uh and then aligning the business goals with you know the actual planning of how do we build a machine Learning System uh machine learning pipeline workflow at the end of the day and then goes into all the nitty-gritties of let's get practical this is what we do this is how we can leverage existing tools that we have in our organization uh what are the tools that we might need to bring how to do kind of the decision making process around pros and cons and breaking it down and not just taking um kind of like a blueprint of something else it's like actually thinking what would be beneficial for the organization that the individual is part of which I think is really critical for uh for people to to do as an exercise and also for people who are interviewing you're new to the space so people are there just joining it's good to have that kind of like critical mindset um and and critical thinking um and then it dives into the actual technology how do you do how do you build um how do you uh deploy different deployment patterns how do you monitor uh your model and production and when do you archive it like when it's done then you need to start training again so yeah because there's also the whole iteration part of this process right exactly which in itself is a lot of work um so why what I can see why this becomes actually quite a large infrastructure Thing by the time you want to actually go to production why did you make Apache spark the backbone of that yeah so Apache spark is one of the most adopted and used Technologies in the world uh in the data and analytics space um we used to call it Big Data today we'll say Advanced analytics or analytics at scale or you know different wording but at the end of the day it means that we need multiple machines in order to compute the results that we want to see at the end of the day because one machine is just not enough or it will take forever to compute um and so Apache spark gives us that generic engine to run distributed computing on top of large data and a lot of people has been using have been using it for for that for analytics for data pipelines for scaling data pipelines but actually when it was just started um back at the University lab amp Labs it was it started it was initiated for helping machine learning researchers scale their efforts because a lot of the tooling they had yeah I know and a lot of people know that but a lot of tooling they had and as were not scalable or Hadoop Hadoop produced in mahut were very hard to to work with for data scientists because you had to understand all the partitioning how to initiate the mapping what's going on in the reduce and so like a lot of distributed systems Concepts in order to actually get things done um and so it was built for that it was built for folks saying hey you don't need all this overhead of how the distributed computing actually works and will give you the API that abstracts away mapreduce operations it doesn't use the adoptma produce it use a different completely different um um its own software but it's credibility abstraction on top of that but the brilliant part so and not a lot of people are aware of that um and also because bless you thank you it's already part of the organizations and data scientists today are struggling with getting access to systems to data to you know having their own tools so they [Music] usually being unfortunately being de-prioritized in terms of workloads and supports from other teams uh in the company so they need to be able I believe the best strategy is what are what exists in the company how can I plug in and two resources that already exist um and then how can I build on top of it and Spark is already part of so many you know data and analytics infrastructure so it will be smart move for data scientists to plug into what exists learn about it use it and then you know if they need other tools to support their workflows they can but at least they have kind of like the main engine to to do their work available for them um so using spark as the backbone to the architecture exactly leveraging what exists in the organization instead of trying to bring new things in that we know is is very very hard especially if those need access to um production data for example or staging data or they need to plug into the rest of the of the architecture that the team the the engineering team is is managing so okay so what's um I don't know how spark actually works under the hood you're saying it can connect to Hadoop it must connect to Hadoop and what kind of API does it present and what spark like to actually use yeah so spark has apis that's available in Python and Scala and Java yeah so there's there's also SQL for folks that prefer to write SQL um and it gives us the ability to run distributed compute and there are top level apis on top of data frames and data sets again abstractions of tables essentially data that enables us to operate on top of that tabular data without the need to understand um you know how to manage that compute at scale which is is really really critical so we don't need to think about oh you know this chunk of data is going to be processed in that machine or I need to start that machine with so and so parameters and so on there's no need to to do that so that's really simplifies uh people's lives uh when we think about you know I just need to know the API for tuning and performance I need to know the internals also but again it's more of advanced first of all let me kind of put something together and start working with it um so that's um this is one thing and it what was the second question um what's what's the underlying storage you said it was Hadoop is it still can you connect to different data sources exactly yeah yeah it has a generic connector that everyone can build on top and this is what makes spark this is what I believe drove a huge huge adoption for spark the generic approach saying hey you can connect any storage that you want whether it you know it can be a local file system if you wish um you know data Lakes S3 Azure blob gcp file system um mongodb Cassandra you know everything that's available in the hdfs and Hadoop nosql world uh MySQL right things that are more in the dbms side of things um Hive a lot of systems are still using Apache Hive I can see myself wanting to connect like analytics from a web server log into some kind of fancy ml model out to maybe uh like a relational database to query the model from and that would be straightforward yeah elasticsearch open search if you think about analytics um blog engine kind of a document file format yeah those are you can connect with with spark and kind of Leverage that I didn't see a lot of people putting them together uh but it is possible uh Kafka has Integrations I think it's like a top top level integration with the open source um yeah okay well in that case tell me um a pipeline you have seen commonly put together talk about tell me about a use case and which pieces I would use to make it happen yeah so a lot of iots like smart cars or devices are leveraging spark usually together with Kafka which is really interesting it's like they're bringing in new messages or informations from sensors and then they want to make decisions about for example what's going on in their um in their organizations or in their Factory or in their cars and so on and so they will bring in events this is what we call events of being in the sensors images videos that are breaking down to to images at the end of the day so we're pulling it in we're doing some ingestion process and then we're starting to do the Transformations on top of them because you know it comes in a specific format it might be Json based it might be jpegs and so on and Spark has a connector for binary for data that it's by an area we can read it it knows how to take images in as well it also knows how to work with Json format so semi-structured data which is also uh great because this is something that we can leverage and and of course we need to clean that data and we need to give it a solid format at the end of the day every single data science project Begins the data cleaning yeah it's like what do you do most of your time well 70 of my time I'm cleaning my data 10 I'm trying to explain what it is that I'm doing to other folks in the company um and 20 of that glorious exciting ml work related and and Julie what a lot of companies will want to do is to translate the business objectives into something that relates to that they can pull out of that data either automation or more information for the user or know if there's an accident with the car or something happened or anomaly detection around you know everything that um the the sensors are sensing and kind of makes make sure that the company and that the car is in a good State um autonomous driving is really really fascinating because what it is that they do they're trying to assess what it's on the road some of their their they have a bunch of machine learning work that they do there like all the big companies now are um rallying uh and you know hiring some of the best uh engineers and data science to to do that uh some of their workloads is actually trying to assess uh what it is on the road uh do they need to stop do they need to continue driving what is the speed limit um what other cars are on the road next to them also that's why they have all the sensors and uh cameras around um yeah so it's very extensive Market yeah yeah and there's always the kind of it's the less headline grabbing one but it's the one we see absolutely all the time which is recommendation systems right yeah Netflix video should you watch next to is this the synthesizer you're trying to buy next that kind of thing football job with recommending things that I didn't buy rather than recommend me the same thing purchased yeah there's one company that keeps Ealing emailing me about other engagement rings I might want to buy it's like that's a one-time purchase my friends yeah um Okay so we've got the pipeline like that take your use case let's get into what I would love to deem fancy myths because I know your book covers a number of different libraries for doing different kinds of fancy meths on the input data so you take that data in and first of all you want to understand what is the business problem and that you want to solve and what is the domain of the data so if I know that this data is censors what are these sensors where what are they sensing what would be kind of a normal rate um and then it means either for the individual to be a domain expert and learn the space or bringing in some domain expert to to consult with so first of all understanding what it is and then thinking through what which features would make sense to extract out of that data because we always what we want to do is we always want to combine multiple data sets in order to create better features that explains the situation in the world in a better way because this is essentially what we're going to inject into the algorithm to extract the machine learning model and just Define features for me quickly for those that don't know like the features of the world that we're mapping so in machine learning we name it feature engineering because different than features in software is these features are modeling the world that we want to automate or the problem space and they give us better information than just what we just got from uh you know as as raw data um it's like picking out the columns that we think are going to be important for the model exactly yeah columns that will be representative of the world and making decisions okay so we've got a bunch of features we're trying to push into our model yeah and then we're combining it with extensive statistics Works to understand if those are statistically significant um and the more data we have the better our uh chances of reaching uh significance in statistics this is how Matt works um but there's also a scaling issue in that right so you say more features equals better model but then if you pick out all the features from all of your data you've got a scaling problem because it takes forever to calculate there is always a trade-off and there's also a question like how do I get access to to the actual data right or data scientists like oh I got to this organization how do I get access to the data this is why plugging into the existing systems uh is critical and if the existing systems is already leveraging distributed computing then you can do like how about leveraging that and say I can do all the feature engineering cleaning and pre-processing with the distributed computing system that is already in in-house um right so we're in this world where hope yeah I can imagine a lot of people join a company because the company is excited about doing some kind of ml with its data and your very first problem is just getting and munging the data in some way and if they've got an existing system that's spread over multiple machines that's going to make scaling the pipeline that much easier right that's what you're saying exactly because you're plugging into what exists and it's like okay I just need access to what already exists in the ecosystem and I don't need to You Know download the data or save it in an unsafe space or um you know other things that data scientists are doing um and especially around security people don't a lot of companies don't want their employees to download data to their own uh laptops to process because of Ip and it's sensitive it could be sensitive customer data so there's a lot of gates to even reach that space so if you can process I think they're already saved space that the company kind of already put together um then it would make uh you know data science life much easier because hey they're plugging into what exists rather than trying to um kind of breakthrough security um like would that involve deploying spark to the existing cluster in some way like onto the existing machines or what's the architecture there no so that means I would as a data science I will I would park in it um and I you know might have access to some notebooks or I need to connect through my laptop for notebooks to run things on the spark cluster um but this is all this is fine I mean this is this is how people are working it's like we're connecting to a remote cluster in the cloud uh so we can run our ad hoc queries and ad hoc work until we figure it out what's the right model to to build uh and then of course we want to automate this process we want to have a machine build a machine learning pipeline where we can do it uh repeatedly right as we're experimenting during the experiments um until we find a good model essentially and then we're moving because that's another part of scale isn't it because you don't do this thing where you train one model and go it's great ship it to production you actually have to iterate over and over and that's a scaling of time exactly and this is uh and you also in machine learning some of the things that you're injecting to the algorithms are hyper parameters and parameters so those are going to change the output as well and the accuracy of of the model so what we're doing we're building a matrix of all the different combinations of those so people when they are so data scientists when they are um plugging the data they can also plug in the metrics the parameter the parametrics that essentially tries out build models that leverage all the different permutations of that of that those parameters and gives us the best model and also the results of all the other models and this is something that is built in with spark ml also right Sparky mail has uh the community developed pipelines machine learning pipelines that are very easy to use and very intuitive and they were also able to give it all the different processing of the data including you know everything that can come to mind tokenization hashing everything that you need also the algorithm and also the the param metrics and then it knows how to run this whole pipeline together so you get a fully automated experience and then at the end of the day it gives you the results like what was the best model now you can take it and you know move it forward in the stack okay so that that actually unpacks a couple of things spark ml is an ml package that ships with spark so why are we also talking about things like tensorflow and uh what is it pytor should an ml lib when do I need those different pieces yeah so Emily been ml spark ml are essentially the same thing uh Emily was a live like that polar Library a kind of a legacy library that is still extensively in use and use different apis of spark it's named rdd resilient distributed data sets that are not being optimized by the uh by the spark query Engine versus spark ml which is a library that is newer uh relatively and it is leveraging the data frames apis and so data frame apis going through the Catalyst which is the spark execution optimization execution engine that helps us optimize all the operations that we're running at scale so these are like there's those essentially the same they're just leveraging different um I guess software pieces within the software architecture it's always better to use the spark email Library if you can find what you need there this is great if not you can use dmlib just be conscious that those won't get optimized um using the Catalyst because of like the hierarchy and the spark software itself okay so it's it's like a bit like a query planner in a relational database exactly job plan okay yeah what about um tensorflow because that's like that's got Google hotness all over it extensive research before and what I've seen and learned from customers and users and also from my own experience at the spark has state of the art algorithms that were developed in machine learning however it doesn't always covers all the stacks so there are relatively new research that came up there's more advanced advances in the Deep learning space in the um neural network space and sometimes spark scheduler itself the mapreduce can become a bottleneck for running uh compute or Advanced compute of a neural network because what's happening in the neural network there is a back propagating in the forward propagating and that means I'm doing a bunch of computations and then I'm doing forward propagating I'm taking all the results on these computations and I'm moving on to the next phase of the computation within my network but then I want to do back propagation so I want to go back and actually recompute some of the things because now I realized that I did a mistake so I want to fix what I did before is kind of like a back and forward uh in inside this this graph of compute at the end of the day that map produce can support but it doesn't do it in the optimal way and it has a bunch of bottlenecks and um you know things that the community is still trying to uh to solve the community did introduce um new scheduler to um to do that but again it's very in spark but it's very very early stage and Pi spark and tensorflow are more advanced in that this is you know when they started they they focused their effort into neural network processing uh kind of the the Deep learning space images and and working with text and so because they were investing in it for so many years they have better tooling and better system and then the question was you know I want to really leverage what exists in the organization so I want to go with spark and then how can I Bridge into other technologies that can enrich my my tool set at the end of the day so I can fully you know build the models and get the quality that I'm looking for so this book tells the story it's like here are all the wonderful things that you can do and then if you need more Tools in Your Arsenal as you continue to developing and go forward then hear how you can Bridge into um these systems as well and running those um at scale so I'm I'm diving deep into uh like how how they scale what it is that you need to do how it looks behind the scenes why their architecture is different from each other and also different from the spark architecture um and so on yeah okay so you really you're going all in on this thesis that spark is that Universal architecture to get in and grow from listen it's everywhere give me a company that needs to run you know distributed computing and advanced analytics and doesn't doesn't have a notion of spark you know I speak with people I speak with companies that you know most of where their work is like log and logs and analytics and some stuff and then oh you know what we also have some spark because our bi folks needs to have their analytics and the the SQL you know the spark SQL works really well on top of all this data so they can make sense of of the bi in the organization so even if it's not something that people are leveraging to um serve their customers and then they they're still like internal analytics or other things that companies are doing so yeah yeah getting hold of the data and connecting it and processing it and sending it somewhere else it's just utterly Universal right is um this is slightly an aside but spark sounds like it sounds a very similar space to Flink way uh although slink was built for streaming right there are still Notions of not produce and in memory and so on but it's it started getting the streaming Space versus uh spark also has structured streaming but it's not it's not the main focus it has batch processing structured streaming SQL and Hawk if people want to or SQL if they want to they address machine learning there is graph there are graph algorithms um so it's it's not the same but it is I mean I can understand how people can get confused sometimes um about these things but it's a different uh it's a different technology although they do leverage you know distributed computing they do leverage some of the basics of of that space which is always interesting especially for people who are moving from one tool to the other um so it kind of makes sense oh you know this is how it works in spark I can see I I can better understand now the flank architecture because I've seen something similar right okay yeah okay in that case let's move onwards um out into production what do you do once spark has given you a model that you can do lovely predictions with [Music] yes on what it is that that we need right so it can be within a microservice right it can be on its own service it can be part of a batch processing that we're running it could be part of a stream processing that we're running in production if we have kind of a stream data pipeline so all of those depends on taking dysfunctions where do I need to plug it in um and Spark enables us multiple you know capabilities if we want if we have already a spark Pipeline and the spark batch processing it's easy to plug in load it and leverage that as part of the pipeline that I already have uh same thing with uh with streaming it's easy to plug it in and and leverage that and also if we're working with tools like ammo flow and I'm covering ammo flow also in the book as part of the ecosystem that supports the work that we do and then ammo flow enables me to wrap methods to wrap a kind of a function and take it and deploy it as a as a function and so I can leverage that in microservices environments as well so as its own if I want to you know put the rest API and kind of have it as its own service and then I can query my machine learning service at the end of the day or if I want to have it in service so attached to an existing um server service that's already running there are some things to think about like around scale and deployment like if I am wrapping it within an existing service that means that my deployment Cycles are also going to be attached to the deployment cycles of the actual service like if the models and the service are not changing um at the same time it's something to bear in mind uh also it might be better to um to kind of unpair those uh also another thing to think about between those two is uh the the hardware if I need GPU or if I need some specific hardware for to run the model in production to get the result fast then I might want to also not put them together I don't want to kind of have that pairing um on that so there are pros and cons for each one of them it really depends on what I need uh what are the release cycles that I have within the organizations uh how those are being tested uh what's the hardware that I need because machine learning could be different than the rest of my software yeah if you're generating images for from a deep learning model that's going to need some fancy gpus right yeah yeah and then you want to be as efficient as I can and I don't want to run any um workloads um on the same server because I want to be efficient and leverage that for the purpose that I you know yeah yeah you don't want every machine in your AWS cluster running the most expensive graphics card that's because some of them need it and looking at the utility and I'm only using five percent of my GPU but I pay for it I'm certain there are companies out there doing that right now I hope they buy a copy of your book um because yeah this is it because we think like deploying ml models is just I've baked my function and that was the hard part but getting into production there's actually quite a chunk of the book you cover dealing with that right getting into production and also monitoring production like Wendy's yeah when do you know what's when you need to run another cycle right when do you know it's a really good question thank you I thought of it myself uh it depends on the use case and the model but essentially what we're looking into and we can get extremely fancy it's like oh let's do windowing how the data is changing how the the model uh the data that we're injecting into the model and so on one of the things I've seen work best with companies um that I worked with and um and helped um is if you can compare the expected results which like we're looking at the model the model gives us accepted expected result with the actual result and then assess accuracy that would be probably the most cost efficient uh way to assess the quality of the current model that runs in production there are more fancy methods like um data drift model drift business Drift But those requires more heavy processing of the data always you need to develop specific data pipelines in order to say oh there was a complete drift in this data I see a bunch of anomalies and now it's like you know the average it's not five now they average is 10. should they do something about that to actually impact on the model and the results um which is great and some companies would go that path and if you know they can't find a better more optimized way but if they can it's usually better to compare the expected versus actual and then get live you know feed of accuracy of the model in production and builds kind of as a feedback loop give me a concrete example of that because I'm trying to think you're not saying I don't think you're saying Dave sits there checking that the pictures actually do contain cats maybe that's what you are saying I don't know so let me give you let's say I'm taking um um a ride okay um ordering a taxi or a cab and they give me kind of an estimation of how long this ride should take right you know you'll be you'll be home by 8pm for example yeah this is the prediction right if you'll take this this camp now you'll be home by 8pm versus if you'll take the the bus uh you'll be home by 8 30. and then I can decide if I okay I want to order the camp and then I can you know the system can track how actually like when did I actually got home tonight or when did I actually got to my destination so this is a great example where I have the expected and also the actual um and so I can compare the two and know if my model actually delivers on the result or it was completely you know far off um from the actuals right yeah I suppose uh recommendation engines will be similar right I can show you five things you might be interested in and see if anyone actually clicks on those yeah for example okay so we can automate it without Dave looking at cats all day I mean you know once we've automated it that frees us up to look at cats that's the other way of looking here um right there'd be so much that uh monitoring would be a part of this but of course it will be that's just every large-scale data system these days is intimately worried about model monitoring yeah every software in the world should think about monitoring and observability and what is actually happening in the system because when things go south you want to know before the customer is being impacted right it's never fun getting in this call from the customer saying oh you know you just killed I don't know half of my infrastructure or you know if we're talking about kind of a BTC or people complain or kind of abandoning the application it's never it's not a place where we can go to uh as we when we build companies and we build software so monitoring is critical and machine learning models in production are similar thing they serve customers and they need to be monitored and even monitored more extensively because of the unknown that you know it's hard to explain what the why the model is making decisions it's kind of like a box that we cannot see through um it's the black boxness of ml that means it's all the more important you see what's coming out as well as going in exactly and explainability is really hard I can run a bunch of you know I can get fresh data I get it into the system get results but not really know why the model got to those specific results um like what why the output the way it is and so observability becomes and monitoring and become a more critical part um of that so I know when to switch right I know when to retire the model and kind of rerun the automation for training another model and deploy that new one to production well this makes me wonder because as you know I'm interested in the world of real-time Computing to what degree are we getting to the point where models will be automatically relearning and redeploying or is it always going to be this two-phase batch thing or are we going to be you know constantly improving the models automatically that's a good question um I believe we'll always need to have humans um observing or overseeing the process um and Ticking boxes making sure things work well um like there is you know some Visionary in the industry would say oh everything would be automated but things always go wrong with data being ingested and cleaned and um the data itself the distribution of the data itself is changing so we need people to tweak the the algorithms uh as well um so I don't I don't see us getting into a place where everything is 100 automated I do see us getting into a place where people uh more people are um able to create that pipeline because of the tools that exist in the ecosystem and the new tools that emerge so it's definitely it's machine learning is a growing space um and we're seeing it especially now now it's like booming open AI has you know showed that machine learning at scale is critical also in the deployment part and also on the training part um yeah doing more and more of it more and more professionally at more production scale more production scale better customer service automating a lot of things that were used to be manual work that people did um and in enabling people to do more essentially yeah well then in that case let's bring it back down to the small to finish um if this is something I thought was in my future should I get started with spark you know if I want to just play around with ML on my laptop is spark a good place to start or is it something I only take once I'm looking at getting into production it could be I mean there are of course um simpler ways to start with machine learning but spark has this really good apis and today you can if you have Docker installed there are images it gives you Pi Sparks or python spark plus notebooks so you can get started right away and I believe it's a better tool to learn with because you actually gain the experience with tools that available and are used in production so it goes beyond just you know producing a model for the sake of producing a model but actually learning a tool you know throughout producing a model you're actually learning a tool that will be useful for you for you know for a career so you think it's worth that slight overhead to go to instead of learning to build toys to build things that could potentially be production Worthy yeah and people can still build toys I mean I you know I can I have a Docker with pi spark a notebook I can you know launch it on on my I can run it on my laptop and I'm building something it's not scalable I'm I can't ingest tons of data into it because I'm running it on one machine I didn't connect it to any cluster that actually runs the distributed computing but the nice thing about it is the same code if I connected it to a distributed cluster would run so yeah yeah you can take it out into production without massive code changes that's always a great thing to have a starting point right most people don't have that starting point most people like at the second they want to go into production they're blocked on free engineering yeah and and this is one of the challenges also with data scientists so they create this great models but they're using tools that you know the rest of the team don't know how to take the production so [Music] 13 cup spark and a copy of a D's new book yeah the book was sold out on Amazon uh but I think there are now it now should be back in stock yes I know I got a couple of messages from people saying I really I can't find a book because it's sold out uh so that's a nice message to get as problems go as an author yeah yeah and you know I find myself you know reaching out to do Riley team was like hey what can we do how can we help them how can they get a message when it's back in stock and so on and so uh um yeah uh now it's back uh so people uh people can go if they want a hard copy it's available now and hard copy as well cool well I think it's a good chance I might see you in person um next week at a conference we might both be at so hopefully I can get a signed copy yes 100 will be your pleasure if you're talking to us Chris thank you so much it was a lot of fun um and I hope you enjoyed as well cheers see you soon see you soon [Music] thank you Eddie if you'd like to learn more I'll put a link to her book in the show notes and a link to Apache spark if you can't wait that long to dive in before you head off if you've enjoyed this episode please take a moment to share it tweet it rate it review it subscribe to it and thumbs up it if I can use thumbs up as a verb which I just did because all that stuff is the feedback loop that helps me feed forward into future episodes so it really helps but until then I've been your host Chris Jenkins this has been developer voices with ADI thanks for listening foreign [Music]