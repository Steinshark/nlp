and basically i'm just gonna do essentially what we did with the handheld version just now that it's on the robot today we are looking at the frontier device it has an embedded computer a multi-camera system a lidar and inside there is hidden an inertial measurement unit the cameras work pretty much as our eyes the lidar basically allows you to get a point cloud of the environment by firing lasers at the environment and by measuring the return time it can get you a metric measurement of what's around you and the imu works pretty much as our internal ear it gives us an idea of where gravity is and how fast we are rotating all of these sensors can be fused together to answer the main question in robotics called slum simultaneous localization and mapping so the localization bit is knowing where you are and mapping is what's around you islam is pretty much like a chicken egg problem you don't know where you are you want to know where you are in the environment but at the same time you also want to know how the environment looks like if you walk around you're going to see an orange line that shows where we were before so if i keep walking around and i come back to the same place i was before i can show you the main ingredient of slum which is loop closure there you go so here you see a red line this red line means that the system who recorded all our previous position recognize if we were back to the same place and in this way we can correct all of the errors we have accumulated this far and have a better map by adjusting it knowing that we were there before the screen was just to quick visualization but all of the problem is being solved in 3d so here is the exact same problem we have sold before so here you see the table we see all of the map in 3d being built and the loop closure is right here so after just a very quick walk we have a complete 3d map of the environment and we also have our past trajectory the map is mainly built using the lidar because it gives you a continuous stream of point clouds but to accurately know where we were uh we also need to fuse the imu because there is physics inside so the army tells us your acceleration for instance so mu is inertial measurement unit yes if for instance there is an error of registration between two point clouds and we don't use the imu the system i just think we teleported somewhere else but the army is telling us no your acceleration was like that so your velocity should have been like that and you should be here instead of somewhere else so if you use the imu on its own it'd be kind of dead reckoning it would be guessing yes yes so there is a component of that draconian but this is fused together with the other sensors so if you were doing that dragon all of the time then we would end up on the moon because there are many smaller errors that gets integrated over time so the imu is just giving us an idea of where we should be between two camera images or between two lidar uh point clouds and then the measurement from the point clouds and the cameras kind of correct those and are merged together so we can have a walk and go downstairs in the lab so we unofficially call it the bath lab and the reason for that is because we have a huge elevator that can carry an entire car here we are going to show the quadruped section of our lab what's funny is the camera can see some of the lidar yeah yes because it's um it's near infrared and camera don't have infrared filters there you go so here we have different quadrupeds today we're gonna see the spot one this is all the way from where we were before here with the the round table the loop closures and we went downstairs so all of the compensation is inside here but for ease of visualization we have some interfaces on the phone and on the computer most of our software is compatible with ros which is a framework that is widely used in robotics community there are many libraries and programs ready to use to accelerate robotics this one in particular is called roseboard and it's running a little server on the computer and on the other side there is a client that gets the outputs from our algorithms as messages and these messages are then relayed and visualized in the in in the web page web browser interface so on the phone that's just basically running yeah i mean the phone is yeah it's just an optional that we we use especially when we do handheld missions we want to have an immediate feedback of what's going on everything is going all right so we have developed this interface slam is very useful for robotics in particular because it allows us obviously to know where the robot is and what the map around it looks like so um i'm just going to drive around the lab a little bit and then we'll be able to see kind of uh what the lab looks like [Music] and now that we're kind of done again you can see we have a map of the environment we have a loop closure and slam is essentially a key component in enabling robot autonomy so a lot of the work that we're doing in kind of remote inspection mission planning all of those kind of things relies on the ability of the robot to know where it is and the structure of the map to actually do useful things um you know nuclear decommissioning those kind of things is what we're um looking at the handheld has its uses but also if we can do this with a robot obviously we don't have to send people into the same places sometimes it's easier to access places with humans so the handheld device is useful but other times we prefer to use a robot so we have both options this is the one we saw in bristol yes on the front we've got what we call a frontier device it's running the exact same mapping software but we're just not running the autonomy part of the system right now what i'm going to show is the algorithm that is running inside the frontier and the main technique that is using it which is called factor graph so a factor graph is a way to solve this islam problem uh by modeling the unknowns as nodes so we might have uh our position x1 and then position x2 position x3 and so on and this is what we want to know in addition to landmarks which are basically points in the environment which eventually will make up our map so we can think of the landmark says l1 l2 l3 and so on and the measurements that come from camera or imu or any other sensor create a relationship between these unknowns the imu might just tell us okay so between x1 and x2 your position should have incremented by this amount because your acceleration was this much over this period therefore velocity and therefore position and at the same time you can have you know multiple imei measurements so you connect this and from x1 you might just see also things from the camera features in the environment for instance a corner that you can track over time so this creates also some other relationships between our position and the landmark and you can see the same landmarks over different times so you can create these sort of relationships and if you keep adding all of these constraints then you have a strong idea of where you should have been and what was around you because all of these relationships are merged into an optimization problem where the unknowns are exactly x1 x2 x3 and l1 l2 l3 and in this way the algorithm will find the best solution that makes sense or all of this relationship together and then the last bit comes to solve the islam problem imagine that we we walked a very very long way so imagine i've made this chain very long and at some point we are at time x 50 and in reality we should be in x2 because we have kept track of all of the measurement we have collected so far and so we have a map of them and we close the loop by thinking okay i'm exactly in the same place as x2 so i can create a relationship between x2 and x50 in this way all of the error we have accumulated up to x50 gets corrected the map gets warped to match reality and we have solved the islam problem it's oversimplified if i say islam problem is solved many many professors will become angry yeah you have solved some problem in this specific situation yes you did but not generally solved no yeah so in practice is yet not solved because there are many problems related to noise in the sensors and inaccuracy in the calibration so there are techniques to make this uh solution more robust for instance so if you have islam system working then you know where you are and what's around you and also robots don't know where they are and what's around them so we can make do autonomous things jane street is a research driven trading firm with offices in new york hong kong and london full of impossibly creative and clever people they like to think the next star individuals that may be working for them could be watching this computer file video right now that's why they're hoping you might peruse their website including this section for internships are these the sorts of jobs that might launch your career check out jane street there's a link in the video description and our thanks to them for supporting this episode of computer file you