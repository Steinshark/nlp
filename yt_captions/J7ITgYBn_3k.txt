there we go that was for flip okay that's very important is that a is that a peen every time I see ASCII art I just assume it's a pen it's not a peen but you could imagine it could be you know what I mean anyways scaling up the Prime video audio video monitoring service and reducing cost by 90 the move from distributed micro Services architecture to a monolith application help achieve higher scale resilience and reduced costs if type one in the chat if anyone here is surprised by this okay there's some ones not a lot of ones not not a huge amount okay at Prime video by the way I should probably bleep that out says you know I don't know if you guys know this but I work at Netflix so I should probably that out you know what I mean uh at Prime video we offer thousands of live streams to our customers to ensure that customers seamlessly receive content Prime video uh set up a tool to monitor every stream viewed by customers that sounds just like Amazon probably goes through ring doorbell this tool allows us to automatically identify perceptually perceptual quality issues for example block corruption uh or audio video sync problems and trigger a process to fix them that's kind of interesting I wonder how they do some of these things uh I wonder why they have a service to monitor it I mean it feels like Amazon should have heard of like Kafka and Atlas by now is that like a thing do they know about Atlas do they know about Kafka like you just set up an alert on Atlas you know and just walk away you know uh our video quality analysis team at Prime video already owned a tool for audio video Quality Inspection but we never intended nor designed it to run at high scale our Target was to monitor thousands of concurrent streams and grow that number over time that sounds like what happened it sounds like a goal achieved while onboarding more streams to the service uh we noticed that running the infrastructure at high scale was very expensive we also noticed scaling bottlenecks that prevented us from monitoring thousands of streams so we took a step back and Revisited the architecture and existing service focused on the cost and scaling bottlenecks I really hope this involves rust the initial version of our let's say our service consisted of a distributed components that were orchestrated by AWS step functions the two most expensive operations in terms of cost were the orchestration workflows when data passed between distributed components to address this we moved all the components into a single process to keep data transfer within process memory which also simplified orchestration logic because we compiled all the operations into a single process uh we could rely on this one huh is this like a story about how not to use Amazon by Amazon this it just seems really strange to me okay I'm just gonna be real with you this seems really strange to me because in my head if you wanted to solve this you would have a client that periodically uploads or updates uh and sends a message up to a server that has like a bit rate or time-weighted bit rate uh or any of these magical kind of things you're gonna you want to you know give to the server along with like what video ID you're watching and some basic other items right uh and you take that little you take that little nice little item and you just throw that through a queue and make it into some big data portal whatever that big data portal is Hive uh I prefer Atlas for real-time monitoring blah blah blah blah blah blah you set all that crap up right and then the next thing you know you can just react to any show going wrong now if they're monitoring individual persons like it kind of almost looks like they're trying to say they're they're identifying individual perception problems and trying to fix that on an individual content basis which seems a little bit kind of or on individual stream basis that seems a little bit weird that seems like you're trying to make a client-side decision probably for it but on the server you know like if you're if you had an AV sync issue and you wanted to fix it you would just fix it on the client seems like I don't know distributed systems overhead our service consists of three major components the media converter converts uh input audio and video streams to frames or decrypted audio buffers that are sent to decoders defect detectors execute algorithms that analyze frames and audio buffers in real time looking for defects what you're looking at all the video what what who does who does that they must have like 10 000 customers and send real-time notifications whenever a defect is found for more information on this topic how uh Prime video uses machine learning to ensure video quality article apparently not Netflix thank you for sharing company secrets so I'm just thinking this is kind of crazy to try to process does that mean they're double streaming always is that what I'm hearing right now is that is is that what I'm hearing effectively the stream goes to two places you know the client could just tell you this you don't need machine learning to tell you the client goes hey uh buddy that stream you gave me right there is all like low quality and a bit  up okay could you take care of that for me uh we designed our initial solution as a distributed system using serverless components serverless oh my goodness everyone loves serverless for example AWS step functions are AWS Lambda I try to say it with the most like intense robot sound uh which was a good choice for Building Services quickly in theory this would allow us to scale each service component independently however the way we use the components caused us to hit a hard scaling limit around five percent of the expected load [Laughter] yeah also the overall cost of the building blocks nevertheless I love it parallel execution detector one detect wait hold on no don't tell me are they actually taking a stream of video and then processing the video more than once like each one of the videos are processed by different compute units with different detector algorithms you know Amazon's front end is in JavaScript you know you could just put a streaming algorithm in JavaScript you know that right you know you can update it you can test it you pay me three million dollars a year right now I'll come over to Amazon make it four actually make it four make it four make it for gonna help you out uh Netflix uh we yeah we do we do a lot of sweet things uh the main scaling bottleneck of the architecture was the orchestration management that was implemented using AWS step functions our server our service performed multiple State transitions for every second of the Stream So we quickly reached account limits the second cause problem we discovered uh was about the way we were passing video frames images around the different components please tell me they decode it and actually look at images please tell me please tell me they're actually transferring images they're not just doing like P and n and B frames they're actually because so like to do a b frame you have to have uh the state of the of the stream in here so the if they're passing around just images to look at stuff they might actually be decoding it and then passing raw dog and images around this dude this would be so great please tell me this is true to reduce computationally expensive video conversion jobs we built the microservice that splits video into frames and temporarily uploads of images just to monitor a service dude your client knows the bit rate oh my goodness and televisions tell you like if they're getting completely effed off oh my goodness what is this world we live in oh oh the defect detectors where each of them also runs a separate Michael service and then downloads images and processes however the high number of tier one calls to the yesterday but speaking of tier ones anyone have a tier one sub for me anybody a little tier one sub a little tier one sub yeah is this a joke this feels like how you would build this this honestly feels like modern JavaScript this the this is like modern JavaScript in my head uh to address the bottlenecks we initially considered fixing problems separately due to reduce cost and increase scaling capabilities we experimented and took a bold decision we decided to re-architect our infrastructure please say rust say rust five dollars a month uh we realized that distributed approach wasn't bringing a lot of benefits to our specific use case so here's the problem is this isn't a distributed approach the problem is that the approach is wrong right like there could be a distributed approach that makes a lot more sense but this is just crazy right can we all agree that this is just crazy version of distributed approach you're literally taking a stream of video which has been optimized to run through like a 90 Hertz decoder to render onto like a 60 hertz renderer and then you're actually taking this this and you're piping you're double piping it when you decode each frame you're uploading it to S3 then you have separate jobs that read from Edge S3 each image to determine the quality of it like that's crazy talk like how are you just didn't know if the client is having a good or bad experience you know the bit rate like you know how many stalls are happening like you know things because you download the video like you know you know up here you know why okay anyways uh conceptually the heart I must have missed something uh we realized that the distributed approach wasn't bringing a lot of benefits really uh so we packed all the components into a single process that eliminated the need for S3 buckets yep yep as an intermediate storage uh for video frames because our data transfer now happened in memory data transfer in memory super efficient uh we also implemented orchestration that controls components within a single instance the following diagram shows the architect of the decision uh after migrating to a monolith I genuinely just think that they're just doing the wrong thing overall like I'm happy that they're discovering that when you do serverless like a lot of serverless it goes crazy I mean it's kind of funny that Amazon's now discovering just a lot of serverless goes crazy goes Burr uh but whatever I can't even think about this conceptually the high level architecture Remains the Same we have exactly the same components we had at the initial design media conversion detectors and orchestration this allowed us to reuse a lot of code and quickly migrate to a new architecture the initial design we could scale several detectors horizontally and each of them are ran as a separate microservice oh Noah so adding a new detector required to do that means they're really splitting these streams how many video streams are going through this system how many are actually going through the system does that mean when you stream once on on AWS you're literally like seven streams it's your stream plus each detector I think I might have to buy puts against Amazon our team regularly adds more detectors to the surface and we already exceed the capacity of a single instance uh to overcome this problem we clone the service multiple times parameterize each copy with a different subset of detectors we also implemented the lightweight orchestration layer to distribute customer requests the following diagram shows our solution for deployment detectors when the capacity of a single instance exceeds they still have a bucket that means they're still like the the analyzing results it still ran from a bucket I know there must I mean dude Real Talk Atlas uh Atlas uh Atlas a real time uh metrics uh there we go Atlas real-time metrics like oh my goodness why are you giving me oh my goodness is trying to is trying to oh has an atlas now oh my goodness don't use Atlas that's not for you all right whatever I'm not even gonna look at it Mongo's now taking over the word and it's impossible to find uh all right let's keep on going we'll look at it later all right let's see microservice and serverless components are tools that do work at high scale are they Amazon sounds like you were failing at like 10 000 but whether to use them over a monolith has made uh uh has to be made on a case-by-case basis moving our service to a monolith reduced our infrastructure cost by over 90 and increased our scaling capabilities today we're able to handle thousands of streams and we still have the capacity to scale the service even further thousands think yeah yeah yeah yeah so there you go there's Atlas there's the atlas one here I'll pull it up for just a second yeah we we do a lot of we do a lot of Alice Alice is great Alice like the fun little story so Atlas is like just effectively a real-time counter engager that's all it is and then you can overlay it with week over week and then on top of that you can take it and when there's a high enough difference between week over week and your new value you can actually cause alerts to go out it's like a super cool tool and what I did is that at Netflix when we are expanding internationally and we are developing all of our Originals because I started before we had next uh house of cards right so I start we get House of Cards uh that was great but that involved all of our own assets and then we get another original and then another original and then all of a sudden we have 20 originals and then the next year was like going from 20 to 400 and we've never we had no way to know are the images actually there in production in the 15 different languages we supported or whatever all the different things are right so it's like very very intense and so what I did is I took every single image and created an atlas metric by uh by a couple different things and if it was missing from the back end I would log it as missing through Atlas and so we could tell at any point what video anywhere was missing images and so that was like a huge huge nice thing sorry if there's ads that's twitch twitch has ads twitch twitch ads everybody um but yeah so it was super cool it solved all of our image Problems by using Atlas alerting and a couple simple metrics right like not even hard you know what I mean it's pretty pretty exciting all right let's keep on going some decisions we've taken are not obvious but they resulted in significant improvements such as streaming all the videos we could handle thousands okay cool thousands thousands of streams all right neat I wonder I wonder if Netflix crosses the billions of streams at this point you're gonna have to 10x your 10x to 10x your 10x to 10x the 10x to get up to Netflix so I hope you got I hope you're ready for that scaling uh whereas running media conversion once in caching its outcome might be considered to be a cheaper option we have found this uh not to be a cost-effective approach the changes we've made allow Prime video to monitor all streams viewed by our customers okay hold on I swear I'm take I swear am I just hearing this here Amazon has thousands of concurrent streams old man Judo you missed the beginning where they literally process every single request every single video okay get this Judo they take the video that the customer receives pipes it to their back end their back end decodes it uploads the images to S3 as cache and then other service reads these out to test the quality of set image and looks for detector problems that is what they mean by cached and scaling and efficiency okay and what I'm reading literally right now is Amazon Prime has thousands of concurrent streams that literally means right now I could very well be like half of Amazon Prime right now I have half of Amazon Prime all right more hey this was just sad to read uh but I love that they're improving engineering I always love to see better performance so good job then but I think you could take a different solution altogether to make this a lot better the name is the stream again I work at Netflix by the way did you know that