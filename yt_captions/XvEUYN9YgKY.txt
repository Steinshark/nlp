ready to go yes we are next now with live with the next talk which will be Matt Bentley we have a bit of an echo so I just ask you to get started okay hey everybody um so title of the talk is performance maners new tricks for old dogs um if you're not familiar with the English phrase It's You Can't Teach an old dog new tricks so that's what we're going to attempt to to do today um so all the stuff that we're covering is stuff that I've kind of done in the past or this year or that sort of thing just a kind of collection of stuff really um so we're going to go through three algorithms three containers and one issue which will remain mysterious Until the End um so the first algorithm is PF fre order Ray which is a portmanto of reorder plus arrays if you're not sure what a portmanto is it's a combination of two words form a new word second one is plf list which is a more case friendly Or unrolled List uh PF IND sort which is a port manto of indir plus sort p q a faster better q p stack a faster better stack and sectors which is a method for random access in containers with multiple memory blocks and a growth factor that's the literal name of the paper that I wrote which is on the website pb.org um all of this stuff is on pb.org now so first thing P free order Rays Evolution extension of the Swap and Pop idium um most people are familiar with Swap and pop IDM if you're not basically if you've got a vector or a deck and you want to erase from anywhere but the back of the container or the front for deck and you don't care about element reordering um what you can do rather than erasing and then having all the elements reallocate backwards by one which can be quite slow if you've got a very large l or non-trivially copyable type um or just a large number of elements is you can swap the back element with the element that you want to erase and then pop the new back element um which is pretty fast most people nowadays realize that instead of swapping and popping you can just move and pop instead which saves a bunch of instructions and also means that you don't have to allocate a temporary buffer for the swap so this is basically what that is but templated and it covers all the sort of odd cases um so it's only useful if you don't care about element order basically um what I found was that this pattern also extends to range rasure provided that you take into account the potential overlap between the range that you want to erase and the back range you've got to try and Kate that range and it also extends to sray sray z type functionality um basically it ends up being the equivalent of range ER Asia using the return from Sid partition with an inverted predicate as the beginning part of the range um but without all the unnecessary swaps and copies that you get from partitioning operation so I'll just give you some performance results so through this talk going to do be giving a few sort of Benchmark averages because don't have time for individual benchmarks only got an hour um so everything that I'm giving in is an average uh across numbers of elements ranging from 10 to a million with 4840 and 490 by types um for reorder is the performance more or less linearly scales with the size of the type and the number of elements and also potentially for some elements um whether they're non-trivially copyable um Etc so singular Erasure averages is about a million times faster than a raise position with a minimum of 10% that's like 10 four by elements for example and a maximum of 25 million which is like 1,490 by elements right uh Ranger Ranger not quite so fascinating um averages 12,000 faster than standard ordered erasa with a minimum of 20% and a maximum of 77 1000% uh scan eraser averages 38% faster than Cay if with a minimum of 0% and a maximum of 180% so that gives some good reasons to use this over regular Erasure if you don't care about order right so range Erasure just go briefly into how that's done so we've got the area that we want to erase in the middle there and then we've got the area at the back that we want to move into that place we've got an overl AP and basically the overlap nothing happens to it it just gets erased so all the area that isn't overlapped gets moved into place and the area at the back gets erased so that's basically it um so the advantage of using what I've done over rolling your own I mean you can just roll your own templates is fine um but just takes care of edge cases like nonmovable and nonno throw copiable types in those cases it will actually allocate a temporary buffer um just in case there's an exception during copy processes in which case it'll copy the elements that were going to be erased back into place um it's also optimized for deck so it'll move and pop from the front instead of the back where appropriate highly optimized users men copy when the type is trivia or that sort of thing um and it also introduces a range based St ra St raise if style order unordered eraser so basically s SAS of uh for when you have a sub range of a container that you want to arrange certain types from rather than the entire container so here's an overview of how that works so you've got the range that you want to raise certain type of element from in this case the 12 uh so we move elements from the back but there's a 12 at the back so we can't use that one 12 gets moved to where the 23 was and we're pretty much done that's kind of simplifying a but it a bit but it gives the general idea so that's the first algorithm of the day moving on to the first container of the day uh this is PF list which is a Cas friendly linked list I did do a talk on this at Pacific C++ in 2017 that's on the website if you want to kind of know more than what I presented here you can go to that talk made one or two mistakes in that talk but it's all right um so the first thing you might ask is is okay um performance why do you care about a Lynch list fair enough um Lynch lists do have some positive performance C uh characteristics in some circumstances uh they have the fastest singular nonb backfront eraser in current stood containers if done during iteration or if you already have an iterator to the location that you're wanting to erase uh they have the fastest nonb front ordered insertion of stit containers they have the fastest sorting for large or non-trivially copyable removable types um why is that it's because sought for St list uh doesn't move elements around it just changes the values of the previous and next pointers that those elements are attached to um so when you've got a very large or non-trivially copiable type it can be a lot faster to just be writing a bunch of pointer values um rather than actually moving elements around and that sort of thing even if the elements themselves aren't contagious in memory um the next point is something that people use St list quite a lot for including an open source projects like labor office um iterates and pointers to non-ar ased elements stay valid regardless of operations around them for example if other elements get spliced in or erased or inserted or emerge happens um the iterators and the pointers to the non- erased elements stay valid which can be quite important if you've got for example multiple containers hosting different elements which point to each other which is a quite common scenario in a game engine but you probably wouldn't see still list used in a game engine for performance reasons um lists linked lists also give you constant time splicing together of lists and of ranges within lists um which can be quite useful particularly if you're returning stuff from concurrent functions and you want to combine the data um and it's relatively easy to create C concurrent versions or to use Lynch list in concurrent scenarios simply because the arrays and insert operations don't have a lot of side effects so they write a couple of pointers to the previous and next nodes but that's about it so plf list versus stood list performance averages 2 93% faster insertion 57% faster rer 17% faster iteration 77% faster sorting 70% faster reversal 91% faster remove remove us 63% faster unique 81% faster clear about a million times faster for triv trivially destructible types uh 1,200% faster destruction 63 100% faster for trivially destructible types we'll get to that and 20 to 24% faster performance overall in ordered use case benchmarking I.E when you're inserting erasing and iterating on data in a container over time so just a circle back to uh clear and destruction so a million perent is a lot so you might be asking yourself why um well for a link list if you're wanting to clear then you've got to go through and you've got to uh destruct all the elements if it's non-trivially destructible but you also have to deallocate each of the elements so you're going through memory generally speaking not contigous in memory unless you're using an allocator and even in that case probably not entirely contagious um so it's jumping all over the place in memory and it's just doing a deallocation for every single node plf list allocates nodes in chunks rather than individually um and what that means is when you get a clear operation if the type is trivially destructible you don't have to process every node you just say goodbye to all the memory blocks you just delate all of them so that's why it's such a high percentage so how's this faster how do we make it faster as I said it's got more continuious storage uh because we're allocating elements and nodes and chunks um so down the bottom there imagine those smiley faces are elements and um so that's how vectors and arrays are stored in memory most hashes Maps lists Etc are kind of all over the show in memory and unrolled lists decks Colony Hive Etc are all in chunks um the second way that we get things faster is having smart erased node reuse so rather than just letting those erased nodes sit there and not be reused we keep track of them and the last thing is that we have an indirect sort techniques so disadvantages there are some disadvantages of this implementation um there's no singular element or range-based splicing between lists there's only full container splicing and that's because we're allocating nodes in chunks so if you're splicing with st list um you can take all these elements and check them under here and it doesn't matter uh cuz they're just individual allocations but uh with PF list you can't split a block in half because then you invalidate all of your pointers and iterators to Elements which means that it's not actually a splice operation anymore it's more like a move or copy element um oper operation um so you do have full container splicing so you can take this plf list and this plf list and combine them um and you can also splice within the list so uh you know taking a range within a given list or an element within a given list and essentially reordering stuff um there's also more side effects for operations because you have to keep track of erased notes and this sort of things so it's less straightforward to use an a lock free or concurrent environment so here's the basic structure of it you have a vector of pointers to groups which are basically app pointed to a memory block plus block metadata um the memory BLX starts small have a growth factor and the maximum size is kept relatively low to allow for better reinsertion we'll get to that later on so raiseed nodes we can't reallocate backwards like you do with a vector or a dick or you invalidate all the pointers to your Elements which is one of the reasons we use p uh any kind of Lynch list so what do we do instead well okay start off with if we don't have erased nose here's what we do when we're inserting to some point in the Lynch list so doesn't matter where we're actually inserting to in memory where our location is um we're just putting the element at the back of the list and then we link up the previous and next pointers pretty straightforward um but not ideal because that means during iteration you're going okay this element this element this element in memory and we're going all the way over here to the back and then we go here here here Etc so not perfect um if we actually have erased nodes then we can make this a little bit better so when erased elements exist um first of all we check to see if uh if there are any erased elements in the same block as the element that we're actually inserting in front of and if they are we insert into that block if not we actually keep a previously stored pointer to the last block uh that we inserted into that had erases so we check there first and if so we reuse that um and if not that then basically we scan from the Block that contains the element uh that we're inserting in front of and we scan out to the right and the left to the different blocks until we find a block with erases in it and you might not think that would be efficient but it is because essentially we have that Vector of uh groups so that means that it's all contigous in memory it's actually a very fast operation um and I did try a bunch of other approaches when I was developing this um and the this combination of approaches had the best performance overall so now if we've got these arrayed nodes but there aren't any arrayed nodes in the block that contains the element that we're inserting in front of then we basically scan outwards and we find an raiseed node and we insert to that pretty straightforward and that having it scan outwards like that tends to increase case locality because when we're iterating we're going okay this block this block this block so even if we're here and we're here and this block is in a different place to this block in memory doesn't matter cuz during iteration we're going to be um going to that block next anyway right so it serves us to have that in the case at that point so even if we're iterating and it goes here here here this block here here here and this then this block still more C friendly right um so just just to go into briefly how we organize this reuse of blocks we have a free list head um for each block which points to the last erased element in that block and uh the next pointer for that erased elements points to the previous one that got erased and the previous one that got erased and that sort of thing so that's how we keep track of stuff so that's insertion Erasure um lastly we'll go into the sort technique uh which is indirect sorting uh so it has four phases gather phase we create a temporary array of in pointers and linearly iterate over the plf list blocks rather than following the sequence because L linearly iterating over the blocks um is more Cas friendly we're not jumping all over the place in memory and we fill the array with pointers to each element that we find in those blocks sort phase we sort the pointers by the value of the elements that they point to Via a d referencing functor and stood it's actually stood sort I forgot to correct that anyway by stood sort um using stood stable sort wouldn't make any sense because of the way that we gathered the elements in the first place is already unstable so then we have the scatter phase we rewrite the previous and next pointers of each list node following the order of the pointer array and then we delete the temporary array so just to show how that works so the sort phase beforehand we have our array of pointers pointing to each of the elements after sorting the array um the array essentially points to the correct elements in sequence so 1 2 points to three points to Four Points to five Etc um and then when we get to the scatter phase so we're actually going to move the elements now well not move the elements change the previous and next pointers on each of the elements so we process the pointer array sequentially and for every pointer at index I we dreference to the node and set the previous and next pointers for that node to the addresses in the pointer array at ius1 and I +1 respectively and we just do that right across the array and then it's sorted then we just need to set the node pointed to by array index zer as the front node for the plf list instance and the node pointed to by the last element in the sorry the last index in the array as the back node and we're done which brings us to our next algorithm which is based on that whole thing it's called pend sort and this was basically me going okay this is kind of useful how can I apply this to like all containers and it's very similar uh we have the same phases but they operate slightly differently The Gather phase we create a temporary array of instructs of pointers plus indexes and fill it with pointers to each element we fill each index with its index into the array so 0 1 2 3 4 Etc sort phase exactly the same we sort the value uh sort the structs by the value of the elements they're point PO is point to Via the D referencing functor and stood sort scap phase we find move chains we'll go into that and swap the elements around until we are complete and then delete the temporary array so again sort phase up the top very similar to what we did with plf list it said that we've got old index there as well after the sort phase um same sort of thing so 0 1 1 2 and then next one is four so basically as soon as we encounter an old index that doesn't match its current index into that array so in this case the first one is the four there then we know that we've got something that needs moving around before that point it's fine we don't need to move anything around so this bit is kind of complicated um it actually took me a while going back over my notes and over the code to actually go Ah that's how it works okay um and I've explained it and simplified it as best I can but it's still kind of complicated so you just going to have to run with it anyway so scatter phase um now what you'll see here are the move Loops so any move operation at the end of the day will end up with a certain number of elements that need to be moved around and uh if there's only two of them then it's essentially a swap which is what you can see with the two and the four there you know the uh old index four that one points to the three old index two that points to the five and you can see that those two elements in the actual container need to be swapped around right and each move chain is exactly like that but with more elements in between in the swap process so what do I mean by that well if you go up to the blue uh move chain there 17 the 15 the 16 you can see that uh the 17 is pointing to the 15 that's where that needs to go uh the 15 is pointing to the 16 um so that's where the 16 needs to go is into that place and the 16 is pointing to the 17 that's where that needs to go into that place so essentially but you know we can't copy we can't just copy those in we have to actually move them all around and keep a temporary there to store the first one that we moved around otherwise we'll end up overwriting stuff before we have a chance to copy its original value into place so essentially it becomes like a swap operation but with more values being swapped in between so a move chain right so basically what we do if we look at the simplest one that we've got there which is the four and the two so we go to the four we go okay four doesn't match index 2 into this array so this is something that needs to be moved and we look at the value that it points to which is the three we store that in temporary variable and then we look at old index that points to four so we go to index 4 into the current array which is has the old index of two and we take the value that that points to the 5 and point we put that into uh the area in memory that uh old index location Four Points to so basically we put the five where the three is and then we go okay this one has an old index of two so we go back to two in the array and we go Ah that's the four that's where we started with so now we know that we're at the end of the move chain and we can take take our temporary variable and copy it into the place pointed to by the last index that we processed which is the one with the old index of two which points to the five so we take our temporary which holds the three and we put it to where the five is and then we're done and then the last part of that process is that we write the new updated old index values to uh the old index so where it is four becomes two and two becomes four that way as we continue to process this array of structs so um after we go past the four we'll go to the five and we go okay that's part of a move chain that needs to be fixed after we've sorted that move chain we'll go to the next one and rather than the old index being two as it was before it'll be four so we know that that one has already been sorted it's already in place and we don't need to process it again so that's the basic Rundown in very uh abstract terms if you want um the more rigorous um pseudo code interpretation of that it's this so iterate over the pointer array until we find a move loop I the old index is not equal to the current index into the array move the element pointed to into a buffer variable store the current array index as disc and also as start store old index as Source move the element at array bracket source. location to array brackets desk. location where location is the pointer to the element in the um original container Copy Source to dist copy array brackets d. old index to source and copy D to array brackets dist do old index keep on doing this until source is equal to start again then move the element in buffer to array brackets disc. location and we're done and go back to step one and continue processing the array until you get to the end of the array so um clear as mud great we'll move on um so there are some technical optimizations that you can do to this um if you're doing this with a random access container you don't actually require the pointer you only require old index because um the current index in that array of structs is the same as the index into the actual container um old index if you're sorting Random Access containers again cuz you don't need the pointer old index can be reduced in bit dep appropriate to the number of elements so if you've got less than 256 elements you can use an 8bit old index for example um and thanks to B for this one um this actually comes from a talk that he did earlier on in the year and I watched it and I went ah um so when size of T is smaller than oh I'll just tell you what he said so he was talking about sorting a stood list and he said oh take the elements in the list and we copied them to an array and we sort the array and then we cop the elements back and I thought yeah that's a lot that's a lot more straightforward than what I'm doing at least for elements where their size of is smaller than size of T Star Plus old index you know if you've got St list of ins or stood list of doubles it makes a lot more sense to just do that copy the elements out to an array sort them copy them back in then it does to do what I've just done but if you've got non-trivially copyable types or if you've got any type that's larger than a couple of pointers basically um then my way of doing it saves a heck of a lot of memory um and will be faster um so I Incorporated technique into what I do with desort so it basically checks the size of the type and whether it's trivially copyable um or movable and it switches um the algorithm based on that so across uh 1 by 2 by 4 by 48 by and 490 by types and number of elements ranging from 10 to a million 250 C uh basically what I did was I took a basic bidirectional compatible intros sord algorithm and I tried that on plf hive and PF Colony um and compared that with Indie sort and it's two IND sort is 250% faster than the uh bidirectional compatible intros sort algorithm um it's 28% faster than S list sort when you're using S list on types smaller than 2 92 byes um again why is that because if you're sorting very large or non-trivially copyable types it's much faster to just change the addresses on the previous and next pointers than it is to move elements around but if the elements are smaller it's much faster to do it this way um it's 146% faster than stood sort uh with vectors or arrays on types larger than 152 bytes and also possibly some non-vi non-trivially copiable um types which are also not movable um same sort of thing if you've got reasonable size types you know doubles small structs Etc um then it's actually faster to just move them around then do this indirect process even though with this indirect process most elements only end up getting moved or copied once uh at most twice which is with that um the first element in the move chain gets copied out to the temporary and back um yeah so that's basically it so it can be very useful uh depending on what your data type is and what type of container that you've got um moving on next container pfq a faster adaptive Q so why is St Q not adequate um it uses deck by default and unfortunately um deck implementations very wildly because they're a little bit underspecified in the standard um you know for example Microsoft's implementation and this is not their fault I'll go into that um it's has a fixed block size of 16 bytes which means that if you have a deck uh with any type that is over 8 bytes um essentially the deck becomes a Lynch list which is not great um that's not their fault they inherited that ABI from dware um I correct me if I'm wrong somebody who was was the original company who supplied their STL implementation before they took it over and started doing it themselves um I know that they want to change that or some people do within the company but that would require an ABI Break um but even if you're not using an implementation like that like the other implementations have their problems as well uh lib C++ that has a fixed block size of 512 which is okay but you know if you've only got a very small number of elements and the elements are very small then it's problematic if you've got a s a size of type that's very large again it can become a link list and lib C++ is one gets around the ladder by having a very large block size but it is a fixed block size of 4,096 bytes which means that if you only have a small number of elements um or a very small element type then you get you know way too much memory waste um so neither of these uh optimal in any sense but it's what they've got and it's fixed in AI so they can't change it um so pfq adapts block capacities to reflect size um so it doesn't have that fixed size it actually changes block capacities over time as the size of the container changes the number of elements um so it's more appropriate to the use of a q than de complementations are it also uses blocks from the front to the back um unless the sizes changed significantly in which case it'll deallocate one of those blocks and get a bigger one or a smaller one and I know that uh lib C++ I think Microsoft's implementation of deck also kind of does this like reserves a block in place when it's um been kind of popped off the front or whatever um I don't think libed c++'s one does um has iterators for debug in purposes and the user can specify minimum and maximum block capacities so Benchmark averages ends up being 20% faster for one by types 10% faster for four by types 15% faster for 8 by types 40% faster for 40 by types and 65% faster for 4 90 by types um this is benchmarked against uh libed C++ so obviously you would get differences and probably quite significant differences when you're benchmarking against Micosoft implementation for the reasons I just specified um The Benchmark is a pump test so elements are pushed and popped consecutively with the total number of elements just sort of fluctuating over time like an accordion so that's that um plf list is I think possibly my first container because it was originally developed to be part of colony to be the thing that stored um erased element locations before I realized there was a much better way of doing that um then it split off into its own thing uh and most of its code is very similar to pfq uh which was developed later so again why is stood stack not adequate uses deck by default as the underlying container for the adapter um same problems there um if you use the underlying container as being a vector that's much worse um the reason for that is that Stacks only do three things they push back they pop back and they read the top element so Vector has very good iteration performance um it does have good back Erasure you know pop back um but it doesn't have as good insertion performance because when you get over capacity you have to allocate to a new block um so deck is a lot faster for insertion so deck's lack of need to reallocate elements upon insertion does help vector's iteration speed does not help but again there's no growth factor in blocks for deck so PF stick creates new blocks with a growth factor of two the user can Define minimum and Max block capacities it also has iterators so basic structure and PF cures very similar to this it's just an intrusive Lynch list of blocks um a you could also do it as a vector of pointers to blocks as an implementation if you wanted to would be the same sort of thing right um bench mark averages versus stood stack basically about 80% faster for all types except for 490 by types and again that's due to libad c++'s deck issue where the fixed block capacity is 512 bytes so when you got a 490 by type or anything that's over 256 bytes um it essentially becomes a length list so that's why that one is so much different uh the benchmarking question is total time taken to construct push all elements read and pop all the elements and then destruct the container now some of you may be thinking yes but couldn't we make a deck with a growth factor in its block and bypass all of this the answer is yes but then the question becomes okay if we want to do that how would we make a fast operator brackets as well as fast iterator operators plus minus plus equals and minus equals and this is where sectors come in um so name of the paper that I wrote Is Random Access of elements in data containers using multiple memory blocks of increasing Capac capacity and sector is um basically the Prototype container that I made to test this Theory and it works I haven't published The Container because I would have to flesh it out more and do a bunch of optimization and for some reason I haven't been bothered yet so um sigor is a portmanto of uh segmented plus Vector the basic idea is you have a Vector of pointers to element blocks uh then you create blocks with a growth factor of two and the total capacity of the container as a whole so for example if your first block capacity is eight next one would be eight that doubles the capacity next one would be 16 Etc uh the initial block capacity must be a power of two use these things together to calculate from the sequence index supplied to operator brackets both the block index and the vector of pointers and the subblock index of the element so the index within the actual block Itself by using power of two block capacities with the growth factor of two we make the most significant active bit IE one instead of zero um of the sequence index that we Supply to operator brackets relate directly to the block index what do I mean by that well okay so here's is how it looks in memory Vector of poins to element blocks first block capacity is four next one is four next one is eight Etc pretty straightforward here's the concepts that you need to understand sequence index is the index of an element within a container block index the sequential number of a given memory block in the vector of pointers subblock index the sequential number of any element within a given memory block bitwise index the index of a bit within a number when measuring from the least significant bit to the most significant bit for example if you have the number three then that is one one and then a bunch of zeros right so the most significant uh bit that has a one in it would be 01 so location one is the bitwise index so most significant active bit is the bitwise index of the most significant active bit of Any Given number a the highest bit set to one so in the case of the three that would be one so here is the pseudo code um for the operator brackets Al algorithm the first two lines only happen once during the container's lifetime so we have m is equal to the most significant active bit index of the first Block's capacity right then sh which is is our shift value um if uh m is equal to Z then it's Z if it's not equal to Z then it is M minus one right and we can pretty much get rid of M at that point because that's the last time we use it um then in the actual operated brackets algorithm we have J is equal to the sequence index I what is supplied to their operator brackets algorithm bit shifted to the right by sh our shift value then B IE the block index is equal to the most significant active bit index of J and then K uh basically if B is equal to zero then K is Zer if B is not equal to Z then it then K is equal to one bit shifted to the left by B the block index plus our shift value and then s i our subblock index is equal to the sequence index minus k okay okay so as an example if we have our first blocks capacity is eight and we have our operator brackets and it's supplied with 15 so we want to get to element 15 in the container now if we think about that logistically um so first block capacity is eight um next block capacity has to double this the capacity of the container as a whole so that will also be eight um operator brackets is measuring from zero which means that 15 will be the last element in the second block right so m is equal to msab index of 8 which is 3 sh is equal to well is 3 equal to Z no so it is 1 * 3 - 1 so it's 2 J is equal to 15 but shifted to the right by two which is three block index is equal to the msab index of 3 which is 1 K is equal to okay is 1 equal to Z no so it's one bit shifted to the left by 1 + 2 which is 8 and then our subblock index is equal to 15 - 8 so we have a block index of one so 0 1 second block and we have a subblock index of seven 0 1 2 3 4 5 6 7 so it's at the end of that block so that works okay now now calculating msab index cuz I've just sort of glossed over that there most modern processors contain an instruction designed specifically to calculate that um so you have the bsrl uh BSR W instructions on Intel x86 CPUs but where that's not the case it's very simple to do code like this where you just get your number and you just bit shift it to the right um until the result is equal to zero and you add one Frid Point uh so pretty straightforward um so the number of total number of operations is only going to be as large as the number of bits in that number so here is our actual code for the brackets operator member function so we get supplied our sequence index that is index and we have unsigned block index we Supply that as the the return value to our assembly in structure which is bsrl we Supply to bsrl index shifted to the right by our shift value which has been pre-calculated and then we return Vector of pointers to our blocks and we index into that to our block index which we've just calculated and then we basically dreference to the actual block itself and the subblock index is our sequence index minus is block index equal to zero no then bit shift one to the left by block index plus the shift value so clear as mud so how does this all relate to Dex well it doesn't you can just Implement a seor by itself and everything will be fine you can just have a sequence container that just expands with a number of blocks in One Direction and the only thing that you have to do if you want to turn it into a a deck is basically have two sectors and have them extending in different directions now the illustration that I've got here uh the seor on the left there I've actually Illustrated it kind of backward backwards to show how it would work um but actually in memory it would be the reverse of that um but this is just easier to illustrate um to make people understand it so this is basically Bally how you would do it you would start with only one Sig tour which is the one on the right and you just have one Sig tour until such Point as the user pushes to the front enough that it extends Beyond uh the first block so say you've got the one on the right there and you and it's filled up the way that it is at the moment and then you push front and so you know you're going beyond the St part of the first block capacity there at that point you would create a second sigor so the one on the left there and you'd start inserting into that um so that much is pretty straightforward and it's actually relatively easy to go back and forward at any point in time there's only ever going to be two sectors in that process or one or two of them right I mean you could start with one over here and then you push front push front push front and you end up with the one on the left there um and then you pop from the back pop back pop back pop back and then it gets rid of the right hand one and then you've just got the left hand one and it could go back and forward like that but at any point in time there's only going to be two of these things now complicates things a little bit if you want to use operator brackets but basically what you would do is if there's more than one sector at any given point in time um um you basically keep separate size Rec uh size records for both of the different sectors and if the sequence index goes over the left sector's size if the left seor exists at that point then you know it's in the right sector and you do uh operate brackets calculation from the previous one um and basically all of your iterator operators plus equals m equal minus equals that sort of thing uh can be implemented via the operator brackets operations so you just okay you've got your element you work out what its um sequence index is in the container you add or subtract a value from that sequence index and then you supply it back to operator brackets or you can do it a more complicated way but that's that's the that's the simplest way of doing it right um okay so that ends that discussion that is how you have a deck with uh multiple memory blocks with increasing capacities um I don't know if I'll ever get around to doing that maybe somebody else can it's a little bit of a complicated operation anyway Food For Thought So final thing of the day this is the uh issue that I was referring to somewhat mysteriously at the beginning of the talk um it doesn't relate entirely to the rest of the conversation here but it is a different slightly different way of thinking about things so it's not quite a new trick but it kind of is so this is my perspective um time complexity is an implementation detail what do I mean by that why do we care about time complexity um and it's all practical things like there's no unless you're going into pure mathematics if you are actually operating on a computer which is a fixed bit of Hardware with a fixed number of chips and this sort of thing um all of the reasons we actually care about time complexity are practical reasons right there's no Theory to it per se we care about performance latency and potentially resource use in some scenarios so number of CES or amount of memory use Etc as it relates to a fixed number in um now we won't go into res resource use that is a valid concern it's not actually one that comes up that often though so we'll focus on the first two there the problem is that you know since the earlier days of computers uh RAM speed is increasingly been dwarfed by CPU speed as almost everybody knows now um so that has decreased time complexity's relevance to the first two by making the whole process of what work is um and what work the computer is doing a lot more complic at what work the user programs is still you know kind of the same but anyway what we have to look at is what really is time complexity in my view it's meant to be a measure of work done right which is arguably correct from a very abstract perspective but um you know this is computer science we're Engineers we're working on Hardware not something out there in the Nether world so it's actually not abstract um and what actual work gets done is always Hardware dependent so um how much work ends up getting done for a given operation that you supply the operation itself may be a singular instruction but it might inadvertently trigger a whole crap load of work right so whether a branch prediction fail forces a pipeline reload is Hardware dependent whether a read from non-c contigous memory causes a c Mish is Hardware dependent has it got a cache most of the time yes some very rare processes don't have C nowadays but the vast vast vast say 99.9% do uh for those reasons because Ram speed is so much slower than CPU speed right and those are much bigger workloads for the hardware than singular instructions so I think when we're talking about uh time complexity largely we're kind of talking about the wrong kind of work or in my view we are weighing it perhaps incorrectly right so to look at work we have to look at what is typically most important to Performance and latency and this is kind of my rough list and it varies a lot based on Hardware don't get me wrong but so this is a very rough thing Cas locality of data inter memory speed parallelism allocation deallocation branching although that's becoming less problematic um as of more recent processes um back in the core two days very significant um and and lastly I'd say time complexity it's very rough as I said um given that that might be the case therefore it's only when the first four are not affecting performance significantly that time complexity becomes a strong concern okay so but case locality parallelism branching are all considered Hardware based implementation details and not something that we talk about in the standard or in computer science in general right um generally speaking I'm sure some professors and computer scientists universities do but you know the reasons why we care about those things are the same reasons we care about time complexity um asides from potentially the resource use um performance and latency so it's only typically once they're eliminated that we should be worrying really significantly about time complexity and the fact that we have to consider that based on our Hardware whether time complexity is actually relevant to the performance in our scenario is what makes it implementation detail in my view and you know you can go back and forth with semantics but this is the way I'm looking at it currently might change my mind so in examples of this so the performance Improvement of reorder over regular Erasure improves based on size of not just the number of elements and this is not purely down to the cost of copying but also the case locality of the larger elements so if you have you know 1 million chars for example and they all fit in memory um you know obviously ly the reallocation operation caused by regular Ria on a vector from nonb is going to be a lot less than if it's a larger element that doesn't all fit in the cache um both St list sort and St sort have o in login complexity but that has nothing to do with their performance which is entirely DCT dictated by the algorithm and Cas locality and that's why with Indie sort it can be a heck of a lot faster to just copy the elements out into memory and then stood sort them and then copy them back right even though you're essentially um you know you're not changing the pointers you're actually moving elements around um P left stack and St deck insertion are both o1 amortized but one of those gets faster over time due to the lower number of allocations because plf stack has a growth factor in the number of blocks more examples P left list insertion is at worst o n in the number of memory blocks in that contain container but it doesn't stop at being 2.93% faster than St list insertion which is01 due to the number of allocations being low and the memory block metadata being contigous in memory for many sets of data it can be faster to do the same operation many times rather than to store the result once in memory due to memory speed i n can be faster than o1 but typically the N is fairly limited um strong example of this Doom 3 the game The BFG addition of that which they basically rewrote the engine 10 years later after the initial release um there's some technical notes for that that you can look up online and one of the things that they found was that they were pre-calculating these meshes and then for each object and then storing those meshes in memory and then you reusing them for each rendering pass and what they found was that it was actually faster to just calculate recalculate that mesh for every single rendering pass and I from memory there could be quite a number of rendering passes um than it was to store it once in memory and that's because the processes had changed so much and the difference in performance of memory speed versus CPU speed had changed so much um that the pre-calculation and storing it retrieving it from memory was slower than just calculating again and lastly differences between alternative skip field methods in pf Colony one of which is o the current one and one of which is o n in the number of elements in that particular block made about 1% performance difference overall so yeah anyway that's basically it so why are we so hung up on time complexity hung up is a bit negative perhaps I should be saying you know why do we put so much emphasis on time complexity and we don't tend to talk about the other ones so much particularly when we're dealing with kind of Standards issues and this sort of thing I think the reasons are largely historical you know back when C++ started time complexity was a much much more relevant concern um the effects that we've had that have changed over the past 30 years of CPUs and memory speed and blah blah blah blah blah um have been gradual they've happened over time um whereas time complexity is sort of something that stays static but its relevance does not stay static and that's the problem but it's got institutional entrenchment so we kind of have to talk about it um and also time complexity is a little bit fun to talk about like I kind of compare it to musical notation understanding music theory and that sort of thing it doesn't make any difference to how good the music sounds at all it doesn't it's not relevant to how good music sounds but um it can be fun to talk about and fun to understand so and time complexity is not irrelevant to Performance like if you get all your ducks in a row if you get everything lined up if you get a vector which has all your data and it's all contigous in memory and your items are you know trivially copyable and you're not doing a lot of branching and um you're not doing a lot of deallocating and allocating this sort of thing then once you've got that scenario then it's all based purely on time complexity and everything just sort of scales according to plan based on that uh value of N and that's great and that's a very real concern and it's a consistently useful concern um but what I'm kind of not okay about is the fact that we spend and waste so much time talking about time complexity um as if it's the be all end or or something like that or has this huge relevance compared to the other things and we don't talk about any of these other things so I think either we should be talking about all of these things or we should be talking about none of them um because at the end of the day it's just another implementation detail to consider once you've got all the other stuff out of the way sorted thought about Etc and that is my talk thank you everybody for listening who listened um all of the Technologies and blah blah blah that I've discussed are on p.org and I wish you all a great day thanks for sitting through till the end bye thank you for your talk that's been great and if there's any more for disc and question please join the lunch table with and have discuss with about all the things he mentioned great talk thank cool thank you