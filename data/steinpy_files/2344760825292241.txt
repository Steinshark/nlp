import torch

class generator(torch.nn.Module):

    def __init__(self,z_len=1024,n_f=128):
        super(generator,self).__init__()

        self.model1 = torch.nn.Sequential(  
                                            #1x1    -> 2x2
                                            torch.nn.ConvTranspose2d(in_channels=z_len,out_channels=n_f*8,kernel_size=4,stride=1,bias=True),
                                            torch.nn.BatchNorm2d(n_f*8),
                                            torch.nn.ReLU(True),
                                            # torch.nn.Conv2d(1024,1024,5,1,2,bias=False),
                                            # torch.nn.BatchNorm2d(1024),
                                            # torch.nn.ReLU(True),

                                            #2x2    -> 4x6
                                            torch.nn.ConvTranspose2d(n_f*8,n_f*8,4,2,1,bias=True),
                                            torch.nn.Upsample(size=(4,6)),
                                            torch.nn.BatchNorm2d(n_f*8),
                                            torch.nn.ReLU(True),
                                            # torch.nn.Conv2d(512,512,5,1,2,bias=False),
                                            # torch.nn.BatchNorm2d(512),
                                            # torch.nn.ReLU(True),

                                            #4x6    -> 8x12
                                            torch.nn.ConvTranspose2d(n_f*8,n_f*8,4,2,1,bias=True),
                                            torch.nn.BatchNorm2d(n_f*8),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(n_f*8,n_f*8,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(n_f*8),
                                            torch.nn.ReLU(True),

                                            #8x12   -> 16x24
                                            torch.nn.ConvTranspose2d(n_f*8,n_f*4,4,2,1,bias=True),
                                            torch.nn.BatchNorm2d(n_f*4),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(n_f*4,n_f*4,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(n_f*4),
                                            torch.nn.ReLU(True),

                                            #16x24  -> 32x48
                                            torch.nn.ConvTranspose2d(n_f*4,n_f*4,4,2,1,bias=True),
                                            torch.nn.BatchNorm2d(n_f*4),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(n_f*4,n_f*4,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(n_f*4),
                                            torch.nn.ReLU(True))
                                            
        self.prev1  =  torch.nn.Sequential(
                                            torch.nn.Conv2d(n_f*4,64,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(64),
                                            torch.nn.ReLU(True),

                                            torch.nn.Conv2d(64,3,3,1,1),
                                            torch.nn.Tanh()
        )
        self.model2 = torch.nn.Sequential(

                                            #32x48  -> 64x96
                                            torch.nn.ConvTranspose2d(n_f*4,n_f*4,4,2,1,bias=True),
                                            torch.nn.BatchNorm2d(n_f*4),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(n_f*4,n_f*4,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(n_f*4),
                                            torch.nn.ReLU(True))
        self.prev2  =  torch.nn.Sequential(
                                            torch.nn.Conv2d(n_f*4,64,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(64),
                                            torch.nn.ReLU(True),

                                            torch.nn.Conv2d(64,3,3,1,1),
                                            torch.nn.Tanh())
        
        self.model3 = torch.nn.Sequential(

                                            #64x96  -> 128x192
                                            torch.nn.ConvTranspose2d(n_f*4,n_f*2,4,2,1,bias=True),
                                            torch.nn.BatchNorm2d(n_f*2),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(n_f*2,n_f*2,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(n_f*2),
                                            torch.nn.ReLU(True))
        self.prev3  =  torch.nn.Sequential(
                                            torch.nn.Conv2d(n_f*2,n_f,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(n_f),
                                            torch.nn.ReLU(True),

                                            torch.nn.Conv2d(n_f,3,3,1,1),
                                            torch.nn.Tanh())
        
        self.model4 = torch.nn.Sequential(

                                            #128x192-> 256x384
                                            torch.nn.ConvTranspose2d(16,8,4,2,1,bias=True),
                                            torch.nn.BatchNorm2d(8),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(8,8,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(8),
                                            torch.nn.ReLU(True),

                                            #256x384-> 512x768
                                            torch.nn.ConvTranspose2d(8,8,4,2,1),
                                            torch.nn.Conv2d(8,8,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(8),
                                            torch.nn.ReLU(True),
                                            torch.nn.Conv2d(8,3,3,1,1),
                                            torch.nn.Tanh()
                                            )

        self.model  = torch.nn.ModuleList([self.model1,self.model2,self.model3,self.prev1,self.prev2,self.prev3,self.model4])
    
    def forward(self,x):
        return None
    
    def forward1(self,x):
        return self.prev1(self.model1(x))
    
    def forward2(self,x):
        return self.prev2(self.model2(self.model1(x)))
    
    def forward3(self,x):
        return self.prev3(self.model3(self.model2(self.model1(x))))

class generator2(torch.nn.Module):

    def __init__(self,in_ch=832):
        super(generator2,self).__init__()

        self.model  = torch.nn.Sequential(  
                                            #UPSAMPMLE LAYERS 
                                            torch.nn.ConvTranspose2d(in_channels=in_ch,out_channels=1024,kernel_size=4,stride=2),
                                            torch.nn.BatchNorm2d(1024),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(1024,512,4,2),
                                            torch.nn.BatchNorm2d(512),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(512,256,4,2),
                                            torch.nn.BatchNorm2d(256),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(256,256,4,2),
                                            torch.nn.BatchNorm2d(256),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(256,128,4,2),
                                            torch.nn.BatchNorm2d(128),
                                            torch.nn.ReLU(),



                                            #CONV LAYERS 
                                            torch.nn.Conv2d(256,128,3,1,1),
                                            torch.nn.BatchNorm2d(128),
                                            torch.nn.ReLU(),

                                            torch.nn.Conv2d(128,32,3,1,1),
                                            torch.nn.BatchNorm2d(32),
                                            torch.nn.ReLU(),

                                            torch.nn.Conv2d(32,3,3,1,1),
                                            torch.nn.Tanh(),

                                            torch.nn.Upsample(size=(500,750))
                                            )

    def forward(self,x):
        return self.model(x)

class generator_lg(torch.nn.Module):

    def __init__(self,in_ch=832):
        super(generator_lg,self).__init__()

        self.model  = torch.nn.Sequential(  
                                            #UPSAMPMLE LAYERS 
                                            torch.nn.ConvTranspose2d(in_channels=in_ch,out_channels=768,kernel_size=4,stride=2,bias=False),
                                            torch.nn.BatchNorm2d(768),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(768,512,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(512),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(512,512,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(512),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(512,256,4,2,bias=False),
                                            torch.nn.BatchNorm2d(256),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(256,256,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(256),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(256,256,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(256),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(256,128,4,2,bias=False),
                                            torch.nn.BatchNorm2d(128),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(128,128,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(128),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(128,128,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(128),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(128,64,4,2,bias=False),
                                            torch.nn.BatchNorm2d(64),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(64,64,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(64),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(64,64,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(64),
                                            torch.nn.ReLU(),

                                            torch.nn.ConvTranspose2d(64,32,4,2,bias=False),
                                            torch.nn.BatchNorm2d(32),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(32,16,3,1,1,bias=False),
                                            torch.nn.BatchNorm2d(16),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(16,3,3,1,2),
                                            torch.nn.Tanh(),

                                            torch.nn.Upsample(size=(512,512+256))
                                            )

    def forward(self,x):
        return self.model(x)
    
class generator_sm(torch.nn.Module):

    def __init__(self,in_ch=832):
        super(generator_sm,self).__init__()

        self.model  = torch.nn.Sequential(  
                                            #UPSAMPMLE LAYERS 
                                            torch.nn.ConvTranspose2d(in_channels=in_ch,out_channels=700,kernel_size=4,stride=2,bias=False),
                                            torch.nn.BatchNorm2d(700),
                                            torch.nn.ReLU(),
                                            

                                            torch.nn.ConvTranspose2d(700,400,4,2,bias=False),
                                            torch.nn.BatchNorm2d(400),
                                            torch.nn.ReLU(),
                                            

                                            torch.nn.ConvTranspose2d(400,200,4,2,bias=False),
                                            torch.nn.BatchNorm2d(200),
                                            torch.nn.ReLU(),
                                            

                                            torch.nn.ConvTranspose2d(200,100,4,2,bias=False),
                                            torch.nn.BatchNorm2d(100),
                                            torch.nn.ReLU(),
                                            

                                            torch.nn.ConvTranspose2d(100,50,4,2,bias=False),
                                            torch.nn.BatchNorm2d(50),
                                            torch.nn.ReLU(),
                                            torch.nn.Conv2d(50,3,3,1,1),
                                            torch.nn.Tanh(),

                                            torch.nn.Upsample(size=(512,512+256))
                                            )

    def forward(self,x):
        return self.model(x)

class generator_progressive(torch.nn.Module):

    def __init__(self,in_ch=832,activation_fn=torch.nn.ReLU):
        super(generator_progressive,self).__init__()
        self.activation_fn   = activation_fn


        #16x24 -> 32x48
        self.upsample_1 = torch.nn.Sequential(
            torch.nn.Upsample(size=(32,48)),
            torch.nn.Conv2d(in_ch,1024,3,1,1,bias=False),
            torch.nn.BatchNorm2d(1024),
            self.activation_fn(),

            torch.nn.Conv2d(1024,1024,3,1,1,bias=False),
            torch.nn.BatchNorm2d(1024),
            self.activation_fn(),

            torch.nn.Conv2d(1024,1024,3,1,1,bias=False),
            torch.nn.BatchNorm2d(1024),
            self.activation_fn()
        )
        self.out_1      = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=1024,out_channels=256,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(256),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(64),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=64,out_channels=3,kernel_size=3,stride=1,padding=1),
            torch.nn.Tanh()
        )


        #32x48 -> 64x96
        self.upsample_2 = torch.nn.Sequential(
            torch.nn.Upsample(size=(64,96)),
            torch.nn.Conv2d(1024,768,3,1,1,bias=False),
            torch.nn.BatchNorm2d(768),
            self.activation_fn(),

            torch.nn.Conv2d(768,768,3,1,1,bias=False),
            torch.nn.BatchNorm2d(768),
            self.activation_fn(),

            torch.nn.Conv2d(768,768,3,1,1,bias=False),
            torch.nn.BatchNorm2d(768),
            self.activation_fn()
        )
        self.out_2      = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=768,out_channels=128,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(128),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(64),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=64,out_channels=3,kernel_size=3,stride=1,padding=1),
            torch.nn.Tanh()
        )

        #64x96 -> 128x192
        self.upsample_3 = torch.nn.Sequential(
            torch.nn.Upsample(size=(128,192)),
            torch.nn.Conv2d(768,768,3,1,1,bias=False),
            torch.nn.BatchNorm2d(768),
            self.activation_fn(),

            torch.nn.Conv2d(512,512,3,1,1,bias=False),
            torch.nn.BatchNorm2d(512),
            self.activation_fn(),

            torch.nn.Conv2d(512,512,3,1,1,bias=False),
            torch.nn.BatchNorm2d(512),
            self.activation_fn()
        )
        self.out_3      = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=512,out_channels=128,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(128),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=128,out_channels=16,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(16),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=16,out_channels=3,kernel_size=3,stride=1,padding=1),
            torch.nn.Tanh()
        )

        #128x192 -> 256x384
        self.upsample_4 = torch.nn.Sequential(
            torch.nn.Upsample(size=(256,384)),
            torch.nn.Conv2d(256,128,3,1,1,bias=False),
            torch.nn.BatchNorm2d(128),
            self.activation_fn(),

            torch.nn.Conv2d(128,128,3,1,1,bias=False),
            torch.nn.BatchNorm2d(128),
            self.activation_fn(),

            torch.nn.Conv2d(128,128,3,1,1,bias=False),
            torch.nn.BatchNorm2d(128),
            self.activation_fn()
        )
        self.out_4      = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=128,out_channels=32,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(32),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=1,bias=False),
            torch.nn.BatchNorm2d(32),
            self.activation_fn(),

            torch.nn.Conv2d(in_channels=32,out_channels=3,kernel_size=3,stride=1,padding=1),
            torch.nn.Tanh()
        )

        #256x384 -> 512x768
        self.upsample_5 = torch.nn.Sequential(
            torch.nn.Upsample(size=(512,768)),
            torch.nn.Conv2d(128,128,3,1,1,bias=False),
            torch.nn.BatchNorm2d(128),
            self.activation_fn(),

            torch.nn.Conv2d(128,64,3,1,1,bias=False),
            torch.nn.BatchNorm2d(64),
            self.activation_fn(),

            torch.nn.Conv2d(64,32,3,1,1,bias=False),
            torch.nn.BatchNorm2d(32),
            self.activation_fn(),

            torch.nn.Conv2d(32,3,3,1,1),
            torch.nn.Tanh()
        )

        self.model      = torch.nn.ModuleList([self.upsample_1,self.upsample_2,self.upsample_3,self.upsample_4,self.upsample_5,self.out_1,self.out_2,self.out_3,self.out_4])


    def forward(self,x,ep):
        if ep == 0:
            return self.forward1(x)
        elif ep == 1:
            return self.forward2(x)
        elif ep == 2:
            return self.forward3(x)
        elif ep == 3:
            return self.forward4(x)
        else:
            return self.forward5(x) 

    def forward1(self,x):
        return self.out_1(self.upsample_1(x))
    
    def forward2(self,x):
        return self.out_2(self.upsample_2(self.upsample_1(x)))
    
    def forward3(self,x):
        return self.out_3(self.upsample_3(self.upsample_2((self.upsample_1(x)))))
    
    def forward4(self,x):
        return self.out_4(self.upsample_4(self.upsample_3(self.upsample_2(self.upsample_1(x)))))
    
    def forward5(self,x):
        return self.upsample_5(self.upsample_4(self.upsample_3(self.upsample_2(self.upsample_1(x)))))

class auto_encoder(torch.nn.Module):

    def __init__(self,activation_fn=torch.nn.LeakyReLU,activation_fn2=torch.nn.ReLU):
        super(auto_encoder,self).__init__()

        self.model      = torch.nn.Sequential(

            #512x768    -> 256x384
            torch.nn.Conv2d(3,32,5,1,2,bias=False),
            torch.nn.BatchNorm2d(32),
            activation_fn(),

            #256x384    -> 128x192
            torch.nn.Conv2d(32,64,5,2,1,bias=False),
            torch.nn.BatchNorm2d(64),
            activation_fn(),

            #128x192    -> 64x96
            torch.nn.Conv2d(64,128,5,2,1,bias=False),
            torch.nn.BatchNorm2d(128),
            activation_fn(),

            #64x96      -> 32x48
            torch.nn.Conv2d(128,128,5,2,1,bias=False),
            torch.nn.BatchNorm2d(128),
            activation_fn(),

            #32x48      -> 16x24
            torch.nn.Conv2d(128,256,5,2,1,bias=False),
            torch.nn.BatchNorm2d(256),
            activation_fn(),

            #8x12       -> 4x6
            torch.nn.Conv2d(256,256,5,2,1,bias=False),
            torch.nn.BatchNorm2d(256),
            activation_fn(),

            #4x6       -> 2x3
            torch.nn.Conv2d(256,256,3,1,1,bias=False),
            torch.nn.BatchNorm2d(256),
            torch.nn.AvgPool2d(2),
            activation_fn(),

            torch.nn.Flatten(),

            torch.nn.Linear(19712,4096),
            activation_fn(), 

            torch.nn.Linear(4096,2048),
            activation_fn(), 

            torch.nn.Linear(2048,4096),
            activation_fn(), 

            torch.nn.Linear(4096,12288),
            activation_fn(), 

            torch.nn.Unflatten(dim=-1,unflattened_size=(int(12288/24),4,6)),

            #4x6        -> 8x12
            torch.nn.ConvTranspose2d(int(12288/24),512,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(512,512,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(512),
            activation_fn2(),

            #8x12       -> 16x24
            torch.nn.ConvTranspose2d(512,256,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(256,256,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(256),
            activation_fn2(),

            #16x24       -> 32x48
            torch.nn.ConvTranspose2d(256,128,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(128,128,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(128),
            activation_fn2(),

            #32x48       -> 64x96
            torch.nn.ConvTranspose2d(128,128,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(128,128,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(128),
            activation_fn2(),

            #64x96       -> 128x192
            torch.nn.ConvTranspose2d(128,128,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(128,128,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(128),
            activation_fn2(),

            #128x192     -> 256x386
            torch.nn.ConvTranspose2d(128,64,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(64,64,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(64),
            activation_fn2(),

            #256x386     -> 512x768
            torch.nn.ConvTranspose2d(64,32,4,2,1,bias=False),
            activation_fn2(),
            torch.nn.Conv2d(32,32,3,1,1),
            activation_fn2(),
            torch.nn.BatchNorm2d(32),
            activation_fn2(),


            #CONV
            torch.nn.Conv2d(32,16,3,1,1,bias=False),
            torch.nn.BatchNorm2d(16),
            activation_fn2(),

            torch.nn.Conv2d(16,8,3,1,1,bias=False),
            torch.nn.BatchNorm2d(8),
            activation_fn2(),

            torch.nn.Conv2d(8,3,3,1,1,bias=False),
            torch.nn.Tanh()
        )   

    def forward(self,x):

        return self.model(x)

class discriminator(torch.nn.Module):

    def __init__(self,activation=torch.nn.LeakyReLU):
        super(discriminator,self).__init__()

        self.activation=activation

        #512x768 -> 128x192
        self.model4  = torch.nn.Sequential( 

                #512x768 -> 256x384
                torch.nn.Conv2d(3,32,3,1,1,bias=False),
                torch.nn.BatchNorm2d(32),
                self.activation(inplace=True),
                torch.nn.MaxPool2d(2),

                #256x384 -> 128x192
                torch.nn.Conv2d(32,64,3,2,1,bias=False),
                torch.nn.BatchNorm2d(64),
                self.activation(inplace=True))
        

        #128x192 -> 64x96
        self.adapter3   = torch.nn.Sequential(
                torch.nn.Conv2d(3,32,3,1,1,bias=False),
                torch.nn.BatchNorm2d(32),
                self.activation(inplace=True),

                torch.nn.Conv2d(32,64,3,1,1,bias=False),
                torch.nn.BatchNorm2d(64),
                self.activation(inplace=True),
        )
        self.model3     = torch.nn.Sequential(
                torch.nn.Conv2d(64,128,5,2,2,bias=False),
                torch.nn.BatchNorm2d(128),
                self.activation(inplace=True))
        

        #64x96  -> 32x48
        self.adapter2   = torch.nn.Sequential(
                torch.nn.Conv2d(3,64,3,1,1,bias=False),
                torch.nn.BatchNorm2d(64),
                self.activation(inplace=True),

                torch.nn.Conv2d(64,128,3,1,1,bias=False),
                torch.nn.BatchNorm2d(128),
                self.activation(inplace=True))
        self.model2     = torch.nn.Sequential(
                torch.nn.Conv2d(128,256,3,2,1,bias=False),
                torch.nn.BatchNorm2d(256),
                self.activation(inplace=True))
        

        #32x48  -> 1x1
        self.adapter1   = torch.nn.Sequential(
                torch.nn.Conv2d(3,32,3,1,1,bias=False),
                torch.nn.BatchNorm2d(32),
                self.activation(inplace=True),

                torch.nn.Conv2d(32,256,3,1,1,bias=False),
                torch.nn.BatchNorm2d(256),
                self.activation(inplace=True),
        )
        self.model1     = torch.nn.Sequential(
                torch.nn.Conv2d(256,512,3,2,1,bias=False),
                torch.nn.BatchNorm2d(512),
                self.activation(inplace=True),

                torch.nn.Conv2d(512,1024,3,2,1,bias=False),
                torch.nn.BatchNorm2d(1024),
                self.activation(inplace=True),

                torch.nn.Conv2d(1024,1024,3,2,1,bias=False),
                torch.nn.BatchNorm2d(1024),
                self.activation(inplace=True),

                torch.nn.Conv2d(1024,1,(4,6),1,0,bias=False),
                torch.nn.Flatten(),
                torch.nn.Sigmoid())
        
        self.model  = torch.nn.ModuleList([self.model1,self.model2,self.model3,self.adapter1,self.adapter2,self.adapter3,self.model4])

    def forward(self,x):
        return None
    
    def forward1(self,x):
        return self.model1(self.adapter1(x))

    def forward2(self,x):
        return self.model1(self.model2(self.adapter2(x)))

    def forward3(self,x):
        return self.model1(self.model2(self.model3(self.adapter3(x))))
    
    def forward4(self,x):
        return self.model1(self.model2(self.model3(x)))




if __name__ == "__main__":
    pass