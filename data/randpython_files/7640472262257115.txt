import torch 
from torch.utils.data import Dataset,DataLoader
import numpy 
from matplotlib import pyplot as plt 
import random 
import time 
from gen import load_locals,preload_ds
import os 
import sys 
sys.path.append("C:/gitrepos/projects/ml/music") 
from functional import rebuild,rebuild_scale

DEV                 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
imgs                = ["C:/data/images/converted_tensors/"+fname for fname in os.listdir("C:/data/images/converted_tensors/")]

class UpSampleNet(torch.nn.Module):


    def __init__(self,start_dim:int,final_dim:int):

        #Super init
        super(UpSampleNet,self).__init__()

        #Define vars 
        conv_act            = torch.nn.GELU
        up_act              = torch.nn.LeakyReLU
        final_act           = torch.nn.LeakyReLU

        #Create modules
        self.convLayers     = torch.nn.Sequential(
            torch.nn.Conv2d(3,32,3,1,1),
            conv_act(),
            torch.nn.Conv2d(32,64,3,1,1),
            conv_act(),
            torch.nn.Conv2d(64,128,3,1,1),
            conv_act(),
            torch.nn.Conv2d(128,256,5,1,2),
            conv_act()
        )

        #Make 2 jumps up to final size - linearly 

        first_dim           = int(start_dim + (final_dim-start_dim)/3) 
        second_dim          = int(start_dim + 2*(final_dim-start_dim)/3)

        self.upLayers       = torch.nn.Sequential(
            torch.nn.Upsample(size=(first_dim,first_dim)),
            torch.nn.Conv2d(256,128,5,1,2),
            up_act(),

            torch.nn.Upsample(size=(second_dim,second_dim)),
            torch.nn.Conv2d(128,64,5,1,2),
            up_act(),


            torch.nn.Upsample(size=(final_dim+4,final_dim+4)),
            torch.nn.Conv2d(64,32,5,1,2),
            up_act()
        )

        self.finalLayers    = torch.nn.Sequential(
            torch.nn.Conv2d(32,16,5,1,2),
            final_act(),

            torch.nn.Conv2d(16,16,3,1,1),
            final_act(),
            
            torch.nn.Conv2d(16,16,3,1,1),
            final_act(),
            
            torch.nn.Conv2d(16,16,3,1,1),
            final_act(),
            
            torch.nn.Conv2d(16,16,3,1,1),
            final_act(),
            
            torch.nn.Conv2d(16,16,3,1,1),
            final_act(),
            
            torch.nn.Conv2d(16,16,3,1,1),
            final_act(),

            torch.nn.Conv2d(16,3,3,1,1),
            torch.nn.Tanh()
        )


    def forward(self,x:torch.Tensor)->torch.Tensor():
        x           = self.convLayers(x)
        x           = self.upLayers(x)
        x           = self.finalLayers(x)
        x           = x[:,:,2:-2,2:-2]
        return x
    

class DiscrNet(torch.nn.Module):

    def __init__(self,in_dim:int):

        #Super init
        super(DiscrNet,self).__init__()

        #Define vars 
        conv_act            = torch.nn.GELU
        lin_act             = torch.nn.LeakyReLU
        final_act           = torch.nn.LeakyReLU

        linear_size         = int(in_dim / 32)

        #Create modules
        self.convLayers     = torch.nn.Sequential(
            torch.nn.Conv2d(3,16,3,1,1,bias=False),
            torch.nn.BatchNorm2d(16),
            conv_act(),

            torch.nn.Conv2d(16,32,5,2,2,bias=False),               #   /2  
            torch.nn.BatchNorm2d(32),
            conv_act(),

            torch.nn.Conv2d(32,64,5,2,2,bias=False),               #   /4
            torch.nn.BatchNorm2d(64),
            conv_act(),

            torch.nn.Conv2d(64,128,5,2,2,bias=False),              #   /8 
            torch.nn.BatchNorm2d(128),
            conv_act(),

            torch.nn.Conv2d(128,128,5,2,2,bias=False),             #   /16
            torch.nn.BatchNorm2d(128),
            conv_act(),

            torch.nn.Conv2d(128,256,5,2,2,bias=False),             #   /32 128:4   256:8  512:16
            torch.nn.BatchNorm2d(256),
            conv_act(),
            torch.nn.Flatten(1),
        )

        self.linearLayers   = torch.nn.Sequential(
            torch.nn.Linear(256 * linear_size * linear_size,256),
            torch.nn.Dropout(p=.05),
            lin_act(),

            torch.nn.Linear(256,64),
            torch.nn.Dropout(p=.025),
            lin_act(),

            torch.nn.Linear(64,1),
            torch.nn.Sigmoid()
        )

    def forward(self,x:torch.Tensor) -> torch.Tensor:
        x           = self.convLayers(x)
        x           = self.linearLayers(x)
        return x


def apply_distortion(img:torch.Tensor,start_dim:int=48,noise_sf:float=.1,noise_iters:int=4) -> torch.Tensor:

    #Downscale 
    sf              = start_dim / img.shape[-1]
    downscaled      = torch.nn.functional.interpolate(img,scale_factor=sf)

    #Apply noise
    for _ in range(noise_iters):
        downscaled  = downscaled + (torch.randn(size=downscaled.shape,device=downscaled.device) * noise_sf)
        downscaled      = torch.clip(downscaled,-1,1)
    # plt.imshow(numpy.transpose(((img[0]+1)/2).numpy(),[1,2,0]))
    # plt.show()
    # plt.imshow(numpy.transpose(((downscaled[0]+1)/2).numpy(),[1,2,0]))
    # plt.show()

    return downscaled


def tfloat_to_np(x:torch.Tensor)->numpy.ndarray:
    
    #convert to 1-255
    x           = x.detach().cpu()
    x           = x + 1 
    x           = x / 2
    x           = x * 255
    x           = x.int()
    x           = x.numpy()
    x           = numpy.transpose(x,[1,2,0])
    return x 


def make_grid(x:torch.Tensor,y:torch.Tensor,ep:int,batch:int,sf=1)->None:
    x           = x.cpu()
    y           = y.cpu()
    #Take first 3 imgs 
    width       = 2* y.shape[-2]
    height      = 3* y.shape[-1]

    #Refurbish1 
    x1              = x[0].unsqueeze_(dim=0)
    x1              = torch.nn.functional.interpolate(x1,scale_factor=sf)[0]
    refurbished1    = torch.ones(size=(y.shape[1:]))
    refurbished1[:,:x1.shape[1],:x1.shape[2]]    = x1
    row1            = torch.cat([refurbished1,y[0]],dim=1)

    #Refurbish2
    x2              = x[1].unsqueeze_(dim=0)
    x2              = torch.nn.functional.interpolate(x2,scale_factor=sf)[0]
    refurbished2    = torch.ones(size=(y.shape[1:]))
    refurbished2[:,:x2.shape[1],:x2.shape[2]]    = x2
    row2            = torch.cat([refurbished2,y[1]],dim=1)

    #Refurbish3
    x3              = x[2].unsqueeze_(dim=0)
    x3              = torch.nn.functional.interpolate(x3,scale_factor=sf)[0]
    refurbished3    = torch.ones(size=(y.shape[1:]))
    refurbished3[:,:x3.shape[1],:x3.shape[2]]    = x3
    row3            = torch.cat([refurbished3,y[2]],dim=1)

    finalimg        = torch.cat([row1,row2,row3],dim=2)



    plt.imshow(tfloat_to_np(finalimg))
    if not os.path.isdir("C:/gitrepos/projects/ml/Image/outs"):
        os.mkdir("C:/gitrepos/projects/ml/Image/outs/")
    plt.savefig(f"C:/gitrepos/projects/ml/Image/outs/ep{ep}_b{batch}")

    
def train(ds_path,n_ep,bs):

    #Clear out
    filenames           = ["C:/gitrepos/projects/ml/Image/outs/"+file for file in os.listdir("C:/gitrepos/projects/ml/Image/outs")]
    for f in filenames:
        os.remove(f)



    #LOCAL OPTIONS
    start_dim           = 48 
    final_dim           = 256
    max_distorts        = 20
    max_strength        = .1
    max_n               = 1024

    #TRAIN OPTIONS 
    lr                  = 1e-4
    wd                  = .001
    betas               = (.75,.9993)
    bs                  = bs


    #Create model and optim
    model               = UpSampleNet(start_dim=start_dim,final_dim=final_dim).to(DEV)
    optim               = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=wd,betas=betas)
    loss_fn             = torch.nn.MSELoss()

    #Prepare data
    dataloader      =   load_locals(bs=bs,processor=None,local_dataset_path=ds_path,max_n=max_n)
    
    #Stats
    losses              = []

    for ep in range(n_ep):
        
        print(f"\n\tEPOCH {ep}")
        losses.append([])    

        #Run train loop
        for batch_i,item in enumerate(dataloader):

            #Zero
            optim.zero_grad()

            #Get base img
            base_img    = torch.nn.functional.interpolate(item.to(DEV).type(torch.float),size=(final_dim,final_dim))

            #Apply distortion
            x           = apply_distortion(base_img,noise_sf=random.random()*max_strength,noise_iters=random.randint(0,max_distorts))    
            #Send thorugh model 
            pred        =  model.forward(x)

            #Loss
            loss        = loss_fn(pred,base_img)
            losses[-1].append(loss.mean().item())
            loss.backward()

            #Grad 
            optim.step()
            if batch_i % 2 == 0:
                batchstr    = " "*20 + "[" + str(batch_i) + f"/{len(dataloader)}]" 
                print(f"\tbatch{batchstr[-12:]}\tloss={(sum(losses[-1])/len(losses[-1])):.5f}")
                #Check work
                with torch.no_grad():
                    model.eval()
                    imgs        = model.forward(x)
                    make_grid(x,imgs,ep,batch_i)
                    model.train()

        print(f"\tep loss={(sum(losses[-1])/len(losses[-1])):.5f}")
 

def compute_svd2(flat_tensors,shuffle=False,max=1_000_000,transpose=True,chunking=100,q=4096,center=True,clipped=False):

    #Load all waveforms 
    torch.manual_seed(512)
    
    #load files 
    vectors     = flat_tensors.copy()
    matrix      = torch.stack(vectors)

    #Do the Transpose 
    if transpose:
        matrix      = matrix.T
    if clipped:
        matrix      = matrix.clip(min=-.5,max=.5) / .5
    #Perform PCA on wavefo
    print(f"performing SVD on {matrix.shape}")
    U, S, V     = torch.pca_lowrank(matrix,q=q,niter=4,center=center)
    V           = V.T
    #U,S,V       = torch.linalg.svd(matrix,full_matrices=False)
    
    
    #print(f"U: {U.shape}\tS:{S.shape}\tV:{V.shape}")
    
    return U,S,V


def to_pca_grayscale(img:torch.Tensor,size:int=64)->torch.Tensor:
    flattener       = torch.nn.Flatten(0)
    return flattener(torch.sum(torch.nn.functional.interpolate(img.float().unsqueeze_(dim=0),size=(size,size)),dim=1)/3)


def from_pca_grascale(img:torch.Tensor,size:int=64)->torch.Tensor:
    img             = torch.unflatten(img,0,sizes=(1,size,size))
    return torch.cat([img,img,img])


def pca_calculation(rank:int=4096,load_n:int=8192,downsize:int=32): 
    local_imgs      = imgs.copy()
    random.shuffle(local_imgs)
    validation      = local_imgs[-16:]
    local_imgs      = local_imgs[:load_n]

    t0              = time.time()
    flattener       = torch.nn.Flatten(0)
    tensors         = [to_pca_grayscale(torch.load(fname),size=downsize) for fname in local_imgs]

    #print(f"shape of -1 is {tensors[-1].shape}")
    u,s,v_t         = compute_svd2(tensors,q=rank,transpose=True,center=False)
    import math
    plt.plot([math.log(s_) for s_ in s])
    plt.show()
    s               = torch.diag(s)
    return u,s,v_t,tensors
    # print(f"U:{u.shape},S:{s.shape},V:{v_t.shape}")
    # tensorload      = to_pca_grayscale(torch.load(validation[0])).unsqueeze(dim=-1).T
    # was_mean        = tensorload.mean()
    # tensorload      = tensorload - was_mean
    # print(f"grayscale size is {tensorload.shape}")
    # reduce          = rebuild(tensorload,u,s,v_t,n_dims=1,mode='reduce').T
    # print(f"reduced shape is {reduce.shape}")
    # rebuilded       = rebuild(reduce,u,s,v_t,n_dims=1,mode='rebuild').T

    # print(f"rebuilded.shape={rebuilded.shape}")
    # print(f"done in {(time.time()-t0):.3f}s")

    # og_load     = from_pca_grascale(tensorload[0] + was_mean)
    # recon_load  = from_pca_grascale(rebuilded[0]) 
    # recon_load  /= max(abs(torch.min(recon_load)),torch.max(recon_load))

    # print(f"reconstructed load min:{torch.min(recon_load)},max:{torch.max(recon_load)}")

    # img         = torch.cat([og_load,recon_load],dim=1)
    # print(f"inmg shape {img.shape}")
    # plt.imshow(tfloat_to_np(img))
    # plt.show()


def hybrid_train(ds_path,n_ep,bs):

    #Keep random
    torch.random.manual_seed(512)

    #LOCAL OPTIONS
    start_dim           = 48 
    final_dim           = 256
    max_distorts        = 50
    max_strength        = .1
    max_n               = 65536
    sim_mode            = 'color'
    show_ratio          = 1/32

    #TRAIN OPTIONS 
    lr                  = 2e-4
    wd                  = 0#.01
    betas               = (.75,.9993)
    bs                  = bs
    GAN_p               = .33


    #Create model and optim
    modelG              = UpSampleNet(start_dim=start_dim,final_dim=final_dim).to(DEV)
    modelD              = DiscrNet(in_dim=final_dim).to(DEV)
    optimG              = torch.optim.Adam(modelG.parameters(),lr=lr,weight_decay=wd,betas=betas)
    optimD              = torch.optim.Adam(modelD.parameters(),lr=lr,weight_decay=wd,betas=betas)
    loss_fnG            = torch.nn.MSELoss()
    loss_fnD            = torch.nn.BCELoss()


    #Prepare data
    dataloader      =   load_locals(bs=bs,processor=None,local_dataset_path=ds_path,max_n=max_n)
    print(f"Training with {len(dataloader)*bs} items\n\n")
    
    #Stats
    losses              = {'realLoss':[],'upscLoss':[],'simLoss':[]}
    evals               = {'realEval':[1],'upscEval':[0],'simLoss':[]}

    #Clear out
    filenames           = ["C:/gitrepos/projects/ml/Image/outs/"+file for file in os.listdir("C:/gitrepos/projects/ml/Image/outs")]
    for f in filenames:
        os.remove(f)

    for ep in range(n_ep):
        
        print(f"\n\tEPOCH {ep}")

        #Run train loop
        for batch_i,item in enumerate(dataloader):

            #Get base imgs and distorted_imgs
            base_imgs       = torch.nn.functional.interpolate(item.to(DEV).type(torch.float),size=(final_dim,final_dim))
            dist_imgs       = apply_distortion(base_imgs,noise_sf=random.random()*max_strength,noise_iters=random.randint(0,max_distorts))    

            #Train discriminator on base_imgs 
            optimD.zero_grad()
            modelG.eval()
            true_prob       = modelD.forward(base_imgs.detach())
            true_labels     = torch.ones(size=true_prob.shape,device=DEV,dtype=torch.float)
            realLoss        = loss_fnD(true_prob,true_labels) * .25
            losses['realLoss'].append(realLoss.cpu().mean().item())
            evals['realEval'].append(true_prob.detach().cpu().mean().item())
            realLoss.backward()

            #Train discriminator on upsc_imgs
            modelG.train()
            upsc_imgs       = modelG.forward(dist_imgs)
            upsc_prob       = modelD.forward(upsc_imgs.detach())
            upsc_labels     = torch.zeros(size=upsc_prob.shape,device=DEV,dtype=torch.float)
            upscLoss        = loss_fnD(upsc_prob,upsc_labels) * .75
            losses['upscLoss'].append(upscLoss.cpu().mean().item())
            evals['upscEval'].append(upsc_prob.detach().cpu().mean().item())
            
            upscLoss.backward()
            torch.nn.utils.clip_grad_norm_(modelD.parameters(),1.15)
            optimD.step()

            #Train generator to fool
            optimG.zero_grad()
            upsc_prob2      = modelD.forward(upsc_imgs)
            upscLossG       = loss_fnD(upsc_prob2,true_labels) * GAN_p
            upscLossG.backward()


            #Loss from similarity
            upsc_imgs       = modelG.forward(dist_imgs)
            if sim_mode     == "grayscale":
                upsc_imgs       = torch.sum(upsc_imgs,dim=1) / 3 
                base_imgs       = torch.sum(base_imgs,dim=1) / 3
            simLoss         = loss_fnG(upsc_imgs,base_imgs) * (1-GAN_p)
            simLoss.backward()
            losses['simLoss'].append(simLoss.cpu().mean().item())
            torch.nn.utils.clip_grad_norm_(modelG.parameters(),1.15)
            
            optimG.step()


            if batch_i % int(show_ratio*len(dataloader)) == 0:
                batchstr    = " "*20 + "[" + str(batch_i) + f"/{len(dataloader)}]" 
                print(f"\tbatch{batchstr[-12:]}\tsimLoss={(sum(losses['simLoss'])/len(losses['simLoss'])):.4f}\trealEval={(sum(evals['realEval'])/len(evals['realEval'])):.4f}\tupscEval={(sum(evals['upscEval'])/len(evals['upscEval'])):.4f}")
                #Check work
                with torch.no_grad():
                    modelG.eval()
                    imgs    = modelG.forward(dist_imgs).detach().cpu()
                    modelG.train()
                make_grid(dist_imgs.detach().cpu(),imgs,ep,batch_i)
        
        if not os.path.isdir("C:/data/images/models/"):
            os.mkdir('C:/data/images/models/')
        torch.save(modelG.state_dict(),f"C:/data/images/models/modelG{ep}")
        torch.save(modelD.state_dict(),f"C:/data/images/models/modelD{ep}")


def find_closest(u,s,v_matrix:torch.Tensor,original,imgsize:int):
    highest_i           = [0,0]
    highest_val         = 0

    correlation_matrix  = v_matrix.T @ s**2 @ v_matrix

    correlation_matrix  = correlation_matrix.detach().cpu().numpy()
    
    for x in range(correlation_matrix.shape[0]):
        print(f"search {x}")
        for y in range(correlation_matrix.shape[1]):

            if correlation_matrix[x][y] > highest_val and not x == y:
                highest_val     = correlation_matrix[x][y]
                highest_i       = [x,y]


    print(f"closest imgs are {highest_i} at {highest_val}")

    img1    = v_matrix[:,highest_i[0]]
    img2    = v_matrix[:,highest_i[1]]

    img1    = rebuild(img1,u,s,v_matrix,n_dims=1,mode='rebuild')
    img2    = rebuild(img2,u,s,v_matrix,n_dims=1,mode='rebuild')

    img1    = from_pca_grascale(img1,size=imgsize)
    img2    = from_pca_grascale(img2,size=imgsize)

    img1    /= max(abs(torch.min(img1)),torch.max(img1))
    img2    /= max(abs(torch.min(img2)),torch.max(img2))

    img     = torch.cat([img1,img2],dim=2)

    img1r   = original[highest_i[0]]
    img2r   = original[highest_i[1]]

    img1r    = from_pca_grascale(img1r,size=imgsize)
    img2r    = from_pca_grascale(img2r,size=imgsize)
    print(f"imgs were shape {img1r.shape},{img2r.shape}")
    img2    = torch.cat([img1r,img2r],dim=2)

    img     = torch.cat([img,img2],dim=1)



    plt.imshow(tfloat_to_np(img))
    plt.show()




        



if __name__ == '__main__':
    # imgsize                 = 32
    # u,s,v,original          = pca_calculation(rank=32,load_n=16384,downsize=imgsize)
    # print(f"finished")
    # find_closest(u,s,v,original,imgsize=imgsize)
    # #hybrid_train("C:/data/images/converted_tensors/",10,64) 
    #train("C:/data/images/converted_tensors/",n_ep=8,bs=64)
    hybrid_train("C:/data/images/converted_tensors/",n_ep=64,bs=16)
        